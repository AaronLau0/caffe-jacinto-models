Logging output to training/imagenet_jacintonet11v2_2017-06-30_02-08-23/train-log_2017-06-30_02-08-23.txt
WARNING: gnome-keyring:: couldn't connect to: /run/user/30409/keyring-KJvviu/pkcs11: Connection refused
p11-kit: skipping module 'gnome-keyring' whose initialization failed: An error occurred on the device
I0630 02:08:24.384852 28163 caffe.cpp:209] Using GPUs 0, 1, 2
I0630 02:08:24.385319 28163 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0630 02:08:24.385648 28163 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0630 02:08:24.385972 28163 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0630 02:08:24.776280 28163 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/imagenet_jacintonet11v2_2017-06-30_02-08-23/initial/train.prototxt"
test_net: "training/imagenet_jacintonet11v2_2017-06-30_02-08-23/initial/test.prototxt"
test_iter: 1000
test_interval: 2000
base_lr: 0
display: 100
max_iter: 100
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/imagenet_jacintonet11v2_2017-06-30_02-08-23/initial/imagenet_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: true
iter_size: 2
type: "SGD"
I0630 02:08:24.776373 28163 solver.cpp:82] Creating training net from train_net file: training/imagenet_jacintonet11v2_2017-06-30_02-08-23/initial/train.prototxt
I0630 02:08:24.776844 28163 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0630 02:08:24.776849 28163 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0630 02:08:24.777000 28163 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 224
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/ilsvrc12_train_lmdb"
    batch_size: 42
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1000"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc1000"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc1000"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0630 02:08:24.777089 28163 layer_factory.hpp:77] Creating layer data
I0630 02:08:24.777180 28163 net.cpp:98] Creating Layer data
I0630 02:08:24.777186 28163 net.cpp:413] data -> data
I0630 02:08:24.777202 28163 net.cpp:413] data -> label
I0630 02:08:24.778026 28196 db_lmdb.cpp:35] Opened lmdb ./data/ilsvrc12_train_lmdb
I0630 02:08:24.780074 28163 data_layer.cpp:78] ReshapePrefetch 42, 3, 224, 224
I0630 02:08:24.780125 28163 data_layer.cpp:83] output data size: 42,3,224,224
I0630 02:08:24.805670 28163 net.cpp:148] Setting up data
I0630 02:08:24.805697 28163 net.cpp:155] Top shape: 42 3 224 224 (6322176)
I0630 02:08:24.805701 28163 net.cpp:155] Top shape: 42 (42)
I0630 02:08:24.805703 28163 net.cpp:163] Memory required for data: 25288872
I0630 02:08:24.805712 28163 layer_factory.hpp:77] Creating layer data/bias
I0630 02:08:24.805728 28163 net.cpp:98] Creating Layer data/bias
I0630 02:08:24.805734 28163 net.cpp:439] data/bias <- data
I0630 02:08:24.805744 28163 net.cpp:413] data/bias -> data/bias
I0630 02:08:24.806771 28163 net.cpp:148] Setting up data/bias
I0630 02:08:24.806779 28163 net.cpp:155] Top shape: 42 3 224 224 (6322176)
I0630 02:08:24.806783 28163 net.cpp:163] Memory required for data: 50577576
I0630 02:08:24.806797 28163 layer_factory.hpp:77] Creating layer conv1a
I0630 02:08:24.806809 28163 net.cpp:98] Creating Layer conv1a
I0630 02:08:24.806813 28163 net.cpp:439] conv1a <- data/bias
I0630 02:08:24.806818 28163 net.cpp:413] conv1a -> conv1a
I0630 02:08:24.809622 28198 blocking_queue.cpp:50] Waiting for data
I0630 02:08:24.810575 28163 net.cpp:148] Setting up conv1a
I0630 02:08:24.810585 28163 net.cpp:155] Top shape: 42 32 112 112 (16859136)
I0630 02:08:24.810588 28163 net.cpp:163] Memory required for data: 118014120
I0630 02:08:24.810598 28163 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 02:08:24.810607 28163 net.cpp:98] Creating Layer conv1a/bn
I0630 02:08:24.810611 28163 net.cpp:439] conv1a/bn <- conv1a
I0630 02:08:24.810617 28163 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 02:08:24.811313 28163 net.cpp:148] Setting up conv1a/bn
I0630 02:08:24.811321 28163 net.cpp:155] Top shape: 42 32 112 112 (16859136)
I0630 02:08:24.811326 28163 net.cpp:163] Memory required for data: 185450664
I0630 02:08:24.811337 28163 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 02:08:24.811342 28163 net.cpp:98] Creating Layer conv1a/relu
I0630 02:08:24.811347 28163 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 02:08:24.811352 28163 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 02:08:24.811365 28163 net.cpp:148] Setting up conv1a/relu
I0630 02:08:24.811370 28163 net.cpp:155] Top shape: 42 32 112 112 (16859136)
I0630 02:08:24.811374 28163 net.cpp:163] Memory required for data: 252887208
I0630 02:08:24.811378 28163 layer_factory.hpp:77] Creating layer conv1b
I0630 02:08:24.811393 28163 net.cpp:98] Creating Layer conv1b
I0630 02:08:24.811395 28163 net.cpp:439] conv1b <- conv1a/bn
I0630 02:08:24.811401 28163 net.cpp:413] conv1b -> conv1b
I0630 02:08:24.811715 28163 net.cpp:148] Setting up conv1b
I0630 02:08:24.811722 28163 net.cpp:155] Top shape: 42 32 112 112 (16859136)
I0630 02:08:24.811727 28163 net.cpp:163] Memory required for data: 320323752
I0630 02:08:24.811734 28163 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 02:08:24.811740 28163 net.cpp:98] Creating Layer conv1b/bn
I0630 02:08:24.811744 28163 net.cpp:439] conv1b/bn <- conv1b
I0630 02:08:24.811749 28163 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 02:08:24.812403 28163 net.cpp:148] Setting up conv1b/bn
I0630 02:08:24.812409 28163 net.cpp:155] Top shape: 42 32 112 112 (16859136)
I0630 02:08:24.812413 28163 net.cpp:163] Memory required for data: 387760296
I0630 02:08:24.812422 28163 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 02:08:24.812425 28163 net.cpp:98] Creating Layer conv1b/relu
I0630 02:08:24.812430 28163 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 02:08:24.812436 28163 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 02:08:24.812441 28163 net.cpp:148] Setting up conv1b/relu
I0630 02:08:24.812446 28163 net.cpp:155] Top shape: 42 32 112 112 (16859136)
I0630 02:08:24.812449 28163 net.cpp:163] Memory required for data: 455196840
I0630 02:08:24.812453 28163 layer_factory.hpp:77] Creating layer pool1
I0630 02:08:24.812461 28163 net.cpp:98] Creating Layer pool1
I0630 02:08:24.812464 28163 net.cpp:439] pool1 <- conv1b/bn
I0630 02:08:24.812469 28163 net.cpp:413] pool1 -> pool1
I0630 02:08:24.812511 28163 net.cpp:148] Setting up pool1
I0630 02:08:24.812516 28163 net.cpp:155] Top shape: 42 32 56 56 (4214784)
I0630 02:08:24.812520 28163 net.cpp:163] Memory required for data: 472055976
I0630 02:08:24.812525 28163 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 02:08:24.812531 28163 net.cpp:98] Creating Layer res2a_branch2a
I0630 02:08:24.812535 28163 net.cpp:439] res2a_branch2a <- pool1
I0630 02:08:24.812541 28163 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 02:08:24.813169 28163 net.cpp:148] Setting up res2a_branch2a
I0630 02:08:24.813176 28163 net.cpp:155] Top shape: 42 64 56 56 (8429568)
I0630 02:08:24.813180 28163 net.cpp:163] Memory required for data: 505774248
I0630 02:08:24.813187 28163 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 02:08:24.813192 28163 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 02:08:24.813196 28163 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 02:08:24.813201 28163 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 02:08:24.813860 28163 net.cpp:148] Setting up res2a_branch2a/bn
I0630 02:08:24.813868 28163 net.cpp:155] Top shape: 42 64 56 56 (8429568)
I0630 02:08:24.813871 28163 net.cpp:163] Memory required for data: 539492520
I0630 02:08:24.813880 28163 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 02:08:24.813884 28163 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 02:08:24.813889 28163 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 02:08:24.813894 28163 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 02:08:24.813899 28163 net.cpp:148] Setting up res2a_branch2a/relu
I0630 02:08:24.813905 28163 net.cpp:155] Top shape: 42 64 56 56 (8429568)
I0630 02:08:24.813908 28163 net.cpp:163] Memory required for data: 573210792
I0630 02:08:24.813912 28163 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 02:08:24.813923 28163 net.cpp:98] Creating Layer res2a_branch2b
I0630 02:08:24.813926 28163 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 02:08:24.813931 28163 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 02:08:24.814391 28163 net.cpp:148] Setting up res2a_branch2b
I0630 02:08:24.814398 28163 net.cpp:155] Top shape: 42 64 56 56 (8429568)
I0630 02:08:24.814401 28163 net.cpp:163] Memory required for data: 606929064
I0630 02:08:24.814406 28163 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 02:08:24.814412 28163 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 02:08:24.814420 28163 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 02:08:24.814426 28163 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 02:08:24.815091 28163 net.cpp:148] Setting up res2a_branch2b/bn
I0630 02:08:24.815099 28163 net.cpp:155] Top shape: 42 64 56 56 (8429568)
I0630 02:08:24.815101 28163 net.cpp:163] Memory required for data: 640647336
I0630 02:08:24.815110 28163 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 02:08:24.815114 28163 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 02:08:24.815119 28163 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 02:08:24.815124 28163 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 02:08:24.815132 28163 net.cpp:148] Setting up res2a_branch2b/relu
I0630 02:08:24.815136 28163 net.cpp:155] Top shape: 42 64 56 56 (8429568)
I0630 02:08:24.815140 28163 net.cpp:163] Memory required for data: 674365608
I0630 02:08:24.815143 28163 layer_factory.hpp:77] Creating layer pool2
I0630 02:08:24.815148 28163 net.cpp:98] Creating Layer pool2
I0630 02:08:24.815152 28163 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 02:08:24.815156 28163 net.cpp:413] pool2 -> pool2
I0630 02:08:24.815196 28163 net.cpp:148] Setting up pool2
I0630 02:08:24.815201 28163 net.cpp:155] Top shape: 42 64 28 28 (2107392)
I0630 02:08:24.815206 28163 net.cpp:163] Memory required for data: 682795176
I0630 02:08:24.815209 28163 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 02:08:24.815219 28163 net.cpp:98] Creating Layer res3a_branch2a
I0630 02:08:24.815223 28163 net.cpp:439] res3a_branch2a <- pool2
I0630 02:08:24.815228 28163 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 02:08:24.818584 28163 net.cpp:148] Setting up res3a_branch2a
I0630 02:08:24.818595 28163 net.cpp:155] Top shape: 42 128 28 28 (4214784)
I0630 02:08:24.818599 28163 net.cpp:163] Memory required for data: 699654312
I0630 02:08:24.818605 28163 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 02:08:24.818612 28163 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 02:08:24.818616 28163 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 02:08:24.818624 28163 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 02:08:24.819257 28163 net.cpp:148] Setting up res3a_branch2a/bn
I0630 02:08:24.819265 28163 net.cpp:155] Top shape: 42 128 28 28 (4214784)
I0630 02:08:24.819268 28163 net.cpp:163] Memory required for data: 716513448
I0630 02:08:24.819279 28163 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 02:08:24.819283 28163 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 02:08:24.819288 28163 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 02:08:24.819298 28163 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 02:08:24.819303 28163 net.cpp:148] Setting up res3a_branch2a/relu
I0630 02:08:24.819308 28163 net.cpp:155] Top shape: 42 128 28 28 (4214784)
I0630 02:08:24.819311 28163 net.cpp:163] Memory required for data: 733372584
I0630 02:08:24.819315 28163 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 02:08:24.819322 28163 net.cpp:98] Creating Layer res3a_branch2b
I0630 02:08:24.819326 28163 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 02:08:24.819331 28163 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 02:08:24.820436 28163 net.cpp:148] Setting up res3a_branch2b
I0630 02:08:24.820444 28163 net.cpp:155] Top shape: 42 128 28 28 (4214784)
I0630 02:08:24.820447 28163 net.cpp:163] Memory required for data: 750231720
I0630 02:08:24.820453 28163 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 02:08:24.820461 28163 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 02:08:24.820464 28163 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 02:08:24.820469 28163 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 02:08:24.821100 28163 net.cpp:148] Setting up res3a_branch2b/bn
I0630 02:08:24.821107 28163 net.cpp:155] Top shape: 42 128 28 28 (4214784)
I0630 02:08:24.821112 28163 net.cpp:163] Memory required for data: 767090856
I0630 02:08:24.821127 28163 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 02:08:24.821132 28163 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 02:08:24.821136 28163 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 02:08:24.821141 28163 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 02:08:24.821147 28163 net.cpp:148] Setting up res3a_branch2b/relu
I0630 02:08:24.821151 28163 net.cpp:155] Top shape: 42 128 28 28 (4214784)
I0630 02:08:24.821156 28163 net.cpp:163] Memory required for data: 783949992
I0630 02:08:24.821159 28163 layer_factory.hpp:77] Creating layer pool3
I0630 02:08:24.821164 28163 net.cpp:98] Creating Layer pool3
I0630 02:08:24.821168 28163 net.cpp:439] pool3 <- res3a_branch2b/bn
I0630 02:08:24.821173 28163 net.cpp:413] pool3 -> pool3
I0630 02:08:24.821216 28163 net.cpp:148] Setting up pool3
I0630 02:08:24.821221 28163 net.cpp:155] Top shape: 42 128 14 14 (1053696)
I0630 02:08:24.821225 28163 net.cpp:163] Memory required for data: 788164776
I0630 02:08:24.821229 28163 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 02:08:24.821236 28163 net.cpp:98] Creating Layer res4a_branch2a
I0630 02:08:24.821239 28163 net.cpp:439] res4a_branch2a <- pool3
I0630 02:08:24.821244 28163 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 02:08:24.827327 28163 net.cpp:148] Setting up res4a_branch2a
I0630 02:08:24.827334 28163 net.cpp:155] Top shape: 42 256 14 14 (2107392)
I0630 02:08:24.827337 28163 net.cpp:163] Memory required for data: 796594344
I0630 02:08:24.827344 28163 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 02:08:24.827350 28163 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 02:08:24.827354 28163 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 02:08:24.827359 28163 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 02:08:24.828004 28163 net.cpp:148] Setting up res4a_branch2a/bn
I0630 02:08:24.828011 28163 net.cpp:155] Top shape: 42 256 14 14 (2107392)
I0630 02:08:24.828016 28163 net.cpp:163] Memory required for data: 805023912
I0630 02:08:24.828024 28163 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 02:08:24.828028 28163 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 02:08:24.828032 28163 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 02:08:24.828037 28163 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 02:08:24.828043 28163 net.cpp:148] Setting up res4a_branch2a/relu
I0630 02:08:24.828047 28163 net.cpp:155] Top shape: 42 256 14 14 (2107392)
I0630 02:08:24.828052 28163 net.cpp:163] Memory required for data: 813453480
I0630 02:08:24.828055 28163 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 02:08:24.828061 28163 net.cpp:98] Creating Layer res4a_branch2b
I0630 02:08:24.828065 28163 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 02:08:24.828070 28163 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 02:08:24.831251 28163 net.cpp:148] Setting up res4a_branch2b
I0630 02:08:24.831259 28163 net.cpp:155] Top shape: 42 256 14 14 (2107392)
I0630 02:08:24.831261 28163 net.cpp:163] Memory required for data: 821883048
I0630 02:08:24.831269 28163 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 02:08:24.831274 28163 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 02:08:24.831279 28163 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 02:08:24.831284 28163 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 02:08:24.831907 28163 net.cpp:148] Setting up res4a_branch2b/bn
I0630 02:08:24.831914 28163 net.cpp:155] Top shape: 42 256 14 14 (2107392)
I0630 02:08:24.831918 28163 net.cpp:163] Memory required for data: 830312616
I0630 02:08:24.831925 28163 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 02:08:24.831930 28163 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 02:08:24.831934 28163 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 02:08:24.831938 28163 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 02:08:24.831944 28163 net.cpp:148] Setting up res4a_branch2b/relu
I0630 02:08:24.831954 28163 net.cpp:155] Top shape: 42 256 14 14 (2107392)
I0630 02:08:24.831957 28163 net.cpp:163] Memory required for data: 838742184
I0630 02:08:24.831961 28163 layer_factory.hpp:77] Creating layer pool4
I0630 02:08:24.831967 28163 net.cpp:98] Creating Layer pool4
I0630 02:08:24.831971 28163 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 02:08:24.831975 28163 net.cpp:413] pool4 -> pool4
I0630 02:08:24.832022 28163 net.cpp:148] Setting up pool4
I0630 02:08:24.832027 28163 net.cpp:155] Top shape: 42 256 7 7 (526848)
I0630 02:08:24.832031 28163 net.cpp:163] Memory required for data: 840849576
I0630 02:08:24.832036 28163 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 02:08:24.832041 28163 net.cpp:98] Creating Layer res5a_branch2a
I0630 02:08:24.832046 28163 net.cpp:439] res5a_branch2a <- pool4
I0630 02:08:24.832051 28163 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 02:08:24.856799 28163 net.cpp:148] Setting up res5a_branch2a
I0630 02:08:24.856820 28163 net.cpp:155] Top shape: 42 512 7 7 (1053696)
I0630 02:08:24.856824 28163 net.cpp:163] Memory required for data: 845064360
I0630 02:08:24.856832 28163 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 02:08:24.856840 28163 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 02:08:24.856845 28163 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 02:08:24.856853 28163 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 02:08:24.857522 28163 net.cpp:148] Setting up res5a_branch2a/bn
I0630 02:08:24.857529 28163 net.cpp:155] Top shape: 42 512 7 7 (1053696)
I0630 02:08:24.857532 28163 net.cpp:163] Memory required for data: 849279144
I0630 02:08:24.857542 28163 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 02:08:24.857547 28163 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 02:08:24.857551 28163 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 02:08:24.857556 28163 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 02:08:24.857563 28163 net.cpp:148] Setting up res5a_branch2a/relu
I0630 02:08:24.857568 28163 net.cpp:155] Top shape: 42 512 7 7 (1053696)
I0630 02:08:24.857571 28163 net.cpp:163] Memory required for data: 853493928
I0630 02:08:24.857574 28163 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 02:08:24.857583 28163 net.cpp:98] Creating Layer res5a_branch2b
I0630 02:08:24.857585 28163 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 02:08:24.857591 28163 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 02:08:24.870481 28163 net.cpp:148] Setting up res5a_branch2b
I0630 02:08:24.870506 28163 net.cpp:155] Top shape: 42 512 7 7 (1053696)
I0630 02:08:24.870509 28163 net.cpp:163] Memory required for data: 857708712
I0630 02:08:24.870524 28163 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 02:08:24.870534 28163 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 02:08:24.870539 28163 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 02:08:24.870545 28163 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 02:08:24.871232 28163 net.cpp:148] Setting up res5a_branch2b/bn
I0630 02:08:24.871240 28163 net.cpp:155] Top shape: 42 512 7 7 (1053696)
I0630 02:08:24.871243 28163 net.cpp:163] Memory required for data: 861923496
I0630 02:08:24.871253 28163 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 02:08:24.871258 28163 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 02:08:24.871261 28163 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 02:08:24.871266 28163 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 02:08:24.871273 28163 net.cpp:148] Setting up res5a_branch2b/relu
I0630 02:08:24.871278 28163 net.cpp:155] Top shape: 42 512 7 7 (1053696)
I0630 02:08:24.871281 28163 net.cpp:163] Memory required for data: 866138280
I0630 02:08:24.871285 28163 layer_factory.hpp:77] Creating layer pool5
I0630 02:08:24.871294 28163 net.cpp:98] Creating Layer pool5
I0630 02:08:24.871297 28163 net.cpp:439] pool5 <- res5a_branch2b/bn
I0630 02:08:24.871302 28163 net.cpp:413] pool5 -> pool5
I0630 02:08:24.871345 28163 net.cpp:148] Setting up pool5
I0630 02:08:24.871350 28163 net.cpp:155] Top shape: 42 512 1 1 (21504)
I0630 02:08:24.871353 28163 net.cpp:163] Memory required for data: 866224296
I0630 02:08:24.871357 28163 layer_factory.hpp:77] Creating layer fc1000
I0630 02:08:24.871368 28163 net.cpp:98] Creating Layer fc1000
I0630 02:08:24.871372 28163 net.cpp:439] fc1000 <- pool5
I0630 02:08:24.871377 28163 net.cpp:413] fc1000 -> fc1000
I0630 02:08:24.882418 28163 net.cpp:148] Setting up fc1000
I0630 02:08:24.882427 28163 net.cpp:155] Top shape: 42 1000 (42000)
I0630 02:08:24.882431 28163 net.cpp:163] Memory required for data: 866392296
I0630 02:08:24.882436 28163 layer_factory.hpp:77] Creating layer loss
I0630 02:08:24.882442 28163 net.cpp:98] Creating Layer loss
I0630 02:08:24.882447 28163 net.cpp:439] loss <- fc1000
I0630 02:08:24.882452 28163 net.cpp:439] loss <- label
I0630 02:08:24.882458 28163 net.cpp:413] loss -> loss
I0630 02:08:24.882468 28163 layer_factory.hpp:77] Creating layer loss
I0630 02:08:24.882619 28163 net.cpp:148] Setting up loss
I0630 02:08:24.882624 28163 net.cpp:155] Top shape: (1)
I0630 02:08:24.882628 28163 net.cpp:158]     with loss weight 1
I0630 02:08:24.882642 28163 net.cpp:163] Memory required for data: 866392300
I0630 02:08:24.882645 28163 net.cpp:224] loss needs backward computation.
I0630 02:08:24.882650 28163 net.cpp:224] fc1000 needs backward computation.
I0630 02:08:24.882654 28163 net.cpp:224] pool5 needs backward computation.
I0630 02:08:24.882658 28163 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 02:08:24.882661 28163 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 02:08:24.882665 28163 net.cpp:224] res5a_branch2b needs backward computation.
I0630 02:08:24.882670 28163 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 02:08:24.882674 28163 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 02:08:24.882678 28163 net.cpp:224] res5a_branch2a needs backward computation.
I0630 02:08:24.882683 28163 net.cpp:224] pool4 needs backward computation.
I0630 02:08:24.882686 28163 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 02:08:24.882690 28163 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 02:08:24.882694 28163 net.cpp:224] res4a_branch2b needs backward computation.
I0630 02:08:24.882699 28163 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 02:08:24.882702 28163 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 02:08:24.882706 28163 net.cpp:224] res4a_branch2a needs backward computation.
I0630 02:08:24.882710 28163 net.cpp:224] pool3 needs backward computation.
I0630 02:08:24.882715 28163 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 02:08:24.882719 28163 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 02:08:24.882724 28163 net.cpp:224] res3a_branch2b needs backward computation.
I0630 02:08:24.882727 28163 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 02:08:24.882731 28163 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 02:08:24.882735 28163 net.cpp:224] res3a_branch2a needs backward computation.
I0630 02:08:24.882740 28163 net.cpp:224] pool2 needs backward computation.
I0630 02:08:24.882743 28163 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 02:08:24.882747 28163 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 02:08:24.882751 28163 net.cpp:224] res2a_branch2b needs backward computation.
I0630 02:08:24.882755 28163 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 02:08:24.882758 28163 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 02:08:24.882763 28163 net.cpp:224] res2a_branch2a needs backward computation.
I0630 02:08:24.882767 28163 net.cpp:224] pool1 needs backward computation.
I0630 02:08:24.882771 28163 net.cpp:224] conv1b/relu needs backward computation.
I0630 02:08:24.882774 28163 net.cpp:224] conv1b/bn needs backward computation.
I0630 02:08:24.882779 28163 net.cpp:224] conv1b needs backward computation.
I0630 02:08:24.882788 28163 net.cpp:224] conv1a/relu needs backward computation.
I0630 02:08:24.882792 28163 net.cpp:224] conv1a/bn needs backward computation.
I0630 02:08:24.882797 28163 net.cpp:224] conv1a needs backward computation.
I0630 02:08:24.882802 28163 net.cpp:226] data/bias does not need backward computation.
I0630 02:08:24.882805 28163 net.cpp:226] data does not need backward computation.
I0630 02:08:24.882809 28163 net.cpp:268] This network produces output loss
I0630 02:08:24.882829 28163 net.cpp:288] Network initialization done.
I0630 02:08:24.883270 28163 solver.cpp:182] Creating test net (#0) specified by test_net file: training/imagenet_jacintonet11v2_2017-06-30_02-08-23/initial/test.prototxt
I0630 02:08:24.883458 28163 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 224
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1000"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc1000"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc1000"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0630 02:08:24.883601 28163 layer_factory.hpp:77] Creating layer data
I0630 02:08:24.883661 28163 net.cpp:98] Creating Layer data
I0630 02:08:24.883666 28163 net.cpp:413] data -> data
I0630 02:08:24.883672 28163 net.cpp:413] data -> label
I0630 02:08:24.902891 28199 db_lmdb.cpp:35] Opened lmdb ./data/ilsvrc12_val_lmdb
I0630 02:08:24.903990 28163 data_layer.cpp:78] ReshapePrefetch 50, 3, 224, 224
I0630 02:08:24.904057 28163 data_layer.cpp:83] output data size: 50,3,224,224
I0630 02:08:24.935562 28163 net.cpp:148] Setting up data
I0630 02:08:24.935583 28163 net.cpp:155] Top shape: 50 3 224 224 (7526400)
I0630 02:08:24.935588 28163 net.cpp:155] Top shape: 50 (50)
I0630 02:08:24.935590 28163 net.cpp:163] Memory required for data: 30105800
I0630 02:08:24.935596 28163 layer_factory.hpp:77] Creating layer label_data_1_split
I0630 02:08:24.935607 28163 net.cpp:98] Creating Layer label_data_1_split
I0630 02:08:24.935611 28163 net.cpp:439] label_data_1_split <- label
I0630 02:08:24.935616 28163 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0630 02:08:24.935626 28163 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0630 02:08:24.935631 28163 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0630 02:08:24.935765 28163 net.cpp:148] Setting up label_data_1_split
I0630 02:08:24.935775 28163 net.cpp:155] Top shape: 50 (50)
I0630 02:08:24.935781 28163 net.cpp:155] Top shape: 50 (50)
I0630 02:08:24.935784 28163 net.cpp:155] Top shape: 50 (50)
I0630 02:08:24.935788 28163 net.cpp:163] Memory required for data: 30106400
I0630 02:08:24.935792 28163 layer_factory.hpp:77] Creating layer data/bias
I0630 02:08:24.935801 28163 net.cpp:98] Creating Layer data/bias
I0630 02:08:24.935804 28163 net.cpp:439] data/bias <- data
I0630 02:08:24.935811 28163 net.cpp:413] data/bias -> data/bias
I0630 02:08:24.935963 28163 net.cpp:148] Setting up data/bias
I0630 02:08:24.935968 28163 net.cpp:155] Top shape: 50 3 224 224 (7526400)
I0630 02:08:24.935972 28163 net.cpp:163] Memory required for data: 60212000
I0630 02:08:24.935979 28163 layer_factory.hpp:77] Creating layer conv1a
I0630 02:08:24.935987 28163 net.cpp:98] Creating Layer conv1a
I0630 02:08:24.935992 28163 net.cpp:439] conv1a <- data/bias
I0630 02:08:24.935997 28163 net.cpp:413] conv1a -> conv1a
I0630 02:08:24.936503 28163 net.cpp:148] Setting up conv1a
I0630 02:08:24.936511 28163 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 02:08:24.936514 28163 net.cpp:163] Memory required for data: 140493600
I0630 02:08:24.936522 28163 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 02:08:24.936529 28163 net.cpp:98] Creating Layer conv1a/bn
I0630 02:08:24.936533 28163 net.cpp:439] conv1a/bn <- conv1a
I0630 02:08:24.936538 28163 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 02:08:24.939589 28163 net.cpp:148] Setting up conv1a/bn
I0630 02:08:24.939596 28163 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 02:08:24.939599 28163 net.cpp:163] Memory required for data: 220775200
I0630 02:08:24.939610 28163 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 02:08:24.939621 28163 net.cpp:98] Creating Layer conv1a/relu
I0630 02:08:24.939625 28163 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 02:08:24.939630 28163 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 02:08:24.939637 28163 net.cpp:148] Setting up conv1a/relu
I0630 02:08:24.939642 28163 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 02:08:24.939646 28163 net.cpp:163] Memory required for data: 301056800
I0630 02:08:24.939649 28163 layer_factory.hpp:77] Creating layer conv1b
I0630 02:08:24.939656 28163 net.cpp:98] Creating Layer conv1b
I0630 02:08:24.939661 28163 net.cpp:439] conv1b <- conv1a/bn
I0630 02:08:24.939666 28163 net.cpp:413] conv1b -> conv1b
I0630 02:08:24.940027 28163 net.cpp:148] Setting up conv1b
I0630 02:08:24.940034 28163 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 02:08:24.940038 28163 net.cpp:163] Memory required for data: 381338400
I0630 02:08:24.940045 28163 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 02:08:24.940052 28163 net.cpp:98] Creating Layer conv1b/bn
I0630 02:08:24.940055 28163 net.cpp:439] conv1b/bn <- conv1b
I0630 02:08:24.940062 28163 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 02:08:24.940778 28163 net.cpp:148] Setting up conv1b/bn
I0630 02:08:24.940785 28163 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 02:08:24.940789 28163 net.cpp:163] Memory required for data: 461620000
I0630 02:08:24.940800 28163 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 02:08:24.940805 28163 net.cpp:98] Creating Layer conv1b/relu
I0630 02:08:24.940810 28163 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 02:08:24.940814 28163 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 02:08:24.940820 28163 net.cpp:148] Setting up conv1b/relu
I0630 02:08:24.940825 28163 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 02:08:24.940829 28163 net.cpp:163] Memory required for data: 541901600
I0630 02:08:24.940832 28163 layer_factory.hpp:77] Creating layer pool1
I0630 02:08:24.940840 28163 net.cpp:98] Creating Layer pool1
I0630 02:08:24.940842 28163 net.cpp:439] pool1 <- conv1b/bn
I0630 02:08:24.940846 28163 net.cpp:413] pool1 -> pool1
I0630 02:08:24.940888 28163 net.cpp:148] Setting up pool1
I0630 02:08:24.940893 28163 net.cpp:155] Top shape: 50 32 56 56 (5017600)
I0630 02:08:24.940897 28163 net.cpp:163] Memory required for data: 561972000
I0630 02:08:24.940901 28163 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 02:08:24.940908 28163 net.cpp:98] Creating Layer res2a_branch2a
I0630 02:08:24.940912 28163 net.cpp:439] res2a_branch2a <- pool1
I0630 02:08:24.940917 28163 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 02:08:24.941573 28163 net.cpp:148] Setting up res2a_branch2a
I0630 02:08:24.941579 28163 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 02:08:24.941583 28163 net.cpp:163] Memory required for data: 602112800
I0630 02:08:24.941591 28163 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 02:08:24.941596 28163 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 02:08:24.941601 28163 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 02:08:24.941606 28163 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 02:08:24.942334 28163 net.cpp:148] Setting up res2a_branch2a/bn
I0630 02:08:24.942342 28163 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 02:08:24.942345 28163 net.cpp:163] Memory required for data: 642253600
I0630 02:08:24.942353 28163 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 02:08:24.942358 28163 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 02:08:24.942363 28163 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 02:08:24.942366 28163 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 02:08:24.942373 28163 net.cpp:148] Setting up res2a_branch2a/relu
I0630 02:08:24.942378 28163 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 02:08:24.942380 28163 net.cpp:163] Memory required for data: 682394400
I0630 02:08:24.942384 28163 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 02:08:24.942390 28163 net.cpp:98] Creating Layer res2a_branch2b
I0630 02:08:24.942399 28163 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 02:08:24.942404 28163 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 02:08:24.942905 28163 net.cpp:148] Setting up res2a_branch2b
I0630 02:08:24.942912 28163 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 02:08:24.942916 28163 net.cpp:163] Memory required for data: 722535200
I0630 02:08:24.942921 28163 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 02:08:24.942927 28163 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 02:08:24.942931 28163 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 02:08:24.942936 28163 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 02:08:24.943667 28163 net.cpp:148] Setting up res2a_branch2b/bn
I0630 02:08:24.943675 28163 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 02:08:24.943678 28163 net.cpp:163] Memory required for data: 762676000
I0630 02:08:24.943687 28163 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 02:08:24.943691 28163 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 02:08:24.943696 28163 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 02:08:24.943701 28163 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 02:08:24.943706 28163 net.cpp:148] Setting up res2a_branch2b/relu
I0630 02:08:24.943711 28163 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 02:08:24.943714 28163 net.cpp:163] Memory required for data: 802816800
I0630 02:08:24.943717 28163 layer_factory.hpp:77] Creating layer pool2
I0630 02:08:24.943723 28163 net.cpp:98] Creating Layer pool2
I0630 02:08:24.943727 28163 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 02:08:24.943730 28163 net.cpp:413] pool2 -> pool2
I0630 02:08:24.943778 28163 net.cpp:148] Setting up pool2
I0630 02:08:24.943783 28163 net.cpp:155] Top shape: 50 64 28 28 (2508800)
I0630 02:08:24.943787 28163 net.cpp:163] Memory required for data: 812852000
I0630 02:08:24.943791 28163 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 02:08:24.943799 28163 net.cpp:98] Creating Layer res3a_branch2a
I0630 02:08:24.943802 28163 net.cpp:439] res3a_branch2a <- pool2
I0630 02:08:24.943807 28163 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 02:08:24.946485 28163 net.cpp:148] Setting up res3a_branch2a
I0630 02:08:24.946494 28163 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 02:08:24.946498 28163 net.cpp:163] Memory required for data: 832922400
I0630 02:08:24.946504 28163 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 02:08:24.946511 28163 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 02:08:24.946514 28163 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 02:08:24.946526 28163 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 02:08:24.947228 28163 net.cpp:148] Setting up res3a_branch2a/bn
I0630 02:08:24.947237 28163 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 02:08:24.947239 28163 net.cpp:163] Memory required for data: 852992800
I0630 02:08:24.947249 28163 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 02:08:24.947257 28163 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 02:08:24.947262 28163 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 02:08:24.947266 28163 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 02:08:24.947273 28163 net.cpp:148] Setting up res3a_branch2a/relu
I0630 02:08:24.947278 28163 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 02:08:24.947281 28163 net.cpp:163] Memory required for data: 873063200
I0630 02:08:24.947284 28163 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 02:08:24.947291 28163 net.cpp:98] Creating Layer res3a_branch2b
I0630 02:08:24.947294 28163 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 02:08:24.947299 28163 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 02:08:24.948344 28163 net.cpp:148] Setting up res3a_branch2b
I0630 02:08:24.948355 28163 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 02:08:24.948359 28163 net.cpp:163] Memory required for data: 893133600
I0630 02:08:24.948372 28163 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 02:08:24.948379 28163 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 02:08:24.948382 28163 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 02:08:24.948387 28163 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 02:08:24.949074 28163 net.cpp:148] Setting up res3a_branch2b/bn
I0630 02:08:24.949081 28163 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 02:08:24.949085 28163 net.cpp:163] Memory required for data: 913204000
I0630 02:08:24.949093 28163 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 02:08:24.949105 28163 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 02:08:24.949108 28163 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 02:08:24.949113 28163 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 02:08:24.949118 28163 net.cpp:148] Setting up res3a_branch2b/relu
I0630 02:08:24.949123 28163 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 02:08:24.949126 28163 net.cpp:163] Memory required for data: 933274400
I0630 02:08:24.949131 28163 layer_factory.hpp:77] Creating layer pool3
I0630 02:08:24.949136 28163 net.cpp:98] Creating Layer pool3
I0630 02:08:24.949139 28163 net.cpp:439] pool3 <- res3a_branch2b/bn
I0630 02:08:24.949144 28163 net.cpp:413] pool3 -> pool3
I0630 02:08:24.949188 28163 net.cpp:148] Setting up pool3
I0630 02:08:24.949193 28163 net.cpp:155] Top shape: 50 128 14 14 (1254400)
I0630 02:08:24.949198 28163 net.cpp:163] Memory required for data: 938292000
I0630 02:08:24.949201 28163 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 02:08:24.949208 28163 net.cpp:98] Creating Layer res4a_branch2a
I0630 02:08:24.949211 28163 net.cpp:439] res4a_branch2a <- pool3
I0630 02:08:24.949218 28163 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 02:08:24.955292 28163 net.cpp:148] Setting up res4a_branch2a
I0630 02:08:24.955301 28163 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 02:08:24.955304 28163 net.cpp:163] Memory required for data: 948327200
I0630 02:08:24.955310 28163 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 02:08:24.955317 28163 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 02:08:24.955320 28163 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 02:08:24.955325 28163 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 02:08:24.956022 28163 net.cpp:148] Setting up res4a_branch2a/bn
I0630 02:08:24.956030 28163 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 02:08:24.956033 28163 net.cpp:163] Memory required for data: 958362400
I0630 02:08:24.956043 28163 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 02:08:24.956048 28163 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 02:08:24.956053 28163 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 02:08:24.956058 28163 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 02:08:24.956065 28163 net.cpp:148] Setting up res4a_branch2a/relu
I0630 02:08:24.956069 28163 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 02:08:24.956073 28163 net.cpp:163] Memory required for data: 968397600
I0630 02:08:24.956077 28163 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 02:08:24.956084 28163 net.cpp:98] Creating Layer res4a_branch2b
I0630 02:08:24.956089 28163 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 02:08:24.956094 28163 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 02:08:24.959396 28163 net.cpp:148] Setting up res4a_branch2b
I0630 02:08:24.959408 28163 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 02:08:24.959411 28163 net.cpp:163] Memory required for data: 978432800
I0630 02:08:24.959417 28163 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 02:08:24.959424 28163 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 02:08:24.959429 28163 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 02:08:24.959434 28163 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 02:08:24.960139 28163 net.cpp:148] Setting up res4a_branch2b/bn
I0630 02:08:24.960146 28163 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 02:08:24.960160 28163 net.cpp:163] Memory required for data: 988468000
I0630 02:08:24.960167 28163 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 02:08:24.960172 28163 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 02:08:24.960176 28163 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 02:08:24.960181 28163 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 02:08:24.960188 28163 net.cpp:148] Setting up res4a_branch2b/relu
I0630 02:08:24.960192 28163 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 02:08:24.960196 28163 net.cpp:163] Memory required for data: 998503200
I0630 02:08:24.960201 28163 layer_factory.hpp:77] Creating layer pool4
I0630 02:08:24.960206 28163 net.cpp:98] Creating Layer pool4
I0630 02:08:24.960211 28163 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 02:08:24.960216 28163 net.cpp:413] pool4 -> pool4
I0630 02:08:24.960261 28163 net.cpp:148] Setting up pool4
I0630 02:08:24.960266 28163 net.cpp:155] Top shape: 50 256 7 7 (627200)
I0630 02:08:24.960270 28163 net.cpp:163] Memory required for data: 1001012000
I0630 02:08:24.960274 28163 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 02:08:24.960281 28163 net.cpp:98] Creating Layer res5a_branch2a
I0630 02:08:24.960284 28163 net.cpp:439] res5a_branch2a <- pool4
I0630 02:08:24.960289 28163 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 02:08:24.985337 28163 net.cpp:148] Setting up res5a_branch2a
I0630 02:08:24.985357 28163 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 02:08:24.985360 28163 net.cpp:163] Memory required for data: 1006029600
I0630 02:08:24.985368 28163 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 02:08:24.985378 28163 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 02:08:24.985383 28163 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 02:08:24.985389 28163 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 02:08:24.986106 28163 net.cpp:148] Setting up res5a_branch2a/bn
I0630 02:08:24.986114 28163 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 02:08:24.986117 28163 net.cpp:163] Memory required for data: 1011047200
I0630 02:08:24.986125 28163 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 02:08:24.986137 28163 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 02:08:24.986141 28163 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 02:08:24.986145 28163 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 02:08:24.986151 28163 net.cpp:148] Setting up res5a_branch2a/relu
I0630 02:08:24.986155 28163 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 02:08:24.986156 28163 net.cpp:163] Memory required for data: 1016064800
I0630 02:08:24.986158 28163 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 02:08:24.986165 28163 net.cpp:98] Creating Layer res5a_branch2b
I0630 02:08:24.986166 28163 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 02:08:24.986169 28163 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 02:08:25.001201 28163 net.cpp:148] Setting up res5a_branch2b
I0630 02:08:25.001217 28163 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 02:08:25.001219 28163 net.cpp:163] Memory required for data: 1021082400
I0630 02:08:25.001227 28163 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 02:08:25.001235 28163 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 02:08:25.001237 28163 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 02:08:25.001240 28163 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 02:08:25.001971 28163 net.cpp:148] Setting up res5a_branch2b/bn
I0630 02:08:25.001977 28163 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 02:08:25.001979 28163 net.cpp:163] Memory required for data: 1026100000
I0630 02:08:25.001984 28163 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 02:08:25.001988 28163 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 02:08:25.001991 28163 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 02:08:25.001992 28163 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 02:08:25.002007 28163 net.cpp:148] Setting up res5a_branch2b/relu
I0630 02:08:25.002012 28163 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 02:08:25.002013 28163 net.cpp:163] Memory required for data: 1031117600
I0630 02:08:25.002017 28163 layer_factory.hpp:77] Creating layer pool5
I0630 02:08:25.002023 28163 net.cpp:98] Creating Layer pool5
I0630 02:08:25.002027 28163 net.cpp:439] pool5 <- res5a_branch2b/bn
I0630 02:08:25.002032 28163 net.cpp:413] pool5 -> pool5
I0630 02:08:25.002063 28163 net.cpp:148] Setting up pool5
I0630 02:08:25.002068 28163 net.cpp:155] Top shape: 50 512 1 1 (25600)
I0630 02:08:25.002069 28163 net.cpp:163] Memory required for data: 1031220000
I0630 02:08:25.002073 28163 layer_factory.hpp:77] Creating layer fc1000
I0630 02:08:25.002075 28163 net.cpp:98] Creating Layer fc1000
I0630 02:08:25.002077 28163 net.cpp:439] fc1000 <- pool5
I0630 02:08:25.002081 28163 net.cpp:413] fc1000 -> fc1000
I0630 02:08:25.013155 28163 net.cpp:148] Setting up fc1000
I0630 02:08:25.013167 28163 net.cpp:155] Top shape: 50 1000 (50000)
I0630 02:08:25.013170 28163 net.cpp:163] Memory required for data: 1031420000
I0630 02:08:25.013175 28163 layer_factory.hpp:77] Creating layer fc1000_fc1000_0_split
I0630 02:08:25.013178 28163 net.cpp:98] Creating Layer fc1000_fc1000_0_split
I0630 02:08:25.013180 28163 net.cpp:439] fc1000_fc1000_0_split <- fc1000
I0630 02:08:25.013183 28163 net.cpp:413] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_0
I0630 02:08:25.013188 28163 net.cpp:413] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_1
I0630 02:08:25.013191 28163 net.cpp:413] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_2
I0630 02:08:25.013255 28163 net.cpp:148] Setting up fc1000_fc1000_0_split
I0630 02:08:25.013260 28163 net.cpp:155] Top shape: 50 1000 (50000)
I0630 02:08:25.013262 28163 net.cpp:155] Top shape: 50 1000 (50000)
I0630 02:08:25.013264 28163 net.cpp:155] Top shape: 50 1000 (50000)
I0630 02:08:25.013267 28163 net.cpp:163] Memory required for data: 1032020000
I0630 02:08:25.013268 28163 layer_factory.hpp:77] Creating layer loss
I0630 02:08:25.013273 28163 net.cpp:98] Creating Layer loss
I0630 02:08:25.013274 28163 net.cpp:439] loss <- fc1000_fc1000_0_split_0
I0630 02:08:25.013276 28163 net.cpp:439] loss <- label_data_1_split_0
I0630 02:08:25.013279 28163 net.cpp:413] loss -> loss
I0630 02:08:25.013283 28163 layer_factory.hpp:77] Creating layer loss
I0630 02:08:25.013439 28163 net.cpp:148] Setting up loss
I0630 02:08:25.013444 28163 net.cpp:155] Top shape: (1)
I0630 02:08:25.013447 28163 net.cpp:158]     with loss weight 1
I0630 02:08:25.013453 28163 net.cpp:163] Memory required for data: 1032020004
I0630 02:08:25.013455 28163 layer_factory.hpp:77] Creating layer accuracy/top1
I0630 02:08:25.013459 28163 net.cpp:98] Creating Layer accuracy/top1
I0630 02:08:25.013461 28163 net.cpp:439] accuracy/top1 <- fc1000_fc1000_0_split_1
I0630 02:08:25.013463 28163 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0630 02:08:25.013466 28163 net.cpp:413] accuracy/top1 -> accuracy/top1
I0630 02:08:25.013471 28163 net.cpp:148] Setting up accuracy/top1
I0630 02:08:25.013473 28163 net.cpp:155] Top shape: (1)
I0630 02:08:25.013476 28163 net.cpp:163] Memory required for data: 1032020008
I0630 02:08:25.013480 28163 layer_factory.hpp:77] Creating layer accuracy/top5
I0630 02:08:25.013489 28163 net.cpp:98] Creating Layer accuracy/top5
I0630 02:08:25.013495 28163 net.cpp:439] accuracy/top5 <- fc1000_fc1000_0_split_2
I0630 02:08:25.013499 28163 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0630 02:08:25.013504 28163 net.cpp:413] accuracy/top5 -> accuracy/top5
I0630 02:08:25.013509 28163 net.cpp:148] Setting up accuracy/top5
I0630 02:08:25.013514 28163 net.cpp:155] Top shape: (1)
I0630 02:08:25.013516 28163 net.cpp:163] Memory required for data: 1032020012
I0630 02:08:25.013520 28163 net.cpp:226] accuracy/top5 does not need backward computation.
I0630 02:08:25.013523 28163 net.cpp:226] accuracy/top1 does not need backward computation.
I0630 02:08:25.013527 28163 net.cpp:224] loss needs backward computation.
I0630 02:08:25.013540 28163 net.cpp:224] fc1000_fc1000_0_split needs backward computation.
I0630 02:08:25.013545 28163 net.cpp:224] fc1000 needs backward computation.
I0630 02:08:25.013548 28163 net.cpp:224] pool5 needs backward computation.
I0630 02:08:25.013552 28163 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 02:08:25.013556 28163 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 02:08:25.013561 28163 net.cpp:224] res5a_branch2b needs backward computation.
I0630 02:08:25.013566 28163 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 02:08:25.013569 28163 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 02:08:25.013573 28163 net.cpp:224] res5a_branch2a needs backward computation.
I0630 02:08:25.013578 28163 net.cpp:224] pool4 needs backward computation.
I0630 02:08:25.013582 28163 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 02:08:25.013586 28163 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 02:08:25.013589 28163 net.cpp:224] res4a_branch2b needs backward computation.
I0630 02:08:25.013593 28163 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 02:08:25.013597 28163 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 02:08:25.013602 28163 net.cpp:224] res4a_branch2a needs backward computation.
I0630 02:08:25.013605 28163 net.cpp:224] pool3 needs backward computation.
I0630 02:08:25.013609 28163 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 02:08:25.013612 28163 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 02:08:25.013617 28163 net.cpp:224] res3a_branch2b needs backward computation.
I0630 02:08:25.013620 28163 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 02:08:25.013624 28163 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 02:08:25.013628 28163 net.cpp:224] res3a_branch2a needs backward computation.
I0630 02:08:25.013633 28163 net.cpp:224] pool2 needs backward computation.
I0630 02:08:25.013636 28163 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 02:08:25.013640 28163 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 02:08:25.013645 28163 net.cpp:224] res2a_branch2b needs backward computation.
I0630 02:08:25.013649 28163 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 02:08:25.013653 28163 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 02:08:25.013658 28163 net.cpp:224] res2a_branch2a needs backward computation.
I0630 02:08:25.013662 28163 net.cpp:224] pool1 needs backward computation.
I0630 02:08:25.013666 28163 net.cpp:224] conv1b/relu needs backward computation.
I0630 02:08:25.013670 28163 net.cpp:224] conv1b/bn needs backward computation.
I0630 02:08:25.013675 28163 net.cpp:224] conv1b needs backward computation.
I0630 02:08:25.013679 28163 net.cpp:224] conv1a/relu needs backward computation.
I0630 02:08:25.013684 28163 net.cpp:224] conv1a/bn needs backward computation.
I0630 02:08:25.013687 28163 net.cpp:224] conv1a needs backward computation.
I0630 02:08:25.013691 28163 net.cpp:226] data/bias does not need backward computation.
I0630 02:08:25.013695 28163 net.cpp:226] label_data_1_split does not need backward computation.
I0630 02:08:25.013700 28163 net.cpp:226] data does not need backward computation.
I0630 02:08:25.013703 28163 net.cpp:268] This network produces output accuracy/top1
I0630 02:08:25.013707 28163 net.cpp:268] This network produces output accuracy/top5
I0630 02:08:25.013711 28163 net.cpp:268] This network produces output loss
I0630 02:08:25.013736 28163 net.cpp:288] Network initialization done.
I0630 02:08:25.013803 28163 solver.cpp:60] Solver scaffolding done.
I0630 02:08:25.017415 28163 caffe.cpp:145] Finetuning from /data/mmcodec_video2_tier3/users/manu/experiments/object/classification/2017.06.new_script/caffe-0.15/jacintonet11_imagenet_2017.06.12_lmdb_caffe-0.15-2gpu(60.89%)/stage0/jacintonet11_iter_320000.caffemodel
I0630 02:08:32.764169 28163 data_layer.cpp:78] ReshapePrefetch 42, 3, 224, 224
I0630 02:08:32.764261 28163 data_layer.cpp:83] output data size: 42,3,224,224
I0630 02:08:33.257032 28163 data_layer.cpp:78] ReshapePrefetch 42, 3, 224, 224
I0630 02:08:33.257115 28163 data_layer.cpp:83] output data size: 42,3,224,224
I0630 02:08:33.802640 28163 parallel.cpp:334] Starting Optimization
I0630 02:08:33.802692 28163 solver.cpp:413] Solving jacintonet11v2_train
I0630 02:08:33.802696 28163 solver.cpp:414] Learning Rate Policy: poly
I0630 02:08:33.808503 28163 solver.cpp:471] Iteration 0, Testing net (#0)
I0630 02:08:33.945459 28163 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 02:09:22.744361 28163 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.608879
I0630 02:09:22.744411 28163 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.832402
I0630 02:09:22.744417 28163 solver.cpp:544]     Test net output #2: loss = 1.32894 (* 1 = 1.32894 loss)
I0630 02:09:23.005231 28163 solver.cpp:290] Iteration 0 (0 iter/s, 49.2011s/100 iter), loss = 1.05952
I0630 02:09:23.005254 28163 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 02:09:23.005261 28163 sgd_solver.cpp:106] Iteration 0, lr = 0
I0630 02:09:38.774896 28163 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/initial/imagenet_jacintonet11v2_iter_100.caffemodel
I0630 02:09:38.806509 28163 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/initial/imagenet_jacintonet11v2_iter_100.solverstate
I0630 02:09:38.835595 28163 solver.cpp:451] Iteration 100, loss = 0.928571
I0630 02:09:38.835618 28163 solver.cpp:456] Optimization Done.
I0630 02:09:38.914487 28163 caffe.cpp:246] Optimization Done.
training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse
WARNING: gnome-keyring:: couldn't connect to: /run/user/30409/keyring-KJvviu/pkcs11: Connection refused
p11-kit: skipping module 'gnome-keyring' whose initialization failed: An error occurred on the device
I0630 02:09:40.151670 29777 caffe.cpp:209] Using GPUs 0, 1, 2
I0630 02:09:40.152137 29777 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0630 02:09:40.152465 29777 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0630 02:09:40.152789 29777 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0630 02:09:40.539373 29777 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/train.prototxt"
test_net: "training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/test.prototxt"
test_iter: 1000
test_interval: 2000
base_lr: 0.01
display: 100
max_iter: 320000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: true
iter_size: 2
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.01
sparsity_step_iter: 2000
sparsity_start_iter: 0
sparsity_start_factor: 0
I0630 02:09:40.539465 29777 solver.cpp:82] Creating training net from train_net file: training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/train.prototxt
I0630 02:09:40.539917 29777 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0630 02:09:40.539922 29777 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0630 02:09:40.540078 29777 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 224
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/ilsvrc12_train_lmdb"
    batch_size: 42
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1000"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc1000"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc1000"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0630 02:09:40.540159 29777 layer_factory.hpp:77] Creating layer data
I0630 02:09:40.540241 29777 net.cpp:98] Creating Layer data
I0630 02:09:40.540248 29777 net.cpp:413] data -> data
I0630 02:09:40.540268 29777 net.cpp:413] data -> label
I0630 02:09:40.541277 29803 db_lmdb.cpp:35] Opened lmdb ./data/ilsvrc12_train_lmdb
I0630 02:09:40.543381 29777 data_layer.cpp:78] ReshapePrefetch 42, 3, 224, 224
I0630 02:09:40.543438 29777 data_layer.cpp:83] output data size: 42,3,224,224
I0630 02:09:40.569190 29777 net.cpp:148] Setting up data
I0630 02:09:40.569226 29777 net.cpp:155] Top shape: 42 3 224 224 (6322176)
I0630 02:09:40.569231 29777 net.cpp:155] Top shape: 42 (42)
I0630 02:09:40.569234 29777 net.cpp:163] Memory required for data: 25288872
I0630 02:09:40.569255 29777 layer_factory.hpp:77] Creating layer data/bias
I0630 02:09:40.569284 29777 net.cpp:98] Creating Layer data/bias
I0630 02:09:40.569291 29777 net.cpp:439] data/bias <- data
I0630 02:09:40.569308 29777 net.cpp:413] data/bias -> data/bias
I0630 02:09:40.570377 29777 net.cpp:148] Setting up data/bias
I0630 02:09:40.570389 29777 net.cpp:155] Top shape: 42 3 224 224 (6322176)
I0630 02:09:40.570391 29777 net.cpp:163] Memory required for data: 50577576
I0630 02:09:40.570402 29777 layer_factory.hpp:77] Creating layer conv1a
I0630 02:09:40.570416 29777 net.cpp:98] Creating Layer conv1a
I0630 02:09:40.570420 29777 net.cpp:439] conv1a <- data/bias
I0630 02:09:40.570425 29777 net.cpp:413] conv1a -> conv1a
I0630 02:09:40.573179 29807 blocking_queue.cpp:50] Waiting for data
I0630 02:09:40.574221 29777 net.cpp:148] Setting up conv1a
I0630 02:09:40.574234 29777 net.cpp:155] Top shape: 42 32 112 112 (16859136)
I0630 02:09:40.574239 29777 net.cpp:163] Memory required for data: 118014120
I0630 02:09:40.574249 29777 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 02:09:40.574257 29777 net.cpp:98] Creating Layer conv1a/bn
I0630 02:09:40.574261 29777 net.cpp:439] conv1a/bn <- conv1a
I0630 02:09:40.574268 29777 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 02:09:40.575001 29777 net.cpp:148] Setting up conv1a/bn
I0630 02:09:40.575007 29777 net.cpp:155] Top shape: 42 32 112 112 (16859136)
I0630 02:09:40.575009 29777 net.cpp:163] Memory required for data: 185450664
I0630 02:09:40.575018 29777 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 02:09:40.575022 29777 net.cpp:98] Creating Layer conv1a/relu
I0630 02:09:40.575024 29777 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 02:09:40.575027 29777 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 02:09:40.575037 29777 net.cpp:148] Setting up conv1a/relu
I0630 02:09:40.575040 29777 net.cpp:155] Top shape: 42 32 112 112 (16859136)
I0630 02:09:40.575050 29777 net.cpp:163] Memory required for data: 252887208
I0630 02:09:40.575052 29777 layer_factory.hpp:77] Creating layer conv1b
I0630 02:09:40.575057 29777 net.cpp:98] Creating Layer conv1b
I0630 02:09:40.575060 29777 net.cpp:439] conv1b <- conv1a/bn
I0630 02:09:40.575063 29777 net.cpp:413] conv1b -> conv1b
I0630 02:09:40.575395 29777 net.cpp:148] Setting up conv1b
I0630 02:09:40.575402 29777 net.cpp:155] Top shape: 42 32 112 112 (16859136)
I0630 02:09:40.575403 29777 net.cpp:163] Memory required for data: 320323752
I0630 02:09:40.575407 29777 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 02:09:40.575412 29777 net.cpp:98] Creating Layer conv1b/bn
I0630 02:09:40.575413 29777 net.cpp:439] conv1b/bn <- conv1b
I0630 02:09:40.575417 29777 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 02:09:40.576089 29777 net.cpp:148] Setting up conv1b/bn
I0630 02:09:40.576095 29777 net.cpp:155] Top shape: 42 32 112 112 (16859136)
I0630 02:09:40.576097 29777 net.cpp:163] Memory required for data: 387760296
I0630 02:09:40.576103 29777 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 02:09:40.576107 29777 net.cpp:98] Creating Layer conv1b/relu
I0630 02:09:40.576108 29777 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 02:09:40.576110 29777 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 02:09:40.576114 29777 net.cpp:148] Setting up conv1b/relu
I0630 02:09:40.576117 29777 net.cpp:155] Top shape: 42 32 112 112 (16859136)
I0630 02:09:40.576118 29777 net.cpp:163] Memory required for data: 455196840
I0630 02:09:40.576120 29777 layer_factory.hpp:77] Creating layer pool1
I0630 02:09:40.576125 29777 net.cpp:98] Creating Layer pool1
I0630 02:09:40.576128 29777 net.cpp:439] pool1 <- conv1b/bn
I0630 02:09:40.576131 29777 net.cpp:413] pool1 -> pool1
I0630 02:09:40.576176 29777 net.cpp:148] Setting up pool1
I0630 02:09:40.576181 29777 net.cpp:155] Top shape: 42 32 56 56 (4214784)
I0630 02:09:40.576184 29777 net.cpp:163] Memory required for data: 472055976
I0630 02:09:40.576185 29777 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 02:09:40.576189 29777 net.cpp:98] Creating Layer res2a_branch2a
I0630 02:09:40.576191 29777 net.cpp:439] res2a_branch2a <- pool1
I0630 02:09:40.576194 29777 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 02:09:40.576836 29777 net.cpp:148] Setting up res2a_branch2a
I0630 02:09:40.576843 29777 net.cpp:155] Top shape: 42 64 56 56 (8429568)
I0630 02:09:40.576844 29777 net.cpp:163] Memory required for data: 505774248
I0630 02:09:40.576848 29777 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 02:09:40.576853 29777 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 02:09:40.576854 29777 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 02:09:40.576858 29777 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 02:09:40.577534 29777 net.cpp:148] Setting up res2a_branch2a/bn
I0630 02:09:40.577540 29777 net.cpp:155] Top shape: 42 64 56 56 (8429568)
I0630 02:09:40.577543 29777 net.cpp:163] Memory required for data: 539492520
I0630 02:09:40.577548 29777 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 02:09:40.577550 29777 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 02:09:40.577553 29777 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 02:09:40.577554 29777 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 02:09:40.577558 29777 net.cpp:148] Setting up res2a_branch2a/relu
I0630 02:09:40.577560 29777 net.cpp:155] Top shape: 42 64 56 56 (8429568)
I0630 02:09:40.577561 29777 net.cpp:163] Memory required for data: 573210792
I0630 02:09:40.577564 29777 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 02:09:40.577571 29777 net.cpp:98] Creating Layer res2a_branch2b
I0630 02:09:40.577575 29777 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 02:09:40.577579 29777 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 02:09:40.578040 29777 net.cpp:148] Setting up res2a_branch2b
I0630 02:09:40.578045 29777 net.cpp:155] Top shape: 42 64 56 56 (8429568)
I0630 02:09:40.578053 29777 net.cpp:163] Memory required for data: 606929064
I0630 02:09:40.578058 29777 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 02:09:40.578061 29777 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 02:09:40.578063 29777 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 02:09:40.578065 29777 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 02:09:40.578747 29777 net.cpp:148] Setting up res2a_branch2b/bn
I0630 02:09:40.578752 29777 net.cpp:155] Top shape: 42 64 56 56 (8429568)
I0630 02:09:40.578754 29777 net.cpp:163] Memory required for data: 640647336
I0630 02:09:40.578759 29777 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 02:09:40.578763 29777 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 02:09:40.578764 29777 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 02:09:40.578766 29777 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 02:09:40.578773 29777 net.cpp:148] Setting up res2a_branch2b/relu
I0630 02:09:40.578774 29777 net.cpp:155] Top shape: 42 64 56 56 (8429568)
I0630 02:09:40.578776 29777 net.cpp:163] Memory required for data: 674365608
I0630 02:09:40.578778 29777 layer_factory.hpp:77] Creating layer pool2
I0630 02:09:40.578781 29777 net.cpp:98] Creating Layer pool2
I0630 02:09:40.578783 29777 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 02:09:40.578785 29777 net.cpp:413] pool2 -> pool2
I0630 02:09:40.578824 29777 net.cpp:148] Setting up pool2
I0630 02:09:40.578829 29777 net.cpp:155] Top shape: 42 64 28 28 (2107392)
I0630 02:09:40.578830 29777 net.cpp:163] Memory required for data: 682795176
I0630 02:09:40.578835 29777 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 02:09:40.578846 29777 net.cpp:98] Creating Layer res3a_branch2a
I0630 02:09:40.578851 29777 net.cpp:439] res3a_branch2a <- pool2
I0630 02:09:40.578855 29777 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 02:09:40.582299 29777 net.cpp:148] Setting up res3a_branch2a
I0630 02:09:40.582315 29777 net.cpp:155] Top shape: 42 128 28 28 (4214784)
I0630 02:09:40.582317 29777 net.cpp:163] Memory required for data: 699654312
I0630 02:09:40.582322 29777 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 02:09:40.582329 29777 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 02:09:40.582332 29777 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 02:09:40.582335 29777 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 02:09:40.582988 29777 net.cpp:148] Setting up res3a_branch2a/bn
I0630 02:09:40.582995 29777 net.cpp:155] Top shape: 42 128 28 28 (4214784)
I0630 02:09:40.582998 29777 net.cpp:163] Memory required for data: 716513448
I0630 02:09:40.583004 29777 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 02:09:40.583009 29777 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 02:09:40.583010 29777 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 02:09:40.583019 29777 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 02:09:40.583021 29777 net.cpp:148] Setting up res3a_branch2a/relu
I0630 02:09:40.583024 29777 net.cpp:155] Top shape: 42 128 28 28 (4214784)
I0630 02:09:40.583025 29777 net.cpp:163] Memory required for data: 733372584
I0630 02:09:40.583029 29777 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 02:09:40.583034 29777 net.cpp:98] Creating Layer res3a_branch2b
I0630 02:09:40.583036 29777 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 02:09:40.583039 29777 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 02:09:40.584044 29777 net.cpp:148] Setting up res3a_branch2b
I0630 02:09:40.584050 29777 net.cpp:155] Top shape: 42 128 28 28 (4214784)
I0630 02:09:40.584053 29777 net.cpp:163] Memory required for data: 750231720
I0630 02:09:40.584055 29777 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 02:09:40.584059 29777 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 02:09:40.584061 29777 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 02:09:40.584064 29777 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 02:09:40.584702 29777 net.cpp:148] Setting up res3a_branch2b/bn
I0630 02:09:40.584714 29777 net.cpp:155] Top shape: 42 128 28 28 (4214784)
I0630 02:09:40.584717 29777 net.cpp:163] Memory required for data: 767090856
I0630 02:09:40.584722 29777 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 02:09:40.584725 29777 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 02:09:40.584728 29777 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 02:09:40.584731 29777 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 02:09:40.584735 29777 net.cpp:148] Setting up res3a_branch2b/relu
I0630 02:09:40.584738 29777 net.cpp:155] Top shape: 42 128 28 28 (4214784)
I0630 02:09:40.584740 29777 net.cpp:163] Memory required for data: 783949992
I0630 02:09:40.584743 29777 layer_factory.hpp:77] Creating layer pool3
I0630 02:09:40.584746 29777 net.cpp:98] Creating Layer pool3
I0630 02:09:40.584749 29777 net.cpp:439] pool3 <- res3a_branch2b/bn
I0630 02:09:40.584753 29777 net.cpp:413] pool3 -> pool3
I0630 02:09:40.584800 29777 net.cpp:148] Setting up pool3
I0630 02:09:40.584805 29777 net.cpp:155] Top shape: 42 128 14 14 (1053696)
I0630 02:09:40.584806 29777 net.cpp:163] Memory required for data: 788164776
I0630 02:09:40.584808 29777 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 02:09:40.584812 29777 net.cpp:98] Creating Layer res4a_branch2a
I0630 02:09:40.584815 29777 net.cpp:439] res4a_branch2a <- pool3
I0630 02:09:40.584817 29777 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 02:09:40.591033 29777 net.cpp:148] Setting up res4a_branch2a
I0630 02:09:40.591045 29777 net.cpp:155] Top shape: 42 256 14 14 (2107392)
I0630 02:09:40.591048 29777 net.cpp:163] Memory required for data: 796594344
I0630 02:09:40.591051 29777 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 02:09:40.591056 29777 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 02:09:40.591058 29777 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 02:09:40.591061 29777 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 02:09:40.591742 29777 net.cpp:148] Setting up res4a_branch2a/bn
I0630 02:09:40.591747 29777 net.cpp:155] Top shape: 42 256 14 14 (2107392)
I0630 02:09:40.591750 29777 net.cpp:163] Memory required for data: 805023912
I0630 02:09:40.591755 29777 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 02:09:40.591758 29777 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 02:09:40.591760 29777 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 02:09:40.591763 29777 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 02:09:40.591766 29777 net.cpp:148] Setting up res4a_branch2a/relu
I0630 02:09:40.591769 29777 net.cpp:155] Top shape: 42 256 14 14 (2107392)
I0630 02:09:40.591771 29777 net.cpp:163] Memory required for data: 813453480
I0630 02:09:40.591773 29777 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 02:09:40.591776 29777 net.cpp:98] Creating Layer res4a_branch2b
I0630 02:09:40.591778 29777 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 02:09:40.591781 29777 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 02:09:40.594979 29777 net.cpp:148] Setting up res4a_branch2b
I0630 02:09:40.594985 29777 net.cpp:155] Top shape: 42 256 14 14 (2107392)
I0630 02:09:40.594987 29777 net.cpp:163] Memory required for data: 821883048
I0630 02:09:40.594990 29777 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 02:09:40.594995 29777 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 02:09:40.594996 29777 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 02:09:40.595000 29777 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 02:09:40.595646 29777 net.cpp:148] Setting up res4a_branch2b/bn
I0630 02:09:40.595652 29777 net.cpp:155] Top shape: 42 256 14 14 (2107392)
I0630 02:09:40.595654 29777 net.cpp:163] Memory required for data: 830312616
I0630 02:09:40.595659 29777 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 02:09:40.595662 29777 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 02:09:40.595664 29777 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 02:09:40.595674 29777 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 02:09:40.595679 29777 net.cpp:148] Setting up res4a_branch2b/relu
I0630 02:09:40.595680 29777 net.cpp:155] Top shape: 42 256 14 14 (2107392)
I0630 02:09:40.595682 29777 net.cpp:163] Memory required for data: 838742184
I0630 02:09:40.595685 29777 layer_factory.hpp:77] Creating layer pool4
I0630 02:09:40.595687 29777 net.cpp:98] Creating Layer pool4
I0630 02:09:40.595690 29777 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 02:09:40.595693 29777 net.cpp:413] pool4 -> pool4
I0630 02:09:40.595736 29777 net.cpp:148] Setting up pool4
I0630 02:09:40.595742 29777 net.cpp:155] Top shape: 42 256 7 7 (526848)
I0630 02:09:40.595744 29777 net.cpp:163] Memory required for data: 840849576
I0630 02:09:40.595746 29777 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 02:09:40.595751 29777 net.cpp:98] Creating Layer res5a_branch2a
I0630 02:09:40.595752 29777 net.cpp:439] res5a_branch2a <- pool4
I0630 02:09:40.595755 29777 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 02:09:40.620772 29777 net.cpp:148] Setting up res5a_branch2a
I0630 02:09:40.620793 29777 net.cpp:155] Top shape: 42 512 7 7 (1053696)
I0630 02:09:40.620795 29777 net.cpp:163] Memory required for data: 845064360
I0630 02:09:40.620801 29777 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 02:09:40.620810 29777 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 02:09:40.620812 29777 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 02:09:40.620816 29777 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 02:09:40.621500 29777 net.cpp:148] Setting up res5a_branch2a/bn
I0630 02:09:40.621506 29777 net.cpp:155] Top shape: 42 512 7 7 (1053696)
I0630 02:09:40.621508 29777 net.cpp:163] Memory required for data: 849279144
I0630 02:09:40.621513 29777 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 02:09:40.621517 29777 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 02:09:40.621520 29777 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 02:09:40.621522 29777 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 02:09:40.621526 29777 net.cpp:148] Setting up res5a_branch2a/relu
I0630 02:09:40.621529 29777 net.cpp:155] Top shape: 42 512 7 7 (1053696)
I0630 02:09:40.621531 29777 net.cpp:163] Memory required for data: 853493928
I0630 02:09:40.621532 29777 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 02:09:40.621537 29777 net.cpp:98] Creating Layer res5a_branch2b
I0630 02:09:40.621539 29777 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 02:09:40.621541 29777 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 02:09:40.634639 29777 net.cpp:148] Setting up res5a_branch2b
I0630 02:09:40.634660 29777 net.cpp:155] Top shape: 42 512 7 7 (1053696)
I0630 02:09:40.634663 29777 net.cpp:163] Memory required for data: 857708712
I0630 02:09:40.634676 29777 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 02:09:40.634685 29777 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 02:09:40.634688 29777 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 02:09:40.634692 29777 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 02:09:40.635387 29777 net.cpp:148] Setting up res5a_branch2b/bn
I0630 02:09:40.635395 29777 net.cpp:155] Top shape: 42 512 7 7 (1053696)
I0630 02:09:40.635396 29777 net.cpp:163] Memory required for data: 861923496
I0630 02:09:40.635401 29777 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 02:09:40.635406 29777 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 02:09:40.635407 29777 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 02:09:40.635409 29777 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 02:09:40.635413 29777 net.cpp:148] Setting up res5a_branch2b/relu
I0630 02:09:40.635416 29777 net.cpp:155] Top shape: 42 512 7 7 (1053696)
I0630 02:09:40.635418 29777 net.cpp:163] Memory required for data: 866138280
I0630 02:09:40.635421 29777 layer_factory.hpp:77] Creating layer pool5
I0630 02:09:40.635426 29777 net.cpp:98] Creating Layer pool5
I0630 02:09:40.635438 29777 net.cpp:439] pool5 <- res5a_branch2b/bn
I0630 02:09:40.635442 29777 net.cpp:413] pool5 -> pool5
I0630 02:09:40.635471 29777 net.cpp:148] Setting up pool5
I0630 02:09:40.635476 29777 net.cpp:155] Top shape: 42 512 1 1 (21504)
I0630 02:09:40.635478 29777 net.cpp:163] Memory required for data: 866224296
I0630 02:09:40.635480 29777 layer_factory.hpp:77] Creating layer fc1000
I0630 02:09:40.635483 29777 net.cpp:98] Creating Layer fc1000
I0630 02:09:40.635486 29777 net.cpp:439] fc1000 <- pool5
I0630 02:09:40.635489 29777 net.cpp:413] fc1000 -> fc1000
I0630 02:09:40.646884 29777 net.cpp:148] Setting up fc1000
I0630 02:09:40.646904 29777 net.cpp:155] Top shape: 42 1000 (42000)
I0630 02:09:40.646906 29777 net.cpp:163] Memory required for data: 866392296
I0630 02:09:40.646914 29777 layer_factory.hpp:77] Creating layer loss
I0630 02:09:40.646924 29777 net.cpp:98] Creating Layer loss
I0630 02:09:40.646927 29777 net.cpp:439] loss <- fc1000
I0630 02:09:40.646930 29777 net.cpp:439] loss <- label
I0630 02:09:40.646935 29777 net.cpp:413] loss -> loss
I0630 02:09:40.646944 29777 layer_factory.hpp:77] Creating layer loss
I0630 02:09:40.647111 29777 net.cpp:148] Setting up loss
I0630 02:09:40.647119 29777 net.cpp:155] Top shape: (1)
I0630 02:09:40.647120 29777 net.cpp:158]     with loss weight 1
I0630 02:09:40.647130 29777 net.cpp:163] Memory required for data: 866392300
I0630 02:09:40.647133 29777 net.cpp:224] loss needs backward computation.
I0630 02:09:40.647135 29777 net.cpp:224] fc1000 needs backward computation.
I0630 02:09:40.647137 29777 net.cpp:224] pool5 needs backward computation.
I0630 02:09:40.647140 29777 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 02:09:40.647141 29777 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 02:09:40.647143 29777 net.cpp:224] res5a_branch2b needs backward computation.
I0630 02:09:40.647145 29777 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 02:09:40.647146 29777 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 02:09:40.647150 29777 net.cpp:224] res5a_branch2a needs backward computation.
I0630 02:09:40.647151 29777 net.cpp:224] pool4 needs backward computation.
I0630 02:09:40.647155 29777 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 02:09:40.647156 29777 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 02:09:40.647158 29777 net.cpp:224] res4a_branch2b needs backward computation.
I0630 02:09:40.647161 29777 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 02:09:40.647163 29777 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 02:09:40.647166 29777 net.cpp:224] res4a_branch2a needs backward computation.
I0630 02:09:40.647168 29777 net.cpp:224] pool3 needs backward computation.
I0630 02:09:40.647171 29777 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 02:09:40.647173 29777 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 02:09:40.647176 29777 net.cpp:224] res3a_branch2b needs backward computation.
I0630 02:09:40.647178 29777 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 02:09:40.647181 29777 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 02:09:40.647184 29777 net.cpp:224] res3a_branch2a needs backward computation.
I0630 02:09:40.647188 29777 net.cpp:224] pool2 needs backward computation.
I0630 02:09:40.647192 29777 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 02:09:40.647195 29777 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 02:09:40.647199 29777 net.cpp:224] res2a_branch2b needs backward computation.
I0630 02:09:40.647213 29777 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 02:09:40.647217 29777 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 02:09:40.647219 29777 net.cpp:224] res2a_branch2a needs backward computation.
I0630 02:09:40.647222 29777 net.cpp:224] pool1 needs backward computation.
I0630 02:09:40.647223 29777 net.cpp:224] conv1b/relu needs backward computation.
I0630 02:09:40.647239 29777 net.cpp:224] conv1b/bn needs backward computation.
I0630 02:09:40.647244 29777 net.cpp:224] conv1b needs backward computation.
I0630 02:09:40.647246 29777 net.cpp:224] conv1a/relu needs backward computation.
I0630 02:09:40.647249 29777 net.cpp:224] conv1a/bn needs backward computation.
I0630 02:09:40.647254 29777 net.cpp:224] conv1a needs backward computation.
I0630 02:09:40.647258 29777 net.cpp:226] data/bias does not need backward computation.
I0630 02:09:40.647263 29777 net.cpp:226] data does not need backward computation.
I0630 02:09:40.647265 29777 net.cpp:268] This network produces output loss
I0630 02:09:40.647285 29777 net.cpp:288] Network initialization done.
I0630 02:09:40.647730 29777 solver.cpp:182] Creating test net (#0) specified by test_net file: training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/test.prototxt
I0630 02:09:40.647917 29777 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 224
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1000"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc1000"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc1000"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0630 02:09:40.648015 29777 layer_factory.hpp:77] Creating layer data
I0630 02:09:40.648085 29777 net.cpp:98] Creating Layer data
I0630 02:09:40.648100 29777 net.cpp:413] data -> data
I0630 02:09:40.648113 29777 net.cpp:413] data -> label
I0630 02:09:40.663002 29818 db_lmdb.cpp:35] Opened lmdb ./data/ilsvrc12_val_lmdb
I0630 02:09:40.664290 29777 data_layer.cpp:78] ReshapePrefetch 50, 3, 224, 224
I0630 02:09:40.664379 29777 data_layer.cpp:83] output data size: 50,3,224,224
I0630 02:09:40.695499 29777 net.cpp:148] Setting up data
I0630 02:09:40.695518 29777 net.cpp:155] Top shape: 50 3 224 224 (7526400)
I0630 02:09:40.695523 29777 net.cpp:155] Top shape: 50 (50)
I0630 02:09:40.695524 29777 net.cpp:163] Memory required for data: 30105800
I0630 02:09:40.695528 29777 layer_factory.hpp:77] Creating layer label_data_1_split
I0630 02:09:40.695538 29777 net.cpp:98] Creating Layer label_data_1_split
I0630 02:09:40.695539 29777 net.cpp:439] label_data_1_split <- label
I0630 02:09:40.695544 29777 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0630 02:09:40.695550 29777 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0630 02:09:40.695554 29777 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0630 02:09:40.695716 29777 net.cpp:148] Setting up label_data_1_split
I0630 02:09:40.695726 29777 net.cpp:155] Top shape: 50 (50)
I0630 02:09:40.695729 29777 net.cpp:155] Top shape: 50 (50)
I0630 02:09:40.695731 29777 net.cpp:155] Top shape: 50 (50)
I0630 02:09:40.695734 29777 net.cpp:163] Memory required for data: 30106400
I0630 02:09:40.695735 29777 layer_factory.hpp:77] Creating layer data/bias
I0630 02:09:40.695741 29777 net.cpp:98] Creating Layer data/bias
I0630 02:09:40.695744 29777 net.cpp:439] data/bias <- data
I0630 02:09:40.695755 29777 net.cpp:413] data/bias -> data/bias
I0630 02:09:40.695916 29777 net.cpp:148] Setting up data/bias
I0630 02:09:40.695922 29777 net.cpp:155] Top shape: 50 3 224 224 (7526400)
I0630 02:09:40.695924 29777 net.cpp:163] Memory required for data: 60212000
I0630 02:09:40.695930 29777 layer_factory.hpp:77] Creating layer conv1a
I0630 02:09:40.695935 29777 net.cpp:98] Creating Layer conv1a
I0630 02:09:40.695936 29777 net.cpp:439] conv1a <- data/bias
I0630 02:09:40.695940 29777 net.cpp:413] conv1a -> conv1a
I0630 02:09:40.696461 29777 net.cpp:148] Setting up conv1a
I0630 02:09:40.696468 29777 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 02:09:40.696470 29777 net.cpp:163] Memory required for data: 140493600
I0630 02:09:40.696475 29777 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 02:09:40.696480 29777 net.cpp:98] Creating Layer conv1a/bn
I0630 02:09:40.696482 29777 net.cpp:439] conv1a/bn <- conv1a
I0630 02:09:40.696485 29777 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 02:09:40.699566 29777 net.cpp:148] Setting up conv1a/bn
I0630 02:09:40.699574 29777 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 02:09:40.699586 29777 net.cpp:163] Memory required for data: 220775200
I0630 02:09:40.699596 29777 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 02:09:40.699601 29777 net.cpp:98] Creating Layer conv1a/relu
I0630 02:09:40.699604 29777 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 02:09:40.699606 29777 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 02:09:40.699611 29777 net.cpp:148] Setting up conv1a/relu
I0630 02:09:40.699614 29777 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 02:09:40.699615 29777 net.cpp:163] Memory required for data: 301056800
I0630 02:09:40.699617 29777 layer_factory.hpp:77] Creating layer conv1b
I0630 02:09:40.699621 29777 net.cpp:98] Creating Layer conv1b
I0630 02:09:40.699623 29777 net.cpp:439] conv1b <- conv1a/bn
I0630 02:09:40.699626 29777 net.cpp:413] conv1b -> conv1b
I0630 02:09:40.699985 29777 net.cpp:148] Setting up conv1b
I0630 02:09:40.699991 29777 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 02:09:40.699995 29777 net.cpp:163] Memory required for data: 381338400
I0630 02:09:40.699998 29777 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 02:09:40.700002 29777 net.cpp:98] Creating Layer conv1b/bn
I0630 02:09:40.700004 29777 net.cpp:439] conv1b/bn <- conv1b
I0630 02:09:40.700007 29777 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 02:09:40.700743 29777 net.cpp:148] Setting up conv1b/bn
I0630 02:09:40.700750 29777 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 02:09:40.700753 29777 net.cpp:163] Memory required for data: 461620000
I0630 02:09:40.700760 29777 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 02:09:40.700764 29777 net.cpp:98] Creating Layer conv1b/relu
I0630 02:09:40.700767 29777 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 02:09:40.700769 29777 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 02:09:40.700773 29777 net.cpp:148] Setting up conv1b/relu
I0630 02:09:40.700776 29777 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 02:09:40.700778 29777 net.cpp:163] Memory required for data: 541901600
I0630 02:09:40.700781 29777 layer_factory.hpp:77] Creating layer pool1
I0630 02:09:40.700785 29777 net.cpp:98] Creating Layer pool1
I0630 02:09:40.700788 29777 net.cpp:439] pool1 <- conv1b/bn
I0630 02:09:40.700789 29777 net.cpp:413] pool1 -> pool1
I0630 02:09:40.700826 29777 net.cpp:148] Setting up pool1
I0630 02:09:40.700831 29777 net.cpp:155] Top shape: 50 32 56 56 (5017600)
I0630 02:09:40.700834 29777 net.cpp:163] Memory required for data: 561972000
I0630 02:09:40.700835 29777 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 02:09:40.700847 29777 net.cpp:98] Creating Layer res2a_branch2a
I0630 02:09:40.700852 29777 net.cpp:439] res2a_branch2a <- pool1
I0630 02:09:40.700857 29777 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 02:09:40.701581 29777 net.cpp:148] Setting up res2a_branch2a
I0630 02:09:40.701587 29777 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 02:09:40.701591 29777 net.cpp:163] Memory required for data: 602112800
I0630 02:09:40.701596 29777 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 02:09:40.701599 29777 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 02:09:40.701602 29777 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 02:09:40.701606 29777 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 02:09:40.702350 29777 net.cpp:148] Setting up res2a_branch2a/bn
I0630 02:09:40.702356 29777 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 02:09:40.702359 29777 net.cpp:163] Memory required for data: 642253600
I0630 02:09:40.702364 29777 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 02:09:40.702368 29777 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 02:09:40.702370 29777 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 02:09:40.702373 29777 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 02:09:40.702378 29777 net.cpp:148] Setting up res2a_branch2a/relu
I0630 02:09:40.702379 29777 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 02:09:40.702381 29777 net.cpp:163] Memory required for data: 682394400
I0630 02:09:40.702390 29777 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 02:09:40.702395 29777 net.cpp:98] Creating Layer res2a_branch2b
I0630 02:09:40.702397 29777 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 02:09:40.702400 29777 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 02:09:40.702908 29777 net.cpp:148] Setting up res2a_branch2b
I0630 02:09:40.702915 29777 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 02:09:40.702919 29777 net.cpp:163] Memory required for data: 722535200
I0630 02:09:40.702922 29777 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 02:09:40.702926 29777 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 02:09:40.702929 29777 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 02:09:40.702932 29777 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 02:09:40.703675 29777 net.cpp:148] Setting up res2a_branch2b/bn
I0630 02:09:40.703681 29777 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 02:09:40.703685 29777 net.cpp:163] Memory required for data: 762676000
I0630 02:09:40.703689 29777 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 02:09:40.703692 29777 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 02:09:40.703696 29777 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 02:09:40.703698 29777 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 02:09:40.703701 29777 net.cpp:148] Setting up res2a_branch2b/relu
I0630 02:09:40.703704 29777 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 02:09:40.703707 29777 net.cpp:163] Memory required for data: 802816800
I0630 02:09:40.703709 29777 layer_factory.hpp:77] Creating layer pool2
I0630 02:09:40.703712 29777 net.cpp:98] Creating Layer pool2
I0630 02:09:40.703716 29777 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 02:09:40.703717 29777 net.cpp:413] pool2 -> pool2
I0630 02:09:40.703757 29777 net.cpp:148] Setting up pool2
I0630 02:09:40.703760 29777 net.cpp:155] Top shape: 50 64 28 28 (2508800)
I0630 02:09:40.703763 29777 net.cpp:163] Memory required for data: 812852000
I0630 02:09:40.703765 29777 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 02:09:40.703769 29777 net.cpp:98] Creating Layer res3a_branch2a
I0630 02:09:40.703771 29777 net.cpp:439] res3a_branch2a <- pool2
I0630 02:09:40.703775 29777 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 02:09:40.706461 29777 net.cpp:148] Setting up res3a_branch2a
I0630 02:09:40.706470 29777 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 02:09:40.706472 29777 net.cpp:163] Memory required for data: 832922400
I0630 02:09:40.706477 29777 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 02:09:40.706481 29777 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 02:09:40.706485 29777 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 02:09:40.706488 29777 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 02:09:40.707192 29777 net.cpp:148] Setting up res3a_branch2a/bn
I0630 02:09:40.707200 29777 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 02:09:40.707201 29777 net.cpp:163] Memory required for data: 852992800
I0630 02:09:40.707208 29777 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 02:09:40.707214 29777 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 02:09:40.707216 29777 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 02:09:40.707219 29777 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 02:09:40.707222 29777 net.cpp:148] Setting up res3a_branch2a/relu
I0630 02:09:40.707226 29777 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 02:09:40.707227 29777 net.cpp:163] Memory required for data: 873063200
I0630 02:09:40.707229 29777 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 02:09:40.707233 29777 net.cpp:98] Creating Layer res3a_branch2b
I0630 02:09:40.707235 29777 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 02:09:40.707238 29777 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 02:09:40.708278 29777 net.cpp:148] Setting up res3a_branch2b
I0630 02:09:40.708294 29777 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 02:09:40.708297 29777 net.cpp:163] Memory required for data: 893133600
I0630 02:09:40.708299 29777 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 02:09:40.708304 29777 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 02:09:40.708307 29777 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 02:09:40.708310 29777 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 02:09:40.708988 29777 net.cpp:148] Setting up res3a_branch2b/bn
I0630 02:09:40.708994 29777 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 02:09:40.708997 29777 net.cpp:163] Memory required for data: 913204000
I0630 02:09:40.709002 29777 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 02:09:40.709008 29777 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 02:09:40.709010 29777 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 02:09:40.709012 29777 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 02:09:40.709017 29777 net.cpp:148] Setting up res3a_branch2b/relu
I0630 02:09:40.709018 29777 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 02:09:40.709020 29777 net.cpp:163] Memory required for data: 933274400
I0630 02:09:40.709022 29777 layer_factory.hpp:77] Creating layer pool3
I0630 02:09:40.709025 29777 net.cpp:98] Creating Layer pool3
I0630 02:09:40.709028 29777 net.cpp:439] pool3 <- res3a_branch2b/bn
I0630 02:09:40.709033 29777 net.cpp:413] pool3 -> pool3
I0630 02:09:40.709074 29777 net.cpp:148] Setting up pool3
I0630 02:09:40.709077 29777 net.cpp:155] Top shape: 50 128 14 14 (1254400)
I0630 02:09:40.709079 29777 net.cpp:163] Memory required for data: 938292000
I0630 02:09:40.709081 29777 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 02:09:40.709086 29777 net.cpp:98] Creating Layer res4a_branch2a
I0630 02:09:40.709089 29777 net.cpp:439] res4a_branch2a <- pool3
I0630 02:09:40.709091 29777 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 02:09:40.715198 29777 net.cpp:148] Setting up res4a_branch2a
I0630 02:09:40.715204 29777 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 02:09:40.715207 29777 net.cpp:163] Memory required for data: 948327200
I0630 02:09:40.715211 29777 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 02:09:40.715215 29777 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 02:09:40.715217 29777 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 02:09:40.715221 29777 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 02:09:40.715937 29777 net.cpp:148] Setting up res4a_branch2a/bn
I0630 02:09:40.715945 29777 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 02:09:40.715947 29777 net.cpp:163] Memory required for data: 958362400
I0630 02:09:40.715952 29777 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 02:09:40.715955 29777 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 02:09:40.715957 29777 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 02:09:40.715960 29777 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 02:09:40.715963 29777 net.cpp:148] Setting up res4a_branch2a/relu
I0630 02:09:40.715966 29777 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 02:09:40.715968 29777 net.cpp:163] Memory required for data: 968397600
I0630 02:09:40.715970 29777 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 02:09:40.715975 29777 net.cpp:98] Creating Layer res4a_branch2b
I0630 02:09:40.715976 29777 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 02:09:40.715979 29777 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 02:09:40.719264 29777 net.cpp:148] Setting up res4a_branch2b
I0630 02:09:40.719274 29777 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 02:09:40.719277 29777 net.cpp:163] Memory required for data: 978432800
I0630 02:09:40.719283 29777 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 02:09:40.719288 29777 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 02:09:40.719292 29777 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 02:09:40.719296 29777 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 02:09:40.720229 29777 net.cpp:148] Setting up res4a_branch2b/bn
I0630 02:09:40.720240 29777 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 02:09:40.720243 29777 net.cpp:163] Memory required for data: 988468000
I0630 02:09:40.720249 29777 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 02:09:40.720254 29777 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 02:09:40.720257 29777 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 02:09:40.720260 29777 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 02:09:40.720264 29777 net.cpp:148] Setting up res4a_branch2b/relu
I0630 02:09:40.720268 29777 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 02:09:40.720269 29777 net.cpp:163] Memory required for data: 998503200
I0630 02:09:40.720270 29777 layer_factory.hpp:77] Creating layer pool4
I0630 02:09:40.720274 29777 net.cpp:98] Creating Layer pool4
I0630 02:09:40.720276 29777 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 02:09:40.720278 29777 net.cpp:413] pool4 -> pool4
I0630 02:09:40.720333 29777 net.cpp:148] Setting up pool4
I0630 02:09:40.720340 29777 net.cpp:155] Top shape: 50 256 7 7 (627200)
I0630 02:09:40.720341 29777 net.cpp:163] Memory required for data: 1001012000
I0630 02:09:40.720343 29777 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 02:09:40.720350 29777 net.cpp:98] Creating Layer res5a_branch2a
I0630 02:09:40.720352 29777 net.cpp:439] res5a_branch2a <- pool4
I0630 02:09:40.720355 29777 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 02:09:40.745447 29777 net.cpp:148] Setting up res5a_branch2a
I0630 02:09:40.745470 29777 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 02:09:40.745472 29777 net.cpp:163] Memory required for data: 1006029600
I0630 02:09:40.745477 29777 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 02:09:40.745487 29777 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 02:09:40.745491 29777 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 02:09:40.745496 29777 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 02:09:40.746234 29777 net.cpp:148] Setting up res5a_branch2a/bn
I0630 02:09:40.746242 29777 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 02:09:40.746243 29777 net.cpp:163] Memory required for data: 1011047200
I0630 02:09:40.746249 29777 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 02:09:40.746258 29777 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 02:09:40.746260 29777 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 02:09:40.746263 29777 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 02:09:40.746268 29777 net.cpp:148] Setting up res5a_branch2a/relu
I0630 02:09:40.746271 29777 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 02:09:40.746274 29777 net.cpp:163] Memory required for data: 1016064800
I0630 02:09:40.746276 29777 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 02:09:40.746282 29777 net.cpp:98] Creating Layer res5a_branch2b
I0630 02:09:40.746284 29777 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 02:09:40.746287 29777 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 02:09:40.759701 29777 net.cpp:148] Setting up res5a_branch2b
I0630 02:09:40.759719 29777 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 02:09:40.759722 29777 net.cpp:163] Memory required for data: 1021082400
I0630 02:09:40.759729 29777 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 02:09:40.759737 29777 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 02:09:40.759739 29777 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 02:09:40.759743 29777 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 02:09:40.760496 29777 net.cpp:148] Setting up res5a_branch2b/bn
I0630 02:09:40.760504 29777 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 02:09:40.760505 29777 net.cpp:163] Memory required for data: 1026100000
I0630 02:09:40.760511 29777 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 02:09:40.760515 29777 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 02:09:40.760527 29777 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 02:09:40.760530 29777 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 02:09:40.760535 29777 net.cpp:148] Setting up res5a_branch2b/relu
I0630 02:09:40.760537 29777 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 02:09:40.760540 29777 net.cpp:163] Memory required for data: 1031117600
I0630 02:09:40.760541 29777 layer_factory.hpp:77] Creating layer pool5
I0630 02:09:40.760546 29777 net.cpp:98] Creating Layer pool5
I0630 02:09:40.760548 29777 net.cpp:439] pool5 <- res5a_branch2b/bn
I0630 02:09:40.760551 29777 net.cpp:413] pool5 -> pool5
I0630 02:09:40.760576 29777 net.cpp:148] Setting up pool5
I0630 02:09:40.760581 29777 net.cpp:155] Top shape: 50 512 1 1 (25600)
I0630 02:09:40.760582 29777 net.cpp:163] Memory required for data: 1031220000
I0630 02:09:40.760584 29777 layer_factory.hpp:77] Creating layer fc1000
I0630 02:09:40.760587 29777 net.cpp:98] Creating Layer fc1000
I0630 02:09:40.760589 29777 net.cpp:439] fc1000 <- pool5
I0630 02:09:40.760592 29777 net.cpp:413] fc1000 -> fc1000
I0630 02:09:40.771718 29777 net.cpp:148] Setting up fc1000
I0630 02:09:40.771730 29777 net.cpp:155] Top shape: 50 1000 (50000)
I0630 02:09:40.771733 29777 net.cpp:163] Memory required for data: 1031420000
I0630 02:09:40.771737 29777 layer_factory.hpp:77] Creating layer fc1000_fc1000_0_split
I0630 02:09:40.771744 29777 net.cpp:98] Creating Layer fc1000_fc1000_0_split
I0630 02:09:40.771745 29777 net.cpp:439] fc1000_fc1000_0_split <- fc1000
I0630 02:09:40.771749 29777 net.cpp:413] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_0
I0630 02:09:40.771754 29777 net.cpp:413] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_1
I0630 02:09:40.771757 29777 net.cpp:413] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_2
I0630 02:09:40.771816 29777 net.cpp:148] Setting up fc1000_fc1000_0_split
I0630 02:09:40.771821 29777 net.cpp:155] Top shape: 50 1000 (50000)
I0630 02:09:40.771823 29777 net.cpp:155] Top shape: 50 1000 (50000)
I0630 02:09:40.771826 29777 net.cpp:155] Top shape: 50 1000 (50000)
I0630 02:09:40.771826 29777 net.cpp:163] Memory required for data: 1032020000
I0630 02:09:40.771829 29777 layer_factory.hpp:77] Creating layer loss
I0630 02:09:40.771832 29777 net.cpp:98] Creating Layer loss
I0630 02:09:40.771836 29777 net.cpp:439] loss <- fc1000_fc1000_0_split_0
I0630 02:09:40.771837 29777 net.cpp:439] loss <- label_data_1_split_0
I0630 02:09:40.771842 29777 net.cpp:413] loss -> loss
I0630 02:09:40.771845 29777 layer_factory.hpp:77] Creating layer loss
I0630 02:09:40.771991 29777 net.cpp:148] Setting up loss
I0630 02:09:40.771996 29777 net.cpp:155] Top shape: (1)
I0630 02:09:40.771998 29777 net.cpp:158]     with loss weight 1
I0630 02:09:40.772006 29777 net.cpp:163] Memory required for data: 1032020004
I0630 02:09:40.772007 29777 layer_factory.hpp:77] Creating layer accuracy/top1
I0630 02:09:40.772017 29777 net.cpp:98] Creating Layer accuracy/top1
I0630 02:09:40.772019 29777 net.cpp:439] accuracy/top1 <- fc1000_fc1000_0_split_1
I0630 02:09:40.772022 29777 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0630 02:09:40.772025 29777 net.cpp:413] accuracy/top1 -> accuracy/top1
I0630 02:09:40.772032 29777 net.cpp:148] Setting up accuracy/top1
I0630 02:09:40.772037 29777 net.cpp:155] Top shape: (1)
I0630 02:09:40.772042 29777 net.cpp:163] Memory required for data: 1032020008
I0630 02:09:40.772044 29777 layer_factory.hpp:77] Creating layer accuracy/top5
I0630 02:09:40.772054 29777 net.cpp:98] Creating Layer accuracy/top5
I0630 02:09:40.772058 29777 net.cpp:439] accuracy/top5 <- fc1000_fc1000_0_split_2
I0630 02:09:40.772061 29777 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0630 02:09:40.772068 29777 net.cpp:413] accuracy/top5 -> accuracy/top5
I0630 02:09:40.772074 29777 net.cpp:148] Setting up accuracy/top5
I0630 02:09:40.772079 29777 net.cpp:155] Top shape: (1)
I0630 02:09:40.772083 29777 net.cpp:163] Memory required for data: 1032020012
I0630 02:09:40.772086 29777 net.cpp:226] accuracy/top5 does not need backward computation.
I0630 02:09:40.772097 29777 net.cpp:226] accuracy/top1 does not need backward computation.
I0630 02:09:40.772101 29777 net.cpp:224] loss needs backward computation.
I0630 02:09:40.772105 29777 net.cpp:224] fc1000_fc1000_0_split needs backward computation.
I0630 02:09:40.772109 29777 net.cpp:224] fc1000 needs backward computation.
I0630 02:09:40.772112 29777 net.cpp:224] pool5 needs backward computation.
I0630 02:09:40.772116 29777 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 02:09:40.772119 29777 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 02:09:40.772123 29777 net.cpp:224] res5a_branch2b needs backward computation.
I0630 02:09:40.772127 29777 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 02:09:40.772131 29777 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 02:09:40.772135 29777 net.cpp:224] res5a_branch2a needs backward computation.
I0630 02:09:40.772138 29777 net.cpp:224] pool4 needs backward computation.
I0630 02:09:40.772142 29777 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 02:09:40.772146 29777 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 02:09:40.772150 29777 net.cpp:224] res4a_branch2b needs backward computation.
I0630 02:09:40.772155 29777 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 02:09:40.772158 29777 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 02:09:40.772163 29777 net.cpp:224] res4a_branch2a needs backward computation.
I0630 02:09:40.772167 29777 net.cpp:224] pool3 needs backward computation.
I0630 02:09:40.772171 29777 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 02:09:40.772176 29777 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 02:09:40.772179 29777 net.cpp:224] res3a_branch2b needs backward computation.
I0630 02:09:40.772183 29777 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 02:09:40.772187 29777 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 02:09:40.772192 29777 net.cpp:224] res3a_branch2a needs backward computation.
I0630 02:09:40.772195 29777 net.cpp:224] pool2 needs backward computation.
I0630 02:09:40.772199 29777 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 02:09:40.772203 29777 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 02:09:40.772207 29777 net.cpp:224] res2a_branch2b needs backward computation.
I0630 02:09:40.772212 29777 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 02:09:40.772215 29777 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 02:09:40.772219 29777 net.cpp:224] res2a_branch2a needs backward computation.
I0630 02:09:40.772225 29777 net.cpp:224] pool1 needs backward computation.
I0630 02:09:40.772229 29777 net.cpp:224] conv1b/relu needs backward computation.
I0630 02:09:40.772233 29777 net.cpp:224] conv1b/bn needs backward computation.
I0630 02:09:40.772238 29777 net.cpp:224] conv1b needs backward computation.
I0630 02:09:40.772241 29777 net.cpp:224] conv1a/relu needs backward computation.
I0630 02:09:40.772245 29777 net.cpp:224] conv1a/bn needs backward computation.
I0630 02:09:40.772249 29777 net.cpp:224] conv1a needs backward computation.
I0630 02:09:40.772253 29777 net.cpp:226] data/bias does not need backward computation.
I0630 02:09:40.772258 29777 net.cpp:226] label_data_1_split does not need backward computation.
I0630 02:09:40.772263 29777 net.cpp:226] data does not need backward computation.
I0630 02:09:40.772266 29777 net.cpp:268] This network produces output accuracy/top1
I0630 02:09:40.772270 29777 net.cpp:268] This network produces output accuracy/top5
I0630 02:09:40.772274 29777 net.cpp:268] This network produces output loss
I0630 02:09:40.772300 29777 net.cpp:288] Network initialization done.
I0630 02:09:40.772368 29777 solver.cpp:60] Solver scaffolding done.
I0630 02:09:40.775967 29777 caffe.cpp:145] Finetuning from training/imagenet_jacintonet11v2_2017-06-30_02-08-23/initial/imagenet_jacintonet11v2_iter_100.caffemodel
I0630 02:09:40.815187 29777 data_layer.cpp:78] ReshapePrefetch 42, 3, 224, 224
I0630 02:09:40.815277 29777 data_layer.cpp:83] output data size: 42,3,224,224
I0630 02:09:41.308568 29777 data_layer.cpp:78] ReshapePrefetch 42, 3, 224, 224
I0630 02:09:41.308657 29777 data_layer.cpp:83] output data size: 42,3,224,224
I0630 02:09:41.862990 29777 parallel.cpp:334] Starting Optimization
I0630 02:09:41.863044 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 02:09:41.874382 29777 solver.cpp:413] Solving jacintonet11v2_train
I0630 02:09:41.874397 29777 solver.cpp:414] Learning Rate Policy: poly
I0630 02:09:41.876631 29777 solver.cpp:471] Iteration 0, Testing net (#0)
I0630 02:09:42.012605 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 02:10:30.786938 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.609159
I0630 02:10:30.787006 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.832002
I0630 02:10:30.787014 29777 solver.cpp:544]     Test net output #2: loss = 1.33232 (* 1 = 1.33232 loss)
I0630 02:10:31.044181 29777 solver.cpp:290] Iteration 0 (0 iter/s, 49.1684s/100 iter), loss = 1.05952
I0630 02:10:31.044205 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 02:10:31.044214 29777 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0630 02:10:31.058009 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.01
I0630 02:10:31.161880 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 02:10:47.122442 29777 solver.cpp:290] Iteration 100 (6.21976 iter/s, 16.0778s/100 iter), loss = 1.07143
I0630 02:10:47.122467 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 02:10:47.122475 29777 sgd_solver.cpp:106] Iteration 100, lr = 0.00999687
I0630 02:11:03.078019 29777 solver.cpp:290] Iteration 200 (6.26759 iter/s, 15.9551s/100 iter), loss = 1.16667
I0630 02:11:03.078119 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 02:11:03.078130 29777 sgd_solver.cpp:106] Iteration 200, lr = 0.00999375
I0630 02:11:19.083906 29777 solver.cpp:290] Iteration 300 (6.24792 iter/s, 16.0053s/100 iter), loss = 1.04762
I0630 02:11:19.083933 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 02:11:19.083942 29777 sgd_solver.cpp:106] Iteration 300, lr = 0.00999062
I0630 02:11:35.040550 29777 solver.cpp:290] Iteration 400 (6.26717 iter/s, 15.9562s/100 iter), loss = 1.10714
I0630 02:11:35.040596 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 02:11:35.040603 29777 sgd_solver.cpp:106] Iteration 400, lr = 0.0099875
I0630 02:11:50.970609 29777 solver.cpp:290] Iteration 500 (6.27764 iter/s, 15.9296s/100 iter), loss = 0.988095
I0630 02:11:50.970640 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 02:11:50.970650 29777 sgd_solver.cpp:106] Iteration 500, lr = 0.00998437
I0630 02:12:06.887112 29777 solver.cpp:290] Iteration 600 (6.28297 iter/s, 15.916s/100 iter), loss = 0.857143
I0630 02:12:06.887166 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 02:12:06.887173 29777 sgd_solver.cpp:106] Iteration 600, lr = 0.00998125
I0630 02:12:22.887756 29777 solver.cpp:290] Iteration 700 (6.24994 iter/s, 16.0001s/100 iter), loss = 1.28571
I0630 02:12:22.887783 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 02:12:22.887828 29777 sgd_solver.cpp:106] Iteration 700, lr = 0.00997812
I0630 02:12:38.857633 29777 solver.cpp:290] Iteration 800 (6.26197 iter/s, 15.9694s/100 iter), loss = 1.15476
I0630 02:12:38.858592 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 02:12:38.858604 29777 sgd_solver.cpp:106] Iteration 800, lr = 0.009975
I0630 02:12:54.849405 29777 solver.cpp:290] Iteration 900 (6.25377 iter/s, 15.9904s/100 iter), loss = 0.928571
I0630 02:12:54.849427 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 02:12:54.849436 29777 sgd_solver.cpp:106] Iteration 900, lr = 0.00997187
I0630 02:13:10.719367 29777 solver.cpp:354] Sparsity after update:
I0630 02:13:10.742584 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:13:10.742599 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 02:13:10.742614 29777 net.cpp:1851] conv1b_param_0(0) 
I0630 02:13:10.742617 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 02:13:10.742622 29777 net.cpp:1851] res2a_branch2a_param_0(0.00998) 
I0630 02:13:10.742626 29777 net.cpp:1851] res2a_branch2b_param_0(0.00998) 
I0630 02:13:10.742630 29777 net.cpp:1851] res3a_branch2a_param_0(0.01) 
I0630 02:13:10.742633 29777 net.cpp:1851] res3a_branch2b_param_0(0.00998) 
I0630 02:13:10.742636 29777 net.cpp:1851] res4a_branch2a_param_0(0.00999) 
I0630 02:13:10.742640 29777 net.cpp:1851] res4a_branch2b_param_0(0.00999) 
I0630 02:13:10.742642 29777 net.cpp:1851] res5a_branch2a_param_0(0.01) 
I0630 02:13:10.742645 29777 net.cpp:1851] res5a_branch2b_param_0(0.00997) 
I0630 02:13:10.742648 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (23474/2.86678e+06) 0.00819
I0630 02:13:10.898599 29777 solver.cpp:290] Iteration 1000 (6.23103 iter/s, 16.0487s/100 iter), loss = 1.55952
I0630 02:13:10.898624 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 02:13:10.898633 29777 sgd_solver.cpp:106] Iteration 1000, lr = 0.00996875
I0630 02:13:26.939337 29777 solver.cpp:290] Iteration 1100 (6.23431 iter/s, 16.0403s/100 iter), loss = 1.13095
I0630 02:13:26.939360 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 02:13:26.939366 29777 sgd_solver.cpp:106] Iteration 1100, lr = 0.00996562
I0630 02:13:42.892735 29777 solver.cpp:290] Iteration 1200 (6.26844 iter/s, 15.9529s/100 iter), loss = 1.34524
I0630 02:13:42.892846 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 02:13:42.892856 29777 sgd_solver.cpp:106] Iteration 1200, lr = 0.0099625
I0630 02:13:58.859582 29777 solver.cpp:290] Iteration 1300 (6.2632 iter/s, 15.9663s/100 iter), loss = 1.05952
I0630 02:13:58.859606 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 02:13:58.859612 29777 sgd_solver.cpp:106] Iteration 1300, lr = 0.00995938
I0630 02:14:14.896472 29777 solver.cpp:290] Iteration 1400 (6.23581 iter/s, 16.0364s/100 iter), loss = 0.857143
I0630 02:14:14.896580 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 02:14:14.896590 29777 sgd_solver.cpp:106] Iteration 1400, lr = 0.00995625
I0630 02:14:30.851013 29777 solver.cpp:290] Iteration 1500 (6.26802 iter/s, 15.954s/100 iter), loss = 1.42857
I0630 02:14:30.851037 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 02:14:30.851044 29777 sgd_solver.cpp:106] Iteration 1500, lr = 0.00995312
I0630 02:14:46.825736 29777 solver.cpp:290] Iteration 1600 (6.26007 iter/s, 15.9743s/100 iter), loss = 1.13095
I0630 02:14:46.825855 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 02:14:46.825865 29777 sgd_solver.cpp:106] Iteration 1600, lr = 0.00995
I0630 02:15:02.765061 29777 solver.cpp:290] Iteration 1700 (6.27401 iter/s, 15.9388s/100 iter), loss = 1.25
I0630 02:15:02.765085 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 02:15:02.765092 29777 sgd_solver.cpp:106] Iteration 1700, lr = 0.00994687
I0630 02:15:18.741606 29777 solver.cpp:290] Iteration 1800 (6.25936 iter/s, 15.9761s/100 iter), loss = 1.08333
I0630 02:15:18.741710 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 02:15:18.741719 29777 sgd_solver.cpp:106] Iteration 1800, lr = 0.00994375
I0630 02:15:34.634449 29777 solver.cpp:290] Iteration 1900 (6.29236 iter/s, 15.8923s/100 iter), loss = 1.5
I0630 02:15:34.634469 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 02:15:34.634476 29777 sgd_solver.cpp:106] Iteration 1900, lr = 0.00994062
I0630 02:15:50.467600 29777 solver.cpp:354] Sparsity after update:
I0630 02:15:50.468886 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:15:50.468894 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 02:15:50.468901 29777 net.cpp:1851] conv1b_param_0(0) 
I0630 02:15:50.468904 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 02:15:50.468905 29777 net.cpp:1851] res2a_branch2a_param_0(0.00998) 
I0630 02:15:50.468909 29777 net.cpp:1851] res2a_branch2b_param_0(0.00998) 
I0630 02:15:50.468910 29777 net.cpp:1851] res3a_branch2a_param_0(0.01) 
I0630 02:15:50.468912 29777 net.cpp:1851] res3a_branch2b_param_0(0.00998) 
I0630 02:15:50.468914 29777 net.cpp:1851] res4a_branch2a_param_0(0.00999) 
I0630 02:15:50.468916 29777 net.cpp:1851] res4a_branch2b_param_0(0.00999) 
I0630 02:15:50.468919 29777 net.cpp:1851] res5a_branch2a_param_0(0.01) 
I0630 02:15:50.468920 29777 net.cpp:1851] res5a_branch2b_param_0(0.00997) 
I0630 02:15:50.468922 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (23474/2.86678e+06) 0.00819
I0630 02:15:50.469015 29777 solver.cpp:471] Iteration 2000, Testing net (#0)
I0630 02:15:50.781328 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 02:16:40.065588 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.52926
I0630 02:16:40.065696 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.773981
I0630 02:16:40.065706 29777 solver.cpp:544]     Test net output #2: loss = 1.7109 (* 1 = 1.7109 loss)
I0630 02:16:40.240593 29777 solver.cpp:290] Iteration 2000 (1.52429 iter/s, 65.6043s/100 iter), loss = 1.08333
I0630 02:16:40.240619 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 02:16:40.240629 29777 sgd_solver.cpp:106] Iteration 2000, lr = 0.0099375
I0630 02:16:40.241605 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.02
I0630 02:16:40.355955 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 02:16:56.325366 29777 solver.cpp:290] Iteration 2100 (6.21725 iter/s, 16.0843s/100 iter), loss = 1.36905
I0630 02:16:56.325392 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 02:16:56.325399 29777 sgd_solver.cpp:106] Iteration 2100, lr = 0.00993438
I0630 02:17:12.309878 29777 solver.cpp:290] Iteration 2200 (6.25624 iter/s, 15.984s/100 iter), loss = 1.05952
I0630 02:17:12.309969 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 02:17:12.309980 29777 sgd_solver.cpp:106] Iteration 2200, lr = 0.00993125
I0630 02:17:28.276195 29777 solver.cpp:290] Iteration 2300 (6.2634 iter/s, 15.9658s/100 iter), loss = 0.857143
I0630 02:17:28.276219 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 02:17:28.276226 29777 sgd_solver.cpp:106] Iteration 2300, lr = 0.00992812
I0630 02:17:44.194790 29777 solver.cpp:290] Iteration 2400 (6.28215 iter/s, 15.9181s/100 iter), loss = 1.20238
I0630 02:17:44.194887 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 02:17:44.194898 29777 sgd_solver.cpp:106] Iteration 2400, lr = 0.009925
I0630 02:18:00.217103 29777 solver.cpp:290] Iteration 2500 (6.24151 iter/s, 16.0218s/100 iter), loss = 0.833333
I0630 02:18:00.217128 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 02:18:00.217134 29777 sgd_solver.cpp:106] Iteration 2500, lr = 0.00992187
I0630 02:18:16.209833 29777 solver.cpp:290] Iteration 2600 (6.25303 iter/s, 15.9923s/100 iter), loss = 1.15476
I0630 02:18:16.209926 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 02:18:16.209938 29777 sgd_solver.cpp:106] Iteration 2600, lr = 0.00991875
I0630 02:18:32.188616 29777 solver.cpp:290] Iteration 2700 (6.25851 iter/s, 15.9782s/100 iter), loss = 1.22619
I0630 02:18:32.188638 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 02:18:32.188645 29777 sgd_solver.cpp:106] Iteration 2700, lr = 0.00991562
I0630 02:18:48.185770 29777 solver.cpp:290] Iteration 2800 (6.25129 iter/s, 15.9967s/100 iter), loss = 0.97619
I0630 02:18:48.185865 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 02:18:48.185876 29777 sgd_solver.cpp:106] Iteration 2800, lr = 0.0099125
I0630 02:19:04.162981 29777 solver.cpp:290] Iteration 2900 (6.25913 iter/s, 15.9767s/100 iter), loss = 0.940476
I0630 02:19:04.163009 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 02:19:04.163018 29777 sgd_solver.cpp:106] Iteration 2900, lr = 0.00990937
I0630 02:19:20.046272 29777 solver.cpp:354] Sparsity after update:
I0630 02:19:20.066694 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:19:20.066707 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 02:19:20.066715 29777 net.cpp:1851] conv1b_param_0(0) 
I0630 02:19:20.066717 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 02:19:20.066720 29777 net.cpp:1851] res2a_branch2a_param_0(0.02) 
I0630 02:19:20.066722 29777 net.cpp:1851] res2a_branch2b_param_0(0.02) 
I0630 02:19:20.066725 29777 net.cpp:1851] res3a_branch2a_param_0(0.02) 
I0630 02:19:20.066726 29777 net.cpp:1851] res3a_branch2b_param_0(0.02) 
I0630 02:19:20.066728 29777 net.cpp:1851] res4a_branch2a_param_0(0.02) 
I0630 02:19:20.066730 29777 net.cpp:1851] res4a_branch2b_param_0(0.02) 
I0630 02:19:20.066732 29777 net.cpp:1851] res5a_branch2a_param_0(0.02) 
I0630 02:19:20.066735 29777 net.cpp:1851] res5a_branch2b_param_0(0.02) 
I0630 02:19:20.066736 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (46981/2.86678e+06) 0.0164
I0630 02:19:20.221180 29777 solver.cpp:290] Iteration 3000 (6.22753 iter/s, 16.0577s/100 iter), loss = 0.880952
I0630 02:19:20.221204 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 02:19:20.221215 29777 sgd_solver.cpp:106] Iteration 3000, lr = 0.00990625
I0630 02:19:36.279616 29777 solver.cpp:290] Iteration 3100 (6.22744 iter/s, 16.058s/100 iter), loss = 0.916667
I0630 02:19:36.279644 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 02:19:36.279652 29777 sgd_solver.cpp:106] Iteration 3100, lr = 0.00990312
I0630 02:19:52.220423 29777 solver.cpp:290] Iteration 3200 (6.27339 iter/s, 15.9403s/100 iter), loss = 1.20238
I0630 02:19:52.220530 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 02:19:52.220541 29777 sgd_solver.cpp:106] Iteration 3200, lr = 0.0099
I0630 02:20:08.236307 29777 solver.cpp:290] Iteration 3300 (6.24402 iter/s, 16.0153s/100 iter), loss = 1.17857
I0630 02:20:08.236332 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 02:20:08.236341 29777 sgd_solver.cpp:106] Iteration 3300, lr = 0.00989687
I0630 02:20:24.157477 29777 solver.cpp:290] Iteration 3400 (6.28113 iter/s, 15.9207s/100 iter), loss = 1.27381
I0630 02:20:24.157570 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 02:20:24.157582 29777 sgd_solver.cpp:106] Iteration 3400, lr = 0.00989375
I0630 02:20:40.178493 29777 solver.cpp:290] Iteration 3500 (6.24201 iter/s, 16.0205s/100 iter), loss = 0.785714
I0630 02:20:40.178520 29777 solver.cpp:309]     Train net output #0: loss = 0.47619 (* 1 = 0.47619 loss)
I0630 02:20:40.178529 29777 sgd_solver.cpp:106] Iteration 3500, lr = 0.00989062
I0630 02:20:56.164333 29777 solver.cpp:290] Iteration 3600 (6.25572 iter/s, 15.9854s/100 iter), loss = 1.28571
I0630 02:20:56.164425 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 02:20:56.164436 29777 sgd_solver.cpp:106] Iteration 3600, lr = 0.0098875
I0630 02:21:12.185576 29777 solver.cpp:290] Iteration 3700 (6.24192 iter/s, 16.0207s/100 iter), loss = 0.988095
I0630 02:21:12.185602 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 02:21:12.185617 29777 sgd_solver.cpp:106] Iteration 3700, lr = 0.00988437
I0630 02:21:28.191432 29777 solver.cpp:290] Iteration 3800 (6.2479 iter/s, 16.0054s/100 iter), loss = 1.21429
I0630 02:21:28.191519 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 02:21:28.191527 29777 sgd_solver.cpp:106] Iteration 3800, lr = 0.00988125
I0630 02:21:44.240229 29777 solver.cpp:290] Iteration 3900 (6.2312 iter/s, 16.0483s/100 iter), loss = 1.25
I0630 02:21:44.240252 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 02:21:44.240259 29777 sgd_solver.cpp:106] Iteration 3900, lr = 0.00987813
I0630 02:22:00.012027 29777 solver.cpp:354] Sparsity after update:
I0630 02:22:00.013463 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:22:00.013469 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 02:22:00.013476 29777 net.cpp:1851] conv1b_param_0(0) 
I0630 02:22:00.013479 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 02:22:00.013481 29777 net.cpp:1851] res2a_branch2a_param_0(0.02) 
I0630 02:22:00.013484 29777 net.cpp:1851] res2a_branch2b_param_0(0.02) 
I0630 02:22:00.013486 29777 net.cpp:1851] res3a_branch2a_param_0(0.02) 
I0630 02:22:00.013489 29777 net.cpp:1851] res3a_branch2b_param_0(0.02) 
I0630 02:22:00.013491 29777 net.cpp:1851] res4a_branch2a_param_0(0.02) 
I0630 02:22:00.013494 29777 net.cpp:1851] res4a_branch2b_param_0(0.02) 
I0630 02:22:00.013496 29777 net.cpp:1851] res5a_branch2a_param_0(0.02) 
I0630 02:22:00.013499 29777 net.cpp:1851] res5a_branch2b_param_0(0.02) 
I0630 02:22:00.013500 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (46981/2.86678e+06) 0.0164
I0630 02:22:00.013589 29777 solver.cpp:471] Iteration 4000, Testing net (#0)
I0630 02:22:00.512575 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 02:22:48.904451 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.547739
I0630 02:22:48.904580 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.787981
I0630 02:22:48.904590 29777 solver.cpp:544]     Test net output #2: loss = 1.63662 (* 1 = 1.63662 loss)
I0630 02:22:49.083796 29777 solver.cpp:290] Iteration 4000 (1.54222 iter/s, 64.8418s/100 iter), loss = 1.17857
I0630 02:22:49.083822 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 02:22:49.083868 29777 sgd_solver.cpp:106] Iteration 4000, lr = 0.009875
I0630 02:22:49.084617 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.03
I0630 02:22:49.198292 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 02:23:05.181386 29777 solver.cpp:290] Iteration 4100 (6.21229 iter/s, 16.0971s/100 iter), loss = 0.785714
I0630 02:23:05.181409 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 02:23:05.181417 29777 sgd_solver.cpp:106] Iteration 4100, lr = 0.00987187
I0630 02:23:21.171454 29777 solver.cpp:290] Iteration 4200 (6.25407 iter/s, 15.9896s/100 iter), loss = 1.5
I0630 02:23:21.171546 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 02:23:21.171557 29777 sgd_solver.cpp:106] Iteration 4200, lr = 0.00986875
I0630 02:23:37.105128 29777 solver.cpp:290] Iteration 4300 (6.27623 iter/s, 15.9331s/100 iter), loss = 0.904762
I0630 02:23:37.105150 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 02:23:37.105157 29777 sgd_solver.cpp:106] Iteration 4300, lr = 0.00986562
I0630 02:23:53.059149 29777 solver.cpp:290] Iteration 4400 (6.2682 iter/s, 15.9536s/100 iter), loss = 1
I0630 02:23:53.059250 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 02:23:53.059262 29777 sgd_solver.cpp:106] Iteration 4400, lr = 0.0098625
I0630 02:24:09.100235 29777 solver.cpp:290] Iteration 4500 (6.2342 iter/s, 16.0405s/100 iter), loss = 1.02381
I0630 02:24:09.100261 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 02:24:09.100270 29777 sgd_solver.cpp:106] Iteration 4500, lr = 0.00985937
I0630 02:24:25.076942 29777 solver.cpp:290] Iteration 4600 (6.2593 iter/s, 15.9762s/100 iter), loss = 0.940476
I0630 02:24:25.077003 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 02:24:25.077010 29777 sgd_solver.cpp:106] Iteration 4600, lr = 0.00985625
I0630 02:24:41.100198 29777 solver.cpp:290] Iteration 4700 (6.24113 iter/s, 16.0228s/100 iter), loss = 1.25
I0630 02:24:41.100220 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 02:24:41.100227 29777 sgd_solver.cpp:106] Iteration 4700, lr = 0.00985312
I0630 02:24:57.122639 29777 solver.cpp:290] Iteration 4800 (6.24143 iter/s, 16.022s/100 iter), loss = 1.04762
I0630 02:24:57.122731 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 02:24:57.122743 29777 sgd_solver.cpp:106] Iteration 4800, lr = 0.00985
I0630 02:25:13.071058 29777 solver.cpp:290] Iteration 4900 (6.27042 iter/s, 15.9479s/100 iter), loss = 1.15476
I0630 02:25:13.071081 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 02:25:13.071089 29777 sgd_solver.cpp:106] Iteration 4900, lr = 0.00984687
I0630 02:25:28.873311 29777 solver.cpp:354] Sparsity after update:
I0630 02:25:28.893715 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:25:28.893752 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 02:25:28.893769 29777 net.cpp:1851] conv1b_param_0(0) 
I0630 02:25:28.893777 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 02:25:28.893784 29777 net.cpp:1851] res2a_branch2a_param_0(0.0299) 
I0630 02:25:28.893793 29777 net.cpp:1851] res2a_branch2b_param_0(0.0299) 
I0630 02:25:28.893801 29777 net.cpp:1851] res3a_branch2a_param_0(0.03) 
I0630 02:25:28.893810 29777 net.cpp:1851] res3a_branch2b_param_0(0.03) 
I0630 02:25:28.893816 29777 net.cpp:1851] res4a_branch2a_param_0(0.03) 
I0630 02:25:28.893824 29777 net.cpp:1851] res4a_branch2b_param_0(0.03) 
I0630 02:25:28.893831 29777 net.cpp:1851] res5a_branch2a_param_0(0.03) 
I0630 02:25:28.893839 29777 net.cpp:1851] res5a_branch2b_param_0(0.03) 
I0630 02:25:28.893847 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (70472/2.86678e+06) 0.0246
I0630 02:25:29.055929 29777 solver.cpp:290] Iteration 5000 (6.2561 iter/s, 15.9844s/100 iter), loss = 0.952381
I0630 02:25:29.055953 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 02:25:29.055959 29777 sgd_solver.cpp:106] Iteration 5000, lr = 0.00984375
I0630 02:25:45.001849 29777 solver.cpp:290] Iteration 5100 (6.27138 iter/s, 15.9454s/100 iter), loss = 1.07143
I0630 02:25:45.001869 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 02:25:45.001876 29777 sgd_solver.cpp:106] Iteration 5100, lr = 0.00984062
I0630 02:26:00.990571 29777 solver.cpp:290] Iteration 5200 (6.25459 iter/s, 15.9883s/100 iter), loss = 1.19048
I0630 02:26:00.990648 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 02:26:00.990656 29777 sgd_solver.cpp:106] Iteration 5200, lr = 0.0098375
I0630 02:26:17.011343 29777 solver.cpp:290] Iteration 5300 (6.2421 iter/s, 16.0202s/100 iter), loss = 0.678571
I0630 02:26:17.011368 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 02:26:17.011374 29777 sgd_solver.cpp:106] Iteration 5300, lr = 0.00983437
I0630 02:26:33.056402 29777 solver.cpp:290] Iteration 5400 (6.23263 iter/s, 16.0446s/100 iter), loss = 1.2619
I0630 02:26:33.056509 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 02:26:33.056524 29777 sgd_solver.cpp:106] Iteration 5400, lr = 0.00983125
I0630 02:26:49.064081 29777 solver.cpp:290] Iteration 5500 (6.24722 iter/s, 16.0071s/100 iter), loss = 1.41667
I0630 02:26:49.064110 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 02:26:49.064119 29777 sgd_solver.cpp:106] Iteration 5500, lr = 0.00982813
I0630 02:27:05.011070 29777 solver.cpp:290] Iteration 5600 (6.27096 iter/s, 15.9465s/100 iter), loss = 1.21429
I0630 02:27:05.011160 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 02:27:05.011170 29777 sgd_solver.cpp:106] Iteration 5600, lr = 0.009825
I0630 02:27:20.998875 29777 solver.cpp:290] Iteration 5700 (6.25498 iter/s, 15.9873s/100 iter), loss = 0.833333
I0630 02:27:20.998899 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 02:27:20.998908 29777 sgd_solver.cpp:106] Iteration 5700, lr = 0.00982188
I0630 02:27:37.005162 29777 solver.cpp:290] Iteration 5800 (6.24773 iter/s, 16.0058s/100 iter), loss = 1.09524
I0630 02:27:37.005244 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 02:27:37.005255 29777 sgd_solver.cpp:106] Iteration 5800, lr = 0.00981875
I0630 02:27:53.079452 29777 solver.cpp:290] Iteration 5900 (6.22132 iter/s, 16.0738s/100 iter), loss = 1.10714
I0630 02:27:53.079476 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 02:27:53.079485 29777 sgd_solver.cpp:106] Iteration 5900, lr = 0.00981562
I0630 02:28:08.850757 29777 solver.cpp:354] Sparsity after update:
I0630 02:28:08.852033 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:28:08.852042 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 02:28:08.852048 29777 net.cpp:1851] conv1b_param_0(0) 
I0630 02:28:08.852051 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 02:28:08.852053 29777 net.cpp:1851] res2a_branch2a_param_0(0.0299) 
I0630 02:28:08.852056 29777 net.cpp:1851] res2a_branch2b_param_0(0.0299) 
I0630 02:28:08.852058 29777 net.cpp:1851] res3a_branch2a_param_0(0.03) 
I0630 02:28:08.852061 29777 net.cpp:1851] res3a_branch2b_param_0(0.03) 
I0630 02:28:08.852063 29777 net.cpp:1851] res4a_branch2a_param_0(0.03) 
I0630 02:28:08.852066 29777 net.cpp:1851] res4a_branch2b_param_0(0.03) 
I0630 02:28:08.852068 29777 net.cpp:1851] res5a_branch2a_param_0(0.03) 
I0630 02:28:08.852071 29777 net.cpp:1851] res5a_branch2b_param_0(0.03) 
I0630 02:28:08.852073 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (70472/2.86678e+06) 0.0246
I0630 02:28:08.852170 29777 solver.cpp:471] Iteration 6000, Testing net (#0)
I0630 02:28:09.528820 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 02:28:57.098083 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5525
I0630 02:28:57.098186 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.792661
I0630 02:28:57.098194 29777 solver.cpp:544]     Test net output #2: loss = 1.599 (* 1 = 1.599 loss)
I0630 02:28:57.273607 29777 solver.cpp:290] Iteration 6000 (1.55782 iter/s, 64.1923s/100 iter), loss = 1.21429
I0630 02:28:57.273628 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 02:28:57.273635 29777 sgd_solver.cpp:106] Iteration 6000, lr = 0.0098125
I0630 02:28:57.274338 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.04
I0630 02:28:57.384799 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 02:29:13.441776 29777 solver.cpp:290] Iteration 6100 (6.18518 iter/s, 16.1677s/100 iter), loss = 1.41667
I0630 02:29:13.441798 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 02:29:13.441807 29777 sgd_solver.cpp:106] Iteration 6100, lr = 0.00980937
I0630 02:29:29.520517 29777 solver.cpp:290] Iteration 6200 (6.21958 iter/s, 16.0783s/100 iter), loss = 1.13095
I0630 02:29:29.520596 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 02:29:29.520606 29777 sgd_solver.cpp:106] Iteration 6200, lr = 0.00980625
I0630 02:29:45.548202 29777 solver.cpp:290] Iteration 6300 (6.23941 iter/s, 16.0272s/100 iter), loss = 1.10714
I0630 02:29:45.548228 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 02:29:45.548236 29777 sgd_solver.cpp:106] Iteration 6300, lr = 0.00980312
I0630 02:30:01.565382 29777 solver.cpp:290] Iteration 6400 (6.24348 iter/s, 16.0167s/100 iter), loss = 1.09524
I0630 02:30:01.565477 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 02:30:01.565488 29777 sgd_solver.cpp:106] Iteration 6400, lr = 0.0098
I0630 02:30:17.646778 29777 solver.cpp:290] Iteration 6500 (6.21858 iter/s, 16.0809s/100 iter), loss = 1.04762
I0630 02:30:17.646802 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 02:30:17.646808 29777 sgd_solver.cpp:106] Iteration 6500, lr = 0.00979687
I0630 02:30:33.646033 29777 solver.cpp:290] Iteration 6600 (6.25048 iter/s, 15.9988s/100 iter), loss = 1.2619
I0630 02:30:33.646152 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 02:30:33.646162 29777 sgd_solver.cpp:106] Iteration 6600, lr = 0.00979375
I0630 02:30:49.623831 29777 solver.cpp:290] Iteration 6700 (6.25891 iter/s, 15.9772s/100 iter), loss = 1.21429
I0630 02:30:49.623857 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 02:30:49.623867 29777 sgd_solver.cpp:106] Iteration 6700, lr = 0.00979062
I0630 02:31:05.495962 29777 solver.cpp:290] Iteration 6800 (6.30054 iter/s, 15.8717s/100 iter), loss = 1.21429
I0630 02:31:05.496037 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 02:31:05.496044 29777 sgd_solver.cpp:106] Iteration 6800, lr = 0.0097875
I0630 02:31:21.468224 29777 solver.cpp:290] Iteration 6900 (6.26106 iter/s, 15.9717s/100 iter), loss = 1.11905
I0630 02:31:21.468251 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 02:31:21.468261 29777 sgd_solver.cpp:106] Iteration 6900, lr = 0.00978437
I0630 02:31:37.292667 29777 solver.cpp:354] Sparsity after update:
I0630 02:31:37.313056 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:31:37.313092 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 02:31:37.313112 29777 net.cpp:1851] conv1b_param_0(0) 
I0630 02:31:37.313123 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 02:31:37.313136 29777 net.cpp:1851] res2a_branch2a_param_0(0.0399) 
I0630 02:31:37.313144 29777 net.cpp:1851] res2a_branch2b_param_0(0.0399) 
I0630 02:31:37.313150 29777 net.cpp:1851] res3a_branch2a_param_0(0.04) 
I0630 02:31:37.313154 29777 net.cpp:1851] res3a_branch2b_param_0(0.04) 
I0630 02:31:37.313158 29777 net.cpp:1851] res4a_branch2a_param_0(0.04) 
I0630 02:31:37.313160 29777 net.cpp:1851] res4a_branch2b_param_0(0.04) 
I0630 02:31:37.313165 29777 net.cpp:1851] res5a_branch2a_param_0(0.04) 
I0630 02:31:37.313169 29777 net.cpp:1851] res5a_branch2b_param_0(0.04) 
I0630 02:31:37.313172 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (93969/2.86678e+06) 0.0328
I0630 02:31:37.469810 29777 solver.cpp:290] Iteration 7000 (6.24957 iter/s, 16.0011s/100 iter), loss = 0.809524
I0630 02:31:37.469837 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 02:31:37.469846 29777 sgd_solver.cpp:106] Iteration 7000, lr = 0.00978125
I0630 02:31:53.478783 29777 solver.cpp:290] Iteration 7100 (6.24668 iter/s, 16.0085s/100 iter), loss = 1.13095
I0630 02:31:53.478842 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 02:31:53.478852 29777 sgd_solver.cpp:106] Iteration 7100, lr = 0.00977813
I0630 02:32:09.499135 29777 solver.cpp:290] Iteration 7200 (6.24226 iter/s, 16.0198s/100 iter), loss = 1.14286
I0630 02:32:09.502990 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 02:32:09.503069 29777 sgd_solver.cpp:106] Iteration 7200, lr = 0.009775
I0630 02:32:25.462734 29777 solver.cpp:290] Iteration 7300 (6.26594 iter/s, 15.9593s/100 iter), loss = 1.29762
I0630 02:32:25.462759 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 02:32:25.462767 29777 sgd_solver.cpp:106] Iteration 7300, lr = 0.00977188
I0630 02:32:41.431506 29777 solver.cpp:290] Iteration 7400 (6.26241 iter/s, 15.9683s/100 iter), loss = 1.04762
I0630 02:32:41.431576 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 02:32:41.431596 29777 sgd_solver.cpp:106] Iteration 7400, lr = 0.00976875
I0630 02:32:57.480398 29777 solver.cpp:290] Iteration 7500 (6.23116 iter/s, 16.0484s/100 iter), loss = 1.34524
I0630 02:32:57.480424 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 02:32:57.480434 29777 sgd_solver.cpp:106] Iteration 7500, lr = 0.00976562
I0630 02:33:13.519101 29777 solver.cpp:290] Iteration 7600 (6.2351 iter/s, 16.0382s/100 iter), loss = 1.22619
I0630 02:33:13.519166 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 02:33:13.519174 29777 sgd_solver.cpp:106] Iteration 7600, lr = 0.0097625
I0630 02:33:29.435500 29777 solver.cpp:290] Iteration 7700 (6.28303 iter/s, 15.9159s/100 iter), loss = 1.22619
I0630 02:33:29.435524 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 02:33:29.435531 29777 sgd_solver.cpp:106] Iteration 7700, lr = 0.00975937
I0630 02:33:45.485832 29777 solver.cpp:290] Iteration 7800 (6.23059 iter/s, 16.0499s/100 iter), loss = 1.45238
I0630 02:33:45.485929 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 02:33:45.485939 29777 sgd_solver.cpp:106] Iteration 7800, lr = 0.00975625
I0630 02:34:01.404512 29777 solver.cpp:290] Iteration 7900 (6.28214 iter/s, 15.9181s/100 iter), loss = 1.25
I0630 02:34:01.404538 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 02:34:01.404547 29777 sgd_solver.cpp:106] Iteration 7900, lr = 0.00975312
I0630 02:34:17.318284 29777 solver.cpp:354] Sparsity after update:
I0630 02:34:17.319923 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:34:17.319931 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 02:34:17.319938 29777 net.cpp:1851] conv1b_param_0(0) 
I0630 02:34:17.319941 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 02:34:17.319942 29777 net.cpp:1851] res2a_branch2a_param_0(0.0399) 
I0630 02:34:17.319946 29777 net.cpp:1851] res2a_branch2b_param_0(0.0399) 
I0630 02:34:17.319947 29777 net.cpp:1851] res3a_branch2a_param_0(0.04) 
I0630 02:34:17.319949 29777 net.cpp:1851] res3a_branch2b_param_0(0.04) 
I0630 02:34:17.319952 29777 net.cpp:1851] res4a_branch2a_param_0(0.04) 
I0630 02:34:17.319953 29777 net.cpp:1851] res4a_branch2b_param_0(0.04) 
I0630 02:34:17.319955 29777 net.cpp:1851] res5a_branch2a_param_0(0.04) 
I0630 02:34:17.319957 29777 net.cpp:1851] res5a_branch2b_param_0(0.04) 
I0630 02:34:17.319959 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (93969/2.86678e+06) 0.0328
I0630 02:34:17.320046 29777 solver.cpp:471] Iteration 8000, Testing net (#0)
I0630 02:34:18.189494 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 02:35:05.951143 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54464
I0630 02:35:05.951251 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.784161
I0630 02:35:05.951261 29777 solver.cpp:544]     Test net output #2: loss = 1.64854 (* 1 = 1.64854 loss)
I0630 02:35:06.129494 29777 solver.cpp:290] Iteration 8000 (1.54504 iter/s, 64.7232s/100 iter), loss = 1.02381
I0630 02:35:06.129520 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 02:35:06.129529 29777 sgd_solver.cpp:106] Iteration 8000, lr = 0.00975
I0630 02:35:06.130506 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.05
I0630 02:35:06.245388 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 02:35:22.352187 29777 solver.cpp:290] Iteration 8100 (6.16439 iter/s, 16.2222s/100 iter), loss = 1.45238
I0630 02:35:22.352210 29777 solver.cpp:309]     Train net output #0: loss = 1.71429 (* 1 = 1.71429 loss)
I0630 02:35:22.352217 29777 sgd_solver.cpp:106] Iteration 8100, lr = 0.00974687
I0630 02:35:38.291359 29777 solver.cpp:290] Iteration 8200 (6.27404 iter/s, 15.9387s/100 iter), loss = 1.2619
I0630 02:35:38.291465 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 02:35:38.291476 29777 sgd_solver.cpp:106] Iteration 8200, lr = 0.00974375
I0630 02:35:54.320405 29777 solver.cpp:290] Iteration 8300 (6.23889 iter/s, 16.0285s/100 iter), loss = 1.17857
I0630 02:35:54.320430 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 02:35:54.320439 29777 sgd_solver.cpp:106] Iteration 8300, lr = 0.00974062
I0630 02:36:10.296932 29777 solver.cpp:290] Iteration 8400 (6.25937 iter/s, 15.976s/100 iter), loss = 1.07143
I0630 02:36:10.297029 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 02:36:10.297044 29777 sgd_solver.cpp:106] Iteration 8400, lr = 0.0097375
I0630 02:36:26.363386 29777 solver.cpp:290] Iteration 8500 (6.22436 iter/s, 16.0659s/100 iter), loss = 1.09524
I0630 02:36:26.363411 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 02:36:26.363420 29777 sgd_solver.cpp:106] Iteration 8500, lr = 0.00973437
I0630 02:36:42.380946 29777 solver.cpp:290] Iteration 8600 (6.24333 iter/s, 16.0171s/100 iter), loss = 1.0119
I0630 02:36:42.381039 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 02:36:42.381050 29777 sgd_solver.cpp:106] Iteration 8600, lr = 0.00973125
I0630 02:36:58.413369 29777 solver.cpp:290] Iteration 8700 (6.23757 iter/s, 16.0319s/100 iter), loss = 1.13095
I0630 02:36:58.413395 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 02:36:58.413403 29777 sgd_solver.cpp:106] Iteration 8700, lr = 0.00972812
I0630 02:37:14.423337 29777 solver.cpp:290] Iteration 8800 (6.24629 iter/s, 16.0095s/100 iter), loss = 1.21429
I0630 02:37:14.423424 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 02:37:14.423435 29777 sgd_solver.cpp:106] Iteration 8800, lr = 0.009725
I0630 02:37:30.398186 29777 solver.cpp:290] Iteration 8900 (6.26005 iter/s, 15.9743s/100 iter), loss = 0.940476
I0630 02:37:30.398211 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 02:37:30.398217 29777 sgd_solver.cpp:106] Iteration 8900, lr = 0.00972188
I0630 02:37:46.299845 29777 solver.cpp:354] Sparsity after update:
I0630 02:37:46.320047 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:37:46.320061 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 02:37:46.320072 29777 net.cpp:1851] conv1b_param_0(0) 
I0630 02:37:46.320076 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 02:37:46.320080 29777 net.cpp:1851] res2a_branch2a_param_0(0.05) 
I0630 02:37:46.320086 29777 net.cpp:1851] res2a_branch2b_param_0(0.0499) 
I0630 02:37:46.320088 29777 net.cpp:1851] res3a_branch2a_param_0(0.05) 
I0630 02:37:46.320091 29777 net.cpp:1851] res3a_branch2b_param_0(0.05) 
I0630 02:37:46.320094 29777 net.cpp:1851] res4a_branch2a_param_0(0.05) 
I0630 02:37:46.320098 29777 net.cpp:1851] res4a_branch2b_param_0(0.05) 
I0630 02:37:46.320109 29777 net.cpp:1851] res5a_branch2a_param_0(0.05) 
I0630 02:37:46.320116 29777 net.cpp:1851] res5a_branch2b_param_0(0.05) 
I0630 02:37:46.320121 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (117484/2.86678e+06) 0.041
I0630 02:37:46.475716 29777 solver.cpp:290] Iteration 9000 (6.22005 iter/s, 16.0771s/100 iter), loss = 1.27381
I0630 02:37:46.475742 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 02:37:46.475750 29777 sgd_solver.cpp:106] Iteration 9000, lr = 0.00971875
I0630 02:38:02.395467 29777 solver.cpp:290] Iteration 9100 (6.28169 iter/s, 15.9193s/100 iter), loss = 0.809524
I0630 02:38:02.395490 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 02:38:02.395498 29777 sgd_solver.cpp:106] Iteration 9100, lr = 0.00971563
I0630 02:38:18.391331 29777 solver.cpp:290] Iteration 9200 (6.2518 iter/s, 15.9954s/100 iter), loss = 1.10714
I0630 02:38:18.391435 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 02:38:18.391445 29777 sgd_solver.cpp:106] Iteration 9200, lr = 0.0097125
I0630 02:38:34.332414 29777 solver.cpp:290] Iteration 9300 (6.27331 iter/s, 15.9405s/100 iter), loss = 1.02381
I0630 02:38:34.332437 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 02:38:34.332443 29777 sgd_solver.cpp:106] Iteration 9300, lr = 0.00970937
I0630 02:38:50.290498 29777 solver.cpp:290] Iteration 9400 (6.2666 iter/s, 15.9576s/100 iter), loss = 1.55952
I0630 02:38:50.290566 29777 solver.cpp:309]     Train net output #0: loss = 1.92857 (* 1 = 1.92857 loss)
I0630 02:38:50.290575 29777 sgd_solver.cpp:106] Iteration 9400, lr = 0.00970625
I0630 02:39:06.416635 29777 solver.cpp:290] Iteration 9500 (6.20131 iter/s, 16.1256s/100 iter), loss = 1.2619
I0630 02:39:06.416661 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 02:39:06.416671 29777 sgd_solver.cpp:106] Iteration 9500, lr = 0.00970312
I0630 02:39:22.480839 29777 solver.cpp:290] Iteration 9600 (6.22521 iter/s, 16.0637s/100 iter), loss = 1.44048
I0630 02:39:22.480949 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 02:39:22.480959 29777 sgd_solver.cpp:106] Iteration 9600, lr = 0.0097
I0630 02:39:38.524018 29777 solver.cpp:290] Iteration 9700 (6.2334 iter/s, 16.0426s/100 iter), loss = 0.77381
I0630 02:39:38.524044 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 02:39:38.524054 29777 sgd_solver.cpp:106] Iteration 9700, lr = 0.00969687
I0630 02:39:54.471120 29777 solver.cpp:290] Iteration 9800 (6.27092 iter/s, 15.9466s/100 iter), loss = 1.17857
I0630 02:39:54.471195 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 02:39:54.471206 29777 sgd_solver.cpp:106] Iteration 9800, lr = 0.00969375
I0630 02:40:10.449753 29777 solver.cpp:290] Iteration 9900 (6.25856 iter/s, 15.9781s/100 iter), loss = 1.10714
I0630 02:40:10.449775 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 02:40:10.449781 29777 sgd_solver.cpp:106] Iteration 9900, lr = 0.00969062
I0630 02:40:26.297960 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_10000.caffemodel
I0630 02:40:26.386431 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_10000.solverstate
I0630 02:40:26.394994 29777 solver.cpp:354] Sparsity after update:
I0630 02:40:26.395985 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:40:26.395994 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 02:40:26.396005 29777 net.cpp:1851] conv1b_param_0(0) 
I0630 02:40:26.396010 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 02:40:26.396013 29777 net.cpp:1851] res2a_branch2a_param_0(0.05) 
I0630 02:40:26.396018 29777 net.cpp:1851] res2a_branch2b_param_0(0.0499) 
I0630 02:40:26.396023 29777 net.cpp:1851] res3a_branch2a_param_0(0.05) 
I0630 02:40:26.396028 29777 net.cpp:1851] res3a_branch2b_param_0(0.05) 
I0630 02:40:26.396030 29777 net.cpp:1851] res4a_branch2a_param_0(0.05) 
I0630 02:40:26.396035 29777 net.cpp:1851] res4a_branch2b_param_0(0.05) 
I0630 02:40:26.396040 29777 net.cpp:1851] res5a_branch2a_param_0(0.05) 
I0630 02:40:26.396044 29777 net.cpp:1851] res5a_branch2b_param_0(0.05) 
I0630 02:40:26.396049 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (117484/2.86678e+06) 0.041
I0630 02:40:26.396150 29777 solver.cpp:471] Iteration 10000, Testing net (#0)
I0630 02:40:27.451498 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 02:41:15.115475 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.53622
I0630 02:41:15.115526 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.7789
I0630 02:41:15.115533 29777 solver.cpp:544]     Test net output #2: loss = 1.70066 (* 1 = 1.70066 loss)
I0630 02:41:15.303112 29777 solver.cpp:290] Iteration 10000 (1.54198 iter/s, 64.8515s/100 iter), loss = 0.892857
I0630 02:41:15.303135 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 02:41:15.303141 29777 sgd_solver.cpp:106] Iteration 10000, lr = 0.0096875
I0630 02:41:15.303835 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.06
I0630 02:41:15.415892 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 02:41:31.358265 29777 solver.cpp:290] Iteration 10100 (6.22871 iter/s, 16.0547s/100 iter), loss = 1.0119
I0630 02:41:31.358289 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 02:41:31.358296 29777 sgd_solver.cpp:106] Iteration 10100, lr = 0.00968437
I0630 02:41:47.327224 29777 solver.cpp:290] Iteration 10200 (6.26233 iter/s, 15.9685s/100 iter), loss = 1.25
I0630 02:41:47.327327 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 02:41:47.327334 29777 sgd_solver.cpp:106] Iteration 10200, lr = 0.00968125
I0630 02:42:03.345801 29777 solver.cpp:290] Iteration 10300 (6.24297 iter/s, 16.018s/100 iter), loss = 1.27381
I0630 02:42:03.345824 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 02:42:03.345830 29777 sgd_solver.cpp:106] Iteration 10300, lr = 0.00967812
I0630 02:42:19.352821 29777 solver.cpp:290] Iteration 10400 (6.24744 iter/s, 16.0065s/100 iter), loss = 0.952381
I0630 02:42:19.352928 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 02:42:19.352942 29777 sgd_solver.cpp:106] Iteration 10400, lr = 0.009675
I0630 02:42:35.309152 29777 solver.cpp:290] Iteration 10500 (6.26732 iter/s, 15.9558s/100 iter), loss = 1.22619
I0630 02:42:35.309175 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 02:42:35.309193 29777 sgd_solver.cpp:106] Iteration 10500, lr = 0.00967188
I0630 02:42:51.239629 29777 solver.cpp:290] Iteration 10600 (6.27746 iter/s, 15.93s/100 iter), loss = 1.03571
I0630 02:42:51.239701 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 02:42:51.239712 29777 sgd_solver.cpp:106] Iteration 10600, lr = 0.00966875
I0630 02:43:07.200438 29777 solver.cpp:290] Iteration 10700 (6.26555 iter/s, 15.9603s/100 iter), loss = 1.72619
I0630 02:43:07.200460 29777 solver.cpp:309]     Train net output #0: loss = 1.88095 (* 1 = 1.88095 loss)
I0630 02:43:07.200467 29777 sgd_solver.cpp:106] Iteration 10700, lr = 0.00966563
I0630 02:43:23.207172 29777 solver.cpp:290] Iteration 10800 (6.24755 iter/s, 16.0063s/100 iter), loss = 1.40476
I0630 02:43:23.207279 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 02:43:23.207289 29777 sgd_solver.cpp:106] Iteration 10800, lr = 0.0096625
I0630 02:43:39.179239 29777 solver.cpp:290] Iteration 10900 (6.26115 iter/s, 15.9715s/100 iter), loss = 1.13095
I0630 02:43:39.179261 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 02:43:39.179267 29777 sgd_solver.cpp:106] Iteration 10900, lr = 0.00965938
I0630 02:43:55.115053 29777 solver.cpp:354] Sparsity after update:
I0630 02:43:55.135666 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:43:55.135711 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 02:43:55.135728 29777 net.cpp:1851] conv1b_param_0(0) 
I0630 02:43:55.135736 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 02:43:55.135745 29777 net.cpp:1851] res2a_branch2a_param_0(0.06) 
I0630 02:43:55.135754 29777 net.cpp:1851] res2a_branch2b_param_0(0.0599) 
I0630 02:43:55.135762 29777 net.cpp:1851] res3a_branch2a_param_0(0.06) 
I0630 02:43:55.135771 29777 net.cpp:1851] res3a_branch2b_param_0(0.06) 
I0630 02:43:55.135779 29777 net.cpp:1851] res4a_branch2a_param_0(0.06) 
I0630 02:43:55.135788 29777 net.cpp:1851] res4a_branch2b_param_0(0.06) 
I0630 02:43:55.135797 29777 net.cpp:1851] res5a_branch2a_param_0(0.06) 
I0630 02:43:55.135804 29777 net.cpp:1851] res5a_branch2b_param_0(0.06) 
I0630 02:43:55.135812 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (140986/2.86678e+06) 0.0492
I0630 02:43:55.290863 29777 solver.cpp:290] Iteration 11000 (6.20688 iter/s, 16.1111s/100 iter), loss = 1.5
I0630 02:43:55.291105 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 02:43:55.291234 29777 sgd_solver.cpp:106] Iteration 11000, lr = 0.00965625
I0630 02:44:11.375176 29777 solver.cpp:290] Iteration 11100 (6.2175 iter/s, 16.0836s/100 iter), loss = 1.2381
I0630 02:44:11.375200 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 02:44:11.375206 29777 sgd_solver.cpp:106] Iteration 11100, lr = 0.00965312
I0630 02:44:27.391824 29777 solver.cpp:290] Iteration 11200 (6.24369 iter/s, 16.0162s/100 iter), loss = 0.964286
I0630 02:44:27.391890 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 02:44:27.391901 29777 sgd_solver.cpp:106] Iteration 11200, lr = 0.00965
I0630 02:44:43.482599 29777 solver.cpp:290] Iteration 11300 (6.21494 iter/s, 16.0903s/100 iter), loss = 1.30952
I0630 02:44:43.482621 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 02:44:43.482630 29777 sgd_solver.cpp:106] Iteration 11300, lr = 0.00964687
I0630 02:44:59.644758 29777 solver.cpp:290] Iteration 11400 (6.18747 iter/s, 16.1617s/100 iter), loss = 0.892857
I0630 02:44:59.644870 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 02:44:59.644881 29777 sgd_solver.cpp:106] Iteration 11400, lr = 0.00964375
I0630 02:45:15.717372 29777 solver.cpp:290] Iteration 11500 (6.22198 iter/s, 16.0721s/100 iter), loss = 1.16667
I0630 02:45:15.717397 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 02:45:15.717407 29777 sgd_solver.cpp:106] Iteration 11500, lr = 0.00964062
I0630 02:45:31.818399 29777 solver.cpp:290] Iteration 11600 (6.21096 iter/s, 16.1006s/100 iter), loss = 0.988095
I0630 02:45:31.818473 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 02:45:31.818482 29777 sgd_solver.cpp:106] Iteration 11600, lr = 0.0096375
I0630 02:45:47.922749 29777 solver.cpp:290] Iteration 11700 (6.2097 iter/s, 16.1038s/100 iter), loss = 1.30952
I0630 02:45:47.922777 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 02:45:47.922791 29777 sgd_solver.cpp:106] Iteration 11700, lr = 0.00963437
I0630 02:46:03.947202 29777 solver.cpp:290] Iteration 11800 (6.24065 iter/s, 16.024s/100 iter), loss = 1.36905
I0630 02:46:03.947283 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 02:46:03.947294 29777 sgd_solver.cpp:106] Iteration 11800, lr = 0.00963125
I0630 02:46:19.927386 29777 solver.cpp:290] Iteration 11900 (6.25795 iter/s, 15.9797s/100 iter), loss = 0.857143
I0630 02:46:19.927409 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 02:46:19.927415 29777 sgd_solver.cpp:106] Iteration 11900, lr = 0.00962812
I0630 02:46:35.744079 29777 solver.cpp:354] Sparsity after update:
I0630 02:46:35.745528 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:46:35.745537 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 02:46:35.745545 29777 net.cpp:1851] conv1b_param_0(0) 
I0630 02:46:35.745549 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 02:46:35.745554 29777 net.cpp:1851] res2a_branch2a_param_0(0.06) 
I0630 02:46:35.745560 29777 net.cpp:1851] res2a_branch2b_param_0(0.0599) 
I0630 02:46:35.745565 29777 net.cpp:1851] res3a_branch2a_param_0(0.06) 
I0630 02:46:35.745570 29777 net.cpp:1851] res3a_branch2b_param_0(0.06) 
I0630 02:46:35.745574 29777 net.cpp:1851] res4a_branch2a_param_0(0.06) 
I0630 02:46:35.745579 29777 net.cpp:1851] res4a_branch2b_param_0(0.06) 
I0630 02:46:35.745584 29777 net.cpp:1851] res5a_branch2a_param_0(0.06) 
I0630 02:46:35.745589 29777 net.cpp:1851] res5a_branch2b_param_0(0.06) 
I0630 02:46:35.745594 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (140986/2.86678e+06) 0.0492
I0630 02:46:35.745685 29777 solver.cpp:471] Iteration 12000, Testing net (#0)
I0630 02:46:36.969911 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 02:47:23.542265 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.552599
I0630 02:47:23.542367 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.791241
I0630 02:47:23.542381 29777 solver.cpp:544]     Test net output #2: loss = 1.59644 (* 1 = 1.59644 loss)
I0630 02:47:23.730800 29777 solver.cpp:290] Iteration 12000 (1.56736 iter/s, 63.8017s/100 iter), loss = 1.10714
I0630 02:47:23.730824 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 02:47:23.730830 29777 sgd_solver.cpp:106] Iteration 12000, lr = 0.009625
I0630 02:47:23.731518 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.07
I0630 02:47:23.860766 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 02:47:39.840665 29777 solver.cpp:290] Iteration 12100 (6.20756 iter/s, 16.1094s/100 iter), loss = 1.27381
I0630 02:47:39.840689 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 02:47:39.840697 29777 sgd_solver.cpp:106] Iteration 12100, lr = 0.00962188
I0630 02:47:55.848570 29777 solver.cpp:290] Iteration 12200 (6.2471 iter/s, 16.0074s/100 iter), loss = 1.14286
I0630 02:47:55.848687 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 02:47:55.848701 29777 sgd_solver.cpp:106] Iteration 12200, lr = 0.00961875
I0630 02:48:11.844678 29777 solver.cpp:290] Iteration 12300 (6.25174 iter/s, 15.9956s/100 iter), loss = 0.964286
I0630 02:48:11.844705 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 02:48:11.844717 29777 sgd_solver.cpp:106] Iteration 12300, lr = 0.00961563
I0630 02:48:27.870720 29777 solver.cpp:290] Iteration 12400 (6.24003 iter/s, 16.0256s/100 iter), loss = 0.904762
I0630 02:48:27.871044 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 02:48:27.871166 29777 sgd_solver.cpp:106] Iteration 12400, lr = 0.0096125
I0630 02:48:44.183939 29777 solver.cpp:290] Iteration 12500 (6.13029 iter/s, 16.3124s/100 iter), loss = 1.25
I0630 02:48:44.183961 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 02:48:44.183967 29777 sgd_solver.cpp:106] Iteration 12500, lr = 0.00960938
I0630 02:49:00.416286 29777 solver.cpp:290] Iteration 12600 (6.16072 iter/s, 16.2319s/100 iter), loss = 0.857143
I0630 02:49:00.416357 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 02:49:00.416368 29777 sgd_solver.cpp:106] Iteration 12600, lr = 0.00960625
I0630 02:49:16.664438 29777 solver.cpp:290] Iteration 12700 (6.15474 iter/s, 16.2476s/100 iter), loss = 1.22619
I0630 02:49:16.664463 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 02:49:16.664474 29777 sgd_solver.cpp:106] Iteration 12700, lr = 0.00960313
I0630 02:49:32.676111 29777 solver.cpp:290] Iteration 12800 (6.24563 iter/s, 16.0112s/100 iter), loss = 0.821429
I0630 02:49:32.676153 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 02:49:32.676161 29777 sgd_solver.cpp:106] Iteration 12800, lr = 0.0096
I0630 02:49:48.792279 29777 solver.cpp:290] Iteration 12900 (6.20513 iter/s, 16.1157s/100 iter), loss = 1.09524
I0630 02:49:48.792302 29777 solver.cpp:309]     Train net output #0: loss = 0.52381 (* 1 = 0.52381 loss)
I0630 02:49:48.792309 29777 sgd_solver.cpp:106] Iteration 12900, lr = 0.00959687
I0630 02:50:04.628621 29777 solver.cpp:354] Sparsity after update:
I0630 02:50:04.649134 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:50:04.649150 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 02:50:04.649159 29777 net.cpp:1851] conv1b_param_0(0.0699) 
I0630 02:50:04.649164 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 02:50:04.649176 29777 net.cpp:1851] res2a_branch2a_param_0(0.07) 
I0630 02:50:04.649186 29777 net.cpp:1851] res2a_branch2b_param_0(0.07) 
I0630 02:50:04.649194 29777 net.cpp:1851] res3a_branch2a_param_0(0.07) 
I0630 02:50:04.649204 29777 net.cpp:1851] res3a_branch2b_param_0(0.07) 
I0630 02:50:04.649217 29777 net.cpp:1851] res4a_branch2a_param_0(0.07) 
I0630 02:50:04.649224 29777 net.cpp:1851] res4a_branch2b_param_0(0.07) 
I0630 02:50:04.649232 29777 net.cpp:1851] res5a_branch2a_param_0(0.07) 
I0630 02:50:04.649242 29777 net.cpp:1851] res5a_branch2b_param_0(0.07) 
I0630 02:50:04.649247 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (164652/2.86678e+06) 0.0574
I0630 02:50:04.808748 29777 solver.cpp:290] Iteration 13000 (6.24375 iter/s, 16.016s/100 iter), loss = 1.64286
I0630 02:50:04.808773 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 02:50:04.808779 29777 sgd_solver.cpp:106] Iteration 13000, lr = 0.00959375
I0630 02:50:20.826068 29777 solver.cpp:290] Iteration 13100 (6.24342 iter/s, 16.0169s/100 iter), loss = 1.11905
I0630 02:50:20.826094 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 02:50:20.826103 29777 sgd_solver.cpp:106] Iteration 13100, lr = 0.00959062
I0630 02:50:36.823475 29777 solver.cpp:290] Iteration 13200 (6.25119 iter/s, 15.9969s/100 iter), loss = 0.952381
I0630 02:50:36.823560 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 02:50:36.823567 29777 sgd_solver.cpp:106] Iteration 13200, lr = 0.0095875
I0630 02:50:52.904510 29777 solver.cpp:290] Iteration 13300 (6.21871 iter/s, 16.0805s/100 iter), loss = 1.10714
I0630 02:50:52.904533 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 02:50:52.904541 29777 sgd_solver.cpp:106] Iteration 13300, lr = 0.00958437
I0630 02:51:08.828534 29777 solver.cpp:290] Iteration 13400 (6.28 iter/s, 15.9236s/100 iter), loss = 1.11905
I0630 02:51:08.828611 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 02:51:08.828624 29777 sgd_solver.cpp:106] Iteration 13400, lr = 0.00958125
I0630 02:51:24.923534 29777 solver.cpp:290] Iteration 13500 (6.21331 iter/s, 16.0945s/100 iter), loss = 0.964286
I0630 02:51:24.923557 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 02:51:24.923564 29777 sgd_solver.cpp:106] Iteration 13500, lr = 0.00957812
I0630 02:51:40.923938 29777 solver.cpp:290] Iteration 13600 (6.25002 iter/s, 15.9999s/100 iter), loss = 0.964286
I0630 02:51:40.924042 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 02:51:40.924052 29777 sgd_solver.cpp:106] Iteration 13600, lr = 0.009575
I0630 02:51:56.884902 29777 solver.cpp:290] Iteration 13700 (6.2655 iter/s, 15.9604s/100 iter), loss = 1.4881
I0630 02:51:56.885200 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 02:51:56.885336 29777 sgd_solver.cpp:106] Iteration 13700, lr = 0.00957187
I0630 02:52:12.895251 29777 solver.cpp:290] Iteration 13800 (6.24624 iter/s, 16.0096s/100 iter), loss = 0.702381
I0630 02:52:12.895303 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 02:52:12.895311 29777 sgd_solver.cpp:106] Iteration 13800, lr = 0.00956875
I0630 02:52:28.804695 29777 solver.cpp:290] Iteration 13900 (6.28577 iter/s, 15.909s/100 iter), loss = 1.19048
I0630 02:52:28.804718 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 02:52:28.804724 29777 sgd_solver.cpp:106] Iteration 13900, lr = 0.00956563
I0630 02:52:44.579962 29777 solver.cpp:354] Sparsity after update:
I0630 02:52:44.581421 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:52:44.581429 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 02:52:44.581439 29777 net.cpp:1851] conv1b_param_0(0.0699) 
I0630 02:52:44.581444 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 02:52:44.581449 29777 net.cpp:1851] res2a_branch2a_param_0(0.07) 
I0630 02:52:44.581454 29777 net.cpp:1851] res2a_branch2b_param_0(0.07) 
I0630 02:52:44.581457 29777 net.cpp:1851] res3a_branch2a_param_0(0.07) 
I0630 02:52:44.581461 29777 net.cpp:1851] res3a_branch2b_param_0(0.07) 
I0630 02:52:44.581465 29777 net.cpp:1851] res4a_branch2a_param_0(0.07) 
I0630 02:52:44.581470 29777 net.cpp:1851] res4a_branch2b_param_0(0.07) 
I0630 02:52:44.581473 29777 net.cpp:1851] res5a_branch2a_param_0(0.07) 
I0630 02:52:44.581477 29777 net.cpp:1851] res5a_branch2b_param_0(0.07) 
I0630 02:52:44.581481 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (164652/2.86678e+06) 0.0574
I0630 02:52:44.581573 29777 solver.cpp:471] Iteration 14000, Testing net (#0)
I0630 02:52:46.026688 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 02:53:34.087837 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54432
I0630 02:53:34.087963 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.785821
I0630 02:53:34.087976 29777 solver.cpp:544]     Test net output #2: loss = 1.63656 (* 1 = 1.63656 loss)
I0630 02:53:34.291817 29777 solver.cpp:290] Iteration 14000 (1.52706 iter/s, 65.4853s/100 iter), loss = 1.13095
I0630 02:53:34.291841 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 02:53:34.291849 29777 sgd_solver.cpp:106] Iteration 14000, lr = 0.0095625
I0630 02:53:34.292564 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.08
I0630 02:53:34.418615 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 02:53:50.466578 29777 solver.cpp:290] Iteration 14100 (6.18265 iter/s, 16.1743s/100 iter), loss = 1.13095
I0630 02:53:50.466601 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 02:53:50.466610 29777 sgd_solver.cpp:106] Iteration 14100, lr = 0.00955938
I0630 02:54:06.452538 29777 solver.cpp:290] Iteration 14200 (6.25567 iter/s, 15.9855s/100 iter), loss = 1.13095
I0630 02:54:06.452594 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 02:54:06.452615 29777 sgd_solver.cpp:106] Iteration 14200, lr = 0.00955625
I0630 02:54:22.632446 29777 solver.cpp:290] Iteration 14300 (6.1807 iter/s, 16.1794s/100 iter), loss = 0.880952
I0630 02:54:22.632469 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 02:54:22.632475 29777 sgd_solver.cpp:106] Iteration 14300, lr = 0.00955313
I0630 02:54:38.834691 29777 solver.cpp:290] Iteration 14400 (6.17216 iter/s, 16.2018s/100 iter), loss = 1.25
I0630 02:54:38.834802 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 02:54:38.834817 29777 sgd_solver.cpp:106] Iteration 14400, lr = 0.00955
I0630 02:54:54.903993 29777 solver.cpp:290] Iteration 14500 (6.22326 iter/s, 16.0688s/100 iter), loss = 0.892857
I0630 02:54:54.904027 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 02:54:54.904037 29777 sgd_solver.cpp:106] Iteration 14500, lr = 0.00954687
I0630 02:55:10.958890 29777 solver.cpp:290] Iteration 14600 (6.22881 iter/s, 16.0544s/100 iter), loss = 1.25
I0630 02:55:10.958963 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 02:55:10.958971 29777 sgd_solver.cpp:106] Iteration 14600, lr = 0.00954375
I0630 02:55:26.915038 29777 solver.cpp:290] Iteration 14700 (6.26738 iter/s, 15.9556s/100 iter), loss = 1.02381
I0630 02:55:26.915065 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 02:55:26.915073 29777 sgd_solver.cpp:106] Iteration 14700, lr = 0.00954062
I0630 02:55:43.015753 29777 solver.cpp:290] Iteration 14800 (6.21108 iter/s, 16.1002s/100 iter), loss = 1.36905
I0630 02:55:43.015830 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 02:55:43.015841 29777 sgd_solver.cpp:106] Iteration 14800, lr = 0.0095375
I0630 02:55:59.214376 29777 solver.cpp:290] Iteration 14900 (6.17356 iter/s, 16.1981s/100 iter), loss = 1.19048
I0630 02:55:59.214399 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 02:55:59.214406 29777 sgd_solver.cpp:106] Iteration 14900, lr = 0.00953437
I0630 02:56:14.996541 29777 solver.cpp:354] Sparsity after update:
I0630 02:56:15.016903 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:56:15.016919 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 02:56:15.016929 29777 net.cpp:1851] conv1b_param_0(0.0799) 
I0630 02:56:15.016933 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 02:56:15.016937 29777 net.cpp:1851] res2a_branch2a_param_0(0.0799) 
I0630 02:56:15.016940 29777 net.cpp:1851] res2a_branch2b_param_0(0.08) 
I0630 02:56:15.016944 29777 net.cpp:1851] res3a_branch2a_param_0(0.08) 
I0630 02:56:15.016947 29777 net.cpp:1851] res3a_branch2b_param_0(0.08) 
I0630 02:56:15.016950 29777 net.cpp:1851] res4a_branch2a_param_0(0.08) 
I0630 02:56:15.016953 29777 net.cpp:1851] res4a_branch2b_param_0(0.08) 
I0630 02:56:15.016957 29777 net.cpp:1851] res5a_branch2a_param_0(0.08) 
I0630 02:56:15.016960 29777 net.cpp:1851] res5a_branch2b_param_0(0.08) 
I0630 02:56:15.016963 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (188166/2.86678e+06) 0.0656
I0630 02:56:15.174706 29777 solver.cpp:290] Iteration 15000 (6.26572 iter/s, 15.9599s/100 iter), loss = 0.821429
I0630 02:56:15.174729 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 02:56:15.174736 29777 sgd_solver.cpp:106] Iteration 15000, lr = 0.00953125
I0630 02:56:31.191126 29777 solver.cpp:290] Iteration 15100 (6.24377 iter/s, 16.016s/100 iter), loss = 1.45238
I0630 02:56:31.191148 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 02:56:31.191154 29777 sgd_solver.cpp:106] Iteration 15100, lr = 0.00952812
I0630 02:56:47.123463 29777 solver.cpp:290] Iteration 15200 (6.27672 iter/s, 15.9319s/100 iter), loss = 0.988095
I0630 02:56:47.123534 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 02:56:47.123548 29777 sgd_solver.cpp:106] Iteration 15200, lr = 0.009525
I0630 02:57:03.084118 29777 solver.cpp:290] Iteration 15300 (6.26561 iter/s, 15.9601s/100 iter), loss = 0.952381
I0630 02:57:03.084141 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 02:57:03.084148 29777 sgd_solver.cpp:106] Iteration 15300, lr = 0.00952187
I0630 02:57:19.104481 29777 solver.cpp:290] Iteration 15400 (6.24224 iter/s, 16.0199s/100 iter), loss = 1.35714
I0630 02:57:19.104583 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 02:57:19.104593 29777 sgd_solver.cpp:106] Iteration 15400, lr = 0.00951875
I0630 02:57:35.128036 29777 solver.cpp:290] Iteration 15500 (6.24102 iter/s, 16.023s/100 iter), loss = 1.15476
I0630 02:57:35.128058 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 02:57:35.128064 29777 sgd_solver.cpp:106] Iteration 15500, lr = 0.00951563
I0630 02:57:51.040678 29777 solver.cpp:290] Iteration 15600 (6.28449 iter/s, 15.9122s/100 iter), loss = 1.04762
I0630 02:57:51.040765 29777 solver.cpp:309]     Train net output #0: loss = 0.547619 (* 1 = 0.547619 loss)
I0630 02:57:51.040771 29777 sgd_solver.cpp:106] Iteration 15600, lr = 0.0095125
I0630 02:58:07.072262 29777 solver.cpp:290] Iteration 15700 (6.23789 iter/s, 16.0311s/100 iter), loss = 1.34524
I0630 02:58:07.072283 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 02:58:07.072290 29777 sgd_solver.cpp:106] Iteration 15700, lr = 0.00950938
I0630 02:58:23.183152 29777 solver.cpp:290] Iteration 15800 (6.20716 iter/s, 16.1104s/100 iter), loss = 0.916667
I0630 02:58:23.183254 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 02:58:23.183262 29777 sgd_solver.cpp:106] Iteration 15800, lr = 0.00950625
I0630 02:58:39.322104 29777 solver.cpp:290] Iteration 15900 (6.1964 iter/s, 16.1384s/100 iter), loss = 1.13095
I0630 02:58:39.322125 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 02:58:39.322134 29777 sgd_solver.cpp:106] Iteration 15900, lr = 0.00950312
I0630 02:58:55.185400 29777 solver.cpp:354] Sparsity after update:
I0630 02:58:55.186856 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:58:55.186862 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 02:58:55.186870 29777 net.cpp:1851] conv1b_param_0(0.0799) 
I0630 02:58:55.186873 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 02:58:55.186875 29777 net.cpp:1851] res2a_branch2a_param_0(0.0799) 
I0630 02:58:55.186877 29777 net.cpp:1851] res2a_branch2b_param_0(0.08) 
I0630 02:58:55.186880 29777 net.cpp:1851] res3a_branch2a_param_0(0.08) 
I0630 02:58:55.186882 29777 net.cpp:1851] res3a_branch2b_param_0(0.08) 
I0630 02:58:55.186884 29777 net.cpp:1851] res4a_branch2a_param_0(0.08) 
I0630 02:58:55.186887 29777 net.cpp:1851] res4a_branch2b_param_0(0.08) 
I0630 02:58:55.186888 29777 net.cpp:1851] res5a_branch2a_param_0(0.08) 
I0630 02:58:55.186892 29777 net.cpp:1851] res5a_branch2b_param_0(0.08) 
I0630 02:58:55.186893 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (188166/2.86678e+06) 0.0656
I0630 02:58:55.186980 29777 solver.cpp:471] Iteration 16000, Testing net (#0)
I0630 02:58:56.805533 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 02:59:44.197516 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54232
I0630 02:59:44.197633 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.783621
I0630 02:59:44.197643 29777 solver.cpp:544]     Test net output #2: loss = 1.63826 (* 1 = 1.63826 loss)
I0630 02:59:44.392650 29777 solver.cpp:290] Iteration 16000 (1.53684 iter/s, 65.0688s/100 iter), loss = 1.25
I0630 02:59:44.392675 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 02:59:44.392683 29777 sgd_solver.cpp:106] Iteration 16000, lr = 0.0095
I0630 02:59:44.393425 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.09
I0630 02:59:44.517319 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 03:00:00.642909 29777 solver.cpp:290] Iteration 16100 (6.15393 iter/s, 16.2498s/100 iter), loss = 1.10714
I0630 03:00:00.642931 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 03:00:00.642938 29777 sgd_solver.cpp:106] Iteration 16100, lr = 0.00949687
I0630 03:00:16.681254 29777 solver.cpp:290] Iteration 16200 (6.23524 iter/s, 16.0379s/100 iter), loss = 0.880952
I0630 03:00:16.681305 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 03:00:16.681313 29777 sgd_solver.cpp:106] Iteration 16200, lr = 0.00949375
I0630 03:00:32.685703 29777 solver.cpp:290] Iteration 16300 (6.24845 iter/s, 16.004s/100 iter), loss = 1.05952
I0630 03:00:32.685727 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 03:00:32.685734 29777 sgd_solver.cpp:106] Iteration 16300, lr = 0.00949063
I0630 03:00:48.633235 29777 solver.cpp:290] Iteration 16400 (6.27075 iter/s, 15.9471s/100 iter), loss = 1.10714
I0630 03:00:48.633327 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 03:00:48.633338 29777 sgd_solver.cpp:106] Iteration 16400, lr = 0.0094875
I0630 03:01:04.640962 29777 solver.cpp:290] Iteration 16500 (6.24719 iter/s, 16.0072s/100 iter), loss = 0.964286
I0630 03:01:04.640986 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 03:01:04.640993 29777 sgd_solver.cpp:106] Iteration 16500, lr = 0.00948437
I0630 03:01:20.629077 29777 solver.cpp:290] Iteration 16600 (6.25483 iter/s, 15.9876s/100 iter), loss = 1.02381
I0630 03:01:20.629163 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 03:01:20.629175 29777 sgd_solver.cpp:106] Iteration 16600, lr = 0.00948125
I0630 03:01:36.659951 29777 solver.cpp:290] Iteration 16700 (6.23817 iter/s, 16.0304s/100 iter), loss = 1.41667
I0630 03:01:36.659974 29777 solver.cpp:309]     Train net output #0: loss = 1.85714 (* 1 = 1.85714 loss)
I0630 03:01:36.659981 29777 sgd_solver.cpp:106] Iteration 16700, lr = 0.00947812
I0630 03:01:52.631623 29777 solver.cpp:290] Iteration 16800 (6.26127 iter/s, 15.9712s/100 iter), loss = 0.857143
I0630 03:01:52.631726 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 03:01:52.631741 29777 sgd_solver.cpp:106] Iteration 16800, lr = 0.009475
I0630 03:02:08.590171 29777 solver.cpp:290] Iteration 16900 (6.26645 iter/s, 15.958s/100 iter), loss = 0.630952
I0630 03:02:08.590195 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 03:02:08.590203 29777 sgd_solver.cpp:106] Iteration 16900, lr = 0.00947187
I0630 03:02:24.418458 29777 solver.cpp:354] Sparsity after update:
I0630 03:02:24.438814 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:02:24.438832 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 03:02:24.438846 29777 net.cpp:1851] conv1b_param_0(0.0898) 
I0630 03:02:24.438860 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:02:24.438869 29777 net.cpp:1851] res2a_branch2a_param_0(0.09) 
I0630 03:02:24.438879 29777 net.cpp:1851] res2a_branch2b_param_0(0.09) 
I0630 03:02:24.438890 29777 net.cpp:1851] res3a_branch2a_param_0(0.09) 
I0630 03:02:24.438896 29777 net.cpp:1851] res3a_branch2b_param_0(0.09) 
I0630 03:02:24.438905 29777 net.cpp:1851] res4a_branch2a_param_0(0.09) 
I0630 03:02:24.438915 29777 net.cpp:1851] res4a_branch2b_param_0(0.09) 
I0630 03:02:24.438920 29777 net.cpp:1851] res5a_branch2a_param_0(0.09) 
I0630 03:02:24.438925 29777 net.cpp:1851] res5a_branch2b_param_0(0.09) 
I0630 03:02:24.438932 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (211694/2.86678e+06) 0.0738
I0630 03:02:24.595706 29777 solver.cpp:290] Iteration 17000 (6.24802 iter/s, 16.0051s/100 iter), loss = 1
I0630 03:02:24.595729 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 03:02:24.595736 29777 sgd_solver.cpp:106] Iteration 17000, lr = 0.00946875
I0630 03:02:40.614715 29777 solver.cpp:290] Iteration 17100 (6.24276 iter/s, 16.0185s/100 iter), loss = 1.2381
I0630 03:02:40.614737 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 03:02:40.614744 29777 sgd_solver.cpp:106] Iteration 17100, lr = 0.00946563
I0630 03:02:56.543800 29777 solver.cpp:290] Iteration 17200 (6.27801 iter/s, 15.9286s/100 iter), loss = 1.20238
I0630 03:02:56.543895 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 03:02:56.543905 29777 sgd_solver.cpp:106] Iteration 17200, lr = 0.0094625
I0630 03:03:12.645879 29777 solver.cpp:290] Iteration 17300 (6.21058 iter/s, 16.1015s/100 iter), loss = 1.16667
I0630 03:03:12.645900 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 03:03:12.645906 29777 sgd_solver.cpp:106] Iteration 17300, lr = 0.00945937
I0630 03:03:28.623194 29777 solver.cpp:290] Iteration 17400 (6.25906 iter/s, 15.9768s/100 iter), loss = 0.833333
I0630 03:03:28.623330 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 03:03:28.623363 29777 sgd_solver.cpp:106] Iteration 17400, lr = 0.00945625
I0630 03:03:44.858052 29777 solver.cpp:290] Iteration 17500 (6.1598 iter/s, 16.2343s/100 iter), loss = 0.845238
I0630 03:03:44.858078 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 03:03:44.858085 29777 sgd_solver.cpp:106] Iteration 17500, lr = 0.00945312
I0630 03:04:00.970170 29777 solver.cpp:290] Iteration 17600 (6.20669 iter/s, 16.1116s/100 iter), loss = 0.988095
I0630 03:04:00.970283 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 03:04:00.970293 29777 sgd_solver.cpp:106] Iteration 17600, lr = 0.00945
I0630 03:04:17.207590 29777 solver.cpp:290] Iteration 17700 (6.15882 iter/s, 16.2369s/100 iter), loss = 1.07143
I0630 03:04:17.207613 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 03:04:17.207620 29777 sgd_solver.cpp:106] Iteration 17700, lr = 0.00944687
I0630 03:04:33.385439 29777 solver.cpp:290] Iteration 17800 (6.18147 iter/s, 16.1774s/100 iter), loss = 1.19048
I0630 03:04:33.385534 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 03:04:33.385545 29777 sgd_solver.cpp:106] Iteration 17800, lr = 0.00944375
I0630 03:04:49.386541 29777 solver.cpp:290] Iteration 17900 (6.24978 iter/s, 16.0006s/100 iter), loss = 1.17857
I0630 03:04:49.386565 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 03:04:49.386581 29777 sgd_solver.cpp:106] Iteration 17900, lr = 0.00944062
I0630 03:05:05.437439 29777 solver.cpp:354] Sparsity after update:
I0630 03:05:05.438724 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:05:05.438731 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 03:05:05.438738 29777 net.cpp:1851] conv1b_param_0(0.0898) 
I0630 03:05:05.438742 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:05:05.438745 29777 net.cpp:1851] res2a_branch2a_param_0(0.09) 
I0630 03:05:05.438747 29777 net.cpp:1851] res2a_branch2b_param_0(0.09) 
I0630 03:05:05.438750 29777 net.cpp:1851] res3a_branch2a_param_0(0.09) 
I0630 03:05:05.438752 29777 net.cpp:1851] res3a_branch2b_param_0(0.09) 
I0630 03:05:05.438755 29777 net.cpp:1851] res4a_branch2a_param_0(0.09) 
I0630 03:05:05.438757 29777 net.cpp:1851] res4a_branch2b_param_0(0.09) 
I0630 03:05:05.438760 29777 net.cpp:1851] res5a_branch2a_param_0(0.09) 
I0630 03:05:05.438762 29777 net.cpp:1851] res5a_branch2b_param_0(0.09) 
I0630 03:05:05.438765 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (211694/2.86678e+06) 0.0738
I0630 03:05:05.438855 29777 solver.cpp:471] Iteration 18000, Testing net (#0)
I0630 03:05:07.488678 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 03:05:55.181005 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.52388
I0630 03:05:55.181079 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.773161
I0630 03:05:55.181087 29777 solver.cpp:544]     Test net output #2: loss = 1.73908 (* 1 = 1.73908 loss)
I0630 03:05:55.356158 29777 solver.cpp:290] Iteration 18000 (1.51589 iter/s, 65.9678s/100 iter), loss = 1.20238
I0630 03:05:55.356182 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 03:05:55.356189 29777 sgd_solver.cpp:106] Iteration 18000, lr = 0.0094375
I0630 03:05:55.356865 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.1
I0630 03:05:55.483129 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 03:06:11.386868 29777 solver.cpp:290] Iteration 18100 (6.23821 iter/s, 16.0302s/100 iter), loss = 1.46429
I0630 03:06:11.386942 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 03:06:11.386970 29777 sgd_solver.cpp:106] Iteration 18100, lr = 0.00943437
I0630 03:06:27.347118 29777 solver.cpp:290] Iteration 18200 (6.26577 iter/s, 15.9597s/100 iter), loss = 1.28571
I0630 03:06:27.347208 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 03:06:27.347218 29777 sgd_solver.cpp:106] Iteration 18200, lr = 0.00943125
I0630 03:06:43.337422 29777 solver.cpp:290] Iteration 18300 (6.254 iter/s, 15.9898s/100 iter), loss = 0.845238
I0630 03:06:43.337445 29777 solver.cpp:309]     Train net output #0: loss = 0.52381 (* 1 = 0.52381 loss)
I0630 03:06:43.337452 29777 sgd_solver.cpp:106] Iteration 18300, lr = 0.00942812
I0630 03:06:59.284631 29777 solver.cpp:290] Iteration 18400 (6.27087 iter/s, 15.9467s/100 iter), loss = 0.761905
I0630 03:06:59.284744 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 03:06:59.284754 29777 sgd_solver.cpp:106] Iteration 18400, lr = 0.009425
I0630 03:07:15.320431 29777 solver.cpp:290] Iteration 18500 (6.23626 iter/s, 16.0352s/100 iter), loss = 1.2381
I0630 03:07:15.320457 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 03:07:15.320466 29777 sgd_solver.cpp:106] Iteration 18500, lr = 0.00942187
I0630 03:07:31.214617 29777 solver.cpp:290] Iteration 18600 (6.29179 iter/s, 15.8937s/100 iter), loss = 1.13095
I0630 03:07:31.214726 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 03:07:31.214735 29777 sgd_solver.cpp:106] Iteration 18600, lr = 0.00941875
I0630 03:07:47.153111 29777 solver.cpp:290] Iteration 18700 (6.27433 iter/s, 15.938s/100 iter), loss = 1.28571
I0630 03:07:47.153133 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 03:07:47.153141 29777 sgd_solver.cpp:106] Iteration 18700, lr = 0.00941562
I0630 03:08:03.119881 29777 solver.cpp:290] Iteration 18800 (6.26319 iter/s, 15.9663s/100 iter), loss = 1.03571
I0630 03:08:03.119945 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 03:08:03.119957 29777 sgd_solver.cpp:106] Iteration 18800, lr = 0.0094125
I0630 03:08:19.082633 29777 solver.cpp:290] Iteration 18900 (6.26478 iter/s, 15.9623s/100 iter), loss = 1.07143
I0630 03:08:19.082659 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 03:08:19.082667 29777 sgd_solver.cpp:106] Iteration 18900, lr = 0.00940937
I0630 03:08:34.896169 29777 solver.cpp:354] Sparsity after update:
I0630 03:08:34.916573 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:08:34.916585 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 03:08:34.916594 29777 net.cpp:1851] conv1b_param_0(0.0998) 
I0630 03:08:34.916596 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:08:34.916599 29777 net.cpp:1851] res2a_branch2a_param_0(0.1) 
I0630 03:08:34.916601 29777 net.cpp:1851] res2a_branch2b_param_0(0.0999) 
I0630 03:08:34.916604 29777 net.cpp:1851] res3a_branch2a_param_0(0.1) 
I0630 03:08:34.916605 29777 net.cpp:1851] res3a_branch2b_param_0(0.1) 
I0630 03:08:34.916607 29777 net.cpp:1851] res4a_branch2a_param_0(0.1) 
I0630 03:08:34.916616 29777 net.cpp:1851] res4a_branch2b_param_0(0.1) 
I0630 03:08:34.916618 29777 net.cpp:1851] res5a_branch2a_param_0(0.1) 
I0630 03:08:34.916620 29777 net.cpp:1851] res5a_branch2b_param_0(0.1) 
I0630 03:08:34.916622 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (235222/2.86678e+06) 0.0821
I0630 03:08:35.082058 29777 solver.cpp:290] Iteration 19000 (6.25041 iter/s, 15.999s/100 iter), loss = 1.95238
I0630 03:08:35.082082 29777 solver.cpp:309]     Train net output #0: loss = 2.66667 (* 1 = 2.66667 loss)
I0630 03:08:35.082088 29777 sgd_solver.cpp:106] Iteration 19000, lr = 0.00940625
I0630 03:08:51.118062 29777 solver.cpp:290] Iteration 19100 (6.23615 iter/s, 16.0355s/100 iter), loss = 0.97619
I0630 03:08:51.118085 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 03:08:51.118091 29777 sgd_solver.cpp:106] Iteration 19100, lr = 0.00940312
I0630 03:09:07.097332 29777 solver.cpp:290] Iteration 19200 (6.25829 iter/s, 15.9788s/100 iter), loss = 0.714286
I0630 03:09:07.097440 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 03:09:07.097450 29777 sgd_solver.cpp:106] Iteration 19200, lr = 0.0094
I0630 03:09:23.221707 29777 solver.cpp:290] Iteration 19300 (6.202 iter/s, 16.1238s/100 iter), loss = 0.988095
I0630 03:09:23.221735 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 03:09:23.221745 29777 sgd_solver.cpp:106] Iteration 19300, lr = 0.00939687
I0630 03:09:39.469307 29777 solver.cpp:290] Iteration 19400 (6.15494 iter/s, 16.2471s/100 iter), loss = 0.988095
I0630 03:09:39.469418 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 03:09:39.469439 29777 sgd_solver.cpp:106] Iteration 19400, lr = 0.00939375
I0630 03:09:55.434504 29777 solver.cpp:290] Iteration 19500 (6.26384 iter/s, 15.9647s/100 iter), loss = 1.10714
I0630 03:09:55.434526 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 03:09:55.434532 29777 sgd_solver.cpp:106] Iteration 19500, lr = 0.00939062
I0630 03:10:11.464148 29777 solver.cpp:290] Iteration 19600 (6.23862 iter/s, 16.0292s/100 iter), loss = 1.30952
I0630 03:10:11.464761 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 03:10:11.464771 29777 sgd_solver.cpp:106] Iteration 19600, lr = 0.0093875
I0630 03:10:27.414158 29777 solver.cpp:290] Iteration 19700 (6.27 iter/s, 15.949s/100 iter), loss = 0.916667
I0630 03:10:27.414182 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 03:10:27.414189 29777 sgd_solver.cpp:106] Iteration 19700, lr = 0.00938438
I0630 03:10:43.404683 29777 solver.cpp:290] Iteration 19800 (6.25388 iter/s, 15.9901s/100 iter), loss = 1.32143
I0630 03:10:43.404786 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 03:10:43.404795 29777 sgd_solver.cpp:106] Iteration 19800, lr = 0.00938125
I0630 03:10:59.346397 29777 solver.cpp:290] Iteration 19900 (6.27306 iter/s, 15.9412s/100 iter), loss = 1.33333
I0630 03:10:59.346420 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 03:10:59.346426 29777 sgd_solver.cpp:106] Iteration 19900, lr = 0.00937812
I0630 03:11:15.153998 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_20000.caffemodel
I0630 03:11:15.173892 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_20000.solverstate
I0630 03:11:15.182598 29777 solver.cpp:354] Sparsity after update:
I0630 03:11:15.183598 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:11:15.183606 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 03:11:15.183614 29777 net.cpp:1851] conv1b_param_0(0.0998) 
I0630 03:11:15.183616 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:11:15.183619 29777 net.cpp:1851] res2a_branch2a_param_0(0.1) 
I0630 03:11:15.183620 29777 net.cpp:1851] res2a_branch2b_param_0(0.0999) 
I0630 03:11:15.183622 29777 net.cpp:1851] res3a_branch2a_param_0(0.1) 
I0630 03:11:15.183624 29777 net.cpp:1851] res3a_branch2b_param_0(0.1) 
I0630 03:11:15.183626 29777 net.cpp:1851] res4a_branch2a_param_0(0.1) 
I0630 03:11:15.183629 29777 net.cpp:1851] res4a_branch2b_param_0(0.1) 
I0630 03:11:15.183630 29777 net.cpp:1851] res5a_branch2a_param_0(0.1) 
I0630 03:11:15.183632 29777 net.cpp:1851] res5a_branch2b_param_0(0.1) 
I0630 03:11:15.183634 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (235222/2.86678e+06) 0.0821
I0630 03:11:15.183728 29777 solver.cpp:471] Iteration 20000, Testing net (#0)
I0630 03:11:17.202011 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 03:12:05.602286 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.555359
I0630 03:12:05.602402 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.794901
I0630 03:12:05.602411 29777 solver.cpp:544]     Test net output #2: loss = 1.60594 (* 1 = 1.60594 loss)
I0630 03:12:05.785555 29777 solver.cpp:290] Iteration 20000 (1.50518 iter/s, 66.4373s/100 iter), loss = 1.57143
I0630 03:12:05.785583 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 03:12:05.785591 29777 sgd_solver.cpp:106] Iteration 20000, lr = 0.009375
I0630 03:12:05.786551 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.11
I0630 03:12:05.911828 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 03:12:21.849020 29777 solver.cpp:290] Iteration 20100 (6.22549 iter/s, 16.063s/100 iter), loss = 1.02381
I0630 03:12:21.849046 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 03:12:21.849056 29777 sgd_solver.cpp:106] Iteration 20100, lr = 0.00937187
I0630 03:12:37.851404 29777 solver.cpp:290] Iteration 20200 (6.24925 iter/s, 16.0019s/100 iter), loss = 0.97619
I0630 03:12:37.851506 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 03:12:37.851516 29777 sgd_solver.cpp:106] Iteration 20200, lr = 0.00936875
I0630 03:12:53.850611 29777 solver.cpp:290] Iteration 20300 (6.25052 iter/s, 15.9987s/100 iter), loss = 1.2381
I0630 03:12:53.850636 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 03:12:53.850643 29777 sgd_solver.cpp:106] Iteration 20300, lr = 0.00936562
I0630 03:13:09.748147 29777 solver.cpp:290] Iteration 20400 (6.29047 iter/s, 15.8971s/100 iter), loss = 1.04762
I0630 03:13:09.748224 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 03:13:09.748230 29777 sgd_solver.cpp:106] Iteration 20400, lr = 0.0093625
I0630 03:13:25.734309 29777 solver.cpp:290] Iteration 20500 (6.25561 iter/s, 15.9856s/100 iter), loss = 1.03571
I0630 03:13:25.734334 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 03:13:25.734344 29777 sgd_solver.cpp:106] Iteration 20500, lr = 0.00935937
I0630 03:13:41.739518 29777 solver.cpp:290] Iteration 20600 (6.24815 iter/s, 16.0047s/100 iter), loss = 1.08333
I0630 03:13:41.739632 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 03:13:41.739641 29777 sgd_solver.cpp:106] Iteration 20600, lr = 0.00935625
I0630 03:13:57.721748 29777 solver.cpp:290] Iteration 20700 (6.25717 iter/s, 15.9817s/100 iter), loss = 1.41667
I0630 03:13:57.721772 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 03:13:57.721779 29777 sgd_solver.cpp:106] Iteration 20700, lr = 0.00935312
I0630 03:14:13.908562 29777 solver.cpp:290] Iteration 20800 (6.17805 iter/s, 16.1863s/100 iter), loss = 1.41667
I0630 03:14:13.908658 29777 solver.cpp:309]     Train net output #0: loss = 1.88095 (* 1 = 1.88095 loss)
I0630 03:14:13.908668 29777 sgd_solver.cpp:106] Iteration 20800, lr = 0.00935
I0630 03:14:30.046999 29777 solver.cpp:290] Iteration 20900 (6.19659 iter/s, 16.1379s/100 iter), loss = 1.25
I0630 03:14:30.047020 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 03:14:30.047027 29777 sgd_solver.cpp:106] Iteration 20900, lr = 0.00934687
I0630 03:14:46.006111 29777 solver.cpp:354] Sparsity after update:
I0630 03:14:46.026659 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:14:46.026676 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 03:14:46.026686 29777 net.cpp:1851] conv1b_param_0(0.11) 
I0630 03:14:46.026690 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:14:46.026693 29777 net.cpp:1851] res2a_branch2a_param_0(0.11) 
I0630 03:14:46.026697 29777 net.cpp:1851] res2a_branch2b_param_0(0.11) 
I0630 03:14:46.026700 29777 net.cpp:1851] res3a_branch2a_param_0(0.11) 
I0630 03:14:46.026705 29777 net.cpp:1851] res3a_branch2b_param_0(0.11) 
I0630 03:14:46.026707 29777 net.cpp:1851] res4a_branch2a_param_0(0.11) 
I0630 03:14:46.026710 29777 net.cpp:1851] res4a_branch2b_param_0(0.11) 
I0630 03:14:46.026713 29777 net.cpp:1851] res5a_branch2a_param_0(0.11) 
I0630 03:14:46.026717 29777 net.cpp:1851] res5a_branch2b_param_0(0.11) 
I0630 03:14:46.026721 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (258746/2.86678e+06) 0.0903
I0630 03:14:46.186498 29777 solver.cpp:290] Iteration 21000 (6.19616 iter/s, 16.139s/100 iter), loss = 1.0119
I0630 03:14:46.186522 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 03:14:46.186529 29777 sgd_solver.cpp:106] Iteration 21000, lr = 0.00934375
I0630 03:15:02.242038 29777 solver.cpp:290] Iteration 21100 (6.22856 iter/s, 16.0551s/100 iter), loss = 1.03571
I0630 03:15:02.242061 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 03:15:02.242069 29777 sgd_solver.cpp:106] Iteration 21100, lr = 0.00934062
I0630 03:15:18.436229 29777 solver.cpp:290] Iteration 21200 (6.17524 iter/s, 16.1937s/100 iter), loss = 1.20238
I0630 03:15:18.436322 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 03:15:18.436337 29777 sgd_solver.cpp:106] Iteration 21200, lr = 0.0093375
I0630 03:15:34.472939 29777 solver.cpp:290] Iteration 21300 (6.2359 iter/s, 16.0362s/100 iter), loss = 1
I0630 03:15:34.472965 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 03:15:34.472971 29777 sgd_solver.cpp:106] Iteration 21300, lr = 0.00933437
I0630 03:15:50.515130 29777 solver.cpp:290] Iteration 21400 (6.23374 iter/s, 16.0417s/100 iter), loss = 1.07143
I0630 03:15:50.515234 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 03:15:50.515244 29777 sgd_solver.cpp:106] Iteration 21400, lr = 0.00933125
I0630 03:16:06.530128 29777 solver.cpp:290] Iteration 21500 (6.24436 iter/s, 16.0145s/100 iter), loss = 1.08333
I0630 03:16:06.530153 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 03:16:06.530161 29777 sgd_solver.cpp:106] Iteration 21500, lr = 0.00932813
I0630 03:16:22.517165 29777 solver.cpp:290] Iteration 21600 (6.25525 iter/s, 15.9866s/100 iter), loss = 1.59524
I0630 03:16:22.517269 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 03:16:22.517282 29777 sgd_solver.cpp:106] Iteration 21600, lr = 0.009325
I0630 03:16:38.605515 29777 solver.cpp:290] Iteration 21700 (6.21589 iter/s, 16.0878s/100 iter), loss = 1.04762
I0630 03:16:38.605538 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 03:16:38.605545 29777 sgd_solver.cpp:106] Iteration 21700, lr = 0.00932187
I0630 03:16:54.538547 29777 solver.cpp:290] Iteration 21800 (6.27645 iter/s, 15.9326s/100 iter), loss = 1.05952
I0630 03:16:54.538622 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 03:16:54.538638 29777 sgd_solver.cpp:106] Iteration 21800, lr = 0.00931875
I0630 03:17:10.500855 29777 solver.cpp:290] Iteration 21900 (6.26496 iter/s, 15.9618s/100 iter), loss = 0.988095
I0630 03:17:10.500881 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 03:17:10.500890 29777 sgd_solver.cpp:106] Iteration 21900, lr = 0.00931562
I0630 03:17:26.370270 29777 solver.cpp:354] Sparsity after update:
I0630 03:17:26.371704 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:17:26.371713 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 03:17:26.371721 29777 net.cpp:1851] conv1b_param_0(0.11) 
I0630 03:17:26.371726 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:17:26.371731 29777 net.cpp:1851] res2a_branch2a_param_0(0.11) 
I0630 03:17:26.371736 29777 net.cpp:1851] res2a_branch2b_param_0(0.11) 
I0630 03:17:26.371739 29777 net.cpp:1851] res3a_branch2a_param_0(0.11) 
I0630 03:17:26.371744 29777 net.cpp:1851] res3a_branch2b_param_0(0.11) 
I0630 03:17:26.371748 29777 net.cpp:1851] res4a_branch2a_param_0(0.11) 
I0630 03:17:26.371752 29777 net.cpp:1851] res4a_branch2b_param_0(0.11) 
I0630 03:17:26.371757 29777 net.cpp:1851] res5a_branch2a_param_0(0.11) 
I0630 03:17:26.371762 29777 net.cpp:1851] res5a_branch2b_param_0(0.11) 
I0630 03:17:26.371765 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (258746/2.86678e+06) 0.0903
I0630 03:17:26.371904 29777 solver.cpp:471] Iteration 22000, Testing net (#0)
I0630 03:17:28.611560 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 03:18:15.730875 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55792
I0630 03:18:15.730931 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.795961
I0630 03:18:15.730938 29777 solver.cpp:544]     Test net output #2: loss = 1.56708 (* 1 = 1.56708 loss)
I0630 03:18:15.909557 29777 solver.cpp:290] Iteration 22000 (1.52889 iter/s, 65.4069s/100 iter), loss = 0.904762
I0630 03:18:15.909581 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 03:18:15.909588 29777 sgd_solver.cpp:106] Iteration 22000, lr = 0.0093125
I0630 03:18:15.910293 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.12
I0630 03:18:16.040550 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 03:18:31.911911 29777 solver.cpp:290] Iteration 22100 (6.24926 iter/s, 16.0019s/100 iter), loss = 1.08333
I0630 03:18:31.911934 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 03:18:31.911943 29777 sgd_solver.cpp:106] Iteration 22100, lr = 0.00930937
I0630 03:18:48.055186 29777 solver.cpp:290] Iteration 22200 (6.19471 iter/s, 16.1428s/100 iter), loss = 1.19048
I0630 03:18:48.055258 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 03:18:48.055265 29777 sgd_solver.cpp:106] Iteration 22200, lr = 0.00930625
I0630 03:19:04.105777 29777 solver.cpp:290] Iteration 22300 (6.2305 iter/s, 16.0501s/100 iter), loss = 0.916667
I0630 03:19:04.105799 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 03:19:04.105806 29777 sgd_solver.cpp:106] Iteration 22300, lr = 0.00930312
I0630 03:19:20.269294 29777 solver.cpp:290] Iteration 22400 (6.18695 iter/s, 16.163s/100 iter), loss = 1.20238
I0630 03:19:20.269403 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 03:19:20.269413 29777 sgd_solver.cpp:106] Iteration 22400, lr = 0.0093
I0630 03:19:36.277614 29777 solver.cpp:290] Iteration 22500 (6.24697 iter/s, 16.0078s/100 iter), loss = 0.916667
I0630 03:19:36.277642 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 03:19:36.277658 29777 sgd_solver.cpp:106] Iteration 22500, lr = 0.00929687
I0630 03:19:52.262001 29777 solver.cpp:290] Iteration 22600 (6.25629 iter/s, 15.9839s/100 iter), loss = 1.10714
I0630 03:19:52.262270 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 03:19:52.262280 29777 sgd_solver.cpp:106] Iteration 22600, lr = 0.00929375
I0630 03:20:08.266427 29777 solver.cpp:290] Iteration 22700 (6.24855 iter/s, 16.0037s/100 iter), loss = 0.964286
I0630 03:20:08.266454 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 03:20:08.266470 29777 sgd_solver.cpp:106] Iteration 22700, lr = 0.00929062
I0630 03:20:24.261739 29777 solver.cpp:290] Iteration 22800 (6.25201 iter/s, 15.9948s/100 iter), loss = 0.97619
I0630 03:20:24.261812 29777 solver.cpp:309]     Train net output #0: loss = 0.595238 (* 1 = 0.595238 loss)
I0630 03:20:24.261823 29777 sgd_solver.cpp:106] Iteration 22800, lr = 0.0092875
I0630 03:20:40.230296 29777 solver.cpp:290] Iteration 22900 (6.26251 iter/s, 15.968s/100 iter), loss = 0.892857
I0630 03:20:40.230320 29777 solver.cpp:309]     Train net output #0: loss = 0.690476 (* 1 = 0.690476 loss)
I0630 03:20:40.230327 29777 sgd_solver.cpp:106] Iteration 22900, lr = 0.00928437
I0630 03:20:56.070777 29777 solver.cpp:354] Sparsity after update:
I0630 03:20:56.091576 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:20:56.091609 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 03:20:56.091626 29777 net.cpp:1851] conv1b_param_0(0.12) 
I0630 03:20:56.091635 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:20:56.091645 29777 net.cpp:1851] res2a_branch2a_param_0(0.12) 
I0630 03:20:56.091653 29777 net.cpp:1851] res2a_branch2b_param_0(0.12) 
I0630 03:20:56.091662 29777 net.cpp:1851] res3a_branch2a_param_0(0.12) 
I0630 03:20:56.091671 29777 net.cpp:1851] res3a_branch2b_param_0(0.12) 
I0630 03:20:56.091681 29777 net.cpp:1851] res4a_branch2a_param_0(0.12) 
I0630 03:20:56.091688 29777 net.cpp:1851] res4a_branch2b_param_0(0.12) 
I0630 03:20:56.091697 29777 net.cpp:1851] res5a_branch2a_param_0(0.12) 
I0630 03:20:56.091706 29777 net.cpp:1851] res5a_branch2b_param_0(0.12) 
I0630 03:20:56.091713 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (282275/2.86678e+06) 0.0985
I0630 03:20:56.251188 29777 solver.cpp:290] Iteration 23000 (6.24203 iter/s, 16.0204s/100 iter), loss = 1.15476
I0630 03:20:56.251212 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 03:20:56.251219 29777 sgd_solver.cpp:106] Iteration 23000, lr = 0.00928125
I0630 03:21:12.173712 29777 solver.cpp:290] Iteration 23100 (6.28059 iter/s, 15.9221s/100 iter), loss = 1.02381
I0630 03:21:12.173738 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 03:21:12.173746 29777 sgd_solver.cpp:106] Iteration 23100, lr = 0.00927813
I0630 03:21:28.152621 29777 solver.cpp:290] Iteration 23200 (6.25843 iter/s, 15.9784s/100 iter), loss = 1.08333
I0630 03:21:28.152699 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 03:21:28.152710 29777 sgd_solver.cpp:106] Iteration 23200, lr = 0.009275
I0630 03:21:44.199025 29777 solver.cpp:290] Iteration 23300 (6.23213 iter/s, 16.0459s/100 iter), loss = 0.904762
I0630 03:21:44.199048 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 03:21:44.199055 29777 sgd_solver.cpp:106] Iteration 23300, lr = 0.00927188
I0630 03:22:00.189292 29777 solver.cpp:290] Iteration 23400 (6.25399 iter/s, 15.9898s/100 iter), loss = 1.16667
I0630 03:22:00.189386 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 03:22:00.189398 29777 sgd_solver.cpp:106] Iteration 23400, lr = 0.00926875
I0630 03:22:16.191864 29777 solver.cpp:290] Iteration 23500 (6.2492 iter/s, 16.002s/100 iter), loss = 0.952381
I0630 03:22:16.191890 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 03:22:16.191896 29777 sgd_solver.cpp:106] Iteration 23500, lr = 0.00926562
I0630 03:22:32.245888 29777 solver.cpp:290] Iteration 23600 (6.22915 iter/s, 16.0536s/100 iter), loss = 0.595238
I0630 03:22:32.245990 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 03:22:32.246001 29777 sgd_solver.cpp:106] Iteration 23600, lr = 0.0092625
I0630 03:22:48.348256 29777 solver.cpp:290] Iteration 23700 (6.21048 iter/s, 16.1018s/100 iter), loss = 0.952381
I0630 03:22:48.348291 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 03:22:48.348347 29777 sgd_solver.cpp:106] Iteration 23700, lr = 0.00925937
I0630 03:23:04.521106 29777 solver.cpp:290] Iteration 23800 (6.18339 iter/s, 16.1724s/100 iter), loss = 0.976191
I0630 03:23:04.521236 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 03:23:04.521268 29777 sgd_solver.cpp:106] Iteration 23800, lr = 0.00925625
I0630 03:23:20.699692 29777 solver.cpp:290] Iteration 23900 (6.18123 iter/s, 16.178s/100 iter), loss = 1.20238
I0630 03:23:20.699714 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 03:23:20.699720 29777 sgd_solver.cpp:106] Iteration 23900, lr = 0.00925312
I0630 03:23:36.678570 29777 solver.cpp:354] Sparsity after update:
I0630 03:23:36.679839 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:23:36.679847 29777 net.cpp:1851] conv1a_param_0(0) 
I0630 03:23:36.679853 29777 net.cpp:1851] conv1b_param_0(0.12) 
I0630 03:23:36.679857 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:23:36.679858 29777 net.cpp:1851] res2a_branch2a_param_0(0.12) 
I0630 03:23:36.679860 29777 net.cpp:1851] res2a_branch2b_param_0(0.12) 
I0630 03:23:36.679862 29777 net.cpp:1851] res3a_branch2a_param_0(0.12) 
I0630 03:23:36.679864 29777 net.cpp:1851] res3a_branch2b_param_0(0.12) 
I0630 03:23:36.679867 29777 net.cpp:1851] res4a_branch2a_param_0(0.12) 
I0630 03:23:36.679868 29777 net.cpp:1851] res4a_branch2b_param_0(0.12) 
I0630 03:23:36.679870 29777 net.cpp:1851] res5a_branch2a_param_0(0.12) 
I0630 03:23:36.679872 29777 net.cpp:1851] res5a_branch2b_param_0(0.12) 
I0630 03:23:36.679874 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (282275/2.86678e+06) 0.0985
I0630 03:23:36.679960 29777 solver.cpp:471] Iteration 24000, Testing net (#0)
I0630 03:23:39.087311 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 03:24:36.234496 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55328
I0630 03:24:36.234539 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.793701
I0630 03:24:36.234544 29777 solver.cpp:544]     Test net output #2: loss = 1.59088 (* 1 = 1.59088 loss)
I0630 03:24:36.416792 29777 solver.cpp:290] Iteration 24000 (1.32074 iter/s, 75.715s/100 iter), loss = 1.40476
I0630 03:24:36.416816 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 03:24:36.416822 29777 sgd_solver.cpp:106] Iteration 24000, lr = 0.00925
I0630 03:24:36.417531 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.13
I0630 03:24:36.543941 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 03:24:52.543066 29777 solver.cpp:290] Iteration 24100 (6.20124 iter/s, 16.1258s/100 iter), loss = 1.20238
I0630 03:24:52.543087 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 03:24:52.543094 29777 sgd_solver.cpp:106] Iteration 24100, lr = 0.00924687
I0630 03:25:08.605882 29777 solver.cpp:290] Iteration 24200 (6.22574 iter/s, 16.0624s/100 iter), loss = 0.880952
I0630 03:25:08.605988 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 03:25:08.605998 29777 sgd_solver.cpp:106] Iteration 24200, lr = 0.00924375
I0630 03:25:24.646394 29777 solver.cpp:290] Iteration 24300 (6.23443 iter/s, 16.04s/100 iter), loss = 1.13095
I0630 03:25:24.646420 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 03:25:24.646430 29777 sgd_solver.cpp:106] Iteration 24300, lr = 0.00924062
I0630 03:25:40.661099 29777 solver.cpp:290] Iteration 24400 (6.24444 iter/s, 16.0142s/100 iter), loss = 1.07143
I0630 03:25:40.661206 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 03:25:40.661216 29777 sgd_solver.cpp:106] Iteration 24400, lr = 0.0092375
I0630 03:25:56.668419 29777 solver.cpp:290] Iteration 24500 (6.24735 iter/s, 16.0068s/100 iter), loss = 1.59524
I0630 03:25:56.668443 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 03:25:56.668450 29777 sgd_solver.cpp:106] Iteration 24500, lr = 0.00923437
I0630 03:26:12.639775 29777 solver.cpp:290] Iteration 24600 (6.26139 iter/s, 15.9709s/100 iter), loss = 1.21429
I0630 03:26:12.639873 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 03:26:12.639883 29777 sgd_solver.cpp:106] Iteration 24600, lr = 0.00923125
I0630 03:26:28.635112 29777 solver.cpp:290] Iteration 24700 (6.25203 iter/s, 15.9948s/100 iter), loss = 1.19048
I0630 03:26:28.635138 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 03:26:28.635146 29777 sgd_solver.cpp:106] Iteration 24700, lr = 0.00922813
I0630 03:26:44.552132 29777 solver.cpp:290] Iteration 24800 (6.28277 iter/s, 15.9166s/100 iter), loss = 1.29762
I0630 03:26:44.552232 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 03:26:44.552242 29777 sgd_solver.cpp:106] Iteration 24800, lr = 0.009225
I0630 03:27:00.530334 29777 solver.cpp:290] Iteration 24900 (6.25874 iter/s, 15.9777s/100 iter), loss = 0.702381
I0630 03:27:00.530357 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 03:27:00.530364 29777 sgd_solver.cpp:106] Iteration 24900, lr = 0.00922188
I0630 03:27:16.351887 29777 solver.cpp:354] Sparsity after update:
I0630 03:27:16.372227 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:27:16.372262 29777 net.cpp:1851] conv1a_param_0(0.0646) 
I0630 03:27:16.372282 29777 net.cpp:1851] conv1b_param_0(0.13) 
I0630 03:27:16.372292 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:27:16.372301 29777 net.cpp:1851] res2a_branch2a_param_0(0.13) 
I0630 03:27:16.372313 29777 net.cpp:1851] res2a_branch2b_param_0(0.13) 
I0630 03:27:16.372323 29777 net.cpp:1851] res3a_branch2a_param_0(0.13) 
I0630 03:27:16.372328 29777 net.cpp:1851] res3a_branch2b_param_0(0.13) 
I0630 03:27:16.372333 29777 net.cpp:1851] res4a_branch2a_param_0(0.13) 
I0630 03:27:16.372336 29777 net.cpp:1851] res4a_branch2b_param_0(0.13) 
I0630 03:27:16.372340 29777 net.cpp:1851] res5a_branch2a_param_0(0.13) 
I0630 03:27:16.372349 29777 net.cpp:1851] res5a_branch2b_param_0(0.13) 
I0630 03:27:16.372360 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (305946/2.86678e+06) 0.107
I0630 03:27:16.528233 29777 solver.cpp:290] Iteration 25000 (6.251 iter/s, 15.9974s/100 iter), loss = 1.25
I0630 03:27:16.528259 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 03:27:16.528267 29777 sgd_solver.cpp:106] Iteration 25000, lr = 0.00921875
I0630 03:27:32.638334 29777 solver.cpp:290] Iteration 25100 (6.20747 iter/s, 16.1096s/100 iter), loss = 1.05952
I0630 03:27:32.638357 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 03:27:32.638365 29777 sgd_solver.cpp:106] Iteration 25100, lr = 0.00921563
I0630 03:27:48.662513 29777 solver.cpp:290] Iteration 25200 (6.24075 iter/s, 16.0237s/100 iter), loss = 1.29762
I0630 03:27:48.662581 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 03:27:48.662590 29777 sgd_solver.cpp:106] Iteration 25200, lr = 0.0092125
I0630 03:28:04.686296 29777 solver.cpp:290] Iteration 25300 (6.24092 iter/s, 16.0233s/100 iter), loss = 0.964286
I0630 03:28:04.686321 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 03:28:04.686326 29777 sgd_solver.cpp:106] Iteration 25300, lr = 0.00920937
I0630 03:28:20.787437 29777 solver.cpp:290] Iteration 25400 (6.21092 iter/s, 16.1007s/100 iter), loss = 0.904762
I0630 03:28:20.787552 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 03:28:20.787564 29777 sgd_solver.cpp:106] Iteration 25400, lr = 0.00920625
I0630 03:28:37.551726 29777 solver.cpp:290] Iteration 25500 (5.96526 iter/s, 16.7637s/100 iter), loss = 1.10714
I0630 03:28:37.551753 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 03:28:37.551762 29777 sgd_solver.cpp:106] Iteration 25500, lr = 0.00920312
I0630 03:28:54.054370 29777 solver.cpp:290] Iteration 25600 (6.05981 iter/s, 16.5022s/100 iter), loss = 1.10714
I0630 03:28:54.054450 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 03:28:54.054461 29777 sgd_solver.cpp:106] Iteration 25600, lr = 0.0092
I0630 03:29:10.283329 29777 solver.cpp:290] Iteration 25700 (6.16202 iter/s, 16.2284s/100 iter), loss = 1.22619
I0630 03:29:10.283355 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 03:29:10.283365 29777 sgd_solver.cpp:106] Iteration 25700, lr = 0.00919687
I0630 03:29:26.576666 29777 solver.cpp:290] Iteration 25800 (6.13766 iter/s, 16.2929s/100 iter), loss = 0.869048
I0630 03:29:26.576716 29777 solver.cpp:309]     Train net output #0: loss = 0.690476 (* 1 = 0.690476 loss)
I0630 03:29:26.576725 29777 sgd_solver.cpp:106] Iteration 25800, lr = 0.00919375
I0630 03:29:42.857398 29777 solver.cpp:290] Iteration 25900 (6.14242 iter/s, 16.2802s/100 iter), loss = 0.654762
I0630 03:29:42.857421 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 03:29:42.857429 29777 sgd_solver.cpp:106] Iteration 25900, lr = 0.00919062
I0630 03:29:59.148258 29777 solver.cpp:354] Sparsity after update:
I0630 03:29:59.149868 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:29:59.149875 29777 net.cpp:1851] conv1a_param_0(0.0646) 
I0630 03:29:59.149886 29777 net.cpp:1851] conv1b_param_0(0.13) 
I0630 03:29:59.149891 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:29:59.149895 29777 net.cpp:1851] res2a_branch2a_param_0(0.13) 
I0630 03:29:59.149899 29777 net.cpp:1851] res2a_branch2b_param_0(0.13) 
I0630 03:29:59.149904 29777 net.cpp:1851] res3a_branch2a_param_0(0.13) 
I0630 03:29:59.149907 29777 net.cpp:1851] res3a_branch2b_param_0(0.13) 
I0630 03:29:59.149911 29777 net.cpp:1851] res4a_branch2a_param_0(0.13) 
I0630 03:29:59.149915 29777 net.cpp:1851] res4a_branch2b_param_0(0.13) 
I0630 03:29:59.149919 29777 net.cpp:1851] res5a_branch2a_param_0(0.13) 
I0630 03:29:59.149924 29777 net.cpp:1851] res5a_branch2b_param_0(0.13) 
I0630 03:29:59.149927 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (305946/2.86678e+06) 0.107
I0630 03:29:59.150018 29777 solver.cpp:471] Iteration 26000, Testing net (#0)
I0630 03:30:02.349386 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 03:30:53.235837 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56438
I0630 03:30:53.235888 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.798901
I0630 03:30:53.235898 29777 solver.cpp:544]     Test net output #2: loss = 1.55636 (* 1 = 1.55636 loss)
I0630 03:30:53.407361 29777 solver.cpp:290] Iteration 26000 (1.41747 iter/s, 70.548s/100 iter), loss = 0.857143
I0630 03:30:53.407384 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 03:30:53.407390 29777 sgd_solver.cpp:106] Iteration 26000, lr = 0.0091875
I0630 03:30:53.408066 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.14
I0630 03:30:53.542979 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 03:31:09.435297 29777 solver.cpp:290] Iteration 26100 (6.23929 iter/s, 16.0275s/100 iter), loss = 1.19048
I0630 03:31:09.435322 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 03:31:09.435331 29777 sgd_solver.cpp:106] Iteration 26100, lr = 0.00918437
I0630 03:31:25.504246 29777 solver.cpp:290] Iteration 26200 (6.22336 iter/s, 16.0685s/100 iter), loss = 1.13095
I0630 03:31:25.504369 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 03:31:25.504379 29777 sgd_solver.cpp:106] Iteration 26200, lr = 0.00918125
I0630 03:31:41.498842 29777 solver.cpp:290] Iteration 26300 (6.25233 iter/s, 15.994s/100 iter), loss = 1.03571
I0630 03:31:41.498867 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 03:31:41.498872 29777 sgd_solver.cpp:106] Iteration 26300, lr = 0.00917812
I0630 03:31:57.430047 29777 solver.cpp:290] Iteration 26400 (6.27717 iter/s, 15.9307s/100 iter), loss = 1.13095
I0630 03:31:57.430136 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 03:31:57.430147 29777 sgd_solver.cpp:106] Iteration 26400, lr = 0.009175
I0630 03:32:13.451997 29777 solver.cpp:290] Iteration 26500 (6.24164 iter/s, 16.0214s/100 iter), loss = 0.952381
I0630 03:32:13.452019 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 03:32:13.452028 29777 sgd_solver.cpp:106] Iteration 26500, lr = 0.00917188
I0630 03:32:29.442997 29777 solver.cpp:290] Iteration 26600 (6.2537 iter/s, 15.9905s/100 iter), loss = 1.16667
I0630 03:32:29.443073 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 03:32:29.443081 29777 sgd_solver.cpp:106] Iteration 26600, lr = 0.00916875
I0630 03:32:45.363565 29777 solver.cpp:290] Iteration 26700 (6.28139 iter/s, 15.9201s/100 iter), loss = 0.964286
I0630 03:32:45.363590 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 03:32:45.363598 29777 sgd_solver.cpp:106] Iteration 26700, lr = 0.00916563
I0630 03:33:01.448293 29777 solver.cpp:290] Iteration 26800 (6.21726 iter/s, 16.0843s/100 iter), loss = 1.29762
I0630 03:33:01.448364 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 03:33:01.448371 29777 sgd_solver.cpp:106] Iteration 26800, lr = 0.0091625
I0630 03:33:17.585579 29777 solver.cpp:290] Iteration 26900 (6.19703 iter/s, 16.1368s/100 iter), loss = 1.04762
I0630 03:33:17.585605 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 03:33:17.585613 29777 sgd_solver.cpp:106] Iteration 26900, lr = 0.00915937
I0630 03:33:33.598153 29777 solver.cpp:354] Sparsity after update:
I0630 03:33:33.618401 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:33:33.618417 29777 net.cpp:1851] conv1a_param_0(0.0696) 
I0630 03:33:33.618428 29777 net.cpp:1851] conv1b_param_0(0.14) 
I0630 03:33:33.618432 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:33:33.618438 29777 net.cpp:1851] res2a_branch2a_param_0(0.14) 
I0630 03:33:33.618440 29777 net.cpp:1851] res2a_branch2b_param_0(0.14) 
I0630 03:33:33.618443 29777 net.cpp:1851] res3a_branch2a_param_0(0.14) 
I0630 03:33:33.618448 29777 net.cpp:1851] res3a_branch2b_param_0(0.14) 
I0630 03:33:33.618450 29777 net.cpp:1851] res4a_branch2a_param_0(0.14) 
I0630 03:33:33.618453 29777 net.cpp:1851] res4a_branch2b_param_0(0.14) 
I0630 03:33:33.618456 29777 net.cpp:1851] res5a_branch2a_param_0(0.14) 
I0630 03:33:33.618460 29777 net.cpp:1851] res5a_branch2b_param_0(0.14) 
I0630 03:33:33.618463 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (329481/2.86678e+06) 0.115
I0630 03:33:33.778237 29777 solver.cpp:290] Iteration 27000 (6.17582 iter/s, 16.1922s/100 iter), loss = 1.15476
I0630 03:33:33.778275 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 03:33:33.778285 29777 sgd_solver.cpp:106] Iteration 27000, lr = 0.00915625
I0630 03:33:50.055773 29777 solver.cpp:290] Iteration 27100 (6.14362 iter/s, 16.2771s/100 iter), loss = 1.04762
I0630 03:33:50.055799 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 03:33:50.055807 29777 sgd_solver.cpp:106] Iteration 27100, lr = 0.00915312
I0630 03:34:06.117333 29777 solver.cpp:290] Iteration 27200 (6.22623 iter/s, 16.0611s/100 iter), loss = 1.28571
I0630 03:34:06.117455 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 03:34:06.117465 29777 sgd_solver.cpp:106] Iteration 27200, lr = 0.00915
I0630 03:34:22.288080 29777 solver.cpp:290] Iteration 27300 (6.18422 iter/s, 16.1702s/100 iter), loss = 0.892857
I0630 03:34:22.288106 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 03:34:22.288113 29777 sgd_solver.cpp:106] Iteration 27300, lr = 0.00914687
I0630 03:34:38.656052 29777 solver.cpp:290] Iteration 27400 (6.10967 iter/s, 16.3675s/100 iter), loss = 1.13095
I0630 03:34:38.656142 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 03:34:38.656152 29777 sgd_solver.cpp:106] Iteration 27400, lr = 0.00914375
I0630 03:34:54.929719 29777 solver.cpp:290] Iteration 27500 (6.1451 iter/s, 16.2731s/100 iter), loss = 1.44048
I0630 03:34:54.929745 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 03:34:54.929754 29777 sgd_solver.cpp:106] Iteration 27500, lr = 0.00914062
I0630 03:35:11.052223 29777 solver.cpp:290] Iteration 27600 (6.2027 iter/s, 16.122s/100 iter), loss = 1.17857
I0630 03:35:11.052362 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 03:35:11.052407 29777 sgd_solver.cpp:106] Iteration 27600, lr = 0.0091375
I0630 03:35:27.187908 29777 solver.cpp:290] Iteration 27700 (6.19766 iter/s, 16.1351s/100 iter), loss = 1.16667
I0630 03:35:27.187932 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 03:35:27.187938 29777 sgd_solver.cpp:106] Iteration 27700, lr = 0.00913437
I0630 03:35:43.261410 29777 solver.cpp:290] Iteration 27800 (6.2216 iter/s, 16.073s/100 iter), loss = 1.22619
I0630 03:35:43.261504 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 03:35:43.261515 29777 sgd_solver.cpp:106] Iteration 27800, lr = 0.00913125
I0630 03:35:59.480319 29777 solver.cpp:290] Iteration 27900 (6.16585 iter/s, 16.2184s/100 iter), loss = 0.97619
I0630 03:35:59.480343 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 03:35:59.480353 29777 sgd_solver.cpp:106] Iteration 27900, lr = 0.00912812
I0630 03:36:15.534498 29777 solver.cpp:354] Sparsity after update:
I0630 03:36:15.536118 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:36:15.536125 29777 net.cpp:1851] conv1a_param_0(0.0696) 
I0630 03:36:15.536131 29777 net.cpp:1851] conv1b_param_0(0.14) 
I0630 03:36:15.536134 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:36:15.536136 29777 net.cpp:1851] res2a_branch2a_param_0(0.14) 
I0630 03:36:15.536139 29777 net.cpp:1851] res2a_branch2b_param_0(0.14) 
I0630 03:36:15.536140 29777 net.cpp:1851] res3a_branch2a_param_0(0.14) 
I0630 03:36:15.536142 29777 net.cpp:1851] res3a_branch2b_param_0(0.14) 
I0630 03:36:15.536144 29777 net.cpp:1851] res4a_branch2a_param_0(0.14) 
I0630 03:36:15.536146 29777 net.cpp:1851] res4a_branch2b_param_0(0.14) 
I0630 03:36:15.536147 29777 net.cpp:1851] res5a_branch2a_param_0(0.14) 
I0630 03:36:15.536149 29777 net.cpp:1851] res5a_branch2b_param_0(0.14) 
I0630 03:36:15.536152 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (329481/2.86678e+06) 0.115
I0630 03:36:15.536247 29777 solver.cpp:471] Iteration 28000, Testing net (#0)
I0630 03:36:18.550643 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 03:37:13.381738 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5643
I0630 03:37:13.381789 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.798461
I0630 03:37:13.381795 29777 solver.cpp:544]     Test net output #2: loss = 1.57284 (* 1 = 1.57284 loss)
I0630 03:37:13.560348 29777 solver.cpp:290] Iteration 28000 (1.34993 iter/s, 74.078s/100 iter), loss = 1.10714
I0630 03:37:13.560374 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 03:37:13.560382 29777 sgd_solver.cpp:106] Iteration 28000, lr = 0.009125
I0630 03:37:13.561390 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.15
I0630 03:37:13.699188 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 03:37:29.820039 29777 solver.cpp:290] Iteration 28100 (6.15036 iter/s, 16.2592s/100 iter), loss = 1.19048
I0630 03:37:29.820065 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 03:37:29.820073 29777 sgd_solver.cpp:106] Iteration 28100, lr = 0.00912188
I0630 03:37:45.824158 29777 solver.cpp:290] Iteration 28200 (6.24857 iter/s, 16.0037s/100 iter), loss = 1.10714
I0630 03:37:45.824282 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 03:37:45.824292 29777 sgd_solver.cpp:106] Iteration 28200, lr = 0.00911875
I0630 03:38:01.798598 29777 solver.cpp:290] Iteration 28300 (6.26022 iter/s, 15.9739s/100 iter), loss = 1.07143
I0630 03:38:01.798620 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 03:38:01.798627 29777 sgd_solver.cpp:106] Iteration 28300, lr = 0.00911563
I0630 03:38:18.119030 29777 solver.cpp:290] Iteration 28400 (6.12747 iter/s, 16.32s/100 iter), loss = 1.42857
I0630 03:38:18.119091 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 03:38:18.119102 29777 sgd_solver.cpp:106] Iteration 28400, lr = 0.0091125
I0630 03:38:34.254365 29777 solver.cpp:290] Iteration 28500 (6.19777 iter/s, 16.1348s/100 iter), loss = 0.97619
I0630 03:38:34.254532 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 03:38:34.254609 29777 sgd_solver.cpp:106] Iteration 28500, lr = 0.00910938
I0630 03:38:50.356402 29777 solver.cpp:290] Iteration 28600 (6.21063 iter/s, 16.1014s/100 iter), loss = 1.39286
I0630 03:38:50.356503 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 03:38:50.356513 29777 sgd_solver.cpp:106] Iteration 28600, lr = 0.00910625
I0630 03:39:06.409301 29777 solver.cpp:290] Iteration 28700 (6.22961 iter/s, 16.0524s/100 iter), loss = 1.10714
I0630 03:39:06.409327 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 03:39:06.409334 29777 sgd_solver.cpp:106] Iteration 28700, lr = 0.00910312
I0630 03:39:22.649255 29777 solver.cpp:290] Iteration 28800 (6.15783 iter/s, 16.2395s/100 iter), loss = 1.22619
I0630 03:39:22.649350 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 03:39:22.649361 29777 sgd_solver.cpp:106] Iteration 28800, lr = 0.0091
I0630 03:39:38.977210 29777 solver.cpp:290] Iteration 28900 (6.12467 iter/s, 16.3274s/100 iter), loss = 1.39286
I0630 03:39:38.977237 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 03:39:38.977252 29777 sgd_solver.cpp:106] Iteration 28900, lr = 0.00909687
I0630 03:39:55.298318 29777 solver.cpp:354] Sparsity after update:
I0630 03:39:55.323714 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:39:55.323736 29777 net.cpp:1851] conv1a_param_0(0.0746) 
I0630 03:39:55.323750 29777 net.cpp:1851] conv1b_param_0(0.15) 
I0630 03:39:55.323755 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:39:55.323758 29777 net.cpp:1851] res2a_branch2a_param_0(0.15) 
I0630 03:39:55.323762 29777 net.cpp:1851] res2a_branch2b_param_0(0.15) 
I0630 03:39:55.323766 29777 net.cpp:1851] res3a_branch2a_param_0(0.15) 
I0630 03:39:55.323770 29777 net.cpp:1851] res3a_branch2b_param_0(0.15) 
I0630 03:39:55.323773 29777 net.cpp:1851] res4a_branch2a_param_0(0.15) 
I0630 03:39:55.323777 29777 net.cpp:1851] res4a_branch2b_param_0(0.15) 
I0630 03:39:55.323784 29777 net.cpp:1851] res5a_branch2a_param_0(0.15) 
I0630 03:39:55.323791 29777 net.cpp:1851] res5a_branch2b_param_0(0.15) 
I0630 03:39:55.323796 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (353028/2.86678e+06) 0.123
I0630 03:39:55.478513 29777 solver.cpp:290] Iteration 29000 (6.0603 iter/s, 16.5008s/100 iter), loss = 1.09524
I0630 03:39:55.478538 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 03:39:55.478549 29777 sgd_solver.cpp:106] Iteration 29000, lr = 0.00909375
I0630 03:40:11.625998 29777 solver.cpp:290] Iteration 29100 (6.1931 iter/s, 16.147s/100 iter), loss = 0.916667
I0630 03:40:11.626214 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 03:40:11.628208 29777 sgd_solver.cpp:106] Iteration 29100, lr = 0.00909062
I0630 03:40:27.981209 29777 solver.cpp:290] Iteration 29200 (6.11525 iter/s, 16.3526s/100 iter), loss = 0.761905
I0630 03:40:27.981333 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 03:40:27.981341 29777 sgd_solver.cpp:106] Iteration 29200, lr = 0.0090875
I0630 03:40:44.194767 29777 solver.cpp:290] Iteration 29300 (6.1679 iter/s, 16.213s/100 iter), loss = 0.940476
I0630 03:40:44.195057 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 03:40:44.195197 29777 sgd_solver.cpp:106] Iteration 29300, lr = 0.00908437
I0630 03:41:00.740064 29777 solver.cpp:290] Iteration 29400 (6.04428 iter/s, 16.5446s/100 iter), loss = 1.14286
I0630 03:41:00.740157 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 03:41:00.740169 29777 sgd_solver.cpp:106] Iteration 29400, lr = 0.00908125
I0630 03:41:17.143558 29777 solver.cpp:290] Iteration 29500 (6.09646 iter/s, 16.403s/100 iter), loss = 1.07143
I0630 03:41:17.143581 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 03:41:17.143587 29777 sgd_solver.cpp:106] Iteration 29500, lr = 0.00907812
I0630 03:41:33.500082 29777 solver.cpp:290] Iteration 29600 (6.11395 iter/s, 16.356s/100 iter), loss = 1.13095
I0630 03:41:33.500342 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 03:41:33.500469 29777 sgd_solver.cpp:106] Iteration 29600, lr = 0.009075
I0630 03:41:49.821794 29777 solver.cpp:290] Iteration 29700 (6.12707 iter/s, 16.321s/100 iter), loss = 1.08333
I0630 03:41:49.821820 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 03:41:49.821826 29777 sgd_solver.cpp:106] Iteration 29700, lr = 0.00907188
I0630 03:42:06.392545 29777 solver.cpp:290] Iteration 29800 (6.03491 iter/s, 16.5702s/100 iter), loss = 1.14286
I0630 03:42:06.392714 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 03:42:06.392758 29777 sgd_solver.cpp:106] Iteration 29800, lr = 0.00906875
I0630 03:42:22.804392 29777 solver.cpp:290] Iteration 29900 (6.09339 iter/s, 16.4112s/100 iter), loss = 0.952381
I0630 03:42:22.804447 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 03:42:22.804471 29777 sgd_solver.cpp:106] Iteration 29900, lr = 0.00906563
I0630 03:42:38.802541 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_30000.caffemodel
I0630 03:42:38.822203 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_30000.solverstate
I0630 03:42:38.831228 29777 solver.cpp:354] Sparsity after update:
I0630 03:42:38.832204 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:42:38.832213 29777 net.cpp:1851] conv1a_param_0(0.0746) 
I0630 03:42:38.832221 29777 net.cpp:1851] conv1b_param_0(0.15) 
I0630 03:42:38.832222 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:42:38.832224 29777 net.cpp:1851] res2a_branch2a_param_0(0.15) 
I0630 03:42:38.832226 29777 net.cpp:1851] res2a_branch2b_param_0(0.15) 
I0630 03:42:38.832228 29777 net.cpp:1851] res3a_branch2a_param_0(0.15) 
I0630 03:42:38.832231 29777 net.cpp:1851] res3a_branch2b_param_0(0.15) 
I0630 03:42:38.832232 29777 net.cpp:1851] res4a_branch2a_param_0(0.15) 
I0630 03:42:38.832234 29777 net.cpp:1851] res4a_branch2b_param_0(0.15) 
I0630 03:42:38.832237 29777 net.cpp:1851] res5a_branch2a_param_0(0.15) 
I0630 03:42:38.832238 29777 net.cpp:1851] res5a_branch2b_param_0(0.15) 
I0630 03:42:38.832240 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (353028/2.86678e+06) 0.123
I0630 03:42:38.832335 29777 solver.cpp:471] Iteration 30000, Testing net (#0)
I0630 03:42:42.954138 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 03:43:45.913530 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55084
I0630 03:43:45.913589 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.791641
I0630 03:43:45.913595 29777 solver.cpp:544]     Test net output #2: loss = 1.61132 (* 1 = 1.61132 loss)
I0630 03:43:46.091377 29777 solver.cpp:290] Iteration 30000 (1.2007 iter/s, 83.2847s/100 iter), loss = 0.690476
I0630 03:43:46.091399 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 03:43:46.091405 29777 sgd_solver.cpp:106] Iteration 30000, lr = 0.0090625
I0630 03:43:46.092080 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.16
I0630 03:43:46.228162 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 03:44:02.637657 29777 solver.cpp:290] Iteration 30100 (6.04383 iter/s, 16.5458s/100 iter), loss = 0.797619
I0630 03:44:02.637687 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 03:44:02.637696 29777 sgd_solver.cpp:106] Iteration 30100, lr = 0.00905938
I0630 03:44:19.179137 29777 solver.cpp:290] Iteration 30200 (6.04559 iter/s, 16.541s/100 iter), loss = 1.22619
I0630 03:44:19.179229 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 03:44:19.179239 29777 sgd_solver.cpp:106] Iteration 30200, lr = 0.00905625
I0630 03:44:35.613289 29777 solver.cpp:290] Iteration 30300 (6.08509 iter/s, 16.4336s/100 iter), loss = 1.02381
I0630 03:44:35.613312 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 03:44:35.613319 29777 sgd_solver.cpp:106] Iteration 30300, lr = 0.00905313
I0630 03:44:51.910449 29777 solver.cpp:290] Iteration 30400 (6.13622 iter/s, 16.2967s/100 iter), loss = 0.964286
I0630 03:44:51.910554 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 03:44:51.910564 29777 sgd_solver.cpp:106] Iteration 30400, lr = 0.00905
I0630 03:45:08.189551 29777 solver.cpp:290] Iteration 30500 (6.14305 iter/s, 16.2786s/100 iter), loss = 1.16667
I0630 03:45:08.189574 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 03:45:08.189581 29777 sgd_solver.cpp:106] Iteration 30500, lr = 0.00904687
I0630 03:45:25.064513 29777 solver.cpp:290] Iteration 30600 (5.92611 iter/s, 16.8745s/100 iter), loss = 1.2619
I0630 03:45:25.064589 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 03:45:25.064610 29777 sgd_solver.cpp:106] Iteration 30600, lr = 0.00904375
I0630 03:45:41.453068 29777 solver.cpp:290] Iteration 30700 (6.10202 iter/s, 16.388s/100 iter), loss = 1.40476
I0630 03:45:41.453128 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 03:45:41.453161 29777 sgd_solver.cpp:106] Iteration 30700, lr = 0.00904062
I0630 03:45:57.450814 29777 solver.cpp:290] Iteration 30800 (6.25107 iter/s, 15.9973s/100 iter), loss = 0.97619
I0630 03:45:57.450875 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 03:45:57.450889 29777 sgd_solver.cpp:106] Iteration 30800, lr = 0.0090375
I0630 03:46:13.813745 29777 solver.cpp:290] Iteration 30900 (6.11157 iter/s, 16.3624s/100 iter), loss = 1.16667
I0630 03:46:13.813807 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 03:46:13.813822 29777 sgd_solver.cpp:106] Iteration 30900, lr = 0.00903437
I0630 03:46:30.283251 29777 solver.cpp:354] Sparsity after update:
I0630 03:46:30.303735 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:46:30.303752 29777 net.cpp:1851] conv1a_param_0(0.08) 
I0630 03:46:30.303763 29777 net.cpp:1851] conv1b_param_0(0.16) 
I0630 03:46:30.303767 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:46:30.303771 29777 net.cpp:1851] res2a_branch2a_param_0(0.16) 
I0630 03:46:30.303774 29777 net.cpp:1851] res2a_branch2b_param_0(0.16) 
I0630 03:46:30.303777 29777 net.cpp:1851] res3a_branch2a_param_0(0.16) 
I0630 03:46:30.303781 29777 net.cpp:1851] res3a_branch2b_param_0(0.16) 
I0630 03:46:30.303783 29777 net.cpp:1851] res4a_branch2a_param_0(0.16) 
I0630 03:46:30.303786 29777 net.cpp:1851] res4a_branch2b_param_0(0.16) 
I0630 03:46:30.303789 29777 net.cpp:1851] res5a_branch2a_param_0(0.16) 
I0630 03:46:30.303793 29777 net.cpp:1851] res5a_branch2b_param_0(0.16) 
I0630 03:46:30.303797 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (376557/2.86678e+06) 0.131
I0630 03:46:30.462828 29777 solver.cpp:290] Iteration 31000 (6.00652 iter/s, 16.6486s/100 iter), loss = 0.940476
I0630 03:46:30.462888 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 03:46:30.462898 29777 sgd_solver.cpp:106] Iteration 31000, lr = 0.00903125
I0630 03:46:46.863420 29777 solver.cpp:290] Iteration 31100 (6.09753 iter/s, 16.4001s/100 iter), loss = 1.10714
I0630 03:46:46.863445 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 03:46:46.863453 29777 sgd_solver.cpp:106] Iteration 31100, lr = 0.00902812
I0630 03:47:03.038067 29777 solver.cpp:290] Iteration 31200 (6.18269 iter/s, 16.1742s/100 iter), loss = 1.0119
I0630 03:47:03.038151 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 03:47:03.038161 29777 sgd_solver.cpp:106] Iteration 31200, lr = 0.009025
I0630 03:47:19.176632 29777 solver.cpp:290] Iteration 31300 (6.19654 iter/s, 16.138s/100 iter), loss = 1.39286
I0630 03:47:19.176659 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 03:47:19.176667 29777 sgd_solver.cpp:106] Iteration 31300, lr = 0.00902187
I0630 03:47:35.494935 29777 solver.cpp:290] Iteration 31400 (6.12827 iter/s, 16.3178s/100 iter), loss = 0.928571
I0630 03:47:35.495038 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 03:47:35.495052 29777 sgd_solver.cpp:106] Iteration 31400, lr = 0.00901875
I0630 03:47:51.980615 29777 solver.cpp:290] Iteration 31500 (6.06607 iter/s, 16.4851s/100 iter), loss = 0.97619
I0630 03:47:51.980641 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 03:47:51.980648 29777 sgd_solver.cpp:106] Iteration 31500, lr = 0.00901563
I0630 03:48:08.106639 29777 solver.cpp:290] Iteration 31600 (6.20134 iter/s, 16.1256s/100 iter), loss = 1.30952
I0630 03:48:08.106712 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 03:48:08.106720 29777 sgd_solver.cpp:106] Iteration 31600, lr = 0.0090125
I0630 03:48:24.265293 29777 solver.cpp:290] Iteration 31700 (6.18883 iter/s, 16.1581s/100 iter), loss = 1.03571
I0630 03:48:24.265317 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 03:48:24.265324 29777 sgd_solver.cpp:106] Iteration 31700, lr = 0.00900938
I0630 03:48:40.649991 29777 solver.cpp:290] Iteration 31800 (6.10343 iter/s, 16.3842s/100 iter), loss = 0.964286
I0630 03:48:40.650084 29777 solver.cpp:309]     Train net output #0: loss = 0.404762 (* 1 = 0.404762 loss)
I0630 03:48:40.650108 29777 sgd_solver.cpp:106] Iteration 31800, lr = 0.00900625
I0630 03:48:56.859874 29777 solver.cpp:290] Iteration 31900 (6.16928 iter/s, 16.2094s/100 iter), loss = 1.11905
I0630 03:48:56.859901 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 03:48:56.859910 29777 sgd_solver.cpp:106] Iteration 31900, lr = 0.00900312
I0630 03:49:12.934684 29777 solver.cpp:354] Sparsity after update:
I0630 03:49:12.936123 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:49:12.936131 29777 net.cpp:1851] conv1a_param_0(0.08) 
I0630 03:49:12.936138 29777 net.cpp:1851] conv1b_param_0(0.16) 
I0630 03:49:12.936141 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:49:12.936143 29777 net.cpp:1851] res2a_branch2a_param_0(0.16) 
I0630 03:49:12.936146 29777 net.cpp:1851] res2a_branch2b_param_0(0.16) 
I0630 03:49:12.936147 29777 net.cpp:1851] res3a_branch2a_param_0(0.16) 
I0630 03:49:12.936151 29777 net.cpp:1851] res3a_branch2b_param_0(0.16) 
I0630 03:49:12.936152 29777 net.cpp:1851] res4a_branch2a_param_0(0.16) 
I0630 03:49:12.936154 29777 net.cpp:1851] res4a_branch2b_param_0(0.16) 
I0630 03:49:12.936156 29777 net.cpp:1851] res5a_branch2a_param_0(0.16) 
I0630 03:49:12.936159 29777 net.cpp:1851] res5a_branch2b_param_0(0.16) 
I0630 03:49:12.936161 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (376557/2.86678e+06) 0.131
I0630 03:49:12.936250 29777 solver.cpp:471] Iteration 32000, Testing net (#0)
I0630 03:49:16.565232 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 03:50:13.470932 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56286
I0630 03:50:13.471000 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.796581
I0630 03:50:13.471007 29777 solver.cpp:544]     Test net output #2: loss = 1.57482 (* 1 = 1.57482 loss)
I0630 03:50:13.643936 29777 solver.cpp:290] Iteration 32000 (1.30239 iter/s, 76.782s/100 iter), loss = 1.2619
I0630 03:50:13.643962 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 03:50:13.643971 29777 sgd_solver.cpp:106] Iteration 32000, lr = 0.009
I0630 03:50:13.644950 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.17
I0630 03:50:13.817746 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 03:50:29.919770 29777 solver.cpp:290] Iteration 32100 (6.14426 iter/s, 16.2754s/100 iter), loss = 0.988095
I0630 03:50:29.919792 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 03:50:29.919800 29777 sgd_solver.cpp:106] Iteration 32100, lr = 0.00899688
I0630 03:50:45.947302 29777 solver.cpp:290] Iteration 32200 (6.23944 iter/s, 16.0271s/100 iter), loss = 1.15476
I0630 03:50:45.947376 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 03:50:45.947382 29777 sgd_solver.cpp:106] Iteration 32200, lr = 0.00899375
I0630 03:51:02.248486 29777 solver.cpp:290] Iteration 32300 (6.13472 iter/s, 16.3007s/100 iter), loss = 1.09524
I0630 03:51:02.248512 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 03:51:02.248522 29777 sgd_solver.cpp:106] Iteration 32300, lr = 0.00899062
I0630 03:51:18.370555 29777 solver.cpp:290] Iteration 32400 (6.20286 iter/s, 16.1216s/100 iter), loss = 1.07143
I0630 03:51:18.370658 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 03:51:18.370668 29777 sgd_solver.cpp:106] Iteration 32400, lr = 0.0089875
I0630 03:51:34.617399 29777 solver.cpp:290] Iteration 32500 (6.15525 iter/s, 16.2463s/100 iter), loss = 1.02381
I0630 03:51:34.617429 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 03:51:34.617440 29777 sgd_solver.cpp:106] Iteration 32500, lr = 0.00898437
I0630 03:51:50.764793 29777 solver.cpp:290] Iteration 32600 (6.19313 iter/s, 16.1469s/100 iter), loss = 1.02381
I0630 03:51:50.764888 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 03:51:50.764897 29777 sgd_solver.cpp:106] Iteration 32600, lr = 0.00898125
I0630 03:52:06.870379 29777 solver.cpp:290] Iteration 32700 (6.20923 iter/s, 16.105s/100 iter), loss = 0.916667
I0630 03:52:06.870412 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 03:52:06.870419 29777 sgd_solver.cpp:106] Iteration 32700, lr = 0.00897812
I0630 03:52:22.950608 29777 solver.cpp:290] Iteration 32800 (6.21901 iter/s, 16.0797s/100 iter), loss = 1.19048
I0630 03:52:22.950773 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 03:52:22.950810 29777 sgd_solver.cpp:106] Iteration 32800, lr = 0.008975
I0630 03:52:39.096552 29777 solver.cpp:290] Iteration 32900 (6.19374 iter/s, 16.1453s/100 iter), loss = 1.32143
I0630 03:52:39.096575 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 03:52:39.096581 29777 sgd_solver.cpp:106] Iteration 32900, lr = 0.00897187
I0630 03:52:54.988059 29777 solver.cpp:354] Sparsity after update:
I0630 03:52:55.008625 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:52:55.008641 29777 net.cpp:1851] conv1a_param_0(0.085) 
I0630 03:52:55.008651 29777 net.cpp:1851] conv1b_param_0(0.17) 
I0630 03:52:55.008652 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:52:55.008654 29777 net.cpp:1851] res2a_branch2a_param_0(0.17) 
I0630 03:52:55.008656 29777 net.cpp:1851] res2a_branch2b_param_0(0.17) 
I0630 03:52:55.008658 29777 net.cpp:1851] res3a_branch2a_param_0(0.17) 
I0630 03:52:55.008668 29777 net.cpp:1851] res3a_branch2b_param_0(0.17) 
I0630 03:52:55.008671 29777 net.cpp:1851] res4a_branch2a_param_0(0.17) 
I0630 03:52:55.008672 29777 net.cpp:1851] res4a_branch2b_param_0(0.17) 
I0630 03:52:55.008677 29777 net.cpp:1851] res5a_branch2a_param_0(0.17) 
I0630 03:52:55.008678 29777 net.cpp:1851] res5a_branch2b_param_0(0.17) 
I0630 03:52:55.008682 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (400093/2.86678e+06) 0.14
I0630 03:52:55.163780 29777 solver.cpp:290] Iteration 33000 (6.22403 iter/s, 16.0668s/100 iter), loss = 0.75
I0630 03:52:55.163803 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 03:52:55.163810 29777 sgd_solver.cpp:106] Iteration 33000, lr = 0.00896875
I0630 03:53:11.263727 29777 solver.cpp:290] Iteration 33100 (6.21138 iter/s, 16.0995s/100 iter), loss = 1.36905
I0630 03:53:11.263751 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 03:53:11.263756 29777 sgd_solver.cpp:106] Iteration 33100, lr = 0.00896563
I0630 03:53:27.484591 29777 solver.cpp:290] Iteration 33200 (6.16508 iter/s, 16.2204s/100 iter), loss = 1.27381
I0630 03:53:27.484663 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 03:53:27.484671 29777 sgd_solver.cpp:106] Iteration 33200, lr = 0.0089625
I0630 03:53:43.639612 29777 solver.cpp:290] Iteration 33300 (6.19022 iter/s, 16.1545s/100 iter), loss = 1.10714
I0630 03:53:43.639636 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 03:53:43.639642 29777 sgd_solver.cpp:106] Iteration 33300, lr = 0.00895937
I0630 03:53:59.659904 29777 solver.cpp:290] Iteration 33400 (6.24227 iter/s, 16.0198s/100 iter), loss = 1.39286
I0630 03:53:59.660009 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 03:53:59.660019 29777 sgd_solver.cpp:106] Iteration 33400, lr = 0.00895625
I0630 03:54:15.717109 29777 solver.cpp:290] Iteration 33500 (6.22795 iter/s, 16.0567s/100 iter), loss = 0.976191
I0630 03:54:15.717140 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 03:54:15.717150 29777 sgd_solver.cpp:106] Iteration 33500, lr = 0.00895312
I0630 03:54:32.057801 29777 solver.cpp:290] Iteration 33600 (6.11988 iter/s, 16.3402s/100 iter), loss = 0.964286
I0630 03:54:32.057909 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 03:54:32.057931 29777 sgd_solver.cpp:106] Iteration 33600, lr = 0.00895
I0630 03:54:48.239308 29777 solver.cpp:290] Iteration 33700 (6.1801 iter/s, 16.181s/100 iter), loss = 1.10714
I0630 03:54:48.239334 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 03:54:48.239343 29777 sgd_solver.cpp:106] Iteration 33700, lr = 0.00894688
I0630 03:55:04.359192 29777 solver.cpp:290] Iteration 33800 (6.2037 iter/s, 16.1194s/100 iter), loss = 1.28571
I0630 03:55:04.359292 29777 solver.cpp:309]     Train net output #0: loss = 0.571429 (* 1 = 0.571429 loss)
I0630 03:55:04.359321 29777 sgd_solver.cpp:106] Iteration 33800, lr = 0.00894375
I0630 03:55:20.463482 29777 solver.cpp:290] Iteration 33900 (6.20973 iter/s, 16.1038s/100 iter), loss = 1.2381
I0630 03:55:20.463505 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 03:55:20.463512 29777 sgd_solver.cpp:106] Iteration 33900, lr = 0.00894063
I0630 03:55:36.361322 29777 solver.cpp:354] Sparsity after update:
I0630 03:55:36.362942 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:55:36.362952 29777 net.cpp:1851] conv1a_param_0(0.085) 
I0630 03:55:36.362960 29777 net.cpp:1851] conv1b_param_0(0.17) 
I0630 03:55:36.362962 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:55:36.362964 29777 net.cpp:1851] res2a_branch2a_param_0(0.17) 
I0630 03:55:36.362967 29777 net.cpp:1851] res2a_branch2b_param_0(0.17) 
I0630 03:55:36.362968 29777 net.cpp:1851] res3a_branch2a_param_0(0.17) 
I0630 03:55:36.362970 29777 net.cpp:1851] res3a_branch2b_param_0(0.17) 
I0630 03:55:36.362972 29777 net.cpp:1851] res4a_branch2a_param_0(0.17) 
I0630 03:55:36.362974 29777 net.cpp:1851] res4a_branch2b_param_0(0.17) 
I0630 03:55:36.362977 29777 net.cpp:1851] res5a_branch2a_param_0(0.17) 
I0630 03:55:36.362978 29777 net.cpp:1851] res5a_branch2b_param_0(0.17) 
I0630 03:55:36.362980 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (400093/2.86678e+06) 0.14
I0630 03:55:36.363071 29777 solver.cpp:471] Iteration 34000, Testing net (#0)
I0630 03:55:40.753731 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 03:56:44.617478 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54646
I0630 03:56:44.617561 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.788641
I0630 03:56:44.617571 29777 solver.cpp:544]     Test net output #2: loss = 1.64766 (* 1 = 1.64766 loss)
I0630 03:56:44.797288 29777 solver.cpp:290] Iteration 34000 (1.1858 iter/s, 84.3315s/100 iter), loss = 1.29762
I0630 03:56:44.797312 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 03:56:44.797319 29777 sgd_solver.cpp:106] Iteration 34000, lr = 0.0089375
I0630 03:56:44.798077 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.18
I0630 03:56:44.921867 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 03:57:01.287376 29777 solver.cpp:290] Iteration 34100 (6.06443 iter/s, 16.4896s/100 iter), loss = 0.916667
I0630 03:57:01.287437 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 03:57:01.287459 29777 sgd_solver.cpp:106] Iteration 34100, lr = 0.00893437
I0630 03:57:17.567765 29777 solver.cpp:290] Iteration 34200 (6.14255 iter/s, 16.2799s/100 iter), loss = 1.02381
I0630 03:57:17.567842 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 03:57:17.567852 29777 sgd_solver.cpp:106] Iteration 34200, lr = 0.00893125
I0630 03:57:33.886263 29777 solver.cpp:290] Iteration 34300 (6.12821 iter/s, 16.318s/100 iter), loss = 0.97619
I0630 03:57:33.886289 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 03:57:33.886298 29777 sgd_solver.cpp:106] Iteration 34300, lr = 0.00892812
I0630 03:57:49.957355 29777 solver.cpp:290] Iteration 34400 (6.22253 iter/s, 16.0706s/100 iter), loss = 0.952381
I0630 03:57:49.957461 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 03:57:49.957475 29777 sgd_solver.cpp:106] Iteration 34400, lr = 0.008925
I0630 03:58:06.444828 29777 solver.cpp:290] Iteration 34500 (6.06542 iter/s, 16.4869s/100 iter), loss = 0.892857
I0630 03:58:06.444916 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 03:58:06.444944 29777 sgd_solver.cpp:106] Iteration 34500, lr = 0.00892187
I0630 03:58:22.659430 29777 solver.cpp:290] Iteration 34600 (6.16748 iter/s, 16.2141s/100 iter), loss = 1.03571
I0630 03:58:22.659538 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 03:58:22.659548 29777 sgd_solver.cpp:106] Iteration 34600, lr = 0.00891875
I0630 03:58:39.116744 29777 solver.cpp:290] Iteration 34700 (6.07653 iter/s, 16.4568s/100 iter), loss = 1.09524
I0630 03:58:39.116768 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 03:58:39.116775 29777 sgd_solver.cpp:106] Iteration 34700, lr = 0.00891562
I0630 03:58:55.286936 29777 solver.cpp:290] Iteration 34800 (6.1844 iter/s, 16.1697s/100 iter), loss = 1.22619
I0630 03:58:55.287117 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 03:58:55.287139 29777 sgd_solver.cpp:106] Iteration 34800, lr = 0.0089125
I0630 03:59:11.672698 29777 solver.cpp:290] Iteration 34900 (6.10309 iter/s, 16.3851s/100 iter), loss = 1.32143
I0630 03:59:11.672791 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 03:59:11.672822 29777 sgd_solver.cpp:106] Iteration 34900, lr = 0.00890937
I0630 03:59:27.868269 29777 solver.cpp:354] Sparsity after update:
I0630 03:59:27.893358 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 03:59:27.893379 29777 net.cpp:1851] conv1a_param_0(0.09) 
I0630 03:59:27.893390 29777 net.cpp:1851] conv1b_param_0(0.18) 
I0630 03:59:27.893394 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 03:59:27.893398 29777 net.cpp:1851] res2a_branch2a_param_0(0.18) 
I0630 03:59:27.893400 29777 net.cpp:1851] res2a_branch2b_param_0(0.18) 
I0630 03:59:27.893411 29777 net.cpp:1851] res3a_branch2a_param_0(0.18) 
I0630 03:59:27.893430 29777 net.cpp:1851] res3a_branch2b_param_0(0.18) 
I0630 03:59:27.893445 29777 net.cpp:1851] res4a_branch2a_param_0(0.18) 
I0630 03:59:27.893450 29777 net.cpp:1851] res4a_branch2b_param_0(0.18) 
I0630 03:59:27.893453 29777 net.cpp:1851] res5a_branch2a_param_0(0.18) 
I0630 03:59:27.893465 29777 net.cpp:1851] res5a_branch2b_param_0(0.18) 
I0630 03:59:27.893471 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (423634/2.86678e+06) 0.148
I0630 03:59:28.050850 29777 solver.cpp:290] Iteration 35000 (6.1059 iter/s, 16.3776s/100 iter), loss = 1.34524
I0630 03:59:28.050876 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 03:59:28.050886 29777 sgd_solver.cpp:106] Iteration 35000, lr = 0.00890625
I0630 03:59:44.458290 29777 solver.cpp:290] Iteration 35100 (6.09497 iter/s, 16.407s/100 iter), loss = 0.75
I0630 03:59:44.458340 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 03:59:44.458364 29777 sgd_solver.cpp:106] Iteration 35100, lr = 0.00890312
I0630 04:00:00.934155 29777 solver.cpp:290] Iteration 35200 (6.06967 iter/s, 16.4754s/100 iter), loss = 1.17857
I0630 04:00:00.934229 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 04:00:00.934237 29777 sgd_solver.cpp:106] Iteration 35200, lr = 0.0089
I0630 04:00:17.028230 29777 solver.cpp:290] Iteration 35300 (6.21367 iter/s, 16.0936s/100 iter), loss = 1
I0630 04:00:17.028252 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 04:00:17.028259 29777 sgd_solver.cpp:106] Iteration 35300, lr = 0.00889687
I0630 04:00:33.304472 29777 solver.cpp:290] Iteration 35400 (6.1441 iter/s, 16.2758s/100 iter), loss = 1.28571
I0630 04:00:33.304543 29777 solver.cpp:309]     Train net output #0: loss = 1.85714 (* 1 = 1.85714 loss)
I0630 04:00:33.304553 29777 sgd_solver.cpp:106] Iteration 35400, lr = 0.00889375
I0630 04:00:49.832002 29777 solver.cpp:290] Iteration 35500 (6.0507 iter/s, 16.527s/100 iter), loss = 1.09524
I0630 04:00:49.832029 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 04:00:49.832038 29777 sgd_solver.cpp:106] Iteration 35500, lr = 0.00889063
I0630 04:01:05.928761 29777 solver.cpp:290] Iteration 35600 (6.21261 iter/s, 16.0963s/100 iter), loss = 1.21429
I0630 04:01:05.928825 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 04:01:05.928833 29777 sgd_solver.cpp:106] Iteration 35600, lr = 0.0088875
I0630 04:01:22.679471 29777 solver.cpp:290] Iteration 35700 (5.97008 iter/s, 16.7502s/100 iter), loss = 1.19048
I0630 04:01:22.679496 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 04:01:22.679502 29777 sgd_solver.cpp:106] Iteration 35700, lr = 0.00888437
I0630 04:01:39.123715 29777 solver.cpp:290] Iteration 35800 (6.08133 iter/s, 16.4438s/100 iter), loss = 1.29762
I0630 04:01:39.123813 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 04:01:39.123821 29777 sgd_solver.cpp:106] Iteration 35800, lr = 0.00888125
I0630 04:01:55.443732 29777 solver.cpp:290] Iteration 35900 (6.12765 iter/s, 16.3195s/100 iter), loss = 0.952381
I0630 04:01:55.443758 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 04:01:55.443766 29777 sgd_solver.cpp:106] Iteration 35900, lr = 0.00887812
I0630 04:02:11.412854 29777 solver.cpp:354] Sparsity after update:
I0630 04:02:11.414500 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:02:11.414513 29777 net.cpp:1851] conv1a_param_0(0.09) 
I0630 04:02:11.414522 29777 net.cpp:1851] conv1b_param_0(0.18) 
I0630 04:02:11.414527 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:02:11.414531 29777 net.cpp:1851] res2a_branch2a_param_0(0.18) 
I0630 04:02:11.414535 29777 net.cpp:1851] res2a_branch2b_param_0(0.18) 
I0630 04:02:11.414538 29777 net.cpp:1851] res3a_branch2a_param_0(0.18) 
I0630 04:02:11.414542 29777 net.cpp:1851] res3a_branch2b_param_0(0.18) 
I0630 04:02:11.414546 29777 net.cpp:1851] res4a_branch2a_param_0(0.18) 
I0630 04:02:11.414548 29777 net.cpp:1851] res4a_branch2b_param_0(0.18) 
I0630 04:02:11.414551 29777 net.cpp:1851] res5a_branch2a_param_0(0.18) 
I0630 04:02:11.414556 29777 net.cpp:1851] res5a_branch2b_param_0(0.18) 
I0630 04:02:11.414558 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (423634/2.86678e+06) 0.148
I0630 04:02:11.414687 29777 solver.cpp:471] Iteration 36000, Testing net (#0)
I0630 04:02:14.709843 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 04:03:17.265198 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.554479
I0630 04:03:17.265255 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.796002
I0630 04:03:17.265261 29777 solver.cpp:544]     Test net output #2: loss = 1.58688 (* 1 = 1.58688 loss)
I0630 04:03:17.448673 29777 solver.cpp:290] Iteration 36000 (1.21947 iter/s, 82.0027s/100 iter), loss = 1.0119
I0630 04:03:17.448695 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 04:03:17.448703 29777 sgd_solver.cpp:106] Iteration 36000, lr = 0.008875
I0630 04:03:17.449451 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.19
I0630 04:03:17.680459 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 04:03:33.739944 29777 solver.cpp:290] Iteration 36100 (6.13843 iter/s, 16.2908s/100 iter), loss = 1.09524
I0630 04:03:33.739969 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 04:03:33.739975 29777 sgd_solver.cpp:106] Iteration 36100, lr = 0.00887187
I0630 04:03:49.892568 29777 solver.cpp:290] Iteration 36200 (6.19113 iter/s, 16.1521s/100 iter), loss = 0.809524
I0630 04:03:49.892660 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 04:03:49.892684 29777 sgd_solver.cpp:106] Iteration 36200, lr = 0.00886875
I0630 04:04:06.157912 29777 solver.cpp:290] Iteration 36300 (6.14824 iter/s, 16.2648s/100 iter), loss = 0.892857
I0630 04:04:06.157935 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 04:04:06.157943 29777 sgd_solver.cpp:106] Iteration 36300, lr = 0.00886562
I0630 04:04:22.481402 29777 solver.cpp:290] Iteration 36400 (6.12632 iter/s, 16.323s/100 iter), loss = 1.29762
I0630 04:04:22.481483 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 04:04:22.481494 29777 sgd_solver.cpp:106] Iteration 36400, lr = 0.0088625
I0630 04:04:38.624049 29777 solver.cpp:290] Iteration 36500 (6.19497 iter/s, 16.1421s/100 iter), loss = 1.47619
I0630 04:04:38.624075 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 04:04:38.624084 29777 sgd_solver.cpp:106] Iteration 36500, lr = 0.00885937
I0630 04:04:54.778462 29777 solver.cpp:290] Iteration 36600 (6.19044 iter/s, 16.1539s/100 iter), loss = 1.11905
I0630 04:04:54.778600 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 04:04:54.778610 29777 sgd_solver.cpp:106] Iteration 36600, lr = 0.00885625
I0630 04:05:11.150753 29777 solver.cpp:290] Iteration 36700 (6.1081 iter/s, 16.3717s/100 iter), loss = 1.05952
I0630 04:05:11.150776 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 04:05:11.150784 29777 sgd_solver.cpp:106] Iteration 36700, lr = 0.00885312
I0630 04:05:27.486974 29777 solver.cpp:290] Iteration 36800 (6.12155 iter/s, 16.3357s/100 iter), loss = 0.928571
I0630 04:05:27.487063 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 04:05:27.487074 29777 sgd_solver.cpp:106] Iteration 36800, lr = 0.00885
I0630 04:05:43.677691 29777 solver.cpp:290] Iteration 36900 (6.17658 iter/s, 16.1902s/100 iter), loss = 0.928571
I0630 04:05:43.677716 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 04:05:43.677728 29777 sgd_solver.cpp:106] Iteration 36900, lr = 0.00884687
I0630 04:05:59.627213 29777 solver.cpp:354] Sparsity after update:
I0630 04:05:59.651159 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:05:59.651232 29777 net.cpp:1851] conv1a_param_0(0.095) 
I0630 04:05:59.651257 29777 net.cpp:1851] conv1b_param_0(0.19) 
I0630 04:05:59.651269 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:05:59.651283 29777 net.cpp:1851] res2a_branch2a_param_0(0.19) 
I0630 04:05:59.651294 29777 net.cpp:1851] res2a_branch2b_param_0(0.19) 
I0630 04:05:59.651306 29777 net.cpp:1851] res3a_branch2a_param_0(0.19) 
I0630 04:05:59.651319 29777 net.cpp:1851] res3a_branch2b_param_0(0.19) 
I0630 04:05:59.651331 29777 net.cpp:1851] res4a_branch2a_param_0(0.19) 
I0630 04:05:59.651355 29777 net.cpp:1851] res4a_branch2b_param_0(0.19) 
I0630 04:05:59.651367 29777 net.cpp:1851] res5a_branch2a_param_0(0.19) 
I0630 04:05:59.651379 29777 net.cpp:1851] res5a_branch2b_param_0(0.19) 
I0630 04:05:59.651391 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (447171/2.86678e+06) 0.156
I0630 04:05:59.817381 29777 solver.cpp:290] Iteration 37000 (6.19609 iter/s, 16.1392s/100 iter), loss = 0.785714
I0630 04:05:59.817405 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 04:05:59.817412 29777 sgd_solver.cpp:106] Iteration 37000, lr = 0.00884375
I0630 04:06:16.070508 29777 solver.cpp:290] Iteration 37100 (6.15284 iter/s, 16.2527s/100 iter), loss = 1.13095
I0630 04:06:16.070530 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 04:06:16.070538 29777 sgd_solver.cpp:106] Iteration 37100, lr = 0.00884063
I0630 04:06:32.291038 29777 solver.cpp:290] Iteration 37200 (6.16521 iter/s, 16.2201s/100 iter), loss = 1.13095
I0630 04:06:32.291132 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 04:06:32.291143 29777 sgd_solver.cpp:106] Iteration 37200, lr = 0.0088375
I0630 04:06:48.478322 29777 solver.cpp:290] Iteration 37300 (6.17789 iter/s, 16.1867s/100 iter), loss = 1.08333
I0630 04:06:48.478346 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 04:06:48.478355 29777 sgd_solver.cpp:106] Iteration 37300, lr = 0.00883438
I0630 04:07:04.541342 29777 solver.cpp:290] Iteration 37400 (6.22566 iter/s, 16.0626s/100 iter), loss = 1.16667
I0630 04:07:04.541432 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 04:07:04.541443 29777 sgd_solver.cpp:106] Iteration 37400, lr = 0.00883125
I0630 04:07:20.847949 29777 solver.cpp:290] Iteration 37500 (6.13268 iter/s, 16.3061s/100 iter), loss = 1.40476
I0630 04:07:20.847972 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 04:07:20.847980 29777 sgd_solver.cpp:106] Iteration 37500, lr = 0.00882812
I0630 04:07:37.024722 29777 solver.cpp:290] Iteration 37600 (6.18188 iter/s, 16.1763s/100 iter), loss = 0.97619
I0630 04:07:37.024829 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 04:07:37.024848 29777 sgd_solver.cpp:106] Iteration 37600, lr = 0.008825
I0630 04:07:53.197958 29777 solver.cpp:290] Iteration 37700 (6.18326 iter/s, 16.1727s/100 iter), loss = 1.02381
I0630 04:07:53.197985 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 04:07:53.197994 29777 sgd_solver.cpp:106] Iteration 37700, lr = 0.00882187
I0630 04:08:09.294817 29777 solver.cpp:290] Iteration 37800 (6.21257 iter/s, 16.0964s/100 iter), loss = 1.07143
I0630 04:08:09.294926 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 04:08:09.294937 29777 sgd_solver.cpp:106] Iteration 37800, lr = 0.00881875
I0630 04:08:25.498790 29777 solver.cpp:290] Iteration 37900 (6.17154 iter/s, 16.2034s/100 iter), loss = 1.5
I0630 04:08:25.498817 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 04:08:25.498827 29777 sgd_solver.cpp:106] Iteration 37900, lr = 0.00881562
I0630 04:08:41.384981 29777 solver.cpp:354] Sparsity after update:
I0630 04:08:41.386425 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:08:41.386431 29777 net.cpp:1851] conv1a_param_0(0.095) 
I0630 04:08:41.386442 29777 net.cpp:1851] conv1b_param_0(0.19) 
I0630 04:08:41.386447 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:08:41.386452 29777 net.cpp:1851] res2a_branch2a_param_0(0.19) 
I0630 04:08:41.386456 29777 net.cpp:1851] res2a_branch2b_param_0(0.19) 
I0630 04:08:41.386461 29777 net.cpp:1851] res3a_branch2a_param_0(0.19) 
I0630 04:08:41.386466 29777 net.cpp:1851] res3a_branch2b_param_0(0.19) 
I0630 04:08:41.386469 29777 net.cpp:1851] res4a_branch2a_param_0(0.19) 
I0630 04:08:41.386473 29777 net.cpp:1851] res4a_branch2b_param_0(0.19) 
I0630 04:08:41.386478 29777 net.cpp:1851] res5a_branch2a_param_0(0.19) 
I0630 04:08:41.386482 29777 net.cpp:1851] res5a_branch2b_param_0(0.19) 
I0630 04:08:41.386487 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (447171/2.86678e+06) 0.156
I0630 04:08:41.386576 29777 solver.cpp:471] Iteration 38000, Testing net (#0)
I0630 04:08:45.586392 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 04:09:39.520534 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56184
I0630 04:09:39.520635 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.797021
I0630 04:09:39.520645 29777 solver.cpp:544]     Test net output #2: loss = 1.55784 (* 1 = 1.55784 loss)
I0630 04:09:39.708645 29777 solver.cpp:290] Iteration 38000 (1.34757 iter/s, 74.2078s/100 iter), loss = 0.690476
I0630 04:09:39.708672 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 04:09:39.708681 29777 sgd_solver.cpp:106] Iteration 38000, lr = 0.0088125
I0630 04:09:39.709666 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.2
I0630 04:09:39.854768 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 04:09:56.000033 29777 solver.cpp:290] Iteration 38100 (6.13839 iter/s, 16.2909s/100 iter), loss = 0.988095
I0630 04:09:56.000084 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 04:09:56.000108 29777 sgd_solver.cpp:106] Iteration 38100, lr = 0.00880937
I0630 04:10:12.189398 29777 solver.cpp:290] Iteration 38200 (6.17709 iter/s, 16.1889s/100 iter), loss = 1.13095
I0630 04:10:12.189544 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 04:10:12.189574 29777 sgd_solver.cpp:106] Iteration 38200, lr = 0.00880625
I0630 04:10:28.304299 29777 solver.cpp:290] Iteration 38300 (6.20567 iter/s, 16.1143s/100 iter), loss = 0.964286
I0630 04:10:28.304365 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 04:10:28.304385 29777 sgd_solver.cpp:106] Iteration 38300, lr = 0.00880312
I0630 04:10:44.377141 29777 solver.cpp:290] Iteration 38400 (6.22187 iter/s, 16.0723s/100 iter), loss = 1.09524
I0630 04:10:44.377246 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 04:10:44.377259 29777 sgd_solver.cpp:106] Iteration 38400, lr = 0.0088
I0630 04:11:00.461158 29777 solver.cpp:290] Iteration 38500 (6.21756 iter/s, 16.0835s/100 iter), loss = 1.05952
I0630 04:11:00.461181 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 04:11:00.461191 29777 sgd_solver.cpp:106] Iteration 38500, lr = 0.00879687
I0630 04:11:16.578414 29777 solver.cpp:290] Iteration 38600 (6.20471 iter/s, 16.1168s/100 iter), loss = 0.869048
I0630 04:11:16.578495 29777 solver.cpp:309]     Train net output #0: loss = 0.571429 (* 1 = 0.571429 loss)
I0630 04:11:16.578503 29777 sgd_solver.cpp:106] Iteration 38600, lr = 0.00879375
I0630 04:11:32.666998 29777 solver.cpp:290] Iteration 38700 (6.21579 iter/s, 16.0881s/100 iter), loss = 0.809524
I0630 04:11:32.667026 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 04:11:32.667042 29777 sgd_solver.cpp:106] Iteration 38700, lr = 0.00879063
I0630 04:11:48.727879 29777 solver.cpp:290] Iteration 38800 (6.22649 iter/s, 16.0604s/100 iter), loss = 0.797619
I0630 04:11:48.727988 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 04:11:48.728003 29777 sgd_solver.cpp:106] Iteration 38800, lr = 0.0087875
I0630 04:12:04.861155 29777 solver.cpp:290] Iteration 38900 (6.19858 iter/s, 16.1327s/100 iter), loss = 1.13095
I0630 04:12:04.861179 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 04:12:04.861186 29777 sgd_solver.cpp:106] Iteration 38900, lr = 0.00878438
I0630 04:12:20.847887 29777 solver.cpp:354] Sparsity after update:
I0630 04:12:20.868929 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:12:20.868950 29777 net.cpp:1851] conv1a_param_0(0.1) 
I0630 04:12:20.868959 29777 net.cpp:1851] conv1b_param_0(0.2) 
I0630 04:12:20.868963 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:12:20.868966 29777 net.cpp:1851] res2a_branch2a_param_0(0.2) 
I0630 04:12:20.868969 29777 net.cpp:1851] res2a_branch2b_param_0(0.2) 
I0630 04:12:20.868973 29777 net.cpp:1851] res3a_branch2a_param_0(0.2) 
I0630 04:12:20.868986 29777 net.cpp:1851] res3a_branch2b_param_0(0.2) 
I0630 04:12:20.868989 29777 net.cpp:1851] res4a_branch2a_param_0(0.2) 
I0630 04:12:20.868993 29777 net.cpp:1851] res4a_branch2b_param_0(0.2) 
I0630 04:12:20.868995 29777 net.cpp:1851] res5a_branch2a_param_0(0.2) 
I0630 04:12:20.868999 29777 net.cpp:1851] res5a_branch2b_param_0(0.2) 
I0630 04:12:20.869002 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (470700/2.86678e+06) 0.164
I0630 04:12:21.022718 29777 solver.cpp:290] Iteration 39000 (6.1877 iter/s, 16.1611s/100 iter), loss = 0.904762
I0630 04:12:21.022810 29777 solver.cpp:309]     Train net output #0: loss = 0.547619 (* 1 = 0.547619 loss)
I0630 04:12:21.022855 29777 sgd_solver.cpp:106] Iteration 39000, lr = 0.00878125
I0630 04:12:37.488791 29777 solver.cpp:290] Iteration 39100 (6.0733 iter/s, 16.4655s/100 iter), loss = 1.02381
I0630 04:12:37.488888 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 04:12:37.488917 29777 sgd_solver.cpp:106] Iteration 39100, lr = 0.00877813
I0630 04:12:53.574532 29777 solver.cpp:290] Iteration 39200 (6.21689 iter/s, 16.0852s/100 iter), loss = 1.08333
I0630 04:12:53.574620 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 04:12:53.574630 29777 sgd_solver.cpp:106] Iteration 39200, lr = 0.008775
I0630 04:13:09.734421 29777 solver.cpp:290] Iteration 39300 (6.18836 iter/s, 16.1594s/100 iter), loss = 1.05952
I0630 04:13:09.734446 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 04:13:09.734452 29777 sgd_solver.cpp:106] Iteration 39300, lr = 0.00877187
I0630 04:13:26.155921 29777 solver.cpp:290] Iteration 39400 (6.08976 iter/s, 16.421s/100 iter), loss = 0.964286
I0630 04:13:26.156042 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 04:13:26.156061 29777 sgd_solver.cpp:106] Iteration 39400, lr = 0.00876875
I0630 04:13:42.472317 29777 solver.cpp:290] Iteration 39500 (6.12902 iter/s, 16.3158s/100 iter), loss = 1.27381
I0630 04:13:42.472344 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 04:13:42.472353 29777 sgd_solver.cpp:106] Iteration 39500, lr = 0.00876562
I0630 04:13:58.734464 29777 solver.cpp:290] Iteration 39600 (6.14943 iter/s, 16.2617s/100 iter), loss = 1.19048
I0630 04:13:58.734578 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 04:13:58.734589 29777 sgd_solver.cpp:106] Iteration 39600, lr = 0.0087625
I0630 04:14:14.981557 29777 solver.cpp:290] Iteration 39700 (6.15516 iter/s, 16.2465s/100 iter), loss = 1.44048
I0630 04:14:14.981580 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 04:14:14.981586 29777 sgd_solver.cpp:106] Iteration 39700, lr = 0.00875937
I0630 04:14:31.312531 29777 solver.cpp:290] Iteration 39800 (6.12351 iter/s, 16.3305s/100 iter), loss = 0.952381
I0630 04:14:31.312624 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 04:14:31.312636 29777 sgd_solver.cpp:106] Iteration 39800, lr = 0.00875625
I0630 04:14:47.668221 29777 solver.cpp:290] Iteration 39900 (6.11428 iter/s, 16.3551s/100 iter), loss = 0.988095
I0630 04:14:47.668242 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 04:14:47.668249 29777 sgd_solver.cpp:106] Iteration 39900, lr = 0.00875312
I0630 04:15:03.858249 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_40000.caffemodel
I0630 04:15:03.878159 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_40000.solverstate
I0630 04:15:03.886739 29777 solver.cpp:354] Sparsity after update:
I0630 04:15:03.887707 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:15:03.887717 29777 net.cpp:1851] conv1a_param_0(0.1) 
I0630 04:15:03.887724 29777 net.cpp:1851] conv1b_param_0(0.2) 
I0630 04:15:03.887727 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:15:03.887730 29777 net.cpp:1851] res2a_branch2a_param_0(0.2) 
I0630 04:15:03.887733 29777 net.cpp:1851] res2a_branch2b_param_0(0.2) 
I0630 04:15:03.887737 29777 net.cpp:1851] res3a_branch2a_param_0(0.2) 
I0630 04:15:03.887738 29777 net.cpp:1851] res3a_branch2b_param_0(0.2) 
I0630 04:15:03.887740 29777 net.cpp:1851] res4a_branch2a_param_0(0.2) 
I0630 04:15:03.887742 29777 net.cpp:1851] res4a_branch2b_param_0(0.2) 
I0630 04:15:03.887744 29777 net.cpp:1851] res5a_branch2a_param_0(0.2) 
I0630 04:15:03.887748 29777 net.cpp:1851] res5a_branch2b_param_0(0.2) 
I0630 04:15:03.887749 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (470700/2.86678e+06) 0.164
I0630 04:15:03.887847 29777 solver.cpp:471] Iteration 40000, Testing net (#0)
I0630 04:15:08.760001 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 04:16:11.910348 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55972
I0630 04:16:11.910490 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.79596
I0630 04:16:11.910509 29777 solver.cpp:544]     Test net output #2: loss = 1.5758 (* 1 = 1.5758 loss)
I0630 04:16:12.159060 29777 solver.cpp:290] Iteration 40000 (1.18359 iter/s, 84.4885s/100 iter), loss = 0.988095
I0630 04:16:12.159107 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 04:16:12.159123 29777 sgd_solver.cpp:106] Iteration 40000, lr = 0.00875
I0630 04:16:12.160686 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.21
I0630 04:16:12.419412 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 04:16:28.510962 29777 solver.cpp:290] Iteration 40100 (6.11568 iter/s, 16.3514s/100 iter), loss = 1.17857
I0630 04:16:28.510984 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 04:16:28.510990 29777 sgd_solver.cpp:106] Iteration 40100, lr = 0.00874687
I0630 04:16:44.741191 29777 solver.cpp:290] Iteration 40200 (6.16152 iter/s, 16.2298s/100 iter), loss = 1.15476
I0630 04:16:44.741297 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 04:16:44.741307 29777 sgd_solver.cpp:106] Iteration 40200, lr = 0.00874375
I0630 04:17:01.068835 29777 solver.cpp:290] Iteration 40300 (6.12479 iter/s, 16.3271s/100 iter), loss = 1.03571
I0630 04:17:01.068857 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 04:17:01.068864 29777 sgd_solver.cpp:106] Iteration 40300, lr = 0.00874062
I0630 04:17:17.316555 29777 solver.cpp:290] Iteration 40400 (6.15489 iter/s, 16.2472s/100 iter), loss = 1.20238
I0630 04:17:17.318898 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 04:17:17.318925 29777 sgd_solver.cpp:106] Iteration 40400, lr = 0.0087375
I0630 04:17:33.543649 29777 solver.cpp:290] Iteration 40500 (6.16359 iter/s, 16.2243s/100 iter), loss = 1.05952
I0630 04:17:33.543716 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 04:17:33.543740 29777 sgd_solver.cpp:106] Iteration 40500, lr = 0.00873438
I0630 04:17:49.880261 29777 solver.cpp:290] Iteration 40600 (6.12141 iter/s, 16.3361s/100 iter), loss = 0.857143
I0630 04:17:49.880347 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 04:17:49.880355 29777 sgd_solver.cpp:106] Iteration 40600, lr = 0.00873125
I0630 04:18:06.516165 29777 solver.cpp:290] Iteration 40700 (6.0113 iter/s, 16.6353s/100 iter), loss = 0.77381
I0630 04:18:06.516432 29777 solver.cpp:309]     Train net output #0: loss = 0.571429 (* 1 = 0.571429 loss)
I0630 04:18:06.516552 29777 sgd_solver.cpp:106] Iteration 40700, lr = 0.00872813
I0630 04:18:23.404144 29777 solver.cpp:290] Iteration 40800 (5.92162 iter/s, 16.8873s/100 iter), loss = 1.39286
I0630 04:18:23.404214 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 04:18:23.404225 29777 sgd_solver.cpp:106] Iteration 40800, lr = 0.008725
I0630 04:18:39.503809 29777 solver.cpp:290] Iteration 40900 (6.21151 iter/s, 16.0992s/100 iter), loss = 1.2619
I0630 04:18:39.503834 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 04:18:39.503840 29777 sgd_solver.cpp:106] Iteration 40900, lr = 0.00872188
I0630 04:18:55.535756 29777 solver.cpp:354] Sparsity after update:
I0630 04:18:55.556022 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:18:55.556044 29777 net.cpp:1851] conv1a_param_0(0.105) 
I0630 04:18:55.556056 29777 net.cpp:1851] conv1b_param_0(0.21) 
I0630 04:18:55.556061 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:18:55.556063 29777 net.cpp:1851] res2a_branch2a_param_0(0.21) 
I0630 04:18:55.556066 29777 net.cpp:1851] res2a_branch2b_param_0(0.21) 
I0630 04:18:55.556071 29777 net.cpp:1851] res3a_branch2a_param_0(0.21) 
I0630 04:18:55.556078 29777 net.cpp:1851] res3a_branch2b_param_0(0.21) 
I0630 04:18:55.556084 29777 net.cpp:1851] res4a_branch2a_param_0(0.21) 
I0630 04:18:55.556089 29777 net.cpp:1851] res4a_branch2b_param_0(0.21) 
I0630 04:18:55.556093 29777 net.cpp:1851] res5a_branch2a_param_0(0.21) 
I0630 04:18:55.556098 29777 net.cpp:1851] res5a_branch2b_param_0(0.21) 
I0630 04:18:55.556103 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (494241/2.86678e+06) 0.172
I0630 04:18:55.724050 29777 solver.cpp:290] Iteration 41000 (6.16532 iter/s, 16.2198s/100 iter), loss = 1.14286
I0630 04:18:55.724104 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 04:18:55.724128 29777 sgd_solver.cpp:106] Iteration 41000, lr = 0.00871875
I0630 04:19:12.116008 29777 solver.cpp:290] Iteration 41100 (6.10074 iter/s, 16.3915s/100 iter), loss = 1.27381
I0630 04:19:12.116055 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 04:19:12.116091 29777 sgd_solver.cpp:106] Iteration 41100, lr = 0.00871562
I0630 04:19:28.380409 29777 solver.cpp:290] Iteration 41200 (6.14858 iter/s, 16.2639s/100 iter), loss = 0.702381
I0630 04:19:28.380523 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 04:19:28.380533 29777 sgd_solver.cpp:106] Iteration 41200, lr = 0.0087125
I0630 04:19:44.542665 29777 solver.cpp:290] Iteration 41300 (6.18747 iter/s, 16.1617s/100 iter), loss = 0.809524
I0630 04:19:44.542690 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 04:19:44.542698 29777 sgd_solver.cpp:106] Iteration 41300, lr = 0.00870937
I0630 04:20:00.680043 29777 solver.cpp:290] Iteration 41400 (6.19697 iter/s, 16.1369s/100 iter), loss = 1.11905
I0630 04:20:00.680155 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 04:20:00.680171 29777 sgd_solver.cpp:106] Iteration 41400, lr = 0.00870625
I0630 04:20:16.893708 29777 solver.cpp:290] Iteration 41500 (6.16785 iter/s, 16.2131s/100 iter), loss = 1.14286
I0630 04:20:16.893733 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 04:20:16.893739 29777 sgd_solver.cpp:106] Iteration 41500, lr = 0.00870312
I0630 04:20:33.191035 29777 solver.cpp:290] Iteration 41600 (6.13615 iter/s, 16.2969s/100 iter), loss = 1.20238
I0630 04:20:33.191125 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 04:20:33.191136 29777 sgd_solver.cpp:106] Iteration 41600, lr = 0.0087
I0630 04:20:49.405822 29777 solver.cpp:290] Iteration 41700 (6.16741 iter/s, 16.2143s/100 iter), loss = 1.10714
I0630 04:20:49.405844 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 04:20:49.405851 29777 sgd_solver.cpp:106] Iteration 41700, lr = 0.00869687
I0630 04:21:05.656167 29777 solver.cpp:290] Iteration 41800 (6.15389 iter/s, 16.2499s/100 iter), loss = 1.03571
I0630 04:21:05.656242 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 04:21:05.656250 29777 sgd_solver.cpp:106] Iteration 41800, lr = 0.00869375
I0630 04:21:21.960932 29777 solver.cpp:290] Iteration 41900 (6.13337 iter/s, 16.3042s/100 iter), loss = 1.19048
I0630 04:21:21.960957 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 04:21:21.960963 29777 sgd_solver.cpp:106] Iteration 41900, lr = 0.00869062
I0630 04:21:38.094594 29777 solver.cpp:354] Sparsity after update:
I0630 04:21:38.095868 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:21:38.095876 29777 net.cpp:1851] conv1a_param_0(0.105) 
I0630 04:21:38.095883 29777 net.cpp:1851] conv1b_param_0(0.21) 
I0630 04:21:38.095885 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:21:38.095887 29777 net.cpp:1851] res2a_branch2a_param_0(0.21) 
I0630 04:21:38.095890 29777 net.cpp:1851] res2a_branch2b_param_0(0.21) 
I0630 04:21:38.095891 29777 net.cpp:1851] res3a_branch2a_param_0(0.21) 
I0630 04:21:38.095893 29777 net.cpp:1851] res3a_branch2b_param_0(0.21) 
I0630 04:21:38.095896 29777 net.cpp:1851] res4a_branch2a_param_0(0.21) 
I0630 04:21:38.095897 29777 net.cpp:1851] res4a_branch2b_param_0(0.21) 
I0630 04:21:38.095899 29777 net.cpp:1851] res5a_branch2a_param_0(0.21) 
I0630 04:21:38.095901 29777 net.cpp:1851] res5a_branch2b_param_0(0.21) 
I0630 04:21:38.095903 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (494241/2.86678e+06) 0.172
I0630 04:21:38.095994 29777 solver.cpp:471] Iteration 42000, Testing net (#0)
I0630 04:21:43.651549 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 04:22:39.943482 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56654
I0630 04:22:39.943528 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.798922
I0630 04:22:39.943536 29777 solver.cpp:544]     Test net output #2: loss = 1.5711 (* 1 = 1.5711 loss)
I0630 04:22:40.128012 29777 solver.cpp:290] Iteration 42000 (1.27935 iter/s, 78.1649s/100 iter), loss = 1.25
I0630 04:22:40.128036 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 04:22:40.128042 29777 sgd_solver.cpp:106] Iteration 42000, lr = 0.0086875
I0630 04:22:40.128787 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.22
I0630 04:22:40.286159 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 04:22:56.466037 29777 solver.cpp:290] Iteration 42100 (6.12087 iter/s, 16.3375s/100 iter), loss = 1.02381
I0630 04:22:56.466060 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 04:22:56.466066 29777 sgd_solver.cpp:106] Iteration 42100, lr = 0.00868438
I0630 04:23:12.629448 29777 solver.cpp:290] Iteration 42200 (6.18699 iter/s, 16.1629s/100 iter), loss = 1.02381
I0630 04:23:12.629554 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 04:23:12.629565 29777 sgd_solver.cpp:106] Iteration 42200, lr = 0.00868125
I0630 04:23:28.604027 29777 solver.cpp:290] Iteration 42300 (6.26016 iter/s, 15.974s/100 iter), loss = 1.32143
I0630 04:23:28.604054 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 04:23:28.604063 29777 sgd_solver.cpp:106] Iteration 42300, lr = 0.00867813
I0630 04:23:44.820602 29777 solver.cpp:290] Iteration 42400 (6.16671 iter/s, 16.2161s/100 iter), loss = 0.928571
I0630 04:23:44.820675 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 04:23:44.820686 29777 sgd_solver.cpp:106] Iteration 42400, lr = 0.008675
I0630 04:24:01.015074 29777 solver.cpp:290] Iteration 42500 (6.17514 iter/s, 16.194s/100 iter), loss = 1.28571
I0630 04:24:01.015100 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 04:24:01.015108 29777 sgd_solver.cpp:106] Iteration 42500, lr = 0.00867188
I0630 04:24:17.253263 29777 solver.cpp:290] Iteration 42600 (6.1585 iter/s, 16.2377s/100 iter), loss = 1.25
I0630 04:24:17.253600 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 04:24:17.253609 29777 sgd_solver.cpp:106] Iteration 42600, lr = 0.00866875
I0630 04:24:33.453022 29777 solver.cpp:290] Iteration 42700 (6.17323 iter/s, 16.199s/100 iter), loss = 1.32143
I0630 04:24:33.453055 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 04:24:33.453065 29777 sgd_solver.cpp:106] Iteration 42700, lr = 0.00866563
I0630 04:24:49.597627 29777 solver.cpp:290] Iteration 42800 (6.1942 iter/s, 16.1441s/100 iter), loss = 0.928571
I0630 04:24:49.597756 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 04:24:49.597774 29777 sgd_solver.cpp:106] Iteration 42800, lr = 0.0086625
I0630 04:25:05.976855 29777 solver.cpp:290] Iteration 42900 (6.10551 iter/s, 16.3787s/100 iter), loss = 1.20238
I0630 04:25:05.976891 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 04:25:05.976902 29777 sgd_solver.cpp:106] Iteration 42900, lr = 0.00865937
I0630 04:25:22.036689 29777 solver.cpp:354] Sparsity after update:
I0630 04:25:22.056932 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:25:22.056967 29777 net.cpp:1851] conv1a_param_0(0.11) 
I0630 04:25:22.056999 29777 net.cpp:1851] conv1b_param_0(0.22) 
I0630 04:25:22.057014 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:25:22.057026 29777 net.cpp:1851] res2a_branch2a_param_0(0.22) 
I0630 04:25:22.057035 29777 net.cpp:1851] res2a_branch2b_param_0(0.22) 
I0630 04:25:22.057044 29777 net.cpp:1851] res3a_branch2a_param_0(0.22) 
I0630 04:25:22.057051 29777 net.cpp:1851] res3a_branch2b_param_0(0.22) 
I0630 04:25:22.057060 29777 net.cpp:1851] res4a_branch2a_param_0(0.22) 
I0630 04:25:22.057067 29777 net.cpp:1851] res4a_branch2b_param_0(0.22) 
I0630 04:25:22.057075 29777 net.cpp:1851] res5a_branch2a_param_0(0.22) 
I0630 04:25:22.057083 29777 net.cpp:1851] res5a_branch2b_param_0(0.22) 
I0630 04:25:22.057091 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (517776/2.86678e+06) 0.181
I0630 04:25:22.219544 29777 solver.cpp:290] Iteration 43000 (6.1568 iter/s, 16.2422s/100 iter), loss = 0.797619
I0630 04:25:22.219574 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 04:25:22.219583 29777 sgd_solver.cpp:106] Iteration 43000, lr = 0.00865625
I0630 04:25:38.518622 29777 solver.cpp:290] Iteration 43100 (6.1355 iter/s, 16.2986s/100 iter), loss = 1.13095
I0630 04:25:38.518645 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 04:25:38.518652 29777 sgd_solver.cpp:106] Iteration 43100, lr = 0.00865312
I0630 04:25:54.546969 29777 solver.cpp:290] Iteration 43200 (6.23913 iter/s, 16.0279s/100 iter), loss = 1.15476
I0630 04:25:54.547379 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 04:25:54.547389 29777 sgd_solver.cpp:106] Iteration 43200, lr = 0.00865
I0630 04:26:10.577729 29777 solver.cpp:290] Iteration 43300 (6.23834 iter/s, 16.0299s/100 iter), loss = 1.41667
I0630 04:26:10.577751 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 04:26:10.577757 29777 sgd_solver.cpp:106] Iteration 43300, lr = 0.00864687
I0630 04:26:26.814250 29777 solver.cpp:290] Iteration 43400 (6.15913 iter/s, 16.2361s/100 iter), loss = 1.36905
I0630 04:26:26.814359 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 04:26:26.814369 29777 sgd_solver.cpp:106] Iteration 43400, lr = 0.00864375
I0630 04:26:43.079380 29777 solver.cpp:290] Iteration 43500 (6.14833 iter/s, 16.2646s/100 iter), loss = 1.09524
I0630 04:26:43.079408 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 04:26:43.079417 29777 sgd_solver.cpp:106] Iteration 43500, lr = 0.00864062
I0630 04:26:59.085940 29777 solver.cpp:290] Iteration 43600 (6.24762 iter/s, 16.0061s/100 iter), loss = 0.869048
I0630 04:26:59.086011 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 04:26:59.086019 29777 sgd_solver.cpp:106] Iteration 43600, lr = 0.0086375
I0630 04:27:15.397831 29777 solver.cpp:290] Iteration 43700 (6.13069 iter/s, 16.3114s/100 iter), loss = 0.821429
I0630 04:27:15.397855 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 04:27:15.397862 29777 sgd_solver.cpp:106] Iteration 43700, lr = 0.00863438
I0630 04:27:31.456531 29777 solver.cpp:290] Iteration 43800 (6.22733 iter/s, 16.0582s/100 iter), loss = 1.15476
I0630 04:27:31.456601 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 04:27:31.456609 29777 sgd_solver.cpp:106] Iteration 43800, lr = 0.00863125
I0630 04:27:47.645931 29777 solver.cpp:290] Iteration 43900 (6.17708 iter/s, 16.1889s/100 iter), loss = 1.34524
I0630 04:27:47.645956 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 04:27:47.645961 29777 sgd_solver.cpp:106] Iteration 43900, lr = 0.00862813
I0630 04:28:03.540266 29777 solver.cpp:354] Sparsity after update:
I0630 04:28:03.541708 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:28:03.541715 29777 net.cpp:1851] conv1a_param_0(0.11) 
I0630 04:28:03.541723 29777 net.cpp:1851] conv1b_param_0(0.22) 
I0630 04:28:03.541725 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:28:03.541728 29777 net.cpp:1851] res2a_branch2a_param_0(0.22) 
I0630 04:28:03.541730 29777 net.cpp:1851] res2a_branch2b_param_0(0.22) 
I0630 04:28:03.541733 29777 net.cpp:1851] res3a_branch2a_param_0(0.22) 
I0630 04:28:03.541733 29777 net.cpp:1851] res3a_branch2b_param_0(0.22) 
I0630 04:28:03.541735 29777 net.cpp:1851] res4a_branch2a_param_0(0.22) 
I0630 04:28:03.541738 29777 net.cpp:1851] res4a_branch2b_param_0(0.22) 
I0630 04:28:03.541739 29777 net.cpp:1851] res5a_branch2a_param_0(0.22) 
I0630 04:28:03.541741 29777 net.cpp:1851] res5a_branch2b_param_0(0.22) 
I0630 04:28:03.541743 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (517776/2.86678e+06) 0.181
I0630 04:28:03.541836 29777 solver.cpp:471] Iteration 44000, Testing net (#0)
I0630 04:28:07.863615 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 04:29:07.743731 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56572
I0630 04:29:07.743850 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.801101
I0630 04:29:07.743860 29777 solver.cpp:544]     Test net output #2: loss = 1.56068 (* 1 = 1.56068 loss)
I0630 04:29:07.923477 29777 solver.cpp:290] Iteration 44000 (1.24571 iter/s, 80.2754s/100 iter), loss = 0.809524
I0630 04:29:07.923501 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 04:29:07.923507 29777 sgd_solver.cpp:106] Iteration 44000, lr = 0.008625
I0630 04:29:07.924203 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.23
I0630 04:29:08.081575 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 04:29:24.062317 29777 solver.cpp:290] Iteration 44100 (6.19641 iter/s, 16.1384s/100 iter), loss = 0.77381
I0630 04:29:24.062369 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 04:29:24.062382 29777 sgd_solver.cpp:106] Iteration 44100, lr = 0.00862188
I0630 04:29:40.269203 29777 solver.cpp:290] Iteration 44200 (6.17041 iter/s, 16.2064s/100 iter), loss = 0.964286
I0630 04:29:40.269294 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 04:29:40.269305 29777 sgd_solver.cpp:106] Iteration 44200, lr = 0.00861875
I0630 04:29:56.713963 29777 solver.cpp:290] Iteration 44300 (6.08116 iter/s, 16.4442s/100 iter), loss = 0.845238
I0630 04:29:56.713989 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 04:29:56.713997 29777 sgd_solver.cpp:106] Iteration 44300, lr = 0.00861563
I0630 04:30:13.610903 29777 solver.cpp:290] Iteration 44400 (5.9184 iter/s, 16.8965s/100 iter), loss = 1.07143
I0630 04:30:13.610958 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 04:30:13.610966 29777 sgd_solver.cpp:106] Iteration 44400, lr = 0.0086125
I0630 04:30:29.832273 29777 solver.cpp:290] Iteration 44500 (6.1649 iter/s, 16.2209s/100 iter), loss = 1.05952
I0630 04:30:29.832298 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 04:30:29.832304 29777 sgd_solver.cpp:106] Iteration 44500, lr = 0.00860937
I0630 04:30:46.269273 29777 solver.cpp:290] Iteration 44600 (6.08401 iter/s, 16.4365s/100 iter), loss = 1.0119
I0630 04:30:46.269556 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 04:30:46.269676 29777 sgd_solver.cpp:106] Iteration 44600, lr = 0.00860625
I0630 04:31:02.710292 29777 solver.cpp:290] Iteration 44700 (6.08262 iter/s, 16.4403s/100 iter), loss = 1.44048
I0630 04:31:02.710510 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 04:31:02.710634 29777 sgd_solver.cpp:106] Iteration 44700, lr = 0.00860312
I0630 04:31:18.954835 29777 solver.cpp:290] Iteration 44800 (6.15617 iter/s, 16.2439s/100 iter), loss = 1.2619
I0630 04:31:18.954932 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 04:31:18.954941 29777 sgd_solver.cpp:106] Iteration 44800, lr = 0.0086
I0630 04:31:35.031038 29777 solver.cpp:290] Iteration 44900 (6.22058 iter/s, 16.0757s/100 iter), loss = 1.35714
I0630 04:31:35.031061 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 04:31:35.031070 29777 sgd_solver.cpp:106] Iteration 44900, lr = 0.00859687
I0630 04:31:51.096997 29777 solver.cpp:354] Sparsity after update:
I0630 04:31:51.117339 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:31:51.117363 29777 net.cpp:1851] conv1a_param_0(0.115) 
I0630 04:31:51.117377 29777 net.cpp:1851] conv1b_param_0(0.23) 
I0630 04:31:51.117380 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:31:51.117384 29777 net.cpp:1851] res2a_branch2a_param_0(0.23) 
I0630 04:31:51.117388 29777 net.cpp:1851] res2a_branch2b_param_0(0.23) 
I0630 04:31:51.117400 29777 net.cpp:1851] res3a_branch2a_param_0(0.23) 
I0630 04:31:51.117406 29777 net.cpp:1851] res3a_branch2b_param_0(0.23) 
I0630 04:31:51.117411 29777 net.cpp:1851] res4a_branch2a_param_0(0.23) 
I0630 04:31:51.117418 29777 net.cpp:1851] res4a_branch2b_param_0(0.23) 
I0630 04:31:51.117420 29777 net.cpp:1851] res5a_branch2a_param_0(0.23) 
I0630 04:31:51.117424 29777 net.cpp:1851] res5a_branch2b_param_0(0.23) 
I0630 04:31:51.117427 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (541310/2.86678e+06) 0.189
I0630 04:31:51.279862 29777 solver.cpp:290] Iteration 45000 (6.15447 iter/s, 16.2483s/100 iter), loss = 0.988095
I0630 04:31:51.279911 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 04:31:51.279929 29777 sgd_solver.cpp:106] Iteration 45000, lr = 0.00859375
I0630 04:32:07.798758 29777 solver.cpp:290] Iteration 45100 (6.05386 iter/s, 16.5184s/100 iter), loss = 1.10714
I0630 04:32:07.798804 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 04:32:07.798844 29777 sgd_solver.cpp:106] Iteration 45100, lr = 0.00859062
I0630 04:32:24.145495 29777 solver.cpp:290] Iteration 45200 (6.11761 iter/s, 16.3462s/100 iter), loss = 0.77381
I0630 04:32:24.145607 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 04:32:24.145617 29777 sgd_solver.cpp:106] Iteration 45200, lr = 0.0085875
I0630 04:32:40.466820 29777 solver.cpp:290] Iteration 45300 (6.12716 iter/s, 16.3208s/100 iter), loss = 1.15476
I0630 04:32:40.466848 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 04:32:40.466856 29777 sgd_solver.cpp:106] Iteration 45300, lr = 0.00858437
I0630 04:32:56.800904 29777 solver.cpp:290] Iteration 45400 (6.12235 iter/s, 16.3336s/100 iter), loss = 1
I0630 04:32:56.800997 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 04:32:56.801009 29777 sgd_solver.cpp:106] Iteration 45400, lr = 0.00858125
I0630 04:33:13.182462 29777 solver.cpp:290] Iteration 45500 (6.10463 iter/s, 16.381s/100 iter), loss = 1.2381
I0630 04:33:13.182485 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 04:33:13.182494 29777 sgd_solver.cpp:106] Iteration 45500, lr = 0.00857813
I0630 04:33:29.466867 29777 solver.cpp:290] Iteration 45600 (6.14102 iter/s, 16.2839s/100 iter), loss = 0.77381
I0630 04:33:29.466953 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 04:33:29.466966 29777 sgd_solver.cpp:106] Iteration 45600, lr = 0.008575
I0630 04:33:45.824398 29777 solver.cpp:290] Iteration 45700 (6.11359 iter/s, 16.357s/100 iter), loss = 1.08333
I0630 04:33:45.824424 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 04:33:45.824432 29777 sgd_solver.cpp:106] Iteration 45700, lr = 0.00857188
I0630 04:34:01.990056 29777 solver.cpp:290] Iteration 45800 (6.18613 iter/s, 16.1652s/100 iter), loss = 0.940476
I0630 04:34:01.990139 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 04:34:01.990149 29777 sgd_solver.cpp:106] Iteration 45800, lr = 0.00856875
I0630 04:34:18.829054 29777 solver.cpp:290] Iteration 45900 (5.93879 iter/s, 16.8385s/100 iter), loss = 1.13095
I0630 04:34:18.829079 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 04:34:18.829088 29777 sgd_solver.cpp:106] Iteration 45900, lr = 0.00856563
I0630 04:34:34.891396 29777 solver.cpp:354] Sparsity after update:
I0630 04:34:34.892837 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:34:34.892843 29777 net.cpp:1851] conv1a_param_0(0.115) 
I0630 04:34:34.892850 29777 net.cpp:1851] conv1b_param_0(0.23) 
I0630 04:34:34.892853 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:34:34.892854 29777 net.cpp:1851] res2a_branch2a_param_0(0.23) 
I0630 04:34:34.892856 29777 net.cpp:1851] res2a_branch2b_param_0(0.23) 
I0630 04:34:34.892858 29777 net.cpp:1851] res3a_branch2a_param_0(0.23) 
I0630 04:34:34.892860 29777 net.cpp:1851] res3a_branch2b_param_0(0.23) 
I0630 04:34:34.892863 29777 net.cpp:1851] res4a_branch2a_param_0(0.23) 
I0630 04:34:34.892864 29777 net.cpp:1851] res4a_branch2b_param_0(0.23) 
I0630 04:34:34.892866 29777 net.cpp:1851] res5a_branch2a_param_0(0.23) 
I0630 04:34:34.892868 29777 net.cpp:1851] res5a_branch2b_param_0(0.23) 
I0630 04:34:34.892870 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (541310/2.86678e+06) 0.189
I0630 04:34:34.892956 29777 solver.cpp:471] Iteration 46000, Testing net (#0)
I0630 04:34:40.761488 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 04:35:43.134003 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.565259
I0630 04:35:43.134126 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.800741
I0630 04:35:43.134136 29777 solver.cpp:544]     Test net output #2: loss = 1.53432 (* 1 = 1.53432 loss)
I0630 04:35:43.313621 29777 solver.cpp:290] Iteration 46000 (1.18368 iter/s, 84.4822s/100 iter), loss = 0.916667
I0630 04:35:43.313643 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 04:35:43.313649 29777 sgd_solver.cpp:106] Iteration 46000, lr = 0.0085625
I0630 04:35:43.314358 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.24
I0630 04:35:43.489681 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 04:35:59.746204 29777 solver.cpp:290] Iteration 46100 (6.08565 iter/s, 16.4321s/100 iter), loss = 0.916667
I0630 04:35:59.746253 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 04:35:59.746268 29777 sgd_solver.cpp:106] Iteration 46100, lr = 0.00855938
I0630 04:36:15.877315 29777 solver.cpp:290] Iteration 46200 (6.19939 iter/s, 16.1306s/100 iter), loss = 1.2619
I0630 04:36:15.877446 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 04:36:15.877481 29777 sgd_solver.cpp:106] Iteration 46200, lr = 0.00855625
I0630 04:36:31.983208 29777 solver.cpp:290] Iteration 46300 (6.20913 iter/s, 16.1053s/100 iter), loss = 0.833333
I0630 04:36:31.983234 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 04:36:31.983242 29777 sgd_solver.cpp:106] Iteration 46300, lr = 0.00855312
I0630 04:36:48.173192 29777 solver.cpp:290] Iteration 46400 (6.17684 iter/s, 16.1895s/100 iter), loss = 1.15476
I0630 04:36:48.173297 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 04:36:48.173307 29777 sgd_solver.cpp:106] Iteration 46400, lr = 0.00855
I0630 04:37:04.314734 29777 solver.cpp:290] Iteration 46500 (6.19541 iter/s, 16.141s/100 iter), loss = 1.36905
I0630 04:37:04.314759 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 04:37:04.314765 29777 sgd_solver.cpp:106] Iteration 46500, lr = 0.00854687
I0630 04:37:20.455193 29777 solver.cpp:290] Iteration 46600 (6.19579 iter/s, 16.14s/100 iter), loss = 0.869048
I0630 04:37:20.455265 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 04:37:20.455276 29777 sgd_solver.cpp:106] Iteration 46600, lr = 0.00854375
I0630 04:37:36.645431 29777 solver.cpp:290] Iteration 46700 (6.17676 iter/s, 16.1897s/100 iter), loss = 0.964286
I0630 04:37:36.645457 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 04:37:36.645476 29777 sgd_solver.cpp:106] Iteration 46700, lr = 0.00854062
I0630 04:37:52.993628 29777 solver.cpp:290] Iteration 46800 (6.11706 iter/s, 16.3477s/100 iter), loss = 1.03571
I0630 04:37:52.993732 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 04:37:52.993742 29777 sgd_solver.cpp:106] Iteration 46800, lr = 0.0085375
I0630 04:38:09.156746 29777 solver.cpp:290] Iteration 46900 (6.18713 iter/s, 16.1626s/100 iter), loss = 0.916667
I0630 04:38:09.156770 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 04:38:09.156777 29777 sgd_solver.cpp:106] Iteration 46900, lr = 0.00853437
I0630 04:38:25.241323 29777 solver.cpp:354] Sparsity after update:
I0630 04:38:25.261468 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:38:25.261530 29777 net.cpp:1851] conv1a_param_0(0.12) 
I0630 04:38:25.261571 29777 net.cpp:1851] conv1b_param_0(0.24) 
I0630 04:38:25.261591 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:38:25.261610 29777 net.cpp:1851] res2a_branch2a_param_0(0.24) 
I0630 04:38:25.261626 29777 net.cpp:1851] res2a_branch2b_param_0(0.24) 
I0630 04:38:25.261641 29777 net.cpp:1851] res3a_branch2a_param_0(0.24) 
I0630 04:38:25.261656 29777 net.cpp:1851] res3a_branch2b_param_0(0.24) 
I0630 04:38:25.261672 29777 net.cpp:1851] res4a_branch2a_param_0(0.24) 
I0630 04:38:25.261687 29777 net.cpp:1851] res4a_branch2b_param_0(0.24) 
I0630 04:38:25.261703 29777 net.cpp:1851] res5a_branch2a_param_0(0.24) 
I0630 04:38:25.261718 29777 net.cpp:1851] res5a_branch2b_param_0(0.24) 
I0630 04:38:25.261735 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (564847/2.86678e+06) 0.197
I0630 04:38:25.418718 29777 solver.cpp:290] Iteration 47000 (6.14949 iter/s, 16.2615s/100 iter), loss = 0.75
I0630 04:38:25.418740 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 04:38:25.418747 29777 sgd_solver.cpp:106] Iteration 47000, lr = 0.00853125
I0630 04:38:41.565496 29777 solver.cpp:290] Iteration 47100 (6.19336 iter/s, 16.1463s/100 iter), loss = 0.869048
I0630 04:38:41.565522 29777 solver.cpp:309]     Train net output #0: loss = 0.571429 (* 1 = 0.571429 loss)
I0630 04:38:41.565531 29777 sgd_solver.cpp:106] Iteration 47100, lr = 0.00852813
I0630 04:38:57.840775 29777 solver.cpp:290] Iteration 47200 (6.14447 iter/s, 16.2748s/100 iter), loss = 1.40476
I0630 04:38:57.840838 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 04:38:57.840847 29777 sgd_solver.cpp:106] Iteration 47200, lr = 0.008525
I0630 04:39:13.931324 29777 solver.cpp:290] Iteration 47300 (6.21502 iter/s, 16.09s/100 iter), loss = 1.11905
I0630 04:39:13.931346 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 04:39:13.931354 29777 sgd_solver.cpp:106] Iteration 47300, lr = 0.00852188
I0630 04:39:30.030982 29777 solver.cpp:290] Iteration 47400 (6.21149 iter/s, 16.0992s/100 iter), loss = 0.964286
I0630 04:39:30.031097 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 04:39:30.031122 29777 sgd_solver.cpp:106] Iteration 47400, lr = 0.00851875
I0630 04:39:46.002210 29777 solver.cpp:290] Iteration 47500 (6.26148 iter/s, 15.9707s/100 iter), loss = 0.964286
I0630 04:39:46.002236 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 04:39:46.002245 29777 sgd_solver.cpp:106] Iteration 47500, lr = 0.00851563
I0630 04:40:02.155885 29777 solver.cpp:290] Iteration 47600 (6.19072 iter/s, 16.1532s/100 iter), loss = 1
I0630 04:40:02.155990 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 04:40:02.156002 29777 sgd_solver.cpp:106] Iteration 47600, lr = 0.0085125
I0630 04:40:18.290607 29777 solver.cpp:290] Iteration 47700 (6.19802 iter/s, 16.1342s/100 iter), loss = 0.869048
I0630 04:40:18.290634 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 04:40:18.290643 29777 sgd_solver.cpp:106] Iteration 47700, lr = 0.00850937
I0630 04:40:34.433589 29777 solver.cpp:290] Iteration 47800 (6.19482 iter/s, 16.1425s/100 iter), loss = 0.869048
I0630 04:40:34.433969 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 04:40:34.433980 29777 sgd_solver.cpp:106] Iteration 47800, lr = 0.00850625
I0630 04:40:50.762356 29777 solver.cpp:290] Iteration 47900 (6.12447 iter/s, 16.3279s/100 iter), loss = 1.10714
I0630 04:40:50.762378 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 04:40:50.762384 29777 sgd_solver.cpp:106] Iteration 47900, lr = 0.00850312
I0630 04:41:06.759085 29777 solver.cpp:354] Sparsity after update:
I0630 04:41:06.760677 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:41:06.760685 29777 net.cpp:1851] conv1a_param_0(0.12) 
I0630 04:41:06.760692 29777 net.cpp:1851] conv1b_param_0(0.24) 
I0630 04:41:06.760694 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:41:06.760696 29777 net.cpp:1851] res2a_branch2a_param_0(0.24) 
I0630 04:41:06.760699 29777 net.cpp:1851] res2a_branch2b_param_0(0.24) 
I0630 04:41:06.760701 29777 net.cpp:1851] res3a_branch2a_param_0(0.24) 
I0630 04:41:06.760702 29777 net.cpp:1851] res3a_branch2b_param_0(0.24) 
I0630 04:41:06.760704 29777 net.cpp:1851] res4a_branch2a_param_0(0.24) 
I0630 04:41:06.760706 29777 net.cpp:1851] res4a_branch2b_param_0(0.24) 
I0630 04:41:06.760709 29777 net.cpp:1851] res5a_branch2a_param_0(0.24) 
I0630 04:41:06.760710 29777 net.cpp:1851] res5a_branch2b_param_0(0.24) 
I0630 04:41:06.760712 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (564847/2.86678e+06) 0.197
I0630 04:41:06.760798 29777 solver.cpp:471] Iteration 48000, Testing net (#0)
I0630 04:41:11.772408 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 04:42:08.044862 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55774
I0630 04:42:08.044947 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.798461
I0630 04:42:08.044957 29777 solver.cpp:544]     Test net output #2: loss = 1.56562 (* 1 = 1.56562 loss)
I0630 04:42:08.220387 29777 solver.cpp:290] Iteration 48000 (1.29106 iter/s, 77.4559s/100 iter), loss = 0.928571
I0630 04:42:08.220410 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 04:42:08.220417 29777 sgd_solver.cpp:106] Iteration 48000, lr = 0.0085
I0630 04:42:08.221120 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.25
I0630 04:42:08.385107 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 04:42:23.752986 29777 solver.cpp:290] Iteration 48100 (6.43826 iter/s, 15.5321s/100 iter), loss = 1.27381
I0630 04:42:23.753010 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 04:42:23.753020 29777 sgd_solver.cpp:106] Iteration 48100, lr = 0.00849687
I0630 04:42:39.886415 29777 solver.cpp:290] Iteration 48200 (6.19849 iter/s, 16.133s/100 iter), loss = 1.42857
I0630 04:42:39.886514 29777 solver.cpp:309]     Train net output #0: loss = 1.88095 (* 1 = 1.88095 loss)
I0630 04:42:39.886523 29777 sgd_solver.cpp:106] Iteration 48200, lr = 0.00849375
I0630 04:42:56.082432 29777 solver.cpp:290] Iteration 48300 (6.17457 iter/s, 16.1955s/100 iter), loss = 1.25
I0630 04:42:56.082631 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 04:42:56.082737 29777 sgd_solver.cpp:106] Iteration 48300, lr = 0.00849062
I0630 04:43:12.221740 29777 solver.cpp:290] Iteration 48400 (6.1963 iter/s, 16.1387s/100 iter), loss = 0.809524
I0630 04:43:12.221844 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 04:43:12.221853 29777 sgd_solver.cpp:106] Iteration 48400, lr = 0.0084875
I0630 04:43:28.421361 29777 solver.cpp:290] Iteration 48500 (6.17319 iter/s, 16.1991s/100 iter), loss = 0.797619
I0630 04:43:28.421387 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 04:43:28.421396 29777 sgd_solver.cpp:106] Iteration 48500, lr = 0.00848437
I0630 04:43:44.479920 29777 solver.cpp:290] Iteration 48600 (6.22739 iter/s, 16.0581s/100 iter), loss = 0.904762
I0630 04:43:44.480013 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 04:43:44.480024 29777 sgd_solver.cpp:106] Iteration 48600, lr = 0.00848125
I0630 04:44:00.560290 29777 solver.cpp:290] Iteration 48700 (6.21897 iter/s, 16.0798s/100 iter), loss = 0.809524
I0630 04:44:00.560312 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 04:44:00.560319 29777 sgd_solver.cpp:106] Iteration 48700, lr = 0.00847813
I0630 04:44:16.771867 29777 solver.cpp:290] Iteration 48800 (6.16861 iter/s, 16.2111s/100 iter), loss = 1.38095
I0630 04:44:16.771920 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 04:44:16.771930 29777 sgd_solver.cpp:106] Iteration 48800, lr = 0.008475
I0630 04:44:32.986497 29777 solver.cpp:290] Iteration 48900 (6.16746 iter/s, 16.2141s/100 iter), loss = 1.35714
I0630 04:44:32.986523 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 04:44:32.986532 29777 sgd_solver.cpp:106] Iteration 48900, lr = 0.00847188
I0630 04:44:48.930455 29777 solver.cpp:354] Sparsity after update:
I0630 04:44:48.952240 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:44:48.952256 29777 net.cpp:1851] conv1a_param_0(0.125) 
I0630 04:44:48.952267 29777 net.cpp:1851] conv1b_param_0(0.25) 
I0630 04:44:48.952271 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:44:48.952276 29777 net.cpp:1851] res2a_branch2a_param_0(0.25) 
I0630 04:44:48.952281 29777 net.cpp:1851] res2a_branch2b_param_0(0.25) 
I0630 04:44:48.952285 29777 net.cpp:1851] res3a_branch2a_param_0(0.25) 
I0630 04:44:48.952287 29777 net.cpp:1851] res3a_branch2b_param_0(0.25) 
I0630 04:44:48.952291 29777 net.cpp:1851] res4a_branch2a_param_0(0.25) 
I0630 04:44:48.952294 29777 net.cpp:1851] res4a_branch2b_param_0(0.25) 
I0630 04:44:48.952298 29777 net.cpp:1851] res5a_branch2a_param_0(0.25) 
I0630 04:44:48.952302 29777 net.cpp:1851] res5a_branch2b_param_0(0.25) 
I0630 04:44:48.952307 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (588394/2.86678e+06) 0.205
I0630 04:44:49.137440 29777 solver.cpp:290] Iteration 49000 (6.19177 iter/s, 16.1505s/100 iter), loss = 1.02381
I0630 04:44:49.137490 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 04:44:49.137509 29777 sgd_solver.cpp:106] Iteration 49000, lr = 0.00846875
I0630 04:45:05.219895 29777 solver.cpp:290] Iteration 49100 (6.21814 iter/s, 16.082s/100 iter), loss = 0.857143
I0630 04:45:05.219918 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 04:45:05.219925 29777 sgd_solver.cpp:106] Iteration 49100, lr = 0.00846562
I0630 04:45:21.479049 29777 solver.cpp:290] Iteration 49200 (6.15056 iter/s, 16.2587s/100 iter), loss = 1.27381
I0630 04:45:21.479152 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 04:45:21.479162 29777 sgd_solver.cpp:106] Iteration 49200, lr = 0.0084625
I0630 04:45:37.996572 29777 solver.cpp:290] Iteration 49300 (6.05438 iter/s, 16.517s/100 iter), loss = 1.02381
I0630 04:45:37.996596 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 04:45:37.996605 29777 sgd_solver.cpp:106] Iteration 49300, lr = 0.00845937
I0630 04:45:54.264950 29777 solver.cpp:290] Iteration 49400 (6.14707 iter/s, 16.2679s/100 iter), loss = 1.21429
I0630 04:45:54.265008 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 04:45:54.265017 29777 sgd_solver.cpp:106] Iteration 49400, lr = 0.00845625
I0630 04:46:10.626422 29777 solver.cpp:290] Iteration 49500 (6.11211 iter/s, 16.361s/100 iter), loss = 0.940476
I0630 04:46:10.626447 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 04:46:10.626456 29777 sgd_solver.cpp:106] Iteration 49500, lr = 0.00845312
I0630 04:46:27.040093 29777 solver.cpp:290] Iteration 49600 (6.09266 iter/s, 16.4132s/100 iter), loss = 1.07143
I0630 04:46:27.040160 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 04:46:27.040170 29777 sgd_solver.cpp:106] Iteration 49600, lr = 0.00845
I0630 04:46:43.375452 29777 solver.cpp:290] Iteration 49700 (6.12188 iter/s, 16.3348s/100 iter), loss = 0.916667
I0630 04:46:43.375478 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 04:46:43.375485 29777 sgd_solver.cpp:106] Iteration 49700, lr = 0.00844688
I0630 04:46:59.567055 29777 solver.cpp:290] Iteration 49800 (6.17622 iter/s, 16.1911s/100 iter), loss = 1.40476
I0630 04:46:59.567178 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 04:46:59.567193 29777 sgd_solver.cpp:106] Iteration 49800, lr = 0.00844375
I0630 04:47:15.928985 29777 solver.cpp:290] Iteration 49900 (6.11196 iter/s, 16.3614s/100 iter), loss = 1.15476
I0630 04:47:15.929013 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 04:47:15.929028 29777 sgd_solver.cpp:106] Iteration 49900, lr = 0.00844062
I0630 04:47:32.103340 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_50000.caffemodel
I0630 04:47:32.122853 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_50000.solverstate
I0630 04:47:32.131590 29777 solver.cpp:354] Sparsity after update:
I0630 04:47:32.132553 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:47:32.132562 29777 net.cpp:1851] conv1a_param_0(0.125) 
I0630 04:47:32.132570 29777 net.cpp:1851] conv1b_param_0(0.25) 
I0630 04:47:32.132572 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:47:32.132575 29777 net.cpp:1851] res2a_branch2a_param_0(0.25) 
I0630 04:47:32.132576 29777 net.cpp:1851] res2a_branch2b_param_0(0.25) 
I0630 04:47:32.132578 29777 net.cpp:1851] res3a_branch2a_param_0(0.25) 
I0630 04:47:32.132580 29777 net.cpp:1851] res3a_branch2b_param_0(0.25) 
I0630 04:47:32.132582 29777 net.cpp:1851] res4a_branch2a_param_0(0.25) 
I0630 04:47:32.132585 29777 net.cpp:1851] res4a_branch2b_param_0(0.25) 
I0630 04:47:32.132586 29777 net.cpp:1851] res5a_branch2a_param_0(0.25) 
I0630 04:47:32.132589 29777 net.cpp:1851] res5a_branch2b_param_0(0.25) 
I0630 04:47:32.132593 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (588394/2.86678e+06) 0.205
I0630 04:47:32.132696 29777 solver.cpp:471] Iteration 50000, Testing net (#0)
I0630 04:47:39.136782 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 04:48:38.582655 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55672
I0630 04:48:38.582725 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.792541
I0630 04:48:38.582731 29777 solver.cpp:544]     Test net output #2: loss = 1.593 (* 1 = 1.593 loss)
I0630 04:48:38.772855 29777 solver.cpp:290] Iteration 50000 (1.20712 iter/s, 82.8416s/100 iter), loss = 1.10714
I0630 04:48:38.772879 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 04:48:38.772886 29777 sgd_solver.cpp:106] Iteration 50000, lr = 0.0084375
I0630 04:48:38.773610 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.26
I0630 04:48:38.951961 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 04:48:54.698387 29777 solver.cpp:290] Iteration 50100 (6.27941 iter/s, 15.9251s/100 iter), loss = 1.27381
I0630 04:48:54.698412 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 04:48:54.698421 29777 sgd_solver.cpp:106] Iteration 50100, lr = 0.00843437
I0630 04:49:10.784447 29777 solver.cpp:290] Iteration 50200 (6.21674 iter/s, 16.0856s/100 iter), loss = 0.642857
I0630 04:49:10.784499 29777 solver.cpp:309]     Train net output #0: loss = 0.52381 (* 1 = 0.52381 loss)
I0630 04:49:10.784507 29777 sgd_solver.cpp:106] Iteration 50200, lr = 0.00843125
I0630 04:49:26.993710 29777 solver.cpp:290] Iteration 50300 (6.1695 iter/s, 16.2088s/100 iter), loss = 1.03571
I0630 04:49:26.993734 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 04:49:26.993741 29777 sgd_solver.cpp:106] Iteration 50300, lr = 0.00842812
I0630 04:49:43.263438 29777 solver.cpp:290] Iteration 50400 (6.14656 iter/s, 16.2693s/100 iter), loss = 1.14286
I0630 04:49:43.263550 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 04:49:43.263567 29777 sgd_solver.cpp:106] Iteration 50400, lr = 0.008425
I0630 04:49:59.637393 29777 solver.cpp:290] Iteration 50500 (6.10747 iter/s, 16.3734s/100 iter), loss = 1.07143
I0630 04:49:59.637419 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 04:49:59.637428 29777 sgd_solver.cpp:106] Iteration 50500, lr = 0.00842187
I0630 04:50:16.024164 29777 solver.cpp:290] Iteration 50600 (6.10266 iter/s, 16.3863s/100 iter), loss = 1.15476
I0630 04:50:16.024238 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 04:50:16.024245 29777 sgd_solver.cpp:106] Iteration 50600, lr = 0.00841875
I0630 04:50:32.394675 29777 solver.cpp:290] Iteration 50700 (6.10874 iter/s, 16.37s/100 iter), loss = 1.22619
I0630 04:50:32.394726 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 04:50:32.394740 29777 sgd_solver.cpp:106] Iteration 50700, lr = 0.00841562
I0630 04:50:48.497213 29777 solver.cpp:290] Iteration 50800 (6.21039 iter/s, 16.102s/100 iter), loss = 1.10714
I0630 04:50:48.497349 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 04:50:48.497367 29777 sgd_solver.cpp:106] Iteration 50800, lr = 0.0084125
I0630 04:51:05.109000 29777 solver.cpp:290] Iteration 50900 (6.02003 iter/s, 16.6112s/100 iter), loss = 1.02381
I0630 04:51:05.109025 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 04:51:05.109032 29777 sgd_solver.cpp:106] Iteration 50900, lr = 0.00840937
I0630 04:51:21.563814 29777 solver.cpp:354] Sparsity after update:
I0630 04:51:21.583976 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:51:21.583992 29777 net.cpp:1851] conv1a_param_0(0.13) 
I0630 04:51:21.584002 29777 net.cpp:1851] conv1b_param_0(0.26) 
I0630 04:51:21.584005 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:51:21.584009 29777 net.cpp:1851] res2a_branch2a_param_0(0.26) 
I0630 04:51:21.584020 29777 net.cpp:1851] res2a_branch2b_param_0(0.26) 
I0630 04:51:21.584026 29777 net.cpp:1851] res3a_branch2a_param_0(0.26) 
I0630 04:51:21.584031 29777 net.cpp:1851] res3a_branch2b_param_0(0.26) 
I0630 04:51:21.584038 29777 net.cpp:1851] res4a_branch2a_param_0(0.26) 
I0630 04:51:21.584043 29777 net.cpp:1851] res4a_branch2b_param_0(0.26) 
I0630 04:51:21.584048 29777 net.cpp:1851] res5a_branch2a_param_0(0.26) 
I0630 04:51:21.584051 29777 net.cpp:1851] res5a_branch2b_param_0(0.26) 
I0630 04:51:21.584055 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (611919/2.86678e+06) 0.213
I0630 04:51:21.743706 29777 solver.cpp:290] Iteration 51000 (6.0117 iter/s, 16.6342s/100 iter), loss = 1.08333
I0630 04:51:21.743729 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 04:51:21.743736 29777 sgd_solver.cpp:106] Iteration 51000, lr = 0.00840625
I0630 04:51:37.844169 29777 solver.cpp:290] Iteration 51100 (6.21118 iter/s, 16.1s/100 iter), loss = 1.35714
I0630 04:51:37.844194 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 04:51:37.844202 29777 sgd_solver.cpp:106] Iteration 51100, lr = 0.00840312
I0630 04:51:53.970340 29777 solver.cpp:290] Iteration 51200 (6.20128 iter/s, 16.1257s/100 iter), loss = 1.15476
I0630 04:51:53.970435 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 04:51:53.970460 29777 sgd_solver.cpp:106] Iteration 51200, lr = 0.0084
I0630 04:52:10.231788 29777 solver.cpp:290] Iteration 51300 (6.14971 iter/s, 16.2609s/100 iter), loss = 0.904762
I0630 04:52:10.231815 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 04:52:10.231824 29777 sgd_solver.cpp:106] Iteration 51300, lr = 0.00839687
I0630 04:52:26.694619 29777 solver.cpp:290] Iteration 51400 (6.07446 iter/s, 16.4624s/100 iter), loss = 1
I0630 04:52:26.694715 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 04:52:26.694726 29777 sgd_solver.cpp:106] Iteration 51400, lr = 0.00839375
I0630 04:52:42.801697 29777 solver.cpp:290] Iteration 51500 (6.20866 iter/s, 16.1065s/100 iter), loss = 1.5
I0630 04:52:42.801718 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 04:52:42.801725 29777 sgd_solver.cpp:106] Iteration 51500, lr = 0.00839063
I0630 04:52:58.918469 29777 solver.cpp:290] Iteration 51600 (6.2049 iter/s, 16.1163s/100 iter), loss = 1.0119
I0630 04:52:58.918555 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 04:52:58.918596 29777 sgd_solver.cpp:106] Iteration 51600, lr = 0.0083875
I0630 04:53:15.238268 29777 solver.cpp:290] Iteration 51700 (6.12773 iter/s, 16.3193s/100 iter), loss = 1.2381
I0630 04:53:15.238364 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 04:53:15.238399 29777 sgd_solver.cpp:106] Iteration 51700, lr = 0.00838437
I0630 04:53:31.525868 29777 solver.cpp:290] Iteration 51800 (6.13984 iter/s, 16.2871s/100 iter), loss = 0.821429
I0630 04:53:31.525984 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 04:53:31.525993 29777 sgd_solver.cpp:106] Iteration 51800, lr = 0.00838125
I0630 04:53:47.708698 29777 solver.cpp:290] Iteration 51900 (6.1796 iter/s, 16.1823s/100 iter), loss = 1.05952
I0630 04:53:47.708739 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 04:53:47.708751 29777 sgd_solver.cpp:106] Iteration 51900, lr = 0.00837812
I0630 04:54:03.647191 29777 solver.cpp:354] Sparsity after update:
I0630 04:54:03.648450 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:54:03.648459 29777 net.cpp:1851] conv1a_param_0(0.13) 
I0630 04:54:03.648466 29777 net.cpp:1851] conv1b_param_0(0.26) 
I0630 04:54:03.648469 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:54:03.648471 29777 net.cpp:1851] res2a_branch2a_param_0(0.26) 
I0630 04:54:03.648474 29777 net.cpp:1851] res2a_branch2b_param_0(0.26) 
I0630 04:54:03.648476 29777 net.cpp:1851] res3a_branch2a_param_0(0.26) 
I0630 04:54:03.648479 29777 net.cpp:1851] res3a_branch2b_param_0(0.26) 
I0630 04:54:03.648481 29777 net.cpp:1851] res4a_branch2a_param_0(0.26) 
I0630 04:54:03.648483 29777 net.cpp:1851] res4a_branch2b_param_0(0.26) 
I0630 04:54:03.648486 29777 net.cpp:1851] res5a_branch2a_param_0(0.26) 
I0630 04:54:03.648488 29777 net.cpp:1851] res5a_branch2b_param_0(0.26) 
I0630 04:54:03.648490 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (611919/2.86678e+06) 0.213
I0630 04:54:03.648581 29777 solver.cpp:471] Iteration 52000, Testing net (#0)
I0630 04:54:10.380851 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 04:55:07.970446 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.564379
I0630 04:55:07.970504 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.799021
I0630 04:55:07.970512 29777 solver.cpp:544]     Test net output #2: loss = 1.56632 (* 1 = 1.56632 loss)
I0630 04:55:08.151274 29777 solver.cpp:290] Iteration 52000 (1.24316 iter/s, 80.4404s/100 iter), loss = 0.821429
I0630 04:55:08.151299 29777 solver.cpp:309]     Train net output #0: loss = 0.5 (* 1 = 0.5 loss)
I0630 04:55:08.151306 29777 sgd_solver.cpp:106] Iteration 52000, lr = 0.008375
I0630 04:55:08.152019 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.27
I0630 04:55:08.325037 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 04:55:23.817401 29777 solver.cpp:290] Iteration 52100 (6.38339 iter/s, 15.6657s/100 iter), loss = 0.785714
I0630 04:55:23.817459 29777 solver.cpp:309]     Train net output #0: loss = 0.595238 (* 1 = 0.595238 loss)
I0630 04:55:23.817481 29777 sgd_solver.cpp:106] Iteration 52100, lr = 0.00837187
I0630 04:55:39.929255 29777 solver.cpp:290] Iteration 52200 (6.2068 iter/s, 16.1114s/100 iter), loss = 0.97619
I0630 04:55:39.929332 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 04:55:39.929342 29777 sgd_solver.cpp:106] Iteration 52200, lr = 0.00836875
I0630 04:55:56.243973 29777 solver.cpp:290] Iteration 52300 (6.12963 iter/s, 16.3142s/100 iter), loss = 0.964286
I0630 04:55:56.243998 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 04:55:56.244004 29777 sgd_solver.cpp:106] Iteration 52300, lr = 0.00836562
I0630 04:56:12.464828 29777 solver.cpp:290] Iteration 52400 (6.16508 iter/s, 16.2204s/100 iter), loss = 0.797619
I0630 04:56:12.464905 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 04:56:12.464913 29777 sgd_solver.cpp:106] Iteration 52400, lr = 0.0083625
I0630 04:56:28.771872 29777 solver.cpp:290] Iteration 52500 (6.13252 iter/s, 16.3065s/100 iter), loss = 1.14286
I0630 04:56:28.771898 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 04:56:28.771908 29777 sgd_solver.cpp:106] Iteration 52500, lr = 0.00835937
I0630 04:56:44.898391 29777 solver.cpp:290] Iteration 52600 (6.20115 iter/s, 16.126s/100 iter), loss = 1
I0630 04:56:44.898551 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 04:56:44.898576 29777 sgd_solver.cpp:106] Iteration 52600, lr = 0.00835625
I0630 04:57:01.165017 29777 solver.cpp:290] Iteration 52700 (6.14778 iter/s, 16.266s/100 iter), loss = 0.940476
I0630 04:57:01.165042 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 04:57:01.165051 29777 sgd_solver.cpp:106] Iteration 52700, lr = 0.00835312
I0630 04:57:17.317651 29777 solver.cpp:290] Iteration 52800 (6.19112 iter/s, 16.1522s/100 iter), loss = 0.821429
I0630 04:57:17.317762 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 04:57:17.317773 29777 sgd_solver.cpp:106] Iteration 52800, lr = 0.00835
I0630 04:57:33.405092 29777 solver.cpp:290] Iteration 52900 (6.21624 iter/s, 16.0869s/100 iter), loss = 1.42857
I0630 04:57:33.405120 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 04:57:33.405129 29777 sgd_solver.cpp:106] Iteration 52900, lr = 0.00834687
I0630 04:57:49.503479 29777 solver.cpp:354] Sparsity after update:
I0630 04:57:49.523887 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 04:57:49.523903 29777 net.cpp:1851] conv1a_param_0(0.135) 
I0630 04:57:49.523911 29777 net.cpp:1851] conv1b_param_0(0.27) 
I0630 04:57:49.523913 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 04:57:49.523916 29777 net.cpp:1851] res2a_branch2a_param_0(0.27) 
I0630 04:57:49.523918 29777 net.cpp:1851] res2a_branch2b_param_0(0.27) 
I0630 04:57:49.523921 29777 net.cpp:1851] res3a_branch2a_param_0(0.27) 
I0630 04:57:49.523922 29777 net.cpp:1851] res3a_branch2b_param_0(0.27) 
I0630 04:57:49.523924 29777 net.cpp:1851] res4a_branch2a_param_0(0.27) 
I0630 04:57:49.523926 29777 net.cpp:1851] res4a_branch2b_param_0(0.27) 
I0630 04:57:49.523927 29777 net.cpp:1851] res5a_branch2a_param_0(0.27) 
I0630 04:57:49.523931 29777 net.cpp:1851] res5a_branch2b_param_0(0.27) 
I0630 04:57:49.523932 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (635461/2.86678e+06) 0.222
I0630 04:57:49.678022 29777 solver.cpp:290] Iteration 53000 (6.14535 iter/s, 16.2725s/100 iter), loss = 0.785714
I0630 04:57:49.678048 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 04:57:49.678056 29777 sgd_solver.cpp:106] Iteration 53000, lr = 0.00834375
I0630 04:58:05.818056 29777 solver.cpp:290] Iteration 53100 (6.19595 iter/s, 16.1396s/100 iter), loss = 1.15476
I0630 04:58:05.818150 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 04:58:05.818186 29777 sgd_solver.cpp:106] Iteration 53100, lr = 0.00834063
I0630 04:58:22.014003 29777 solver.cpp:290] Iteration 53200 (6.17459 iter/s, 16.1954s/100 iter), loss = 0.77381
I0630 04:58:22.014111 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 04:58:22.014120 29777 sgd_solver.cpp:106] Iteration 53200, lr = 0.0083375
I0630 04:58:38.211182 29777 solver.cpp:290] Iteration 53300 (6.17413 iter/s, 16.1966s/100 iter), loss = 0.77381
I0630 04:58:38.211232 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 04:58:38.211249 29777 sgd_solver.cpp:106] Iteration 53300, lr = 0.00833437
I0630 04:58:54.382899 29777 solver.cpp:290] Iteration 53400 (6.18383 iter/s, 16.1712s/100 iter), loss = 0.940476
I0630 04:58:54.383033 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 04:58:54.383066 29777 sgd_solver.cpp:106] Iteration 53400, lr = 0.00833125
I0630 04:59:10.789826 29777 solver.cpp:290] Iteration 53500 (6.0952 iter/s, 16.4063s/100 iter), loss = 1.2619
I0630 04:59:10.789855 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 04:59:10.789863 29777 sgd_solver.cpp:106] Iteration 53500, lr = 0.00832812
I0630 04:59:27.073099 29777 solver.cpp:290] Iteration 53600 (6.14145 iter/s, 16.2828s/100 iter), loss = 1.15476
I0630 04:59:27.073185 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 04:59:27.073194 29777 sgd_solver.cpp:106] Iteration 53600, lr = 0.008325
I0630 04:59:43.228124 29777 solver.cpp:290] Iteration 53700 (6.19023 iter/s, 16.1545s/100 iter), loss = 0.821429
I0630 04:59:43.228219 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 04:59:43.228255 29777 sgd_solver.cpp:106] Iteration 53700, lr = 0.00832187
I0630 04:59:59.255550 29777 solver.cpp:290] Iteration 53800 (6.23951 iter/s, 16.0269s/100 iter), loss = 1.13095
I0630 04:59:59.255635 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 04:59:59.255642 29777 sgd_solver.cpp:106] Iteration 53800, lr = 0.00831875
I0630 05:00:15.417161 29777 solver.cpp:290] Iteration 53900 (6.1877 iter/s, 16.1611s/100 iter), loss = 1.05952
I0630 05:00:15.417192 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 05:00:15.417202 29777 sgd_solver.cpp:106] Iteration 53900, lr = 0.00831562
I0630 05:00:31.371332 29777 solver.cpp:354] Sparsity after update:
I0630 05:00:31.372993 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:00:31.373001 29777 net.cpp:1851] conv1a_param_0(0.135) 
I0630 05:00:31.373008 29777 net.cpp:1851] conv1b_param_0(0.27) 
I0630 05:00:31.373010 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:00:31.373013 29777 net.cpp:1851] res2a_branch2a_param_0(0.27) 
I0630 05:00:31.373015 29777 net.cpp:1851] res2a_branch2b_param_0(0.27) 
I0630 05:00:31.373016 29777 net.cpp:1851] res3a_branch2a_param_0(0.27) 
I0630 05:00:31.373018 29777 net.cpp:1851] res3a_branch2b_param_0(0.27) 
I0630 05:00:31.373020 29777 net.cpp:1851] res4a_branch2a_param_0(0.27) 
I0630 05:00:31.373023 29777 net.cpp:1851] res4a_branch2b_param_0(0.27) 
I0630 05:00:31.373024 29777 net.cpp:1851] res5a_branch2a_param_0(0.27) 
I0630 05:00:31.373026 29777 net.cpp:1851] res5a_branch2b_param_0(0.27) 
I0630 05:00:31.373028 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (635461/2.86678e+06) 0.222
I0630 05:00:31.373126 29777 solver.cpp:471] Iteration 54000, Testing net (#0)
I0630 05:00:37.095865 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 05:01:31.203810 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.570359
I0630 05:01:31.203935 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.804982
I0630 05:01:31.203960 29777 solver.cpp:544]     Test net output #2: loss = 1.54 (* 1 = 1.54 loss)
I0630 05:01:31.413530 29777 solver.cpp:290] Iteration 54000 (1.31589 iter/s, 75.9943s/100 iter), loss = 1.05952
I0630 05:01:31.413574 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 05:01:31.413584 29777 sgd_solver.cpp:106] Iteration 54000, lr = 0.0083125
I0630 05:01:31.414944 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.28
I0630 05:01:31.737804 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 05:01:47.851517 29777 solver.cpp:290] Iteration 54100 (6.08365 iter/s, 16.4375s/100 iter), loss = 0.964286
I0630 05:01:47.851542 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 05:01:47.851550 29777 sgd_solver.cpp:106] Iteration 54100, lr = 0.00830937
I0630 05:02:04.060655 29777 solver.cpp:290] Iteration 54200 (6.16954 iter/s, 16.2087s/100 iter), loss = 0.976191
I0630 05:02:04.062906 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 05:02:04.062932 29777 sgd_solver.cpp:106] Iteration 54200, lr = 0.00830625
I0630 05:02:20.437620 29777 solver.cpp:290] Iteration 54300 (6.10714 iter/s, 16.3743s/100 iter), loss = 1.11905
I0630 05:02:20.437649 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 05:02:20.437659 29777 sgd_solver.cpp:106] Iteration 54300, lr = 0.00830312
I0630 05:02:36.973553 29777 solver.cpp:290] Iteration 54400 (6.04761 iter/s, 16.5355s/100 iter), loss = 1.11905
I0630 05:02:36.973659 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 05:02:36.973670 29777 sgd_solver.cpp:106] Iteration 54400, lr = 0.0083
I0630 05:02:53.265305 29777 solver.cpp:290] Iteration 54500 (6.13828 iter/s, 16.2912s/100 iter), loss = 1.07143
I0630 05:02:53.265332 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 05:02:53.265338 29777 sgd_solver.cpp:106] Iteration 54500, lr = 0.00829687
I0630 05:03:09.457733 29777 solver.cpp:290] Iteration 54600 (6.17591 iter/s, 16.192s/100 iter), loss = 1.13095
I0630 05:03:09.457834 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 05:03:09.457844 29777 sgd_solver.cpp:106] Iteration 54600, lr = 0.00829375
I0630 05:03:25.719650 29777 solver.cpp:290] Iteration 54700 (6.14954 iter/s, 16.2614s/100 iter), loss = 0.595238
I0630 05:03:25.719674 29777 solver.cpp:309]     Train net output #0: loss = 0.428571 (* 1 = 0.428571 loss)
I0630 05:03:25.719681 29777 sgd_solver.cpp:106] Iteration 54700, lr = 0.00829063
I0630 05:03:42.012815 29777 solver.cpp:290] Iteration 54800 (6.13772 iter/s, 16.2927s/100 iter), loss = 0.904762
I0630 05:03:42.012926 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 05:03:42.012936 29777 sgd_solver.cpp:106] Iteration 54800, lr = 0.0082875
I0630 05:03:58.519570 29777 solver.cpp:290] Iteration 54900 (6.05833 iter/s, 16.5062s/100 iter), loss = 0.952381
I0630 05:03:58.519592 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 05:03:58.519598 29777 sgd_solver.cpp:106] Iteration 54900, lr = 0.00828438
I0630 05:04:14.764194 29777 solver.cpp:354] Sparsity after update:
I0630 05:04:14.784940 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:04:14.784957 29777 net.cpp:1851] conv1a_param_0(0.14) 
I0630 05:04:14.784967 29777 net.cpp:1851] conv1b_param_0(0.28) 
I0630 05:04:14.784971 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:04:14.784974 29777 net.cpp:1851] res2a_branch2a_param_0(0.28) 
I0630 05:04:14.784978 29777 net.cpp:1851] res2a_branch2b_param_0(0.28) 
I0630 05:04:14.784991 29777 net.cpp:1851] res3a_branch2a_param_0(0.28) 
I0630 05:04:14.784996 29777 net.cpp:1851] res3a_branch2b_param_0(0.28) 
I0630 05:04:14.785001 29777 net.cpp:1851] res4a_branch2a_param_0(0.28) 
I0630 05:04:14.785006 29777 net.cpp:1851] res4a_branch2b_param_0(0.28) 
I0630 05:04:14.785010 29777 net.cpp:1851] res5a_branch2a_param_0(0.28) 
I0630 05:04:14.785014 29777 net.cpp:1851] res5a_branch2b_param_0(0.28) 
I0630 05:04:14.785019 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (658988/2.86678e+06) 0.23
I0630 05:04:14.942106 29777 solver.cpp:290] Iteration 55000 (6.08937 iter/s, 16.4221s/100 iter), loss = 0.988095
I0630 05:04:14.942129 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 05:04:14.942137 29777 sgd_solver.cpp:106] Iteration 55000, lr = 0.00828125
I0630 05:04:31.031431 29777 solver.cpp:290] Iteration 55100 (6.21548 iter/s, 16.0889s/100 iter), loss = 0.821429
I0630 05:04:31.031460 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 05:04:31.031466 29777 sgd_solver.cpp:106] Iteration 55100, lr = 0.00827812
I0630 05:04:47.246340 29777 solver.cpp:290] Iteration 55200 (6.16735 iter/s, 16.2144s/100 iter), loss = 1.03571
I0630 05:04:47.246397 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 05:04:47.246408 29777 sgd_solver.cpp:106] Iteration 55200, lr = 0.008275
I0630 05:05:03.703407 29777 solver.cpp:290] Iteration 55300 (6.07661 iter/s, 16.4566s/100 iter), loss = 0.75
I0630 05:05:03.703429 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 05:05:03.703436 29777 sgd_solver.cpp:106] Iteration 55300, lr = 0.00827187
I0630 05:05:19.962138 29777 solver.cpp:290] Iteration 55400 (6.15072 iter/s, 16.2583s/100 iter), loss = 1.11905
I0630 05:05:19.962241 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 05:05:19.962252 29777 sgd_solver.cpp:106] Iteration 55400, lr = 0.00826875
I0630 05:05:36.133003 29777 solver.cpp:290] Iteration 55500 (6.18417 iter/s, 16.1703s/100 iter), loss = 1.13095
I0630 05:05:36.133028 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 05:05:36.133038 29777 sgd_solver.cpp:106] Iteration 55500, lr = 0.00826562
I0630 05:05:52.739537 29777 solver.cpp:290] Iteration 55600 (6.0219 iter/s, 16.606s/100 iter), loss = 1.0119
I0630 05:05:52.739647 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 05:05:52.739676 29777 sgd_solver.cpp:106] Iteration 55600, lr = 0.0082625
I0630 05:06:09.117949 29777 solver.cpp:290] Iteration 55700 (6.10581 iter/s, 16.3779s/100 iter), loss = 1.08333
I0630 05:06:09.117972 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 05:06:09.117979 29777 sgd_solver.cpp:106] Iteration 55700, lr = 0.00825937
I0630 05:06:25.531023 29777 solver.cpp:290] Iteration 55800 (6.09288 iter/s, 16.4126s/100 iter), loss = 0.904762
I0630 05:06:25.531113 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 05:06:25.531126 29777 sgd_solver.cpp:106] Iteration 55800, lr = 0.00825625
I0630 05:06:41.636349 29777 solver.cpp:290] Iteration 55900 (6.20933 iter/s, 16.1048s/100 iter), loss = 0.821429
I0630 05:06:41.636375 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 05:06:41.636384 29777 sgd_solver.cpp:106] Iteration 55900, lr = 0.00825312
I0630 05:06:57.981808 29777 solver.cpp:354] Sparsity after update:
I0630 05:06:57.983289 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:06:57.983296 29777 net.cpp:1851] conv1a_param_0(0.14) 
I0630 05:06:57.983304 29777 net.cpp:1851] conv1b_param_0(0.28) 
I0630 05:06:57.983305 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:06:57.983307 29777 net.cpp:1851] res2a_branch2a_param_0(0.28) 
I0630 05:06:57.983309 29777 net.cpp:1851] res2a_branch2b_param_0(0.28) 
I0630 05:06:57.983311 29777 net.cpp:1851] res3a_branch2a_param_0(0.28) 
I0630 05:06:57.983314 29777 net.cpp:1851] res3a_branch2b_param_0(0.28) 
I0630 05:06:57.983315 29777 net.cpp:1851] res4a_branch2a_param_0(0.28) 
I0630 05:06:57.983317 29777 net.cpp:1851] res4a_branch2b_param_0(0.28) 
I0630 05:06:57.983319 29777 net.cpp:1851] res5a_branch2a_param_0(0.28) 
I0630 05:06:57.983321 29777 net.cpp:1851] res5a_branch2b_param_0(0.28) 
I0630 05:06:57.983324 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (658988/2.86678e+06) 0.23
I0630 05:06:57.983418 29777 solver.cpp:471] Iteration 56000, Testing net (#0)
I0630 05:07:05.359558 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 05:08:04.120538 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56848
I0630 05:08:04.120587 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.803662
I0630 05:08:04.120594 29777 solver.cpp:544]     Test net output #2: loss = 1.53788 (* 1 = 1.53788 loss)
I0630 05:08:04.299991 29777 solver.cpp:290] Iteration 56000 (1.20976 iter/s, 82.6613s/100 iter), loss = 1.17857
I0630 05:08:04.300019 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 05:08:04.300027 29777 sgd_solver.cpp:106] Iteration 56000, lr = 0.00825
I0630 05:08:04.301014 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.29
I0630 05:08:04.520175 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 05:08:20.784216 29777 solver.cpp:290] Iteration 56100 (6.06659 iter/s, 16.4837s/100 iter), loss = 1.02381
I0630 05:08:20.784243 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 05:08:20.784252 29777 sgd_solver.cpp:106] Iteration 56100, lr = 0.00824687
I0630 05:08:37.072357 29777 solver.cpp:290] Iteration 56200 (6.13962 iter/s, 16.2876s/100 iter), loss = 1.2381
I0630 05:08:37.072504 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 05:08:37.072537 29777 sgd_solver.cpp:106] Iteration 56200, lr = 0.00824375
I0630 05:08:53.377466 29777 solver.cpp:290] Iteration 56300 (6.13327 iter/s, 16.3045s/100 iter), loss = 1.07143
I0630 05:08:53.377490 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 05:08:53.377497 29777 sgd_solver.cpp:106] Iteration 56300, lr = 0.00824062
I0630 05:09:09.559407 29777 solver.cpp:290] Iteration 56400 (6.17991 iter/s, 16.1815s/100 iter), loss = 0.880952
I0630 05:09:09.559454 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 05:09:09.559463 29777 sgd_solver.cpp:106] Iteration 56400, lr = 0.0082375
I0630 05:09:25.741428 29777 solver.cpp:290] Iteration 56500 (6.17989 iter/s, 16.1815s/100 iter), loss = 1.04762
I0630 05:09:25.741451 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 05:09:25.741457 29777 sgd_solver.cpp:106] Iteration 56500, lr = 0.00823438
I0630 05:09:41.849153 29777 solver.cpp:290] Iteration 56600 (6.20838 iter/s, 16.1073s/100 iter), loss = 0.785714
I0630 05:09:41.849247 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 05:09:41.849258 29777 sgd_solver.cpp:106] Iteration 56600, lr = 0.00823125
I0630 05:09:58.279036 29777 solver.cpp:290] Iteration 56700 (6.08667 iter/s, 16.4293s/100 iter), loss = 1.11905
I0630 05:09:58.279059 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 05:09:58.279067 29777 sgd_solver.cpp:106] Iteration 56700, lr = 0.00822813
I0630 05:10:14.387305 29777 solver.cpp:290] Iteration 56800 (6.20817 iter/s, 16.1078s/100 iter), loss = 0.904762
I0630 05:10:14.387397 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 05:10:14.387406 29777 sgd_solver.cpp:106] Iteration 56800, lr = 0.008225
I0630 05:10:30.470862 29777 solver.cpp:290] Iteration 56900 (6.21774 iter/s, 16.083s/100 iter), loss = 1.20238
I0630 05:10:30.470890 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 05:10:30.470904 29777 sgd_solver.cpp:106] Iteration 56900, lr = 0.00822187
I0630 05:10:46.552045 29777 solver.cpp:354] Sparsity after update:
I0630 05:10:46.572372 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:10:46.572409 29777 net.cpp:1851] conv1a_param_0(0.145) 
I0630 05:10:46.572428 29777 net.cpp:1851] conv1b_param_0(0.29) 
I0630 05:10:46.572438 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:10:46.572446 29777 net.cpp:1851] res2a_branch2a_param_0(0.29) 
I0630 05:10:46.572451 29777 net.cpp:1851] res2a_branch2b_param_0(0.29) 
I0630 05:10:46.572454 29777 net.cpp:1851] res3a_branch2a_param_0(0.29) 
I0630 05:10:46.572458 29777 net.cpp:1851] res3a_branch2b_param_0(0.29) 
I0630 05:10:46.572463 29777 net.cpp:1851] res4a_branch2a_param_0(0.29) 
I0630 05:10:46.572466 29777 net.cpp:1851] res4a_branch2b_param_0(0.29) 
I0630 05:10:46.572470 29777 net.cpp:1851] res5a_branch2a_param_0(0.29) 
I0630 05:10:46.572474 29777 net.cpp:1851] res5a_branch2b_param_0(0.29) 
I0630 05:10:46.572482 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (682534/2.86678e+06) 0.238
I0630 05:10:46.731741 29777 solver.cpp:290] Iteration 57000 (6.14991 iter/s, 16.2604s/100 iter), loss = 0.869048
I0630 05:10:46.731817 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 05:10:46.731861 29777 sgd_solver.cpp:106] Iteration 57000, lr = 0.00821875
I0630 05:11:02.937239 29777 solver.cpp:290] Iteration 57100 (6.17094 iter/s, 16.205s/100 iter), loss = 0.72619
I0630 05:11:02.937266 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 05:11:02.937276 29777 sgd_solver.cpp:106] Iteration 57100, lr = 0.00821562
I0630 05:11:19.187510 29777 solver.cpp:290] Iteration 57200 (6.15393 iter/s, 16.2498s/100 iter), loss = 1.13095
I0630 05:11:19.187625 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 05:11:19.187635 29777 sgd_solver.cpp:106] Iteration 57200, lr = 0.0082125
I0630 05:11:35.366870 29777 solver.cpp:290] Iteration 57300 (6.18093 iter/s, 16.1788s/100 iter), loss = 1.11905
I0630 05:11:35.366896 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 05:11:35.366905 29777 sgd_solver.cpp:106] Iteration 57300, lr = 0.00820937
I0630 05:11:51.590668 29777 solver.cpp:290] Iteration 57400 (6.16397 iter/s, 16.2233s/100 iter), loss = 0.940476
I0630 05:11:51.590723 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 05:11:51.590741 29777 sgd_solver.cpp:106] Iteration 57400, lr = 0.00820625
I0630 05:12:07.807888 29777 solver.cpp:290] Iteration 57500 (6.16648 iter/s, 16.2167s/100 iter), loss = 0.964286
I0630 05:12:07.807934 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 05:12:07.807945 29777 sgd_solver.cpp:106] Iteration 57500, lr = 0.00820312
I0630 05:12:24.136198 29777 solver.cpp:290] Iteration 57600 (6.12452 iter/s, 16.3278s/100 iter), loss = 1.21429
I0630 05:12:24.136272 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 05:12:24.136281 29777 sgd_solver.cpp:106] Iteration 57600, lr = 0.0082
I0630 05:12:40.201326 29777 solver.cpp:290] Iteration 57700 (6.22486 iter/s, 16.0646s/100 iter), loss = 1.13095
I0630 05:12:40.201354 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 05:12:40.201369 29777 sgd_solver.cpp:106] Iteration 57700, lr = 0.00819687
I0630 05:12:56.370133 29777 solver.cpp:290] Iteration 57800 (6.18493 iter/s, 16.1683s/100 iter), loss = 1.09524
I0630 05:12:56.370216 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 05:12:56.370234 29777 sgd_solver.cpp:106] Iteration 57800, lr = 0.00819375
I0630 05:13:12.631819 29777 solver.cpp:290] Iteration 57900 (6.14962 iter/s, 16.2612s/100 iter), loss = 0.821429
I0630 05:13:12.631840 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 05:13:12.631846 29777 sgd_solver.cpp:106] Iteration 57900, lr = 0.00819062
I0630 05:13:28.777496 29777 solver.cpp:354] Sparsity after update:
I0630 05:13:28.779129 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:13:28.779139 29777 net.cpp:1851] conv1a_param_0(0.145) 
I0630 05:13:28.779148 29777 net.cpp:1851] conv1b_param_0(0.29) 
I0630 05:13:28.779150 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:13:28.779152 29777 net.cpp:1851] res2a_branch2a_param_0(0.29) 
I0630 05:13:28.779155 29777 net.cpp:1851] res2a_branch2b_param_0(0.29) 
I0630 05:13:28.779156 29777 net.cpp:1851] res3a_branch2a_param_0(0.29) 
I0630 05:13:28.779158 29777 net.cpp:1851] res3a_branch2b_param_0(0.29) 
I0630 05:13:28.779160 29777 net.cpp:1851] res4a_branch2a_param_0(0.29) 
I0630 05:13:28.779163 29777 net.cpp:1851] res4a_branch2b_param_0(0.29) 
I0630 05:13:28.779165 29777 net.cpp:1851] res5a_branch2a_param_0(0.29) 
I0630 05:13:28.779167 29777 net.cpp:1851] res5a_branch2b_param_0(0.29) 
I0630 05:13:28.779170 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (682534/2.86678e+06) 0.238
I0630 05:13:28.779263 29777 solver.cpp:471] Iteration 58000, Testing net (#0)
I0630 05:13:35.708022 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 05:14:32.205770 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56918
I0630 05:14:32.205880 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.803802
I0630 05:14:32.205890 29777 solver.cpp:544]     Test net output #2: loss = 1.523 (* 1 = 1.523 loss)
I0630 05:14:32.385406 29777 solver.cpp:290] Iteration 58000 (1.2539 iter/s, 79.7514s/100 iter), loss = 1.10714
I0630 05:14:32.385434 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 05:14:32.385443 29777 sgd_solver.cpp:106] Iteration 58000, lr = 0.0081875
I0630 05:14:32.386422 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.3
I0630 05:14:32.586091 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 05:14:48.194196 29777 solver.cpp:290] Iteration 58100 (6.32578 iter/s, 15.8083s/100 iter), loss = 1
I0630 05:14:48.194219 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 05:14:48.194226 29777 sgd_solver.cpp:106] Iteration 58100, lr = 0.00818438
I0630 05:15:04.193425 29777 solver.cpp:290] Iteration 58200 (6.25048 iter/s, 15.9988s/100 iter), loss = 1.04762
I0630 05:15:04.193545 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 05:15:04.193555 29777 sgd_solver.cpp:106] Iteration 58200, lr = 0.00818125
I0630 05:15:20.311866 29777 solver.cpp:290] Iteration 58300 (6.20429 iter/s, 16.1179s/100 iter), loss = 1.2619
I0630 05:15:20.311894 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 05:15:20.311903 29777 sgd_solver.cpp:106] Iteration 58300, lr = 0.00817813
I0630 05:15:36.727098 29777 solver.cpp:290] Iteration 58400 (6.09208 iter/s, 16.4147s/100 iter), loss = 1.38095
I0630 05:15:36.727180 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 05:15:36.727190 29777 sgd_solver.cpp:106] Iteration 58400, lr = 0.008175
I0630 05:15:52.965693 29777 solver.cpp:290] Iteration 58500 (6.15837 iter/s, 16.2381s/100 iter), loss = 1.21429
I0630 05:15:52.965745 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 05:15:52.965759 29777 sgd_solver.cpp:106] Iteration 58500, lr = 0.00817188
I0630 05:16:08.999169 29777 solver.cpp:290] Iteration 58600 (6.23714 iter/s, 16.033s/100 iter), loss = 1.2381
I0630 05:16:08.999214 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 05:16:08.999222 29777 sgd_solver.cpp:106] Iteration 58600, lr = 0.00816875
I0630 05:16:25.157897 29777 solver.cpp:290] Iteration 58700 (6.18879 iter/s, 16.1582s/100 iter), loss = 0.821429
I0630 05:16:25.157922 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 05:16:25.157929 29777 sgd_solver.cpp:106] Iteration 58700, lr = 0.00816562
I0630 05:16:41.223886 29777 solver.cpp:290] Iteration 58800 (6.22451 iter/s, 16.0655s/100 iter), loss = 1.15476
I0630 05:16:41.223971 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 05:16:41.223983 29777 sgd_solver.cpp:106] Iteration 58800, lr = 0.0081625
I0630 05:16:57.445137 29777 solver.cpp:290] Iteration 58900 (6.16496 iter/s, 16.2207s/100 iter), loss = 0.97619
I0630 05:16:57.445161 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 05:16:57.445170 29777 sgd_solver.cpp:106] Iteration 58900, lr = 0.00815937
I0630 05:17:13.444344 29777 solver.cpp:354] Sparsity after update:
I0630 05:17:13.464781 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:17:13.464795 29777 net.cpp:1851] conv1a_param_0(0.15) 
I0630 05:17:13.464807 29777 net.cpp:1851] conv1b_param_0(0.3) 
I0630 05:17:13.464810 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:17:13.464813 29777 net.cpp:1851] res2a_branch2a_param_0(0.3) 
I0630 05:17:13.464818 29777 net.cpp:1851] res2a_branch2b_param_0(0.3) 
I0630 05:17:13.464820 29777 net.cpp:1851] res3a_branch2a_param_0(0.3) 
I0630 05:17:13.464823 29777 net.cpp:1851] res3a_branch2b_param_0(0.3) 
I0630 05:17:13.464826 29777 net.cpp:1851] res4a_branch2a_param_0(0.3) 
I0630 05:17:13.464829 29777 net.cpp:1851] res4a_branch2b_param_0(0.3) 
I0630 05:17:13.464833 29777 net.cpp:1851] res5a_branch2a_param_0(0.3) 
I0630 05:17:13.464843 29777 net.cpp:1851] res5a_branch2b_param_0(0.3) 
I0630 05:17:13.464848 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (706063/2.86678e+06) 0.246
I0630 05:17:13.625425 29777 solver.cpp:290] Iteration 59000 (6.18054 iter/s, 16.1798s/100 iter), loss = 0.940476
I0630 05:17:13.625455 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 05:17:13.625465 29777 sgd_solver.cpp:106] Iteration 59000, lr = 0.00815625
I0630 05:17:29.707725 29777 solver.cpp:290] Iteration 59100 (6.2182 iter/s, 16.0818s/100 iter), loss = 1.25
I0630 05:17:29.707751 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 05:17:29.707759 29777 sgd_solver.cpp:106] Iteration 59100, lr = 0.00815312
I0630 05:17:46.109089 29777 solver.cpp:290] Iteration 59200 (6.09723 iter/s, 16.4009s/100 iter), loss = 1.20238
I0630 05:17:46.109197 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 05:17:46.109232 29777 sgd_solver.cpp:106] Iteration 59200, lr = 0.00815
I0630 05:18:02.425653 29777 solver.cpp:290] Iteration 59300 (6.12895 iter/s, 16.316s/100 iter), loss = 0.916667
I0630 05:18:02.425680 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 05:18:02.425689 29777 sgd_solver.cpp:106] Iteration 59300, lr = 0.00814687
I0630 05:18:18.778046 29777 solver.cpp:290] Iteration 59400 (6.11549 iter/s, 16.3519s/100 iter), loss = 1.05952
I0630 05:18:18.778095 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 05:18:18.778102 29777 sgd_solver.cpp:106] Iteration 59400, lr = 0.00814375
I0630 05:18:34.981899 29777 solver.cpp:290] Iteration 59500 (6.17156 iter/s, 16.2034s/100 iter), loss = 0.857143
I0630 05:18:34.981927 29777 solver.cpp:309]     Train net output #0: loss = 0.690476 (* 1 = 0.690476 loss)
I0630 05:18:34.981936 29777 sgd_solver.cpp:106] Iteration 59500, lr = 0.00814062
I0630 05:18:51.428448 29777 solver.cpp:290] Iteration 59600 (6.08048 iter/s, 16.4461s/100 iter), loss = 1.36905
I0630 05:18:51.428501 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 05:18:51.428511 29777 sgd_solver.cpp:106] Iteration 59600, lr = 0.0081375
I0630 05:19:08.232545 29777 solver.cpp:290] Iteration 59700 (5.95112 iter/s, 16.8036s/100 iter), loss = 1.52381
I0630 05:19:08.232650 29777 solver.cpp:309]     Train net output #0: loss = 1.88095 (* 1 = 1.88095 loss)
I0630 05:19:08.232692 29777 sgd_solver.cpp:106] Iteration 59700, lr = 0.00813438
I0630 05:19:24.592200 29777 solver.cpp:290] Iteration 59800 (6.1128 iter/s, 16.3591s/100 iter), loss = 1.14286
I0630 05:19:24.592306 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 05:19:24.592316 29777 sgd_solver.cpp:106] Iteration 59800, lr = 0.00813125
I0630 05:19:40.881265 29777 solver.cpp:290] Iteration 59900 (6.1393 iter/s, 16.2885s/100 iter), loss = 0.964286
I0630 05:19:40.881295 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 05:19:40.881304 29777 sgd_solver.cpp:106] Iteration 59900, lr = 0.00812813
I0630 05:19:57.069869 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_60000.caffemodel
I0630 05:19:57.089951 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_60000.solverstate
I0630 05:19:57.098562 29777 solver.cpp:354] Sparsity after update:
I0630 05:19:57.099535 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:19:57.099544 29777 net.cpp:1851] conv1a_param_0(0.15) 
I0630 05:19:57.099550 29777 net.cpp:1851] conv1b_param_0(0.3) 
I0630 05:19:57.099553 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:19:57.099555 29777 net.cpp:1851] res2a_branch2a_param_0(0.3) 
I0630 05:19:57.099557 29777 net.cpp:1851] res2a_branch2b_param_0(0.3) 
I0630 05:19:57.099560 29777 net.cpp:1851] res3a_branch2a_param_0(0.3) 
I0630 05:19:57.099561 29777 net.cpp:1851] res3a_branch2b_param_0(0.3) 
I0630 05:19:57.099563 29777 net.cpp:1851] res4a_branch2a_param_0(0.3) 
I0630 05:19:57.099565 29777 net.cpp:1851] res4a_branch2b_param_0(0.3) 
I0630 05:19:57.099567 29777 net.cpp:1851] res5a_branch2a_param_0(0.3) 
I0630 05:19:57.099570 29777 net.cpp:1851] res5a_branch2b_param_0(0.3) 
I0630 05:19:57.099571 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (706063/2.86678e+06) 0.246
I0630 05:19:57.099666 29777 solver.cpp:471] Iteration 60000, Testing net (#0)
I0630 05:20:05.472725 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 05:21:05.400691 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5674
I0630 05:21:05.400799 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.803521
I0630 05:21:05.400822 29777 solver.cpp:544]     Test net output #2: loss = 1.56204 (* 1 = 1.56204 loss)
I0630 05:21:05.576153 29777 solver.cpp:290] Iteration 60000 (1.18074 iter/s, 84.6925s/100 iter), loss = 1.16667
I0630 05:21:05.576223 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 05:21:05.576256 29777 sgd_solver.cpp:106] Iteration 60000, lr = 0.008125
I0630 05:21:05.578207 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.31
I0630 05:21:05.891165 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 05:21:21.531790 29777 solver.cpp:290] Iteration 60100 (6.26758 iter/s, 15.9551s/100 iter), loss = 1.10714
I0630 05:21:21.531821 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 05:21:21.531831 29777 sgd_solver.cpp:106] Iteration 60100, lr = 0.00812188
I0630 05:21:37.713980 29777 solver.cpp:290] Iteration 60200 (6.17982 iter/s, 16.1817s/100 iter), loss = 1.2619
I0630 05:21:37.714087 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 05:21:37.714097 29777 sgd_solver.cpp:106] Iteration 60200, lr = 0.00811875
I0630 05:21:53.923384 29777 solver.cpp:290] Iteration 60300 (6.16947 iter/s, 16.2088s/100 iter), loss = 1.14286
I0630 05:21:53.923408 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 05:21:53.923415 29777 sgd_solver.cpp:106] Iteration 60300, lr = 0.00811563
I0630 05:22:10.128101 29777 solver.cpp:290] Iteration 60400 (6.17122 iter/s, 16.2042s/100 iter), loss = 0.869048
I0630 05:22:10.128222 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 05:22:10.128237 29777 sgd_solver.cpp:106] Iteration 60400, lr = 0.0081125
I0630 05:22:26.566727 29777 solver.cpp:290] Iteration 60500 (6.08345 iter/s, 16.438s/100 iter), loss = 0.857143
I0630 05:22:26.566751 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 05:22:26.566758 29777 sgd_solver.cpp:106] Iteration 60500, lr = 0.00810937
I0630 05:22:42.759768 29777 solver.cpp:290] Iteration 60600 (6.17567 iter/s, 16.1926s/100 iter), loss = 0.761905
I0630 05:22:42.759882 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 05:22:42.759892 29777 sgd_solver.cpp:106] Iteration 60600, lr = 0.00810625
I0630 05:22:59.226130 29777 solver.cpp:290] Iteration 60700 (6.0732 iter/s, 16.4658s/100 iter), loss = 1.03571
I0630 05:22:59.226205 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 05:22:59.226227 29777 sgd_solver.cpp:106] Iteration 60700, lr = 0.00810312
I0630 05:23:15.392938 29777 solver.cpp:290] Iteration 60800 (6.18571 iter/s, 16.1663s/100 iter), loss = 0.964286
I0630 05:23:15.393033 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 05:23:15.393055 29777 sgd_solver.cpp:106] Iteration 60800, lr = 0.0081
I0630 05:23:31.668308 29777 solver.cpp:290] Iteration 60900 (6.14446 iter/s, 16.2748s/100 iter), loss = 1.16667
I0630 05:23:31.668357 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 05:23:31.668372 29777 sgd_solver.cpp:106] Iteration 60900, lr = 0.00809687
I0630 05:23:47.781945 29777 solver.cpp:354] Sparsity after update:
I0630 05:23:47.802371 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:23:47.802386 29777 net.cpp:1851] conv1a_param_0(0.155) 
I0630 05:23:47.802395 29777 net.cpp:1851] conv1b_param_0(0.31) 
I0630 05:23:47.802399 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:23:47.802408 29777 net.cpp:1851] res2a_branch2a_param_0(0.31) 
I0630 05:23:47.802412 29777 net.cpp:1851] res2a_branch2b_param_0(0.31) 
I0630 05:23:47.802417 29777 net.cpp:1851] res3a_branch2a_param_0(0.31) 
I0630 05:23:47.802419 29777 net.cpp:1851] res3a_branch2b_param_0(0.31) 
I0630 05:23:47.802424 29777 net.cpp:1851] res4a_branch2a_param_0(0.31) 
I0630 05:23:47.802429 29777 net.cpp:1851] res4a_branch2b_param_0(0.31) 
I0630 05:23:47.802434 29777 net.cpp:1851] res5a_branch2a_param_0(0.31) 
I0630 05:23:47.802439 29777 net.cpp:1851] res5a_branch2b_param_0(0.31) 
I0630 05:23:47.802444 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (729596/2.86678e+06) 0.254
I0630 05:23:47.958499 29777 solver.cpp:290] Iteration 61000 (6.13885 iter/s, 16.2897s/100 iter), loss = 0.738095
I0630 05:23:47.958525 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 05:23:47.958534 29777 sgd_solver.cpp:106] Iteration 61000, lr = 0.00809375
I0630 05:24:04.681843 29777 solver.cpp:290] Iteration 61100 (5.97984 iter/s, 16.7229s/100 iter), loss = 1.72619
I0630 05:24:04.681869 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 05:24:04.681877 29777 sgd_solver.cpp:106] Iteration 61100, lr = 0.00809062
I0630 05:24:21.129529 29777 solver.cpp:290] Iteration 61200 (6.08006 iter/s, 16.4472s/100 iter), loss = 1.2381
I0630 05:24:21.129611 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 05:24:21.129619 29777 sgd_solver.cpp:106] Iteration 61200, lr = 0.0080875
I0630 05:24:37.497675 29777 solver.cpp:290] Iteration 61300 (6.10963 iter/s, 16.3676s/100 iter), loss = 0.857143
I0630 05:24:37.497701 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 05:24:37.497710 29777 sgd_solver.cpp:106] Iteration 61300, lr = 0.00808437
I0630 05:24:53.800626 29777 solver.cpp:290] Iteration 61400 (6.13404 iter/s, 16.3025s/100 iter), loss = 0.988095
I0630 05:24:53.800727 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 05:24:53.800735 29777 sgd_solver.cpp:106] Iteration 61400, lr = 0.00808125
I0630 05:25:10.046016 29777 solver.cpp:290] Iteration 61500 (6.1558 iter/s, 16.2448s/100 iter), loss = 0.97619
I0630 05:25:10.046038 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 05:25:10.046046 29777 sgd_solver.cpp:106] Iteration 61500, lr = 0.00807813
I0630 05:25:26.403537 29777 solver.cpp:290] Iteration 61600 (6.11358 iter/s, 16.357s/100 iter), loss = 1.10714
I0630 05:25:26.403616 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 05:25:26.403627 29777 sgd_solver.cpp:106] Iteration 61600, lr = 0.008075
I0630 05:25:42.486711 29777 solver.cpp:290] Iteration 61700 (6.21788 iter/s, 16.0827s/100 iter), loss = 0.97619
I0630 05:25:42.486733 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 05:25:42.486740 29777 sgd_solver.cpp:106] Iteration 61700, lr = 0.00807188
I0630 05:25:58.638936 29777 solver.cpp:290] Iteration 61800 (6.19128 iter/s, 16.1518s/100 iter), loss = 1.0119
I0630 05:25:58.639040 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 05:25:58.639051 29777 sgd_solver.cpp:106] Iteration 61800, lr = 0.00806875
I0630 05:26:14.930058 29777 solver.cpp:290] Iteration 61900 (6.13852 iter/s, 16.2906s/100 iter), loss = 0.928571
I0630 05:26:14.930081 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 05:26:14.930088 29777 sgd_solver.cpp:106] Iteration 61900, lr = 0.00806563
I0630 05:26:30.977524 29777 solver.cpp:354] Sparsity after update:
I0630 05:26:30.978782 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:26:30.978790 29777 net.cpp:1851] conv1a_param_0(0.155) 
I0630 05:26:30.978797 29777 net.cpp:1851] conv1b_param_0(0.31) 
I0630 05:26:30.978801 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:26:30.978802 29777 net.cpp:1851] res2a_branch2a_param_0(0.31) 
I0630 05:26:30.978804 29777 net.cpp:1851] res2a_branch2b_param_0(0.31) 
I0630 05:26:30.978806 29777 net.cpp:1851] res3a_branch2a_param_0(0.31) 
I0630 05:26:30.978808 29777 net.cpp:1851] res3a_branch2b_param_0(0.31) 
I0630 05:26:30.978811 29777 net.cpp:1851] res4a_branch2a_param_0(0.31) 
I0630 05:26:30.978812 29777 net.cpp:1851] res4a_branch2b_param_0(0.31) 
I0630 05:26:30.978813 29777 net.cpp:1851] res5a_branch2a_param_0(0.31) 
I0630 05:26:30.978816 29777 net.cpp:1851] res5a_branch2b_param_0(0.31) 
I0630 05:26:30.978818 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (729596/2.86678e+06) 0.254
I0630 05:26:30.978945 29777 solver.cpp:471] Iteration 62000, Testing net (#0)
I0630 05:26:36.924193 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 05:27:37.237669 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.571079
I0630 05:27:37.237740 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.802861
I0630 05:27:37.237748 29777 solver.cpp:544]     Test net output #2: loss = 1.53324 (* 1 = 1.53324 loss)
I0630 05:27:37.419924 29777 solver.cpp:290] Iteration 62000 (1.2123 iter/s, 82.4876s/100 iter), loss = 0.952381
I0630 05:27:37.419948 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 05:27:37.419955 29777 sgd_solver.cpp:106] Iteration 62000, lr = 0.0080625
I0630 05:27:37.420610 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.32
I0630 05:27:37.673977 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 05:27:53.178243 29777 solver.cpp:290] Iteration 62100 (6.34604 iter/s, 15.7579s/100 iter), loss = 1.2381
I0630 05:27:53.178269 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 05:27:53.178277 29777 sgd_solver.cpp:106] Iteration 62100, lr = 0.00805937
I0630 05:28:09.296972 29777 solver.cpp:290] Iteration 62200 (6.20415 iter/s, 16.1182s/100 iter), loss = 0.77381
I0630 05:28:09.297078 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 05:28:09.297088 29777 sgd_solver.cpp:106] Iteration 62200, lr = 0.00805625
I0630 05:28:25.568265 29777 solver.cpp:290] Iteration 62300 (6.146 iter/s, 16.2707s/100 iter), loss = 0.952381
I0630 05:28:25.568289 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 05:28:25.568296 29777 sgd_solver.cpp:106] Iteration 62300, lr = 0.00805312
I0630 05:28:41.802789 29777 solver.cpp:290] Iteration 62400 (6.15989 iter/s, 16.234s/100 iter), loss = 1.34524
I0630 05:28:41.802860 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 05:28:41.802870 29777 sgd_solver.cpp:106] Iteration 62400, lr = 0.00805
I0630 05:28:57.818825 29777 solver.cpp:290] Iteration 62500 (6.24395 iter/s, 16.0155s/100 iter), loss = 1.08333
I0630 05:28:57.818917 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 05:28:57.818938 29777 sgd_solver.cpp:106] Iteration 62500, lr = 0.00804687
I0630 05:29:13.985005 29777 solver.cpp:290] Iteration 62600 (6.18596 iter/s, 16.1656s/100 iter), loss = 1.2619
I0630 05:29:13.985093 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 05:29:13.985105 29777 sgd_solver.cpp:106] Iteration 62600, lr = 0.00804375
I0630 05:29:30.186307 29777 solver.cpp:290] Iteration 62700 (6.17255 iter/s, 16.2008s/100 iter), loss = 1.19048
I0630 05:29:30.186331 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 05:29:30.186337 29777 sgd_solver.cpp:106] Iteration 62700, lr = 0.00804062
I0630 05:29:46.496214 29777 solver.cpp:290] Iteration 62800 (6.13142 iter/s, 16.3094s/100 iter), loss = 1.28571
I0630 05:29:46.496286 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 05:29:46.496294 29777 sgd_solver.cpp:106] Iteration 62800, lr = 0.0080375
I0630 05:30:02.686251 29777 solver.cpp:290] Iteration 62900 (6.17684 iter/s, 16.1895s/100 iter), loss = 0.738095
I0630 05:30:02.686273 29777 solver.cpp:309]     Train net output #0: loss = 0.5 (* 1 = 0.5 loss)
I0630 05:30:02.686282 29777 sgd_solver.cpp:106] Iteration 62900, lr = 0.00803437
I0630 05:30:18.579928 29777 solver.cpp:354] Sparsity after update:
I0630 05:30:18.600260 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:30:18.600277 29777 net.cpp:1851] conv1a_param_0(0.16) 
I0630 05:30:18.600286 29777 net.cpp:1851] conv1b_param_0(0.32) 
I0630 05:30:18.600291 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:30:18.600294 29777 net.cpp:1851] res2a_branch2a_param_0(0.32) 
I0630 05:30:18.600297 29777 net.cpp:1851] res2a_branch2b_param_0(0.32) 
I0630 05:30:18.600301 29777 net.cpp:1851] res3a_branch2a_param_0(0.32) 
I0630 05:30:18.600304 29777 net.cpp:1851] res3a_branch2b_param_0(0.32) 
I0630 05:30:18.600307 29777 net.cpp:1851] res4a_branch2a_param_0(0.32) 
I0630 05:30:18.600311 29777 net.cpp:1851] res4a_branch2b_param_0(0.32) 
I0630 05:30:18.600313 29777 net.cpp:1851] res5a_branch2a_param_0(0.32) 
I0630 05:30:18.600317 29777 net.cpp:1851] res5a_branch2b_param_0(0.32) 
I0630 05:30:18.600327 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (753138/2.86678e+06) 0.263
I0630 05:30:18.756444 29777 solver.cpp:290] Iteration 63000 (6.22288 iter/s, 16.0697s/100 iter), loss = 0.904762
I0630 05:30:18.756474 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 05:30:18.756482 29777 sgd_solver.cpp:106] Iteration 63000, lr = 0.00803125
I0630 05:30:34.839946 29777 solver.cpp:290] Iteration 63100 (6.21773 iter/s, 16.083s/100 iter), loss = 1.08333
I0630 05:30:34.839972 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 05:30:34.839980 29777 sgd_solver.cpp:106] Iteration 63100, lr = 0.00802813
I0630 05:30:50.929953 29777 solver.cpp:290] Iteration 63200 (6.21522 iter/s, 16.0895s/100 iter), loss = 1.27381
I0630 05:30:50.930083 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 05:30:50.930125 29777 sgd_solver.cpp:106] Iteration 63200, lr = 0.008025
I0630 05:31:07.018976 29777 solver.cpp:290] Iteration 63300 (6.21564 iter/s, 16.0885s/100 iter), loss = 1.17857
I0630 05:31:07.019001 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 05:31:07.019011 29777 sgd_solver.cpp:106] Iteration 63300, lr = 0.00802188
I0630 05:31:23.154352 29777 solver.cpp:290] Iteration 63400 (6.19774 iter/s, 16.1349s/100 iter), loss = 0.797619
I0630 05:31:23.154425 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 05:31:23.154433 29777 sgd_solver.cpp:106] Iteration 63400, lr = 0.00801875
I0630 05:31:39.272320 29777 solver.cpp:290] Iteration 63500 (6.20446 iter/s, 16.1174s/100 iter), loss = 1.03571
I0630 05:31:39.272346 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 05:31:39.272361 29777 sgd_solver.cpp:106] Iteration 63500, lr = 0.00801562
I0630 05:31:55.484444 29777 solver.cpp:290] Iteration 63600 (6.1684 iter/s, 16.2116s/100 iter), loss = 0.940476
I0630 05:31:55.484536 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 05:31:55.484547 29777 sgd_solver.cpp:106] Iteration 63600, lr = 0.0080125
I0630 05:32:11.901114 29777 solver.cpp:290] Iteration 63700 (6.09157 iter/s, 16.4161s/100 iter), loss = 1.17857
I0630 05:32:11.901146 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 05:32:11.901155 29777 sgd_solver.cpp:106] Iteration 63700, lr = 0.00800938
I0630 05:32:28.029683 29777 solver.cpp:290] Iteration 63800 (6.20036 iter/s, 16.1281s/100 iter), loss = 1.28571
I0630 05:32:28.029804 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 05:32:28.029826 29777 sgd_solver.cpp:106] Iteration 63800, lr = 0.00800625
I0630 05:32:44.179368 29777 solver.cpp:290] Iteration 63900 (6.19229 iter/s, 16.1491s/100 iter), loss = 0.916667
I0630 05:32:44.179395 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 05:32:44.179404 29777 sgd_solver.cpp:106] Iteration 63900, lr = 0.00800312
I0630 05:33:00.217743 29777 solver.cpp:354] Sparsity after update:
I0630 05:33:00.219329 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:33:00.219339 29777 net.cpp:1851] conv1a_param_0(0.16) 
I0630 05:33:00.219352 29777 net.cpp:1851] conv1b_param_0(0.32) 
I0630 05:33:00.219355 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:33:00.219359 29777 net.cpp:1851] res2a_branch2a_param_0(0.32) 
I0630 05:33:00.219362 29777 net.cpp:1851] res2a_branch2b_param_0(0.32) 
I0630 05:33:00.219367 29777 net.cpp:1851] res3a_branch2a_param_0(0.32) 
I0630 05:33:00.219369 29777 net.cpp:1851] res3a_branch2b_param_0(0.32) 
I0630 05:33:00.219373 29777 net.cpp:1851] res4a_branch2a_param_0(0.32) 
I0630 05:33:00.219377 29777 net.cpp:1851] res4a_branch2b_param_0(0.32) 
I0630 05:33:00.219382 29777 net.cpp:1851] res5a_branch2a_param_0(0.32) 
I0630 05:33:00.219385 29777 net.cpp:1851] res5a_branch2b_param_0(0.32) 
I0630 05:33:00.219388 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (753138/2.86678e+06) 0.263
I0630 05:33:00.219518 29777 solver.cpp:471] Iteration 64000, Testing net (#0)
I0630 05:33:07.090535 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 05:34:00.842942 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.57342
I0630 05:34:00.843026 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.806721
I0630 05:34:00.843035 29777 solver.cpp:544]     Test net output #2: loss = 1.50864 (* 1 = 1.50864 loss)
I0630 05:34:01.031721 29777 solver.cpp:290] Iteration 64000 (1.30123 iter/s, 76.8502s/100 iter), loss = 0.761905
I0630 05:34:01.031755 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 05:34:01.031765 29777 sgd_solver.cpp:106] Iteration 64000, lr = 0.008
I0630 05:34:01.032480 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.33
I0630 05:34:01.484958 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 05:34:17.421885 29777 solver.cpp:290] Iteration 64100 (6.1014 iter/s, 16.3897s/100 iter), loss = 1.5119
I0630 05:34:17.421912 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 05:34:17.421921 29777 sgd_solver.cpp:106] Iteration 64100, lr = 0.00799687
I0630 05:34:33.756852 29777 solver.cpp:290] Iteration 64200 (6.12202 iter/s, 16.3345s/100 iter), loss = 1.02381
I0630 05:34:33.756959 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 05:34:33.756971 29777 sgd_solver.cpp:106] Iteration 64200, lr = 0.00799375
I0630 05:34:50.017091 29777 solver.cpp:290] Iteration 64300 (6.15018 iter/s, 16.2597s/100 iter), loss = 0.880952
I0630 05:34:50.017117 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 05:34:50.017125 29777 sgd_solver.cpp:106] Iteration 64300, lr = 0.00799062
I0630 05:35:06.158254 29777 solver.cpp:290] Iteration 64400 (6.19554 iter/s, 16.1406s/100 iter), loss = 0.880952
I0630 05:35:06.158445 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 05:35:06.158471 29777 sgd_solver.cpp:106] Iteration 64400, lr = 0.0079875
I0630 05:35:22.813314 29777 solver.cpp:290] Iteration 64500 (6.00442 iter/s, 16.6544s/100 iter), loss = 1.2381
I0630 05:35:22.813344 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 05:35:22.813352 29777 sgd_solver.cpp:106] Iteration 64500, lr = 0.00798437
I0630 05:35:39.505733 29777 solver.cpp:290] Iteration 64600 (5.99092 iter/s, 16.6919s/100 iter), loss = 1.08333
I0630 05:35:39.505991 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 05:35:39.506126 29777 sgd_solver.cpp:106] Iteration 64600, lr = 0.00798125
I0630 05:35:55.866629 29777 solver.cpp:290] Iteration 64700 (6.1124 iter/s, 16.3602s/100 iter), loss = 1.21429
I0630 05:35:55.866657 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 05:35:55.866667 29777 sgd_solver.cpp:106] Iteration 64700, lr = 0.00797813
I0630 05:36:12.151263 29777 solver.cpp:290] Iteration 64800 (6.14094 iter/s, 16.2842s/100 iter), loss = 1.13095
I0630 05:36:12.151358 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 05:36:12.151371 29777 sgd_solver.cpp:106] Iteration 64800, lr = 0.007975
I0630 05:36:28.652493 29777 solver.cpp:290] Iteration 64900 (6.06036 iter/s, 16.5007s/100 iter), loss = 0.904762
I0630 05:36:28.652545 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 05:36:28.652580 29777 sgd_solver.cpp:106] Iteration 64900, lr = 0.00797187
I0630 05:36:44.611688 29777 solver.cpp:354] Sparsity after update:
I0630 05:36:44.632334 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:36:44.632350 29777 net.cpp:1851] conv1a_param_0(0.165) 
I0630 05:36:44.632360 29777 net.cpp:1851] conv1b_param_0(0.33) 
I0630 05:36:44.632364 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:36:44.632367 29777 net.cpp:1851] res2a_branch2a_param_0(0.33) 
I0630 05:36:44.632372 29777 net.cpp:1851] res2a_branch2b_param_0(0.33) 
I0630 05:36:44.632375 29777 net.cpp:1851] res3a_branch2a_param_0(0.33) 
I0630 05:36:44.632378 29777 net.cpp:1851] res3a_branch2b_param_0(0.33) 
I0630 05:36:44.632382 29777 net.cpp:1851] res4a_branch2a_param_0(0.33) 
I0630 05:36:44.632385 29777 net.cpp:1851] res4a_branch2b_param_0(0.33) 
I0630 05:36:44.632388 29777 net.cpp:1851] res5a_branch2a_param_0(0.33) 
I0630 05:36:44.632391 29777 net.cpp:1851] res5a_branch2b_param_0(0.33) 
I0630 05:36:44.632395 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (776672/2.86678e+06) 0.271
I0630 05:36:44.792695 29777 solver.cpp:290] Iteration 65000 (6.1959 iter/s, 16.1397s/100 iter), loss = 1.63095
I0630 05:36:44.792721 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 05:36:44.792729 29777 sgd_solver.cpp:106] Iteration 65000, lr = 0.00796875
I0630 05:37:01.142014 29777 solver.cpp:290] Iteration 65100 (6.11664 iter/s, 16.3488s/100 iter), loss = 1.0119
I0630 05:37:01.142037 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 05:37:01.142045 29777 sgd_solver.cpp:106] Iteration 65100, lr = 0.00796562
I0630 05:37:17.294513 29777 solver.cpp:290] Iteration 65200 (6.19117 iter/s, 16.152s/100 iter), loss = 1.10714
I0630 05:37:17.305039 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 05:37:17.305061 29777 sgd_solver.cpp:106] Iteration 65200, lr = 0.0079625
I0630 05:37:33.581264 29777 solver.cpp:290] Iteration 65300 (6.1441 iter/s, 16.2758s/100 iter), loss = 1.27381
I0630 05:37:33.581286 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 05:37:33.581293 29777 sgd_solver.cpp:106] Iteration 65300, lr = 0.00795937
I0630 05:37:49.714903 29777 solver.cpp:290] Iteration 65400 (6.19841 iter/s, 16.1332s/100 iter), loss = 1.45238
I0630 05:37:49.714952 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 05:37:49.714962 29777 sgd_solver.cpp:106] Iteration 65400, lr = 0.00795625
I0630 05:38:06.191995 29777 solver.cpp:290] Iteration 65500 (6.06922 iter/s, 16.4766s/100 iter), loss = 0.833333
I0630 05:38:06.192020 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 05:38:06.192029 29777 sgd_solver.cpp:106] Iteration 65500, lr = 0.00795313
I0630 05:38:22.300556 29777 solver.cpp:290] Iteration 65600 (6.20806 iter/s, 16.1081s/100 iter), loss = 1.19048
I0630 05:38:22.300657 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 05:38:22.300689 29777 sgd_solver.cpp:106] Iteration 65600, lr = 0.00795
I0630 05:38:38.649194 29777 solver.cpp:290] Iteration 65700 (6.11692 iter/s, 16.3481s/100 iter), loss = 1.08333
I0630 05:38:38.649217 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 05:38:38.649224 29777 sgd_solver.cpp:106] Iteration 65700, lr = 0.00794687
I0630 05:38:55.108309 29777 solver.cpp:290] Iteration 65800 (6.07584 iter/s, 16.4586s/100 iter), loss = 0.809524
I0630 05:38:55.108415 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 05:38:55.108434 29777 sgd_solver.cpp:106] Iteration 65800, lr = 0.00794375
I0630 05:39:11.432003 29777 solver.cpp:290] Iteration 65900 (6.12627 iter/s, 16.3231s/100 iter), loss = 1.05952
I0630 05:39:11.432025 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 05:39:11.432032 29777 sgd_solver.cpp:106] Iteration 65900, lr = 0.00794062
I0630 05:39:27.506494 29777 solver.cpp:354] Sparsity after update:
I0630 05:39:27.507956 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:39:27.507964 29777 net.cpp:1851] conv1a_param_0(0.165) 
I0630 05:39:27.507972 29777 net.cpp:1851] conv1b_param_0(0.33) 
I0630 05:39:27.507973 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:39:27.507975 29777 net.cpp:1851] res2a_branch2a_param_0(0.33) 
I0630 05:39:27.507977 29777 net.cpp:1851] res2a_branch2b_param_0(0.33) 
I0630 05:39:27.507979 29777 net.cpp:1851] res3a_branch2a_param_0(0.33) 
I0630 05:39:27.507982 29777 net.cpp:1851] res3a_branch2b_param_0(0.33) 
I0630 05:39:27.507983 29777 net.cpp:1851] res4a_branch2a_param_0(0.33) 
I0630 05:39:27.507985 29777 net.cpp:1851] res4a_branch2b_param_0(0.33) 
I0630 05:39:27.507987 29777 net.cpp:1851] res5a_branch2a_param_0(0.33) 
I0630 05:39:27.507989 29777 net.cpp:1851] res5a_branch2b_param_0(0.33) 
I0630 05:39:27.507990 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (776672/2.86678e+06) 0.271
I0630 05:39:27.508082 29777 solver.cpp:471] Iteration 66000, Testing net (#0)
I0630 05:39:34.564610 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 05:40:34.491297 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56654
I0630 05:40:34.491348 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.804281
I0630 05:40:34.491354 29777 solver.cpp:544]     Test net output #2: loss = 1.53276 (* 1 = 1.53276 loss)
I0630 05:40:34.669544 29777 solver.cpp:290] Iteration 66000 (1.20141 iter/s, 83.2352s/100 iter), loss = 1.04762
I0630 05:40:34.669566 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 05:40:34.669574 29777 sgd_solver.cpp:106] Iteration 66000, lr = 0.0079375
I0630 05:40:34.670271 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.34
I0630 05:40:34.893188 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 05:40:50.252248 29777 solver.cpp:290] Iteration 66100 (6.41756 iter/s, 15.5822s/100 iter), loss = 1.16667
I0630 05:40:50.252274 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 05:40:50.252282 29777 sgd_solver.cpp:106] Iteration 66100, lr = 0.00793437
I0630 05:41:07.219377 29777 solver.cpp:290] Iteration 66200 (5.89392 iter/s, 16.9666s/100 iter), loss = 1.13095
I0630 05:41:07.219429 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 05:41:07.219437 29777 sgd_solver.cpp:106] Iteration 66200, lr = 0.00793125
I0630 05:41:23.698922 29777 solver.cpp:290] Iteration 66300 (6.06832 iter/s, 16.479s/100 iter), loss = 1.09524
I0630 05:41:23.699028 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 05:41:23.699064 29777 sgd_solver.cpp:106] Iteration 66300, lr = 0.00792812
I0630 05:41:39.919354 29777 solver.cpp:290] Iteration 66400 (6.16527 iter/s, 16.2199s/100 iter), loss = 0.880952
I0630 05:41:39.919426 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 05:41:39.919433 29777 sgd_solver.cpp:106] Iteration 66400, lr = 0.007925
I0630 05:41:56.045202 29777 solver.cpp:290] Iteration 66500 (6.20142 iter/s, 16.1253s/100 iter), loss = 1.2381
I0630 05:41:56.045223 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 05:41:56.045230 29777 sgd_solver.cpp:106] Iteration 66500, lr = 0.00792187
I0630 05:42:12.345329 29777 solver.cpp:290] Iteration 66600 (6.1351 iter/s, 16.2997s/100 iter), loss = 0.821429
I0630 05:42:12.345432 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 05:42:12.345441 29777 sgd_solver.cpp:106] Iteration 66600, lr = 0.00791875
I0630 05:42:28.472012 29777 solver.cpp:290] Iteration 66700 (6.20111 iter/s, 16.1261s/100 iter), loss = 1.0119
I0630 05:42:28.472035 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 05:42:28.472041 29777 sgd_solver.cpp:106] Iteration 66700, lr = 0.00791562
I0630 05:42:44.704329 29777 solver.cpp:290] Iteration 66800 (6.16073 iter/s, 16.2318s/100 iter), loss = 0.916667
I0630 05:42:44.704432 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 05:42:44.704444 29777 sgd_solver.cpp:106] Iteration 66800, lr = 0.0079125
I0630 05:43:00.832502 29777 solver.cpp:290] Iteration 66900 (6.20054 iter/s, 16.1276s/100 iter), loss = 1.41667
I0630 05:43:00.832556 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 05:43:00.832590 29777 sgd_solver.cpp:106] Iteration 66900, lr = 0.00790937
I0630 05:43:16.823930 29777 solver.cpp:354] Sparsity after update:
I0630 05:43:16.845173 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:43:16.845360 29777 net.cpp:1851] conv1a_param_0(0.17) 
I0630 05:43:16.845376 29777 net.cpp:1851] conv1b_param_0(0.34) 
I0630 05:43:16.845379 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:43:16.845383 29777 net.cpp:1851] res2a_branch2a_param_0(0.34) 
I0630 05:43:16.845446 29777 net.cpp:1851] res2a_branch2b_param_0(0.34) 
I0630 05:43:16.845511 29777 net.cpp:1851] res3a_branch2a_param_0(0.34) 
I0630 05:43:16.845520 29777 net.cpp:1851] res3a_branch2b_param_0(0.34) 
I0630 05:43:16.845523 29777 net.cpp:1851] res4a_branch2a_param_0(0.34) 
I0630 05:43:16.845600 29777 net.cpp:1851] res4a_branch2b_param_0(0.34) 
I0630 05:43:16.845613 29777 net.cpp:1851] res5a_branch2a_param_0(0.34) 
I0630 05:43:16.845692 29777 net.cpp:1851] res5a_branch2b_param_0(0.34) 
I0630 05:43:16.845706 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (800210/2.86678e+06) 0.279
I0630 05:43:17.029714 29777 solver.cpp:290] Iteration 67000 (6.17409 iter/s, 16.1967s/100 iter), loss = 1.27381
I0630 05:43:17.029739 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 05:43:17.029747 29777 sgd_solver.cpp:106] Iteration 67000, lr = 0.00790625
I0630 05:43:33.156610 29777 solver.cpp:290] Iteration 67100 (6.201 iter/s, 16.1264s/100 iter), loss = 0.690476
I0630 05:43:33.156690 29777 solver.cpp:309]     Train net output #0: loss = 0.5 (* 1 = 0.5 loss)
I0630 05:43:33.156718 29777 sgd_solver.cpp:106] Iteration 67100, lr = 0.00790313
I0630 05:43:49.432188 29777 solver.cpp:290] Iteration 67200 (6.14437 iter/s, 16.2751s/100 iter), loss = 0.678571
I0630 05:43:49.432263 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 05:43:49.432271 29777 sgd_solver.cpp:106] Iteration 67200, lr = 0.0079
I0630 05:44:05.660360 29777 solver.cpp:290] Iteration 67300 (6.16232 iter/s, 16.2276s/100 iter), loss = 1.08333
I0630 05:44:05.660382 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 05:44:05.660389 29777 sgd_solver.cpp:106] Iteration 67300, lr = 0.00789688
I0630 05:44:21.888247 29777 solver.cpp:290] Iteration 67400 (6.16241 iter/s, 16.2274s/100 iter), loss = 0.988095
I0630 05:44:21.888339 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 05:44:21.888350 29777 sgd_solver.cpp:106] Iteration 67400, lr = 0.00789375
I0630 05:44:38.115219 29777 solver.cpp:290] Iteration 67500 (6.16278 iter/s, 16.2264s/100 iter), loss = 1.08333
I0630 05:44:38.115242 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 05:44:38.115248 29777 sgd_solver.cpp:106] Iteration 67500, lr = 0.00789062
I0630 05:44:54.275766 29777 solver.cpp:290] Iteration 67600 (6.18809 iter/s, 16.1601s/100 iter), loss = 1.35714
I0630 05:44:54.275836 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 05:44:54.275843 29777 sgd_solver.cpp:106] Iteration 67600, lr = 0.0078875
I0630 05:45:10.528537 29777 solver.cpp:290] Iteration 67700 (6.15299 iter/s, 16.2523s/100 iter), loss = 1.27381
I0630 05:45:10.528561 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 05:45:10.528568 29777 sgd_solver.cpp:106] Iteration 67700, lr = 0.00788437
I0630 05:45:26.684849 29777 solver.cpp:290] Iteration 67800 (6.18971 iter/s, 16.1558s/100 iter), loss = 0.738095
I0630 05:45:26.684969 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 05:45:26.684979 29777 sgd_solver.cpp:106] Iteration 67800, lr = 0.00788125
I0630 05:45:42.860692 29777 solver.cpp:290] Iteration 67900 (6.18227 iter/s, 16.1753s/100 iter), loss = 0.738095
I0630 05:45:42.860713 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 05:45:42.860720 29777 sgd_solver.cpp:106] Iteration 67900, lr = 0.00787812
I0630 05:45:59.166110 29777 solver.cpp:354] Sparsity after update:
I0630 05:45:59.167131 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:45:59.167140 29777 net.cpp:1851] conv1a_param_0(0.17) 
I0630 05:45:59.167150 29777 net.cpp:1851] conv1b_param_0(0.34) 
I0630 05:45:59.167151 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:45:59.167153 29777 net.cpp:1851] res2a_branch2a_param_0(0.34) 
I0630 05:45:59.167156 29777 net.cpp:1851] res2a_branch2b_param_0(0.34) 
I0630 05:45:59.167158 29777 net.cpp:1851] res3a_branch2a_param_0(0.34) 
I0630 05:45:59.167160 29777 net.cpp:1851] res3a_branch2b_param_0(0.34) 
I0630 05:45:59.167162 29777 net.cpp:1851] res4a_branch2a_param_0(0.34) 
I0630 05:45:59.167165 29777 net.cpp:1851] res4a_branch2b_param_0(0.34) 
I0630 05:45:59.167166 29777 net.cpp:1851] res5a_branch2a_param_0(0.34) 
I0630 05:45:59.167168 29777 net.cpp:1851] res5a_branch2b_param_0(0.34) 
I0630 05:45:59.167171 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (800210/2.86678e+06) 0.279
I0630 05:45:59.167258 29777 solver.cpp:471] Iteration 68000, Testing net (#0)
I0630 05:46:07.570441 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 05:47:01.394939 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.57304
I0630 05:47:01.395020 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.805981
I0630 05:47:01.395030 29777 solver.cpp:544]     Test net output #2: loss = 1.51236 (* 1 = 1.51236 loss)
I0630 05:47:01.570538 29777 solver.cpp:290] Iteration 68000 (1.27052 iter/s, 78.7077s/100 iter), loss = 1.19048
I0630 05:47:01.570562 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 05:47:01.570569 29777 sgd_solver.cpp:106] Iteration 68000, lr = 0.007875
I0630 05:47:01.571272 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.35
I0630 05:47:01.793678 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 05:47:17.477854 29777 solver.cpp:290] Iteration 68100 (6.2866 iter/s, 15.9069s/100 iter), loss = 0.928571
I0630 05:47:17.477880 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 05:47:17.477890 29777 sgd_solver.cpp:106] Iteration 68100, lr = 0.00787187
I0630 05:47:33.669076 29777 solver.cpp:290] Iteration 68200 (6.17637 iter/s, 16.1907s/100 iter), loss = 1.14286
I0630 05:47:33.669180 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 05:47:33.669190 29777 sgd_solver.cpp:106] Iteration 68200, lr = 0.00786875
I0630 05:47:49.728314 29777 solver.cpp:290] Iteration 68300 (6.22716 iter/s, 16.0587s/100 iter), loss = 1.15476
I0630 05:47:49.728339 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 05:47:49.728348 29777 sgd_solver.cpp:106] Iteration 68300, lr = 0.00786562
I0630 05:48:05.855743 29777 solver.cpp:290] Iteration 68400 (6.2008 iter/s, 16.127s/100 iter), loss = 1.53571
I0630 05:48:05.855814 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 05:48:05.855821 29777 sgd_solver.cpp:106] Iteration 68400, lr = 0.0078625
I0630 05:48:22.117815 29777 solver.cpp:290] Iteration 68500 (6.14948 iter/s, 16.2615s/100 iter), loss = 0.940476
I0630 05:48:22.117854 29777 solver.cpp:309]     Train net output #0: loss = 0.571429 (* 1 = 0.571429 loss)
I0630 05:48:22.117866 29777 sgd_solver.cpp:106] Iteration 68500, lr = 0.00785937
I0630 05:48:38.310878 29777 solver.cpp:290] Iteration 68600 (6.17567 iter/s, 16.1926s/100 iter), loss = 1.39286
I0630 05:48:38.311132 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 05:48:38.311244 29777 sgd_solver.cpp:106] Iteration 68600, lr = 0.00785625
I0630 05:48:54.410362 29777 solver.cpp:290] Iteration 68700 (6.21165 iter/s, 16.0988s/100 iter), loss = 0.928571
I0630 05:48:54.410388 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 05:48:54.410398 29777 sgd_solver.cpp:106] Iteration 68700, lr = 0.00785313
I0630 05:49:10.480068 29777 solver.cpp:290] Iteration 68800 (6.22307 iter/s, 16.0692s/100 iter), loss = 0.72619
I0630 05:49:10.480162 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 05:49:10.480173 29777 sgd_solver.cpp:106] Iteration 68800, lr = 0.00785
I0630 05:49:26.772305 29777 solver.cpp:290] Iteration 68900 (6.1381 iter/s, 16.2917s/100 iter), loss = 1.03571
I0630 05:49:26.772334 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 05:49:26.772347 29777 sgd_solver.cpp:106] Iteration 68900, lr = 0.00784688
I0630 05:49:42.817756 29777 solver.cpp:354] Sparsity after update:
I0630 05:49:42.838160 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:49:42.838173 29777 net.cpp:1851] conv1a_param_0(0.175) 
I0630 05:49:42.838182 29777 net.cpp:1851] conv1b_param_0(0.35) 
I0630 05:49:42.838184 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:49:42.838186 29777 net.cpp:1851] res2a_branch2a_param_0(0.35) 
I0630 05:49:42.838188 29777 net.cpp:1851] res2a_branch2b_param_0(0.35) 
I0630 05:49:42.838191 29777 net.cpp:1851] res3a_branch2a_param_0(0.35) 
I0630 05:49:42.838192 29777 net.cpp:1851] res3a_branch2b_param_0(0.35) 
I0630 05:49:42.838194 29777 net.cpp:1851] res4a_branch2a_param_0(0.35) 
I0630 05:49:42.838196 29777 net.cpp:1851] res4a_branch2b_param_0(0.35) 
I0630 05:49:42.838201 29777 net.cpp:1851] res5a_branch2a_param_0(0.35) 
I0630 05:49:42.838203 29777 net.cpp:1851] res5a_branch2b_param_0(0.35) 
I0630 05:49:42.838205 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (823746/2.86678e+06) 0.287
I0630 05:49:42.990581 29777 solver.cpp:290] Iteration 69000 (6.16607 iter/s, 16.2178s/100 iter), loss = 1.25
I0630 05:49:42.990602 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 05:49:42.990608 29777 sgd_solver.cpp:106] Iteration 69000, lr = 0.00784375
I0630 05:49:59.154991 29777 solver.cpp:290] Iteration 69100 (6.18661 iter/s, 16.1639s/100 iter), loss = 1.17857
I0630 05:49:59.155020 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 05:49:59.155030 29777 sgd_solver.cpp:106] Iteration 69100, lr = 0.00784063
I0630 05:50:15.469106 29777 solver.cpp:290] Iteration 69200 (6.12984 iter/s, 16.3136s/100 iter), loss = 1.13095
I0630 05:50:15.469190 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 05:50:15.469198 29777 sgd_solver.cpp:106] Iteration 69200, lr = 0.0078375
I0630 05:50:31.620386 29777 solver.cpp:290] Iteration 69300 (6.19166 iter/s, 16.1507s/100 iter), loss = 1
I0630 05:50:31.620411 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 05:50:31.620419 29777 sgd_solver.cpp:106] Iteration 69300, lr = 0.00783437
I0630 05:50:47.993705 29777 solver.cpp:290] Iteration 69400 (6.10768 iter/s, 16.3728s/100 iter), loss = 1.33333
I0630 05:50:47.993801 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 05:50:47.993813 29777 sgd_solver.cpp:106] Iteration 69400, lr = 0.00783125
I0630 05:51:04.752816 29777 solver.cpp:290] Iteration 69500 (5.9671 iter/s, 16.7586s/100 iter), loss = 0.928571
I0630 05:51:04.752841 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 05:51:04.752851 29777 sgd_solver.cpp:106] Iteration 69500, lr = 0.00782812
I0630 05:51:21.054244 29777 solver.cpp:290] Iteration 69600 (6.13461 iter/s, 16.301s/100 iter), loss = 1.16667
I0630 05:51:21.054337 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 05:51:21.054347 29777 sgd_solver.cpp:106] Iteration 69600, lr = 0.007825
I0630 05:51:37.022455 29777 solver.cpp:290] Iteration 69700 (6.26265 iter/s, 15.9677s/100 iter), loss = 0.988095
I0630 05:51:37.022480 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 05:51:37.022490 29777 sgd_solver.cpp:106] Iteration 69700, lr = 0.00782187
I0630 05:51:53.019091 29777 solver.cpp:290] Iteration 69800 (6.2515 iter/s, 15.9962s/100 iter), loss = 0.97619
I0630 05:51:53.019194 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 05:51:53.019204 29777 sgd_solver.cpp:106] Iteration 69800, lr = 0.00781875
I0630 05:52:09.046422 29777 solver.cpp:290] Iteration 69900 (6.23955 iter/s, 16.0268s/100 iter), loss = 1.39286
I0630 05:52:09.046444 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 05:52:09.046452 29777 sgd_solver.cpp:106] Iteration 69900, lr = 0.00781562
I0630 05:52:25.098418 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_70000.caffemodel
I0630 05:52:25.117558 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_70000.solverstate
I0630 05:52:25.126152 29777 solver.cpp:354] Sparsity after update:
I0630 05:52:25.127122 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:52:25.127130 29777 net.cpp:1851] conv1a_param_0(0.175) 
I0630 05:52:25.127137 29777 net.cpp:1851] conv1b_param_0(0.35) 
I0630 05:52:25.127140 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:52:25.127141 29777 net.cpp:1851] res2a_branch2a_param_0(0.35) 
I0630 05:52:25.127143 29777 net.cpp:1851] res2a_branch2b_param_0(0.35) 
I0630 05:52:25.127146 29777 net.cpp:1851] res3a_branch2a_param_0(0.35) 
I0630 05:52:25.127147 29777 net.cpp:1851] res3a_branch2b_param_0(0.35) 
I0630 05:52:25.127149 29777 net.cpp:1851] res4a_branch2a_param_0(0.35) 
I0630 05:52:25.127151 29777 net.cpp:1851] res4a_branch2b_param_0(0.35) 
I0630 05:52:25.127153 29777 net.cpp:1851] res5a_branch2a_param_0(0.35) 
I0630 05:52:25.127156 29777 net.cpp:1851] res5a_branch2b_param_0(0.35) 
I0630 05:52:25.127158 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (823746/2.86678e+06) 0.287
I0630 05:52:25.127251 29777 solver.cpp:471] Iteration 70000, Testing net (#0)
I0630 05:52:31.240478 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 05:53:13.637768 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.564759
I0630 05:53:13.637822 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.801401
I0630 05:53:13.637828 29777 solver.cpp:544]     Test net output #2: loss = 1.56544 (* 1 = 1.56544 loss)
I0630 05:53:13.814138 29777 solver.cpp:290] Iteration 70000 (1.54402 iter/s, 64.7659s/100 iter), loss = 0.476191
I0630 05:53:13.814163 29777 solver.cpp:309]     Train net output #0: loss = 0.52381 (* 1 = 0.52381 loss)
I0630 05:53:13.814170 29777 sgd_solver.cpp:106] Iteration 70000, lr = 0.0078125
I0630 05:53:13.814898 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.36
I0630 05:53:14.057551 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 05:53:30.148615 29777 solver.cpp:290] Iteration 70100 (6.1222 iter/s, 16.334s/100 iter), loss = 1.2619
I0630 05:53:30.148641 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 05:53:30.148650 29777 sgd_solver.cpp:106] Iteration 70100, lr = 0.00780937
I0630 05:53:46.203061 29777 solver.cpp:290] Iteration 70200 (6.22899 iter/s, 16.054s/100 iter), loss = 1.10714
I0630 05:53:46.203176 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 05:53:46.203186 29777 sgd_solver.cpp:106] Iteration 70200, lr = 0.00780625
I0630 05:54:02.356597 29777 solver.cpp:290] Iteration 70300 (6.19081 iter/s, 16.153s/100 iter), loss = 1.17857
I0630 05:54:02.356624 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 05:54:02.356634 29777 sgd_solver.cpp:106] Iteration 70300, lr = 0.00780312
I0630 05:54:18.394696 29777 solver.cpp:290] Iteration 70400 (6.23534 iter/s, 16.0376s/100 iter), loss = 1.46429
I0630 05:54:18.394799 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 05:54:18.394809 29777 sgd_solver.cpp:106] Iteration 70400, lr = 0.0078
I0630 05:54:34.437590 29777 solver.cpp:290] Iteration 70500 (6.2335 iter/s, 16.0423s/100 iter), loss = 1.71429
I0630 05:54:34.437618 29777 solver.cpp:309]     Train net output #0: loss = 1.90476 (* 1 = 1.90476 loss)
I0630 05:54:34.437626 29777 sgd_solver.cpp:106] Iteration 70500, lr = 0.00779688
I0630 05:54:50.548130 29777 solver.cpp:290] Iteration 70600 (6.2073 iter/s, 16.1101s/100 iter), loss = 1.25
I0630 05:54:50.548224 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 05:54:50.548235 29777 sgd_solver.cpp:106] Iteration 70600, lr = 0.00779375
I0630 05:55:06.809847 29777 solver.cpp:290] Iteration 70700 (6.14962 iter/s, 16.2612s/100 iter), loss = 1.36905
I0630 05:55:06.809870 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 05:55:06.809878 29777 sgd_solver.cpp:106] Iteration 70700, lr = 0.00779063
I0630 05:55:22.980314 29777 solver.cpp:290] Iteration 70800 (6.18429 iter/s, 16.17s/100 iter), loss = 1.2619
I0630 05:55:22.980414 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 05:55:22.980422 29777 sgd_solver.cpp:106] Iteration 70800, lr = 0.0077875
I0630 05:55:39.048033 29777 solver.cpp:290] Iteration 70900 (6.22387 iter/s, 16.0672s/100 iter), loss = 1.15476
I0630 05:55:39.048056 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 05:55:39.048063 29777 sgd_solver.cpp:106] Iteration 70900, lr = 0.00778437
I0630 05:55:55.052947 29777 solver.cpp:354] Sparsity after update:
I0630 05:55:55.073216 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:55:55.073230 29777 net.cpp:1851] conv1a_param_0(0.18) 
I0630 05:55:55.073241 29777 net.cpp:1851] conv1b_param_0(0.36) 
I0630 05:55:55.073246 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:55:55.073248 29777 net.cpp:1851] res2a_branch2a_param_0(0.36) 
I0630 05:55:55.073253 29777 net.cpp:1851] res2a_branch2b_param_0(0.36) 
I0630 05:55:55.073257 29777 net.cpp:1851] res3a_branch2a_param_0(0.36) 
I0630 05:55:55.073261 29777 net.cpp:1851] res3a_branch2b_param_0(0.36) 
I0630 05:55:55.073263 29777 net.cpp:1851] res4a_branch2a_param_0(0.36) 
I0630 05:55:55.073267 29777 net.cpp:1851] res4a_branch2b_param_0(0.36) 
I0630 05:55:55.073271 29777 net.cpp:1851] res5a_branch2a_param_0(0.36) 
I0630 05:55:55.073276 29777 net.cpp:1851] res5a_branch2b_param_0(0.36) 
I0630 05:55:55.073279 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (847283/2.86678e+06) 0.296
I0630 05:55:55.228878 29777 solver.cpp:290] Iteration 71000 (6.18033 iter/s, 16.1804s/100 iter), loss = 0.952381
I0630 05:55:55.228904 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 05:55:55.228912 29777 sgd_solver.cpp:106] Iteration 71000, lr = 0.00778125
I0630 05:56:11.297562 29777 solver.cpp:290] Iteration 71100 (6.22347 iter/s, 16.0682s/100 iter), loss = 0.97619
I0630 05:56:11.297586 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 05:56:11.297593 29777 sgd_solver.cpp:106] Iteration 71100, lr = 0.00777812
I0630 05:56:27.311363 29777 solver.cpp:290] Iteration 71200 (6.24479 iter/s, 16.0133s/100 iter), loss = 1.32143
I0630 05:56:27.311453 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 05:56:27.311463 29777 sgd_solver.cpp:106] Iteration 71200, lr = 0.007775
I0630 05:56:43.274574 29777 solver.cpp:290] Iteration 71300 (6.26461 iter/s, 15.9627s/100 iter), loss = 1.46429
I0630 05:56:43.274601 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 05:56:43.274610 29777 sgd_solver.cpp:106] Iteration 71300, lr = 0.00777187
I0630 05:56:59.290525 29777 solver.cpp:290] Iteration 71400 (6.24396 iter/s, 16.0155s/100 iter), loss = 0.928571
I0630 05:56:59.290588 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 05:56:59.290596 29777 sgd_solver.cpp:106] Iteration 71400, lr = 0.00776875
I0630 05:57:15.362407 29777 solver.cpp:290] Iteration 71500 (6.22224 iter/s, 16.0714s/100 iter), loss = 1.46429
I0630 05:57:15.362432 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 05:57:15.362437 29777 sgd_solver.cpp:106] Iteration 71500, lr = 0.00776563
I0630 05:57:31.510815 29777 solver.cpp:290] Iteration 71600 (6.19274 iter/s, 16.1479s/100 iter), loss = 0.928571
I0630 05:57:31.510901 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 05:57:31.510916 29777 sgd_solver.cpp:106] Iteration 71600, lr = 0.0077625
I0630 05:57:47.689947 29777 solver.cpp:290] Iteration 71700 (6.181 iter/s, 16.1786s/100 iter), loss = 1.35714
I0630 05:57:47.689973 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 05:57:47.689981 29777 sgd_solver.cpp:106] Iteration 71700, lr = 0.00775938
I0630 05:58:03.790956 29777 solver.cpp:290] Iteration 71800 (6.21097 iter/s, 16.1005s/100 iter), loss = 1.07143
I0630 05:58:03.791069 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 05:58:03.791079 29777 sgd_solver.cpp:106] Iteration 71800, lr = 0.00775625
I0630 05:58:20.042714 29777 solver.cpp:290] Iteration 71900 (6.15339 iter/s, 16.2512s/100 iter), loss = 1.45238
I0630 05:58:20.042735 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 05:58:20.042743 29777 sgd_solver.cpp:106] Iteration 71900, lr = 0.00775312
I0630 05:58:35.946403 29777 solver.cpp:354] Sparsity after update:
I0630 05:58:35.947840 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 05:58:35.947849 29777 net.cpp:1851] conv1a_param_0(0.18) 
I0630 05:58:35.947855 29777 net.cpp:1851] conv1b_param_0(0.36) 
I0630 05:58:35.947859 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 05:58:35.947860 29777 net.cpp:1851] res2a_branch2a_param_0(0.36) 
I0630 05:58:35.947863 29777 net.cpp:1851] res2a_branch2b_param_0(0.36) 
I0630 05:58:35.947865 29777 net.cpp:1851] res3a_branch2a_param_0(0.36) 
I0630 05:58:35.947867 29777 net.cpp:1851] res3a_branch2b_param_0(0.36) 
I0630 05:58:35.947870 29777 net.cpp:1851] res4a_branch2a_param_0(0.36) 
I0630 05:58:35.947872 29777 net.cpp:1851] res4a_branch2b_param_0(0.36) 
I0630 05:58:35.947875 29777 net.cpp:1851] res5a_branch2a_param_0(0.36) 
I0630 05:58:35.947876 29777 net.cpp:1851] res5a_branch2b_param_0(0.36) 
I0630 05:58:35.947880 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (847283/2.86678e+06) 0.296
I0630 05:58:35.947964 29777 solver.cpp:471] Iteration 72000, Testing net (#0)
I0630 05:58:42.345530 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 05:59:24.880152 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.571939
I0630 05:59:24.880259 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.805861
I0630 05:59:24.880269 29777 solver.cpp:544]     Test net output #2: loss = 1.51954 (* 1 = 1.51954 loss)
I0630 05:59:25.056967 29777 solver.cpp:290] Iteration 72000 (1.53817 iter/s, 65.0125s/100 iter), loss = 1.02381
I0630 05:59:25.056991 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 05:59:25.057000 29777 sgd_solver.cpp:106] Iteration 72000, lr = 0.00775
I0630 05:59:25.057981 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.37
I0630 05:59:25.304527 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 05:59:41.333969 29777 solver.cpp:290] Iteration 72100 (6.14382 iter/s, 16.2765s/100 iter), loss = 1.13095
I0630 05:59:41.333995 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 05:59:41.334008 29777 sgd_solver.cpp:106] Iteration 72100, lr = 0.00774688
I0630 05:59:57.426584 29777 solver.cpp:290] Iteration 72200 (6.21421 iter/s, 16.0921s/100 iter), loss = 0.75
I0630 05:59:57.426694 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 05:59:57.426705 29777 sgd_solver.cpp:106] Iteration 72200, lr = 0.00774375
I0630 06:00:13.558910 29777 solver.cpp:290] Iteration 72300 (6.19895 iter/s, 16.1318s/100 iter), loss = 1.28571
I0630 06:00:13.558935 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 06:00:13.558954 29777 sgd_solver.cpp:106] Iteration 72300, lr = 0.00774063
I0630 06:00:29.781744 29777 solver.cpp:290] Iteration 72400 (6.16433 iter/s, 16.2224s/100 iter), loss = 0.964286
I0630 06:00:29.783833 29777 solver.cpp:309]     Train net output #0: loss = 0.690476 (* 1 = 0.690476 loss)
I0630 06:00:29.783854 29777 sgd_solver.cpp:106] Iteration 72400, lr = 0.0077375
I0630 06:00:45.917464 29777 solver.cpp:290] Iteration 72500 (6.1984 iter/s, 16.1332s/100 iter), loss = 1.17857
I0630 06:00:45.917488 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 06:00:45.917497 29777 sgd_solver.cpp:106] Iteration 72500, lr = 0.00773437
I0630 06:01:02.153213 29777 solver.cpp:290] Iteration 72600 (6.15943 iter/s, 16.2353s/100 iter), loss = 0.869048
I0630 06:01:02.153270 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 06:01:02.153278 29777 sgd_solver.cpp:106] Iteration 72600, lr = 0.00773125
I0630 06:01:18.296633 29777 solver.cpp:290] Iteration 72700 (6.19467 iter/s, 16.1429s/100 iter), loss = 1.61905
I0630 06:01:18.296659 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 06:01:18.296669 29777 sgd_solver.cpp:106] Iteration 72700, lr = 0.00772812
I0630 06:01:34.296741 29777 solver.cpp:290] Iteration 72800 (6.25014 iter/s, 15.9996s/100 iter), loss = 1.2619
I0630 06:01:34.296813 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 06:01:34.296820 29777 sgd_solver.cpp:106] Iteration 72800, lr = 0.007725
I0630 06:01:50.283690 29777 solver.cpp:290] Iteration 72900 (6.2553 iter/s, 15.9864s/100 iter), loss = 1.0119
I0630 06:01:50.283715 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 06:01:50.283725 29777 sgd_solver.cpp:106] Iteration 72900, lr = 0.00772187
I0630 06:02:06.194926 29777 solver.cpp:354] Sparsity after update:
I0630 06:02:06.215346 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:02:06.215382 29777 net.cpp:1851] conv1a_param_0(0.185) 
I0630 06:02:06.215399 29777 net.cpp:1851] conv1b_param_0(0.37) 
I0630 06:02:06.215406 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:02:06.215415 29777 net.cpp:1851] res2a_branch2a_param_0(0.37) 
I0630 06:02:06.215425 29777 net.cpp:1851] res2a_branch2b_param_0(0.37) 
I0630 06:02:06.215435 29777 net.cpp:1851] res3a_branch2a_param_0(0.37) 
I0630 06:02:06.215442 29777 net.cpp:1851] res3a_branch2b_param_0(0.37) 
I0630 06:02:06.215451 29777 net.cpp:1851] res4a_branch2a_param_0(0.37) 
I0630 06:02:06.215458 29777 net.cpp:1851] res4a_branch2b_param_0(0.37) 
I0630 06:02:06.215466 29777 net.cpp:1851] res5a_branch2a_param_0(0.37) 
I0630 06:02:06.215474 29777 net.cpp:1851] res5a_branch2b_param_0(0.37) 
I0630 06:02:06.215482 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (870817/2.86678e+06) 0.304
I0630 06:02:06.374341 29777 solver.cpp:290] Iteration 73000 (6.21497 iter/s, 16.0902s/100 iter), loss = 1.08333
I0630 06:02:06.374363 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 06:02:06.374370 29777 sgd_solver.cpp:106] Iteration 73000, lr = 0.00771875
I0630 06:02:22.420152 29777 solver.cpp:290] Iteration 73100 (6.23234 iter/s, 16.0453s/100 iter), loss = 1.13095
I0630 06:02:22.420176 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 06:02:22.420182 29777 sgd_solver.cpp:106] Iteration 73100, lr = 0.00771563
I0630 06:02:38.426194 29777 solver.cpp:290] Iteration 73200 (6.24782 iter/s, 16.0056s/100 iter), loss = 1
I0630 06:02:38.426285 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 06:02:38.426291 29777 sgd_solver.cpp:106] Iteration 73200, lr = 0.0077125
I0630 06:02:54.433547 29777 solver.cpp:290] Iteration 73300 (6.24734 iter/s, 16.0068s/100 iter), loss = 1.11905
I0630 06:02:54.433574 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 06:02:54.433583 29777 sgd_solver.cpp:106] Iteration 73300, lr = 0.00770937
I0630 06:03:10.396828 29777 solver.cpp:290] Iteration 73400 (6.26456 iter/s, 15.9628s/100 iter), loss = 1
I0630 06:03:10.396936 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 06:03:10.396946 29777 sgd_solver.cpp:106] Iteration 73400, lr = 0.00770625
I0630 06:03:26.480128 29777 solver.cpp:290] Iteration 73500 (6.21784 iter/s, 16.0827s/100 iter), loss = 1.13095
I0630 06:03:26.480155 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 06:03:26.480172 29777 sgd_solver.cpp:106] Iteration 73500, lr = 0.00770312
I0630 06:03:42.454568 29777 solver.cpp:290] Iteration 73600 (6.26018 iter/s, 15.974s/100 iter), loss = 1.03571
I0630 06:03:42.454638 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 06:03:42.454645 29777 sgd_solver.cpp:106] Iteration 73600, lr = 0.0077
I0630 06:03:58.477660 29777 solver.cpp:290] Iteration 73700 (6.24119 iter/s, 16.0226s/100 iter), loss = 1.22619
I0630 06:03:58.477710 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 06:03:58.477730 29777 sgd_solver.cpp:106] Iteration 73700, lr = 0.00769688
I0630 06:04:14.669879 29777 solver.cpp:290] Iteration 73800 (6.17599 iter/s, 16.1917s/100 iter), loss = 1.16667
I0630 06:04:14.669987 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 06:04:14.669997 29777 sgd_solver.cpp:106] Iteration 73800, lr = 0.00769375
I0630 06:04:30.751170 29777 solver.cpp:290] Iteration 73900 (6.21862 iter/s, 16.0807s/100 iter), loss = 1.04762
I0630 06:04:30.751197 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 06:04:30.751206 29777 sgd_solver.cpp:106] Iteration 73900, lr = 0.00769062
I0630 06:04:46.802836 29777 solver.cpp:354] Sparsity after update:
I0630 06:04:46.804098 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:04:46.804106 29777 net.cpp:1851] conv1a_param_0(0.185) 
I0630 06:04:46.804113 29777 net.cpp:1851] conv1b_param_0(0.37) 
I0630 06:04:46.804116 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:04:46.804118 29777 net.cpp:1851] res2a_branch2a_param_0(0.37) 
I0630 06:04:46.804121 29777 net.cpp:1851] res2a_branch2b_param_0(0.37) 
I0630 06:04:46.804122 29777 net.cpp:1851] res3a_branch2a_param_0(0.37) 
I0630 06:04:46.804124 29777 net.cpp:1851] res3a_branch2b_param_0(0.37) 
I0630 06:04:46.804126 29777 net.cpp:1851] res4a_branch2a_param_0(0.37) 
I0630 06:04:46.804128 29777 net.cpp:1851] res4a_branch2b_param_0(0.37) 
I0630 06:04:46.804131 29777 net.cpp:1851] res5a_branch2a_param_0(0.37) 
I0630 06:04:46.804132 29777 net.cpp:1851] res5a_branch2b_param_0(0.37) 
I0630 06:04:46.804134 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (870817/2.86678e+06) 0.304
I0630 06:04:46.804251 29777 solver.cpp:471] Iteration 74000, Testing net (#0)
I0630 06:04:54.446146 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 06:05:37.169081 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.567319
I0630 06:05:37.169133 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.802282
I0630 06:05:37.169139 29777 solver.cpp:544]     Test net output #2: loss = 1.54096 (* 1 = 1.54096 loss)
I0630 06:05:37.344485 29777 solver.cpp:290] Iteration 74000 (1.50169 iter/s, 66.5915s/100 iter), loss = 0.892857
I0630 06:05:37.344511 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 06:05:37.344519 29777 sgd_solver.cpp:106] Iteration 74000, lr = 0.0076875
I0630 06:05:37.345504 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.38
I0630 06:05:37.601541 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 06:05:53.510392 29777 solver.cpp:290] Iteration 74100 (6.18604 iter/s, 16.1654s/100 iter), loss = 0.940476
I0630 06:05:53.510414 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 06:05:53.510422 29777 sgd_solver.cpp:106] Iteration 74100, lr = 0.00768437
I0630 06:06:09.464521 29777 solver.cpp:290] Iteration 74200 (6.26815 iter/s, 15.9537s/100 iter), loss = 0.880952
I0630 06:06:09.464603 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 06:06:09.464612 29777 sgd_solver.cpp:106] Iteration 74200, lr = 0.00768125
I0630 06:06:25.432833 29777 solver.cpp:290] Iteration 74300 (6.26261 iter/s, 15.9678s/100 iter), loss = 0.619048
I0630 06:06:25.432857 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 06:06:25.432863 29777 sgd_solver.cpp:106] Iteration 74300, lr = 0.00767812
I0630 06:06:41.509405 29777 solver.cpp:290] Iteration 74400 (6.22041 iter/s, 16.0761s/100 iter), loss = 1.25
I0630 06:06:41.509524 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 06:06:41.509534 29777 sgd_solver.cpp:106] Iteration 74400, lr = 0.007675
I0630 06:06:57.502697 29777 solver.cpp:290] Iteration 74500 (6.25284 iter/s, 15.9927s/100 iter), loss = 0.738095
I0630 06:06:57.502724 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 06:06:57.502733 29777 sgd_solver.cpp:106] Iteration 74500, lr = 0.00767187
I0630 06:07:13.572283 29777 solver.cpp:290] Iteration 74600 (6.22312 iter/s, 16.0691s/100 iter), loss = 1.27381
I0630 06:07:13.572386 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 06:07:13.572396 29777 sgd_solver.cpp:106] Iteration 74600, lr = 0.00766875
I0630 06:07:29.659207 29777 solver.cpp:290] Iteration 74700 (6.21644 iter/s, 16.0864s/100 iter), loss = 1.0119
I0630 06:07:29.659234 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 06:07:29.659240 29777 sgd_solver.cpp:106] Iteration 74700, lr = 0.00766562
I0630 06:07:45.908524 29777 solver.cpp:290] Iteration 74800 (6.15429 iter/s, 16.2488s/100 iter), loss = 1.16667
I0630 06:07:45.908619 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 06:07:45.908637 29777 sgd_solver.cpp:106] Iteration 74800, lr = 0.0076625
I0630 06:08:02.080229 29777 solver.cpp:290] Iteration 74900 (6.18384 iter/s, 16.1712s/100 iter), loss = 1.09524
I0630 06:08:02.080255 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 06:08:02.080265 29777 sgd_solver.cpp:106] Iteration 74900, lr = 0.00765937
I0630 06:08:18.457754 29777 solver.cpp:354] Sparsity after update:
I0630 06:08:18.477988 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:08:18.478024 29777 net.cpp:1851] conv1a_param_0(0.19) 
I0630 06:08:18.478036 29777 net.cpp:1851] conv1b_param_0(0.38) 
I0630 06:08:18.478040 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:08:18.478045 29777 net.cpp:1851] res2a_branch2a_param_0(0.38) 
I0630 06:08:18.478049 29777 net.cpp:1851] res2a_branch2b_param_0(0.38) 
I0630 06:08:18.478052 29777 net.cpp:1851] res3a_branch2a_param_0(0.38) 
I0630 06:08:18.478055 29777 net.cpp:1851] res3a_branch2b_param_0(0.38) 
I0630 06:08:18.478060 29777 net.cpp:1851] res4a_branch2a_param_0(0.38) 
I0630 06:08:18.478062 29777 net.cpp:1851] res4a_branch2b_param_0(0.38) 
I0630 06:08:18.478065 29777 net.cpp:1851] res5a_branch2a_param_0(0.38) 
I0630 06:08:18.478068 29777 net.cpp:1851] res5a_branch2b_param_0(0.38) 
I0630 06:08:18.478072 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (894351/2.86678e+06) 0.312
I0630 06:08:18.637269 29777 solver.cpp:290] Iteration 75000 (6.0399 iter/s, 16.5566s/100 iter), loss = 0.892857
I0630 06:08:18.637292 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 06:08:18.637300 29777 sgd_solver.cpp:106] Iteration 75000, lr = 0.00765625
I0630 06:08:35.074187 29777 solver.cpp:290] Iteration 75100 (6.08404 iter/s, 16.4364s/100 iter), loss = 1.16667
I0630 06:08:35.074210 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 06:08:35.074218 29777 sgd_solver.cpp:106] Iteration 75100, lr = 0.00765312
I0630 06:08:51.274108 29777 solver.cpp:290] Iteration 75200 (6.17305 iter/s, 16.1994s/100 iter), loss = 0.892857
I0630 06:08:51.274471 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 06:08:51.274487 29777 sgd_solver.cpp:106] Iteration 75200, lr = 0.00765
I0630 06:09:07.400424 29777 solver.cpp:290] Iteration 75300 (6.20136 iter/s, 16.1255s/100 iter), loss = 1.04762
I0630 06:09:07.400524 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 06:09:07.400560 29777 sgd_solver.cpp:106] Iteration 75300, lr = 0.00764687
I0630 06:09:23.726533 29777 solver.cpp:290] Iteration 75400 (6.12536 iter/s, 16.3256s/100 iter), loss = 0.940476
I0630 06:09:23.726585 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 06:09:23.726593 29777 sgd_solver.cpp:106] Iteration 75400, lr = 0.00764375
I0630 06:09:39.967880 29777 solver.cpp:290] Iteration 75500 (6.15731 iter/s, 16.2408s/100 iter), loss = 0.904762
I0630 06:09:39.967905 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 06:09:39.967911 29777 sgd_solver.cpp:106] Iteration 75500, lr = 0.00764062
I0630 06:09:56.004814 29777 solver.cpp:290] Iteration 75600 (6.23579 iter/s, 16.0365s/100 iter), loss = 1.11905
I0630 06:09:56.004918 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 06:09:56.004928 29777 sgd_solver.cpp:106] Iteration 75600, lr = 0.0076375
I0630 06:10:12.032929 29777 solver.cpp:290] Iteration 75700 (6.23925 iter/s, 16.0276s/100 iter), loss = 1.40476
I0630 06:10:12.032953 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 06:10:12.032959 29777 sgd_solver.cpp:106] Iteration 75700, lr = 0.00763437
I0630 06:10:28.131887 29777 solver.cpp:290] Iteration 75800 (6.21176 iter/s, 16.0985s/100 iter), loss = 0.75
I0630 06:10:28.131989 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 06:10:28.131996 29777 sgd_solver.cpp:106] Iteration 75800, lr = 0.00763125
I0630 06:10:44.160990 29777 solver.cpp:290] Iteration 75900 (6.23886 iter/s, 16.0286s/100 iter), loss = 0.964286
I0630 06:10:44.161016 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 06:10:44.161026 29777 sgd_solver.cpp:106] Iteration 75900, lr = 0.00762812
I0630 06:11:00.111584 29777 solver.cpp:354] Sparsity after update:
I0630 06:11:00.113143 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:11:00.113152 29777 net.cpp:1851] conv1a_param_0(0.19) 
I0630 06:11:00.113160 29777 net.cpp:1851] conv1b_param_0(0.38) 
I0630 06:11:00.113163 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:11:00.113165 29777 net.cpp:1851] res2a_branch2a_param_0(0.38) 
I0630 06:11:00.113168 29777 net.cpp:1851] res2a_branch2b_param_0(0.38) 
I0630 06:11:00.113170 29777 net.cpp:1851] res3a_branch2a_param_0(0.38) 
I0630 06:11:00.113173 29777 net.cpp:1851] res3a_branch2b_param_0(0.38) 
I0630 06:11:00.113174 29777 net.cpp:1851] res4a_branch2a_param_0(0.38) 
I0630 06:11:00.113178 29777 net.cpp:1851] res4a_branch2b_param_0(0.38) 
I0630 06:11:00.113179 29777 net.cpp:1851] res5a_branch2a_param_0(0.38) 
I0630 06:11:00.113181 29777 net.cpp:1851] res5a_branch2b_param_0(0.38) 
I0630 06:11:00.113184 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (894351/2.86678e+06) 0.312
I0630 06:11:00.113271 29777 solver.cpp:471] Iteration 76000, Testing net (#0)
I0630 06:11:06.824879 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 06:11:48.616935 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56896
I0630 06:11:48.617012 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.803841
I0630 06:11:48.617019 29777 solver.cpp:544]     Test net output #2: loss = 1.5288 (* 1 = 1.5288 loss)
I0630 06:11:48.800097 29777 solver.cpp:290] Iteration 76000 (1.54709 iter/s, 64.6373s/100 iter), loss = 1.04762
I0630 06:11:48.800120 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 06:11:48.800127 29777 sgd_solver.cpp:106] Iteration 76000, lr = 0.007625
I0630 06:11:48.800801 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.39
I0630 06:11:49.065292 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 06:12:05.050587 29777 solver.cpp:290] Iteration 76100 (6.15384 iter/s, 16.25s/100 iter), loss = 0.964286
I0630 06:12:05.050621 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 06:12:05.050632 29777 sgd_solver.cpp:106] Iteration 76100, lr = 0.00762187
I0630 06:12:21.168040 29777 solver.cpp:290] Iteration 76200 (6.20464 iter/s, 16.117s/100 iter), loss = 1.32143
I0630 06:12:21.168148 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 06:12:21.168157 29777 sgd_solver.cpp:106] Iteration 76200, lr = 0.00761875
I0630 06:12:37.168347 29777 solver.cpp:290] Iteration 76300 (6.25009 iter/s, 15.9998s/100 iter), loss = 1.16667
I0630 06:12:37.168370 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 06:12:37.168376 29777 sgd_solver.cpp:106] Iteration 76300, lr = 0.00761562
I0630 06:12:53.189332 29777 solver.cpp:290] Iteration 76400 (6.242 iter/s, 16.0205s/100 iter), loss = 1.4881
I0630 06:12:53.189700 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 06:12:53.189713 29777 sgd_solver.cpp:106] Iteration 76400, lr = 0.0076125
I0630 06:13:09.146690 29777 solver.cpp:290] Iteration 76500 (6.26702 iter/s, 15.9565s/100 iter), loss = 1.04762
I0630 06:13:09.146715 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 06:13:09.146724 29777 sgd_solver.cpp:106] Iteration 76500, lr = 0.00760937
I0630 06:13:25.312368 29777 solver.cpp:290] Iteration 76600 (6.18613 iter/s, 16.1652s/100 iter), loss = 1
I0630 06:13:25.312476 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 06:13:25.312486 29777 sgd_solver.cpp:106] Iteration 76600, lr = 0.00760625
I0630 06:13:41.515457 29777 solver.cpp:290] Iteration 76700 (6.17187 iter/s, 16.2025s/100 iter), loss = 1.42857
I0630 06:13:41.515482 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 06:13:41.515491 29777 sgd_solver.cpp:106] Iteration 76700, lr = 0.00760312
I0630 06:13:57.922080 29777 solver.cpp:290] Iteration 76800 (6.09528 iter/s, 16.4061s/100 iter), loss = 0.714286
I0630 06:13:57.922186 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 06:13:57.922197 29777 sgd_solver.cpp:106] Iteration 76800, lr = 0.0076
I0630 06:14:14.119640 29777 solver.cpp:290] Iteration 76900 (6.17398 iter/s, 16.197s/100 iter), loss = 1
I0630 06:14:14.119663 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 06:14:14.119671 29777 sgd_solver.cpp:106] Iteration 76900, lr = 0.00759687
I0630 06:14:30.045660 29777 solver.cpp:354] Sparsity after update:
I0630 06:14:30.068601 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:14:30.068619 29777 net.cpp:1851] conv1a_param_0(0.195) 
I0630 06:14:30.068630 29777 net.cpp:1851] conv1b_param_0(0.39) 
I0630 06:14:30.068634 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:14:30.068637 29777 net.cpp:1851] res2a_branch2a_param_0(0.39) 
I0630 06:14:30.068644 29777 net.cpp:1851] res2a_branch2b_param_0(0.39) 
I0630 06:14:30.068646 29777 net.cpp:1851] res3a_branch2a_param_0(0.39) 
I0630 06:14:30.068650 29777 net.cpp:1851] res3a_branch2b_param_0(0.39) 
I0630 06:14:30.068652 29777 net.cpp:1851] res4a_branch2a_param_0(0.39) 
I0630 06:14:30.068656 29777 net.cpp:1851] res4a_branch2b_param_0(0.39) 
I0630 06:14:30.068660 29777 net.cpp:1851] res5a_branch2a_param_0(0.39) 
I0630 06:14:30.068661 29777 net.cpp:1851] res5a_branch2b_param_0(0.39) 
I0630 06:14:30.068665 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (917890/2.86678e+06) 0.32
I0630 06:14:30.226913 29777 solver.cpp:290] Iteration 77000 (6.20856 iter/s, 16.1068s/100 iter), loss = 1.15476
I0630 06:14:30.226936 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 06:14:30.226943 29777 sgd_solver.cpp:106] Iteration 77000, lr = 0.00759375
I0630 06:14:46.314347 29777 solver.cpp:290] Iteration 77100 (6.21621 iter/s, 16.087s/100 iter), loss = 0.714286
I0630 06:14:46.314371 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 06:14:46.314378 29777 sgd_solver.cpp:106] Iteration 77100, lr = 0.00759062
I0630 06:15:02.671041 29777 solver.cpp:290] Iteration 77200 (6.11388 iter/s, 16.3562s/100 iter), loss = 1.38095
I0630 06:15:02.671154 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 06:15:02.671161 29777 sgd_solver.cpp:106] Iteration 77200, lr = 0.0075875
I0630 06:15:18.816169 29777 solver.cpp:290] Iteration 77300 (6.19403 iter/s, 16.1446s/100 iter), loss = 0.678571
I0630 06:15:18.816195 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 06:15:18.816202 29777 sgd_solver.cpp:106] Iteration 77300, lr = 0.00758437
I0630 06:15:35.072160 29777 solver.cpp:290] Iteration 77400 (6.15176 iter/s, 16.2555s/100 iter), loss = 0.833333
I0630 06:15:35.072253 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 06:15:35.072265 29777 sgd_solver.cpp:106] Iteration 77400, lr = 0.00758125
I0630 06:15:51.123841 29777 solver.cpp:290] Iteration 77500 (6.23009 iter/s, 16.0511s/100 iter), loss = 0.97619
I0630 06:15:51.123868 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 06:15:51.123878 29777 sgd_solver.cpp:106] Iteration 77500, lr = 0.00757812
I0630 06:16:07.376058 29777 solver.cpp:290] Iteration 77600 (6.15319 iter/s, 16.2517s/100 iter), loss = 0.821429
I0630 06:16:07.376163 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 06:16:07.376173 29777 sgd_solver.cpp:106] Iteration 77600, lr = 0.007575
I0630 06:16:23.504421 29777 solver.cpp:290] Iteration 77700 (6.20047 iter/s, 16.1278s/100 iter), loss = 0.940476
I0630 06:16:23.504446 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 06:16:23.504452 29777 sgd_solver.cpp:106] Iteration 77700, lr = 0.00757187
I0630 06:16:39.788432 29777 solver.cpp:290] Iteration 77800 (6.14117 iter/s, 16.2835s/100 iter), loss = 1.0119
I0630 06:16:39.788539 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 06:16:39.788564 29777 sgd_solver.cpp:106] Iteration 77800, lr = 0.00756875
I0630 06:16:55.932598 29777 solver.cpp:290] Iteration 77900 (6.1944 iter/s, 16.1436s/100 iter), loss = 1.17857
I0630 06:16:55.932621 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 06:16:55.932627 29777 sgd_solver.cpp:106] Iteration 77900, lr = 0.00756562
I0630 06:17:11.893530 29777 solver.cpp:354] Sparsity after update:
I0630 06:17:11.895141 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:17:11.895149 29777 net.cpp:1851] conv1a_param_0(0.195) 
I0630 06:17:11.895156 29777 net.cpp:1851] conv1b_param_0(0.39) 
I0630 06:17:11.895159 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:17:11.895161 29777 net.cpp:1851] res2a_branch2a_param_0(0.39) 
I0630 06:17:11.895164 29777 net.cpp:1851] res2a_branch2b_param_0(0.39) 
I0630 06:17:11.895165 29777 net.cpp:1851] res3a_branch2a_param_0(0.39) 
I0630 06:17:11.895167 29777 net.cpp:1851] res3a_branch2b_param_0(0.39) 
I0630 06:17:11.895169 29777 net.cpp:1851] res4a_branch2a_param_0(0.39) 
I0630 06:17:11.895171 29777 net.cpp:1851] res4a_branch2b_param_0(0.39) 
I0630 06:17:11.895174 29777 net.cpp:1851] res5a_branch2a_param_0(0.39) 
I0630 06:17:11.895175 29777 net.cpp:1851] res5a_branch2b_param_0(0.39) 
I0630 06:17:11.895177 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (917890/2.86678e+06) 0.32
I0630 06:17:11.895263 29777 solver.cpp:471] Iteration 78000, Testing net (#0)
I0630 06:17:19.875638 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 06:18:10.950081 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.57452
I0630 06:18:10.950206 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.807881
I0630 06:18:10.950215 29777 solver.cpp:544]     Test net output #2: loss = 1.49564 (* 1 = 1.49564 loss)
I0630 06:18:11.127274 29777 solver.cpp:290] Iteration 78000 (1.32992 iter/s, 75.1926s/100 iter), loss = 1.42857
I0630 06:18:11.127297 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 06:18:11.127303 29777 sgd_solver.cpp:106] Iteration 78000, lr = 0.0075625
I0630 06:18:11.128005 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.4
I0630 06:18:11.396749 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 06:18:27.496374 29777 solver.cpp:290] Iteration 78100 (6.10925 iter/s, 16.3686s/100 iter), loss = 1.11905
I0630 06:18:27.496400 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 06:18:27.496409 29777 sgd_solver.cpp:106] Iteration 78100, lr = 0.00755937
I0630 06:18:43.597479 29777 solver.cpp:290] Iteration 78200 (6.21094 iter/s, 16.1006s/100 iter), loss = 1.16667
I0630 06:18:43.597658 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 06:18:43.597669 29777 sgd_solver.cpp:106] Iteration 78200, lr = 0.00755625
I0630 06:18:59.742182 29777 solver.cpp:290] Iteration 78300 (6.19422 iter/s, 16.1441s/100 iter), loss = 0.928571
I0630 06:18:59.742208 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 06:18:59.742216 29777 sgd_solver.cpp:106] Iteration 78300, lr = 0.00755312
I0630 06:19:15.850533 29777 solver.cpp:290] Iteration 78400 (6.20814 iter/s, 16.1079s/100 iter), loss = 1
I0630 06:19:15.850646 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 06:19:15.850654 29777 sgd_solver.cpp:106] Iteration 78400, lr = 0.00755
I0630 06:19:31.915833 29777 solver.cpp:290] Iteration 78500 (6.22481 iter/s, 16.0647s/100 iter), loss = 0.904762
I0630 06:19:31.915856 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 06:19:31.915863 29777 sgd_solver.cpp:106] Iteration 78500, lr = 0.00754687
I0630 06:19:48.128402 29777 solver.cpp:290] Iteration 78600 (6.16823 iter/s, 16.2121s/100 iter), loss = 1.0119
I0630 06:19:48.128473 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 06:19:48.128480 29777 sgd_solver.cpp:106] Iteration 78600, lr = 0.00754375
I0630 06:20:04.300549 29777 solver.cpp:290] Iteration 78700 (6.18367 iter/s, 16.1716s/100 iter), loss = 0.952381
I0630 06:20:04.300572 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 06:20:04.300580 29777 sgd_solver.cpp:106] Iteration 78700, lr = 0.00754063
I0630 06:20:20.447013 29777 solver.cpp:290] Iteration 78800 (6.19349 iter/s, 16.146s/100 iter), loss = 1.10714
I0630 06:20:20.447082 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 06:20:20.447090 29777 sgd_solver.cpp:106] Iteration 78800, lr = 0.0075375
I0630 06:20:36.771998 29777 solver.cpp:290] Iteration 78900 (6.12577 iter/s, 16.3245s/100 iter), loss = 1.08333
I0630 06:20:36.772022 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 06:20:36.772029 29777 sgd_solver.cpp:106] Iteration 78900, lr = 0.00753437
I0630 06:20:52.792124 29777 solver.cpp:354] Sparsity after update:
I0630 06:20:52.812558 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:20:52.812572 29777 net.cpp:1851] conv1a_param_0(0.2) 
I0630 06:20:52.812582 29777 net.cpp:1851] conv1b_param_0(0.4) 
I0630 06:20:52.812585 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:20:52.812589 29777 net.cpp:1851] res2a_branch2a_param_0(0.4) 
I0630 06:20:52.812592 29777 net.cpp:1851] res2a_branch2b_param_0(0.4) 
I0630 06:20:52.812595 29777 net.cpp:1851] res3a_branch2a_param_0(0.4) 
I0630 06:20:52.812599 29777 net.cpp:1851] res3a_branch2b_param_0(0.4) 
I0630 06:20:52.812602 29777 net.cpp:1851] res4a_branch2a_param_0(0.4) 
I0630 06:20:52.812605 29777 net.cpp:1851] res4a_branch2b_param_0(0.4) 
I0630 06:20:52.812608 29777 net.cpp:1851] res5a_branch2a_param_0(0.4) 
I0630 06:20:52.812611 29777 net.cpp:1851] res5a_branch2b_param_0(0.4) 
I0630 06:20:52.812616 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (941426/2.86678e+06) 0.328
I0630 06:20:52.985693 29777 solver.cpp:290] Iteration 79000 (6.1678 iter/s, 16.2132s/100 iter), loss = 1.20238
I0630 06:20:52.985716 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 06:20:52.985724 29777 sgd_solver.cpp:106] Iteration 79000, lr = 0.00753125
I0630 06:21:09.210898 29777 solver.cpp:290] Iteration 79100 (6.16343 iter/s, 16.2247s/100 iter), loss = 1.44048
I0630 06:21:09.210947 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 06:21:09.210961 29777 sgd_solver.cpp:106] Iteration 79100, lr = 0.00752812
I0630 06:21:25.741720 29777 solver.cpp:290] Iteration 79200 (6.04949 iter/s, 16.5303s/100 iter), loss = 1.2381
I0630 06:21:25.741827 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 06:21:25.741837 29777 sgd_solver.cpp:106] Iteration 79200, lr = 0.007525
I0630 06:21:41.811091 29777 solver.cpp:290] Iteration 79300 (6.22323 iter/s, 16.0688s/100 iter), loss = 1.2619
I0630 06:21:41.811113 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 06:21:41.811120 29777 sgd_solver.cpp:106] Iteration 79300, lr = 0.00752187
I0630 06:21:58.590492 29777 solver.cpp:290] Iteration 79400 (5.95986 iter/s, 16.7789s/100 iter), loss = 1.22619
I0630 06:21:58.590593 29777 solver.cpp:309]     Train net output #0: loss = 0.690476 (* 1 = 0.690476 loss)
I0630 06:21:58.590601 29777 sgd_solver.cpp:106] Iteration 79400, lr = 0.00751875
I0630 06:22:14.884452 29777 solver.cpp:290] Iteration 79500 (6.13745 iter/s, 16.2934s/100 iter), loss = 1.22619
I0630 06:22:14.884477 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 06:22:14.884485 29777 sgd_solver.cpp:106] Iteration 79500, lr = 0.00751562
I0630 06:22:31.618468 29777 solver.cpp:290] Iteration 79600 (5.97602 iter/s, 16.7335s/100 iter), loss = 0.964286
I0630 06:22:31.618572 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 06:22:31.618582 29777 sgd_solver.cpp:106] Iteration 79600, lr = 0.0075125
I0630 06:22:47.912135 29777 solver.cpp:290] Iteration 79700 (6.13756 iter/s, 16.2931s/100 iter), loss = 1.17857
I0630 06:22:47.912163 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 06:22:47.912173 29777 sgd_solver.cpp:106] Iteration 79700, lr = 0.00750937
I0630 06:23:04.103423 29777 solver.cpp:290] Iteration 79800 (6.17634 iter/s, 16.1908s/100 iter), loss = 1.07143
I0630 06:23:04.103513 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 06:23:04.103524 29777 sgd_solver.cpp:106] Iteration 79800, lr = 0.00750625
I0630 06:23:20.379906 29777 solver.cpp:290] Iteration 79900 (6.14404 iter/s, 16.2759s/100 iter), loss = 1.14286
I0630 06:23:20.379981 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 06:23:20.380004 29777 sgd_solver.cpp:106] Iteration 79900, lr = 0.00750312
I0630 06:23:36.685129 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_80000.caffemodel
I0630 06:23:36.708729 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_80000.solverstate
I0630 06:23:36.717552 29777 solver.cpp:354] Sparsity after update:
I0630 06:23:36.718611 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:23:36.718621 29777 net.cpp:1851] conv1a_param_0(0.2) 
I0630 06:23:36.718629 29777 net.cpp:1851] conv1b_param_0(0.4) 
I0630 06:23:36.718632 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:23:36.718636 29777 net.cpp:1851] res2a_branch2a_param_0(0.4) 
I0630 06:23:36.718639 29777 net.cpp:1851] res2a_branch2b_param_0(0.4) 
I0630 06:23:36.718641 29777 net.cpp:1851] res3a_branch2a_param_0(0.4) 
I0630 06:23:36.718644 29777 net.cpp:1851] res3a_branch2b_param_0(0.4) 
I0630 06:23:36.718647 29777 net.cpp:1851] res4a_branch2a_param_0(0.4) 
I0630 06:23:36.718648 29777 net.cpp:1851] res4a_branch2b_param_0(0.4) 
I0630 06:23:36.718650 29777 net.cpp:1851] res5a_branch2a_param_0(0.4) 
I0630 06:23:36.718653 29777 net.cpp:1851] res5a_branch2b_param_0(0.4) 
I0630 06:23:36.718655 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (941426/2.86678e+06) 0.328
I0630 06:23:36.718757 29777 solver.cpp:471] Iteration 80000, Testing net (#0)
I0630 06:23:45.855628 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 06:24:43.202711 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.57586
I0630 06:24:43.202793 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.807101
I0630 06:24:43.202802 29777 solver.cpp:544]     Test net output #2: loss = 1.51202 (* 1 = 1.51202 loss)
I0630 06:24:43.397799 29777 solver.cpp:290] Iteration 80000 (1.20459 iter/s, 83.0156s/100 iter), loss = 0.797619
I0630 06:24:43.397857 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 06:24:43.397876 29777 sgd_solver.cpp:106] Iteration 80000, lr = 0.0075
I0630 06:24:43.399686 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.41
I0630 06:24:43.819041 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 06:25:00.232731 29777 solver.cpp:290] Iteration 80100 (5.94021 iter/s, 16.8344s/100 iter), loss = 1.39286
I0630 06:25:00.232756 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 06:25:00.232775 29777 sgd_solver.cpp:106] Iteration 80100, lr = 0.00749687
I0630 06:25:16.261931 29777 solver.cpp:290] Iteration 80200 (6.2388 iter/s, 16.0287s/100 iter), loss = 1.0119
I0630 06:25:16.262024 29777 solver.cpp:309]     Train net output #0: loss = 0.595238 (* 1 = 0.595238 loss)
I0630 06:25:16.262037 29777 sgd_solver.cpp:106] Iteration 80200, lr = 0.00749375
I0630 06:25:32.787369 29777 solver.cpp:290] Iteration 80300 (6.05148 iter/s, 16.5249s/100 iter), loss = 1
I0630 06:25:32.787394 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 06:25:32.787400 29777 sgd_solver.cpp:106] Iteration 80300, lr = 0.00749063
I0630 06:25:49.190680 29777 solver.cpp:290] Iteration 80400 (6.09651 iter/s, 16.4028s/100 iter), loss = 1.13095
I0630 06:25:49.190768 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 06:25:49.190775 29777 sgd_solver.cpp:106] Iteration 80400, lr = 0.0074875
I0630 06:26:05.451460 29777 solver.cpp:290] Iteration 80500 (6.14997 iter/s, 16.2602s/100 iter), loss = 0.892857
I0630 06:26:05.451490 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 06:26:05.451500 29777 sgd_solver.cpp:106] Iteration 80500, lr = 0.00748438
I0630 06:26:21.618427 29777 solver.cpp:290] Iteration 80600 (6.18563 iter/s, 16.1665s/100 iter), loss = 0.916667
I0630 06:26:21.618535 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 06:26:21.618546 29777 sgd_solver.cpp:106] Iteration 80600, lr = 0.00748125
I0630 06:26:37.840133 29777 solver.cpp:290] Iteration 80700 (6.16479 iter/s, 16.2212s/100 iter), loss = 0.988095
I0630 06:26:37.840159 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 06:26:37.840167 29777 sgd_solver.cpp:106] Iteration 80700, lr = 0.00747812
I0630 06:26:53.967936 29777 solver.cpp:290] Iteration 80800 (6.20065 iter/s, 16.1273s/100 iter), loss = 1.2619
I0630 06:26:53.968053 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 06:26:53.968065 29777 sgd_solver.cpp:106] Iteration 80800, lr = 0.007475
I0630 06:27:10.269810 29777 solver.cpp:290] Iteration 80900 (6.13448 iter/s, 16.3013s/100 iter), loss = 1.13095
I0630 06:27:10.269834 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 06:27:10.269840 29777 sgd_solver.cpp:106] Iteration 80900, lr = 0.00747187
I0630 06:27:26.262645 29777 solver.cpp:354] Sparsity after update:
I0630 06:27:26.282907 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:27:26.282946 29777 net.cpp:1851] conv1a_param_0(0.205) 
I0630 06:27:26.282966 29777 net.cpp:1851] conv1b_param_0(0.41) 
I0630 06:27:26.282977 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:27:26.282984 29777 net.cpp:1851] res2a_branch2a_param_0(0.41) 
I0630 06:27:26.282986 29777 net.cpp:1851] res2a_branch2b_param_0(0.41) 
I0630 06:27:26.282990 29777 net.cpp:1851] res3a_branch2a_param_0(0.41) 
I0630 06:27:26.282994 29777 net.cpp:1851] res3a_branch2b_param_0(0.41) 
I0630 06:27:26.283004 29777 net.cpp:1851] res4a_branch2a_param_0(0.41) 
I0630 06:27:26.283008 29777 net.cpp:1851] res4a_branch2b_param_0(0.41) 
I0630 06:27:26.283012 29777 net.cpp:1851] res5a_branch2a_param_0(0.41) 
I0630 06:27:26.283015 29777 net.cpp:1851] res5a_branch2b_param_0(0.41) 
I0630 06:27:26.283020 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (964961/2.86678e+06) 0.337
I0630 06:27:26.439648 29777 solver.cpp:290] Iteration 81000 (6.18453 iter/s, 16.1694s/100 iter), loss = 1.15476
I0630 06:27:26.439674 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 06:27:26.439683 29777 sgd_solver.cpp:106] Iteration 81000, lr = 0.00746875
I0630 06:27:42.710240 29777 solver.cpp:290] Iteration 81100 (6.14624 iter/s, 16.2701s/100 iter), loss = 1.10714
I0630 06:27:42.710333 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 06:27:42.710369 29777 sgd_solver.cpp:106] Iteration 81100, lr = 0.00746562
I0630 06:27:58.980336 29777 solver.cpp:290] Iteration 81200 (6.14645 iter/s, 16.2696s/100 iter), loss = 0.976191
I0630 06:27:58.980449 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 06:27:58.980459 29777 sgd_solver.cpp:106] Iteration 81200, lr = 0.0074625
I0630 06:28:15.337571 29777 solver.cpp:290] Iteration 81300 (6.11371 iter/s, 16.3567s/100 iter), loss = 1.04762
I0630 06:28:15.337597 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 06:28:15.337607 29777 sgd_solver.cpp:106] Iteration 81300, lr = 0.00745937
I0630 06:28:31.798763 29777 solver.cpp:290] Iteration 81400 (6.07507 iter/s, 16.4607s/100 iter), loss = 0.892857
I0630 06:28:31.798815 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 06:28:31.798822 29777 sgd_solver.cpp:106] Iteration 81400, lr = 0.00745625
I0630 06:28:48.212049 29777 solver.cpp:290] Iteration 81500 (6.09282 iter/s, 16.4128s/100 iter), loss = 1.08333
I0630 06:28:48.212138 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 06:28:48.212163 29777 sgd_solver.cpp:106] Iteration 81500, lr = 0.00745312
I0630 06:29:04.433792 29777 solver.cpp:290] Iteration 81600 (6.16477 iter/s, 16.2212s/100 iter), loss = 1.32143
I0630 06:29:04.433861 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 06:29:04.433868 29777 sgd_solver.cpp:106] Iteration 81600, lr = 0.00745
I0630 06:29:20.687135 29777 solver.cpp:290] Iteration 81700 (6.15278 iter/s, 16.2528s/100 iter), loss = 1.14286
I0630 06:29:20.687181 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 06:29:20.687203 29777 sgd_solver.cpp:106] Iteration 81700, lr = 0.00744687
I0630 06:29:36.924715 29777 solver.cpp:290] Iteration 81800 (6.15874 iter/s, 16.2371s/100 iter), loss = 0.761905
I0630 06:29:36.924878 29777 solver.cpp:309]     Train net output #0: loss = 0.547619 (* 1 = 0.547619 loss)
I0630 06:29:36.924904 29777 sgd_solver.cpp:106] Iteration 81800, lr = 0.00744375
I0630 06:29:52.940383 29777 solver.cpp:290] Iteration 81900 (6.24412 iter/s, 16.0151s/100 iter), loss = 0.845238
I0630 06:29:52.940407 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 06:29:52.940414 29777 sgd_solver.cpp:106] Iteration 81900, lr = 0.00744063
I0630 06:30:09.122658 29777 solver.cpp:354] Sparsity after update:
I0630 06:30:09.124270 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:30:09.124279 29777 net.cpp:1851] conv1a_param_0(0.205) 
I0630 06:30:09.124285 29777 net.cpp:1851] conv1b_param_0(0.41) 
I0630 06:30:09.124289 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:30:09.124290 29777 net.cpp:1851] res2a_branch2a_param_0(0.41) 
I0630 06:30:09.124292 29777 net.cpp:1851] res2a_branch2b_param_0(0.41) 
I0630 06:30:09.124294 29777 net.cpp:1851] res3a_branch2a_param_0(0.41) 
I0630 06:30:09.124296 29777 net.cpp:1851] res3a_branch2b_param_0(0.41) 
I0630 06:30:09.124299 29777 net.cpp:1851] res4a_branch2a_param_0(0.41) 
I0630 06:30:09.124300 29777 net.cpp:1851] res4a_branch2b_param_0(0.41) 
I0630 06:30:09.124302 29777 net.cpp:1851] res5a_branch2a_param_0(0.41) 
I0630 06:30:09.124303 29777 net.cpp:1851] res5a_branch2b_param_0(0.41) 
I0630 06:30:09.124306 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (964961/2.86678e+06) 0.337
I0630 06:30:09.124392 29777 solver.cpp:471] Iteration 82000, Testing net (#0)
I0630 06:30:18.320488 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 06:31:11.702306 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.577419
I0630 06:31:11.702396 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.807662
I0630 06:31:11.702405 29777 solver.cpp:544]     Test net output #2: loss = 1.51882 (* 1 = 1.51882 loss)
I0630 06:31:11.939276 29777 solver.cpp:290] Iteration 82000 (1.26588 iter/s, 78.9967s/100 iter), loss = 0.833333
I0630 06:31:11.939321 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 06:31:11.939338 29777 sgd_solver.cpp:106] Iteration 82000, lr = 0.0074375
I0630 06:31:11.941112 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.42
I0630 06:31:12.359869 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 06:31:28.484802 29777 solver.cpp:290] Iteration 82100 (6.04411 iter/s, 16.545s/100 iter), loss = 0.857143
I0630 06:31:28.484830 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 06:31:28.484844 29777 sgd_solver.cpp:106] Iteration 82100, lr = 0.00743438
I0630 06:31:44.604691 29777 solver.cpp:290] Iteration 82200 (6.2037 iter/s, 16.1194s/100 iter), loss = 1.10714
I0630 06:31:44.604794 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 06:31:44.604804 29777 sgd_solver.cpp:106] Iteration 82200, lr = 0.00743125
I0630 06:32:00.788182 29777 solver.cpp:290] Iteration 82300 (6.17935 iter/s, 16.1829s/100 iter), loss = 1.38095
I0630 06:32:00.788205 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 06:32:00.788213 29777 sgd_solver.cpp:106] Iteration 82300, lr = 0.00742813
I0630 06:32:16.913143 29777 solver.cpp:290] Iteration 82400 (6.20175 iter/s, 16.1245s/100 iter), loss = 0.869048
I0630 06:32:16.913239 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 06:32:16.913249 29777 sgd_solver.cpp:106] Iteration 82400, lr = 0.007425
I0630 06:32:33.055394 29777 solver.cpp:290] Iteration 82500 (6.19513 iter/s, 16.1417s/100 iter), loss = 1.11905
I0630 06:32:33.055419 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 06:32:33.055429 29777 sgd_solver.cpp:106] Iteration 82500, lr = 0.00742187
I0630 06:32:49.310662 29777 solver.cpp:290] Iteration 82600 (6.15203 iter/s, 16.2548s/100 iter), loss = 1.04762
I0630 06:32:49.310768 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 06:32:49.310788 29777 sgd_solver.cpp:106] Iteration 82600, lr = 0.00741875
I0630 06:33:05.413328 29777 solver.cpp:290] Iteration 82700 (6.21036 iter/s, 16.1021s/100 iter), loss = 1.19048
I0630 06:33:05.413352 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 06:33:05.413359 29777 sgd_solver.cpp:106] Iteration 82700, lr = 0.00741562
I0630 06:33:21.642601 29777 solver.cpp:290] Iteration 82800 (6.16188 iter/s, 16.2288s/100 iter), loss = 1.0119
I0630 06:33:21.642696 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 06:33:21.642707 29777 sgd_solver.cpp:106] Iteration 82800, lr = 0.0074125
I0630 06:33:37.767966 29777 solver.cpp:290] Iteration 82900 (6.20162 iter/s, 16.1248s/100 iter), loss = 0.97619
I0630 06:33:37.767988 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 06:33:37.767994 29777 sgd_solver.cpp:106] Iteration 82900, lr = 0.00740937
I0630 06:33:53.642482 29777 solver.cpp:354] Sparsity after update:
I0630 06:33:53.662889 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:33:53.662907 29777 net.cpp:1851] conv1a_param_0(0.21) 
I0630 06:33:53.662916 29777 net.cpp:1851] conv1b_param_0(0.42) 
I0630 06:33:53.662919 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:33:53.662924 29777 net.cpp:1851] res2a_branch2a_param_0(0.42) 
I0630 06:33:53.662930 29777 net.cpp:1851] res2a_branch2b_param_0(0.42) 
I0630 06:33:53.662932 29777 net.cpp:1851] res3a_branch2a_param_0(0.42) 
I0630 06:33:53.662935 29777 net.cpp:1851] res3a_branch2b_param_0(0.42) 
I0630 06:33:53.662940 29777 net.cpp:1851] res4a_branch2a_param_0(0.42) 
I0630 06:33:53.662942 29777 net.cpp:1851] res4a_branch2b_param_0(0.42) 
I0630 06:33:53.662945 29777 net.cpp:1851] res5a_branch2a_param_0(0.42) 
I0630 06:33:53.662948 29777 net.cpp:1851] res5a_branch2b_param_0(0.42) 
I0630 06:33:53.662950 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (988493/2.86678e+06) 0.345
I0630 06:33:53.820355 29777 solver.cpp:290] Iteration 83000 (6.22978 iter/s, 16.0519s/100 iter), loss = 1.05952
I0630 06:33:53.820377 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 06:33:53.820384 29777 sgd_solver.cpp:106] Iteration 83000, lr = 0.00740625
I0630 06:34:09.952865 29777 solver.cpp:290] Iteration 83100 (6.19884 iter/s, 16.132s/100 iter), loss = 1.05952
I0630 06:34:09.952888 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 06:34:09.952895 29777 sgd_solver.cpp:106] Iteration 83100, lr = 0.00740312
I0630 06:34:26.148269 29777 solver.cpp:290] Iteration 83200 (6.17477 iter/s, 16.1949s/100 iter), loss = 1.2619
I0630 06:34:26.148325 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 06:34:26.148336 29777 sgd_solver.cpp:106] Iteration 83200, lr = 0.0074
I0630 06:34:42.265838 29777 solver.cpp:290] Iteration 83300 (6.2046 iter/s, 16.1171s/100 iter), loss = 0.702381
I0630 06:34:42.265924 29777 solver.cpp:309]     Train net output #0: loss = 0.571429 (* 1 = 0.571429 loss)
I0630 06:34:42.265965 29777 sgd_solver.cpp:106] Iteration 83300, lr = 0.00739687
I0630 06:34:58.441809 29777 solver.cpp:290] Iteration 83400 (6.18221 iter/s, 16.1754s/100 iter), loss = 0.928571
I0630 06:34:58.441913 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 06:34:58.441923 29777 sgd_solver.cpp:106] Iteration 83400, lr = 0.00739375
I0630 06:35:14.569697 29777 solver.cpp:290] Iteration 83500 (6.20065 iter/s, 16.1273s/100 iter), loss = 0.72619
I0630 06:35:14.569721 29777 solver.cpp:309]     Train net output #0: loss = 0.595238 (* 1 = 0.595238 loss)
I0630 06:35:14.569727 29777 sgd_solver.cpp:106] Iteration 83500, lr = 0.00739062
I0630 06:35:30.684242 29777 solver.cpp:290] Iteration 83600 (6.20576 iter/s, 16.1141s/100 iter), loss = 1.09524
I0630 06:35:30.684356 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 06:35:30.684365 29777 sgd_solver.cpp:106] Iteration 83600, lr = 0.0073875
I0630 06:35:46.742591 29777 solver.cpp:290] Iteration 83700 (6.22751 iter/s, 16.0578s/100 iter), loss = 1.13095
I0630 06:35:46.742617 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 06:35:46.742626 29777 sgd_solver.cpp:106] Iteration 83700, lr = 0.00738438
I0630 06:36:02.858237 29777 solver.cpp:290] Iteration 83800 (6.20533 iter/s, 16.1152s/100 iter), loss = 0.952381
I0630 06:36:02.858330 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 06:36:02.858342 29777 sgd_solver.cpp:106] Iteration 83800, lr = 0.00738125
I0630 06:36:19.189333 29777 solver.cpp:290] Iteration 83900 (6.12349 iter/s, 16.3306s/100 iter), loss = 1.03571
I0630 06:36:19.189359 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 06:36:19.189368 29777 sgd_solver.cpp:106] Iteration 83900, lr = 0.00737813
I0630 06:36:35.244951 29777 solver.cpp:354] Sparsity after update:
I0630 06:36:35.246381 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:36:35.246388 29777 net.cpp:1851] conv1a_param_0(0.21) 
I0630 06:36:35.246397 29777 net.cpp:1851] conv1b_param_0(0.42) 
I0630 06:36:35.246399 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:36:35.246402 29777 net.cpp:1851] res2a_branch2a_param_0(0.42) 
I0630 06:36:35.246405 29777 net.cpp:1851] res2a_branch2b_param_0(0.42) 
I0630 06:36:35.246407 29777 net.cpp:1851] res3a_branch2a_param_0(0.42) 
I0630 06:36:35.246409 29777 net.cpp:1851] res3a_branch2b_param_0(0.42) 
I0630 06:36:35.246412 29777 net.cpp:1851] res4a_branch2a_param_0(0.42) 
I0630 06:36:35.246414 29777 net.cpp:1851] res4a_branch2b_param_0(0.42) 
I0630 06:36:35.246417 29777 net.cpp:1851] res5a_branch2a_param_0(0.42) 
I0630 06:36:35.246419 29777 net.cpp:1851] res5a_branch2b_param_0(0.42) 
I0630 06:36:35.246421 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (988493/2.86678e+06) 0.345
I0630 06:36:35.246508 29777 solver.cpp:471] Iteration 84000, Testing net (#0)
I0630 06:36:44.770190 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 06:37:46.528805 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.572639
I0630 06:37:46.528882 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.806181
I0630 06:37:46.528897 29777 solver.cpp:544]     Test net output #2: loss = 1.52242 (* 1 = 1.52242 loss)
I0630 06:37:46.751307 29777 solver.cpp:290] Iteration 84000 (1.14208 iter/s, 87.5596s/100 iter), loss = 0.904762
I0630 06:37:46.751355 29777 solver.cpp:309]     Train net output #0: loss = 0.690476 (* 1 = 0.690476 loss)
I0630 06:37:46.751389 29777 sgd_solver.cpp:106] Iteration 84000, lr = 0.007375
I0630 06:37:46.752627 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.43
I0630 06:37:47.289585 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 06:38:03.455045 29777 solver.cpp:290] Iteration 84100 (5.98686 iter/s, 16.7032s/100 iter), loss = 1.42857
I0630 06:38:03.455066 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 06:38:03.455073 29777 sgd_solver.cpp:106] Iteration 84100, lr = 0.00737187
I0630 06:38:19.781816 29777 solver.cpp:290] Iteration 84200 (6.12509 iter/s, 16.3263s/100 iter), loss = 0.833333
I0630 06:38:19.781929 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 06:38:19.781939 29777 sgd_solver.cpp:106] Iteration 84200, lr = 0.00736875
I0630 06:38:36.126260 29777 solver.cpp:290] Iteration 84300 (6.11851 iter/s, 16.3439s/100 iter), loss = 0.952381
I0630 06:38:36.126412 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 06:38:36.126479 29777 sgd_solver.cpp:106] Iteration 84300, lr = 0.00736562
I0630 06:38:52.744114 29777 solver.cpp:290] Iteration 84400 (6.01783 iter/s, 16.6173s/100 iter), loss = 0.857143
I0630 06:38:52.744197 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 06:38:52.744205 29777 sgd_solver.cpp:106] Iteration 84400, lr = 0.0073625
I0630 06:39:09.083123 29777 solver.cpp:290] Iteration 84500 (6.12052 iter/s, 16.3385s/100 iter), loss = 0.916667
I0630 06:39:09.083145 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 06:39:09.083153 29777 sgd_solver.cpp:106] Iteration 84500, lr = 0.00735937
I0630 06:39:25.290276 29777 solver.cpp:290] Iteration 84600 (6.1703 iter/s, 16.2067s/100 iter), loss = 1.04762
I0630 06:39:25.290459 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 06:39:25.290501 29777 sgd_solver.cpp:106] Iteration 84600, lr = 0.00735625
I0630 06:39:41.898582 29777 solver.cpp:290] Iteration 84700 (6.02131 iter/s, 16.6077s/100 iter), loss = 1.20238
I0630 06:39:41.898605 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 06:39:41.898612 29777 sgd_solver.cpp:106] Iteration 84700, lr = 0.00735312
I0630 06:39:58.346871 29777 solver.cpp:290] Iteration 84800 (6.07984 iter/s, 16.4478s/100 iter), loss = 0.857143
I0630 06:39:58.346966 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 06:39:58.346976 29777 sgd_solver.cpp:106] Iteration 84800, lr = 0.00735
I0630 06:40:14.547617 29777 solver.cpp:290] Iteration 84900 (6.17276 iter/s, 16.2002s/100 iter), loss = 0.821429
I0630 06:40:14.547642 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 06:40:14.547652 29777 sgd_solver.cpp:106] Iteration 84900, lr = 0.00734688
I0630 06:40:30.719399 29777 solver.cpp:354] Sparsity after update:
I0630 06:40:30.740253 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:40:30.740268 29777 net.cpp:1851] conv1a_param_0(0.215) 
I0630 06:40:30.740276 29777 net.cpp:1851] conv1b_param_0(0.43) 
I0630 06:40:30.740278 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:40:30.740280 29777 net.cpp:1851] res2a_branch2a_param_0(0.43) 
I0630 06:40:30.740283 29777 net.cpp:1851] res2a_branch2b_param_0(0.43) 
I0630 06:40:30.740285 29777 net.cpp:1851] res3a_branch2a_param_0(0.43) 
I0630 06:40:30.740294 29777 net.cpp:1851] res3a_branch2b_param_0(0.43) 
I0630 06:40:30.740296 29777 net.cpp:1851] res4a_branch2a_param_0(0.43) 
I0630 06:40:30.740298 29777 net.cpp:1851] res4a_branch2b_param_0(0.43) 
I0630 06:40:30.740300 29777 net.cpp:1851] res5a_branch2a_param_0(0.43) 
I0630 06:40:30.740303 29777 net.cpp:1851] res5a_branch2b_param_0(0.43) 
I0630 06:40:30.740304 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.01204e+06/2.86678e+06) 0.353
I0630 06:40:30.902043 29777 solver.cpp:290] Iteration 85000 (6.11473 iter/s, 16.354s/100 iter), loss = 1.2619
I0630 06:40:30.902072 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 06:40:30.902081 29777 sgd_solver.cpp:106] Iteration 85000, lr = 0.00734375
I0630 06:40:47.322499 29777 solver.cpp:290] Iteration 85100 (6.09015 iter/s, 16.42s/100 iter), loss = 0.940476
I0630 06:40:47.322546 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 06:40:47.322561 29777 sgd_solver.cpp:106] Iteration 85100, lr = 0.00734062
I0630 06:41:03.647415 29777 solver.cpp:290] Iteration 85200 (6.12579 iter/s, 16.3244s/100 iter), loss = 1.09524
I0630 06:41:03.647521 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 06:41:03.647529 29777 sgd_solver.cpp:106] Iteration 85200, lr = 0.0073375
I0630 06:41:20.045801 29777 solver.cpp:290] Iteration 85300 (6.09837 iter/s, 16.3978s/100 iter), loss = 1.46429
I0630 06:41:20.045825 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 06:41:20.045830 29777 sgd_solver.cpp:106] Iteration 85300, lr = 0.00733438
I0630 06:41:36.182952 29777 solver.cpp:290] Iteration 85400 (6.19706 iter/s, 16.1367s/100 iter), loss = 0.75
I0630 06:41:36.183069 29777 solver.cpp:309]     Train net output #0: loss = 0.547619 (* 1 = 0.547619 loss)
I0630 06:41:36.183079 29777 sgd_solver.cpp:106] Iteration 85400, lr = 0.00733125
I0630 06:41:52.496016 29777 solver.cpp:290] Iteration 85500 (6.13027 iter/s, 16.3125s/100 iter), loss = 0.654762
I0630 06:41:52.496040 29777 solver.cpp:309]     Train net output #0: loss = 0.5 (* 1 = 0.5 loss)
I0630 06:41:52.496047 29777 sgd_solver.cpp:106] Iteration 85500, lr = 0.00732813
I0630 06:42:08.739696 29777 solver.cpp:290] Iteration 85600 (6.15642 iter/s, 16.2432s/100 iter), loss = 1.05952
I0630 06:42:08.739811 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 06:42:08.739821 29777 sgd_solver.cpp:106] Iteration 85600, lr = 0.007325
I0630 06:42:25.089757 29777 solver.cpp:290] Iteration 85700 (6.1164 iter/s, 16.3495s/100 iter), loss = 1.09524
I0630 06:42:25.089781 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 06:42:25.089790 29777 sgd_solver.cpp:106] Iteration 85700, lr = 0.00732188
I0630 06:42:41.371026 29777 solver.cpp:290] Iteration 85800 (6.14221 iter/s, 16.2808s/100 iter), loss = 0.97619
I0630 06:42:41.371085 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 06:42:41.371093 29777 sgd_solver.cpp:106] Iteration 85800, lr = 0.00731875
I0630 06:42:57.687921 29777 solver.cpp:290] Iteration 85900 (6.12881 iter/s, 16.3164s/100 iter), loss = 1.11905
I0630 06:42:57.687968 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 06:42:57.687994 29777 sgd_solver.cpp:106] Iteration 85900, lr = 0.00731562
I0630 06:43:13.752717 29777 solver.cpp:354] Sparsity after update:
I0630 06:43:13.754958 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:43:13.754976 29777 net.cpp:1851] conv1a_param_0(0.215) 
I0630 06:43:13.755000 29777 net.cpp:1851] conv1b_param_0(0.43) 
I0630 06:43:13.755013 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:43:13.755023 29777 net.cpp:1851] res2a_branch2a_param_0(0.43) 
I0630 06:43:13.755036 29777 net.cpp:1851] res2a_branch2b_param_0(0.43) 
I0630 06:43:13.755048 29777 net.cpp:1851] res3a_branch2a_param_0(0.43) 
I0630 06:43:13.755058 29777 net.cpp:1851] res3a_branch2b_param_0(0.43) 
I0630 06:43:13.755066 29777 net.cpp:1851] res4a_branch2a_param_0(0.43) 
I0630 06:43:13.755076 29777 net.cpp:1851] res4a_branch2b_param_0(0.43) 
I0630 06:43:13.755080 29777 net.cpp:1851] res5a_branch2a_param_0(0.43) 
I0630 06:43:13.755090 29777 net.cpp:1851] res5a_branch2b_param_0(0.43) 
I0630 06:43:13.755100 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.01204e+06/2.86678e+06) 0.353
I0630 06:43:13.755388 29777 solver.cpp:471] Iteration 86000, Testing net (#0)
I0630 06:43:24.491957 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 06:44:21.584195 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.57256
I0630 06:44:21.584285 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.806822
I0630 06:44:21.584298 29777 solver.cpp:544]     Test net output #2: loss = 1.50944 (* 1 = 1.50944 loss)
I0630 06:44:21.771813 29777 solver.cpp:290] Iteration 86000 (1.18932 iter/s, 84.0816s/100 iter), loss = 0.940476
I0630 06:44:21.771867 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 06:44:21.771888 29777 sgd_solver.cpp:106] Iteration 86000, lr = 0.0073125
I0630 06:44:21.773881 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.44
I0630 06:44:22.237265 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 06:44:38.528652 29777 solver.cpp:290] Iteration 86100 (5.96789 iter/s, 16.7563s/100 iter), loss = 1.13095
I0630 06:44:38.528676 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 06:44:38.528686 29777 sgd_solver.cpp:106] Iteration 86100, lr = 0.00730937
I0630 06:44:54.913862 29777 solver.cpp:290] Iteration 86200 (6.10324 iter/s, 16.3847s/100 iter), loss = 1.33333
I0630 06:44:54.913946 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 06:44:54.913956 29777 sgd_solver.cpp:106] Iteration 86200, lr = 0.00730625
I0630 06:45:11.054575 29777 solver.cpp:290] Iteration 86300 (6.19572 iter/s, 16.1402s/100 iter), loss = 1.29762
I0630 06:45:11.054601 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 06:45:11.054610 29777 sgd_solver.cpp:106] Iteration 86300, lr = 0.00730312
I0630 06:45:27.311164 29777 solver.cpp:290] Iteration 86400 (6.15153 iter/s, 16.2561s/100 iter), loss = 1.22619
I0630 06:45:27.311247 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 06:45:27.311255 29777 sgd_solver.cpp:106] Iteration 86400, lr = 0.0073
I0630 06:45:44.277745 29777 solver.cpp:290] Iteration 86500 (5.89413 iter/s, 16.966s/100 iter), loss = 0.916667
I0630 06:45:44.277781 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 06:45:44.277792 29777 sgd_solver.cpp:106] Iteration 86500, lr = 0.00729688
I0630 06:46:00.556797 29777 solver.cpp:290] Iteration 86600 (6.14305 iter/s, 16.2786s/100 iter), loss = 0.916667
I0630 06:46:00.556888 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 06:46:00.556900 29777 sgd_solver.cpp:106] Iteration 86600, lr = 0.00729375
I0630 06:46:16.614493 29777 solver.cpp:290] Iteration 86700 (6.22775 iter/s, 16.0572s/100 iter), loss = 0.904762
I0630 06:46:16.614521 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 06:46:16.614529 29777 sgd_solver.cpp:106] Iteration 86700, lr = 0.00729063
I0630 06:46:32.887228 29777 solver.cpp:290] Iteration 86800 (6.14543 iter/s, 16.2723s/100 iter), loss = 1.11905
I0630 06:46:32.887325 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 06:46:32.887429 29777 sgd_solver.cpp:106] Iteration 86800, lr = 0.0072875
I0630 06:46:49.129099 29777 solver.cpp:290] Iteration 86900 (6.15713 iter/s, 16.2413s/100 iter), loss = 1.03571
I0630 06:46:49.129123 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 06:46:49.129130 29777 sgd_solver.cpp:106] Iteration 86900, lr = 0.00728438
I0630 06:47:05.173180 29777 solver.cpp:354] Sparsity after update:
I0630 06:47:05.193804 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:47:05.193819 29777 net.cpp:1851] conv1a_param_0(0.22) 
I0630 06:47:05.193827 29777 net.cpp:1851] conv1b_param_0(0.44) 
I0630 06:47:05.193830 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:47:05.193831 29777 net.cpp:1851] res2a_branch2a_param_0(0.44) 
I0630 06:47:05.193833 29777 net.cpp:1851] res2a_branch2b_param_0(0.44) 
I0630 06:47:05.193835 29777 net.cpp:1851] res3a_branch2a_param_0(0.44) 
I0630 06:47:05.193837 29777 net.cpp:1851] res3a_branch2b_param_0(0.44) 
I0630 06:47:05.193840 29777 net.cpp:1851] res4a_branch2a_param_0(0.44) 
I0630 06:47:05.193841 29777 net.cpp:1851] res4a_branch2b_param_0(0.44) 
I0630 06:47:05.193843 29777 net.cpp:1851] res5a_branch2a_param_0(0.44) 
I0630 06:47:05.193845 29777 net.cpp:1851] res5a_branch2b_param_0(0.44) 
I0630 06:47:05.193847 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.03557e+06/2.86678e+06) 0.361
I0630 06:47:05.351423 29777 solver.cpp:290] Iteration 87000 (6.16452 iter/s, 16.2219s/100 iter), loss = 1.02381
I0630 06:47:05.351449 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 06:47:05.351464 29777 sgd_solver.cpp:106] Iteration 87000, lr = 0.00728125
I0630 06:47:21.491919 29777 solver.cpp:290] Iteration 87100 (6.19578 iter/s, 16.14s/100 iter), loss = 0.809524
I0630 06:47:21.491945 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 06:47:21.491951 29777 sgd_solver.cpp:106] Iteration 87100, lr = 0.00727813
I0630 06:47:37.765035 29777 solver.cpp:290] Iteration 87200 (6.14529 iter/s, 16.2726s/100 iter), loss = 1.21429
I0630 06:47:37.765121 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 06:47:37.765141 29777 sgd_solver.cpp:106] Iteration 87200, lr = 0.007275
I0630 06:47:53.957363 29777 solver.cpp:290] Iteration 87300 (6.17597 iter/s, 16.1918s/100 iter), loss = 1.04762
I0630 06:47:53.957389 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 06:47:53.957398 29777 sgd_solver.cpp:106] Iteration 87300, lr = 0.00727188
I0630 06:48:10.105154 29777 solver.cpp:290] Iteration 87400 (6.19298 iter/s, 16.1473s/100 iter), loss = 1.19048
I0630 06:48:10.105265 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 06:48:10.105278 29777 sgd_solver.cpp:106] Iteration 87400, lr = 0.00726875
I0630 06:48:26.349705 29777 solver.cpp:290] Iteration 87500 (6.15612 iter/s, 16.244s/100 iter), loss = 1.05952
I0630 06:48:26.349730 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 06:48:26.349738 29777 sgd_solver.cpp:106] Iteration 87500, lr = 0.00726563
I0630 06:48:42.493671 29777 solver.cpp:290] Iteration 87600 (6.19444 iter/s, 16.1435s/100 iter), loss = 0.940476
I0630 06:48:42.493746 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 06:48:42.493754 29777 sgd_solver.cpp:106] Iteration 87600, lr = 0.0072625
I0630 06:48:58.754565 29777 solver.cpp:290] Iteration 87700 (6.14992 iter/s, 16.2604s/100 iter), loss = 1.42857
I0630 06:48:58.754596 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 06:48:58.754606 29777 sgd_solver.cpp:106] Iteration 87700, lr = 0.00725937
I0630 06:49:14.980186 29777 solver.cpp:290] Iteration 87800 (6.16328 iter/s, 16.2251s/100 iter), loss = 0.821429
I0630 06:49:14.980290 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 06:49:14.980316 29777 sgd_solver.cpp:106] Iteration 87800, lr = 0.00725625
I0630 06:49:31.195127 29777 solver.cpp:290] Iteration 87900 (6.16736 iter/s, 16.2144s/100 iter), loss = 1.36905
I0630 06:49:31.195152 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 06:49:31.195171 29777 sgd_solver.cpp:106] Iteration 87900, lr = 0.00725312
I0630 06:49:47.215178 29777 solver.cpp:354] Sparsity after update:
I0630 06:49:47.216609 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:49:47.216616 29777 net.cpp:1851] conv1a_param_0(0.22) 
I0630 06:49:47.216624 29777 net.cpp:1851] conv1b_param_0(0.44) 
I0630 06:49:47.216627 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:49:47.216629 29777 net.cpp:1851] res2a_branch2a_param_0(0.44) 
I0630 06:49:47.216631 29777 net.cpp:1851] res2a_branch2b_param_0(0.44) 
I0630 06:49:47.216634 29777 net.cpp:1851] res3a_branch2a_param_0(0.44) 
I0630 06:49:47.216635 29777 net.cpp:1851] res3a_branch2b_param_0(0.44) 
I0630 06:49:47.216639 29777 net.cpp:1851] res4a_branch2a_param_0(0.44) 
I0630 06:49:47.216640 29777 net.cpp:1851] res4a_branch2b_param_0(0.44) 
I0630 06:49:47.216642 29777 net.cpp:1851] res5a_branch2a_param_0(0.44) 
I0630 06:49:47.216645 29777 net.cpp:1851] res5a_branch2b_param_0(0.44) 
I0630 06:49:47.216646 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.03557e+06/2.86678e+06) 0.361
I0630 06:49:47.216733 29777 solver.cpp:471] Iteration 88000, Testing net (#0)
I0630 06:49:55.499549 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 06:50:46.897066 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.57454
I0630 06:50:46.897111 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.807801
I0630 06:50:46.897119 29777 solver.cpp:544]     Test net output #2: loss = 1.5161 (* 1 = 1.5161 loss)
I0630 06:50:47.082335 29777 solver.cpp:290] Iteration 88000 (1.31778 iter/s, 75.8851s/100 iter), loss = 1.20238
I0630 06:50:47.082358 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 06:50:47.082365 29777 sgd_solver.cpp:106] Iteration 88000, lr = 0.00725
I0630 06:50:47.083067 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.45
I0630 06:50:47.385828 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 06:51:03.452255 29777 solver.cpp:290] Iteration 88100 (6.10895 iter/s, 16.3694s/100 iter), loss = 0.928571
I0630 06:51:03.452307 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 06:51:03.452325 29777 sgd_solver.cpp:106] Iteration 88100, lr = 0.00724687
I0630 06:51:19.628193 29777 solver.cpp:290] Iteration 88200 (6.18222 iter/s, 16.1754s/100 iter), loss = 1.47619
I0630 06:51:19.628473 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 06:51:19.628578 29777 sgd_solver.cpp:106] Iteration 88200, lr = 0.00724375
I0630 06:51:35.614503 29777 solver.cpp:290] Iteration 88300 (6.25563 iter/s, 15.9856s/100 iter), loss = 1.05952
I0630 06:51:35.614526 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 06:51:35.614533 29777 sgd_solver.cpp:106] Iteration 88300, lr = 0.00724062
I0630 06:51:51.727732 29777 solver.cpp:290] Iteration 88400 (6.20626 iter/s, 16.1128s/100 iter), loss = 1.25
I0630 06:51:51.727803 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 06:51:51.727811 29777 sgd_solver.cpp:106] Iteration 88400, lr = 0.0072375
I0630 06:52:07.837265 29777 solver.cpp:290] Iteration 88500 (6.2077 iter/s, 16.109s/100 iter), loss = 1.35714
I0630 06:52:07.837290 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 06:52:07.837296 29777 sgd_solver.cpp:106] Iteration 88500, lr = 0.00723437
I0630 06:52:23.904026 29777 solver.cpp:290] Iteration 88600 (6.22421 iter/s, 16.0663s/100 iter), loss = 0.952381
I0630 06:52:23.904469 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 06:52:23.904479 29777 sgd_solver.cpp:106] Iteration 88600, lr = 0.00723125
I0630 06:52:40.040081 29777 solver.cpp:290] Iteration 88700 (6.19764 iter/s, 16.1352s/100 iter), loss = 1.22619
I0630 06:52:40.040104 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 06:52:40.040113 29777 sgd_solver.cpp:106] Iteration 88700, lr = 0.00722813
I0630 06:52:56.157356 29777 solver.cpp:290] Iteration 88800 (6.20471 iter/s, 16.1168s/100 iter), loss = 1.30952
I0630 06:52:56.157449 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 06:52:56.157461 29777 sgd_solver.cpp:106] Iteration 88800, lr = 0.007225
I0630 06:53:12.314633 29777 solver.cpp:290] Iteration 88900 (6.18937 iter/s, 16.1567s/100 iter), loss = 0.904762
I0630 06:53:12.314687 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 06:53:12.314708 29777 sgd_solver.cpp:106] Iteration 88900, lr = 0.00722188
I0630 06:53:28.411833 29777 solver.cpp:354] Sparsity after update:
I0630 06:53:28.452571 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:53:28.452594 29777 net.cpp:1851] conv1a_param_0(0.225) 
I0630 06:53:28.452606 29777 net.cpp:1851] conv1b_param_0(0.45) 
I0630 06:53:28.452610 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:53:28.452613 29777 net.cpp:1851] res2a_branch2a_param_0(0.45) 
I0630 06:53:28.452616 29777 net.cpp:1851] res2a_branch2b_param_0(0.45) 
I0630 06:53:28.452620 29777 net.cpp:1851] res3a_branch2a_param_0(0.45) 
I0630 06:53:28.452622 29777 net.cpp:1851] res3a_branch2b_param_0(0.45) 
I0630 06:53:28.452626 29777 net.cpp:1851] res4a_branch2a_param_0(0.45) 
I0630 06:53:28.452630 29777 net.cpp:1851] res4a_branch2b_param_0(0.45) 
I0630 06:53:28.452632 29777 net.cpp:1851] res5a_branch2a_param_0(0.45) 
I0630 06:53:28.452636 29777 net.cpp:1851] res5a_branch2b_param_0(0.45) 
I0630 06:53:28.452638 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.0591e+06/2.86678e+06) 0.369
I0630 06:53:28.607592 29777 solver.cpp:290] Iteration 89000 (6.13781 iter/s, 16.2924s/100 iter), loss = 0.714286
I0630 06:53:28.607643 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 06:53:28.607661 29777 sgd_solver.cpp:106] Iteration 89000, lr = 0.00721875
I0630 06:53:45.044205 29777 solver.cpp:290] Iteration 89100 (6.08416 iter/s, 16.4361s/100 iter), loss = 0.869048
I0630 06:53:45.044229 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 06:53:45.044246 29777 sgd_solver.cpp:106] Iteration 89100, lr = 0.00721562
I0630 06:54:01.430315 29777 solver.cpp:290] Iteration 89200 (6.10291 iter/s, 16.3856s/100 iter), loss = 0.952381
I0630 06:54:01.430454 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 06:54:01.430479 29777 sgd_solver.cpp:106] Iteration 89200, lr = 0.0072125
I0630 06:54:17.749402 29777 solver.cpp:290] Iteration 89300 (6.12801 iter/s, 16.3185s/100 iter), loss = 0.97619
I0630 06:54:17.749426 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 06:54:17.749433 29777 sgd_solver.cpp:106] Iteration 89300, lr = 0.00720937
I0630 06:54:33.868598 29777 solver.cpp:290] Iteration 89400 (6.20396 iter/s, 16.1187s/100 iter), loss = 1.17857
I0630 06:54:33.868671 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 06:54:33.868680 29777 sgd_solver.cpp:106] Iteration 89400, lr = 0.00720625
I0630 06:54:50.338081 29777 solver.cpp:290] Iteration 89500 (6.07204 iter/s, 16.4689s/100 iter), loss = 1.2619
I0630 06:54:50.338140 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 06:54:50.338181 29777 sgd_solver.cpp:106] Iteration 89500, lr = 0.00720312
I0630 06:55:06.726348 29777 solver.cpp:290] Iteration 89600 (6.10212 iter/s, 16.3878s/100 iter), loss = 1.03571
I0630 06:55:06.726445 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 06:55:06.726455 29777 sgd_solver.cpp:106] Iteration 89600, lr = 0.0072
I0630 06:55:23.066794 29777 solver.cpp:290] Iteration 89700 (6.11999 iter/s, 16.3399s/100 iter), loss = 0.916667
I0630 06:55:23.066825 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 06:55:23.066843 29777 sgd_solver.cpp:106] Iteration 89700, lr = 0.00719687
I0630 06:55:39.161701 29777 solver.cpp:290] Iteration 89800 (6.21333 iter/s, 16.0944s/100 iter), loss = 1.21429
I0630 06:55:39.161811 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 06:55:39.161826 29777 sgd_solver.cpp:106] Iteration 89800, lr = 0.00719375
I0630 06:55:55.692869 29777 solver.cpp:290] Iteration 89900 (6.04939 iter/s, 16.5306s/100 iter), loss = 1.04762
I0630 06:55:55.692896 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 06:55:55.692906 29777 sgd_solver.cpp:106] Iteration 89900, lr = 0.00719062
I0630 06:56:11.979128 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_90000.caffemodel
I0630 06:56:11.999280 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_90000.solverstate
I0630 06:56:12.008086 29777 solver.cpp:354] Sparsity after update:
I0630 06:56:12.009048 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:56:12.009057 29777 net.cpp:1851] conv1a_param_0(0.225) 
I0630 06:56:12.009063 29777 net.cpp:1851] conv1b_param_0(0.45) 
I0630 06:56:12.009065 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:56:12.009068 29777 net.cpp:1851] res2a_branch2a_param_0(0.45) 
I0630 06:56:12.009069 29777 net.cpp:1851] res2a_branch2b_param_0(0.45) 
I0630 06:56:12.009071 29777 net.cpp:1851] res3a_branch2a_param_0(0.45) 
I0630 06:56:12.009073 29777 net.cpp:1851] res3a_branch2b_param_0(0.45) 
I0630 06:56:12.009075 29777 net.cpp:1851] res4a_branch2a_param_0(0.45) 
I0630 06:56:12.009078 29777 net.cpp:1851] res4a_branch2b_param_0(0.45) 
I0630 06:56:12.009079 29777 net.cpp:1851] res5a_branch2a_param_0(0.45) 
I0630 06:56:12.009081 29777 net.cpp:1851] res5a_branch2b_param_0(0.45) 
I0630 06:56:12.009083 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.0591e+06/2.86678e+06) 0.369
I0630 06:56:12.009178 29777 solver.cpp:471] Iteration 90000, Testing net (#0)
I0630 06:56:20.671902 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 06:57:01.753468 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56932
I0630 06:57:01.753597 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.804682
I0630 06:57:01.753607 29777 solver.cpp:544]     Test net output #2: loss = 1.53104 (* 1 = 1.53104 loss)
I0630 06:57:01.941627 29777 solver.cpp:290] Iteration 90000 (1.5095 iter/s, 66.2469s/100 iter), loss = 1.25
I0630 06:57:01.941653 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 06:57:01.941660 29777 sgd_solver.cpp:106] Iteration 90000, lr = 0.0071875
I0630 06:57:01.942380 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.46
I0630 06:57:02.253700 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 06:57:18.227555 29777 solver.cpp:290] Iteration 90100 (6.14045 iter/s, 16.2854s/100 iter), loss = 0.809524
I0630 06:57:18.227581 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 06:57:18.227591 29777 sgd_solver.cpp:106] Iteration 90100, lr = 0.00718437
I0630 06:57:34.280931 29777 solver.cpp:290] Iteration 90200 (6.2294 iter/s, 16.0529s/100 iter), loss = 1.02381
I0630 06:57:34.280988 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 06:57:34.280997 29777 sgd_solver.cpp:106] Iteration 90200, lr = 0.00718125
I0630 06:57:50.340977 29777 solver.cpp:290] Iteration 90300 (6.22683 iter/s, 16.0595s/100 iter), loss = 0.761905
I0630 06:57:50.341081 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 06:57:50.341123 29777 sgd_solver.cpp:106] Iteration 90300, lr = 0.00717813
I0630 06:58:06.433862 29777 solver.cpp:290] Iteration 90400 (6.21414 iter/s, 16.0923s/100 iter), loss = 0.940476
I0630 06:58:06.433954 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 06:58:06.433965 29777 sgd_solver.cpp:106] Iteration 90400, lr = 0.007175
I0630 06:58:22.602619 29777 solver.cpp:290] Iteration 90500 (6.18497 iter/s, 16.1682s/100 iter), loss = 1.10714
I0630 06:58:22.602645 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 06:58:22.602654 29777 sgd_solver.cpp:106] Iteration 90500, lr = 0.00717187
I0630 06:58:38.615814 29777 solver.cpp:290] Iteration 90600 (6.24503 iter/s, 16.0127s/100 iter), loss = 0.845238
I0630 06:58:38.616080 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 06:58:38.616093 29777 sgd_solver.cpp:106] Iteration 90600, lr = 0.00716875
I0630 06:58:54.676383 29777 solver.cpp:290] Iteration 90700 (6.22671 iter/s, 16.0598s/100 iter), loss = 1.04762
I0630 06:58:54.676462 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 06:58:54.676491 29777 sgd_solver.cpp:106] Iteration 90700, lr = 0.00716562
I0630 06:59:10.893788 29777 solver.cpp:290] Iteration 90800 (6.16642 iter/s, 16.2169s/100 iter), loss = 0.880952
I0630 06:59:10.894075 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 06:59:10.894171 29777 sgd_solver.cpp:106] Iteration 90800, lr = 0.0071625
I0630 06:59:27.111131 29777 solver.cpp:290] Iteration 90900 (6.16652 iter/s, 16.2166s/100 iter), loss = 1.22619
I0630 06:59:27.111155 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 06:59:27.111161 29777 sgd_solver.cpp:106] Iteration 90900, lr = 0.00715937
I0630 06:59:43.008638 29777 solver.cpp:354] Sparsity after update:
I0630 06:59:43.029054 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 06:59:43.029090 29777 net.cpp:1851] conv1a_param_0(0.23) 
I0630 06:59:43.029115 29777 net.cpp:1851] conv1b_param_0(0.46) 
I0630 06:59:43.029129 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 06:59:43.029135 29777 net.cpp:1851] res2a_branch2a_param_0(0.46) 
I0630 06:59:43.029142 29777 net.cpp:1851] res2a_branch2b_param_0(0.46) 
I0630 06:59:43.029163 29777 net.cpp:1851] res3a_branch2a_param_0(0.46) 
I0630 06:59:43.029175 29777 net.cpp:1851] res3a_branch2b_param_0(0.46) 
I0630 06:59:43.029186 29777 net.cpp:1851] res4a_branch2a_param_0(0.46) 
I0630 06:59:43.029196 29777 net.cpp:1851] res4a_branch2b_param_0(0.46) 
I0630 06:59:43.029206 29777 net.cpp:1851] res5a_branch2a_param_0(0.46) 
I0630 06:59:43.029217 29777 net.cpp:1851] res5a_branch2b_param_0(0.46) 
I0630 06:59:43.029227 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.08264e+06/2.86678e+06) 0.378
I0630 06:59:43.189227 29777 solver.cpp:290] Iteration 91000 (6.21982 iter/s, 16.0776s/100 iter), loss = 0.940476
I0630 06:59:43.189254 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 06:59:43.189262 29777 sgd_solver.cpp:106] Iteration 91000, lr = 0.00715625
I0630 06:59:59.318253 29777 solver.cpp:290] Iteration 91100 (6.20019 iter/s, 16.1285s/100 iter), loss = 0.940476
I0630 06:59:59.318276 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 06:59:59.318284 29777 sgd_solver.cpp:106] Iteration 91100, lr = 0.00715312
I0630 07:00:15.403050 29777 solver.cpp:290] Iteration 91200 (6.21723 iter/s, 16.0843s/100 iter), loss = 0.797619
I0630 07:00:15.403146 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 07:00:15.403164 29777 sgd_solver.cpp:106] Iteration 91200, lr = 0.00715
I0630 07:00:31.409936 29777 solver.cpp:290] Iteration 91300 (6.24752 iter/s, 16.0063s/100 iter), loss = 0.952381
I0630 07:00:31.409962 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 07:00:31.409971 29777 sgd_solver.cpp:106] Iteration 91300, lr = 0.00714687
I0630 07:00:47.367290 29777 solver.cpp:290] Iteration 91400 (6.26689 iter/s, 15.9569s/100 iter), loss = 1.09524
I0630 07:00:47.367377 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 07:00:47.367388 29777 sgd_solver.cpp:106] Iteration 91400, lr = 0.00714375
I0630 07:01:03.329533 29777 solver.cpp:290] Iteration 91500 (6.26499 iter/s, 15.9617s/100 iter), loss = 1.03571
I0630 07:01:03.329560 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 07:01:03.329568 29777 sgd_solver.cpp:106] Iteration 91500, lr = 0.00714062
I0630 07:01:19.363656 29777 solver.cpp:290] Iteration 91600 (6.23688 iter/s, 16.0337s/100 iter), loss = 0.869048
I0630 07:01:19.363732 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 07:01:19.363739 29777 sgd_solver.cpp:106] Iteration 91600, lr = 0.0071375
I0630 07:01:35.400560 29777 solver.cpp:290] Iteration 91700 (6.23582 iter/s, 16.0364s/100 iter), loss = 1.21429
I0630 07:01:35.400586 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 07:01:35.400593 29777 sgd_solver.cpp:106] Iteration 91700, lr = 0.00713437
I0630 07:01:51.487488 29777 solver.cpp:290] Iteration 91800 (6.21641 iter/s, 16.0865s/100 iter), loss = 0.928571
I0630 07:01:51.487592 29777 solver.cpp:309]     Train net output #0: loss = 0.690476 (* 1 = 0.690476 loss)
I0630 07:01:51.487602 29777 sgd_solver.cpp:106] Iteration 91800, lr = 0.00713125
I0630 07:02:07.508640 29777 solver.cpp:290] Iteration 91900 (6.24196 iter/s, 16.0206s/100 iter), loss = 0.797619
I0630 07:02:07.508663 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 07:02:07.508671 29777 sgd_solver.cpp:106] Iteration 91900, lr = 0.00712813
I0630 07:02:23.442005 29777 solver.cpp:354] Sparsity after update:
I0630 07:02:23.443478 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:02:23.443486 29777 net.cpp:1851] conv1a_param_0(0.23) 
I0630 07:02:23.443495 29777 net.cpp:1851] conv1b_param_0(0.46) 
I0630 07:02:23.443497 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:02:23.443500 29777 net.cpp:1851] res2a_branch2a_param_0(0.46) 
I0630 07:02:23.443502 29777 net.cpp:1851] res2a_branch2b_param_0(0.46) 
I0630 07:02:23.443505 29777 net.cpp:1851] res3a_branch2a_param_0(0.46) 
I0630 07:02:23.443506 29777 net.cpp:1851] res3a_branch2b_param_0(0.46) 
I0630 07:02:23.443508 29777 net.cpp:1851] res4a_branch2a_param_0(0.46) 
I0630 07:02:23.443511 29777 net.cpp:1851] res4a_branch2b_param_0(0.46) 
I0630 07:02:23.443512 29777 net.cpp:1851] res5a_branch2a_param_0(0.46) 
I0630 07:02:23.443514 29777 net.cpp:1851] res5a_branch2b_param_0(0.46) 
I0630 07:02:23.443517 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.08264e+06/2.86678e+06) 0.378
I0630 07:02:23.443604 29777 solver.cpp:471] Iteration 92000, Testing net (#0)
I0630 07:02:31.168041 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 07:03:11.781383 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.571899
I0630 07:03:11.781445 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.806461
I0630 07:03:11.781455 29777 solver.cpp:544]     Test net output #2: loss = 1.5251 (* 1 = 1.5251 loss)
I0630 07:03:11.959621 29777 solver.cpp:290] Iteration 92000 (1.55161 iter/s, 64.4492s/100 iter), loss = 1.20238
I0630 07:03:11.959643 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 07:03:11.959662 29777 sgd_solver.cpp:106] Iteration 92000, lr = 0.007125
I0630 07:03:11.960363 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.47
I0630 07:03:12.263739 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 07:03:28.290139 29777 solver.cpp:290] Iteration 92100 (6.12368 iter/s, 16.33s/100 iter), loss = 1.04762
I0630 07:03:28.290163 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 07:03:28.290169 29777 sgd_solver.cpp:106] Iteration 92100, lr = 0.00712187
I0630 07:03:44.320260 29777 solver.cpp:290] Iteration 92200 (6.23844 iter/s, 16.0296s/100 iter), loss = 1.32143
I0630 07:03:44.320355 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 07:03:44.320366 29777 sgd_solver.cpp:106] Iteration 92200, lr = 0.00711875
I0630 07:04:00.386499 29777 solver.cpp:290] Iteration 92300 (6.22444 iter/s, 16.0657s/100 iter), loss = 1.05952
I0630 07:04:00.386525 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 07:04:00.386534 29777 sgd_solver.cpp:106] Iteration 92300, lr = 0.00711562
I0630 07:04:16.553071 29777 solver.cpp:290] Iteration 92400 (6.18579 iter/s, 16.1661s/100 iter), loss = 1.03571
I0630 07:04:16.553174 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 07:04:16.553184 29777 sgd_solver.cpp:106] Iteration 92400, lr = 0.0071125
I0630 07:04:32.627995 29777 solver.cpp:290] Iteration 92500 (6.22108 iter/s, 16.0744s/100 iter), loss = 1.15476
I0630 07:04:32.628018 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 07:04:32.628024 29777 sgd_solver.cpp:106] Iteration 92500, lr = 0.00710937
I0630 07:04:48.770323 29777 solver.cpp:290] Iteration 92600 (6.19508 iter/s, 16.1419s/100 iter), loss = 1.4881
I0630 07:04:48.770444 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 07:04:48.770459 29777 sgd_solver.cpp:106] Iteration 92600, lr = 0.00710625
I0630 07:05:04.854398 29777 solver.cpp:290] Iteration 92700 (6.21755 iter/s, 16.0835s/100 iter), loss = 1.65476
I0630 07:05:04.854430 29777 solver.cpp:309]     Train net output #0: loss = 1.85714 (* 1 = 1.85714 loss)
I0630 07:05:04.854441 29777 sgd_solver.cpp:106] Iteration 92700, lr = 0.00710312
I0630 07:05:20.835855 29777 solver.cpp:290] Iteration 92800 (6.25744 iter/s, 15.981s/100 iter), loss = 1.25
I0630 07:05:20.835927 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 07:05:20.835933 29777 sgd_solver.cpp:106] Iteration 92800, lr = 0.0071
I0630 07:05:36.821202 29777 solver.cpp:290] Iteration 92900 (6.25593 iter/s, 15.9848s/100 iter), loss = 1
I0630 07:05:36.821229 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 07:05:36.821238 29777 sgd_solver.cpp:106] Iteration 92900, lr = 0.00709687
I0630 07:05:52.757498 29777 solver.cpp:354] Sparsity after update:
I0630 07:05:52.779494 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:05:52.779510 29777 net.cpp:1851] conv1a_param_0(0.235) 
I0630 07:05:52.779517 29777 net.cpp:1851] conv1b_param_0(0.47) 
I0630 07:05:52.779520 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:05:52.779522 29777 net.cpp:1851] res2a_branch2a_param_0(0.47) 
I0630 07:05:52.779525 29777 net.cpp:1851] res2a_branch2b_param_0(0.47) 
I0630 07:05:52.779526 29777 net.cpp:1851] res3a_branch2a_param_0(0.47) 
I0630 07:05:52.779530 29777 net.cpp:1851] res3a_branch2b_param_0(0.47) 
I0630 07:05:52.779531 29777 net.cpp:1851] res4a_branch2a_param_0(0.47) 
I0630 07:05:52.779533 29777 net.cpp:1851] res4a_branch2b_param_0(0.47) 
I0630 07:05:52.779536 29777 net.cpp:1851] res5a_branch2a_param_0(0.47) 
I0630 07:05:52.779539 29777 net.cpp:1851] res5a_branch2b_param_0(0.47) 
I0630 07:05:52.779542 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.10618e+06/2.86678e+06) 0.386
I0630 07:05:52.937866 29777 solver.cpp:290] Iteration 93000 (6.20494 iter/s, 16.1162s/100 iter), loss = 0.904762
I0630 07:05:52.937889 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 07:05:52.937896 29777 sgd_solver.cpp:106] Iteration 93000, lr = 0.00709375
I0630 07:06:09.019920 29777 solver.cpp:290] Iteration 93100 (6.2183 iter/s, 16.0816s/100 iter), loss = 1.03571
I0630 07:06:09.019994 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 07:06:09.020018 29777 sgd_solver.cpp:106] Iteration 93100, lr = 0.00709062
I0630 07:06:25.073945 29777 solver.cpp:290] Iteration 93200 (6.22917 iter/s, 16.0535s/100 iter), loss = 0.916667
I0630 07:06:25.073989 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 07:06:25.073997 29777 sgd_solver.cpp:106] Iteration 93200, lr = 0.0070875
I0630 07:06:41.180604 29777 solver.cpp:290] Iteration 93300 (6.2088 iter/s, 16.1062s/100 iter), loss = 1.09524
I0630 07:06:41.180629 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 07:06:41.180637 29777 sgd_solver.cpp:106] Iteration 93300, lr = 0.00708437
I0630 07:06:57.187494 29777 solver.cpp:290] Iteration 93400 (6.24749 iter/s, 16.0064s/100 iter), loss = 0.785714
I0630 07:06:57.187566 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 07:06:57.187575 29777 sgd_solver.cpp:106] Iteration 93400, lr = 0.00708125
I0630 07:07:13.397532 29777 solver.cpp:290] Iteration 93500 (6.16922 iter/s, 16.2095s/100 iter), loss = 0.785714
I0630 07:07:13.397554 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 07:07:13.397562 29777 sgd_solver.cpp:106] Iteration 93500, lr = 0.00707812
I0630 07:07:29.643139 29777 solver.cpp:290] Iteration 93600 (6.15569 iter/s, 16.2451s/100 iter), loss = 0.738095
I0630 07:07:29.643251 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 07:07:29.643261 29777 sgd_solver.cpp:106] Iteration 93600, lr = 0.007075
I0630 07:07:45.599570 29777 solver.cpp:290] Iteration 93700 (6.26728 iter/s, 15.9559s/100 iter), loss = 1.22619
I0630 07:07:45.599596 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 07:07:45.599612 29777 sgd_solver.cpp:106] Iteration 93700, lr = 0.00707188
I0630 07:08:01.537533 29777 solver.cpp:290] Iteration 93800 (6.27451 iter/s, 15.9375s/100 iter), loss = 1.2381
I0630 07:08:01.537614 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 07:08:01.537623 29777 sgd_solver.cpp:106] Iteration 93800, lr = 0.00706875
I0630 07:08:17.567939 29777 solver.cpp:290] Iteration 93900 (6.23835 iter/s, 16.0299s/100 iter), loss = 0.97619
I0630 07:08:17.567961 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 07:08:17.567967 29777 sgd_solver.cpp:106] Iteration 93900, lr = 0.00706562
I0630 07:08:33.418453 29777 solver.cpp:354] Sparsity after update:
I0630 07:08:33.419890 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:08:33.419898 29777 net.cpp:1851] conv1a_param_0(0.235) 
I0630 07:08:33.419905 29777 net.cpp:1851] conv1b_param_0(0.47) 
I0630 07:08:33.419908 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:08:33.419909 29777 net.cpp:1851] res2a_branch2a_param_0(0.47) 
I0630 07:08:33.419911 29777 net.cpp:1851] res2a_branch2b_param_0(0.47) 
I0630 07:08:33.419914 29777 net.cpp:1851] res3a_branch2a_param_0(0.47) 
I0630 07:08:33.419915 29777 net.cpp:1851] res3a_branch2b_param_0(0.47) 
I0630 07:08:33.419917 29777 net.cpp:1851] res4a_branch2a_param_0(0.47) 
I0630 07:08:33.419919 29777 net.cpp:1851] res4a_branch2b_param_0(0.47) 
I0630 07:08:33.419921 29777 net.cpp:1851] res5a_branch2a_param_0(0.47) 
I0630 07:08:33.419922 29777 net.cpp:1851] res5a_branch2b_param_0(0.47) 
I0630 07:08:33.419925 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.10618e+06/2.86678e+06) 0.386
I0630 07:08:33.420011 29777 solver.cpp:471] Iteration 94000, Testing net (#0)
I0630 07:08:41.349144 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 07:09:22.296231 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5727
I0630 07:09:22.296303 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.806361
I0630 07:09:22.296310 29777 solver.cpp:544]     Test net output #2: loss = 1.51912 (* 1 = 1.51912 loss)
I0630 07:09:22.469818 29777 solver.cpp:290] Iteration 94000 (1.54083 iter/s, 64.9001s/100 iter), loss = 0.77381
I0630 07:09:22.469841 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 07:09:22.469863 29777 sgd_solver.cpp:106] Iteration 94000, lr = 0.0070625
I0630 07:09:22.470576 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.48
I0630 07:09:22.780665 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 07:09:38.133929 29777 solver.cpp:290] Iteration 94100 (6.38421 iter/s, 15.6637s/100 iter), loss = 1.27381
I0630 07:09:38.133954 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 07:09:38.133960 29777 sgd_solver.cpp:106] Iteration 94100, lr = 0.00705937
I0630 07:09:54.005964 29777 solver.cpp:290] Iteration 94200 (6.30057 iter/s, 15.8716s/100 iter), loss = 1.30952
I0630 07:09:54.006053 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 07:09:54.006060 29777 sgd_solver.cpp:106] Iteration 94200, lr = 0.00705625
I0630 07:10:09.956017 29777 solver.cpp:290] Iteration 94300 (6.26978 iter/s, 15.9495s/100 iter), loss = 0.940476
I0630 07:10:09.956043 29777 solver.cpp:309]     Train net output #0: loss = 0.5 (* 1 = 0.5 loss)
I0630 07:10:09.956053 29777 sgd_solver.cpp:106] Iteration 94300, lr = 0.00705312
I0630 07:10:26.047200 29777 solver.cpp:290] Iteration 94400 (6.21477 iter/s, 16.0907s/100 iter), loss = 1.05952
I0630 07:10:26.047291 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 07:10:26.047302 29777 sgd_solver.cpp:106] Iteration 94400, lr = 0.00705
I0630 07:10:42.099599 29777 solver.cpp:290] Iteration 94500 (6.22981 iter/s, 16.0519s/100 iter), loss = 1.20238
I0630 07:10:42.099623 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 07:10:42.099632 29777 sgd_solver.cpp:106] Iteration 94500, lr = 0.00704687
I0630 07:10:58.309980 29777 solver.cpp:290] Iteration 94600 (6.16908 iter/s, 16.2099s/100 iter), loss = 0.904762
I0630 07:10:58.312425 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 07:10:58.312608 29777 sgd_solver.cpp:106] Iteration 94600, lr = 0.00704375
I0630 07:11:14.906527 29777 solver.cpp:290] Iteration 94700 (6.02639 iter/s, 16.5937s/100 iter), loss = 1
I0630 07:11:14.906551 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 07:11:14.906560 29777 sgd_solver.cpp:106] Iteration 94700, lr = 0.00704062
I0630 07:11:31.201421 29777 solver.cpp:290] Iteration 94800 (6.13707 iter/s, 16.2944s/100 iter), loss = 0.904762
I0630 07:11:31.201478 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 07:11:31.201488 29777 sgd_solver.cpp:106] Iteration 94800, lr = 0.0070375
I0630 07:11:47.553442 29777 solver.cpp:290] Iteration 94900 (6.11564 iter/s, 16.3515s/100 iter), loss = 1
I0630 07:11:47.553468 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 07:11:47.553478 29777 sgd_solver.cpp:106] Iteration 94900, lr = 0.00703437
I0630 07:12:03.433233 29777 solver.cpp:354] Sparsity after update:
I0630 07:12:03.459612 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:12:03.459812 29777 net.cpp:1851] conv1a_param_0(0.24) 
I0630 07:12:03.459954 29777 net.cpp:1851] conv1b_param_0(0.48) 
I0630 07:12:03.460062 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:12:03.460162 29777 net.cpp:1851] res2a_branch2a_param_0(0.48) 
I0630 07:12:03.460263 29777 net.cpp:1851] res2a_branch2b_param_0(0.48) 
I0630 07:12:03.460372 29777 net.cpp:1851] res3a_branch2a_param_0(0.48) 
I0630 07:12:03.460477 29777 net.cpp:1851] res3a_branch2b_param_0(0.48) 
I0630 07:12:03.460582 29777 net.cpp:1851] res4a_branch2a_param_0(0.48) 
I0630 07:12:03.460690 29777 net.cpp:1851] res4a_branch2b_param_0(0.48) 
I0630 07:12:03.460718 29777 net.cpp:1851] res5a_branch2a_param_0(0.48) 
I0630 07:12:03.460739 29777 net.cpp:1851] res5a_branch2b_param_0(0.48) 
I0630 07:12:03.460747 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.12971e+06/2.86678e+06) 0.394
I0630 07:12:03.638916 29777 solver.cpp:290] Iteration 95000 (6.21697 iter/s, 16.085s/100 iter), loss = 0.916667
I0630 07:12:03.638942 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 07:12:03.638948 29777 sgd_solver.cpp:106] Iteration 95000, lr = 0.00703125
I0630 07:12:19.943042 29777 solver.cpp:290] Iteration 95100 (6.1336 iter/s, 16.3036s/100 iter), loss = 1.15476
I0630 07:12:19.943066 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 07:12:19.943073 29777 sgd_solver.cpp:106] Iteration 95100, lr = 0.00702812
I0630 07:12:36.273773 29777 solver.cpp:290] Iteration 95200 (6.1236 iter/s, 16.3303s/100 iter), loss = 1.25
I0630 07:12:36.273844 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 07:12:36.273851 29777 sgd_solver.cpp:106] Iteration 95200, lr = 0.007025
I0630 07:12:52.599480 29777 solver.cpp:290] Iteration 95300 (6.12551 iter/s, 16.3252s/100 iter), loss = 1.29762
I0630 07:12:52.599506 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 07:12:52.599515 29777 sgd_solver.cpp:106] Iteration 95300, lr = 0.00702188
I0630 07:13:08.887076 29777 solver.cpp:290] Iteration 95400 (6.13982 iter/s, 16.2871s/100 iter), loss = 1.14286
I0630 07:13:08.887156 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 07:13:08.887163 29777 sgd_solver.cpp:106] Iteration 95400, lr = 0.00701875
I0630 07:13:25.143143 29777 solver.cpp:290] Iteration 95500 (6.15175 iter/s, 16.2555s/100 iter), loss = 1
I0630 07:13:25.143168 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 07:13:25.143177 29777 sgd_solver.cpp:106] Iteration 95500, lr = 0.00701563
I0630 07:13:41.397339 29777 solver.cpp:290] Iteration 95600 (6.15244 iter/s, 16.2537s/100 iter), loss = 1.28571
I0630 07:13:41.397450 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 07:13:41.397461 29777 sgd_solver.cpp:106] Iteration 95600, lr = 0.0070125
I0630 07:13:57.454426 29777 solver.cpp:290] Iteration 95700 (6.22799 iter/s, 16.0565s/100 iter), loss = 0.988095
I0630 07:13:57.454461 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 07:13:57.454470 29777 sgd_solver.cpp:106] Iteration 95700, lr = 0.00700937
I0630 07:14:13.589856 29777 solver.cpp:290] Iteration 95800 (6.19773 iter/s, 16.1349s/100 iter), loss = 1.02381
I0630 07:14:13.589931 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 07:14:13.589942 29777 sgd_solver.cpp:106] Iteration 95800, lr = 0.00700625
I0630 07:14:29.892318 29777 solver.cpp:290] Iteration 95900 (6.13424 iter/s, 16.3019s/100 iter), loss = 0.845238
I0630 07:14:29.892349 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 07:14:29.892359 29777 sgd_solver.cpp:106] Iteration 95900, lr = 0.00700312
I0630 07:14:46.035542 29777 solver.cpp:354] Sparsity after update:
I0630 07:14:46.037170 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:14:46.037178 29777 net.cpp:1851] conv1a_param_0(0.24) 
I0630 07:14:46.037185 29777 net.cpp:1851] conv1b_param_0(0.48) 
I0630 07:14:46.037189 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:14:46.037190 29777 net.cpp:1851] res2a_branch2a_param_0(0.48) 
I0630 07:14:46.037192 29777 net.cpp:1851] res2a_branch2b_param_0(0.48) 
I0630 07:14:46.037194 29777 net.cpp:1851] res3a_branch2a_param_0(0.48) 
I0630 07:14:46.037196 29777 net.cpp:1851] res3a_branch2b_param_0(0.48) 
I0630 07:14:46.037199 29777 net.cpp:1851] res4a_branch2a_param_0(0.48) 
I0630 07:14:46.037200 29777 net.cpp:1851] res4a_branch2b_param_0(0.48) 
I0630 07:14:46.037202 29777 net.cpp:1851] res5a_branch2a_param_0(0.48) 
I0630 07:14:46.037204 29777 net.cpp:1851] res5a_branch2b_param_0(0.48) 
I0630 07:14:46.037206 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.12971e+06/2.86678e+06) 0.394
I0630 07:14:46.037302 29777 solver.cpp:471] Iteration 96000, Testing net (#0)
I0630 07:14:57.547770 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 07:15:51.573626 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5696
I0630 07:15:51.573704 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.801901
I0630 07:15:51.573711 29777 solver.cpp:544]     Test net output #2: loss = 1.53412 (* 1 = 1.53412 loss)
I0630 07:15:51.753326 29777 solver.cpp:290] Iteration 96000 (1.22162 iter/s, 81.8587s/100 iter), loss = 1.19048
I0630 07:15:51.753350 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 07:15:51.753355 29777 sgd_solver.cpp:106] Iteration 96000, lr = 0.007
I0630 07:15:51.754076 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.49
I0630 07:15:52.084887 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 07:16:07.855725 29777 solver.cpp:290] Iteration 96100 (6.21044 iter/s, 16.1019s/100 iter), loss = 0.940476
I0630 07:16:07.855748 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 07:16:07.855756 29777 sgd_solver.cpp:106] Iteration 96100, lr = 0.00699687
I0630 07:16:24.268051 29777 solver.cpp:290] Iteration 96200 (6.09316 iter/s, 16.4118s/100 iter), loss = 1.21429
I0630 07:16:24.268155 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 07:16:24.268170 29777 sgd_solver.cpp:106] Iteration 96200, lr = 0.00699375
I0630 07:16:40.295428 29777 solver.cpp:290] Iteration 96300 (6.23954 iter/s, 16.0268s/100 iter), loss = 1.25
I0630 07:16:40.295449 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 07:16:40.295456 29777 sgd_solver.cpp:106] Iteration 96300, lr = 0.00699062
I0630 07:16:56.520004 29777 solver.cpp:290] Iteration 96400 (6.16367 iter/s, 16.2241s/100 iter), loss = 0.97619
I0630 07:16:56.520097 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 07:16:56.520107 29777 sgd_solver.cpp:106] Iteration 96400, lr = 0.0069875
I0630 07:17:13.086413 29777 solver.cpp:290] Iteration 96500 (6.03651 iter/s, 16.5659s/100 iter), loss = 0.928571
I0630 07:17:13.086447 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 07:17:13.086458 29777 sgd_solver.cpp:106] Iteration 96500, lr = 0.00698437
I0630 07:17:29.260211 29777 solver.cpp:290] Iteration 96600 (6.18302 iter/s, 16.1733s/100 iter), loss = 1.07143
I0630 07:17:29.260267 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 07:17:29.260277 29777 sgd_solver.cpp:106] Iteration 96600, lr = 0.00698125
I0630 07:17:45.819687 29777 solver.cpp:290] Iteration 96700 (6.03903 iter/s, 16.559s/100 iter), loss = 1.21429
I0630 07:17:45.819743 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 07:17:45.819763 29777 sgd_solver.cpp:106] Iteration 96700, lr = 0.00697812
I0630 07:18:02.080148 29777 solver.cpp:290] Iteration 96800 (6.15008 iter/s, 16.26s/100 iter), loss = 0.869048
I0630 07:18:02.080255 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 07:18:02.080266 29777 sgd_solver.cpp:106] Iteration 96800, lr = 0.006975
I0630 07:18:18.306888 29777 solver.cpp:290] Iteration 96900 (6.16288 iter/s, 16.2262s/100 iter), loss = 1.10714
I0630 07:18:18.306917 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 07:18:18.306927 29777 sgd_solver.cpp:106] Iteration 96900, lr = 0.00697188
I0630 07:18:34.318522 29777 solver.cpp:354] Sparsity after update:
I0630 07:18:34.338989 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:18:34.339005 29777 net.cpp:1851] conv1a_param_0(0.245) 
I0630 07:18:34.339016 29777 net.cpp:1851] conv1b_param_0(0.49) 
I0630 07:18:34.339020 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:18:34.339023 29777 net.cpp:1851] res2a_branch2a_param_0(0.49) 
I0630 07:18:34.339028 29777 net.cpp:1851] res2a_branch2b_param_0(0.49) 
I0630 07:18:34.339032 29777 net.cpp:1851] res3a_branch2a_param_0(0.49) 
I0630 07:18:34.339035 29777 net.cpp:1851] res3a_branch2b_param_0(0.49) 
I0630 07:18:34.339038 29777 net.cpp:1851] res4a_branch2a_param_0(0.49) 
I0630 07:18:34.339041 29777 net.cpp:1851] res4a_branch2b_param_0(0.49) 
I0630 07:18:34.339045 29777 net.cpp:1851] res5a_branch2a_param_0(0.49) 
I0630 07:18:34.339047 29777 net.cpp:1851] res5a_branch2b_param_0(0.49) 
I0630 07:18:34.339051 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.15324e+06/2.86678e+06) 0.402
I0630 07:18:34.497432 29777 solver.cpp:290] Iteration 97000 (6.17663 iter/s, 16.1901s/100 iter), loss = 1.4881
I0630 07:18:34.497459 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 07:18:34.497467 29777 sgd_solver.cpp:106] Iteration 97000, lr = 0.00696875
I0630 07:18:50.759623 29777 solver.cpp:290] Iteration 97100 (6.14941 iter/s, 16.2617s/100 iter), loss = 0.97619
I0630 07:18:50.759645 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 07:18:50.759652 29777 sgd_solver.cpp:106] Iteration 97100, lr = 0.00696563
I0630 07:19:06.828428 29777 solver.cpp:290] Iteration 97200 (6.22342 iter/s, 16.0683s/100 iter), loss = 1.30952
I0630 07:19:06.828558 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 07:19:06.828583 29777 sgd_solver.cpp:106] Iteration 97200, lr = 0.0069625
I0630 07:19:22.927889 29777 solver.cpp:290] Iteration 97300 (6.21161 iter/s, 16.0989s/100 iter), loss = 1.03571
I0630 07:19:22.927917 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 07:19:22.927927 29777 sgd_solver.cpp:106] Iteration 97300, lr = 0.00695937
I0630 07:19:39.138386 29777 solver.cpp:290] Iteration 97400 (6.16902 iter/s, 16.21s/100 iter), loss = 0.821429
I0630 07:19:39.138476 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 07:19:39.138487 29777 sgd_solver.cpp:106] Iteration 97400, lr = 0.00695625
I0630 07:19:55.381108 29777 solver.cpp:290] Iteration 97500 (6.15681 iter/s, 16.2422s/100 iter), loss = 1
I0630 07:19:55.381136 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 07:19:55.381146 29777 sgd_solver.cpp:106] Iteration 97500, lr = 0.00695312
I0630 07:20:11.444953 29777 solver.cpp:290] Iteration 97600 (6.22534 iter/s, 16.0634s/100 iter), loss = 0.940476
I0630 07:20:11.445060 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 07:20:11.445073 29777 sgd_solver.cpp:106] Iteration 97600, lr = 0.00695
I0630 07:20:27.710088 29777 solver.cpp:290] Iteration 97700 (6.14833 iter/s, 16.2646s/100 iter), loss = 1.5
I0630 07:20:27.710115 29777 solver.cpp:309]     Train net output #0: loss = 1.97619 (* 1 = 1.97619 loss)
I0630 07:20:27.710125 29777 sgd_solver.cpp:106] Iteration 97700, lr = 0.00694687
I0630 07:20:43.902027 29777 solver.cpp:290] Iteration 97800 (6.1761 iter/s, 16.1915s/100 iter), loss = 1.22619
I0630 07:20:43.902165 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 07:20:43.902182 29777 sgd_solver.cpp:106] Iteration 97800, lr = 0.00694375
I0630 07:21:00.107983 29777 solver.cpp:290] Iteration 97900 (6.17079 iter/s, 16.2054s/100 iter), loss = 1.25
I0630 07:21:00.108044 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 07:21:00.108065 29777 sgd_solver.cpp:106] Iteration 97900, lr = 0.00694062
I0630 07:21:15.956831 29777 solver.cpp:354] Sparsity after update:
I0630 07:21:15.960362 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:21:15.960376 29777 net.cpp:1851] conv1a_param_0(0.245) 
I0630 07:21:15.960387 29777 net.cpp:1851] conv1b_param_0(0.49) 
I0630 07:21:15.960392 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:21:15.960397 29777 net.cpp:1851] res2a_branch2a_param_0(0.49) 
I0630 07:21:15.960400 29777 net.cpp:1851] res2a_branch2b_param_0(0.49) 
I0630 07:21:15.960404 29777 net.cpp:1851] res3a_branch2a_param_0(0.49) 
I0630 07:21:15.960410 29777 net.cpp:1851] res3a_branch2b_param_0(0.49) 
I0630 07:21:15.960417 29777 net.cpp:1851] res4a_branch2a_param_0(0.49) 
I0630 07:21:15.960422 29777 net.cpp:1851] res4a_branch2b_param_0(0.49) 
I0630 07:21:15.960427 29777 net.cpp:1851] res5a_branch2a_param_0(0.49) 
I0630 07:21:15.960433 29777 net.cpp:1851] res5a_branch2b_param_0(0.49) 
I0630 07:21:15.960438 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.15324e+06/2.86678e+06) 0.402
I0630 07:21:15.960608 29777 solver.cpp:471] Iteration 98000, Testing net (#0)
I0630 07:21:25.056980 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 07:22:18.236917 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5747
I0630 07:22:18.236977 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.806081
I0630 07:22:18.236987 29777 solver.cpp:544]     Test net output #2: loss = 1.51332 (* 1 = 1.51332 loss)
I0630 07:22:18.457389 29777 solver.cpp:290] Iteration 98000 (1.27637 iter/s, 78.3472s/100 iter), loss = 1.04762
I0630 07:22:18.457415 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 07:22:18.457423 29777 sgd_solver.cpp:106] Iteration 98000, lr = 0.0069375
I0630 07:22:18.458171 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.5
I0630 07:22:18.796192 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 07:22:34.223121 29777 solver.cpp:290] Iteration 98100 (6.34306 iter/s, 15.7653s/100 iter), loss = 1.45238
I0630 07:22:34.223147 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 07:22:34.223156 29777 sgd_solver.cpp:106] Iteration 98100, lr = 0.00693437
I0630 07:22:50.304414 29777 solver.cpp:290] Iteration 98200 (6.21859 iter/s, 16.0808s/100 iter), loss = 0.845238
I0630 07:22:50.304515 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 07:22:50.304527 29777 sgd_solver.cpp:106] Iteration 98200, lr = 0.00693125
I0630 07:23:06.415215 29777 solver.cpp:290] Iteration 98300 (6.20723 iter/s, 16.1103s/100 iter), loss = 1.08333
I0630 07:23:06.415237 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 07:23:06.415244 29777 sgd_solver.cpp:106] Iteration 98300, lr = 0.00692812
I0630 07:23:22.468160 29777 solver.cpp:290] Iteration 98400 (6.22957 iter/s, 16.0525s/100 iter), loss = 1.36905
I0630 07:23:22.468264 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 07:23:22.468273 29777 sgd_solver.cpp:106] Iteration 98400, lr = 0.006925
I0630 07:23:38.567101 29777 solver.cpp:290] Iteration 98500 (6.2118 iter/s, 16.0984s/100 iter), loss = 1.10714
I0630 07:23:38.567128 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 07:23:38.567137 29777 sgd_solver.cpp:106] Iteration 98500, lr = 0.00692187
I0630 07:23:54.707849 29777 solver.cpp:290] Iteration 98600 (6.19568 iter/s, 16.1403s/100 iter), loss = 0.714286
I0630 07:23:54.707979 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 07:23:54.707995 29777 sgd_solver.cpp:106] Iteration 98600, lr = 0.00691875
I0630 07:24:10.934453 29777 solver.cpp:290] Iteration 98700 (6.16294 iter/s, 16.226s/100 iter), loss = 1.11905
I0630 07:24:10.934478 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 07:24:10.934486 29777 sgd_solver.cpp:106] Iteration 98700, lr = 0.00691563
I0630 07:24:27.420579 29777 solver.cpp:290] Iteration 98800 (6.06588 iter/s, 16.4856s/100 iter), loss = 1.45238
I0630 07:24:27.420686 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 07:24:27.420696 29777 sgd_solver.cpp:106] Iteration 98800, lr = 0.0069125
I0630 07:24:43.760769 29777 solver.cpp:290] Iteration 98900 (6.12009 iter/s, 16.3396s/100 iter), loss = 1.10714
I0630 07:24:43.760794 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 07:24:43.760804 29777 sgd_solver.cpp:106] Iteration 98900, lr = 0.00690938
I0630 07:24:59.855713 29777 solver.cpp:354] Sparsity after update:
I0630 07:24:59.876938 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:24:59.876974 29777 net.cpp:1851] conv1a_param_0(0.25) 
I0630 07:24:59.876997 29777 net.cpp:1851] conv1b_param_0(0.5) 
I0630 07:24:59.877007 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:24:59.877017 29777 net.cpp:1851] res2a_branch2a_param_0(0.5) 
I0630 07:24:59.877025 29777 net.cpp:1851] res2a_branch2b_param_0(0.5) 
I0630 07:24:59.877037 29777 net.cpp:1851] res3a_branch2a_param_0(0.5) 
I0630 07:24:59.877044 29777 net.cpp:1851] res3a_branch2b_param_0(0.5) 
I0630 07:24:59.877053 29777 net.cpp:1851] res4a_branch2a_param_0(0.5) 
I0630 07:24:59.877061 29777 net.cpp:1851] res4a_branch2b_param_0(0.5) 
I0630 07:24:59.877069 29777 net.cpp:1851] res5a_branch2a_param_0(0.5) 
I0630 07:24:59.877079 29777 net.cpp:1851] res5a_branch2b_param_0(0.5) 
I0630 07:24:59.877086 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.17678e+06/2.86678e+06) 0.41
I0630 07:25:00.037503 29777 solver.cpp:290] Iteration 99000 (6.14392 iter/s, 16.2763s/100 iter), loss = 0.916667
I0630 07:25:00.037528 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 07:25:00.037537 29777 sgd_solver.cpp:106] Iteration 99000, lr = 0.00690625
I0630 07:25:16.300492 29777 solver.cpp:290] Iteration 99100 (6.14911 iter/s, 16.2625s/100 iter), loss = 1.04762
I0630 07:25:16.300513 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 07:25:16.300520 29777 sgd_solver.cpp:106] Iteration 99100, lr = 0.00690312
I0630 07:25:32.770664 29777 solver.cpp:290] Iteration 99200 (6.07176 iter/s, 16.4697s/100 iter), loss = 1.04762
I0630 07:25:32.770773 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 07:25:32.770782 29777 sgd_solver.cpp:106] Iteration 99200, lr = 0.0069
I0630 07:25:49.023319 29777 solver.cpp:290] Iteration 99300 (6.15305 iter/s, 16.2521s/100 iter), loss = 0.880952
I0630 07:25:49.023347 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 07:25:49.023356 29777 sgd_solver.cpp:106] Iteration 99300, lr = 0.00689687
I0630 07:26:05.158578 29777 solver.cpp:290] Iteration 99400 (6.19779 iter/s, 16.1348s/100 iter), loss = 1.10714
I0630 07:26:05.158640 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 07:26:05.158648 29777 sgd_solver.cpp:106] Iteration 99400, lr = 0.00689375
I0630 07:26:21.609359 29777 solver.cpp:290] Iteration 99500 (6.07893 iter/s, 16.4503s/100 iter), loss = 0.857143
I0630 07:26:21.609382 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 07:26:21.609401 29777 sgd_solver.cpp:106] Iteration 99500, lr = 0.00689062
I0630 07:26:37.972930 29777 solver.cpp:290] Iteration 99600 (6.11131 iter/s, 16.3631s/100 iter), loss = 0.833333
I0630 07:26:37.973026 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 07:26:37.973033 29777 sgd_solver.cpp:106] Iteration 99600, lr = 0.0068875
I0630 07:26:54.382437 29777 solver.cpp:290] Iteration 99700 (6.09423 iter/s, 16.409s/100 iter), loss = 1.02381
I0630 07:26:54.382470 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 07:26:54.382478 29777 sgd_solver.cpp:106] Iteration 99700, lr = 0.00688437
I0630 07:27:10.666761 29777 solver.cpp:290] Iteration 99800 (6.14106 iter/s, 16.2838s/100 iter), loss = 0.702381
I0630 07:27:10.666808 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 07:27:10.666816 29777 sgd_solver.cpp:106] Iteration 99800, lr = 0.00688125
I0630 07:27:27.015818 29777 solver.cpp:290] Iteration 99900 (6.11675 iter/s, 16.3486s/100 iter), loss = 1.2619
I0630 07:27:27.015844 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 07:27:27.015852 29777 sgd_solver.cpp:106] Iteration 99900, lr = 0.00687813
I0630 07:27:43.451939 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_100000.caffemodel
I0630 07:27:43.477591 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_100000.solverstate
I0630 07:27:43.486457 29777 solver.cpp:354] Sparsity after update:
I0630 07:27:43.487548 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:27:43.487558 29777 net.cpp:1851] conv1a_param_0(0.25) 
I0630 07:27:43.487567 29777 net.cpp:1851] conv1b_param_0(0.5) 
I0630 07:27:43.487570 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:27:43.487571 29777 net.cpp:1851] res2a_branch2a_param_0(0.5) 
I0630 07:27:43.487573 29777 net.cpp:1851] res2a_branch2b_param_0(0.5) 
I0630 07:27:43.487576 29777 net.cpp:1851] res3a_branch2a_param_0(0.5) 
I0630 07:27:43.487577 29777 net.cpp:1851] res3a_branch2b_param_0(0.5) 
I0630 07:27:43.487579 29777 net.cpp:1851] res4a_branch2a_param_0(0.5) 
I0630 07:27:43.487581 29777 net.cpp:1851] res4a_branch2b_param_0(0.5) 
I0630 07:27:43.487583 29777 net.cpp:1851] res5a_branch2a_param_0(0.5) 
I0630 07:27:43.487586 29777 net.cpp:1851] res5a_branch2b_param_0(0.5) 
I0630 07:27:43.487587 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.17678e+06/2.86678e+06) 0.41
I0630 07:27:43.487690 29777 solver.cpp:471] Iteration 100000, Testing net (#0)
I0630 07:27:54.503427 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 07:28:48.337787 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5721
I0630 07:28:48.337893 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.804821
I0630 07:28:48.337903 29777 solver.cpp:544]     Test net output #2: loss = 1.509 (* 1 = 1.509 loss)
I0630 07:28:48.582720 29777 solver.cpp:290] Iteration 100000 (1.22602 iter/s, 81.5647s/100 iter), loss = 1.04762
I0630 07:28:48.582770 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 07:28:48.582784 29777 sgd_solver.cpp:106] Iteration 100000, lr = 0.006875
I0630 07:28:48.584121 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.51
I0630 07:28:49.276468 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 07:29:05.059249 29777 solver.cpp:290] Iteration 100100 (6.06943 iter/s, 16.476s/100 iter), loss = 1.30952
I0630 07:29:05.059274 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 07:29:05.059284 29777 sgd_solver.cpp:106] Iteration 100100, lr = 0.00687187
I0630 07:29:21.282745 29777 solver.cpp:290] Iteration 100200 (6.16408 iter/s, 16.223s/100 iter), loss = 1.10714
I0630 07:29:21.282819 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 07:29:21.282826 29777 sgd_solver.cpp:106] Iteration 100200, lr = 0.00686875
I0630 07:29:37.506414 29777 solver.cpp:290] Iteration 100300 (6.16403 iter/s, 16.2231s/100 iter), loss = 1.20238
I0630 07:29:37.506470 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 07:29:37.506494 29777 sgd_solver.cpp:106] Iteration 100300, lr = 0.00686563
I0630 07:29:53.816896 29777 solver.cpp:290] Iteration 100400 (6.13121 iter/s, 16.31s/100 iter), loss = 1.04762
I0630 07:29:53.817013 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 07:29:53.817023 29777 sgd_solver.cpp:106] Iteration 100400, lr = 0.0068625
I0630 07:30:10.165825 29777 solver.cpp:290] Iteration 100500 (6.11682 iter/s, 16.3484s/100 iter), loss = 1.28571
I0630 07:30:10.165848 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 07:30:10.165854 29777 sgd_solver.cpp:106] Iteration 100500, lr = 0.00685938
I0630 07:30:26.564766 29777 solver.cpp:290] Iteration 100600 (6.09814 iter/s, 16.3985s/100 iter), loss = 0.821429
I0630 07:30:26.564910 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 07:30:26.564950 29777 sgd_solver.cpp:106] Iteration 100600, lr = 0.00685625
I0630 07:30:42.780472 29777 solver.cpp:290] Iteration 100700 (6.16708 iter/s, 16.2151s/100 iter), loss = 1.16667
I0630 07:30:42.780498 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 07:30:42.780508 29777 sgd_solver.cpp:106] Iteration 100700, lr = 0.00685313
I0630 07:30:59.019151 29777 solver.cpp:290] Iteration 100800 (6.15832 iter/s, 16.2382s/100 iter), loss = 1.19048
I0630 07:30:59.019254 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 07:30:59.019264 29777 sgd_solver.cpp:106] Iteration 100800, lr = 0.00685
I0630 07:31:15.268990 29777 solver.cpp:290] Iteration 100900 (6.15411 iter/s, 16.2493s/100 iter), loss = 0.797619
I0630 07:31:15.269014 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 07:31:15.269023 29777 sgd_solver.cpp:106] Iteration 100900, lr = 0.00684687
I0630 07:31:31.463002 29777 solver.cpp:354] Sparsity after update:
I0630 07:31:31.483280 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:31:31.483297 29777 net.cpp:1851] conv1a_param_0(0.255) 
I0630 07:31:31.483309 29777 net.cpp:1851] conv1b_param_0(0.51) 
I0630 07:31:31.483314 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:31:31.483324 29777 net.cpp:1851] res2a_branch2a_param_0(0.51) 
I0630 07:31:31.483330 29777 net.cpp:1851] res2a_branch2b_param_0(0.51) 
I0630 07:31:31.483335 29777 net.cpp:1851] res3a_branch2a_param_0(0.51) 
I0630 07:31:31.483340 29777 net.cpp:1851] res3a_branch2b_param_0(0.51) 
I0630 07:31:31.483345 29777 net.cpp:1851] res4a_branch2a_param_0(0.51) 
I0630 07:31:31.483350 29777 net.cpp:1851] res4a_branch2b_param_0(0.51) 
I0630 07:31:31.483355 29777 net.cpp:1851] res5a_branch2a_param_0(0.51) 
I0630 07:31:31.483357 29777 net.cpp:1851] res5a_branch2b_param_0(0.51) 
I0630 07:31:31.483361 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.20032e+06/2.86678e+06) 0.419
I0630 07:31:31.704705 29777 solver.cpp:290] Iteration 101000 (6.08449 iter/s, 16.4352s/100 iter), loss = 1.19048
I0630 07:31:31.704758 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 07:31:31.704780 29777 sgd_solver.cpp:106] Iteration 101000, lr = 0.00684375
I0630 07:31:47.995200 29777 solver.cpp:290] Iteration 101100 (6.13874 iter/s, 16.29s/100 iter), loss = 0.797619
I0630 07:31:47.995227 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 07:31:47.995236 29777 sgd_solver.cpp:106] Iteration 101100, lr = 0.00684062
I0630 07:32:04.316638 29777 solver.cpp:290] Iteration 101200 (6.12709 iter/s, 16.321s/100 iter), loss = 1.2381
I0630 07:32:04.316689 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 07:32:04.316705 29777 sgd_solver.cpp:106] Iteration 101200, lr = 0.0068375
I0630 07:32:20.473383 29777 solver.cpp:290] Iteration 101300 (6.18956 iter/s, 16.1562s/100 iter), loss = 1.2381
I0630 07:32:20.473410 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 07:32:20.473418 29777 sgd_solver.cpp:106] Iteration 101300, lr = 0.00683437
I0630 07:32:36.688289 29777 solver.cpp:290] Iteration 101400 (6.16735 iter/s, 16.2144s/100 iter), loss = 1.27381
I0630 07:32:36.688407 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 07:32:36.688417 29777 sgd_solver.cpp:106] Iteration 101400, lr = 0.00683125
I0630 07:32:53.026412 29777 solver.cpp:290] Iteration 101500 (6.12087 iter/s, 16.3376s/100 iter), loss = 1.16667
I0630 07:32:53.026432 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 07:32:53.026439 29777 sgd_solver.cpp:106] Iteration 101500, lr = 0.00682813
I0630 07:33:09.079766 29777 solver.cpp:290] Iteration 101600 (6.22941 iter/s, 16.0529s/100 iter), loss = 0.857143
I0630 07:33:09.079874 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 07:33:09.079888 29777 sgd_solver.cpp:106] Iteration 101600, lr = 0.006825
I0630 07:33:25.285321 29777 solver.cpp:290] Iteration 101700 (6.17093 iter/s, 16.205s/100 iter), loss = 1.32143
I0630 07:33:25.285343 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 07:33:25.285351 29777 sgd_solver.cpp:106] Iteration 101700, lr = 0.00682187
I0630 07:33:42.128540 29777 solver.cpp:290] Iteration 101800 (5.93728 iter/s, 16.8427s/100 iter), loss = 0.97619
I0630 07:33:42.128618 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 07:33:42.128630 29777 sgd_solver.cpp:106] Iteration 101800, lr = 0.00681875
I0630 07:33:58.287410 29777 solver.cpp:290] Iteration 101900 (6.18875 iter/s, 16.1584s/100 iter), loss = 1.0119
I0630 07:33:58.287433 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 07:33:58.287441 29777 sgd_solver.cpp:106] Iteration 101900, lr = 0.00681563
I0630 07:34:14.157063 29777 solver.cpp:354] Sparsity after update:
I0630 07:34:14.158525 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:34:14.158534 29777 net.cpp:1851] conv1a_param_0(0.255) 
I0630 07:34:14.158540 29777 net.cpp:1851] conv1b_param_0(0.51) 
I0630 07:34:14.158541 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:34:14.158543 29777 net.cpp:1851] res2a_branch2a_param_0(0.51) 
I0630 07:34:14.158545 29777 net.cpp:1851] res2a_branch2b_param_0(0.51) 
I0630 07:34:14.158547 29777 net.cpp:1851] res3a_branch2a_param_0(0.51) 
I0630 07:34:14.158550 29777 net.cpp:1851] res3a_branch2b_param_0(0.51) 
I0630 07:34:14.158551 29777 net.cpp:1851] res4a_branch2a_param_0(0.51) 
I0630 07:34:14.158553 29777 net.cpp:1851] res4a_branch2b_param_0(0.51) 
I0630 07:34:14.158555 29777 net.cpp:1851] res5a_branch2a_param_0(0.51) 
I0630 07:34:14.158557 29777 net.cpp:1851] res5a_branch2b_param_0(0.51) 
I0630 07:34:14.158560 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.20032e+06/2.86678e+06) 0.419
I0630 07:34:14.158651 29777 solver.cpp:471] Iteration 102000, Testing net (#0)
I0630 07:34:24.705775 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 07:35:15.351846 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.571659
I0630 07:35:15.351949 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.804861
I0630 07:35:15.351960 29777 solver.cpp:544]     Test net output #2: loss = 1.52482 (* 1 = 1.52482 loss)
I0630 07:35:15.532583 29777 solver.cpp:290] Iteration 102000 (1.29461 iter/s, 77.243s/100 iter), loss = 1.35714
I0630 07:35:15.532609 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 07:35:15.532618 29777 sgd_solver.cpp:106] Iteration 102000, lr = 0.0068125
I0630 07:35:15.533602 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.52
I0630 07:35:16.082391 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 07:35:31.587787 29777 solver.cpp:290] Iteration 102100 (6.22869 iter/s, 16.0547s/100 iter), loss = 1.32143
I0630 07:35:31.587810 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 07:35:31.587816 29777 sgd_solver.cpp:106] Iteration 102100, lr = 0.00680938
I0630 07:35:47.613355 29777 solver.cpp:290] Iteration 102200 (6.24021 iter/s, 16.0251s/100 iter), loss = 1.5
I0630 07:35:47.613481 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 07:35:47.613490 29777 sgd_solver.cpp:106] Iteration 102200, lr = 0.00680625
I0630 07:36:03.750735 29777 solver.cpp:290] Iteration 102300 (6.19701 iter/s, 16.1368s/100 iter), loss = 1.17857
I0630 07:36:03.750761 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 07:36:03.750766 29777 sgd_solver.cpp:106] Iteration 102300, lr = 0.00680313
I0630 07:36:20.077519 29777 solver.cpp:290] Iteration 102400 (6.12509 iter/s, 16.3263s/100 iter), loss = 1.15476
I0630 07:36:20.077628 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 07:36:20.077652 29777 sgd_solver.cpp:106] Iteration 102400, lr = 0.0068
I0630 07:36:36.115378 29777 solver.cpp:290] Iteration 102500 (6.23546 iter/s, 16.0373s/100 iter), loss = 1
I0630 07:36:36.115404 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 07:36:36.115413 29777 sgd_solver.cpp:106] Iteration 102500, lr = 0.00679688
I0630 07:36:52.219579 29777 solver.cpp:290] Iteration 102600 (6.20974 iter/s, 16.1037s/100 iter), loss = 1.04762
I0630 07:36:52.219652 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 07:36:52.219660 29777 sgd_solver.cpp:106] Iteration 102600, lr = 0.00679375
I0630 07:37:08.411275 29777 solver.cpp:290] Iteration 102700 (6.1762 iter/s, 16.1912s/100 iter), loss = 0.928571
I0630 07:37:08.411299 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 07:37:08.411306 29777 sgd_solver.cpp:106] Iteration 102700, lr = 0.00679062
I0630 07:37:24.610047 29777 solver.cpp:290] Iteration 102800 (6.17349 iter/s, 16.1983s/100 iter), loss = 0.904762
I0630 07:37:24.610121 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 07:37:24.610131 29777 sgd_solver.cpp:106] Iteration 102800, lr = 0.0067875
I0630 07:37:40.651485 29777 solver.cpp:290] Iteration 102900 (6.23406 iter/s, 16.0409s/100 iter), loss = 0.892857
I0630 07:37:40.651513 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 07:37:40.651522 29777 sgd_solver.cpp:106] Iteration 102900, lr = 0.00678437
I0630 07:37:56.640216 29777 solver.cpp:354] Sparsity after update:
I0630 07:37:56.660684 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:37:56.660697 29777 net.cpp:1851] conv1a_param_0(0.26) 
I0630 07:37:56.660708 29777 net.cpp:1851] conv1b_param_0(0.52) 
I0630 07:37:56.660712 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:37:56.660718 29777 net.cpp:1851] res2a_branch2a_param_0(0.52) 
I0630 07:37:56.660722 29777 net.cpp:1851] res2a_branch2b_param_0(0.52) 
I0630 07:37:56.660727 29777 net.cpp:1851] res3a_branch2a_param_0(0.52) 
I0630 07:37:56.660729 29777 net.cpp:1851] res3a_branch2b_param_0(0.52) 
I0630 07:37:56.660733 29777 net.cpp:1851] res4a_branch2a_param_0(0.52) 
I0630 07:37:56.660737 29777 net.cpp:1851] res4a_branch2b_param_0(0.52) 
I0630 07:37:56.660742 29777 net.cpp:1851] res5a_branch2a_param_0(0.52) 
I0630 07:37:56.660745 29777 net.cpp:1851] res5a_branch2b_param_0(0.52) 
I0630 07:37:56.660749 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.22386e+06/2.86678e+06) 0.427
I0630 07:37:56.816612 29777 solver.cpp:290] Iteration 103000 (6.18634 iter/s, 16.1647s/100 iter), loss = 0.892857
I0630 07:37:56.816637 29777 solver.cpp:309]     Train net output #0: loss = 0.571429 (* 1 = 0.571429 loss)
I0630 07:37:56.816646 29777 sgd_solver.cpp:106] Iteration 103000, lr = 0.00678125
I0630 07:38:12.949226 29777 solver.cpp:290] Iteration 103100 (6.19881 iter/s, 16.1321s/100 iter), loss = 1.03571
I0630 07:38:12.949329 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 07:38:12.949363 29777 sgd_solver.cpp:106] Iteration 103100, lr = 0.00677812
I0630 07:38:29.257956 29777 solver.cpp:290] Iteration 103200 (6.13189 iter/s, 16.3082s/100 iter), loss = 1.03571
I0630 07:38:29.258092 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 07:38:29.258107 29777 sgd_solver.cpp:106] Iteration 103200, lr = 0.006775
I0630 07:38:45.199625 29777 solver.cpp:290] Iteration 103300 (6.27309 iter/s, 15.9411s/100 iter), loss = 0.845238
I0630 07:38:45.199651 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 07:38:45.199661 29777 sgd_solver.cpp:106] Iteration 103300, lr = 0.00677188
I0630 07:39:01.231135 29777 solver.cpp:290] Iteration 103400 (6.2379 iter/s, 16.031s/100 iter), loss = 1
I0630 07:39:01.231227 29777 solver.cpp:309]     Train net output #0: loss = 0.595238 (* 1 = 0.595238 loss)
I0630 07:39:01.231240 29777 sgd_solver.cpp:106] Iteration 103400, lr = 0.00676875
I0630 07:39:17.423539 29777 solver.cpp:290] Iteration 103500 (6.17594 iter/s, 16.1919s/100 iter), loss = 1.15476
I0630 07:39:17.423563 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 07:39:17.423569 29777 sgd_solver.cpp:106] Iteration 103500, lr = 0.00676562
I0630 07:39:33.627444 29777 solver.cpp:290] Iteration 103600 (6.17153 iter/s, 16.2034s/100 iter), loss = 1.10714
I0630 07:39:33.627516 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 07:39:33.627523 29777 sgd_solver.cpp:106] Iteration 103600, lr = 0.0067625
I0630 07:39:49.925143 29777 solver.cpp:290] Iteration 103700 (6.13603 iter/s, 16.2972s/100 iter), loss = 1
I0630 07:39:49.925173 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 07:39:49.925179 29777 sgd_solver.cpp:106] Iteration 103700, lr = 0.00675938
I0630 07:40:06.114267 29777 solver.cpp:290] Iteration 103800 (6.17717 iter/s, 16.1886s/100 iter), loss = 0.797619
I0630 07:40:06.114362 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 07:40:06.114373 29777 sgd_solver.cpp:106] Iteration 103800, lr = 0.00675625
I0630 07:40:22.753217 29777 solver.cpp:290] Iteration 103900 (6.01019 iter/s, 16.6384s/100 iter), loss = 1.32143
I0630 07:40:22.753240 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 07:40:22.753247 29777 sgd_solver.cpp:106] Iteration 103900, lr = 0.00675313
I0630 07:40:39.114748 29777 solver.cpp:354] Sparsity after update:
I0630 07:40:39.117218 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:40:39.117238 29777 net.cpp:1851] conv1a_param_0(0.26) 
I0630 07:40:39.117261 29777 net.cpp:1851] conv1b_param_0(0.52) 
I0630 07:40:39.117272 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:40:39.117281 29777 net.cpp:1851] res2a_branch2a_param_0(0.52) 
I0630 07:40:39.117290 29777 net.cpp:1851] res2a_branch2b_param_0(0.52) 
I0630 07:40:39.117300 29777 net.cpp:1851] res3a_branch2a_param_0(0.52) 
I0630 07:40:39.117308 29777 net.cpp:1851] res3a_branch2b_param_0(0.52) 
I0630 07:40:39.117317 29777 net.cpp:1851] res4a_branch2a_param_0(0.52) 
I0630 07:40:39.117326 29777 net.cpp:1851] res4a_branch2b_param_0(0.52) 
I0630 07:40:39.117334 29777 net.cpp:1851] res5a_branch2a_param_0(0.52) 
I0630 07:40:39.117342 29777 net.cpp:1851] res5a_branch2b_param_0(0.52) 
I0630 07:40:39.117352 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.22386e+06/2.86678e+06) 0.427
I0630 07:40:39.117529 29777 solver.cpp:471] Iteration 104000, Testing net (#0)
I0630 07:40:52.617594 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 07:41:48.303063 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.57362
I0630 07:41:48.303138 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.805801
I0630 07:41:48.303145 29777 solver.cpp:544]     Test net output #2: loss = 1.51788 (* 1 = 1.51788 loss)
I0630 07:41:48.549633 29777 solver.cpp:290] Iteration 104000 (1.16558 iter/s, 85.794s/100 iter), loss = 0.964286
I0630 07:41:48.549676 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 07:41:48.549690 29777 sgd_solver.cpp:106] Iteration 104000, lr = 0.00675
I0630 07:41:48.551014 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.53
I0630 07:41:48.982612 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 07:42:04.553561 29777 solver.cpp:290] Iteration 104100 (6.24865 iter/s, 16.0034s/100 iter), loss = 1.03571
I0630 07:42:04.553583 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 07:42:04.553589 29777 sgd_solver.cpp:106] Iteration 104100, lr = 0.00674688
I0630 07:42:20.772212 29777 solver.cpp:290] Iteration 104200 (6.16592 iter/s, 16.2182s/100 iter), loss = 1.10714
I0630 07:42:20.772343 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 07:42:20.772357 29777 sgd_solver.cpp:106] Iteration 104200, lr = 0.00674375
I0630 07:42:36.914286 29777 solver.cpp:290] Iteration 104300 (6.19521 iter/s, 16.1415s/100 iter), loss = 1.30952
I0630 07:42:36.914311 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 07:42:36.914320 29777 sgd_solver.cpp:106] Iteration 104300, lr = 0.00674062
I0630 07:42:53.472534 29777 solver.cpp:290] Iteration 104400 (6.03946 iter/s, 16.5578s/100 iter), loss = 0.952381
I0630 07:42:53.472666 29777 solver.cpp:309]     Train net output #0: loss = 0.547619 (* 1 = 0.547619 loss)
I0630 07:42:53.472687 29777 sgd_solver.cpp:106] Iteration 104400, lr = 0.0067375
I0630 07:43:09.990480 29777 solver.cpp:290] Iteration 104500 (6.05424 iter/s, 16.5174s/100 iter), loss = 0.892857
I0630 07:43:09.990530 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 07:43:09.990551 29777 sgd_solver.cpp:106] Iteration 104500, lr = 0.00673437
I0630 07:43:26.352824 29777 solver.cpp:290] Iteration 104600 (6.11178 iter/s, 16.3619s/100 iter), loss = 1.08333
I0630 07:43:26.352903 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 07:43:26.352913 29777 sgd_solver.cpp:106] Iteration 104600, lr = 0.00673125
I0630 07:43:42.606130 29777 solver.cpp:290] Iteration 104700 (6.1528 iter/s, 16.2528s/100 iter), loss = 1.13095
I0630 07:43:42.606209 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 07:43:42.606243 29777 sgd_solver.cpp:106] Iteration 104700, lr = 0.00672812
I0630 07:43:58.864976 29777 solver.cpp:290] Iteration 104800 (6.1507 iter/s, 16.2583s/100 iter), loss = 0.988095
I0630 07:43:58.865078 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 07:43:58.865101 29777 sgd_solver.cpp:106] Iteration 104800, lr = 0.006725
I0630 07:44:15.182354 29777 solver.cpp:290] Iteration 104900 (6.12864 iter/s, 16.3168s/100 iter), loss = 1.04762
I0630 07:44:15.182384 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 07:44:15.182395 29777 sgd_solver.cpp:106] Iteration 104900, lr = 0.00672187
I0630 07:44:31.304615 29777 solver.cpp:354] Sparsity after update:
I0630 07:44:31.325371 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:44:31.325400 29777 net.cpp:1851] conv1a_param_0(0.265) 
I0630 07:44:31.325412 29777 net.cpp:1851] conv1b_param_0(0.53) 
I0630 07:44:31.325417 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:44:31.325419 29777 net.cpp:1851] res2a_branch2a_param_0(0.53) 
I0630 07:44:31.325423 29777 net.cpp:1851] res2a_branch2b_param_0(0.53) 
I0630 07:44:31.325428 29777 net.cpp:1851] res3a_branch2a_param_0(0.53) 
I0630 07:44:31.325430 29777 net.cpp:1851] res3a_branch2b_param_0(0.53) 
I0630 07:44:31.325434 29777 net.cpp:1851] res4a_branch2a_param_0(0.53) 
I0630 07:44:31.325438 29777 net.cpp:1851] res4a_branch2b_param_0(0.53) 
I0630 07:44:31.325440 29777 net.cpp:1851] res5a_branch2a_param_0(0.53) 
I0630 07:44:31.325443 29777 net.cpp:1851] res5a_branch2b_param_0(0.53) 
I0630 07:44:31.325446 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.24739e+06/2.86678e+06) 0.435
I0630 07:44:31.487812 29777 solver.cpp:290] Iteration 105000 (6.1331 iter/s, 16.305s/100 iter), loss = 0.928571
I0630 07:44:31.487843 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 07:44:31.487851 29777 sgd_solver.cpp:106] Iteration 105000, lr = 0.00671875
I0630 07:44:47.665801 29777 solver.cpp:290] Iteration 105100 (6.18142 iter/s, 16.1775s/100 iter), loss = 1
I0630 07:44:47.665823 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 07:44:47.665830 29777 sgd_solver.cpp:106] Iteration 105100, lr = 0.00671562
I0630 07:45:03.852416 29777 solver.cpp:290] Iteration 105200 (6.17813 iter/s, 16.1861s/100 iter), loss = 1.25
I0630 07:45:03.852524 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 07:45:03.852553 29777 sgd_solver.cpp:106] Iteration 105200, lr = 0.0067125
I0630 07:45:20.080515 29777 solver.cpp:290] Iteration 105300 (6.16236 iter/s, 16.2276s/100 iter), loss = 1.09524
I0630 07:45:20.080538 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 07:45:20.080545 29777 sgd_solver.cpp:106] Iteration 105300, lr = 0.00670938
I0630 07:45:36.430857 29777 solver.cpp:290] Iteration 105400 (6.11626 iter/s, 16.3499s/100 iter), loss = 1.03571
I0630 07:45:36.430920 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 07:45:36.430928 29777 sgd_solver.cpp:106] Iteration 105400, lr = 0.00670625
I0630 07:45:52.795323 29777 solver.cpp:290] Iteration 105500 (6.11099 iter/s, 16.364s/100 iter), loss = 0.880952
I0630 07:45:52.795351 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 07:45:52.795359 29777 sgd_solver.cpp:106] Iteration 105500, lr = 0.00670313
I0630 07:46:08.991853 29777 solver.cpp:290] Iteration 105600 (6.17434 iter/s, 16.1961s/100 iter), loss = 1.20238
I0630 07:46:08.991961 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 07:46:08.991971 29777 sgd_solver.cpp:106] Iteration 105600, lr = 0.0067
I0630 07:46:25.340873 29777 solver.cpp:290] Iteration 105700 (6.11678 iter/s, 16.3485s/100 iter), loss = 1.04762
I0630 07:46:25.340896 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 07:46:25.340903 29777 sgd_solver.cpp:106] Iteration 105700, lr = 0.00669687
I0630 07:46:41.699932 29777 solver.cpp:290] Iteration 105800 (6.113 iter/s, 16.3586s/100 iter), loss = 0.833333
I0630 07:46:41.700059 29777 solver.cpp:309]     Train net output #0: loss = 0.47619 (* 1 = 0.47619 loss)
I0630 07:46:41.700080 29777 sgd_solver.cpp:106] Iteration 105800, lr = 0.00669375
I0630 07:46:58.056116 29777 solver.cpp:290] Iteration 105900 (6.11411 iter/s, 16.3556s/100 iter), loss = 1.2381
I0630 07:46:58.056138 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 07:46:58.056145 29777 sgd_solver.cpp:106] Iteration 105900, lr = 0.00669062
I0630 07:47:13.987532 29777 solver.cpp:354] Sparsity after update:
I0630 07:47:13.988951 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:47:13.988960 29777 net.cpp:1851] conv1a_param_0(0.265) 
I0630 07:47:13.988970 29777 net.cpp:1851] conv1b_param_0(0.53) 
I0630 07:47:13.988976 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:47:13.988979 29777 net.cpp:1851] res2a_branch2a_param_0(0.53) 
I0630 07:47:13.988983 29777 net.cpp:1851] res2a_branch2b_param_0(0.53) 
I0630 07:47:13.988988 29777 net.cpp:1851] res3a_branch2a_param_0(0.53) 
I0630 07:47:13.988992 29777 net.cpp:1851] res3a_branch2b_param_0(0.53) 
I0630 07:47:13.988996 29777 net.cpp:1851] res4a_branch2a_param_0(0.53) 
I0630 07:47:13.989001 29777 net.cpp:1851] res4a_branch2b_param_0(0.53) 
I0630 07:47:13.989004 29777 net.cpp:1851] res5a_branch2a_param_0(0.53) 
I0630 07:47:13.989008 29777 net.cpp:1851] res5a_branch2b_param_0(0.53) 
I0630 07:47:13.989012 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.24739e+06/2.86678e+06) 0.435
I0630 07:47:13.989104 29777 solver.cpp:471] Iteration 106000, Testing net (#0)
I0630 07:47:25.913780 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 07:48:18.647572 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.57596
I0630 07:48:18.647644 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.807121
I0630 07:48:18.647651 29777 solver.cpp:544]     Test net output #2: loss = 1.51344 (* 1 = 1.51344 loss)
I0630 07:48:18.822662 29777 solver.cpp:290] Iteration 106000 (1.23817 iter/s, 80.7643s/100 iter), loss = 1
I0630 07:48:18.822693 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 07:48:18.822705 29777 sgd_solver.cpp:106] Iteration 106000, lr = 0.0066875
I0630 07:48:18.823801 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.54
I0630 07:48:19.196768 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 07:48:34.746918 29777 solver.cpp:290] Iteration 106100 (6.27992 iter/s, 15.9238s/100 iter), loss = 0.964286
I0630 07:48:34.747103 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 07:48:34.747213 29777 sgd_solver.cpp:106] Iteration 106100, lr = 0.00668437
I0630 07:48:51.189357 29777 solver.cpp:290] Iteration 106200 (6.08206 iter/s, 16.4418s/100 iter), loss = 0.904762
I0630 07:48:51.189410 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 07:48:51.189420 29777 sgd_solver.cpp:106] Iteration 106200, lr = 0.00668125
I0630 07:49:07.405514 29777 solver.cpp:290] Iteration 106300 (6.16688 iter/s, 16.2157s/100 iter), loss = 1.09524
I0630 07:49:07.405540 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 07:49:07.405550 29777 sgd_solver.cpp:106] Iteration 106300, lr = 0.00667812
I0630 07:49:23.568080 29777 solver.cpp:290] Iteration 106400 (6.18732 iter/s, 16.1621s/100 iter), loss = 1.09524
I0630 07:49:23.568163 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 07:49:23.568173 29777 sgd_solver.cpp:106] Iteration 106400, lr = 0.006675
I0630 07:49:39.726014 29777 solver.cpp:290] Iteration 106500 (6.18911 iter/s, 16.1574s/100 iter), loss = 0.869048
I0630 07:49:39.726039 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 07:49:39.726048 29777 sgd_solver.cpp:106] Iteration 106500, lr = 0.00667187
I0630 07:49:55.881134 29777 solver.cpp:290] Iteration 106600 (6.19017 iter/s, 16.1546s/100 iter), loss = 1.0119
I0630 07:49:55.881207 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 07:49:55.881214 29777 sgd_solver.cpp:106] Iteration 106600, lr = 0.00666875
I0630 07:50:12.071477 29777 solver.cpp:290] Iteration 106700 (6.17672 iter/s, 16.1898s/100 iter), loss = 1.47619
I0630 07:50:12.071502 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 07:50:12.071511 29777 sgd_solver.cpp:106] Iteration 106700, lr = 0.00666562
I0630 07:50:28.361138 29777 solver.cpp:290] Iteration 106800 (6.13904 iter/s, 16.2892s/100 iter), loss = 1.03571
I0630 07:50:28.361331 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 07:50:28.361344 29777 sgd_solver.cpp:106] Iteration 106800, lr = 0.0066625
I0630 07:50:44.728368 29777 solver.cpp:290] Iteration 106900 (6.11001 iter/s, 16.3666s/100 iter), loss = 1.04762
I0630 07:50:44.728390 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 07:50:44.728397 29777 sgd_solver.cpp:106] Iteration 106900, lr = 0.00665938
I0630 07:51:00.884335 29777 solver.cpp:354] Sparsity after update:
I0630 07:51:00.904899 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:51:00.904955 29777 net.cpp:1851] conv1a_param_0(0.27) 
I0630 07:51:00.904984 29777 net.cpp:1851] conv1b_param_0(0.54) 
I0630 07:51:00.905000 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:51:00.905015 29777 net.cpp:1851] res2a_branch2a_param_0(0.54) 
I0630 07:51:00.905040 29777 net.cpp:1851] res2a_branch2b_param_0(0.54) 
I0630 07:51:00.905051 29777 net.cpp:1851] res3a_branch2a_param_0(0.54) 
I0630 07:51:00.905064 29777 net.cpp:1851] res3a_branch2b_param_0(0.54) 
I0630 07:51:00.905071 29777 net.cpp:1851] res4a_branch2a_param_0(0.54) 
I0630 07:51:00.905079 29777 net.cpp:1851] res4a_branch2b_param_0(0.54) 
I0630 07:51:00.905093 29777 net.cpp:1851] res5a_branch2a_param_0(0.54) 
I0630 07:51:00.905108 29777 net.cpp:1851] res5a_branch2b_param_0(0.54) 
I0630 07:51:00.905122 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.27093e+06/2.86678e+06) 0.443
I0630 07:51:01.059376 29777 solver.cpp:290] Iteration 107000 (6.1235 iter/s, 16.3305s/100 iter), loss = 0.880952
I0630 07:51:01.059398 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 07:51:01.059406 29777 sgd_solver.cpp:106] Iteration 107000, lr = 0.00665625
I0630 07:51:17.102820 29777 solver.cpp:290] Iteration 107100 (6.23326 iter/s, 16.043s/100 iter), loss = 1.17857
I0630 07:51:17.102849 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 07:51:17.102856 29777 sgd_solver.cpp:106] Iteration 107100, lr = 0.00665312
I0630 07:51:33.288313 29777 solver.cpp:290] Iteration 107200 (6.17855 iter/s, 16.185s/100 iter), loss = 1.02381
I0630 07:51:33.288396 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 07:51:33.288408 29777 sgd_solver.cpp:106] Iteration 107200, lr = 0.00665
I0630 07:51:49.587514 29777 solver.cpp:290] Iteration 107300 (6.13547 iter/s, 16.2987s/100 iter), loss = 0.833333
I0630 07:51:49.587538 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 07:51:49.587543 29777 sgd_solver.cpp:106] Iteration 107300, lr = 0.00664687
I0630 07:52:05.816499 29777 solver.cpp:290] Iteration 107400 (6.162 iter/s, 16.2285s/100 iter), loss = 1.05952
I0630 07:52:05.816599 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 07:52:05.816630 29777 sgd_solver.cpp:106] Iteration 107400, lr = 0.00664375
I0630 07:52:22.142081 29777 solver.cpp:290] Iteration 107500 (6.12556 iter/s, 16.325s/100 iter), loss = 0.892857
I0630 07:52:22.142125 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 07:52:22.142138 29777 sgd_solver.cpp:106] Iteration 107500, lr = 0.00664062
I0630 07:52:38.270103 29777 solver.cpp:290] Iteration 107600 (6.20057 iter/s, 16.1275s/100 iter), loss = 0.964286
I0630 07:52:38.270213 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 07:52:38.270223 29777 sgd_solver.cpp:106] Iteration 107600, lr = 0.0066375
I0630 07:52:54.477495 29777 solver.cpp:290] Iteration 107700 (6.17023 iter/s, 16.2068s/100 iter), loss = 1.53571
I0630 07:52:54.477517 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 07:52:54.477524 29777 sgd_solver.cpp:106] Iteration 107700, lr = 0.00663437
I0630 07:53:10.646661 29777 solver.cpp:290] Iteration 107800 (6.18479 iter/s, 16.1687s/100 iter), loss = 1.0119
I0630 07:53:10.646739 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 07:53:10.646749 29777 sgd_solver.cpp:106] Iteration 107800, lr = 0.00663125
I0630 07:53:26.918864 29777 solver.cpp:290] Iteration 107900 (6.14565 iter/s, 16.2717s/100 iter), loss = 1.02381
I0630 07:53:26.918889 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 07:53:26.918898 29777 sgd_solver.cpp:106] Iteration 107900, lr = 0.00662812
I0630 07:53:42.908438 29777 solver.cpp:354] Sparsity after update:
I0630 07:53:42.909723 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:53:42.909730 29777 net.cpp:1851] conv1a_param_0(0.27) 
I0630 07:53:42.909736 29777 net.cpp:1851] conv1b_param_0(0.54) 
I0630 07:53:42.909739 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:53:42.909741 29777 net.cpp:1851] res2a_branch2a_param_0(0.54) 
I0630 07:53:42.909744 29777 net.cpp:1851] res2a_branch2b_param_0(0.54) 
I0630 07:53:42.909745 29777 net.cpp:1851] res3a_branch2a_param_0(0.54) 
I0630 07:53:42.909747 29777 net.cpp:1851] res3a_branch2b_param_0(0.54) 
I0630 07:53:42.909749 29777 net.cpp:1851] res4a_branch2a_param_0(0.54) 
I0630 07:53:42.909751 29777 net.cpp:1851] res4a_branch2b_param_0(0.54) 
I0630 07:53:42.909752 29777 net.cpp:1851] res5a_branch2a_param_0(0.54) 
I0630 07:53:42.909754 29777 net.cpp:1851] res5a_branch2b_param_0(0.54) 
I0630 07:53:42.909757 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.27093e+06/2.86678e+06) 0.443
I0630 07:53:42.909844 29777 solver.cpp:471] Iteration 108000, Testing net (#0)
I0630 07:53:53.836494 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 07:54:42.173295 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.567079
I0630 07:54:42.173467 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.801862
I0630 07:54:42.173487 29777 solver.cpp:544]     Test net output #2: loss = 1.53852 (* 1 = 1.53852 loss)
I0630 07:54:42.354887 29777 solver.cpp:290] Iteration 108000 (1.32566 iter/s, 75.4339s/100 iter), loss = 0.892857
I0630 07:54:42.354964 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 07:54:42.354990 29777 sgd_solver.cpp:106] Iteration 108000, lr = 0.006625
I0630 07:54:42.356597 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.55
I0630 07:54:42.805866 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 07:54:58.271358 29777 solver.cpp:290] Iteration 108100 (6.283 iter/s, 15.916s/100 iter), loss = 1
I0630 07:54:58.271384 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 07:54:58.271394 29777 sgd_solver.cpp:106] Iteration 108100, lr = 0.00662187
I0630 07:55:14.461977 29777 solver.cpp:290] Iteration 108200 (6.1766 iter/s, 16.1901s/100 iter), loss = 1.03571
I0630 07:55:14.462090 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 07:55:14.462103 29777 sgd_solver.cpp:106] Iteration 108200, lr = 0.00661875
I0630 07:55:30.664628 29777 solver.cpp:290] Iteration 108300 (6.17204 iter/s, 16.2021s/100 iter), loss = 1.58333
I0630 07:55:30.664652 29777 solver.cpp:309]     Train net output #0: loss = 1.85714 (* 1 = 1.85714 loss)
I0630 07:55:30.664661 29777 sgd_solver.cpp:106] Iteration 108300, lr = 0.00661562
I0630 07:55:46.783730 29777 solver.cpp:290] Iteration 108400 (6.204 iter/s, 16.1186s/100 iter), loss = 1.08333
I0630 07:55:46.783826 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 07:55:46.783838 29777 sgd_solver.cpp:106] Iteration 108400, lr = 0.0066125
I0630 07:56:02.967479 29777 solver.cpp:290] Iteration 108500 (6.17924 iter/s, 16.1832s/100 iter), loss = 1.40476
I0630 07:56:02.967504 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 07:56:02.967510 29777 sgd_solver.cpp:106] Iteration 108500, lr = 0.00660937
I0630 07:56:19.194046 29777 solver.cpp:290] Iteration 108600 (6.16291 iter/s, 16.2261s/100 iter), loss = 1.28571
I0630 07:56:19.194141 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 07:56:19.194152 29777 sgd_solver.cpp:106] Iteration 108600, lr = 0.00660625
I0630 07:56:35.472362 29777 solver.cpp:290] Iteration 108700 (6.14335 iter/s, 16.2778s/100 iter), loss = 0.714286
I0630 07:56:35.472388 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 07:56:35.472396 29777 sgd_solver.cpp:106] Iteration 108700, lr = 0.00660313
I0630 07:56:52.050586 29777 solver.cpp:290] Iteration 108800 (6.03218 iter/s, 16.5777s/100 iter), loss = 1.09524
I0630 07:56:52.050689 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 07:56:52.050698 29777 sgd_solver.cpp:106] Iteration 108800, lr = 0.0066
I0630 07:57:08.463157 29777 solver.cpp:290] Iteration 108900 (6.0931 iter/s, 16.412s/100 iter), loss = 0.833333
I0630 07:57:08.463207 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 07:57:08.463222 29777 sgd_solver.cpp:106] Iteration 108900, lr = 0.00659687
I0630 07:57:24.768363 29777 solver.cpp:354] Sparsity after update:
I0630 07:57:24.788774 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 07:57:24.788790 29777 net.cpp:1851] conv1a_param_0(0.275) 
I0630 07:57:24.788801 29777 net.cpp:1851] conv1b_param_0(0.55) 
I0630 07:57:24.788805 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 07:57:24.788810 29777 net.cpp:1851] res2a_branch2a_param_0(0.55) 
I0630 07:57:24.788813 29777 net.cpp:1851] res2a_branch2b_param_0(0.55) 
I0630 07:57:24.788817 29777 net.cpp:1851] res3a_branch2a_param_0(0.55) 
I0630 07:57:24.788820 29777 net.cpp:1851] res3a_branch2b_param_0(0.55) 
I0630 07:57:24.788825 29777 net.cpp:1851] res4a_branch2a_param_0(0.55) 
I0630 07:57:24.788828 29777 net.cpp:1851] res4a_branch2b_param_0(0.55) 
I0630 07:57:24.788832 29777 net.cpp:1851] res5a_branch2a_param_0(0.55) 
I0630 07:57:24.788836 29777 net.cpp:1851] res5a_branch2b_param_0(0.55) 
I0630 07:57:24.788839 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.29446e+06/2.86678e+06) 0.452
I0630 07:57:24.951231 29777 solver.cpp:290] Iteration 109000 (6.06518 iter/s, 16.4876s/100 iter), loss = 1.2381
I0630 07:57:24.951298 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 07:57:24.951320 29777 sgd_solver.cpp:106] Iteration 109000, lr = 0.00659375
I0630 07:57:41.230401 29777 solver.cpp:290] Iteration 109100 (6.14301 iter/s, 16.2787s/100 iter), loss = 0.928571
I0630 07:57:41.230443 29777 solver.cpp:309]     Train net output #0: loss = 0.5 (* 1 = 0.5 loss)
I0630 07:57:41.230458 29777 sgd_solver.cpp:106] Iteration 109100, lr = 0.00659062
I0630 07:57:57.899016 29777 solver.cpp:290] Iteration 109200 (5.99948 iter/s, 16.6681s/100 iter), loss = 0.904762
I0630 07:57:57.899087 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 07:57:57.899096 29777 sgd_solver.cpp:106] Iteration 109200, lr = 0.0065875
I0630 07:58:14.129142 29777 solver.cpp:290] Iteration 109300 (6.16158 iter/s, 16.2296s/100 iter), loss = 0.869048
I0630 07:58:14.129190 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 07:58:14.129212 29777 sgd_solver.cpp:106] Iteration 109300, lr = 0.00658437
I0630 07:58:30.626194 29777 solver.cpp:290] Iteration 109400 (6.06187 iter/s, 16.4966s/100 iter), loss = 0.952381
I0630 07:58:30.626286 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 07:58:30.626296 29777 sgd_solver.cpp:106] Iteration 109400, lr = 0.00658125
I0630 07:58:46.840401 29777 solver.cpp:290] Iteration 109500 (6.16764 iter/s, 16.2137s/100 iter), loss = 1.08333
I0630 07:58:46.840427 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 07:58:46.840436 29777 sgd_solver.cpp:106] Iteration 109500, lr = 0.00657812
I0630 07:59:03.132642 29777 solver.cpp:290] Iteration 109600 (6.13807 iter/s, 16.2918s/100 iter), loss = 1.13095
I0630 07:59:03.132736 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 07:59:03.132747 29777 sgd_solver.cpp:106] Iteration 109600, lr = 0.006575
I0630 07:59:19.471849 29777 solver.cpp:290] Iteration 109700 (6.12045 iter/s, 16.3387s/100 iter), loss = 0.809524
I0630 07:59:19.471877 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 07:59:19.471884 29777 sgd_solver.cpp:106] Iteration 109700, lr = 0.00657187
I0630 07:59:35.788025 29777 solver.cpp:290] Iteration 109800 (6.12907 iter/s, 16.3157s/100 iter), loss = 0.821429
I0630 07:59:35.788137 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 07:59:35.788152 29777 sgd_solver.cpp:106] Iteration 109800, lr = 0.00656875
I0630 07:59:52.027007 29777 solver.cpp:290] Iteration 109900 (6.15823 iter/s, 16.2384s/100 iter), loss = 0.892857
I0630 07:59:52.027029 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 07:59:52.027036 29777 sgd_solver.cpp:106] Iteration 109900, lr = 0.00656562
I0630 08:00:07.993528 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_110000.caffemodel
I0630 08:00:08.012820 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_110000.solverstate
I0630 08:00:08.022315 29777 solver.cpp:354] Sparsity after update:
I0630 08:00:08.023484 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:00:08.023495 29777 net.cpp:1851] conv1a_param_0(0.275) 
I0630 08:00:08.023504 29777 net.cpp:1851] conv1b_param_0(0.55) 
I0630 08:00:08.023509 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:00:08.023511 29777 net.cpp:1851] res2a_branch2a_param_0(0.55) 
I0630 08:00:08.023515 29777 net.cpp:1851] res2a_branch2b_param_0(0.55) 
I0630 08:00:08.023519 29777 net.cpp:1851] res3a_branch2a_param_0(0.55) 
I0630 08:00:08.023521 29777 net.cpp:1851] res3a_branch2b_param_0(0.55) 
I0630 08:00:08.023525 29777 net.cpp:1851] res4a_branch2a_param_0(0.55) 
I0630 08:00:08.023528 29777 net.cpp:1851] res4a_branch2b_param_0(0.55) 
I0630 08:00:08.023531 29777 net.cpp:1851] res5a_branch2a_param_0(0.55) 
I0630 08:00:08.023543 29777 net.cpp:1851] res5a_branch2b_param_0(0.55) 
I0630 08:00:08.023546 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.29446e+06/2.86678e+06) 0.452
I0630 08:00:08.023684 29777 solver.cpp:471] Iteration 110000, Testing net (#0)
I0630 08:00:21.173820 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 08:01:12.191597 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.57248
I0630 08:01:12.191653 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.806001
I0630 08:01:12.191659 29777 solver.cpp:544]     Test net output #2: loss = 1.52902 (* 1 = 1.52902 loss)
I0630 08:01:12.386816 29777 solver.cpp:290] Iteration 110000 (1.24444 iter/s, 80.3576s/100 iter), loss = 0.892857
I0630 08:01:12.386903 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 08:01:12.386926 29777 sgd_solver.cpp:106] Iteration 110000, lr = 0.0065625
I0630 08:01:12.389011 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.56
I0630 08:01:12.829803 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 08:01:28.413211 29777 solver.cpp:290] Iteration 110100 (6.23991 iter/s, 16.0259s/100 iter), loss = 1.2381
I0630 08:01:28.413239 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 08:01:28.413247 29777 sgd_solver.cpp:106] Iteration 110100, lr = 0.00655937
I0630 08:01:44.568617 29777 solver.cpp:290] Iteration 110200 (6.19006 iter/s, 16.1549s/100 iter), loss = 0.952381
I0630 08:01:44.568703 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 08:01:44.568713 29777 sgd_solver.cpp:106] Iteration 110200, lr = 0.00655625
I0630 08:02:01.060174 29777 solver.cpp:290] Iteration 110300 (6.06391 iter/s, 16.491s/100 iter), loss = 0.988095
I0630 08:02:01.060204 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 08:02:01.060214 29777 sgd_solver.cpp:106] Iteration 110300, lr = 0.00655313
I0630 08:02:17.541973 29777 solver.cpp:290] Iteration 110400 (6.06748 iter/s, 16.4813s/100 iter), loss = 0.928571
I0630 08:02:17.542035 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 08:02:17.542045 29777 sgd_solver.cpp:106] Iteration 110400, lr = 0.00655
I0630 08:02:34.433809 29777 solver.cpp:290] Iteration 110500 (5.92021 iter/s, 16.8913s/100 iter), loss = 1.21429
I0630 08:02:34.433862 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 08:02:34.433886 29777 sgd_solver.cpp:106] Iteration 110500, lr = 0.00654687
I0630 08:02:51.393424 29777 solver.cpp:290] Iteration 110600 (5.89654 iter/s, 16.9591s/100 iter), loss = 1.08333
I0630 08:02:51.393501 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 08:02:51.393512 29777 sgd_solver.cpp:106] Iteration 110600, lr = 0.00654375
I0630 08:03:07.723445 29777 solver.cpp:290] Iteration 110700 (6.12389 iter/s, 16.3295s/100 iter), loss = 1.29762
I0630 08:03:07.723469 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 08:03:07.723476 29777 sgd_solver.cpp:106] Iteration 110700, lr = 0.00654062
I0630 08:03:23.784597 29777 solver.cpp:290] Iteration 110800 (6.22638 iter/s, 16.0607s/100 iter), loss = 0.857143
I0630 08:03:23.784704 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 08:03:23.784715 29777 sgd_solver.cpp:106] Iteration 110800, lr = 0.0065375
I0630 08:03:39.727510 29777 solver.cpp:290] Iteration 110900 (6.27259 iter/s, 15.9424s/100 iter), loss = 1.04762
I0630 08:03:39.727536 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 08:03:39.727550 29777 sgd_solver.cpp:106] Iteration 110900, lr = 0.00653437
I0630 08:03:55.577052 29777 solver.cpp:354] Sparsity after update:
I0630 08:03:55.597440 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:03:55.597455 29777 net.cpp:1851] conv1a_param_0(0.28) 
I0630 08:03:55.597466 29777 net.cpp:1851] conv1b_param_0(0.56) 
I0630 08:03:55.597470 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:03:55.597473 29777 net.cpp:1851] res2a_branch2a_param_0(0.56) 
I0630 08:03:55.597479 29777 net.cpp:1851] res2a_branch2b_param_0(0.56) 
I0630 08:03:55.597483 29777 net.cpp:1851] res3a_branch2a_param_0(0.56) 
I0630 08:03:55.597486 29777 net.cpp:1851] res3a_branch2b_param_0(0.56) 
I0630 08:03:55.597489 29777 net.cpp:1851] res4a_branch2a_param_0(0.56) 
I0630 08:03:55.597493 29777 net.cpp:1851] res4a_branch2b_param_0(0.56) 
I0630 08:03:55.597496 29777 net.cpp:1851] res5a_branch2a_param_0(0.56) 
I0630 08:03:55.597499 29777 net.cpp:1851] res5a_branch2b_param_0(0.56) 
I0630 08:03:55.597502 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.318e+06/2.86678e+06) 0.46
I0630 08:03:55.759356 29777 solver.cpp:290] Iteration 111000 (6.23777 iter/s, 16.0314s/100 iter), loss = 0.797619
I0630 08:03:55.759380 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 08:03:55.759387 29777 sgd_solver.cpp:106] Iteration 111000, lr = 0.00653125
I0630 08:04:11.803537 29777 solver.cpp:290] Iteration 111100 (6.23297 iter/s, 16.0437s/100 iter), loss = 1.36905
I0630 08:04:11.803560 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 08:04:11.803566 29777 sgd_solver.cpp:106] Iteration 111100, lr = 0.00652812
I0630 08:04:27.752033 29777 solver.cpp:290] Iteration 111200 (6.27036 iter/s, 15.948s/100 iter), loss = 1.13095
I0630 08:04:27.752110 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 08:04:27.752118 29777 sgd_solver.cpp:106] Iteration 111200, lr = 0.006525
I0630 08:04:43.904434 29777 solver.cpp:290] Iteration 111300 (6.19123 iter/s, 16.1519s/100 iter), loss = 0.97619
I0630 08:04:43.904458 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 08:04:43.904465 29777 sgd_solver.cpp:106] Iteration 111300, lr = 0.00652187
I0630 08:04:59.926144 29777 solver.cpp:290] Iteration 111400 (6.24171 iter/s, 16.0212s/100 iter), loss = 1.22619
I0630 08:04:59.926237 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 08:04:59.926249 29777 sgd_solver.cpp:106] Iteration 111400, lr = 0.00651875
I0630 08:05:15.942682 29777 solver.cpp:290] Iteration 111500 (6.24375 iter/s, 16.016s/100 iter), loss = 1.17857
I0630 08:05:15.942708 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 08:05:15.942718 29777 sgd_solver.cpp:106] Iteration 111500, lr = 0.00651562
I0630 08:05:32.107967 29777 solver.cpp:290] Iteration 111600 (6.18627 iter/s, 16.1648s/100 iter), loss = 0.940476
I0630 08:05:32.108042 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 08:05:32.108050 29777 sgd_solver.cpp:106] Iteration 111600, lr = 0.0065125
I0630 08:05:48.061496 29777 solver.cpp:290] Iteration 111700 (6.26841 iter/s, 15.953s/100 iter), loss = 1.28571
I0630 08:05:48.061520 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 08:05:48.061527 29777 sgd_solver.cpp:106] Iteration 111700, lr = 0.00650937
I0630 08:06:04.078338 29777 solver.cpp:290] Iteration 111800 (6.24361 iter/s, 16.0164s/100 iter), loss = 1.10714
I0630 08:06:04.078444 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 08:06:04.078455 29777 sgd_solver.cpp:106] Iteration 111800, lr = 0.00650625
I0630 08:06:20.016496 29777 solver.cpp:290] Iteration 111900 (6.27446 iter/s, 15.9376s/100 iter), loss = 1.52381
I0630 08:06:20.016518 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 08:06:20.016525 29777 sgd_solver.cpp:106] Iteration 111900, lr = 0.00650313
I0630 08:06:35.894642 29777 solver.cpp:354] Sparsity after update:
I0630 08:06:35.896075 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:06:35.896081 29777 net.cpp:1851] conv1a_param_0(0.28) 
I0630 08:06:35.896088 29777 net.cpp:1851] conv1b_param_0(0.56) 
I0630 08:06:35.896090 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:06:35.896093 29777 net.cpp:1851] res2a_branch2a_param_0(0.56) 
I0630 08:06:35.896095 29777 net.cpp:1851] res2a_branch2b_param_0(0.56) 
I0630 08:06:35.896097 29777 net.cpp:1851] res3a_branch2a_param_0(0.56) 
I0630 08:06:35.896100 29777 net.cpp:1851] res3a_branch2b_param_0(0.56) 
I0630 08:06:35.896100 29777 net.cpp:1851] res4a_branch2a_param_0(0.56) 
I0630 08:06:35.896102 29777 net.cpp:1851] res4a_branch2b_param_0(0.56) 
I0630 08:06:35.896105 29777 net.cpp:1851] res5a_branch2a_param_0(0.56) 
I0630 08:06:35.896106 29777 net.cpp:1851] res5a_branch2b_param_0(0.56) 
I0630 08:06:35.896108 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.318e+06/2.86678e+06) 0.46
I0630 08:06:35.896194 29777 solver.cpp:471] Iteration 112000, Testing net (#0)
I0630 08:06:44.722719 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 08:07:24.091167 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.57388
I0630 08:07:24.091248 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.806881
I0630 08:07:24.091259 29777 solver.cpp:544]     Test net output #2: loss = 1.51082 (* 1 = 1.51082 loss)
I0630 08:07:24.275326 29777 solver.cpp:290] Iteration 112000 (1.55625 iter/s, 64.2571s/100 iter), loss = 1.08333
I0630 08:07:24.275349 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 08:07:24.275357 29777 sgd_solver.cpp:106] Iteration 112000, lr = 0.0065
I0630 08:07:24.276048 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.57
I0630 08:07:24.688493 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 08:07:39.990787 29777 solver.cpp:290] Iteration 112100 (6.36335 iter/s, 15.715s/100 iter), loss = 1.2619
I0630 08:07:39.990808 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 08:07:39.990815 29777 sgd_solver.cpp:106] Iteration 112100, lr = 0.00649688
I0630 08:07:55.924131 29777 solver.cpp:290] Iteration 112200 (6.27633 iter/s, 15.9329s/100 iter), loss = 0.809524
I0630 08:07:55.924222 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 08:07:55.924233 29777 sgd_solver.cpp:106] Iteration 112200, lr = 0.00649375
I0630 08:08:12.153019 29777 solver.cpp:290] Iteration 112300 (6.16206 iter/s, 16.2284s/100 iter), loss = 1.04762
I0630 08:08:12.153043 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 08:08:12.153049 29777 sgd_solver.cpp:106] Iteration 112300, lr = 0.00649062
I0630 08:08:28.263257 29777 solver.cpp:290] Iteration 112400 (6.20741 iter/s, 16.1098s/100 iter), loss = 1.14286
I0630 08:08:28.263350 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 08:08:28.263360 29777 sgd_solver.cpp:106] Iteration 112400, lr = 0.0064875
I0630 08:08:44.286947 29777 solver.cpp:290] Iteration 112500 (6.24097 iter/s, 16.0232s/100 iter), loss = 1.02381
I0630 08:08:44.286980 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 08:08:44.286991 29777 sgd_solver.cpp:106] Iteration 112500, lr = 0.00648437
I0630 08:09:00.478353 29777 solver.cpp:290] Iteration 112600 (6.1763 iter/s, 16.1909s/100 iter), loss = 1.2619
I0630 08:09:00.478458 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 08:09:00.478471 29777 sgd_solver.cpp:106] Iteration 112600, lr = 0.00648125
I0630 08:09:16.485157 29777 solver.cpp:290] Iteration 112700 (6.24756 iter/s, 16.0063s/100 iter), loss = 1
I0630 08:09:16.485179 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 08:09:16.485186 29777 sgd_solver.cpp:106] Iteration 112700, lr = 0.00647812
I0630 08:09:32.604308 29777 solver.cpp:290] Iteration 112800 (6.20398 iter/s, 16.1187s/100 iter), loss = 1.20238
I0630 08:09:32.604405 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 08:09:32.604416 29777 sgd_solver.cpp:106] Iteration 112800, lr = 0.006475
I0630 08:09:48.644151 29777 solver.cpp:290] Iteration 112900 (6.23468 iter/s, 16.0393s/100 iter), loss = 1
I0630 08:09:48.644178 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 08:09:48.644192 29777 sgd_solver.cpp:106] Iteration 112900, lr = 0.00647187
I0630 08:10:04.541458 29777 solver.cpp:354] Sparsity after update:
I0630 08:10:04.561870 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:10:04.561885 29777 net.cpp:1851] conv1a_param_0(0.285) 
I0630 08:10:04.561893 29777 net.cpp:1851] conv1b_param_0(0.57) 
I0630 08:10:04.561897 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:10:04.561898 29777 net.cpp:1851] res2a_branch2a_param_0(0.57) 
I0630 08:10:04.561900 29777 net.cpp:1851] res2a_branch2b_param_0(0.57) 
I0630 08:10:04.561902 29777 net.cpp:1851] res3a_branch2a_param_0(0.57) 
I0630 08:10:04.561904 29777 net.cpp:1851] res3a_branch2b_param_0(0.57) 
I0630 08:10:04.561908 29777 net.cpp:1851] res4a_branch2a_param_0(0.57) 
I0630 08:10:04.561913 29777 net.cpp:1851] res4a_branch2b_param_0(0.57) 
I0630 08:10:04.561916 29777 net.cpp:1851] res5a_branch2a_param_0(0.57) 
I0630 08:10:04.561919 29777 net.cpp:1851] res5a_branch2b_param_0(0.57) 
I0630 08:10:04.561930 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.34154e+06/2.86678e+06) 0.468
I0630 08:10:04.718971 29777 solver.cpp:290] Iteration 113000 (6.22109 iter/s, 16.0744s/100 iter), loss = 1.20238
I0630 08:10:04.718997 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 08:10:04.719005 29777 sgd_solver.cpp:106] Iteration 113000, lr = 0.00646875
I0630 08:10:20.795873 29777 solver.cpp:290] Iteration 113100 (6.22029 iter/s, 16.0764s/100 iter), loss = 1.11905
I0630 08:10:20.795899 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 08:10:20.795908 29777 sgd_solver.cpp:106] Iteration 113100, lr = 0.00646562
I0630 08:10:36.730283 29777 solver.cpp:290] Iteration 113200 (6.27591 iter/s, 15.9339s/100 iter), loss = 1.10714
I0630 08:10:36.730368 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 08:10:36.730378 29777 sgd_solver.cpp:106] Iteration 113200, lr = 0.0064625
I0630 08:10:52.908241 29777 solver.cpp:290] Iteration 113300 (6.18145 iter/s, 16.1774s/100 iter), loss = 1.19048
I0630 08:10:52.908277 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 08:10:52.908287 29777 sgd_solver.cpp:106] Iteration 113300, lr = 0.00645937
I0630 08:11:08.928565 29777 solver.cpp:290] Iteration 113400 (6.24226 iter/s, 16.0198s/100 iter), loss = 1.41667
I0630 08:11:08.928649 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 08:11:08.928661 29777 sgd_solver.cpp:106] Iteration 113400, lr = 0.00645625
I0630 08:11:25.090379 29777 solver.cpp:290] Iteration 113500 (6.18763 iter/s, 16.1613s/100 iter), loss = 1.28571
I0630 08:11:25.090437 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 08:11:25.090461 29777 sgd_solver.cpp:106] Iteration 113500, lr = 0.00645312
I0630 08:11:41.240504 29777 solver.cpp:290] Iteration 113600 (6.19209 iter/s, 16.1496s/100 iter), loss = 1.41667
I0630 08:11:41.240615 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 08:11:41.240625 29777 sgd_solver.cpp:106] Iteration 113600, lr = 0.00645
I0630 08:11:57.618443 29777 solver.cpp:290] Iteration 113700 (6.10598 iter/s, 16.3774s/100 iter), loss = 1.05952
I0630 08:11:57.618468 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 08:11:57.618476 29777 sgd_solver.cpp:106] Iteration 113700, lr = 0.00644688
I0630 08:12:13.815795 29777 solver.cpp:290] Iteration 113800 (6.17403 iter/s, 16.1969s/100 iter), loss = 0.75
I0630 08:12:13.815853 29777 solver.cpp:309]     Train net output #0: loss = 0.690476 (* 1 = 0.690476 loss)
I0630 08:12:13.815865 29777 sgd_solver.cpp:106] Iteration 113800, lr = 0.00644375
I0630 08:12:30.117985 29777 solver.cpp:290] Iteration 113900 (6.13435 iter/s, 16.3017s/100 iter), loss = 1
I0630 08:12:30.118125 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 08:12:30.118212 29777 sgd_solver.cpp:106] Iteration 113900, lr = 0.00644063
I0630 08:12:46.029978 29777 solver.cpp:354] Sparsity after update:
I0630 08:12:46.031255 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:12:46.031263 29777 net.cpp:1851] conv1a_param_0(0.285) 
I0630 08:12:46.031270 29777 net.cpp:1851] conv1b_param_0(0.57) 
I0630 08:12:46.031272 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:12:46.031275 29777 net.cpp:1851] res2a_branch2a_param_0(0.57) 
I0630 08:12:46.031276 29777 net.cpp:1851] res2a_branch2b_param_0(0.57) 
I0630 08:12:46.031278 29777 net.cpp:1851] res3a_branch2a_param_0(0.57) 
I0630 08:12:46.031280 29777 net.cpp:1851] res3a_branch2b_param_0(0.57) 
I0630 08:12:46.031282 29777 net.cpp:1851] res4a_branch2a_param_0(0.57) 
I0630 08:12:46.031285 29777 net.cpp:1851] res4a_branch2b_param_0(0.57) 
I0630 08:12:46.031286 29777 net.cpp:1851] res5a_branch2a_param_0(0.57) 
I0630 08:12:46.031288 29777 net.cpp:1851] res5a_branch2b_param_0(0.57) 
I0630 08:12:46.031291 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.34154e+06/2.86678e+06) 0.468
I0630 08:12:46.031375 29777 solver.cpp:471] Iteration 114000, Testing net (#0)
I0630 08:12:57.410655 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 08:13:43.401199 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.57178
I0630 08:13:43.401283 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.806642
I0630 08:13:43.401291 29777 solver.cpp:544]     Test net output #2: loss = 1.51618 (* 1 = 1.51618 loss)
I0630 08:13:43.580051 29777 solver.cpp:290] Iteration 114000 (1.36129 iter/s, 73.46s/100 iter), loss = 0.892857
I0630 08:13:43.580077 29777 solver.cpp:309]     Train net output #0: loss = 0.547619 (* 1 = 0.547619 loss)
I0630 08:13:43.580086 29777 sgd_solver.cpp:106] Iteration 114000, lr = 0.0064375
I0630 08:13:43.581081 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.58
I0630 08:13:43.976541 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 08:13:59.220140 29777 solver.cpp:290] Iteration 114100 (6.39401 iter/s, 15.6396s/100 iter), loss = 1.2381
I0630 08:13:59.220163 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 08:13:59.220170 29777 sgd_solver.cpp:106] Iteration 114100, lr = 0.00643437
I0630 08:14:15.153630 29777 solver.cpp:290] Iteration 114200 (6.27627 iter/s, 15.933s/100 iter), loss = 0.678571
I0630 08:14:15.153674 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 08:14:15.153682 29777 sgd_solver.cpp:106] Iteration 114200, lr = 0.00643125
I0630 08:14:31.188549 29777 solver.cpp:290] Iteration 114300 (6.23658 iter/s, 16.0344s/100 iter), loss = 0.797619
I0630 08:14:31.188576 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 08:14:31.188585 29777 sgd_solver.cpp:106] Iteration 114300, lr = 0.00642812
I0630 08:14:47.175791 29777 solver.cpp:290] Iteration 114400 (6.25517 iter/s, 15.9868s/100 iter), loss = 1.17857
I0630 08:14:47.175899 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 08:14:47.175909 29777 sgd_solver.cpp:106] Iteration 114400, lr = 0.006425
I0630 08:15:03.148685 29777 solver.cpp:290] Iteration 114500 (6.26082 iter/s, 15.9724s/100 iter), loss = 1.04762
I0630 08:15:03.148708 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 08:15:03.148715 29777 sgd_solver.cpp:106] Iteration 114500, lr = 0.00642187
I0630 08:15:19.224300 29777 solver.cpp:290] Iteration 114600 (6.22078 iter/s, 16.0751s/100 iter), loss = 1.2619
I0630 08:15:19.224416 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 08:15:19.224426 29777 sgd_solver.cpp:106] Iteration 114600, lr = 0.00641875
I0630 08:15:35.249169 29777 solver.cpp:290] Iteration 114700 (6.24052 iter/s, 16.0243s/100 iter), loss = 1.42857
I0630 08:15:35.249195 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 08:15:35.249204 29777 sgd_solver.cpp:106] Iteration 114700, lr = 0.00641562
I0630 08:15:51.246506 29777 solver.cpp:290] Iteration 114800 (6.25122 iter/s, 15.9969s/100 iter), loss = 1.17857
I0630 08:15:51.246613 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 08:15:51.246623 29777 sgd_solver.cpp:106] Iteration 114800, lr = 0.0064125
I0630 08:16:07.293347 29777 solver.cpp:290] Iteration 114900 (6.23197 iter/s, 16.0463s/100 iter), loss = 1.05952
I0630 08:16:07.293375 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 08:16:07.293385 29777 sgd_solver.cpp:106] Iteration 114900, lr = 0.00640937
I0630 08:16:23.152521 29777 solver.cpp:354] Sparsity after update:
I0630 08:16:23.172767 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:16:23.172811 29777 net.cpp:1851] conv1a_param_0(0.29) 
I0630 08:16:23.172827 29777 net.cpp:1851] conv1b_param_0(0.58) 
I0630 08:16:23.172835 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:16:23.172844 29777 net.cpp:1851] res2a_branch2a_param_0(0.58) 
I0630 08:16:23.172852 29777 net.cpp:1851] res2a_branch2b_param_0(0.58) 
I0630 08:16:23.172860 29777 net.cpp:1851] res3a_branch2a_param_0(0.58) 
I0630 08:16:23.172868 29777 net.cpp:1851] res3a_branch2b_param_0(0.58) 
I0630 08:16:23.172876 29777 net.cpp:1851] res4a_branch2a_param_0(0.58) 
I0630 08:16:23.172884 29777 net.cpp:1851] res4a_branch2b_param_0(0.58) 
I0630 08:16:23.172893 29777 net.cpp:1851] res5a_branch2a_param_0(0.58) 
I0630 08:16:23.172900 29777 net.cpp:1851] res5a_branch2b_param_0(0.58) 
I0630 08:16:23.172909 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.36507e+06/2.86678e+06) 0.476
I0630 08:16:23.331513 29777 solver.cpp:290] Iteration 115000 (6.23531 iter/s, 16.0377s/100 iter), loss = 1.27381
I0630 08:16:23.331539 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 08:16:23.331548 29777 sgd_solver.cpp:106] Iteration 115000, lr = 0.00640625
I0630 08:16:39.305328 29777 solver.cpp:290] Iteration 115100 (6.26043 iter/s, 15.9734s/100 iter), loss = 1.5119
I0630 08:16:39.305351 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 08:16:39.305358 29777 sgd_solver.cpp:106] Iteration 115100, lr = 0.00640312
I0630 08:16:55.324610 29777 solver.cpp:290] Iteration 115200 (6.24266 iter/s, 16.0188s/100 iter), loss = 1.09524
I0630 08:16:55.324707 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 08:16:55.324717 29777 sgd_solver.cpp:106] Iteration 115200, lr = 0.0064
I0630 08:17:11.333113 29777 solver.cpp:290] Iteration 115300 (6.24689 iter/s, 16.008s/100 iter), loss = 1.40476
I0630 08:17:11.333137 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 08:17:11.333143 29777 sgd_solver.cpp:106] Iteration 115300, lr = 0.00639688
I0630 08:17:27.326900 29777 solver.cpp:290] Iteration 115400 (6.25261 iter/s, 15.9933s/100 iter), loss = 1.35714
I0630 08:17:27.327005 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 08:17:27.327018 29777 sgd_solver.cpp:106] Iteration 115400, lr = 0.00639375
I0630 08:17:43.320245 29777 solver.cpp:290] Iteration 115500 (6.25281 iter/s, 15.9928s/100 iter), loss = 0.72619
I0630 08:17:43.320269 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 08:17:43.320278 29777 sgd_solver.cpp:106] Iteration 115500, lr = 0.00639063
I0630 08:17:59.363898 29777 solver.cpp:290] Iteration 115600 (6.23318 iter/s, 16.0432s/100 iter), loss = 1.53571
I0630 08:17:59.364002 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 08:17:59.364012 29777 sgd_solver.cpp:106] Iteration 115600, lr = 0.0063875
I0630 08:18:15.410986 29777 solver.cpp:290] Iteration 115700 (6.23187 iter/s, 16.0465s/100 iter), loss = 1.42857
I0630 08:18:15.411015 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 08:18:15.411025 29777 sgd_solver.cpp:106] Iteration 115700, lr = 0.00638438
I0630 08:18:31.560508 29777 solver.cpp:290] Iteration 115800 (6.19232 iter/s, 16.149s/100 iter), loss = 1.19048
I0630 08:18:31.560583 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 08:18:31.560596 29777 sgd_solver.cpp:106] Iteration 115800, lr = 0.00638125
I0630 08:18:47.674118 29777 solver.cpp:290] Iteration 115900 (6.20613 iter/s, 16.1131s/100 iter), loss = 0.809524
I0630 08:18:47.674144 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 08:18:47.674155 29777 sgd_solver.cpp:106] Iteration 115900, lr = 0.00637812
I0630 08:19:03.791718 29777 solver.cpp:354] Sparsity after update:
I0630 08:19:03.793679 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:19:03.793696 29777 net.cpp:1851] conv1a_param_0(0.29) 
I0630 08:19:03.793714 29777 net.cpp:1851] conv1b_param_0(0.58) 
I0630 08:19:03.793720 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:19:03.793726 29777 net.cpp:1851] res2a_branch2a_param_0(0.58) 
I0630 08:19:03.793732 29777 net.cpp:1851] res2a_branch2b_param_0(0.58) 
I0630 08:19:03.793738 29777 net.cpp:1851] res3a_branch2a_param_0(0.58) 
I0630 08:19:03.793747 29777 net.cpp:1851] res3a_branch2b_param_0(0.58) 
I0630 08:19:03.793750 29777 net.cpp:1851] res4a_branch2a_param_0(0.58) 
I0630 08:19:03.793751 29777 net.cpp:1851] res4a_branch2b_param_0(0.58) 
I0630 08:19:03.793754 29777 net.cpp:1851] res5a_branch2a_param_0(0.58) 
I0630 08:19:03.793758 29777 net.cpp:1851] res5a_branch2b_param_0(0.58) 
I0630 08:19:03.793763 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.36507e+06/2.86678e+06) 0.476
I0630 08:19:03.793967 29777 solver.cpp:471] Iteration 116000, Testing net (#0)
I0630 08:19:16.224978 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 08:20:06.721899 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.570719
I0630 08:20:06.722421 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.802122
I0630 08:20:06.722477 29777 solver.cpp:544]     Test net output #2: loss = 1.53858 (* 1 = 1.53858 loss)
I0630 08:20:06.973714 29777 solver.cpp:290] Iteration 116000 (1.26108 iter/s, 79.2974s/100 iter), loss = 0.678571
I0630 08:20:06.973737 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 08:20:06.973744 29777 sgd_solver.cpp:106] Iteration 116000, lr = 0.006375
I0630 08:20:06.974421 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.59
I0630 08:20:07.390542 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 08:20:23.419039 29777 solver.cpp:290] Iteration 116100 (6.08093 iter/s, 16.4448s/100 iter), loss = 0.821429
I0630 08:20:23.419062 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 08:20:23.419070 29777 sgd_solver.cpp:106] Iteration 116100, lr = 0.00637187
I0630 08:20:39.618309 29777 solver.cpp:290] Iteration 116200 (6.1733 iter/s, 16.1988s/100 iter), loss = 1.05952
I0630 08:20:39.618412 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 08:20:39.618423 29777 sgd_solver.cpp:106] Iteration 116200, lr = 0.00636875
I0630 08:20:55.747448 29777 solver.cpp:290] Iteration 116300 (6.20017 iter/s, 16.1286s/100 iter), loss = 0.857143
I0630 08:20:55.747473 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 08:20:55.747483 29777 sgd_solver.cpp:106] Iteration 116300, lr = 0.00636562
I0630 08:21:11.865787 29777 solver.cpp:290] Iteration 116400 (6.2043 iter/s, 16.1179s/100 iter), loss = 0.952381
I0630 08:21:11.865890 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 08:21:11.865900 29777 sgd_solver.cpp:106] Iteration 116400, lr = 0.0063625
I0630 08:21:28.175535 29777 solver.cpp:290] Iteration 116500 (6.13151 iter/s, 16.3092s/100 iter), loss = 0.97619
I0630 08:21:28.175559 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 08:21:28.175566 29777 sgd_solver.cpp:106] Iteration 116500, lr = 0.00635938
I0630 08:21:44.338268 29777 solver.cpp:290] Iteration 116600 (6.18725 iter/s, 16.1623s/100 iter), loss = 0.97619
I0630 08:21:44.338373 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 08:21:44.338383 29777 sgd_solver.cpp:106] Iteration 116600, lr = 0.00635625
I0630 08:22:00.423683 29777 solver.cpp:290] Iteration 116700 (6.21702 iter/s, 16.0849s/100 iter), loss = 0.869048
I0630 08:22:00.423710 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 08:22:00.423719 29777 sgd_solver.cpp:106] Iteration 116700, lr = 0.00635312
I0630 08:22:16.444398 29777 solver.cpp:290] Iteration 116800 (6.2421 iter/s, 16.0202s/100 iter), loss = 1.19048
I0630 08:22:16.444484 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 08:22:16.444504 29777 sgd_solver.cpp:106] Iteration 116800, lr = 0.00635
I0630 08:22:32.645952 29777 solver.cpp:290] Iteration 116900 (6.17245 iter/s, 16.201s/100 iter), loss = 0.75
I0630 08:22:32.645977 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 08:22:32.645987 29777 sgd_solver.cpp:106] Iteration 116900, lr = 0.00634688
I0630 08:22:48.920500 29777 solver.cpp:354] Sparsity after update:
I0630 08:22:48.941114 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:22:48.941129 29777 net.cpp:1851] conv1a_param_0(0.295) 
I0630 08:22:48.941140 29777 net.cpp:1851] conv1b_param_0(0.59) 
I0630 08:22:48.941144 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:22:48.941148 29777 net.cpp:1851] res2a_branch2a_param_0(0.59) 
I0630 08:22:48.941153 29777 net.cpp:1851] res2a_branch2b_param_0(0.59) 
I0630 08:22:48.941155 29777 net.cpp:1851] res3a_branch2a_param_0(0.59) 
I0630 08:22:48.941159 29777 net.cpp:1851] res3a_branch2b_param_0(0.59) 
I0630 08:22:48.941162 29777 net.cpp:1851] res4a_branch2a_param_0(0.59) 
I0630 08:22:48.941165 29777 net.cpp:1851] res4a_branch2b_param_0(0.59) 
I0630 08:22:48.941170 29777 net.cpp:1851] res5a_branch2a_param_0(0.59) 
I0630 08:22:48.941174 29777 net.cpp:1851] res5a_branch2b_param_0(0.59) 
I0630 08:22:48.941177 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.3886e+06/2.86678e+06) 0.484
I0630 08:22:49.102000 29777 solver.cpp:290] Iteration 117000 (6.07697 iter/s, 16.4556s/100 iter), loss = 1.08333
I0630 08:22:49.102027 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 08:22:49.102036 29777 sgd_solver.cpp:106] Iteration 117000, lr = 0.00634375
I0630 08:23:05.308074 29777 solver.cpp:290] Iteration 117100 (6.17071 iter/s, 16.2056s/100 iter), loss = 0.964286
I0630 08:23:05.308101 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 08:23:05.308111 29777 sgd_solver.cpp:106] Iteration 117100, lr = 0.00634063
I0630 08:23:21.393532 29777 solver.cpp:290] Iteration 117200 (6.21698 iter/s, 16.085s/100 iter), loss = 1.21429
I0630 08:23:21.393637 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 08:23:21.393648 29777 sgd_solver.cpp:106] Iteration 117200, lr = 0.0063375
I0630 08:23:37.600213 29777 solver.cpp:290] Iteration 117300 (6.1705 iter/s, 16.2061s/100 iter), loss = 1.04762
I0630 08:23:37.600234 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 08:23:37.600241 29777 sgd_solver.cpp:106] Iteration 117300, lr = 0.00633438
I0630 08:23:53.681560 29777 solver.cpp:290] Iteration 117400 (6.21856 iter/s, 16.0809s/100 iter), loss = 1.09524
I0630 08:23:53.681638 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 08:23:53.681645 29777 sgd_solver.cpp:106] Iteration 117400, lr = 0.00633125
I0630 08:24:09.775118 29777 solver.cpp:290] Iteration 117500 (6.21387 iter/s, 16.093s/100 iter), loss = 1.13095
I0630 08:24:09.775141 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 08:24:09.775148 29777 sgd_solver.cpp:106] Iteration 117500, lr = 0.00632813
I0630 08:24:25.888612 29777 solver.cpp:290] Iteration 117600 (6.20616 iter/s, 16.113s/100 iter), loss = 1.13095
I0630 08:24:25.888716 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 08:24:25.888726 29777 sgd_solver.cpp:106] Iteration 117600, lr = 0.006325
I0630 08:24:41.987941 29777 solver.cpp:290] Iteration 117700 (6.21165 iter/s, 16.0988s/100 iter), loss = 1.21429
I0630 08:24:41.987965 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 08:24:41.987974 29777 sgd_solver.cpp:106] Iteration 117700, lr = 0.00632187
I0630 08:24:58.214001 29777 solver.cpp:290] Iteration 117800 (6.1631 iter/s, 16.2256s/100 iter), loss = 0.666667
I0630 08:24:58.214092 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 08:24:58.214103 29777 sgd_solver.cpp:106] Iteration 117800, lr = 0.00631875
I0630 08:25:14.323825 29777 solver.cpp:290] Iteration 117900 (6.2076 iter/s, 16.1093s/100 iter), loss = 1.03571
I0630 08:25:14.323873 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 08:25:14.323894 29777 sgd_solver.cpp:106] Iteration 117900, lr = 0.00631562
I0630 08:25:30.290593 29777 solver.cpp:354] Sparsity after update:
I0630 08:25:30.292053 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:25:30.292062 29777 net.cpp:1851] conv1a_param_0(0.295) 
I0630 08:25:30.292069 29777 net.cpp:1851] conv1b_param_0(0.59) 
I0630 08:25:30.292071 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:25:30.292073 29777 net.cpp:1851] res2a_branch2a_param_0(0.59) 
I0630 08:25:30.292075 29777 net.cpp:1851] res2a_branch2b_param_0(0.59) 
I0630 08:25:30.292078 29777 net.cpp:1851] res3a_branch2a_param_0(0.59) 
I0630 08:25:30.292079 29777 net.cpp:1851] res3a_branch2b_param_0(0.59) 
I0630 08:25:30.292081 29777 net.cpp:1851] res4a_branch2a_param_0(0.59) 
I0630 08:25:30.292083 29777 net.cpp:1851] res4a_branch2b_param_0(0.59) 
I0630 08:25:30.292085 29777 net.cpp:1851] res5a_branch2a_param_0(0.59) 
I0630 08:25:30.292086 29777 net.cpp:1851] res5a_branch2b_param_0(0.59) 
I0630 08:25:30.292088 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.3886e+06/2.86678e+06) 0.484
I0630 08:25:30.292182 29777 solver.cpp:471] Iteration 118000, Testing net (#0)
I0630 08:25:41.829180 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 08:26:33.348059 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56882
I0630 08:26:33.348166 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.803421
I0630 08:26:33.348176 29777 solver.cpp:544]     Test net output #2: loss = 1.5295 (* 1 = 1.5295 loss)
I0630 08:26:33.542799 29777 solver.cpp:290] Iteration 118000 (1.26236 iter/s, 79.2168s/100 iter), loss = 1.32143
I0630 08:26:33.542824 29777 solver.cpp:309]     Train net output #0: loss = 1.78571 (* 1 = 1.78571 loss)
I0630 08:26:33.542840 29777 sgd_solver.cpp:106] Iteration 118000, lr = 0.0063125
I0630 08:26:33.543828 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.6
I0630 08:26:33.968734 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 08:26:50.103344 29777 solver.cpp:290] Iteration 118100 (6.03862 iter/s, 16.5601s/100 iter), loss = 1.17857
I0630 08:26:50.103368 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 08:26:50.103374 29777 sgd_solver.cpp:106] Iteration 118100, lr = 0.00630937
I0630 08:27:06.355602 29777 solver.cpp:290] Iteration 118200 (6.15317 iter/s, 16.2518s/100 iter), loss = 0.928571
I0630 08:27:06.355684 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 08:27:06.355692 29777 sgd_solver.cpp:106] Iteration 118200, lr = 0.00630625
I0630 08:27:22.747236 29777 solver.cpp:290] Iteration 118300 (6.10087 iter/s, 16.3911s/100 iter), loss = 1.0119
I0630 08:27:22.747265 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 08:27:22.747273 29777 sgd_solver.cpp:106] Iteration 118300, lr = 0.00630313
I0630 08:27:39.019071 29777 solver.cpp:290] Iteration 118400 (6.14577 iter/s, 16.2714s/100 iter), loss = 1.25
I0630 08:27:39.019196 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 08:27:39.019210 29777 sgd_solver.cpp:106] Iteration 118400, lr = 0.0063
I0630 08:27:55.228672 29777 solver.cpp:290] Iteration 118500 (6.1694 iter/s, 16.209s/100 iter), loss = 1.19048
I0630 08:27:55.228698 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 08:27:55.228704 29777 sgd_solver.cpp:106] Iteration 118500, lr = 0.00629687
I0630 08:28:11.569475 29777 solver.cpp:290] Iteration 118600 (6.11983 iter/s, 16.3403s/100 iter), loss = 1.05952
I0630 08:28:11.569546 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 08:28:11.569555 29777 sgd_solver.cpp:106] Iteration 118600, lr = 0.00629375
I0630 08:28:27.824003 29777 solver.cpp:290] Iteration 118700 (6.15233 iter/s, 16.254s/100 iter), loss = 0.880952
I0630 08:28:27.824024 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 08:28:27.824031 29777 sgd_solver.cpp:106] Iteration 118700, lr = 0.00629063
I0630 08:28:43.954242 29777 solver.cpp:290] Iteration 118800 (6.19971 iter/s, 16.1298s/100 iter), loss = 0.75
I0630 08:28:43.954315 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 08:28:43.954324 29777 sgd_solver.cpp:106] Iteration 118800, lr = 0.0062875
I0630 08:29:00.247769 29777 solver.cpp:290] Iteration 118900 (6.13761 iter/s, 16.293s/100 iter), loss = 1.39286
I0630 08:29:00.247833 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 08:29:00.247856 29777 sgd_solver.cpp:106] Iteration 118900, lr = 0.00628438
I0630 08:29:16.440778 29777 solver.cpp:354] Sparsity after update:
I0630 08:29:16.461040 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:29:16.461063 29777 net.cpp:1851] conv1a_param_0(0.3) 
I0630 08:29:16.461076 29777 net.cpp:1851] conv1b_param_0(0.6) 
I0630 08:29:16.461081 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:29:16.461084 29777 net.cpp:1851] res2a_branch2a_param_0(0.6) 
I0630 08:29:16.461102 29777 net.cpp:1851] res2a_branch2b_param_0(0.6) 
I0630 08:29:16.461117 29777 net.cpp:1851] res3a_branch2a_param_0(0.6) 
I0630 08:29:16.461122 29777 net.cpp:1851] res3a_branch2b_param_0(0.6) 
I0630 08:29:16.461127 29777 net.cpp:1851] res4a_branch2a_param_0(0.6) 
I0630 08:29:16.461130 29777 net.cpp:1851] res4a_branch2b_param_0(0.6) 
I0630 08:29:16.461133 29777 net.cpp:1851] res5a_branch2a_param_0(0.6) 
I0630 08:29:16.461145 29777 net.cpp:1851] res5a_branch2b_param_0(0.6) 
I0630 08:29:16.461150 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.41214e+06/2.86678e+06) 0.493
I0630 08:29:16.618075 29777 solver.cpp:290] Iteration 119000 (6.10881 iter/s, 16.3698s/100 iter), loss = 0.97619
I0630 08:29:16.618098 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 08:29:16.618105 29777 sgd_solver.cpp:106] Iteration 119000, lr = 0.00628125
I0630 08:29:32.934602 29777 solver.cpp:290] Iteration 119100 (6.12893 iter/s, 16.3161s/100 iter), loss = 1.36905
I0630 08:29:32.934625 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 08:29:32.934631 29777 sgd_solver.cpp:106] Iteration 119100, lr = 0.00627813
I0630 08:29:49.086527 29777 solver.cpp:290] Iteration 119200 (6.19139 iter/s, 16.1515s/100 iter), loss = 1.21429
I0630 08:29:49.086630 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 08:29:49.086642 29777 sgd_solver.cpp:106] Iteration 119200, lr = 0.006275
I0630 08:30:05.380281 29777 solver.cpp:290] Iteration 119300 (6.13753 iter/s, 16.2932s/100 iter), loss = 1.07143
I0630 08:30:05.380372 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 08:30:05.380410 29777 sgd_solver.cpp:106] Iteration 119300, lr = 0.00627187
I0630 08:30:21.774119 29777 solver.cpp:290] Iteration 119400 (6.10005 iter/s, 16.3933s/100 iter), loss = 1.11905
I0630 08:30:21.774188 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 08:30:21.774196 29777 sgd_solver.cpp:106] Iteration 119400, lr = 0.00626875
I0630 08:30:37.998144 29777 solver.cpp:290] Iteration 119500 (6.16389 iter/s, 16.2235s/100 iter), loss = 1.14286
I0630 08:30:37.998169 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 08:30:37.998178 29777 sgd_solver.cpp:106] Iteration 119500, lr = 0.00626562
I0630 08:30:54.327968 29777 solver.cpp:290] Iteration 119600 (6.12395 iter/s, 16.3293s/100 iter), loss = 1
I0630 08:30:54.328045 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 08:30:54.328054 29777 sgd_solver.cpp:106] Iteration 119600, lr = 0.0062625
I0630 08:31:10.504844 29777 solver.cpp:290] Iteration 119700 (6.18186 iter/s, 16.1764s/100 iter), loss = 1.16667
I0630 08:31:10.504868 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 08:31:10.504874 29777 sgd_solver.cpp:106] Iteration 119700, lr = 0.00625937
I0630 08:31:26.913166 29777 solver.cpp:290] Iteration 119800 (6.09465 iter/s, 16.4078s/100 iter), loss = 1.17857
I0630 08:31:26.913307 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 08:31:26.913344 29777 sgd_solver.cpp:106] Iteration 119800, lr = 0.00625625
I0630 08:31:43.208776 29777 solver.cpp:290] Iteration 119900 (6.13684 iter/s, 16.295s/100 iter), loss = 1.04762
I0630 08:31:43.208802 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 08:31:43.208812 29777 sgd_solver.cpp:106] Iteration 119900, lr = 0.00625313
I0630 08:31:59.166673 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_120000.caffemodel
I0630 08:31:59.186028 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_120000.solverstate
I0630 08:31:59.194638 29777 solver.cpp:354] Sparsity after update:
I0630 08:31:59.195613 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:31:59.195622 29777 net.cpp:1851] conv1a_param_0(0.3) 
I0630 08:31:59.195632 29777 net.cpp:1851] conv1b_param_0(0.6) 
I0630 08:31:59.195637 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:31:59.195641 29777 net.cpp:1851] res2a_branch2a_param_0(0.6) 
I0630 08:31:59.195647 29777 net.cpp:1851] res2a_branch2b_param_0(0.6) 
I0630 08:31:59.195649 29777 net.cpp:1851] res3a_branch2a_param_0(0.6) 
I0630 08:31:59.195653 29777 net.cpp:1851] res3a_branch2b_param_0(0.6) 
I0630 08:31:59.195657 29777 net.cpp:1851] res4a_branch2a_param_0(0.6) 
I0630 08:31:59.195662 29777 net.cpp:1851] res4a_branch2b_param_0(0.6) 
I0630 08:31:59.195665 29777 net.cpp:1851] res5a_branch2a_param_0(0.6) 
I0630 08:31:59.195668 29777 net.cpp:1851] res5a_branch2b_param_0(0.6) 
I0630 08:31:59.195673 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.41214e+06/2.86678e+06) 0.493
I0630 08:31:59.195775 29777 solver.cpp:471] Iteration 120000, Testing net (#0)
I0630 08:32:12.745062 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 08:33:06.412230 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.565979
I0630 08:33:06.412398 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.801382
I0630 08:33:06.412420 29777 solver.cpp:544]     Test net output #2: loss = 1.545 (* 1 = 1.545 loss)
I0630 08:33:06.668694 29777 solver.cpp:290] Iteration 120000 (1.19821 iter/s, 83.4576s/100 iter), loss = 1.15476
I0630 08:33:06.668802 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 08:33:06.668845 29777 sgd_solver.cpp:106] Iteration 120000, lr = 0.00625
I0630 08:33:06.670960 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.61
I0630 08:33:07.211007 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 08:33:23.595166 29777 solver.cpp:290] Iteration 120100 (5.9081 iter/s, 16.9259s/100 iter), loss = 1.44048
I0630 08:33:23.595190 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 08:33:23.595196 29777 sgd_solver.cpp:106] Iteration 120100, lr = 0.00624687
I0630 08:33:39.615950 29777 solver.cpp:290] Iteration 120200 (6.24207 iter/s, 16.0203s/100 iter), loss = 1.41667
I0630 08:33:39.616060 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 08:33:39.616080 29777 sgd_solver.cpp:106] Iteration 120200, lr = 0.00624375
I0630 08:33:56.018244 29777 solver.cpp:290] Iteration 120300 (6.09692 iter/s, 16.4017s/100 iter), loss = 1.35714
I0630 08:33:56.018270 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 08:33:56.018285 29777 sgd_solver.cpp:106] Iteration 120300, lr = 0.00624063
I0630 08:34:12.380879 29777 solver.cpp:290] Iteration 120400 (6.11166 iter/s, 16.3622s/100 iter), loss = 0.97619
I0630 08:34:12.380986 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 08:34:12.381000 29777 sgd_solver.cpp:106] Iteration 120400, lr = 0.0062375
I0630 08:34:28.684070 29777 solver.cpp:290] Iteration 120500 (6.13398 iter/s, 16.3026s/100 iter), loss = 0.77381
I0630 08:34:28.684096 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 08:34:28.684105 29777 sgd_solver.cpp:106] Iteration 120500, lr = 0.00623438
I0630 08:34:44.935412 29777 solver.cpp:290] Iteration 120600 (6.15352 iter/s, 16.2509s/100 iter), loss = 1.25
I0630 08:34:44.935495 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 08:34:44.935505 29777 sgd_solver.cpp:106] Iteration 120600, lr = 0.00623125
I0630 08:35:01.184883 29777 solver.cpp:290] Iteration 120700 (6.15425 iter/s, 16.2489s/100 iter), loss = 1.19048
I0630 08:35:01.184907 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 08:35:01.184916 29777 sgd_solver.cpp:106] Iteration 120700, lr = 0.00622813
I0630 08:35:17.353598 29777 solver.cpp:290] Iteration 120800 (6.18496 iter/s, 16.1682s/100 iter), loss = 1.17857
I0630 08:35:17.353700 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 08:35:17.353710 29777 sgd_solver.cpp:106] Iteration 120800, lr = 0.006225
I0630 08:35:33.426321 29777 solver.cpp:290] Iteration 120900 (6.22193 iter/s, 16.0722s/100 iter), loss = 1.07143
I0630 08:35:33.426347 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 08:35:33.426353 29777 sgd_solver.cpp:106] Iteration 120900, lr = 0.00622187
I0630 08:35:49.518206 29777 solver.cpp:354] Sparsity after update:
I0630 08:35:49.538476 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:35:49.538492 29777 net.cpp:1851] conv1a_param_0(0.305) 
I0630 08:35:49.538504 29777 net.cpp:1851] conv1b_param_0(0.61) 
I0630 08:35:49.538508 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:35:49.538511 29777 net.cpp:1851] res2a_branch2a_param_0(0.61) 
I0630 08:35:49.538516 29777 net.cpp:1851] res2a_branch2b_param_0(0.61) 
I0630 08:35:49.538518 29777 net.cpp:1851] res3a_branch2a_param_0(0.61) 
I0630 08:35:49.538522 29777 net.cpp:1851] res3a_branch2b_param_0(0.61) 
I0630 08:35:49.538524 29777 net.cpp:1851] res4a_branch2a_param_0(0.61) 
I0630 08:35:49.538528 29777 net.cpp:1851] res4a_branch2b_param_0(0.61) 
I0630 08:35:49.538532 29777 net.cpp:1851] res5a_branch2a_param_0(0.61) 
I0630 08:35:49.538534 29777 net.cpp:1851] res5a_branch2b_param_0(0.61) 
I0630 08:35:49.538537 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.43568e+06/2.86678e+06) 0.501
I0630 08:35:49.702041 29777 solver.cpp:290] Iteration 121000 (6.1443 iter/s, 16.2752s/100 iter), loss = 1.46429
I0630 08:35:49.702086 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 08:35:49.702098 29777 sgd_solver.cpp:106] Iteration 121000, lr = 0.00621875
I0630 08:36:05.873190 29777 solver.cpp:290] Iteration 121100 (6.18404 iter/s, 16.1707s/100 iter), loss = 0.916667
I0630 08:36:05.873214 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 08:36:05.873219 29777 sgd_solver.cpp:106] Iteration 121100, lr = 0.00621562
I0630 08:36:22.135720 29777 solver.cpp:290] Iteration 121200 (6.14928 iter/s, 16.2621s/100 iter), loss = 1.21429
I0630 08:36:22.135838 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 08:36:22.135848 29777 sgd_solver.cpp:106] Iteration 121200, lr = 0.0062125
I0630 08:36:38.361183 29777 solver.cpp:290] Iteration 121300 (6.16337 iter/s, 16.2249s/100 iter), loss = 1
I0630 08:36:38.361207 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 08:36:38.361214 29777 sgd_solver.cpp:106] Iteration 121300, lr = 0.00620937
I0630 08:36:54.503625 29777 solver.cpp:290] Iteration 121400 (6.19503 iter/s, 16.142s/100 iter), loss = 1.15476
I0630 08:36:54.503718 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 08:36:54.503729 29777 sgd_solver.cpp:106] Iteration 121400, lr = 0.00620625
I0630 08:37:10.702803 29777 solver.cpp:290] Iteration 121500 (6.17336 iter/s, 16.1986s/100 iter), loss = 1.11905
I0630 08:37:10.702829 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 08:37:10.702844 29777 sgd_solver.cpp:106] Iteration 121500, lr = 0.00620312
I0630 08:37:26.801235 29777 solver.cpp:290] Iteration 121600 (6.21197 iter/s, 16.098s/100 iter), loss = 1.15476
I0630 08:37:26.801342 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 08:37:26.801353 29777 sgd_solver.cpp:106] Iteration 121600, lr = 0.0062
I0630 08:37:42.937922 29777 solver.cpp:290] Iteration 121700 (6.19727 iter/s, 16.1361s/100 iter), loss = 0.928571
I0630 08:37:42.937943 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 08:37:42.937950 29777 sgd_solver.cpp:106] Iteration 121700, lr = 0.00619687
I0630 08:37:59.044009 29777 solver.cpp:290] Iteration 121800 (6.20901 iter/s, 16.1056s/100 iter), loss = 1.08333
I0630 08:37:59.044109 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 08:37:59.044117 29777 sgd_solver.cpp:106] Iteration 121800, lr = 0.00619375
I0630 08:38:15.083454 29777 solver.cpp:290] Iteration 121900 (6.23484 iter/s, 16.0389s/100 iter), loss = 1.14286
I0630 08:38:15.083479 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 08:38:15.083485 29777 sgd_solver.cpp:106] Iteration 121900, lr = 0.00619063
I0630 08:38:31.086170 29777 solver.cpp:354] Sparsity after update:
I0630 08:38:31.087604 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:38:31.087612 29777 net.cpp:1851] conv1a_param_0(0.305) 
I0630 08:38:31.087623 29777 net.cpp:1851] conv1b_param_0(0.61) 
I0630 08:38:31.087627 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:38:31.087631 29777 net.cpp:1851] res2a_branch2a_param_0(0.61) 
I0630 08:38:31.087635 29777 net.cpp:1851] res2a_branch2b_param_0(0.61) 
I0630 08:38:31.087641 29777 net.cpp:1851] res3a_branch2a_param_0(0.61) 
I0630 08:38:31.087646 29777 net.cpp:1851] res3a_branch2b_param_0(0.61) 
I0630 08:38:31.087649 29777 net.cpp:1851] res4a_branch2a_param_0(0.61) 
I0630 08:38:31.087653 29777 net.cpp:1851] res4a_branch2b_param_0(0.61) 
I0630 08:38:31.087657 29777 net.cpp:1851] res5a_branch2a_param_0(0.61) 
I0630 08:38:31.087661 29777 net.cpp:1851] res5a_branch2b_param_0(0.61) 
I0630 08:38:31.087666 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.43568e+06/2.86678e+06) 0.501
I0630 08:38:31.087761 29777 solver.cpp:471] Iteration 122000, Testing net (#0)
I0630 08:38:43.554365 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 08:39:34.559162 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.568719
I0630 08:39:34.559286 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.804902
I0630 08:39:34.559296 29777 solver.cpp:544]     Test net output #2: loss = 1.52016 (* 1 = 1.52016 loss)
I0630 08:39:34.742465 29777 solver.cpp:290] Iteration 122000 (1.25539 iter/s, 79.6568s/100 iter), loss = 1.27381
I0630 08:39:34.742488 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 08:39:34.742494 29777 sgd_solver.cpp:106] Iteration 122000, lr = 0.0061875
I0630 08:39:34.743191 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.62
I0630 08:39:35.500141 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 08:39:51.824208 29777 solver.cpp:290] Iteration 122100 (5.85438 iter/s, 17.0812s/100 iter), loss = 1.39286
I0630 08:39:51.824283 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 08:39:51.824301 29777 sgd_solver.cpp:106] Iteration 122100, lr = 0.00618438
I0630 08:40:08.128087 29777 solver.cpp:290] Iteration 122200 (6.13371 iter/s, 16.3033s/100 iter), loss = 1.04762
I0630 08:40:08.128226 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 08:40:08.128243 29777 sgd_solver.cpp:106] Iteration 122200, lr = 0.00618125
I0630 08:40:24.170380 29777 solver.cpp:290] Iteration 122300 (6.23375 iter/s, 16.0417s/100 iter), loss = 1.39286
I0630 08:40:24.170403 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 08:40:24.170410 29777 sgd_solver.cpp:106] Iteration 122300, lr = 0.00617812
I0630 08:40:40.226912 29777 solver.cpp:290] Iteration 122400 (6.22818 iter/s, 16.0561s/100 iter), loss = 1.10714
I0630 08:40:40.227013 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 08:40:40.227022 29777 sgd_solver.cpp:106] Iteration 122400, lr = 0.006175
I0630 08:40:56.290544 29777 solver.cpp:290] Iteration 122500 (6.22545 iter/s, 16.0631s/100 iter), loss = 1.30952
I0630 08:40:56.290570 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 08:40:56.290580 29777 sgd_solver.cpp:106] Iteration 122500, lr = 0.00617187
I0630 08:41:12.373133 29777 solver.cpp:290] Iteration 122600 (6.21809 iter/s, 16.0821s/100 iter), loss = 1.16667
I0630 08:41:12.373236 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 08:41:12.373245 29777 sgd_solver.cpp:106] Iteration 122600, lr = 0.00616875
I0630 08:41:28.545722 29777 solver.cpp:290] Iteration 122700 (6.18351 iter/s, 16.172s/100 iter), loss = 1.22619
I0630 08:41:28.545745 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 08:41:28.545753 29777 sgd_solver.cpp:106] Iteration 122700, lr = 0.00616562
I0630 08:41:44.623886 29777 solver.cpp:290] Iteration 122800 (6.2198 iter/s, 16.0777s/100 iter), loss = 1.35714
I0630 08:41:44.623982 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 08:41:44.623992 29777 sgd_solver.cpp:106] Iteration 122800, lr = 0.0061625
I0630 08:42:00.737437 29777 solver.cpp:290] Iteration 122900 (6.20616 iter/s, 16.113s/100 iter), loss = 1
I0630 08:42:00.737463 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 08:42:00.737473 29777 sgd_solver.cpp:106] Iteration 122900, lr = 0.00615937
I0630 08:42:16.605516 29777 solver.cpp:354] Sparsity after update:
I0630 08:42:16.626008 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:42:16.626024 29777 net.cpp:1851] conv1a_param_0(0.31) 
I0630 08:42:16.626034 29777 net.cpp:1851] conv1b_param_0(0.62) 
I0630 08:42:16.626037 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:42:16.626040 29777 net.cpp:1851] res2a_branch2a_param_0(0.62) 
I0630 08:42:16.626056 29777 net.cpp:1851] res2a_branch2b_param_0(0.62) 
I0630 08:42:16.626065 29777 net.cpp:1851] res3a_branch2a_param_0(0.62) 
I0630 08:42:16.626073 29777 net.cpp:1851] res3a_branch2b_param_0(0.62) 
I0630 08:42:16.626080 29777 net.cpp:1851] res4a_branch2a_param_0(0.62) 
I0630 08:42:16.626088 29777 net.cpp:1851] res4a_branch2b_param_0(0.62) 
I0630 08:42:16.626096 29777 net.cpp:1851] res5a_branch2a_param_0(0.62) 
I0630 08:42:16.626104 29777 net.cpp:1851] res5a_branch2b_param_0(0.62) 
I0630 08:42:16.626111 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.45921e+06/2.86678e+06) 0.509
I0630 08:42:16.787721 29777 solver.cpp:290] Iteration 123000 (6.2306 iter/s, 16.0498s/100 iter), loss = 0.75
I0630 08:42:16.787747 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 08:42:16.787755 29777 sgd_solver.cpp:106] Iteration 123000, lr = 0.00615625
I0630 08:42:32.954603 29777 solver.cpp:290] Iteration 123100 (6.18567 iter/s, 16.1664s/100 iter), loss = 0.797619
I0630 08:42:32.954627 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 08:42:32.954634 29777 sgd_solver.cpp:106] Iteration 123100, lr = 0.00615312
I0630 08:42:49.056402 29777 solver.cpp:290] Iteration 123200 (6.21067 iter/s, 16.1013s/100 iter), loss = 1.14286
I0630 08:42:49.056506 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 08:42:49.056529 29777 sgd_solver.cpp:106] Iteration 123200, lr = 0.00615
I0630 08:43:05.325013 29777 solver.cpp:290] Iteration 123300 (6.14702 iter/s, 16.2681s/100 iter), loss = 0.857143
I0630 08:43:05.325073 29777 solver.cpp:309]     Train net output #0: loss = 0.690476 (* 1 = 0.690476 loss)
I0630 08:43:05.325104 29777 sgd_solver.cpp:106] Iteration 123300, lr = 0.00614687
I0630 08:43:21.608083 29777 solver.cpp:290] Iteration 123400 (6.14154 iter/s, 16.2826s/100 iter), loss = 1.44048
I0630 08:43:21.614898 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 08:43:21.614922 29777 sgd_solver.cpp:106] Iteration 123400, lr = 0.00614375
I0630 08:43:37.889539 29777 solver.cpp:290] Iteration 123500 (6.14469 iter/s, 16.2742s/100 iter), loss = 0.97619
I0630 08:43:37.889564 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 08:43:37.889570 29777 sgd_solver.cpp:106] Iteration 123500, lr = 0.00614062
I0630 08:43:54.081073 29777 solver.cpp:290] Iteration 123600 (6.17628 iter/s, 16.191s/100 iter), loss = 1.14286
I0630 08:43:54.081444 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 08:43:54.081625 29777 sgd_solver.cpp:106] Iteration 123600, lr = 0.0061375
I0630 08:44:10.399248 29777 solver.cpp:290] Iteration 123700 (6.12842 iter/s, 16.3174s/100 iter), loss = 1.33333
I0630 08:44:10.399271 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 08:44:10.399278 29777 sgd_solver.cpp:106] Iteration 123700, lr = 0.00613437
I0630 08:44:26.913446 29777 solver.cpp:290] Iteration 123800 (6.05557 iter/s, 16.5137s/100 iter), loss = 1.40476
I0630 08:44:26.913606 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 08:44:26.913626 29777 sgd_solver.cpp:106] Iteration 123800, lr = 0.00613125
I0630 08:44:43.275666 29777 solver.cpp:290] Iteration 123900 (6.11187 iter/s, 16.3616s/100 iter), loss = 0.97619
I0630 08:44:43.275732 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 08:44:43.275769 29777 sgd_solver.cpp:106] Iteration 123900, lr = 0.00612812
I0630 08:44:59.363989 29777 solver.cpp:354] Sparsity after update:
I0630 08:44:59.365947 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:44:59.365963 29777 net.cpp:1851] conv1a_param_0(0.31) 
I0630 08:44:59.365980 29777 net.cpp:1851] conv1b_param_0(0.62) 
I0630 08:44:59.365986 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:44:59.365991 29777 net.cpp:1851] res2a_branch2a_param_0(0.62) 
I0630 08:44:59.365998 29777 net.cpp:1851] res2a_branch2b_param_0(0.62) 
I0630 08:44:59.366003 29777 net.cpp:1851] res3a_branch2a_param_0(0.62) 
I0630 08:44:59.366008 29777 net.cpp:1851] res3a_branch2b_param_0(0.62) 
I0630 08:44:59.366014 29777 net.cpp:1851] res4a_branch2a_param_0(0.62) 
I0630 08:44:59.366019 29777 net.cpp:1851] res4a_branch2b_param_0(0.62) 
I0630 08:44:59.366024 29777 net.cpp:1851] res5a_branch2a_param_0(0.62) 
I0630 08:44:59.366029 29777 net.cpp:1851] res5a_branch2b_param_0(0.62) 
I0630 08:44:59.366037 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.45921e+06/2.86678e+06) 0.509
I0630 08:44:59.366143 29777 solver.cpp:471] Iteration 124000, Testing net (#0)
I0630 08:45:15.595832 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 08:46:13.628177 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.57044
I0630 08:46:13.628262 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.804482
I0630 08:46:13.628278 29777 solver.cpp:544]     Test net output #2: loss = 1.53716 (* 1 = 1.53716 loss)
I0630 08:46:13.834507 29777 solver.cpp:290] Iteration 124000 (1.10429 iter/s, 90.5563s/100 iter), loss = 0.940476
I0630 08:46:13.834540 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 08:46:13.834552 29777 sgd_solver.cpp:106] Iteration 124000, lr = 0.006125
I0630 08:46:13.835953 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.63
I0630 08:46:14.589928 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 08:46:30.839817 29777 solver.cpp:290] Iteration 124100 (5.88069 iter/s, 17.0048s/100 iter), loss = 1.20238
I0630 08:46:30.839843 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 08:46:30.839849 29777 sgd_solver.cpp:106] Iteration 124100, lr = 0.00612187
I0630 08:46:47.378805 29777 solver.cpp:290] Iteration 124200 (6.04649 iter/s, 16.5385s/100 iter), loss = 1.21429
I0630 08:46:47.379110 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 08:46:47.379232 29777 sgd_solver.cpp:106] Iteration 124200, lr = 0.00611875
I0630 08:47:03.790946 29777 solver.cpp:290] Iteration 124300 (6.09333 iter/s, 16.4114s/100 iter), loss = 1.36905
I0630 08:47:03.790977 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 08:47:03.790987 29777 sgd_solver.cpp:106] Iteration 124300, lr = 0.00611562
I0630 08:47:20.036160 29777 solver.cpp:290] Iteration 124400 (6.15584 iter/s, 16.2447s/100 iter), loss = 0.97619
I0630 08:47:20.036263 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 08:47:20.036273 29777 sgd_solver.cpp:106] Iteration 124400, lr = 0.0061125
I0630 08:47:36.261740 29777 solver.cpp:290] Iteration 124500 (6.16331 iter/s, 16.225s/100 iter), loss = 1.04762
I0630 08:47:36.261770 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 08:47:36.261782 29777 sgd_solver.cpp:106] Iteration 124500, lr = 0.00610937
I0630 08:47:52.570355 29777 solver.cpp:290] Iteration 124600 (6.13191 iter/s, 16.3081s/100 iter), loss = 0.988095
I0630 08:47:52.570469 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 08:47:52.570497 29777 sgd_solver.cpp:106] Iteration 124600, lr = 0.00610625
I0630 08:48:08.837975 29777 solver.cpp:290] Iteration 124700 (6.14739 iter/s, 16.2671s/100 iter), loss = 1.32143
I0630 08:48:08.838006 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 08:48:08.838016 29777 sgd_solver.cpp:106] Iteration 124700, lr = 0.00610312
I0630 08:48:25.160842 29777 solver.cpp:290] Iteration 124800 (6.12655 iter/s, 16.3224s/100 iter), loss = 1.11905
I0630 08:48:25.160959 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 08:48:25.160969 29777 sgd_solver.cpp:106] Iteration 124800, lr = 0.0061
I0630 08:48:41.429492 29777 solver.cpp:290] Iteration 124900 (6.14701 iter/s, 16.2681s/100 iter), loss = 1.44048
I0630 08:48:41.429584 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 08:48:41.429616 29777 sgd_solver.cpp:106] Iteration 124900, lr = 0.00609687
I0630 08:48:57.484591 29777 solver.cpp:354] Sparsity after update:
I0630 08:48:57.504988 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:48:57.505003 29777 net.cpp:1851] conv1a_param_0(0.315) 
I0630 08:48:57.505013 29777 net.cpp:1851] conv1b_param_0(0.63) 
I0630 08:48:57.505017 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:48:57.505020 29777 net.cpp:1851] res2a_branch2a_param_0(0.63) 
I0630 08:48:57.505023 29777 net.cpp:1851] res2a_branch2b_param_0(0.63) 
I0630 08:48:57.505026 29777 net.cpp:1851] res3a_branch2a_param_0(0.63) 
I0630 08:48:57.505029 29777 net.cpp:1851] res3a_branch2b_param_0(0.63) 
I0630 08:48:57.505033 29777 net.cpp:1851] res4a_branch2a_param_0(0.63) 
I0630 08:48:57.505036 29777 net.cpp:1851] res4a_branch2b_param_0(0.63) 
I0630 08:48:57.505039 29777 net.cpp:1851] res5a_branch2a_param_0(0.63) 
I0630 08:48:57.505043 29777 net.cpp:1851] res5a_branch2b_param_0(0.63) 
I0630 08:48:57.505045 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.48275e+06/2.86678e+06) 0.517
I0630 08:48:57.662609 29777 solver.cpp:290] Iteration 125000 (6.16044 iter/s, 16.2326s/100 iter), loss = 1.14286
I0630 08:48:57.662636 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 08:48:57.662642 29777 sgd_solver.cpp:106] Iteration 125000, lr = 0.00609375
I0630 08:49:13.792953 29777 solver.cpp:290] Iteration 125100 (6.19967 iter/s, 16.1299s/100 iter), loss = 1.22619
I0630 08:49:13.792974 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 08:49:13.792982 29777 sgd_solver.cpp:106] Iteration 125100, lr = 0.00609062
I0630 08:49:30.143087 29777 solver.cpp:290] Iteration 125200 (6.11633 iter/s, 16.3497s/100 iter), loss = 0.928571
I0630 08:49:30.143174 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 08:49:30.143182 29777 sgd_solver.cpp:106] Iteration 125200, lr = 0.0060875
I0630 08:49:46.457427 29777 solver.cpp:290] Iteration 125300 (6.12978 iter/s, 16.3138s/100 iter), loss = 1.13095
I0630 08:49:46.457532 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 08:49:46.457572 29777 sgd_solver.cpp:106] Iteration 125300, lr = 0.00608438
I0630 08:50:02.671777 29777 solver.cpp:290] Iteration 125400 (6.16758 iter/s, 16.2138s/100 iter), loss = 1.38095
I0630 08:50:02.671916 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 08:50:02.671962 29777 sgd_solver.cpp:106] Iteration 125400, lr = 0.00608125
I0630 08:50:18.986888 29777 solver.cpp:290] Iteration 125500 (6.1295 iter/s, 16.3145s/100 iter), loss = 1.20238
I0630 08:50:18.986994 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 08:50:18.987028 29777 sgd_solver.cpp:106] Iteration 125500, lr = 0.00607812
I0630 08:50:35.308106 29777 solver.cpp:290] Iteration 125600 (6.1272 iter/s, 16.3207s/100 iter), loss = 1.11905
I0630 08:50:35.308181 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 08:50:35.308187 29777 sgd_solver.cpp:106] Iteration 125600, lr = 0.006075
I0630 08:50:51.730250 29777 solver.cpp:290] Iteration 125700 (6.08953 iter/s, 16.4216s/100 iter), loss = 0.821428
I0630 08:50:51.730306 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 08:50:51.730329 29777 sgd_solver.cpp:106] Iteration 125700, lr = 0.00607187
I0630 08:51:08.031261 29777 solver.cpp:290] Iteration 125800 (6.13477 iter/s, 16.3005s/100 iter), loss = 0.952381
I0630 08:51:08.031344 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 08:51:08.031352 29777 sgd_solver.cpp:106] Iteration 125800, lr = 0.00606875
I0630 08:51:24.167966 29777 solver.cpp:290] Iteration 125900 (6.19725 iter/s, 16.1362s/100 iter), loss = 1.0119
I0630 08:51:24.167992 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 08:51:24.167999 29777 sgd_solver.cpp:106] Iteration 125900, lr = 0.00606562
I0630 08:51:40.312041 29777 solver.cpp:354] Sparsity after update:
I0630 08:51:40.313486 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:51:40.313494 29777 net.cpp:1851] conv1a_param_0(0.315) 
I0630 08:51:40.313503 29777 net.cpp:1851] conv1b_param_0(0.63) 
I0630 08:51:40.313504 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:51:40.313508 29777 net.cpp:1851] res2a_branch2a_param_0(0.63) 
I0630 08:51:40.313509 29777 net.cpp:1851] res2a_branch2b_param_0(0.63) 
I0630 08:51:40.313511 29777 net.cpp:1851] res3a_branch2a_param_0(0.63) 
I0630 08:51:40.313514 29777 net.cpp:1851] res3a_branch2b_param_0(0.63) 
I0630 08:51:40.313516 29777 net.cpp:1851] res4a_branch2a_param_0(0.63) 
I0630 08:51:40.313519 29777 net.cpp:1851] res4a_branch2b_param_0(0.63) 
I0630 08:51:40.313521 29777 net.cpp:1851] res5a_branch2a_param_0(0.63) 
I0630 08:51:40.313524 29777 net.cpp:1851] res5a_branch2b_param_0(0.63) 
I0630 08:51:40.313525 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.48275e+06/2.86678e+06) 0.517
I0630 08:51:40.313612 29777 solver.cpp:471] Iteration 126000, Testing net (#0)
I0630 08:51:54.066119 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 08:52:45.013309 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5678
I0630 08:52:45.013412 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.802161
I0630 08:52:45.013422 29777 solver.cpp:544]     Test net output #2: loss = 1.53408 (* 1 = 1.53408 loss)
I0630 08:52:45.204396 29777 solver.cpp:290] Iteration 126000 (1.23405 iter/s, 81.0342s/100 iter), loss = 1
I0630 08:52:45.204432 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 08:52:45.204442 29777 sgd_solver.cpp:106] Iteration 126000, lr = 0.0060625
I0630 08:52:45.205538 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.64
I0630 08:52:45.733831 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 08:53:01.805825 29777 solver.cpp:290] Iteration 126100 (6.02376 iter/s, 16.6009s/100 iter), loss = 1.02381
I0630 08:53:01.805915 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 08:53:01.805946 29777 sgd_solver.cpp:106] Iteration 126100, lr = 0.00605937
I0630 08:53:17.981392 29777 solver.cpp:290] Iteration 126200 (6.18237 iter/s, 16.175s/100 iter), loss = 1.04762
I0630 08:53:17.981791 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 08:53:17.981801 29777 sgd_solver.cpp:106] Iteration 126200, lr = 0.00605625
I0630 08:53:34.086359 29777 solver.cpp:290] Iteration 126300 (6.20959 iter/s, 16.1041s/100 iter), loss = 1.07143
I0630 08:53:34.086381 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 08:53:34.086387 29777 sgd_solver.cpp:106] Iteration 126300, lr = 0.00605312
I0630 08:53:50.187671 29777 solver.cpp:290] Iteration 126400 (6.21085 iter/s, 16.1008s/100 iter), loss = 1.66667
I0630 08:53:50.187765 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 08:53:50.187777 29777 sgd_solver.cpp:106] Iteration 126400, lr = 0.00605
I0630 08:54:06.299789 29777 solver.cpp:290] Iteration 126500 (6.20671 iter/s, 16.1116s/100 iter), loss = 1.38095
I0630 08:54:06.299811 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 08:54:06.299818 29777 sgd_solver.cpp:106] Iteration 126500, lr = 0.00604687
I0630 08:54:22.445708 29777 solver.cpp:290] Iteration 126600 (6.19369 iter/s, 16.1455s/100 iter), loss = 1.10714
I0630 08:54:22.445787 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 08:54:22.445799 29777 sgd_solver.cpp:106] Iteration 126600, lr = 0.00604375
I0630 08:54:38.549582 29777 solver.cpp:290] Iteration 126700 (6.20989 iter/s, 16.1034s/100 iter), loss = 1.5
I0630 08:54:38.549607 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 08:54:38.549615 29777 sgd_solver.cpp:106] Iteration 126700, lr = 0.00604062
I0630 08:54:54.659253 29777 solver.cpp:290] Iteration 126800 (6.20763 iter/s, 16.1092s/100 iter), loss = 1.35714
I0630 08:54:54.659323 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 08:54:54.659332 29777 sgd_solver.cpp:106] Iteration 126800, lr = 0.0060375
I0630 08:55:10.901685 29777 solver.cpp:290] Iteration 126900 (6.15691 iter/s, 16.2419s/100 iter), loss = 0.988095
I0630 08:55:10.901715 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 08:55:10.901722 29777 sgd_solver.cpp:106] Iteration 126900, lr = 0.00603438
I0630 08:55:26.861246 29777 solver.cpp:354] Sparsity after update:
I0630 08:55:26.882738 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:55:26.882760 29777 net.cpp:1851] conv1a_param_0(0.32) 
I0630 08:55:26.882771 29777 net.cpp:1851] conv1b_param_0(0.64) 
I0630 08:55:26.882773 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:55:26.882777 29777 net.cpp:1851] res2a_branch2a_param_0(0.64) 
I0630 08:55:26.882783 29777 net.cpp:1851] res2a_branch2b_param_0(0.64) 
I0630 08:55:26.882786 29777 net.cpp:1851] res3a_branch2a_param_0(0.64) 
I0630 08:55:26.882789 29777 net.cpp:1851] res3a_branch2b_param_0(0.64) 
I0630 08:55:26.882792 29777 net.cpp:1851] res4a_branch2a_param_0(0.64) 
I0630 08:55:26.882797 29777 net.cpp:1851] res4a_branch2b_param_0(0.64) 
I0630 08:55:26.882800 29777 net.cpp:1851] res5a_branch2a_param_0(0.64) 
I0630 08:55:26.882804 29777 net.cpp:1851] res5a_branch2b_param_0(0.64) 
I0630 08:55:26.882808 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.50629e+06/2.86678e+06) 0.525
I0630 08:55:27.050297 29777 solver.cpp:290] Iteration 127000 (6.19266 iter/s, 16.1481s/100 iter), loss = 1.07143
I0630 08:55:27.050323 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 08:55:27.050331 29777 sgd_solver.cpp:106] Iteration 127000, lr = 0.00603125
I0630 08:55:43.114858 29777 solver.cpp:290] Iteration 127100 (6.22506 iter/s, 16.0641s/100 iter), loss = 0.952381
I0630 08:55:43.114881 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 08:55:43.114887 29777 sgd_solver.cpp:106] Iteration 127100, lr = 0.00602813
I0630 08:55:59.792313 29777 solver.cpp:290] Iteration 127200 (5.99629 iter/s, 16.677s/100 iter), loss = 1.53571
I0630 08:55:59.798913 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 08:55:59.798943 29777 sgd_solver.cpp:106] Iteration 127200, lr = 0.006025
I0630 08:56:15.943640 29777 solver.cpp:290] Iteration 127300 (6.19414 iter/s, 16.1443s/100 iter), loss = 1.34524
I0630 08:56:15.943665 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 08:56:15.943671 29777 sgd_solver.cpp:106] Iteration 127300, lr = 0.00602187
I0630 08:56:32.055953 29777 solver.cpp:290] Iteration 127400 (6.20661 iter/s, 16.1118s/100 iter), loss = 1.61905
I0630 08:56:32.056066 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 08:56:32.056076 29777 sgd_solver.cpp:106] Iteration 127400, lr = 0.00601875
I0630 08:56:48.160037 29777 solver.cpp:290] Iteration 127500 (6.20982 iter/s, 16.1035s/100 iter), loss = 1.07143
I0630 08:56:48.160073 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 08:56:48.160082 29777 sgd_solver.cpp:106] Iteration 127500, lr = 0.00601562
I0630 08:57:04.178724 29777 solver.cpp:290] Iteration 127600 (6.24289 iter/s, 16.0182s/100 iter), loss = 1.02381
I0630 08:57:04.178856 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 08:57:04.178874 29777 sgd_solver.cpp:106] Iteration 127600, lr = 0.0060125
I0630 08:57:20.130977 29777 solver.cpp:290] Iteration 127700 (6.26893 iter/s, 15.9517s/100 iter), loss = 0.821428
I0630 08:57:20.131001 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 08:57:20.131011 29777 sgd_solver.cpp:106] Iteration 127700, lr = 0.00600937
I0630 08:57:36.282450 29777 solver.cpp:290] Iteration 127800 (6.19157 iter/s, 16.151s/100 iter), loss = 1.04762
I0630 08:57:36.282546 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 08:57:36.282567 29777 sgd_solver.cpp:106] Iteration 127800, lr = 0.00600625
I0630 08:57:52.302196 29777 solver.cpp:290] Iteration 127900 (6.2425 iter/s, 16.0192s/100 iter), loss = 1
I0630 08:57:52.302227 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 08:57:52.302234 29777 sgd_solver.cpp:106] Iteration 127900, lr = 0.00600312
I0630 08:58:08.301021 29777 solver.cpp:354] Sparsity after update:
I0630 08:58:08.302662 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 08:58:08.302670 29777 net.cpp:1851] conv1a_param_0(0.32) 
I0630 08:58:08.302677 29777 net.cpp:1851] conv1b_param_0(0.64) 
I0630 08:58:08.302680 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 08:58:08.302682 29777 net.cpp:1851] res2a_branch2a_param_0(0.64) 
I0630 08:58:08.302685 29777 net.cpp:1851] res2a_branch2b_param_0(0.64) 
I0630 08:58:08.302686 29777 net.cpp:1851] res3a_branch2a_param_0(0.64) 
I0630 08:58:08.302688 29777 net.cpp:1851] res3a_branch2b_param_0(0.64) 
I0630 08:58:08.302691 29777 net.cpp:1851] res4a_branch2a_param_0(0.64) 
I0630 08:58:08.302693 29777 net.cpp:1851] res4a_branch2b_param_0(0.64) 
I0630 08:58:08.302695 29777 net.cpp:1851] res5a_branch2a_param_0(0.64) 
I0630 08:58:08.302698 29777 net.cpp:1851] res5a_branch2b_param_0(0.64) 
I0630 08:58:08.302700 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.50629e+06/2.86678e+06) 0.525
I0630 08:58:08.302788 29777 solver.cpp:471] Iteration 128000, Testing net (#0)
I0630 08:58:20.508844 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 08:59:08.896580 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5667
I0630 08:59:08.896620 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.801702
I0630 08:59:08.896625 29777 solver.cpp:544]     Test net output #2: loss = 1.53722 (* 1 = 1.53722 loss)
I0630 08:59:09.070977 29777 solver.cpp:290] Iteration 128000 (1.30265 iter/s, 76.7667s/100 iter), loss = 1.53571
I0630 08:59:09.071008 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 08:59:09.071015 29777 sgd_solver.cpp:106] Iteration 128000, lr = 0.006
I0630 08:59:09.071756 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.65
I0630 08:59:09.548189 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 08:59:25.609868 29777 solver.cpp:290] Iteration 128100 (6.04653 iter/s, 16.5384s/100 iter), loss = 1.07143
I0630 08:59:25.609895 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 08:59:25.609941 29777 sgd_solver.cpp:106] Iteration 128100, lr = 0.00599687
I0630 08:59:41.600159 29777 solver.cpp:290] Iteration 128200 (6.25398 iter/s, 15.9898s/100 iter), loss = 1.35714
I0630 08:59:41.600215 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 08:59:41.600227 29777 sgd_solver.cpp:106] Iteration 128200, lr = 0.00599375
I0630 08:59:57.669185 29777 solver.cpp:290] Iteration 128300 (6.22334 iter/s, 16.0685s/100 iter), loss = 1.25
I0630 08:59:57.669214 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 08:59:57.669224 29777 sgd_solver.cpp:106] Iteration 128300, lr = 0.00599062
I0630 09:00:13.805974 29777 solver.cpp:290] Iteration 128400 (6.1972 iter/s, 16.1363s/100 iter), loss = 1.34524
I0630 09:00:13.806088 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 09:00:13.806100 29777 sgd_solver.cpp:106] Iteration 128400, lr = 0.0059875
I0630 09:00:30.023108 29777 solver.cpp:290] Iteration 128500 (6.16653 iter/s, 16.2166s/100 iter), loss = 1.05952
I0630 09:00:30.023130 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 09:00:30.023140 29777 sgd_solver.cpp:106] Iteration 128500, lr = 0.00598437
I0630 09:00:46.249060 29777 solver.cpp:290] Iteration 128600 (6.16315 iter/s, 16.2255s/100 iter), loss = 1.02381
I0630 09:00:46.249135 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 09:00:46.249147 29777 sgd_solver.cpp:106] Iteration 128600, lr = 0.00598125
I0630 09:01:02.726933 29777 solver.cpp:290] Iteration 128700 (6.06894 iter/s, 16.4774s/100 iter), loss = 0.97619
I0630 09:01:02.726955 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 09:01:02.726963 29777 sgd_solver.cpp:106] Iteration 128700, lr = 0.00597813
I0630 09:01:18.912879 29777 solver.cpp:290] Iteration 128800 (6.17838 iter/s, 16.1855s/100 iter), loss = 1.22619
I0630 09:01:18.912938 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 09:01:18.912950 29777 sgd_solver.cpp:106] Iteration 128800, lr = 0.005975
I0630 09:01:34.927116 29777 solver.cpp:290] Iteration 128900 (6.24464 iter/s, 16.0137s/100 iter), loss = 1.15476
I0630 09:01:34.927140 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 09:01:34.927147 29777 sgd_solver.cpp:106] Iteration 128900, lr = 0.00597188
I0630 09:01:51.060925 29777 solver.cpp:354] Sparsity after update:
I0630 09:01:51.081272 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:01:51.081288 29777 net.cpp:1851] conv1a_param_0(0.325) 
I0630 09:01:51.081298 29777 net.cpp:1851] conv1b_param_0(0.65) 
I0630 09:01:51.081302 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:01:51.081305 29777 net.cpp:1851] res2a_branch2a_param_0(0.65) 
I0630 09:01:51.081310 29777 net.cpp:1851] res2a_branch2b_param_0(0.65) 
I0630 09:01:51.081312 29777 net.cpp:1851] res3a_branch2a_param_0(0.65) 
I0630 09:01:51.081315 29777 net.cpp:1851] res3a_branch2b_param_0(0.65) 
I0630 09:01:51.081320 29777 net.cpp:1851] res4a_branch2a_param_0(0.65) 
I0630 09:01:51.081322 29777 net.cpp:1851] res4a_branch2b_param_0(0.65) 
I0630 09:01:51.081326 29777 net.cpp:1851] res5a_branch2a_param_0(0.65) 
I0630 09:01:51.081329 29777 net.cpp:1851] res5a_branch2b_param_0(0.65) 
I0630 09:01:51.081332 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.52982e+06/2.86678e+06) 0.534
I0630 09:01:51.242887 29777 solver.cpp:290] Iteration 129000 (6.12922 iter/s, 16.3153s/100 iter), loss = 1.47619
I0630 09:01:51.243005 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 09:01:51.243058 29777 sgd_solver.cpp:106] Iteration 129000, lr = 0.00596875
I0630 09:02:07.666628 29777 solver.cpp:290] Iteration 129100 (6.08895 iter/s, 16.4232s/100 iter), loss = 1.33333
I0630 09:02:07.666693 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 09:02:07.666713 29777 sgd_solver.cpp:106] Iteration 129100, lr = 0.00596562
I0630 09:02:24.151291 29777 solver.cpp:290] Iteration 129200 (6.06643 iter/s, 16.4842s/100 iter), loss = 1
I0630 09:02:24.151396 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 09:02:24.151406 29777 sgd_solver.cpp:106] Iteration 129200, lr = 0.0059625
I0630 09:02:40.207111 29777 solver.cpp:290] Iteration 129300 (6.22848 iter/s, 16.0553s/100 iter), loss = 0.928571
I0630 09:02:40.207144 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 09:02:40.207154 29777 sgd_solver.cpp:106] Iteration 129300, lr = 0.00595937
I0630 09:02:56.455726 29777 solver.cpp:290] Iteration 129400 (6.15455 iter/s, 16.2481s/100 iter), loss = 1.27381
I0630 09:02:56.455843 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 09:02:56.455853 29777 sgd_solver.cpp:106] Iteration 129400, lr = 0.00595625
I0630 09:03:12.853202 29777 solver.cpp:290] Iteration 129500 (6.09871 iter/s, 16.3969s/100 iter), loss = 1.02381
I0630 09:03:12.853226 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 09:03:12.853233 29777 sgd_solver.cpp:106] Iteration 129500, lr = 0.00595312
I0630 09:03:29.299922 29777 solver.cpp:290] Iteration 129600 (6.08042 iter/s, 16.4462s/100 iter), loss = 0.916667
I0630 09:03:29.300027 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 09:03:29.300040 29777 sgd_solver.cpp:106] Iteration 129600, lr = 0.00595
I0630 09:03:45.425953 29777 solver.cpp:290] Iteration 129700 (6.20136 iter/s, 16.1255s/100 iter), loss = 1.36905
I0630 09:03:45.425978 29777 solver.cpp:309]     Train net output #0: loss = 2.04762 (* 1 = 2.04762 loss)
I0630 09:03:45.425987 29777 sgd_solver.cpp:106] Iteration 129700, lr = 0.00594687
I0630 09:04:01.646579 29777 solver.cpp:290] Iteration 129800 (6.16517 iter/s, 16.2202s/100 iter), loss = 1.13095
I0630 09:04:01.646684 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 09:04:01.646694 29777 sgd_solver.cpp:106] Iteration 129800, lr = 0.00594375
I0630 09:04:17.787869 29777 solver.cpp:290] Iteration 129900 (6.1955 iter/s, 16.1407s/100 iter), loss = 1.21429
I0630 09:04:17.787894 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 09:04:17.787900 29777 sgd_solver.cpp:106] Iteration 129900, lr = 0.00594062
I0630 09:04:33.992607 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_130000.caffemodel
I0630 09:04:34.011883 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_130000.solverstate
I0630 09:04:34.020962 29777 solver.cpp:354] Sparsity after update:
I0630 09:04:34.021946 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:04:34.021955 29777 net.cpp:1851] conv1a_param_0(0.325) 
I0630 09:04:34.021961 29777 net.cpp:1851] conv1b_param_0(0.65) 
I0630 09:04:34.021965 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:04:34.021966 29777 net.cpp:1851] res2a_branch2a_param_0(0.65) 
I0630 09:04:34.021968 29777 net.cpp:1851] res2a_branch2b_param_0(0.65) 
I0630 09:04:34.021970 29777 net.cpp:1851] res3a_branch2a_param_0(0.65) 
I0630 09:04:34.021972 29777 net.cpp:1851] res3a_branch2b_param_0(0.65) 
I0630 09:04:34.021975 29777 net.cpp:1851] res4a_branch2a_param_0(0.65) 
I0630 09:04:34.021976 29777 net.cpp:1851] res4a_branch2b_param_0(0.65) 
I0630 09:04:34.021977 29777 net.cpp:1851] res5a_branch2a_param_0(0.65) 
I0630 09:04:34.021980 29777 net.cpp:1851] res5a_branch2b_param_0(0.65) 
I0630 09:04:34.021982 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.52982e+06/2.86678e+06) 0.534
I0630 09:04:34.022084 29777 solver.cpp:471] Iteration 130000, Testing net (#0)
I0630 09:04:48.376539 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 09:05:42.467036 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56354
I0630 09:05:42.467134 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.801322
I0630 09:05:42.467159 29777 solver.cpp:544]     Test net output #2: loss = 1.5496 (* 1 = 1.5496 loss)
I0630 09:05:42.704638 29777 solver.cpp:290] Iteration 130000 (1.17766 iter/s, 84.9144s/100 iter), loss = 1.08333
I0630 09:05:42.704701 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 09:05:42.704720 29777 sgd_solver.cpp:106] Iteration 130000, lr = 0.0059375
I0630 09:05:42.706704 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.66
I0630 09:05:43.382153 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 09:05:59.733119 29777 solver.cpp:290] Iteration 130100 (5.87269 iter/s, 17.028s/100 iter), loss = 1.30952
I0630 09:05:59.733141 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 09:05:59.733148 29777 sgd_solver.cpp:106] Iteration 130100, lr = 0.00593437
I0630 09:06:15.899345 29777 solver.cpp:290] Iteration 130200 (6.18591 iter/s, 16.1658s/100 iter), loss = 1.02381
I0630 09:06:15.899443 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 09:06:15.899452 29777 sgd_solver.cpp:106] Iteration 130200, lr = 0.00593125
I0630 09:06:32.407945 29777 solver.cpp:290] Iteration 130300 (6.05765 iter/s, 16.508s/100 iter), loss = 1.25
I0630 09:06:32.407971 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 09:06:32.407980 29777 sgd_solver.cpp:106] Iteration 130300, lr = 0.00592813
I0630 09:06:48.678009 29777 solver.cpp:290] Iteration 130400 (6.14644 iter/s, 16.2696s/100 iter), loss = 0.833333
I0630 09:06:48.678133 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 09:06:48.678156 29777 sgd_solver.cpp:106] Iteration 130400, lr = 0.005925
I0630 09:07:04.787436 29777 solver.cpp:290] Iteration 130500 (6.20776 iter/s, 16.1089s/100 iter), loss = 1.17857
I0630 09:07:04.787461 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 09:07:04.787469 29777 sgd_solver.cpp:106] Iteration 130500, lr = 0.00592188
I0630 09:07:20.874743 29777 solver.cpp:290] Iteration 130600 (6.21626 iter/s, 16.0868s/100 iter), loss = 0.880952
I0630 09:07:20.874876 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 09:07:20.874897 29777 sgd_solver.cpp:106] Iteration 130600, lr = 0.00591875
I0630 09:07:37.012239 29777 solver.cpp:290] Iteration 130700 (6.19697 iter/s, 16.1369s/100 iter), loss = 1.53571
I0630 09:07:37.012265 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 09:07:37.012274 29777 sgd_solver.cpp:106] Iteration 130700, lr = 0.00591563
I0630 09:07:53.199587 29777 solver.cpp:290] Iteration 130800 (6.17784 iter/s, 16.1869s/100 iter), loss = 1.21429
I0630 09:07:53.199678 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 09:07:53.199689 29777 sgd_solver.cpp:106] Iteration 130800, lr = 0.0059125
I0630 09:08:09.310163 29777 solver.cpp:290] Iteration 130900 (6.20731 iter/s, 16.11s/100 iter), loss = 0.940476
I0630 09:08:09.310189 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 09:08:09.310195 29777 sgd_solver.cpp:106] Iteration 130900, lr = 0.00590937
I0630 09:08:25.320837 29777 solver.cpp:354] Sparsity after update:
I0630 09:08:25.341207 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:08:25.341223 29777 net.cpp:1851] conv1a_param_0(0.33) 
I0630 09:08:25.341233 29777 net.cpp:1851] conv1b_param_0(0.66) 
I0630 09:08:25.341236 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:08:25.341240 29777 net.cpp:1851] res2a_branch2a_param_0(0.66) 
I0630 09:08:25.341243 29777 net.cpp:1851] res2a_branch2b_param_0(0.66) 
I0630 09:08:25.341246 29777 net.cpp:1851] res3a_branch2a_param_0(0.66) 
I0630 09:08:25.341250 29777 net.cpp:1851] res3a_branch2b_param_0(0.66) 
I0630 09:08:25.341254 29777 net.cpp:1851] res4a_branch2a_param_0(0.66) 
I0630 09:08:25.341258 29777 net.cpp:1851] res4a_branch2b_param_0(0.66) 
I0630 09:08:25.341260 29777 net.cpp:1851] res5a_branch2a_param_0(0.66) 
I0630 09:08:25.341264 29777 net.cpp:1851] res5a_branch2b_param_0(0.66) 
I0630 09:08:25.341266 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.55336e+06/2.86678e+06) 0.542
I0630 09:08:25.497954 29777 solver.cpp:290] Iteration 131000 (6.17767 iter/s, 16.1873s/100 iter), loss = 1.0119
I0630 09:08:25.497982 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 09:08:25.497990 29777 sgd_solver.cpp:106] Iteration 131000, lr = 0.00590625
I0630 09:08:41.597138 29777 solver.cpp:290] Iteration 131100 (6.21168 iter/s, 16.0987s/100 iter), loss = 0.833333
I0630 09:08:41.597265 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 09:08:41.597303 29777 sgd_solver.cpp:106] Iteration 131100, lr = 0.00590312
I0630 09:08:57.795806 29777 solver.cpp:290] Iteration 131200 (6.17356 iter/s, 16.1981s/100 iter), loss = 1.36905
I0630 09:08:57.795891 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 09:08:57.795898 29777 sgd_solver.cpp:106] Iteration 131200, lr = 0.0059
I0630 09:09:14.127984 29777 solver.cpp:290] Iteration 131300 (6.12308 iter/s, 16.3316s/100 iter), loss = 1.22619
I0630 09:09:14.128031 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 09:09:14.128060 29777 sgd_solver.cpp:106] Iteration 131300, lr = 0.00589687
I0630 09:09:30.177973 29777 solver.cpp:290] Iteration 131400 (6.23072 iter/s, 16.0495s/100 iter), loss = 0.916667
I0630 09:09:30.178045 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 09:09:30.178052 29777 sgd_solver.cpp:106] Iteration 131400, lr = 0.00589375
I0630 09:09:46.272634 29777 solver.cpp:290] Iteration 131500 (6.21344 iter/s, 16.0942s/100 iter), loss = 0.904762
I0630 09:09:46.272660 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 09:09:46.272670 29777 sgd_solver.cpp:106] Iteration 131500, lr = 0.00589063
I0630 09:10:02.421627 29777 solver.cpp:290] Iteration 131600 (6.19251 iter/s, 16.1485s/100 iter), loss = 0.988095
I0630 09:10:02.421720 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 09:10:02.421731 29777 sgd_solver.cpp:106] Iteration 131600, lr = 0.0058875
I0630 09:10:18.451019 29777 solver.cpp:290] Iteration 131700 (6.23875 iter/s, 16.0289s/100 iter), loss = 1.21429
I0630 09:10:18.451045 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 09:10:18.451055 29777 sgd_solver.cpp:106] Iteration 131700, lr = 0.00588437
I0630 09:10:34.595127 29777 solver.cpp:290] Iteration 131800 (6.19439 iter/s, 16.1436s/100 iter), loss = 1.38095
I0630 09:10:34.595232 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 09:10:34.595242 29777 sgd_solver.cpp:106] Iteration 131800, lr = 0.00588125
I0630 09:10:50.614478 29777 solver.cpp:290] Iteration 131900 (6.24266 iter/s, 16.0188s/100 iter), loss = 1.14286
I0630 09:10:50.614504 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 09:10:50.614512 29777 sgd_solver.cpp:106] Iteration 131900, lr = 0.00587813
I0630 09:11:06.651406 29777 solver.cpp:354] Sparsity after update:
I0630 09:11:06.652858 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:11:06.652865 29777 net.cpp:1851] conv1a_param_0(0.33) 
I0630 09:11:06.652873 29777 net.cpp:1851] conv1b_param_0(0.66) 
I0630 09:11:06.652874 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:11:06.652876 29777 net.cpp:1851] res2a_branch2a_param_0(0.66) 
I0630 09:11:06.652878 29777 net.cpp:1851] res2a_branch2b_param_0(0.66) 
I0630 09:11:06.652880 29777 net.cpp:1851] res3a_branch2a_param_0(0.66) 
I0630 09:11:06.652882 29777 net.cpp:1851] res3a_branch2b_param_0(0.66) 
I0630 09:11:06.652884 29777 net.cpp:1851] res4a_branch2a_param_0(0.66) 
I0630 09:11:06.652886 29777 net.cpp:1851] res4a_branch2b_param_0(0.66) 
I0630 09:11:06.652889 29777 net.cpp:1851] res5a_branch2a_param_0(0.66) 
I0630 09:11:06.652890 29777 net.cpp:1851] res5a_branch2b_param_0(0.66) 
I0630 09:11:06.652892 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.55336e+06/2.86678e+06) 0.542
I0630 09:11:06.652978 29777 solver.cpp:471] Iteration 132000, Testing net (#0)
I0630 09:11:19.244792 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 09:12:07.736179 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.565259
I0630 09:12:07.736325 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.800361
I0630 09:12:07.736344 29777 solver.cpp:544]     Test net output #2: loss = 1.54672 (* 1 = 1.54672 loss)
I0630 09:12:07.918881 29777 solver.cpp:290] Iteration 132000 (1.29362 iter/s, 77.3023s/100 iter), loss = 0.630952
I0630 09:12:07.919172 29777 solver.cpp:309]     Train net output #0: loss = 0.404762 (* 1 = 0.404762 loss)
I0630 09:12:07.919309 29777 sgd_solver.cpp:106] Iteration 132000, lr = 0.005875
I0630 09:12:07.921347 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.67
I0630 09:12:08.540423 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 09:12:24.768144 29777 solver.cpp:290] Iteration 132100 (5.93524 iter/s, 16.8485s/100 iter), loss = 0.797619
I0630 09:12:24.768169 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 09:12:24.768179 29777 sgd_solver.cpp:106] Iteration 132100, lr = 0.00587188
I0630 09:12:40.887068 29777 solver.cpp:290] Iteration 132200 (6.20407 iter/s, 16.1185s/100 iter), loss = 1.14286
I0630 09:12:40.887166 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 09:12:40.887176 29777 sgd_solver.cpp:106] Iteration 132200, lr = 0.00586875
I0630 09:12:57.493207 29777 solver.cpp:290] Iteration 132300 (6.02207 iter/s, 16.6056s/100 iter), loss = 1.39286
I0630 09:12:57.493232 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 09:12:57.493242 29777 sgd_solver.cpp:106] Iteration 132300, lr = 0.00586563
I0630 09:13:13.539386 29777 solver.cpp:290] Iteration 132400 (6.23219 iter/s, 16.0457s/100 iter), loss = 1.38095
I0630 09:13:13.539494 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 09:13:13.539504 29777 sgd_solver.cpp:106] Iteration 132400, lr = 0.0058625
I0630 09:13:29.627353 29777 solver.cpp:290] Iteration 132500 (6.21604 iter/s, 16.0874s/100 iter), loss = 1.86905
I0630 09:13:29.627379 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 09:13:29.627389 29777 sgd_solver.cpp:106] Iteration 132500, lr = 0.00585938
I0630 09:13:45.590751 29777 solver.cpp:290] Iteration 132600 (6.26451 iter/s, 15.9629s/100 iter), loss = 1.20238
I0630 09:13:45.590848 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 09:13:45.590857 29777 sgd_solver.cpp:106] Iteration 132600, lr = 0.00585625
I0630 09:14:01.533146 29777 solver.cpp:290] Iteration 132700 (6.27279 iter/s, 15.9419s/100 iter), loss = 1.32143
I0630 09:14:01.533171 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 09:14:01.533180 29777 sgd_solver.cpp:106] Iteration 132700, lr = 0.00585312
I0630 09:14:17.513208 29777 solver.cpp:290] Iteration 132800 (6.25798 iter/s, 15.9796s/100 iter), loss = 1.36905
I0630 09:14:17.513288 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 09:14:17.513296 29777 sgd_solver.cpp:106] Iteration 132800, lr = 0.00585
I0630 09:14:33.800808 29777 solver.cpp:290] Iteration 132900 (6.13984 iter/s, 16.2871s/100 iter), loss = 1.27381
I0630 09:14:33.800830 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 09:14:33.800837 29777 sgd_solver.cpp:106] Iteration 132900, lr = 0.00584687
I0630 09:14:49.814072 29777 solver.cpp:354] Sparsity after update:
I0630 09:14:49.834440 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:14:49.834455 29777 net.cpp:1851] conv1a_param_0(0.335) 
I0630 09:14:49.834465 29777 net.cpp:1851] conv1b_param_0(0.67) 
I0630 09:14:49.834470 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:14:49.834473 29777 net.cpp:1851] res2a_branch2a_param_0(0.67) 
I0630 09:14:49.834483 29777 net.cpp:1851] res2a_branch2b_param_0(0.67) 
I0630 09:14:49.834488 29777 net.cpp:1851] res3a_branch2a_param_0(0.67) 
I0630 09:14:49.834493 29777 net.cpp:1851] res3a_branch2b_param_0(0.67) 
I0630 09:14:49.834498 29777 net.cpp:1851] res4a_branch2a_param_0(0.67) 
I0630 09:14:49.834502 29777 net.cpp:1851] res4a_branch2b_param_0(0.67) 
I0630 09:14:49.834507 29777 net.cpp:1851] res5a_branch2a_param_0(0.67) 
I0630 09:14:49.834511 29777 net.cpp:1851] res5a_branch2b_param_0(0.67) 
I0630 09:14:49.834516 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.57689e+06/2.86678e+06) 0.55
I0630 09:14:49.991252 29777 solver.cpp:290] Iteration 133000 (6.17666 iter/s, 16.19s/100 iter), loss = 1.15476
I0630 09:14:49.991281 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 09:14:49.991289 29777 sgd_solver.cpp:106] Iteration 133000, lr = 0.00584375
I0630 09:15:06.073607 29777 solver.cpp:290] Iteration 133100 (6.21817 iter/s, 16.0819s/100 iter), loss = 1.25
I0630 09:15:06.073632 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 09:15:06.073640 29777 sgd_solver.cpp:106] Iteration 133100, lr = 0.00584062
I0630 09:15:22.213333 29777 solver.cpp:290] Iteration 133200 (6.19607 iter/s, 16.1393s/100 iter), loss = 1.46429
I0630 09:15:22.213460 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 09:15:22.213469 29777 sgd_solver.cpp:106] Iteration 133200, lr = 0.0058375
I0630 09:15:38.215041 29777 solver.cpp:290] Iteration 133300 (6.24955 iter/s, 16.0011s/100 iter), loss = 1.39286
I0630 09:15:38.215068 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 09:15:38.215085 29777 sgd_solver.cpp:106] Iteration 133300, lr = 0.00583438
I0630 09:15:54.198822 29777 solver.cpp:290] Iteration 133400 (6.25652 iter/s, 15.9833s/100 iter), loss = 1.38095
I0630 09:15:54.198896 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 09:15:54.198904 29777 sgd_solver.cpp:106] Iteration 133400, lr = 0.00583125
I0630 09:16:10.434185 29777 solver.cpp:290] Iteration 133500 (6.15959 iter/s, 16.2348s/100 iter), loss = 1.19048
I0630 09:16:10.434208 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 09:16:10.434216 29777 sgd_solver.cpp:106] Iteration 133500, lr = 0.00582812
I0630 09:16:26.653506 29777 solver.cpp:290] Iteration 133600 (6.16566 iter/s, 16.2189s/100 iter), loss = 1.19048
I0630 09:16:26.653615 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 09:16:26.653630 29777 sgd_solver.cpp:106] Iteration 133600, lr = 0.005825
I0630 09:16:42.779341 29777 solver.cpp:290] Iteration 133700 (6.20144 iter/s, 16.1253s/100 iter), loss = 1.15476
I0630 09:16:42.779369 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 09:16:42.779378 29777 sgd_solver.cpp:106] Iteration 133700, lr = 0.00582188
I0630 09:16:59.048053 29777 solver.cpp:290] Iteration 133800 (6.14695 iter/s, 16.2682s/100 iter), loss = 0.773809
I0630 09:16:59.048158 29777 solver.cpp:309]     Train net output #0: loss = 0.571429 (* 1 = 0.571429 loss)
I0630 09:16:59.048168 29777 sgd_solver.cpp:106] Iteration 133800, lr = 0.00581875
I0630 09:17:15.349735 29777 solver.cpp:290] Iteration 133900 (6.13454 iter/s, 16.3011s/100 iter), loss = 1.22619
I0630 09:17:15.349769 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 09:17:15.349781 29777 sgd_solver.cpp:106] Iteration 133900, lr = 0.00581563
I0630 09:17:31.370638 29777 solver.cpp:354] Sparsity after update:
I0630 09:17:31.372298 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:17:31.372306 29777 net.cpp:1851] conv1a_param_0(0.335) 
I0630 09:17:31.372314 29777 net.cpp:1851] conv1b_param_0(0.67) 
I0630 09:17:31.372318 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:17:31.372319 29777 net.cpp:1851] res2a_branch2a_param_0(0.67) 
I0630 09:17:31.372321 29777 net.cpp:1851] res2a_branch2b_param_0(0.67) 
I0630 09:17:31.372323 29777 net.cpp:1851] res3a_branch2a_param_0(0.67) 
I0630 09:17:31.372325 29777 net.cpp:1851] res3a_branch2b_param_0(0.67) 
I0630 09:17:31.372328 29777 net.cpp:1851] res4a_branch2a_param_0(0.67) 
I0630 09:17:31.372328 29777 net.cpp:1851] res4a_branch2b_param_0(0.67) 
I0630 09:17:31.372330 29777 net.cpp:1851] res5a_branch2a_param_0(0.67) 
I0630 09:17:31.372333 29777 net.cpp:1851] res5a_branch2b_param_0(0.67) 
I0630 09:17:31.372334 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.57689e+06/2.86678e+06) 0.55
I0630 09:17:31.372433 29777 solver.cpp:471] Iteration 134000, Testing net (#0)
I0630 09:17:44.840793 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 09:18:40.456619 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.561479
I0630 09:18:40.456677 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.797582
I0630 09:18:40.456684 29777 solver.cpp:544]     Test net output #2: loss = 1.55842 (* 1 = 1.55842 loss)
I0630 09:18:40.637398 29777 solver.cpp:290] Iteration 134000 (1.17253 iter/s, 85.2853s/100 iter), loss = 1.15476
I0630 09:18:40.637425 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 09:18:40.637435 29777 sgd_solver.cpp:106] Iteration 134000, lr = 0.0058125
I0630 09:18:40.638427 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.68
I0630 09:18:41.138677 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 09:18:57.310595 29777 solver.cpp:290] Iteration 134100 (5.99782 iter/s, 16.6727s/100 iter), loss = 1.45238
I0630 09:18:57.310618 29777 solver.cpp:309]     Train net output #0: loss = 1.85714 (* 1 = 1.85714 loss)
I0630 09:18:57.310626 29777 sgd_solver.cpp:106] Iteration 134100, lr = 0.00580938
I0630 09:19:13.505194 29777 solver.cpp:290] Iteration 134200 (6.17508 iter/s, 16.1941s/100 iter), loss = 1.29762
I0630 09:19:13.505287 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 09:19:13.505298 29777 sgd_solver.cpp:106] Iteration 134200, lr = 0.00580625
I0630 09:19:30.162317 29777 solver.cpp:290] Iteration 134300 (6.00364 iter/s, 16.6566s/100 iter), loss = 1.09524
I0630 09:19:30.162339 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 09:19:30.162346 29777 sgd_solver.cpp:106] Iteration 134300, lr = 0.00580312
I0630 09:19:46.745518 29777 solver.cpp:290] Iteration 134400 (6.03037 iter/s, 16.5827s/100 iter), loss = 1.60714
I0630 09:19:46.745602 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 09:19:46.745613 29777 sgd_solver.cpp:106] Iteration 134400, lr = 0.0058
I0630 09:20:02.877579 29777 solver.cpp:290] Iteration 134500 (6.19904 iter/s, 16.1315s/100 iter), loss = 1.2619
I0630 09:20:02.877604 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 09:20:02.877612 29777 sgd_solver.cpp:106] Iteration 134500, lr = 0.00579687
I0630 09:20:18.965355 29777 solver.cpp:290] Iteration 134600 (6.21608 iter/s, 16.0873s/100 iter), loss = 1.0119
I0630 09:20:18.965461 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 09:20:18.965471 29777 sgd_solver.cpp:106] Iteration 134600, lr = 0.00579375
I0630 09:20:35.127043 29777 solver.cpp:290] Iteration 134700 (6.18768 iter/s, 16.1611s/100 iter), loss = 1.07143
I0630 09:20:35.127066 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 09:20:35.127074 29777 sgd_solver.cpp:106] Iteration 134700, lr = 0.00579062
I0630 09:20:51.430505 29777 solver.cpp:290] Iteration 134800 (6.13384 iter/s, 16.303s/100 iter), loss = 1.27381
I0630 09:20:51.430572 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 09:20:51.430580 29777 sgd_solver.cpp:106] Iteration 134800, lr = 0.0057875
I0630 09:21:07.692463 29777 solver.cpp:290] Iteration 134900 (6.14951 iter/s, 16.2615s/100 iter), loss = 1.13095
I0630 09:21:07.692492 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 09:21:07.692499 29777 sgd_solver.cpp:106] Iteration 134900, lr = 0.00578438
I0630 09:21:23.641304 29777 solver.cpp:354] Sparsity after update:
I0630 09:21:23.660159 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:21:23.660185 29777 net.cpp:1851] conv1a_param_0(0.34) 
I0630 09:21:23.660207 29777 net.cpp:1851] conv1b_param_0(0.68) 
I0630 09:21:23.660213 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:21:23.660219 29777 net.cpp:1851] res2a_branch2a_param_0(0.68) 
I0630 09:21:23.660226 29777 net.cpp:1851] res2a_branch2b_param_0(0.68) 
I0630 09:21:23.660233 29777 net.cpp:1851] res3a_branch2a_param_0(0.68) 
I0630 09:21:23.660238 29777 net.cpp:1851] res3a_branch2b_param_0(0.68) 
I0630 09:21:23.660243 29777 net.cpp:1851] res4a_branch2a_param_0(0.68) 
I0630 09:21:23.660248 29777 net.cpp:1851] res4a_branch2b_param_0(0.68) 
I0630 09:21:23.660254 29777 net.cpp:1851] res5a_branch2a_param_0(0.68) 
I0630 09:21:23.660260 29777 net.cpp:1851] res5a_branch2b_param_0(0.68) 
I0630 09:21:23.660266 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.60043e+06/2.86678e+06) 0.558
I0630 09:21:23.823943 29777 solver.cpp:290] Iteration 135000 (6.19924 iter/s, 16.131s/100 iter), loss = 1.2381
I0630 09:21:23.823987 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 09:21:23.824004 29777 sgd_solver.cpp:106] Iteration 135000, lr = 0.00578125
I0630 09:21:39.999841 29777 solver.cpp:290] Iteration 135100 (6.18222 iter/s, 16.1754s/100 iter), loss = 1.30952
I0630 09:21:39.999866 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 09:21:39.999873 29777 sgd_solver.cpp:106] Iteration 135100, lr = 0.00577813
I0630 09:21:56.203161 29777 solver.cpp:290] Iteration 135200 (6.17175 iter/s, 16.2029s/100 iter), loss = 1.10714
I0630 09:21:56.203279 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 09:21:56.203289 29777 sgd_solver.cpp:106] Iteration 135200, lr = 0.005775
I0630 09:22:12.359628 29777 solver.cpp:290] Iteration 135300 (6.18968 iter/s, 16.1559s/100 iter), loss = 1.38095
I0630 09:22:12.359653 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 09:22:12.359663 29777 sgd_solver.cpp:106] Iteration 135300, lr = 0.00577188
I0630 09:22:28.533769 29777 solver.cpp:290] Iteration 135400 (6.18289 iter/s, 16.1737s/100 iter), loss = 1.19048
I0630 09:22:28.534308 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 09:22:28.534318 29777 sgd_solver.cpp:106] Iteration 135400, lr = 0.00576875
I0630 09:22:44.689162 29777 solver.cpp:290] Iteration 135500 (6.19026 iter/s, 16.1544s/100 iter), loss = 1.10714
I0630 09:22:44.689185 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 09:22:44.689194 29777 sgd_solver.cpp:106] Iteration 135500, lr = 0.00576563
I0630 09:23:00.894497 29777 solver.cpp:290] Iteration 135600 (6.17099 iter/s, 16.2049s/100 iter), loss = 1.11905
I0630 09:23:00.894548 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 09:23:00.894558 29777 sgd_solver.cpp:106] Iteration 135600, lr = 0.0057625
I0630 09:23:17.195624 29777 solver.cpp:290] Iteration 135700 (6.13473 iter/s, 16.3006s/100 iter), loss = 1.15476
I0630 09:23:17.195647 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 09:23:17.195654 29777 sgd_solver.cpp:106] Iteration 135700, lr = 0.00575938
I0630 09:23:33.328140 29777 solver.cpp:290] Iteration 135800 (6.19884 iter/s, 16.1321s/100 iter), loss = 1.02381
I0630 09:23:33.328240 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 09:23:33.328250 29777 sgd_solver.cpp:106] Iteration 135800, lr = 0.00575625
I0630 09:23:49.456189 29777 solver.cpp:290] Iteration 135900 (6.20058 iter/s, 16.1275s/100 iter), loss = 1.67857
I0630 09:23:49.456213 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 09:23:49.456223 29777 sgd_solver.cpp:106] Iteration 135900, lr = 0.00575312
I0630 09:24:05.446089 29777 solver.cpp:354] Sparsity after update:
I0630 09:24:05.447788 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:24:05.447799 29777 net.cpp:1851] conv1a_param_0(0.34) 
I0630 09:24:05.447808 29777 net.cpp:1851] conv1b_param_0(0.68) 
I0630 09:24:05.447809 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:24:05.447811 29777 net.cpp:1851] res2a_branch2a_param_0(0.68) 
I0630 09:24:05.447813 29777 net.cpp:1851] res2a_branch2b_param_0(0.68) 
I0630 09:24:05.447815 29777 net.cpp:1851] res3a_branch2a_param_0(0.68) 
I0630 09:24:05.447818 29777 net.cpp:1851] res3a_branch2b_param_0(0.68) 
I0630 09:24:05.447819 29777 net.cpp:1851] res4a_branch2a_param_0(0.68) 
I0630 09:24:05.447821 29777 net.cpp:1851] res4a_branch2b_param_0(0.68) 
I0630 09:24:05.447824 29777 net.cpp:1851] res5a_branch2a_param_0(0.68) 
I0630 09:24:05.447826 29777 net.cpp:1851] res5a_branch2b_param_0(0.68) 
I0630 09:24:05.447829 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.60043e+06/2.86678e+06) 0.558
I0630 09:24:05.447966 29777 solver.cpp:471] Iteration 136000, Testing net (#0)
I0630 09:24:19.663147 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 09:25:08.257068 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.563079
I0630 09:25:08.257241 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.798141
I0630 09:25:08.257261 29777 solver.cpp:544]     Test net output #2: loss = 1.5569 (* 1 = 1.5569 loss)
I0630 09:25:08.469457 29777 solver.cpp:290] Iteration 136000 (1.26564 iter/s, 79.0111s/100 iter), loss = 1.09524
I0630 09:25:08.469480 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 09:25:08.469487 29777 sgd_solver.cpp:106] Iteration 136000, lr = 0.00575
I0630 09:25:08.470211 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.69
I0630 09:25:09.314415 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 09:25:25.513217 29777 solver.cpp:290] Iteration 136100 (5.86742 iter/s, 17.0433s/100 iter), loss = 1.32143
I0630 09:25:25.513273 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 09:25:25.513294 29777 sgd_solver.cpp:106] Iteration 136100, lr = 0.00574687
I0630 09:25:41.740330 29777 solver.cpp:290] Iteration 136200 (6.16271 iter/s, 16.2266s/100 iter), loss = 0.642857
I0630 09:25:41.740382 29777 solver.cpp:309]     Train net output #0: loss = 0.690476 (* 1 = 0.690476 loss)
I0630 09:25:41.740391 29777 sgd_solver.cpp:106] Iteration 136200, lr = 0.00574375
I0630 09:25:57.878613 29777 solver.cpp:290] Iteration 136300 (6.19664 iter/s, 16.1378s/100 iter), loss = 1.2619
I0630 09:25:57.878639 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 09:25:57.878648 29777 sgd_solver.cpp:106] Iteration 136300, lr = 0.00574062
I0630 09:26:13.928861 29777 solver.cpp:290] Iteration 136400 (6.23063 iter/s, 16.0497s/100 iter), loss = 1.58333
I0630 09:26:13.929002 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 09:26:13.929039 29777 sgd_solver.cpp:106] Iteration 136400, lr = 0.0057375
I0630 09:26:30.137477 29777 solver.cpp:290] Iteration 136500 (6.16978 iter/s, 16.208s/100 iter), loss = 1.35714
I0630 09:26:30.137508 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 09:26:30.137518 29777 sgd_solver.cpp:106] Iteration 136500, lr = 0.00573438
I0630 09:26:46.160846 29777 solver.cpp:290] Iteration 136600 (6.24107 iter/s, 16.0229s/100 iter), loss = 1.58333
I0630 09:26:46.160918 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 09:26:46.160926 29777 sgd_solver.cpp:106] Iteration 136600, lr = 0.00573125
I0630 09:27:02.212064 29777 solver.cpp:290] Iteration 136700 (6.23026 iter/s, 16.0507s/100 iter), loss = 1.02381
I0630 09:27:02.212090 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 09:27:02.212100 29777 sgd_solver.cpp:106] Iteration 136700, lr = 0.00572812
I0630 09:27:18.185240 29777 solver.cpp:290] Iteration 136800 (6.26068 iter/s, 15.9727s/100 iter), loss = 1.27381
I0630 09:27:18.185318 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 09:27:18.185329 29777 sgd_solver.cpp:106] Iteration 136800, lr = 0.005725
I0630 09:27:34.343789 29777 solver.cpp:290] Iteration 136900 (6.18887 iter/s, 16.158s/100 iter), loss = 1.29762
I0630 09:27:34.343816 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 09:27:34.343832 29777 sgd_solver.cpp:106] Iteration 136900, lr = 0.00572188
I0630 09:27:50.289688 29777 solver.cpp:354] Sparsity after update:
I0630 09:27:50.309926 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:27:50.309942 29777 net.cpp:1851] conv1a_param_0(0.345) 
I0630 09:27:50.309955 29777 net.cpp:1851] conv1b_param_0(0.69) 
I0630 09:27:50.309959 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:27:50.309963 29777 net.cpp:1851] res2a_branch2a_param_0(0.69) 
I0630 09:27:50.309967 29777 net.cpp:1851] res2a_branch2b_param_0(0.69) 
I0630 09:27:50.309972 29777 net.cpp:1851] res3a_branch2a_param_0(0.69) 
I0630 09:27:50.309975 29777 net.cpp:1851] res3a_branch2b_param_0(0.69) 
I0630 09:27:50.309979 29777 net.cpp:1851] res4a_branch2a_param_0(0.69) 
I0630 09:27:50.309983 29777 net.cpp:1851] res4a_branch2b_param_0(0.69) 
I0630 09:27:50.309988 29777 net.cpp:1851] res5a_branch2a_param_0(0.69) 
I0630 09:27:50.309990 29777 net.cpp:1851] res5a_branch2b_param_0(0.69) 
I0630 09:27:50.309994 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.62397e+06/2.86678e+06) 0.566
I0630 09:27:50.478109 29777 solver.cpp:290] Iteration 137000 (6.19815 iter/s, 16.1339s/100 iter), loss = 1.35714
I0630 09:27:50.478132 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 09:27:50.478139 29777 sgd_solver.cpp:106] Iteration 137000, lr = 0.00571875
I0630 09:28:06.544859 29777 solver.cpp:290] Iteration 137100 (6.22421 iter/s, 16.0663s/100 iter), loss = 1.08333
I0630 09:28:06.544881 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 09:28:06.544888 29777 sgd_solver.cpp:106] Iteration 137100, lr = 0.00571563
I0630 09:28:22.558109 29777 solver.cpp:290] Iteration 137200 (6.24501 iter/s, 16.0128s/100 iter), loss = 1.14286
I0630 09:28:22.558223 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 09:28:22.558238 29777 sgd_solver.cpp:106] Iteration 137200, lr = 0.0057125
I0630 09:28:38.703881 29777 solver.cpp:290] Iteration 137300 (6.19378 iter/s, 16.1452s/100 iter), loss = 0.964286
I0630 09:28:38.703907 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 09:28:38.703912 29777 sgd_solver.cpp:106] Iteration 137300, lr = 0.00570937
I0630 09:28:55.361989 29777 solver.cpp:290] Iteration 137400 (6.00326 iter/s, 16.6576s/100 iter), loss = 1.14286
I0630 09:28:55.362084 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 09:28:55.362095 29777 sgd_solver.cpp:106] Iteration 137400, lr = 0.00570625
I0630 09:29:11.403564 29777 solver.cpp:290] Iteration 137500 (6.23401 iter/s, 16.041s/100 iter), loss = 1
I0630 09:29:11.403590 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 09:29:11.403599 29777 sgd_solver.cpp:106] Iteration 137500, lr = 0.00570312
I0630 09:29:27.578222 29777 solver.cpp:290] Iteration 137600 (6.18269 iter/s, 16.1742s/100 iter), loss = 1.44048
I0630 09:29:27.578327 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 09:29:27.578337 29777 sgd_solver.cpp:106] Iteration 137600, lr = 0.0057
I0630 09:29:43.601927 29777 solver.cpp:290] Iteration 137700 (6.24097 iter/s, 16.0232s/100 iter), loss = 1.15476
I0630 09:29:43.601956 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 09:29:43.601965 29777 sgd_solver.cpp:106] Iteration 137700, lr = 0.00569687
I0630 09:29:59.602347 29777 solver.cpp:290] Iteration 137800 (6.25002 iter/s, 16s/100 iter), loss = 1.05952
I0630 09:29:59.602424 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 09:29:59.602432 29777 sgd_solver.cpp:106] Iteration 137800, lr = 0.00569375
I0630 09:30:15.692628 29777 solver.cpp:290] Iteration 137900 (6.21513 iter/s, 16.0898s/100 iter), loss = 1.36905
I0630 09:30:15.692659 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 09:30:15.692669 29777 sgd_solver.cpp:106] Iteration 137900, lr = 0.00569062
I0630 09:30:31.556931 29777 solver.cpp:354] Sparsity after update:
I0630 09:30:31.558547 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:30:31.558554 29777 net.cpp:1851] conv1a_param_0(0.345) 
I0630 09:30:31.558562 29777 net.cpp:1851] conv1b_param_0(0.69) 
I0630 09:30:31.558564 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:30:31.558567 29777 net.cpp:1851] res2a_branch2a_param_0(0.69) 
I0630 09:30:31.558568 29777 net.cpp:1851] res2a_branch2b_param_0(0.69) 
I0630 09:30:31.558570 29777 net.cpp:1851] res3a_branch2a_param_0(0.69) 
I0630 09:30:31.558573 29777 net.cpp:1851] res3a_branch2b_param_0(0.69) 
I0630 09:30:31.558574 29777 net.cpp:1851] res4a_branch2a_param_0(0.69) 
I0630 09:30:31.558576 29777 net.cpp:1851] res4a_branch2b_param_0(0.69) 
I0630 09:30:31.558578 29777 net.cpp:1851] res5a_branch2a_param_0(0.69) 
I0630 09:30:31.558580 29777 net.cpp:1851] res5a_branch2b_param_0(0.69) 
I0630 09:30:31.558583 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.62397e+06/2.86678e+06) 0.566
I0630 09:30:31.558670 29777 solver.cpp:471] Iteration 138000, Testing net (#0)
I0630 09:30:44.488788 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 09:31:33.372510 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.561619
I0630 09:31:33.372568 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.796681
I0630 09:31:33.372575 29777 solver.cpp:544]     Test net output #2: loss = 1.5555 (* 1 = 1.5555 loss)
I0630 09:31:33.547799 29777 solver.cpp:290] Iteration 138000 (1.28447 iter/s, 77.853s/100 iter), loss = 1.2619
I0630 09:31:33.547825 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 09:31:33.547833 29777 sgd_solver.cpp:106] Iteration 138000, lr = 0.0056875
I0630 09:31:33.548812 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.7
I0630 09:31:34.132237 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 09:31:50.137717 29777 solver.cpp:290] Iteration 138100 (6.02793 iter/s, 16.5894s/100 iter), loss = 1.4881
I0630 09:31:50.137744 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 09:31:50.137753 29777 sgd_solver.cpp:106] Iteration 138100, lr = 0.00568437
I0630 09:32:06.116412 29777 solver.cpp:290] Iteration 138200 (6.25852 iter/s, 15.9782s/100 iter), loss = 1.04762
I0630 09:32:06.116554 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 09:32:06.116583 29777 sgd_solver.cpp:106] Iteration 138200, lr = 0.00568125
I0630 09:32:22.307919 29777 solver.cpp:290] Iteration 138300 (6.1763 iter/s, 16.1909s/100 iter), loss = 1.08333
I0630 09:32:22.307945 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 09:32:22.307951 29777 sgd_solver.cpp:106] Iteration 138300, lr = 0.00567812
I0630 09:32:38.623220 29777 solver.cpp:290] Iteration 138400 (6.12939 iter/s, 16.3148s/100 iter), loss = 0.77381
I0630 09:32:38.623325 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 09:32:38.623335 29777 sgd_solver.cpp:106] Iteration 138400, lr = 0.005675
I0630 09:32:54.680619 29777 solver.cpp:290] Iteration 138500 (6.22787 iter/s, 16.0569s/100 iter), loss = 0.916667
I0630 09:32:54.680644 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 09:32:54.680652 29777 sgd_solver.cpp:106] Iteration 138500, lr = 0.00567187
I0630 09:33:10.938160 29777 solver.cpp:290] Iteration 138600 (6.15117 iter/s, 16.2571s/100 iter), loss = 1.27381
I0630 09:33:10.938269 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 09:33:10.938279 29777 sgd_solver.cpp:106] Iteration 138600, lr = 0.00566875
I0630 09:33:27.239873 29777 solver.cpp:290] Iteration 138700 (6.13453 iter/s, 16.3012s/100 iter), loss = 1.41667
I0630 09:33:27.239897 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 09:33:27.239905 29777 sgd_solver.cpp:106] Iteration 138700, lr = 0.00566562
I0630 09:33:43.583907 29777 solver.cpp:290] Iteration 138800 (6.11862 iter/s, 16.3436s/100 iter), loss = 1.25
I0630 09:33:43.584055 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 09:33:43.584075 29777 sgd_solver.cpp:106] Iteration 138800, lr = 0.0056625
I0630 09:33:59.751896 29777 solver.cpp:290] Iteration 138900 (6.18528 iter/s, 16.1674s/100 iter), loss = 1.21429
I0630 09:33:59.751929 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 09:33:59.751941 29777 sgd_solver.cpp:106] Iteration 138900, lr = 0.00565937
I0630 09:34:15.934825 29777 solver.cpp:354] Sparsity after update:
I0630 09:34:15.955112 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:34:15.955129 29777 net.cpp:1851] conv1a_param_0(0.35) 
I0630 09:34:15.955142 29777 net.cpp:1851] conv1b_param_0(0.7) 
I0630 09:34:15.955147 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:34:15.955149 29777 net.cpp:1851] res2a_branch2a_param_0(0.7) 
I0630 09:34:15.955153 29777 net.cpp:1851] res2a_branch2b_param_0(0.7) 
I0630 09:34:15.955157 29777 net.cpp:1851] res3a_branch2a_param_0(0.7) 
I0630 09:34:15.955159 29777 net.cpp:1851] res3a_branch2b_param_0(0.7) 
I0630 09:34:15.955163 29777 net.cpp:1851] res4a_branch2a_param_0(0.7) 
I0630 09:34:15.955165 29777 net.cpp:1851] res4a_branch2b_param_0(0.7) 
I0630 09:34:15.955169 29777 net.cpp:1851] res5a_branch2a_param_0(0.7) 
I0630 09:34:15.955173 29777 net.cpp:1851] res5a_branch2b_param_0(0.7) 
I0630 09:34:15.955175 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.6475e+06/2.86678e+06) 0.575
I0630 09:34:16.113108 29777 solver.cpp:290] Iteration 139000 (6.11219 iter/s, 16.3607s/100 iter), loss = 1.13095
I0630 09:34:16.113131 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 09:34:16.113138 29777 sgd_solver.cpp:106] Iteration 139000, lr = 0.00565625
I0630 09:34:32.188415 29777 solver.cpp:290] Iteration 139100 (6.22091 iter/s, 16.0748s/100 iter), loss = 1.03571
I0630 09:34:32.188525 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 09:34:32.188575 29777 sgd_solver.cpp:106] Iteration 139100, lr = 0.00565312
I0630 09:34:48.473958 29777 solver.cpp:290] Iteration 139200 (6.14062 iter/s, 16.285s/100 iter), loss = 0.964286
I0630 09:34:48.474040 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 09:34:48.474054 29777 sgd_solver.cpp:106] Iteration 139200, lr = 0.00565
I0630 09:35:04.602993 29777 solver.cpp:290] Iteration 139300 (6.2002 iter/s, 16.1285s/100 iter), loss = 1.38095
I0630 09:35:04.603044 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 09:35:04.603068 29777 sgd_solver.cpp:106] Iteration 139300, lr = 0.00564687
I0630 09:35:20.780688 29777 solver.cpp:290] Iteration 139400 (6.18154 iter/s, 16.1772s/100 iter), loss = 1.05952
I0630 09:35:20.780760 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 09:35:20.780766 29777 sgd_solver.cpp:106] Iteration 139400, lr = 0.00564375
I0630 09:35:36.918231 29777 solver.cpp:290] Iteration 139500 (6.19693 iter/s, 16.137s/100 iter), loss = 1.19048
I0630 09:35:36.918253 29777 solver.cpp:309]     Train net output #0: loss = 0.690476 (* 1 = 0.690476 loss)
I0630 09:35:36.918262 29777 sgd_solver.cpp:106] Iteration 139500, lr = 0.00564062
I0630 09:35:53.036064 29777 solver.cpp:290] Iteration 139600 (6.20449 iter/s, 16.1174s/100 iter), loss = 1
I0630 09:35:53.036115 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 09:35:53.036126 29777 sgd_solver.cpp:106] Iteration 139600, lr = 0.0056375
I0630 09:36:09.435600 29777 solver.cpp:290] Iteration 139700 (6.09792 iter/s, 16.399s/100 iter), loss = 1.04762
I0630 09:36:09.435654 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 09:36:09.435676 29777 sgd_solver.cpp:106] Iteration 139700, lr = 0.00563437
I0630 09:36:25.791177 29777 solver.cpp:290] Iteration 139800 (6.11431 iter/s, 16.3551s/100 iter), loss = 1.39286
I0630 09:36:25.791291 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 09:36:25.791299 29777 sgd_solver.cpp:106] Iteration 139800, lr = 0.00563125
I0630 09:36:41.901000 29777 solver.cpp:290] Iteration 139900 (6.20761 iter/s, 16.1093s/100 iter), loss = 1
I0630 09:36:41.901027 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 09:36:41.901056 29777 sgd_solver.cpp:106] Iteration 139900, lr = 0.00562812
I0630 09:36:57.872534 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_140000.caffemodel
I0630 09:36:57.892005 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_140000.solverstate
I0630 09:36:57.900665 29777 solver.cpp:354] Sparsity after update:
I0630 09:36:57.901618 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:36:57.901625 29777 net.cpp:1851] conv1a_param_0(0.35) 
I0630 09:36:57.901633 29777 net.cpp:1851] conv1b_param_0(0.7) 
I0630 09:36:57.901635 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:36:57.901638 29777 net.cpp:1851] res2a_branch2a_param_0(0.7) 
I0630 09:36:57.901640 29777 net.cpp:1851] res2a_branch2b_param_0(0.7) 
I0630 09:36:57.901641 29777 net.cpp:1851] res3a_branch2a_param_0(0.7) 
I0630 09:36:57.901643 29777 net.cpp:1851] res3a_branch2b_param_0(0.7) 
I0630 09:36:57.901645 29777 net.cpp:1851] res4a_branch2a_param_0(0.7) 
I0630 09:36:57.901648 29777 net.cpp:1851] res4a_branch2b_param_0(0.7) 
I0630 09:36:57.901649 29777 net.cpp:1851] res5a_branch2a_param_0(0.7) 
I0630 09:36:57.901651 29777 net.cpp:1851] res5a_branch2b_param_0(0.7) 
I0630 09:36:57.901654 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.6475e+06/2.86678e+06) 0.575
I0630 09:36:57.901749 29777 solver.cpp:471] Iteration 140000, Testing net (#0)
I0630 09:37:14.295258 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 09:38:04.797722 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5604
I0630 09:38:04.797842 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.796201
I0630 09:38:04.797868 29777 solver.cpp:544]     Test net output #2: loss = 1.56316 (* 1 = 1.56316 loss)
I0630 09:38:05.016595 29777 solver.cpp:290] Iteration 140000 (1.20318 iter/s, 83.1133s/100 iter), loss = 1.2619
I0630 09:38:05.016618 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 09:38:05.016625 29777 sgd_solver.cpp:106] Iteration 140000, lr = 0.005625
I0630 09:38:05.017287 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.71
I0630 09:38:05.674437 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 09:38:21.983137 29777 solver.cpp:290] Iteration 140100 (5.89412 iter/s, 16.9661s/100 iter), loss = 0.988095
I0630 09:38:21.983162 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 09:38:21.983186 29777 sgd_solver.cpp:106] Iteration 140100, lr = 0.00562187
I0630 09:38:38.190757 29777 solver.cpp:290] Iteration 140200 (6.17012 iter/s, 16.2072s/100 iter), loss = 1.35714
I0630 09:38:38.190826 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 09:38:38.190840 29777 sgd_solver.cpp:106] Iteration 140200, lr = 0.00561875
I0630 09:38:54.334029 29777 solver.cpp:290] Iteration 140300 (6.19473 iter/s, 16.1428s/100 iter), loss = 1.27381
I0630 09:38:54.334053 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 09:38:54.334059 29777 sgd_solver.cpp:106] Iteration 140300, lr = 0.00561563
I0630 09:39:10.400801 29777 solver.cpp:290] Iteration 140400 (6.22421 iter/s, 16.0663s/100 iter), loss = 1.0119
I0630 09:39:10.400910 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 09:39:10.400941 29777 sgd_solver.cpp:106] Iteration 140400, lr = 0.0056125
I0630 09:39:26.627302 29777 solver.cpp:290] Iteration 140500 (6.16297 iter/s, 16.2259s/100 iter), loss = 0.916667
I0630 09:39:26.627331 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 09:39:26.627341 29777 sgd_solver.cpp:106] Iteration 140500, lr = 0.00560937
I0630 09:39:42.932924 29777 solver.cpp:290] Iteration 140600 (6.13303 iter/s, 16.3051s/100 iter), loss = 1.28571
I0630 09:39:42.933035 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 09:39:42.933046 29777 sgd_solver.cpp:106] Iteration 140600, lr = 0.00560625
I0630 09:39:58.944522 29777 solver.cpp:290] Iteration 140700 (6.24569 iter/s, 16.0111s/100 iter), loss = 1.09524
I0630 09:39:58.944547 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 09:39:58.944556 29777 sgd_solver.cpp:106] Iteration 140700, lr = 0.00560312
I0630 09:40:15.015307 29777 solver.cpp:290] Iteration 140800 (6.22265 iter/s, 16.0703s/100 iter), loss = 1.13095
I0630 09:40:15.015529 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 09:40:15.015557 29777 sgd_solver.cpp:106] Iteration 140800, lr = 0.0056
I0630 09:40:31.121309 29777 solver.cpp:290] Iteration 140900 (6.20912 iter/s, 16.1053s/100 iter), loss = 1.14286
I0630 09:40:31.121332 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 09:40:31.121338 29777 sgd_solver.cpp:106] Iteration 140900, lr = 0.00559687
I0630 09:40:46.882495 29777 solver.cpp:354] Sparsity after update:
I0630 09:40:46.903064 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:40:46.903077 29777 net.cpp:1851] conv1a_param_0(0.355) 
I0630 09:40:46.903085 29777 net.cpp:1851] conv1b_param_0(0.71) 
I0630 09:40:46.903089 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:40:46.903090 29777 net.cpp:1851] res2a_branch2a_param_0(0.71) 
I0630 09:40:46.903092 29777 net.cpp:1851] res2a_branch2b_param_0(0.71) 
I0630 09:40:46.903095 29777 net.cpp:1851] res3a_branch2a_param_0(0.71) 
I0630 09:40:46.903096 29777 net.cpp:1851] res3a_branch2b_param_0(0.71) 
I0630 09:40:46.903098 29777 net.cpp:1851] res4a_branch2a_param_0(0.71) 
I0630 09:40:46.903100 29777 net.cpp:1851] res4a_branch2b_param_0(0.71) 
I0630 09:40:46.903102 29777 net.cpp:1851] res5a_branch2a_param_0(0.71) 
I0630 09:40:46.903105 29777 net.cpp:1851] res5a_branch2b_param_0(0.71) 
I0630 09:40:46.903106 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.67104e+06/2.86678e+06) 0.583
I0630 09:40:47.060039 29777 solver.cpp:290] Iteration 141000 (6.27421 iter/s, 15.9383s/100 iter), loss = 1.32143
I0630 09:40:47.060062 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 09:40:47.060068 29777 sgd_solver.cpp:106] Iteration 141000, lr = 0.00559375
I0630 09:41:03.104647 29777 solver.cpp:290] Iteration 141100 (6.2328 iter/s, 16.0441s/100 iter), loss = 1.25
I0630 09:41:03.104673 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 09:41:03.104682 29777 sgd_solver.cpp:106] Iteration 141100, lr = 0.00559062
I0630 09:41:19.145864 29777 solver.cpp:290] Iteration 141200 (6.23412 iter/s, 16.0408s/100 iter), loss = 1.07143
I0630 09:41:19.145954 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 09:41:19.145965 29777 sgd_solver.cpp:106] Iteration 141200, lr = 0.0055875
I0630 09:41:35.128746 29777 solver.cpp:290] Iteration 141300 (6.2569 iter/s, 15.9824s/100 iter), loss = 1.19048
I0630 09:41:35.128772 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 09:41:35.128782 29777 sgd_solver.cpp:106] Iteration 141300, lr = 0.00558437
I0630 09:41:51.060901 29777 solver.cpp:290] Iteration 141400 (6.2768 iter/s, 15.9317s/100 iter), loss = 1.33333
I0630 09:41:51.061005 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 09:41:51.061014 29777 sgd_solver.cpp:106] Iteration 141400, lr = 0.00558125
I0630 09:42:07.152451 29777 solver.cpp:290] Iteration 141500 (6.21465 iter/s, 16.091s/100 iter), loss = 1.34524
I0630 09:42:07.152474 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 09:42:07.152480 29777 sgd_solver.cpp:106] Iteration 141500, lr = 0.00557812
I0630 09:42:23.303364 29777 solver.cpp:290] Iteration 141600 (6.19178 iter/s, 16.1504s/100 iter), loss = 1.04762
I0630 09:42:23.303459 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 09:42:23.303470 29777 sgd_solver.cpp:106] Iteration 141600, lr = 0.005575
I0630 09:42:39.234614 29777 solver.cpp:290] Iteration 141700 (6.27718 iter/s, 15.9307s/100 iter), loss = 1.67857
I0630 09:42:39.234637 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 09:42:39.234652 29777 sgd_solver.cpp:106] Iteration 141700, lr = 0.00557187
I0630 09:42:55.225003 29777 solver.cpp:290] Iteration 141800 (6.25394 iter/s, 15.9899s/100 iter), loss = 1.5119
I0630 09:42:55.225075 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 09:42:55.225085 29777 sgd_solver.cpp:106] Iteration 141800, lr = 0.00556875
I0630 09:43:11.152107 29777 solver.cpp:290] Iteration 141900 (6.2788 iter/s, 15.9266s/100 iter), loss = 1.19048
I0630 09:43:11.152133 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 09:43:11.152143 29777 sgd_solver.cpp:106] Iteration 141900, lr = 0.00556563
I0630 09:43:26.976514 29777 solver.cpp:354] Sparsity after update:
I0630 09:43:26.977957 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:43:26.977963 29777 net.cpp:1851] conv1a_param_0(0.355) 
I0630 09:43:26.977972 29777 net.cpp:1851] conv1b_param_0(0.71) 
I0630 09:43:26.977973 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:43:26.977977 29777 net.cpp:1851] res2a_branch2a_param_0(0.71) 
I0630 09:43:26.977978 29777 net.cpp:1851] res2a_branch2b_param_0(0.71) 
I0630 09:43:26.977980 29777 net.cpp:1851] res3a_branch2a_param_0(0.71) 
I0630 09:43:26.977982 29777 net.cpp:1851] res3a_branch2b_param_0(0.71) 
I0630 09:43:26.977984 29777 net.cpp:1851] res4a_branch2a_param_0(0.71) 
I0630 09:43:26.977988 29777 net.cpp:1851] res4a_branch2b_param_0(0.71) 
I0630 09:43:26.977989 29777 net.cpp:1851] res5a_branch2a_param_0(0.71) 
I0630 09:43:26.977991 29777 net.cpp:1851] res5a_branch2b_param_0(0.71) 
I0630 09:43:26.977993 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.67104e+06/2.86678e+06) 0.583
I0630 09:43:26.978081 29777 solver.cpp:471] Iteration 142000, Testing net (#0)
I0630 09:43:38.118890 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 09:44:15.749691 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5545
I0630 09:44:15.749758 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.792361
I0630 09:44:15.749768 29777 solver.cpp:544]     Test net output #2: loss = 1.58894 (* 1 = 1.58894 loss)
I0630 09:44:15.924554 29777 solver.cpp:290] Iteration 142000 (1.54391 iter/s, 64.7707s/100 iter), loss = 1.64286
I0630 09:44:15.924582 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 09:44:15.924590 29777 sgd_solver.cpp:106] Iteration 142000, lr = 0.0055625
I0630 09:44:15.925572 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.72
I0630 09:44:16.487495 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 09:44:32.438103 29777 solver.cpp:290] Iteration 142100 (6.05581 iter/s, 16.5131s/100 iter), loss = 1.20238
I0630 09:44:32.438128 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 09:44:32.438135 29777 sgd_solver.cpp:106] Iteration 142100, lr = 0.00555938
I0630 09:44:48.412338 29777 solver.cpp:290] Iteration 142200 (6.26026 iter/s, 15.9738s/100 iter), loss = 1.11905
I0630 09:44:48.412417 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 09:44:48.412425 29777 sgd_solver.cpp:106] Iteration 142200, lr = 0.00555625
I0630 09:45:04.491364 29777 solver.cpp:290] Iteration 142300 (6.21948 iter/s, 16.0785s/100 iter), loss = 1.33333
I0630 09:45:04.491389 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 09:45:04.491395 29777 sgd_solver.cpp:106] Iteration 142300, lr = 0.00555312
I0630 09:45:20.536253 29777 solver.cpp:290] Iteration 142400 (6.2327 iter/s, 16.0444s/100 iter), loss = 1.34524
I0630 09:45:20.536381 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 09:45:20.536391 29777 sgd_solver.cpp:106] Iteration 142400, lr = 0.00555
I0630 09:45:36.514550 29777 solver.cpp:290] Iteration 142500 (6.25871 iter/s, 15.9777s/100 iter), loss = 1.64286
I0630 09:45:36.514575 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 09:45:36.514585 29777 sgd_solver.cpp:106] Iteration 142500, lr = 0.00554687
I0630 09:45:52.612013 29777 solver.cpp:290] Iteration 142600 (6.21234 iter/s, 16.097s/100 iter), loss = 1.19048
I0630 09:45:52.612085 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 09:45:52.612095 29777 sgd_solver.cpp:106] Iteration 142600, lr = 0.00554375
I0630 09:46:08.608492 29777 solver.cpp:290] Iteration 142700 (6.25157 iter/s, 15.996s/100 iter), loss = 1.15476
I0630 09:46:08.608515 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 09:46:08.608522 29777 sgd_solver.cpp:106] Iteration 142700, lr = 0.00554062
I0630 09:46:24.709108 29777 solver.cpp:290] Iteration 142800 (6.21112 iter/s, 16.1002s/100 iter), loss = 1.29762
I0630 09:46:24.709170 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 09:46:24.709182 29777 sgd_solver.cpp:106] Iteration 142800, lr = 0.0055375
I0630 09:46:40.742142 29777 solver.cpp:290] Iteration 142900 (6.23732 iter/s, 16.0325s/100 iter), loss = 1.22619
I0630 09:46:40.742169 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 09:46:40.742179 29777 sgd_solver.cpp:106] Iteration 142900, lr = 0.00553437
I0630 09:46:56.577419 29777 solver.cpp:354] Sparsity after update:
I0630 09:46:56.597921 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:46:56.597937 29777 net.cpp:1851] conv1a_param_0(0.36) 
I0630 09:46:56.597947 29777 net.cpp:1851] conv1b_param_0(0.715) 
I0630 09:46:56.597951 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:46:56.597955 29777 net.cpp:1851] res2a_branch2a_param_0(0.72) 
I0630 09:46:56.597960 29777 net.cpp:1851] res2a_branch2b_param_0(0.72) 
I0630 09:46:56.597964 29777 net.cpp:1851] res3a_branch2a_param_0(0.72) 
I0630 09:46:56.597966 29777 net.cpp:1851] res3a_branch2b_param_0(0.72) 
I0630 09:46:56.597970 29777 net.cpp:1851] res4a_branch2a_param_0(0.72) 
I0630 09:46:56.597973 29777 net.cpp:1851] res4a_branch2b_param_0(0.72) 
I0630 09:46:56.597976 29777 net.cpp:1851] res5a_branch2a_param_0(0.72) 
I0630 09:46:56.597980 29777 net.cpp:1851] res5a_branch2b_param_0(0.72) 
I0630 09:46:56.597983 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.69456e+06/2.86678e+06) 0.591
I0630 09:46:56.754029 29777 solver.cpp:290] Iteration 143000 (6.24554 iter/s, 16.0114s/100 iter), loss = 1.82143
I0630 09:46:56.754051 29777 solver.cpp:309]     Train net output #0: loss = 2.07143 (* 1 = 2.07143 loss)
I0630 09:46:56.754060 29777 sgd_solver.cpp:106] Iteration 143000, lr = 0.00553125
I0630 09:47:12.773581 29777 solver.cpp:290] Iteration 143100 (6.24255 iter/s, 16.0191s/100 iter), loss = 1.15476
I0630 09:47:12.773605 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 09:47:12.773614 29777 sgd_solver.cpp:106] Iteration 143100, lr = 0.00552812
I0630 09:47:28.847419 29777 solver.cpp:290] Iteration 143200 (6.22147 iter/s, 16.0734s/100 iter), loss = 1.21429
I0630 09:47:28.847527 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 09:47:28.847537 29777 sgd_solver.cpp:106] Iteration 143200, lr = 0.005525
I0630 09:47:44.988567 29777 solver.cpp:290] Iteration 143300 (6.19556 iter/s, 16.1406s/100 iter), loss = 1.4881
I0630 09:47:44.988590 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 09:47:44.988597 29777 sgd_solver.cpp:106] Iteration 143300, lr = 0.00552187
I0630 09:48:01.018296 29777 solver.cpp:290] Iteration 143400 (6.23859 iter/s, 16.0293s/100 iter), loss = 1.61905
I0630 09:48:01.018370 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 09:48:01.018383 29777 sgd_solver.cpp:106] Iteration 143400, lr = 0.00551875
I0630 09:48:17.067549 29777 solver.cpp:290] Iteration 143500 (6.23102 iter/s, 16.0487s/100 iter), loss = 1.47619
I0630 09:48:17.067575 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 09:48:17.067584 29777 sgd_solver.cpp:106] Iteration 143500, lr = 0.00551562
I0630 09:48:33.154450 29777 solver.cpp:290] Iteration 143600 (6.21642 iter/s, 16.0864s/100 iter), loss = 0.904762
I0630 09:48:33.154525 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 09:48:33.154534 29777 sgd_solver.cpp:106] Iteration 143600, lr = 0.0055125
I0630 09:48:49.197538 29777 solver.cpp:290] Iteration 143700 (6.23341 iter/s, 16.0426s/100 iter), loss = 1.5119
I0630 09:48:49.197587 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 09:48:49.197602 29777 sgd_solver.cpp:106] Iteration 143700, lr = 0.00550938
I0630 09:49:05.186848 29777 solver.cpp:290] Iteration 143800 (6.25437 iter/s, 15.9888s/100 iter), loss = 1.42857
I0630 09:49:05.186940 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 09:49:05.186961 29777 sgd_solver.cpp:106] Iteration 143800, lr = 0.00550625
I0630 09:49:21.317437 29777 solver.cpp:290] Iteration 143900 (6.1996 iter/s, 16.1301s/100 iter), loss = 1.16667
I0630 09:49:21.317461 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 09:49:21.317467 29777 sgd_solver.cpp:106] Iteration 143900, lr = 0.00550313
I0630 09:49:37.214335 29777 solver.cpp:354] Sparsity after update:
I0630 09:49:37.216246 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:49:37.216262 29777 net.cpp:1851] conv1a_param_0(0.36) 
I0630 09:49:37.216274 29777 net.cpp:1851] conv1b_param_0(0.715) 
I0630 09:49:37.216277 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:49:37.216279 29777 net.cpp:1851] res2a_branch2a_param_0(0.72) 
I0630 09:49:37.216282 29777 net.cpp:1851] res2a_branch2b_param_0(0.72) 
I0630 09:49:37.216285 29777 net.cpp:1851] res3a_branch2a_param_0(0.72) 
I0630 09:49:37.216289 29777 net.cpp:1851] res3a_branch2b_param_0(0.72) 
I0630 09:49:37.216291 29777 net.cpp:1851] res4a_branch2a_param_0(0.72) 
I0630 09:49:37.216295 29777 net.cpp:1851] res4a_branch2b_param_0(0.72) 
I0630 09:49:37.216296 29777 net.cpp:1851] res5a_branch2a_param_0(0.72) 
I0630 09:49:37.216297 29777 net.cpp:1851] res5a_branch2b_param_0(0.72) 
I0630 09:49:37.216300 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.69456e+06/2.86678e+06) 0.591
I0630 09:49:37.216406 29777 solver.cpp:471] Iteration 144000, Testing net (#0)
I0630 09:49:50.949750 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 09:50:29.207171 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55906
I0630 09:50:29.207281 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.794261
I0630 09:50:29.207293 29777 solver.cpp:544]     Test net output #2: loss = 1.56848 (* 1 = 1.56848 loss)
I0630 09:50:29.390657 29777 solver.cpp:290] Iteration 144000 (1.46905 iter/s, 68.0714s/100 iter), loss = 0.904762
I0630 09:50:29.390682 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 09:50:29.390691 29777 sgd_solver.cpp:106] Iteration 144000, lr = 0.0055
I0630 09:50:29.391404 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.73
I0630 09:50:29.950243 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 09:50:45.935781 29777 solver.cpp:290] Iteration 144100 (6.04425 iter/s, 16.5446s/100 iter), loss = 1.44048
I0630 09:50:45.935806 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 09:50:45.935811 29777 sgd_solver.cpp:106] Iteration 144100, lr = 0.00549687
I0630 09:51:02.125020 29777 solver.cpp:290] Iteration 144200 (6.17712 iter/s, 16.1888s/100 iter), loss = 1.2619
I0630 09:51:02.125113 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 09:51:02.125124 29777 sgd_solver.cpp:106] Iteration 144200, lr = 0.00549375
I0630 09:51:18.287909 29777 solver.cpp:290] Iteration 144300 (6.18722 iter/s, 16.1624s/100 iter), loss = 1.53571
I0630 09:51:18.287932 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 09:51:18.287938 29777 sgd_solver.cpp:106] Iteration 144300, lr = 0.00549062
I0630 09:51:34.489845 29777 solver.cpp:290] Iteration 144400 (6.17228 iter/s, 16.2015s/100 iter), loss = 1.47619
I0630 09:51:34.489943 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 09:51:34.489976 29777 sgd_solver.cpp:106] Iteration 144400, lr = 0.0054875
I0630 09:51:50.501173 29777 solver.cpp:290] Iteration 144500 (6.24578 iter/s, 16.0108s/100 iter), loss = 0.916667
I0630 09:51:50.501196 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 09:51:50.501204 29777 sgd_solver.cpp:106] Iteration 144500, lr = 0.00548437
I0630 09:52:06.516988 29777 solver.cpp:290] Iteration 144600 (6.24401 iter/s, 16.0154s/100 iter), loss = 1.2381
I0630 09:52:06.517091 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 09:52:06.517101 29777 sgd_solver.cpp:106] Iteration 144600, lr = 0.00548125
I0630 09:52:22.488925 29777 solver.cpp:290] Iteration 144700 (6.26119 iter/s, 15.9714s/100 iter), loss = 1.11905
I0630 09:52:22.488947 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 09:52:22.488955 29777 sgd_solver.cpp:106] Iteration 144700, lr = 0.00547812
I0630 09:52:38.405555 29777 solver.cpp:290] Iteration 144800 (6.28292 iter/s, 15.9162s/100 iter), loss = 1.30952
I0630 09:52:38.405614 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 09:52:38.405625 29777 sgd_solver.cpp:106] Iteration 144800, lr = 0.005475
I0630 09:52:54.549932 29777 solver.cpp:290] Iteration 144900 (6.1943 iter/s, 16.1439s/100 iter), loss = 1.36905
I0630 09:52:54.549955 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 09:52:54.549962 29777 sgd_solver.cpp:106] Iteration 144900, lr = 0.00547187
I0630 09:53:10.424510 29777 solver.cpp:354] Sparsity after update:
I0630 09:53:10.445008 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:53:10.445062 29777 net.cpp:1851] conv1a_param_0(0.365) 
I0630 09:53:10.445088 29777 net.cpp:1851] conv1b_param_0(0.717) 
I0630 09:53:10.445101 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:53:10.445111 29777 net.cpp:1851] res2a_branch2a_param_0(0.73) 
I0630 09:53:10.445123 29777 net.cpp:1851] res2a_branch2b_param_0(0.73) 
I0630 09:53:10.445133 29777 net.cpp:1851] res3a_branch2a_param_0(0.73) 
I0630 09:53:10.445143 29777 net.cpp:1851] res3a_branch2b_param_0(0.73) 
I0630 09:53:10.445154 29777 net.cpp:1851] res4a_branch2a_param_0(0.73) 
I0630 09:53:10.445163 29777 net.cpp:1851] res4a_branch2b_param_0(0.73) 
I0630 09:53:10.445174 29777 net.cpp:1851] res5a_branch2a_param_0(0.73) 
I0630 09:53:10.445184 29777 net.cpp:1851] res5a_branch2b_param_0(0.73) 
I0630 09:53:10.445195 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.71808e+06/2.86678e+06) 0.599
I0630 09:53:10.661783 29777 solver.cpp:290] Iteration 145000 (6.20679 iter/s, 16.1114s/100 iter), loss = 0.976191
I0630 09:53:10.661809 29777 solver.cpp:309]     Train net output #0: loss = 0.52381 (* 1 = 0.52381 loss)
I0630 09:53:10.661815 29777 sgd_solver.cpp:106] Iteration 145000, lr = 0.00546875
I0630 09:53:26.730996 29777 solver.cpp:290] Iteration 145100 (6.22326 iter/s, 16.0688s/100 iter), loss = 1.38095
I0630 09:53:26.731019 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 09:53:26.731026 29777 sgd_solver.cpp:106] Iteration 145100, lr = 0.00546562
I0630 09:53:42.833976 29777 solver.cpp:290] Iteration 145200 (6.21021 iter/s, 16.1025s/100 iter), loss = 1.15476
I0630 09:53:42.834058 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 09:53:42.834065 29777 sgd_solver.cpp:106] Iteration 145200, lr = 0.0054625
I0630 09:53:58.837447 29777 solver.cpp:290] Iteration 145300 (6.24885 iter/s, 16.003s/100 iter), loss = 0.916667
I0630 09:53:58.837471 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 09:53:58.837476 29777 sgd_solver.cpp:106] Iteration 145300, lr = 0.00545938
I0630 09:54:14.763298 29777 solver.cpp:290] Iteration 145400 (6.27928 iter/s, 15.9254s/100 iter), loss = 1.39286
I0630 09:54:14.763357 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 09:54:14.763368 29777 sgd_solver.cpp:106] Iteration 145400, lr = 0.00545625
I0630 09:54:30.863906 29777 solver.cpp:290] Iteration 145500 (6.21114 iter/s, 16.1001s/100 iter), loss = 1.4881
I0630 09:54:30.863930 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 09:54:30.863937 29777 sgd_solver.cpp:106] Iteration 145500, lr = 0.00545313
I0630 09:54:46.871554 29777 solver.cpp:290] Iteration 145600 (6.24719 iter/s, 16.0072s/100 iter), loss = 1.14286
I0630 09:54:46.871659 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 09:54:46.871667 29777 sgd_solver.cpp:106] Iteration 145600, lr = 0.00545
I0630 09:55:03.037358 29777 solver.cpp:290] Iteration 145700 (6.18611 iter/s, 16.1653s/100 iter), loss = 1.35714
I0630 09:55:03.037380 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 09:55:03.037387 29777 sgd_solver.cpp:106] Iteration 145700, lr = 0.00544688
I0630 09:55:19.064007 29777 solver.cpp:290] Iteration 145800 (6.23979 iter/s, 16.0262s/100 iter), loss = 1.03571
I0630 09:55:19.064110 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 09:55:19.064119 29777 sgd_solver.cpp:106] Iteration 145800, lr = 0.00544375
I0630 09:55:35.141397 29777 solver.cpp:290] Iteration 145900 (6.22012 iter/s, 16.0768s/100 iter), loss = 0.785714
I0630 09:55:35.141419 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 09:55:35.141425 29777 sgd_solver.cpp:106] Iteration 145900, lr = 0.00544062
I0630 09:55:50.917417 29777 solver.cpp:354] Sparsity after update:
I0630 09:55:50.919719 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:55:50.919832 29777 net.cpp:1851] conv1a_param_0(0.365) 
I0630 09:55:50.919919 29777 net.cpp:1851] conv1b_param_0(0.717) 
I0630 09:55:50.919991 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:55:50.920063 29777 net.cpp:1851] res2a_branch2a_param_0(0.73) 
I0630 09:55:50.920135 29777 net.cpp:1851] res2a_branch2b_param_0(0.73) 
I0630 09:55:50.920210 29777 net.cpp:1851] res3a_branch2a_param_0(0.73) 
I0630 09:55:50.920287 29777 net.cpp:1851] res3a_branch2b_param_0(0.73) 
I0630 09:55:50.920361 29777 net.cpp:1851] res4a_branch2a_param_0(0.73) 
I0630 09:55:50.920435 29777 net.cpp:1851] res4a_branch2b_param_0(0.73) 
I0630 09:55:50.920509 29777 net.cpp:1851] res5a_branch2a_param_0(0.73) 
I0630 09:55:50.920583 29777 net.cpp:1851] res5a_branch2b_param_0(0.73) 
I0630 09:55:50.920657 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.71808e+06/2.86678e+06) 0.599
I0630 09:55:50.921016 29777 solver.cpp:471] Iteration 146000, Testing net (#0)
I0630 09:56:02.880383 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 09:56:40.294417 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5558
I0630 09:56:40.294463 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.793021
I0630 09:56:40.294471 29777 solver.cpp:544]     Test net output #2: loss = 1.59158 (* 1 = 1.59158 loss)
I0630 09:56:40.473960 29777 solver.cpp:290] Iteration 146000 (1.53067 iter/s, 65.3308s/100 iter), loss = 1.16667
I0630 09:56:40.473991 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 09:56:40.474001 29777 sgd_solver.cpp:106] Iteration 146000, lr = 0.0054375
I0630 09:56:40.475139 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.74
I0630 09:56:41.047827 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 09:56:56.958389 29777 solver.cpp:290] Iteration 146100 (6.06651 iter/s, 16.4839s/100 iter), loss = 1.13095
I0630 09:56:56.958413 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 09:56:56.958422 29777 sgd_solver.cpp:106] Iteration 146100, lr = 0.00543437
I0630 09:57:12.982522 29777 solver.cpp:290] Iteration 146200 (6.24077 iter/s, 16.0237s/100 iter), loss = 1.45238
I0630 09:57:12.982643 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 09:57:12.982653 29777 sgd_solver.cpp:106] Iteration 146200, lr = 0.00543125
I0630 09:57:29.010248 29777 solver.cpp:290] Iteration 146300 (6.23941 iter/s, 16.0272s/100 iter), loss = 1.36905
I0630 09:57:29.010277 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 09:57:29.010287 29777 sgd_solver.cpp:106] Iteration 146300, lr = 0.00542812
I0630 09:57:45.020198 29777 solver.cpp:290] Iteration 146400 (6.2463 iter/s, 16.0095s/100 iter), loss = 1.13095
I0630 09:57:45.020308 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 09:57:45.020326 29777 sgd_solver.cpp:106] Iteration 146400, lr = 0.005425
I0630 09:58:01.105541 29777 solver.cpp:290] Iteration 146500 (6.21705 iter/s, 16.0848s/100 iter), loss = 1.42857
I0630 09:58:01.105563 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 09:58:01.105571 29777 sgd_solver.cpp:106] Iteration 146500, lr = 0.00542188
I0630 09:58:17.100450 29777 solver.cpp:290] Iteration 146600 (6.25217 iter/s, 15.9944s/100 iter), loss = 1.44048
I0630 09:58:17.100499 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 09:58:17.100507 29777 sgd_solver.cpp:106] Iteration 146600, lr = 0.00541875
I0630 09:58:33.140748 29777 solver.cpp:290] Iteration 146700 (6.23449 iter/s, 16.0398s/100 iter), loss = 1.58333
I0630 09:58:33.140789 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 09:58:33.140800 29777 sgd_solver.cpp:106] Iteration 146700, lr = 0.00541562
I0630 09:58:49.152766 29777 solver.cpp:290] Iteration 146800 (6.2455 iter/s, 16.0115s/100 iter), loss = 1.16667
I0630 09:58:49.152869 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 09:58:49.152879 29777 sgd_solver.cpp:106] Iteration 146800, lr = 0.0054125
I0630 09:59:05.173409 29777 solver.cpp:290] Iteration 146900 (6.24216 iter/s, 16.0201s/100 iter), loss = 1.2619
I0630 09:59:05.173436 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 09:59:05.173454 29777 sgd_solver.cpp:106] Iteration 146900, lr = 0.00540938
I0630 09:59:21.014077 29777 solver.cpp:354] Sparsity after update:
I0630 09:59:21.034693 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 09:59:21.034713 29777 net.cpp:1851] conv1a_param_0(0.37) 
I0630 09:59:21.034723 29777 net.cpp:1851] conv1b_param_0(0.72) 
I0630 09:59:21.034728 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 09:59:21.034730 29777 net.cpp:1851] res2a_branch2a_param_0(0.74) 
I0630 09:59:21.034734 29777 net.cpp:1851] res2a_branch2b_param_0(0.74) 
I0630 09:59:21.034741 29777 net.cpp:1851] res3a_branch2a_param_0(0.74) 
I0630 09:59:21.034746 29777 net.cpp:1851] res3a_branch2b_param_0(0.74) 
I0630 09:59:21.034751 29777 net.cpp:1851] res4a_branch2a_param_0(0.74) 
I0630 09:59:21.034756 29777 net.cpp:1851] res4a_branch2b_param_0(0.74) 
I0630 09:59:21.034761 29777 net.cpp:1851] res5a_branch2a_param_0(0.74) 
I0630 09:59:21.034766 29777 net.cpp:1851] res5a_branch2b_param_0(0.74) 
I0630 09:59:21.034770 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.7416e+06/2.86678e+06) 0.608
I0630 09:59:21.191740 29777 solver.cpp:290] Iteration 147000 (6.24303 iter/s, 16.0179s/100 iter), loss = 0.928571
I0630 09:59:21.191763 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 09:59:21.191769 29777 sgd_solver.cpp:106] Iteration 147000, lr = 0.00540625
I0630 09:59:37.301872 29777 solver.cpp:290] Iteration 147100 (6.20745 iter/s, 16.1097s/100 iter), loss = 1.38095
I0630 09:59:37.301894 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 09:59:37.301901 29777 sgd_solver.cpp:106] Iteration 147100, lr = 0.00540313
I0630 09:59:53.365015 29777 solver.cpp:290] Iteration 147200 (6.22561 iter/s, 16.0627s/100 iter), loss = 1.14286
I0630 09:59:53.365131 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 09:59:53.365142 29777 sgd_solver.cpp:106] Iteration 147200, lr = 0.0054
I0630 10:00:09.277881 29777 solver.cpp:290] Iteration 147300 (6.28444 iter/s, 15.9123s/100 iter), loss = 1.46429
I0630 10:00:09.277907 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 10:00:09.277923 29777 sgd_solver.cpp:106] Iteration 147300, lr = 0.00539688
I0630 10:00:25.316447 29777 solver.cpp:290] Iteration 147400 (6.23515 iter/s, 16.0381s/100 iter), loss = 1.22619
I0630 10:00:25.316504 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 10:00:25.316514 29777 sgd_solver.cpp:106] Iteration 147400, lr = 0.00539375
I0630 10:00:41.264178 29777 solver.cpp:290] Iteration 147500 (6.27068 iter/s, 15.9472s/100 iter), loss = 1.58333
I0630 10:00:41.264205 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 10:00:41.264214 29777 sgd_solver.cpp:106] Iteration 147500, lr = 0.00539062
I0630 10:00:57.221186 29777 solver.cpp:290] Iteration 147600 (6.26702 iter/s, 15.9565s/100 iter), loss = 1.19048
I0630 10:00:57.221293 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 10:00:57.221303 29777 sgd_solver.cpp:106] Iteration 147600, lr = 0.0053875
I0630 10:01:13.285178 29777 solver.cpp:290] Iteration 147700 (6.22531 iter/s, 16.0634s/100 iter), loss = 1.2381
I0630 10:01:13.285202 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 10:01:13.285208 29777 sgd_solver.cpp:106] Iteration 147700, lr = 0.00538437
I0630 10:01:29.275897 29777 solver.cpp:290] Iteration 147800 (6.25381 iter/s, 15.9903s/100 iter), loss = 1.35714
I0630 10:01:29.276008 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 10:01:29.276017 29777 sgd_solver.cpp:106] Iteration 147800, lr = 0.00538125
I0630 10:01:45.502769 29777 solver.cpp:290] Iteration 147900 (6.16283 iter/s, 16.2263s/100 iter), loss = 1.05952
I0630 10:01:45.502792 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 10:01:45.502799 29777 sgd_solver.cpp:106] Iteration 147900, lr = 0.00537812
I0630 10:02:01.323433 29777 solver.cpp:354] Sparsity after update:
I0630 10:02:01.324715 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:02:01.324723 29777 net.cpp:1851] conv1a_param_0(0.37) 
I0630 10:02:01.324729 29777 net.cpp:1851] conv1b_param_0(0.72) 
I0630 10:02:01.324731 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:02:01.324733 29777 net.cpp:1851] res2a_branch2a_param_0(0.74) 
I0630 10:02:01.324735 29777 net.cpp:1851] res2a_branch2b_param_0(0.74) 
I0630 10:02:01.324738 29777 net.cpp:1851] res3a_branch2a_param_0(0.74) 
I0630 10:02:01.324739 29777 net.cpp:1851] res3a_branch2b_param_0(0.74) 
I0630 10:02:01.324741 29777 net.cpp:1851] res4a_branch2a_param_0(0.74) 
I0630 10:02:01.324743 29777 net.cpp:1851] res4a_branch2b_param_0(0.74) 
I0630 10:02:01.324745 29777 net.cpp:1851] res5a_branch2a_param_0(0.74) 
I0630 10:02:01.324748 29777 net.cpp:1851] res5a_branch2b_param_0(0.74) 
I0630 10:02:01.324749 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.7416e+06/2.86678e+06) 0.608
I0630 10:02:01.324836 29777 solver.cpp:471] Iteration 148000, Testing net (#0)
I0630 10:02:13.386930 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 10:02:49.851163 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55324
I0630 10:02:49.851287 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.792421
I0630 10:02:49.851297 29777 solver.cpp:544]     Test net output #2: loss = 1.58856 (* 1 = 1.58856 loss)
I0630 10:02:50.027214 29777 solver.cpp:290] Iteration 148000 (1.54984 iter/s, 64.5227s/100 iter), loss = 1.03571
I0630 10:02:50.027236 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 10:02:50.027243 29777 sgd_solver.cpp:106] Iteration 148000, lr = 0.005375
I0630 10:02:50.028033 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.75
I0630 10:02:50.595365 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 10:03:06.509083 29777 solver.cpp:290] Iteration 148100 (6.06745 iter/s, 16.4814s/100 iter), loss = 1.35714
I0630 10:03:06.509109 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 10:03:06.509117 29777 sgd_solver.cpp:106] Iteration 148100, lr = 0.00537187
I0630 10:03:22.497797 29777 solver.cpp:290] Iteration 148200 (6.25459 iter/s, 15.9883s/100 iter), loss = 1.0119
I0630 10:03:22.497920 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 10:03:22.497936 29777 sgd_solver.cpp:106] Iteration 148200, lr = 0.00536875
I0630 10:03:38.526119 29777 solver.cpp:290] Iteration 148300 (6.23917 iter/s, 16.0278s/100 iter), loss = 1.09524
I0630 10:03:38.526144 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 10:03:38.526152 29777 sgd_solver.cpp:106] Iteration 148300, lr = 0.00536563
I0630 10:03:54.655853 29777 solver.cpp:290] Iteration 148400 (6.19991 iter/s, 16.1293s/100 iter), loss = 1.2619
I0630 10:03:54.655956 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 10:03:54.655966 29777 sgd_solver.cpp:106] Iteration 148400, lr = 0.0053625
I0630 10:04:10.641510 29777 solver.cpp:290] Iteration 148500 (6.25582 iter/s, 15.9851s/100 iter), loss = 1.2619
I0630 10:04:10.641532 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 10:04:10.641540 29777 sgd_solver.cpp:106] Iteration 148500, lr = 0.00535937
I0630 10:04:26.609143 29777 solver.cpp:290] Iteration 148600 (6.26285 iter/s, 15.9672s/100 iter), loss = 1.42857
I0630 10:04:26.609222 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 10:04:26.609233 29777 sgd_solver.cpp:106] Iteration 148600, lr = 0.00535625
I0630 10:04:42.559481 29777 solver.cpp:290] Iteration 148700 (6.26966 iter/s, 15.9498s/100 iter), loss = 1.07143
I0630 10:04:42.559507 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 10:04:42.559515 29777 sgd_solver.cpp:106] Iteration 148700, lr = 0.00535313
I0630 10:04:58.598582 29777 solver.cpp:290] Iteration 148800 (6.23494 iter/s, 16.0386s/100 iter), loss = 0.964286
I0630 10:04:58.598646 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 10:04:58.598659 29777 sgd_solver.cpp:106] Iteration 148800, lr = 0.00535
I0630 10:05:14.471549 29777 solver.cpp:290] Iteration 148900 (6.30022 iter/s, 15.8725s/100 iter), loss = 1.34524
I0630 10:05:14.471570 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 10:05:14.471577 29777 sgd_solver.cpp:106] Iteration 148900, lr = 0.00534688
I0630 10:05:30.319589 29777 solver.cpp:354] Sparsity after update:
I0630 10:05:30.340821 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:05:30.340836 29777 net.cpp:1851] conv1a_param_0(0.375) 
I0630 10:05:30.340845 29777 net.cpp:1851] conv1b_param_0(0.72) 
I0630 10:05:30.340848 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:05:30.340852 29777 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0630 10:05:30.340855 29777 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0630 10:05:30.340865 29777 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0630 10:05:30.340870 29777 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0630 10:05:30.340874 29777 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0630 10:05:30.340878 29777 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0630 10:05:30.340881 29777 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0630 10:05:30.340886 29777 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0630 10:05:30.340891 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.76511e+06/2.86678e+06) 0.616
I0630 10:05:30.497732 29777 solver.cpp:290] Iteration 149000 (6.23997 iter/s, 16.0257s/100 iter), loss = 1.47619
I0630 10:05:30.497757 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 10:05:30.497766 29777 sgd_solver.cpp:106] Iteration 149000, lr = 0.00534375
I0630 10:05:46.620770 29777 solver.cpp:290] Iteration 149100 (6.20249 iter/s, 16.1226s/100 iter), loss = 1.40476
I0630 10:05:46.620857 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 10:05:46.620883 29777 sgd_solver.cpp:106] Iteration 149100, lr = 0.00534063
I0630 10:06:02.704632 29777 solver.cpp:290] Iteration 149200 (6.21761 iter/s, 16.0833s/100 iter), loss = 1.04762
I0630 10:06:02.704738 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 10:06:02.704746 29777 sgd_solver.cpp:106] Iteration 149200, lr = 0.0053375
I0630 10:06:18.775022 29777 solver.cpp:290] Iteration 149300 (6.22283 iter/s, 16.0698s/100 iter), loss = 1.22619
I0630 10:06:18.775044 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 10:06:18.775051 29777 sgd_solver.cpp:106] Iteration 149300, lr = 0.00533437
I0630 10:06:34.832060 29777 solver.cpp:290] Iteration 149400 (6.22798 iter/s, 16.0566s/100 iter), loss = 1.15476
I0630 10:06:34.832134 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 10:06:34.832141 29777 sgd_solver.cpp:106] Iteration 149400, lr = 0.00533125
I0630 10:06:50.951056 29777 solver.cpp:290] Iteration 149500 (6.20406 iter/s, 16.1185s/100 iter), loss = 1.29762
I0630 10:06:50.951081 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 10:06:50.951086 29777 sgd_solver.cpp:106] Iteration 149500, lr = 0.00532812
I0630 10:07:07.041116 29777 solver.cpp:290] Iteration 149600 (6.2152 iter/s, 16.0896s/100 iter), loss = 1.32143
I0630 10:07:07.041223 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 10:07:07.041232 29777 sgd_solver.cpp:106] Iteration 149600, lr = 0.005325
I0630 10:07:23.097823 29777 solver.cpp:290] Iteration 149700 (6.22814 iter/s, 16.0562s/100 iter), loss = 1.08333
I0630 10:07:23.097852 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 10:07:23.097860 29777 sgd_solver.cpp:106] Iteration 149700, lr = 0.00532187
I0630 10:07:39.182751 29777 solver.cpp:290] Iteration 149800 (6.21718 iter/s, 16.0845s/100 iter), loss = 1.41667
I0630 10:07:39.182821 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 10:07:39.182828 29777 sgd_solver.cpp:106] Iteration 149800, lr = 0.00531875
I0630 10:07:55.247972 29777 solver.cpp:290] Iteration 149900 (6.22482 iter/s, 16.0647s/100 iter), loss = 1.47619
I0630 10:07:55.247994 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 10:07:55.248001 29777 sgd_solver.cpp:106] Iteration 149900, lr = 0.00531563
I0630 10:08:11.138454 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_150000.caffemodel
I0630 10:08:11.158221 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_150000.solverstate
I0630 10:08:11.166895 29777 solver.cpp:354] Sparsity after update:
I0630 10:08:11.167876 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:08:11.167884 29777 net.cpp:1851] conv1a_param_0(0.375) 
I0630 10:08:11.167892 29777 net.cpp:1851] conv1b_param_0(0.72) 
I0630 10:08:11.167894 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:08:11.167897 29777 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0630 10:08:11.167899 29777 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0630 10:08:11.167901 29777 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0630 10:08:11.167903 29777 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0630 10:08:11.167906 29777 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0630 10:08:11.167906 29777 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0630 10:08:11.167908 29777 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0630 10:08:11.167910 29777 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0630 10:08:11.167912 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.76511e+06/2.86678e+06) 0.616
I0630 10:08:11.168009 29777 solver.cpp:471] Iteration 150000, Testing net (#0)
I0630 10:08:23.452693 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 10:08:59.359616 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55572
I0630 10:08:59.359745 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.790761
I0630 10:08:59.359755 29777 solver.cpp:544]     Test net output #2: loss = 1.59656 (* 1 = 1.59656 loss)
I0630 10:08:59.535370 29777 solver.cpp:290] Iteration 150000 (1.55556 iter/s, 64.2856s/100 iter), loss = 1.34524
I0630 10:08:59.535395 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 10:08:59.535401 29777 sgd_solver.cpp:106] Iteration 150000, lr = 0.0053125
I0630 10:08:59.536097 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.76
I0630 10:09:00.145678 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 10:09:16.038522 29777 solver.cpp:290] Iteration 150100 (6.05962 iter/s, 16.5027s/100 iter), loss = 1.02381
I0630 10:09:16.038545 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 10:09:16.038552 29777 sgd_solver.cpp:106] Iteration 150100, lr = 0.00530938
I0630 10:09:32.113008 29777 solver.cpp:290] Iteration 150200 (6.22122 iter/s, 16.074s/100 iter), loss = 1.47619
I0630 10:09:32.113116 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 10:09:32.113126 29777 sgd_solver.cpp:106] Iteration 150200, lr = 0.00530625
I0630 10:09:48.186712 29777 solver.cpp:290] Iteration 150300 (6.22155 iter/s, 16.0732s/100 iter), loss = 1.03571
I0630 10:09:48.186738 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 10:09:48.186748 29777 sgd_solver.cpp:106] Iteration 150300, lr = 0.00530313
I0630 10:10:04.265913 29777 solver.cpp:290] Iteration 150400 (6.2194 iter/s, 16.0787s/100 iter), loss = 1.09524
I0630 10:10:04.266008 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 10:10:04.266019 29777 sgd_solver.cpp:106] Iteration 150400, lr = 0.0053
I0630 10:10:20.298063 29777 solver.cpp:290] Iteration 150500 (6.23767 iter/s, 16.0316s/100 iter), loss = 1.72619
I0630 10:10:20.298087 29777 solver.cpp:309]     Train net output #0: loss = 1.97619 (* 1 = 1.97619 loss)
I0630 10:10:20.298096 29777 sgd_solver.cpp:106] Iteration 150500, lr = 0.00529688
I0630 10:10:36.355993 29777 solver.cpp:290] Iteration 150600 (6.22763 iter/s, 16.0575s/100 iter), loss = 1.29762
I0630 10:10:36.356083 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 10:10:36.356094 29777 sgd_solver.cpp:106] Iteration 150600, lr = 0.00529375
I0630 10:10:52.373821 29777 solver.cpp:290] Iteration 150700 (6.24325 iter/s, 16.0173s/100 iter), loss = 1.16667
I0630 10:10:52.373847 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 10:10:52.373855 29777 sgd_solver.cpp:106] Iteration 150700, lr = 0.00529063
I0630 10:11:08.287565 29777 solver.cpp:290] Iteration 150800 (6.28406 iter/s, 15.9133s/100 iter), loss = 1.08333
I0630 10:11:08.287674 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 10:11:08.287683 29777 sgd_solver.cpp:106] Iteration 150800, lr = 0.0052875
I0630 10:11:24.541869 29777 solver.cpp:290] Iteration 150900 (6.15243 iter/s, 16.2537s/100 iter), loss = 1.09524
I0630 10:11:24.541894 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 10:11:24.541903 29777 sgd_solver.cpp:106] Iteration 150900, lr = 0.00528438
I0630 10:11:40.440045 29777 solver.cpp:354] Sparsity after update:
I0630 10:11:40.460494 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:11:40.460513 29777 net.cpp:1851] conv1a_param_0(0.38) 
I0630 10:11:40.460525 29777 net.cpp:1851] conv1b_param_0(0.721) 
I0630 10:11:40.460527 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:11:40.460531 29777 net.cpp:1851] res2a_branch2a_param_0(0.76) 
I0630 10:11:40.460537 29777 net.cpp:1851] res2a_branch2b_param_0(0.76) 
I0630 10:11:40.460541 29777 net.cpp:1851] res3a_branch2a_param_0(0.76) 
I0630 10:11:40.460544 29777 net.cpp:1851] res3a_branch2b_param_0(0.76) 
I0630 10:11:40.460547 29777 net.cpp:1851] res4a_branch2a_param_0(0.76) 
I0630 10:11:40.460551 29777 net.cpp:1851] res4a_branch2b_param_0(0.76) 
I0630 10:11:40.460556 29777 net.cpp:1851] res5a_branch2a_param_0(0.76) 
I0630 10:11:40.460559 29777 net.cpp:1851] res5a_branch2b_param_0(0.76) 
I0630 10:11:40.460562 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.78863e+06/2.86678e+06) 0.624
I0630 10:11:40.617455 29777 solver.cpp:290] Iteration 151000 (6.22079 iter/s, 16.0751s/100 iter), loss = 1.39286
I0630 10:11:40.617478 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 10:11:40.617485 29777 sgd_solver.cpp:106] Iteration 151000, lr = 0.00528125
I0630 10:11:56.651868 29777 solver.cpp:290] Iteration 151100 (6.23677 iter/s, 16.0339s/100 iter), loss = 1.45238
I0630 10:11:56.651895 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 10:11:56.651904 29777 sgd_solver.cpp:106] Iteration 151100, lr = 0.00527812
I0630 10:12:12.617372 29777 solver.cpp:290] Iteration 151200 (6.26369 iter/s, 15.965s/100 iter), loss = 1.30952
I0630 10:12:12.617465 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 10:12:12.617476 29777 sgd_solver.cpp:106] Iteration 151200, lr = 0.005275
I0630 10:12:28.726431 29777 solver.cpp:290] Iteration 151300 (6.20789 iter/s, 16.1085s/100 iter), loss = 1.28571
I0630 10:12:28.726456 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 10:12:28.726465 29777 sgd_solver.cpp:106] Iteration 151300, lr = 0.00527187
I0630 10:12:44.650995 29777 solver.cpp:290] Iteration 151400 (6.27979 iter/s, 15.9241s/100 iter), loss = 1.58333
I0630 10:12:44.651067 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 10:12:44.651075 29777 sgd_solver.cpp:106] Iteration 151400, lr = 0.00526875
I0630 10:13:00.632557 29777 solver.cpp:290] Iteration 151500 (6.25741 iter/s, 15.9811s/100 iter), loss = 0.988095
I0630 10:13:00.632583 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 10:13:00.632592 29777 sgd_solver.cpp:106] Iteration 151500, lr = 0.00526563
I0630 10:13:16.553815 29777 solver.cpp:290] Iteration 151600 (6.28109 iter/s, 15.9208s/100 iter), loss = 1.08333
I0630 10:13:16.553860 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 10:13:16.553869 29777 sgd_solver.cpp:106] Iteration 151600, lr = 0.0052625
I0630 10:13:32.643013 29777 solver.cpp:290] Iteration 151700 (6.21554 iter/s, 16.0887s/100 iter), loss = 0.904762
I0630 10:13:32.643036 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 10:13:32.643043 29777 sgd_solver.cpp:106] Iteration 151700, lr = 0.00525938
I0630 10:13:48.589253 29777 solver.cpp:290] Iteration 151800 (6.27125 iter/s, 15.9458s/100 iter), loss = 1.69048
I0630 10:13:48.589334 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 10:13:48.589344 29777 sgd_solver.cpp:106] Iteration 151800, lr = 0.00525625
I0630 10:14:04.560622 29777 solver.cpp:290] Iteration 151900 (6.26141 iter/s, 15.9709s/100 iter), loss = 1.46429
I0630 10:14:04.560647 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 10:14:04.560654 29777 sgd_solver.cpp:106] Iteration 151900, lr = 0.00525313
I0630 10:14:20.432607 29777 solver.cpp:354] Sparsity after update:
I0630 10:14:20.433876 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:14:20.433883 29777 net.cpp:1851] conv1a_param_0(0.38) 
I0630 10:14:20.433890 29777 net.cpp:1851] conv1b_param_0(0.721) 
I0630 10:14:20.433892 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:14:20.433895 29777 net.cpp:1851] res2a_branch2a_param_0(0.76) 
I0630 10:14:20.433897 29777 net.cpp:1851] res2a_branch2b_param_0(0.76) 
I0630 10:14:20.433899 29777 net.cpp:1851] res3a_branch2a_param_0(0.76) 
I0630 10:14:20.433900 29777 net.cpp:1851] res3a_branch2b_param_0(0.76) 
I0630 10:14:20.433902 29777 net.cpp:1851] res4a_branch2a_param_0(0.76) 
I0630 10:14:20.433904 29777 net.cpp:1851] res4a_branch2b_param_0(0.76) 
I0630 10:14:20.433907 29777 net.cpp:1851] res5a_branch2a_param_0(0.76) 
I0630 10:14:20.433908 29777 net.cpp:1851] res5a_branch2b_param_0(0.76) 
I0630 10:14:20.433910 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.78863e+06/2.86678e+06) 0.624
I0630 10:14:20.434026 29777 solver.cpp:471] Iteration 152000, Testing net (#0)
I0630 10:14:32.651437 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 10:15:08.872351 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55262
I0630 10:15:08.872438 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.788501
I0630 10:15:08.872447 29777 solver.cpp:544]     Test net output #2: loss = 1.6017 (* 1 = 1.6017 loss)
I0630 10:15:09.042445 29777 solver.cpp:290] Iteration 152000 (1.55087 iter/s, 64.4801s/100 iter), loss = 1.39286
I0630 10:15:09.042471 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 10:15:09.042480 29777 sgd_solver.cpp:106] Iteration 152000, lr = 0.00525
I0630 10:15:09.043460 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.77
I0630 10:15:09.644316 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 10:15:25.634402 29777 solver.cpp:290] Iteration 152100 (6.02719 iter/s, 16.5915s/100 iter), loss = 1.2381
I0630 10:15:25.634429 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 10:15:25.634438 29777 sgd_solver.cpp:106] Iteration 152100, lr = 0.00524688
I0630 10:15:41.618468 29777 solver.cpp:290] Iteration 152200 (6.25641 iter/s, 15.9836s/100 iter), loss = 1.07143
I0630 10:15:41.618580 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 10:15:41.618594 29777 sgd_solver.cpp:106] Iteration 152200, lr = 0.00524375
I0630 10:15:57.638623 29777 solver.cpp:290] Iteration 152300 (6.24235 iter/s, 16.0196s/100 iter), loss = 1.16667
I0630 10:15:57.638646 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 10:15:57.638654 29777 sgd_solver.cpp:106] Iteration 152300, lr = 0.00524063
I0630 10:16:13.600672 29777 solver.cpp:290] Iteration 152400 (6.26504 iter/s, 15.9616s/100 iter), loss = 1.40476
I0630 10:16:13.600765 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 10:16:13.600777 29777 sgd_solver.cpp:106] Iteration 152400, lr = 0.0052375
I0630 10:16:29.626588 29777 solver.cpp:290] Iteration 152500 (6.2401 iter/s, 16.0254s/100 iter), loss = 1.4881
I0630 10:16:29.626615 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 10:16:29.626624 29777 sgd_solver.cpp:106] Iteration 152500, lr = 0.00523437
I0630 10:16:45.826342 29777 solver.cpp:290] Iteration 152600 (6.17311 iter/s, 16.1993s/100 iter), loss = 1.08333
I0630 10:16:45.826380 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 10:16:45.826387 29777 sgd_solver.cpp:106] Iteration 152600, lr = 0.00523125
I0630 10:17:01.870028 29777 solver.cpp:290] Iteration 152700 (6.23317 iter/s, 16.0432s/100 iter), loss = 1.5119
I0630 10:17:01.870090 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 10:17:01.870110 29777 sgd_solver.cpp:106] Iteration 152700, lr = 0.00522812
I0630 10:17:17.873430 29777 solver.cpp:290] Iteration 152800 (6.24886 iter/s, 16.0029s/100 iter), loss = 1.65476
I0630 10:17:17.873553 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 10:17:17.873566 29777 sgd_solver.cpp:106] Iteration 152800, lr = 0.005225
I0630 10:17:33.914526 29777 solver.cpp:290] Iteration 152900 (6.2342 iter/s, 16.0405s/100 iter), loss = 1.38095
I0630 10:17:33.914551 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 10:17:33.914561 29777 sgd_solver.cpp:106] Iteration 152900, lr = 0.00522187
I0630 10:17:49.830705 29777 solver.cpp:354] Sparsity after update:
I0630 10:17:49.851135 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:17:49.851150 29777 net.cpp:1851] conv1a_param_0(0.385) 
I0630 10:17:49.851162 29777 net.cpp:1851] conv1b_param_0(0.723) 
I0630 10:17:49.851166 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:17:49.851177 29777 net.cpp:1851] res2a_branch2a_param_0(0.77) 
I0630 10:17:49.851181 29777 net.cpp:1851] res2a_branch2b_param_0(0.77) 
I0630 10:17:49.851186 29777 net.cpp:1851] res3a_branch2a_param_0(0.77) 
I0630 10:17:49.851191 29777 net.cpp:1851] res3a_branch2b_param_0(0.77) 
I0630 10:17:49.851197 29777 net.cpp:1851] res4a_branch2a_param_0(0.77) 
I0630 10:17:49.851202 29777 net.cpp:1851] res4a_branch2b_param_0(0.77) 
I0630 10:17:49.851207 29777 net.cpp:1851] res5a_branch2a_param_0(0.77) 
I0630 10:17:49.851212 29777 net.cpp:1851] res5a_branch2b_param_0(0.77) 
I0630 10:17:49.851214 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.81214e+06/2.86678e+06) 0.632
I0630 10:17:50.012084 29777 solver.cpp:290] Iteration 153000 (6.2123 iter/s, 16.0971s/100 iter), loss = 1.16667
I0630 10:17:50.012106 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 10:17:50.012114 29777 sgd_solver.cpp:106] Iteration 153000, lr = 0.00521875
I0630 10:18:06.062115 29777 solver.cpp:290] Iteration 153100 (6.2307 iter/s, 16.0496s/100 iter), loss = 1.40476
I0630 10:18:06.062139 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 10:18:06.062147 29777 sgd_solver.cpp:106] Iteration 153100, lr = 0.00521562
I0630 10:18:22.053290 29777 solver.cpp:290] Iteration 153200 (6.25363 iter/s, 15.9907s/100 iter), loss = 1.35714
I0630 10:18:22.064286 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 10:18:22.064327 29777 sgd_solver.cpp:106] Iteration 153200, lr = 0.0052125
I0630 10:18:38.090785 29777 solver.cpp:290] Iteration 153300 (6.23983 iter/s, 16.0261s/100 iter), loss = 1.10714
I0630 10:18:38.090809 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 10:18:38.090817 29777 sgd_solver.cpp:106] Iteration 153300, lr = 0.00520937
I0630 10:18:54.189734 29777 solver.cpp:290] Iteration 153400 (6.21177 iter/s, 16.0985s/100 iter), loss = 1.14286
I0630 10:18:54.189813 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 10:18:54.189824 29777 sgd_solver.cpp:106] Iteration 153400, lr = 0.00520625
I0630 10:19:10.104131 29777 solver.cpp:290] Iteration 153500 (6.28382 iter/s, 15.9139s/100 iter), loss = 1.34524
I0630 10:19:10.104154 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 10:19:10.104161 29777 sgd_solver.cpp:106] Iteration 153500, lr = 0.00520312
I0630 10:19:26.141939 29777 solver.cpp:290] Iteration 153600 (6.23545 iter/s, 16.0373s/100 iter), loss = 1.25
I0630 10:19:26.142045 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 10:19:26.142055 29777 sgd_solver.cpp:106] Iteration 153600, lr = 0.0052
I0630 10:19:42.178843 29777 solver.cpp:290] Iteration 153700 (6.23583 iter/s, 16.0364s/100 iter), loss = 1.28571
I0630 10:19:42.178866 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 10:19:42.178872 29777 sgd_solver.cpp:106] Iteration 153700, lr = 0.00519688
I0630 10:19:58.211697 29777 solver.cpp:290] Iteration 153800 (6.23737 iter/s, 16.0324s/100 iter), loss = 1.25
I0630 10:19:58.211781 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 10:19:58.211788 29777 sgd_solver.cpp:106] Iteration 153800, lr = 0.00519375
I0630 10:20:14.147678 29777 solver.cpp:290] Iteration 153900 (6.27531 iter/s, 15.9355s/100 iter), loss = 0.928571
I0630 10:20:14.147702 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 10:20:14.147708 29777 sgd_solver.cpp:106] Iteration 153900, lr = 0.00519062
I0630 10:20:30.021332 29777 solver.cpp:354] Sparsity after update:
I0630 10:20:30.022819 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:20:30.022827 29777 net.cpp:1851] conv1a_param_0(0.385) 
I0630 10:20:30.022841 29777 net.cpp:1851] conv1b_param_0(0.723) 
I0630 10:20:30.022845 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:20:30.022847 29777 net.cpp:1851] res2a_branch2a_param_0(0.77) 
I0630 10:20:30.022851 29777 net.cpp:1851] res2a_branch2b_param_0(0.77) 
I0630 10:20:30.022855 29777 net.cpp:1851] res3a_branch2a_param_0(0.77) 
I0630 10:20:30.022856 29777 net.cpp:1851] res3a_branch2b_param_0(0.77) 
I0630 10:20:30.022858 29777 net.cpp:1851] res4a_branch2a_param_0(0.77) 
I0630 10:20:30.022862 29777 net.cpp:1851] res4a_branch2b_param_0(0.77) 
I0630 10:20:30.022864 29777 net.cpp:1851] res5a_branch2a_param_0(0.77) 
I0630 10:20:30.022866 29777 net.cpp:1851] res5a_branch2b_param_0(0.77) 
I0630 10:20:30.022868 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.81214e+06/2.86678e+06) 0.632
I0630 10:20:30.022958 29777 solver.cpp:471] Iteration 154000, Testing net (#0)
I0630 10:20:42.628768 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 10:21:19.032301 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5499
I0630 10:21:19.032407 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.787041
I0630 10:21:19.032418 29777 solver.cpp:544]     Test net output #2: loss = 1.6187 (* 1 = 1.6187 loss)
I0630 10:21:19.215479 29777 solver.cpp:290] Iteration 154000 (1.5369 iter/s, 65.066s/100 iter), loss = 1.2619
I0630 10:21:19.215502 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 10:21:19.215508 29777 sgd_solver.cpp:106] Iteration 154000, lr = 0.0051875
I0630 10:21:19.216224 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.78
I0630 10:21:19.826805 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 10:21:35.772130 29777 solver.cpp:290] Iteration 154100 (6.04004 iter/s, 16.5562s/100 iter), loss = 1.30952
I0630 10:21:35.772155 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 10:21:35.772163 29777 sgd_solver.cpp:106] Iteration 154100, lr = 0.00518437
I0630 10:21:52.038074 29777 solver.cpp:290] Iteration 154200 (6.14799 iter/s, 16.2655s/100 iter), loss = 1.17857
I0630 10:21:52.038166 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 10:21:52.038177 29777 sgd_solver.cpp:106] Iteration 154200, lr = 0.00518125
I0630 10:22:08.104444 29777 solver.cpp:290] Iteration 154300 (6.22439 iter/s, 16.0658s/100 iter), loss = 1.0119
I0630 10:22:08.104470 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 10:22:08.104478 29777 sgd_solver.cpp:106] Iteration 154300, lr = 0.00517812
I0630 10:22:24.150056 29777 solver.cpp:290] Iteration 154400 (6.23241 iter/s, 16.0451s/100 iter), loss = 1.25
I0630 10:22:24.150161 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 10:22:24.150171 29777 sgd_solver.cpp:106] Iteration 154400, lr = 0.005175
I0630 10:22:40.215025 29777 solver.cpp:290] Iteration 154500 (6.22493 iter/s, 16.0644s/100 iter), loss = 1.15476
I0630 10:22:40.215050 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 10:22:40.215056 29777 sgd_solver.cpp:106] Iteration 154500, lr = 0.00517187
I0630 10:22:56.214934 29777 solver.cpp:290] Iteration 154600 (6.25022 iter/s, 15.9994s/100 iter), loss = 1.32143
I0630 10:22:56.215049 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 10:22:56.215059 29777 sgd_solver.cpp:106] Iteration 154600, lr = 0.00516875
I0630 10:23:12.244137 29777 solver.cpp:290] Iteration 154700 (6.23883 iter/s, 16.0287s/100 iter), loss = 0.964286
I0630 10:23:12.244163 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 10:23:12.244173 29777 sgd_solver.cpp:106] Iteration 154700, lr = 0.00516562
I0630 10:23:28.307354 29777 solver.cpp:290] Iteration 154800 (6.22558 iter/s, 16.0628s/100 iter), loss = 1.04762
I0630 10:23:28.307464 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 10:23:28.307478 29777 sgd_solver.cpp:106] Iteration 154800, lr = 0.0051625
I0630 10:23:44.366080 29777 solver.cpp:290] Iteration 154900 (6.22736 iter/s, 16.0582s/100 iter), loss = 1.4881
I0630 10:23:44.366102 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 10:23:44.366108 29777 sgd_solver.cpp:106] Iteration 154900, lr = 0.00515937
I0630 10:24:00.164317 29777 solver.cpp:354] Sparsity after update:
I0630 10:24:00.184420 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:24:00.184439 29777 net.cpp:1851] conv1a_param_0(0.39) 
I0630 10:24:00.184449 29777 net.cpp:1851] conv1b_param_0(0.723) 
I0630 10:24:00.184453 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:24:00.184465 29777 net.cpp:1851] res2a_branch2a_param_0(0.78) 
I0630 10:24:00.184474 29777 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 10:24:00.184485 29777 net.cpp:1851] res3a_branch2a_param_0(0.78) 
I0630 10:24:00.184491 29777 net.cpp:1851] res3a_branch2b_param_0(0.78) 
I0630 10:24:00.184495 29777 net.cpp:1851] res4a_branch2a_param_0(0.78) 
I0630 10:24:00.184504 29777 net.cpp:1851] res4a_branch2b_param_0(0.78) 
I0630 10:24:00.184512 29777 net.cpp:1851] res5a_branch2a_param_0(0.78) 
I0630 10:24:00.184518 29777 net.cpp:1851] res5a_branch2b_param_0(0.78) 
I0630 10:24:00.184522 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.83566e+06/2.86678e+06) 0.64
I0630 10:24:00.343108 29777 solver.cpp:290] Iteration 155000 (6.25917 iter/s, 15.9766s/100 iter), loss = 1.28571
I0630 10:24:00.343132 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 10:24:00.343142 29777 sgd_solver.cpp:106] Iteration 155000, lr = 0.00515625
I0630 10:24:16.354801 29777 solver.cpp:290] Iteration 155100 (6.24562 iter/s, 16.0112s/100 iter), loss = 1.54762
I0630 10:24:16.354823 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 10:24:16.354830 29777 sgd_solver.cpp:106] Iteration 155100, lr = 0.00515312
I0630 10:24:32.517768 29777 solver.cpp:290] Iteration 155200 (6.18716 iter/s, 16.1625s/100 iter), loss = 1.16667
I0630 10:24:32.517825 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 10:24:32.517835 29777 sgd_solver.cpp:106] Iteration 155200, lr = 0.00515
I0630 10:24:48.619030 29777 solver.cpp:290] Iteration 155300 (6.21089 iter/s, 16.1008s/100 iter), loss = 1.33333
I0630 10:24:48.619053 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 10:24:48.619060 29777 sgd_solver.cpp:106] Iteration 155300, lr = 0.00514688
I0630 10:25:04.749992 29777 solver.cpp:290] Iteration 155400 (6.19944 iter/s, 16.1305s/100 iter), loss = 0.928571
I0630 10:25:04.750068 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 10:25:04.750078 29777 sgd_solver.cpp:106] Iteration 155400, lr = 0.00514375
I0630 10:25:20.808483 29777 solver.cpp:290] Iteration 155500 (6.22744 iter/s, 16.058s/100 iter), loss = 1.14286
I0630 10:25:20.808535 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 10:25:20.808545 29777 sgd_solver.cpp:106] Iteration 155500, lr = 0.00514062
I0630 10:25:36.899405 29777 solver.cpp:290] Iteration 155600 (6.21487 iter/s, 16.0904s/100 iter), loss = 1.25
I0630 10:25:36.899488 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 10:25:36.899497 29777 sgd_solver.cpp:106] Iteration 155600, lr = 0.0051375
I0630 10:25:52.920020 29777 solver.cpp:290] Iteration 155700 (6.24216 iter/s, 16.0201s/100 iter), loss = 1.22619
I0630 10:25:52.920044 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 10:25:52.920051 29777 sgd_solver.cpp:106] Iteration 155700, lr = 0.00513437
I0630 10:26:08.922111 29777 solver.cpp:290] Iteration 155800 (6.24936 iter/s, 16.0016s/100 iter), loss = 1.28571
I0630 10:26:08.922183 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 10:26:08.922191 29777 sgd_solver.cpp:106] Iteration 155800, lr = 0.00513125
I0630 10:26:24.946848 29777 solver.cpp:290] Iteration 155900 (6.24055 iter/s, 16.0242s/100 iter), loss = 1.11905
I0630 10:26:24.946873 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 10:26:24.946882 29777 sgd_solver.cpp:106] Iteration 155900, lr = 0.00512812
I0630 10:26:40.794594 29777 solver.cpp:354] Sparsity after update:
I0630 10:26:40.795979 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:26:40.795994 29777 net.cpp:1851] conv1a_param_0(0.39) 
I0630 10:26:40.796001 29777 net.cpp:1851] conv1b_param_0(0.723) 
I0630 10:26:40.796005 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:26:40.796006 29777 net.cpp:1851] res2a_branch2a_param_0(0.78) 
I0630 10:26:40.796008 29777 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 10:26:40.796010 29777 net.cpp:1851] res3a_branch2a_param_0(0.78) 
I0630 10:26:40.796012 29777 net.cpp:1851] res3a_branch2b_param_0(0.78) 
I0630 10:26:40.796015 29777 net.cpp:1851] res4a_branch2a_param_0(0.78) 
I0630 10:26:40.796016 29777 net.cpp:1851] res4a_branch2b_param_0(0.78) 
I0630 10:26:40.796018 29777 net.cpp:1851] res5a_branch2a_param_0(0.78) 
I0630 10:26:40.796020 29777 net.cpp:1851] res5a_branch2b_param_0(0.78) 
I0630 10:26:40.796022 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.83566e+06/2.86678e+06) 0.64
I0630 10:26:40.796134 29777 solver.cpp:471] Iteration 156000, Testing net (#0)
I0630 10:26:53.922366 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 10:27:30.209213 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54306
I0630 10:27:30.209314 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.783901
I0630 10:27:30.209324 29777 solver.cpp:544]     Test net output #2: loss = 1.638 (* 1 = 1.638 loss)
I0630 10:27:30.389008 29777 solver.cpp:290] Iteration 156000 (1.52811 iter/s, 65.4404s/100 iter), loss = 1.03571
I0630 10:27:30.389031 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 10:27:30.389039 29777 sgd_solver.cpp:106] Iteration 156000, lr = 0.005125
I0630 10:27:30.389770 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.79
I0630 10:27:31.015298 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 10:27:46.905560 29777 solver.cpp:290] Iteration 156100 (6.05471 iter/s, 16.5161s/100 iter), loss = 1.36905
I0630 10:27:46.905589 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 10:27:46.905598 29777 sgd_solver.cpp:106] Iteration 156100, lr = 0.00512187
I0630 10:28:02.912519 29777 solver.cpp:290] Iteration 156200 (6.24747 iter/s, 16.0065s/100 iter), loss = 1.10714
I0630 10:28:02.912626 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 10:28:02.912652 29777 sgd_solver.cpp:106] Iteration 156200, lr = 0.00511875
I0630 10:28:18.975527 29777 solver.cpp:290] Iteration 156300 (6.22572 iter/s, 16.0624s/100 iter), loss = 1.55952
I0630 10:28:18.975746 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 10:28:18.975841 29777 sgd_solver.cpp:106] Iteration 156300, lr = 0.00511562
I0630 10:28:34.936666 29777 solver.cpp:290] Iteration 156400 (6.26547 iter/s, 15.9605s/100 iter), loss = 1.60714
I0630 10:28:34.936753 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 10:28:34.936761 29777 sgd_solver.cpp:106] Iteration 156400, lr = 0.0051125
I0630 10:28:50.927079 29777 solver.cpp:290] Iteration 156500 (6.25395 iter/s, 15.9899s/100 iter), loss = 1.71429
I0630 10:28:50.927103 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 10:28:50.927109 29777 sgd_solver.cpp:106] Iteration 156500, lr = 0.00510937
I0630 10:29:07.020639 29777 solver.cpp:290] Iteration 156600 (6.21385 iter/s, 16.0931s/100 iter), loss = 1.5
I0630 10:29:07.020743 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 10:29:07.020752 29777 sgd_solver.cpp:106] Iteration 156600, lr = 0.00510625
I0630 10:29:23.017369 29777 solver.cpp:290] Iteration 156700 (6.25149 iter/s, 15.9962s/100 iter), loss = 1.61905
I0630 10:29:23.017396 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 10:29:23.017418 29777 sgd_solver.cpp:106] Iteration 156700, lr = 0.00510312
I0630 10:29:39.074558 29777 solver.cpp:290] Iteration 156800 (6.22792 iter/s, 16.0567s/100 iter), loss = 1.33333
I0630 10:29:39.074659 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 10:29:39.074669 29777 sgd_solver.cpp:106] Iteration 156800, lr = 0.0051
I0630 10:29:55.216855 29777 solver.cpp:290] Iteration 156900 (6.19511 iter/s, 16.1418s/100 iter), loss = 1.45238
I0630 10:29:55.216878 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 10:29:55.216884 29777 sgd_solver.cpp:106] Iteration 156900, lr = 0.00509688
I0630 10:30:11.120019 29777 solver.cpp:354] Sparsity after update:
I0630 10:30:11.140383 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:30:11.140398 29777 net.cpp:1851] conv1a_param_0(0.395) 
I0630 10:30:11.140408 29777 net.cpp:1851] conv1b_param_0(0.724) 
I0630 10:30:11.140413 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:30:11.140422 29777 net.cpp:1851] res2a_branch2a_param_0(0.79) 
I0630 10:30:11.140427 29777 net.cpp:1851] res2a_branch2b_param_0(0.79) 
I0630 10:30:11.140430 29777 net.cpp:1851] res3a_branch2a_param_0(0.79) 
I0630 10:30:11.140437 29777 net.cpp:1851] res3a_branch2b_param_0(0.79) 
I0630 10:30:11.140442 29777 net.cpp:1851] res4a_branch2a_param_0(0.79) 
I0630 10:30:11.140447 29777 net.cpp:1851] res4a_branch2b_param_0(0.79) 
I0630 10:30:11.140452 29777 net.cpp:1851] res5a_branch2a_param_0(0.79) 
I0630 10:30:11.140456 29777 net.cpp:1851] res5a_branch2b_param_0(0.79) 
I0630 10:30:11.140460 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.85917e+06/2.86678e+06) 0.649
I0630 10:30:11.298322 29777 solver.cpp:290] Iteration 157000 (6.21852 iter/s, 16.081s/100 iter), loss = 1.27381
I0630 10:30:11.298346 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 10:30:11.298354 29777 sgd_solver.cpp:106] Iteration 157000, lr = 0.00509375
I0630 10:30:27.313750 29777 solver.cpp:290] Iteration 157100 (6.24416 iter/s, 16.015s/100 iter), loss = 0.952381
I0630 10:30:27.313776 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 10:30:27.313786 29777 sgd_solver.cpp:106] Iteration 157100, lr = 0.00509063
I0630 10:30:43.304684 29777 solver.cpp:290] Iteration 157200 (6.25372 iter/s, 15.9905s/100 iter), loss = 1.5119
I0630 10:30:43.304762 29777 solver.cpp:309]     Train net output #0: loss = 2 (* 1 = 2 loss)
I0630 10:30:43.304770 29777 sgd_solver.cpp:106] Iteration 157200, lr = 0.0050875
I0630 10:30:59.428436 29777 solver.cpp:290] Iteration 157300 (6.20223 iter/s, 16.1232s/100 iter), loss = 1.28571
I0630 10:30:59.428462 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 10:30:59.428468 29777 sgd_solver.cpp:106] Iteration 157300, lr = 0.00508437
I0630 10:31:15.418639 29777 solver.cpp:290] Iteration 157400 (6.25401 iter/s, 15.9897s/100 iter), loss = 1.30952
I0630 10:31:15.418742 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 10:31:15.418753 29777 sgd_solver.cpp:106] Iteration 157400, lr = 0.00508125
I0630 10:31:31.443712 29777 solver.cpp:290] Iteration 157500 (6.24043 iter/s, 16.0245s/100 iter), loss = 1.30952
I0630 10:31:31.443732 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 10:31:31.443739 29777 sgd_solver.cpp:106] Iteration 157500, lr = 0.00507812
I0630 10:31:47.539150 29777 solver.cpp:290] Iteration 157600 (6.21312 iter/s, 16.095s/100 iter), loss = 1.39286
I0630 10:31:47.539257 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 10:31:47.539266 29777 sgd_solver.cpp:106] Iteration 157600, lr = 0.005075
I0630 10:32:03.511286 29777 solver.cpp:290] Iteration 157700 (6.26111 iter/s, 15.9716s/100 iter), loss = 1.35714
I0630 10:32:03.511308 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 10:32:03.511315 29777 sgd_solver.cpp:106] Iteration 157700, lr = 0.00507187
I0630 10:32:19.440987 29777 solver.cpp:290] Iteration 157800 (6.27776 iter/s, 15.9292s/100 iter), loss = 1.57143
I0630 10:32:19.441057 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 10:32:19.441066 29777 sgd_solver.cpp:106] Iteration 157800, lr = 0.00506875
I0630 10:32:35.501801 29777 solver.cpp:290] Iteration 157900 (6.22653 iter/s, 16.0603s/100 iter), loss = 1.30952
I0630 10:32:35.501827 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 10:32:35.501834 29777 sgd_solver.cpp:106] Iteration 157900, lr = 0.00506562
I0630 10:32:51.520088 29777 solver.cpp:354] Sparsity after update:
I0630 10:32:51.522500 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:32:51.522514 29777 net.cpp:1851] conv1a_param_0(0.395) 
I0630 10:32:51.522537 29777 net.cpp:1851] conv1b_param_0(0.724) 
I0630 10:32:51.522548 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:32:51.522558 29777 net.cpp:1851] res2a_branch2a_param_0(0.79) 
I0630 10:32:51.522568 29777 net.cpp:1851] res2a_branch2b_param_0(0.79) 
I0630 10:32:51.522578 29777 net.cpp:1851] res3a_branch2a_param_0(0.79) 
I0630 10:32:51.522588 29777 net.cpp:1851] res3a_branch2b_param_0(0.79) 
I0630 10:32:51.522596 29777 net.cpp:1851] res4a_branch2a_param_0(0.79) 
I0630 10:32:51.522605 29777 net.cpp:1851] res4a_branch2b_param_0(0.79) 
I0630 10:32:51.522614 29777 net.cpp:1851] res5a_branch2a_param_0(0.79) 
I0630 10:32:51.522622 29777 net.cpp:1851] res5a_branch2b_param_0(0.79) 
I0630 10:32:51.522631 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.85917e+06/2.86678e+06) 0.649
I0630 10:32:51.522847 29777 solver.cpp:471] Iteration 158000, Testing net (#0)
I0630 10:33:09.507359 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 10:33:58.029201 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54332
I0630 10:33:58.029325 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.783981
I0630 10:33:58.029345 29777 solver.cpp:544]     Test net output #2: loss = 1.64274 (* 1 = 1.64274 loss)
I0630 10:33:58.244833 29777 solver.cpp:290] Iteration 158000 (1.20859 iter/s, 82.7408s/100 iter), loss = 1.70238
I0630 10:33:58.244860 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 10:33:58.244869 29777 sgd_solver.cpp:106] Iteration 158000, lr = 0.0050625
I0630 10:33:58.245867 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.8
I0630 10:33:58.892460 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 10:34:15.097195 29777 solver.cpp:290] Iteration 158100 (5.93406 iter/s, 16.8519s/100 iter), loss = 1.32143
I0630 10:34:15.097219 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 10:34:15.097228 29777 sgd_solver.cpp:106] Iteration 158100, lr = 0.00505937
I0630 10:34:31.317203 29777 solver.cpp:290] Iteration 158200 (6.1654 iter/s, 16.2195s/100 iter), loss = 1.08333
I0630 10:34:31.317320 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 10:34:31.317330 29777 sgd_solver.cpp:106] Iteration 158200, lr = 0.00505625
I0630 10:34:47.407727 29777 solver.cpp:290] Iteration 158300 (6.21505 iter/s, 16.09s/100 iter), loss = 1.28571
I0630 10:34:47.407773 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 10:34:47.407788 29777 sgd_solver.cpp:106] Iteration 158300, lr = 0.00505312
I0630 10:35:03.549024 29777 solver.cpp:290] Iteration 158400 (6.19548 iter/s, 16.1408s/100 iter), loss = 1.38095
I0630 10:35:03.549123 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 10:35:03.549151 29777 sgd_solver.cpp:106] Iteration 158400, lr = 0.00505
I0630 10:35:19.730125 29777 solver.cpp:290] Iteration 158500 (6.18025 iter/s, 16.1806s/100 iter), loss = 1.72619
I0630 10:35:19.730149 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 10:35:19.730155 29777 sgd_solver.cpp:106] Iteration 158500, lr = 0.00504687
I0630 10:35:36.127668 29777 solver.cpp:290] Iteration 158600 (6.09865 iter/s, 16.3971s/100 iter), loss = 1.15476
I0630 10:35:36.127737 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 10:35:36.127743 29777 sgd_solver.cpp:106] Iteration 158600, lr = 0.00504375
I0630 10:35:52.185997 29777 solver.cpp:290] Iteration 158700 (6.22749 iter/s, 16.0578s/100 iter), loss = 1.03571
I0630 10:35:52.186019 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 10:35:52.186027 29777 sgd_solver.cpp:106] Iteration 158700, lr = 0.00504063
I0630 10:36:08.375013 29777 solver.cpp:290] Iteration 158800 (6.17721 iter/s, 16.1886s/100 iter), loss = 1.42857
I0630 10:36:08.375083 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 10:36:08.375090 29777 sgd_solver.cpp:106] Iteration 158800, lr = 0.0050375
I0630 10:36:24.687595 29777 solver.cpp:290] Iteration 158900 (6.13043 iter/s, 16.3121s/100 iter), loss = 1.40476
I0630 10:36:24.687621 29777 solver.cpp:309]     Train net output #0: loss = 1.88095 (* 1 = 1.88095 loss)
I0630 10:36:24.687630 29777 sgd_solver.cpp:106] Iteration 158900, lr = 0.00503438
I0630 10:36:40.746436 29777 solver.cpp:354] Sparsity after update:
I0630 10:36:40.770929 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:36:40.770956 29777 net.cpp:1851] conv1a_param_0(0.4) 
I0630 10:36:40.770977 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 10:36:40.770992 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:36:40.771001 29777 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 10:36:40.771009 29777 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 10:36:40.771018 29777 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 10:36:40.771026 29777 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 10:36:40.771034 29777 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 10:36:40.771041 29777 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 10:36:40.771050 29777 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 10:36:40.771057 29777 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 10:36:40.771064 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88269e+06/2.86678e+06) 0.657
I0630 10:36:40.938520 29777 solver.cpp:290] Iteration 159000 (6.15367 iter/s, 16.2505s/100 iter), loss = 1.2381
I0630 10:36:40.938544 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 10:36:40.938562 29777 sgd_solver.cpp:106] Iteration 159000, lr = 0.00503125
I0630 10:36:57.121991 29777 solver.cpp:290] Iteration 159100 (6.17932 iter/s, 16.183s/100 iter), loss = 1.15476
I0630 10:36:57.122016 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 10:36:57.122023 29777 sgd_solver.cpp:106] Iteration 159100, lr = 0.00502812
I0630 10:37:13.141692 29777 solver.cpp:290] Iteration 159200 (6.24249 iter/s, 16.0192s/100 iter), loss = 1.5119
I0630 10:37:13.141798 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 10:37:13.141808 29777 sgd_solver.cpp:106] Iteration 159200, lr = 0.005025
I0630 10:37:29.114895 29777 solver.cpp:290] Iteration 159300 (6.2607 iter/s, 15.9727s/100 iter), loss = 1.16667
I0630 10:37:29.114917 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 10:37:29.114924 29777 sgd_solver.cpp:106] Iteration 159300, lr = 0.00502187
I0630 10:37:45.432468 29777 solver.cpp:290] Iteration 159400 (6.12854 iter/s, 16.3171s/100 iter), loss = 1.16667
I0630 10:37:45.432585 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 10:37:45.432595 29777 sgd_solver.cpp:106] Iteration 159400, lr = 0.00501875
I0630 10:38:01.580978 29777 solver.cpp:290] Iteration 159500 (6.19273 iter/s, 16.148s/100 iter), loss = 1.2619
I0630 10:38:01.581019 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 10:38:01.581028 29777 sgd_solver.cpp:106] Iteration 159500, lr = 0.00501562
I0630 10:38:17.790410 29777 solver.cpp:290] Iteration 159600 (6.16943 iter/s, 16.209s/100 iter), loss = 1.30952
I0630 10:38:17.790484 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 10:38:17.790493 29777 sgd_solver.cpp:106] Iteration 159600, lr = 0.0050125
I0630 10:38:34.048342 29777 solver.cpp:290] Iteration 159700 (6.15104 iter/s, 16.2574s/100 iter), loss = 1.19048
I0630 10:38:34.048364 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 10:38:34.048372 29777 sgd_solver.cpp:106] Iteration 159700, lr = 0.00500937
I0630 10:38:50.155164 29777 solver.cpp:290] Iteration 159800 (6.20873 iter/s, 16.1064s/100 iter), loss = 1.29762
I0630 10:38:50.155237 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 10:38:50.155248 29777 sgd_solver.cpp:106] Iteration 159800, lr = 0.00500625
I0630 10:39:06.267529 29777 solver.cpp:290] Iteration 159900 (6.20661 iter/s, 16.1119s/100 iter), loss = 1.45238
I0630 10:39:06.267552 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 10:39:06.267560 29777 sgd_solver.cpp:106] Iteration 159900, lr = 0.00500312
I0630 10:39:22.274595 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_160000.caffemodel
I0630 10:39:22.307953 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_160000.solverstate
I0630 10:39:22.316609 29777 solver.cpp:354] Sparsity after update:
I0630 10:39:22.317569 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:39:22.317577 29777 net.cpp:1851] conv1a_param_0(0.4) 
I0630 10:39:22.317589 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 10:39:22.317595 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:39:22.317598 29777 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 10:39:22.317602 29777 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 10:39:22.317605 29777 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 10:39:22.317610 29777 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 10:39:22.317615 29777 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 10:39:22.317620 29777 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 10:39:22.317622 29777 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 10:39:22.317627 29777 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 10:39:22.317631 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88269e+06/2.86678e+06) 0.657
I0630 10:39:22.317730 29777 solver.cpp:471] Iteration 160000, Testing net (#0)
I0630 10:39:36.737339 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 10:40:12.551654 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.536839
I0630 10:40:12.551767 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.777721
I0630 10:40:12.551776 29777 solver.cpp:544]     Test net output #2: loss = 1.65744 (* 1 = 1.65744 loss)
I0630 10:40:12.731901 29777 solver.cpp:290] Iteration 160000 (1.50461 iter/s, 66.4626s/100 iter), loss = 1.5
I0630 10:40:12.731927 29777 solver.cpp:309]     Train net output #0: loss = 2.14286 (* 1 = 2.14286 loss)
I0630 10:40:12.731936 29777 sgd_solver.cpp:106] Iteration 160000, lr = 0.005
I0630 10:40:12.732722 29777 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.81
I0630 10:40:13.376741 29777 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 10:40:29.373914 29777 solver.cpp:290] Iteration 160100 (6.00906 iter/s, 16.6415s/100 iter), loss = 1.45238
I0630 10:40:29.373937 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 10:40:29.373944 29777 sgd_solver.cpp:106] Iteration 160100, lr = 0.00499687
I0630 10:40:45.333282 29777 solver.cpp:290] Iteration 160200 (6.26609 iter/s, 15.9589s/100 iter), loss = 1.03571
I0630 10:40:45.333413 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 10:40:45.333423 29777 sgd_solver.cpp:106] Iteration 160200, lr = 0.00499375
I0630 10:41:01.380261 29777 solver.cpp:290] Iteration 160300 (6.23192 iter/s, 16.0464s/100 iter), loss = 1.14286
I0630 10:41:01.380285 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 10:41:01.380291 29777 sgd_solver.cpp:106] Iteration 160300, lr = 0.00499062
I0630 10:41:17.363517 29777 solver.cpp:290] Iteration 160400 (6.25673 iter/s, 15.9828s/100 iter), loss = 1.88095
I0630 10:41:17.363602 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 10:41:17.363610 29777 sgd_solver.cpp:106] Iteration 160400, lr = 0.0049875
I0630 10:41:33.357611 29777 solver.cpp:290] Iteration 160500 (6.25251 iter/s, 15.9936s/100 iter), loss = 1.7381
I0630 10:41:33.357637 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 10:41:33.357645 29777 sgd_solver.cpp:106] Iteration 160500, lr = 0.00498438
I0630 10:41:49.285326 29777 solver.cpp:290] Iteration 160600 (6.27855 iter/s, 15.9273s/100 iter), loss = 1.46429
I0630 10:41:49.285404 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 10:41:49.285415 29777 sgd_solver.cpp:106] Iteration 160600, lr = 0.00498125
I0630 10:42:05.315732 29777 solver.cpp:290] Iteration 160700 (6.23835 iter/s, 16.0299s/100 iter), loss = 1.47619
I0630 10:42:05.315755 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 10:42:05.315762 29777 sgd_solver.cpp:106] Iteration 160700, lr = 0.00497812
I0630 10:42:21.258358 29777 solver.cpp:290] Iteration 160800 (6.27267 iter/s, 15.9422s/100 iter), loss = 0.892857
I0630 10:42:21.258468 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 10:42:21.258483 29777 sgd_solver.cpp:106] Iteration 160800, lr = 0.004975
I0630 10:42:37.309499 29777 solver.cpp:290] Iteration 160900 (6.2303 iter/s, 16.0506s/100 iter), loss = 1.46429
I0630 10:42:37.309521 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 10:42:37.309528 29777 sgd_solver.cpp:106] Iteration 160900, lr = 0.00497187
I0630 10:42:53.104693 29777 solver.cpp:354] Sparsity after update:
I0630 10:42:53.125166 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:42:53.125211 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 10:42:53.125228 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 10:42:53.125237 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:42:53.125247 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 10:42:53.125252 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 10:42:53.125255 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 10:42:53.125258 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 10:42:53.125262 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 10:42:53.125267 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 10:42:53.125270 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 10:42:53.125274 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 10:42:53.125279 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 10:42:53.282583 29777 solver.cpp:290] Iteration 161000 (6.26071 iter/s, 15.9726s/100 iter), loss = 1.41667
I0630 10:42:53.282609 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 10:42:53.282618 29777 sgd_solver.cpp:106] Iteration 161000, lr = 0.00496875
I0630 10:43:09.302855 29777 solver.cpp:290] Iteration 161100 (6.24227 iter/s, 16.0198s/100 iter), loss = 1.42857
I0630 10:43:09.302881 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 10:43:09.302891 29777 sgd_solver.cpp:106] Iteration 161100, lr = 0.00496562
I0630 10:43:25.300101 29777 solver.cpp:290] Iteration 161200 (6.25126 iter/s, 15.9968s/100 iter), loss = 1.32143
I0630 10:43:25.300216 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 10:43:25.300231 29777 sgd_solver.cpp:106] Iteration 161200, lr = 0.0049625
I0630 10:43:41.270568 29777 solver.cpp:290] Iteration 161300 (6.26177 iter/s, 15.9699s/100 iter), loss = 0.880952
I0630 10:43:41.270591 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 10:43:41.270597 29777 sgd_solver.cpp:106] Iteration 161300, lr = 0.00495938
I0630 10:43:57.291651 29777 solver.cpp:290] Iteration 161400 (6.24195 iter/s, 16.0206s/100 iter), loss = 1.69048
I0630 10:43:57.291741 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 10:43:57.291751 29777 sgd_solver.cpp:106] Iteration 161400, lr = 0.00495625
I0630 10:44:13.325085 29777 solver.cpp:290] Iteration 161500 (6.23717 iter/s, 16.0329s/100 iter), loss = 1.09524
I0630 10:44:13.325108 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 10:44:13.325114 29777 sgd_solver.cpp:106] Iteration 161500, lr = 0.00495313
I0630 10:44:29.503792 29777 solver.cpp:290] Iteration 161600 (6.18114 iter/s, 16.1782s/100 iter), loss = 1.02381
I0630 10:44:29.503883 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 10:44:29.503891 29777 sgd_solver.cpp:106] Iteration 161600, lr = 0.00495
I0630 10:44:45.542359 29777 solver.cpp:290] Iteration 161700 (6.23518 iter/s, 16.038s/100 iter), loss = 1.57143
I0630 10:44:45.542385 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 10:44:45.542394 29777 sgd_solver.cpp:106] Iteration 161700, lr = 0.00494687
I0630 10:45:01.555516 29777 solver.cpp:290] Iteration 161800 (6.24505 iter/s, 16.0127s/100 iter), loss = 1.44048
I0630 10:45:01.555609 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 10:45:01.555620 29777 sgd_solver.cpp:106] Iteration 161800, lr = 0.00494375
I0630 10:45:17.692878 29777 solver.cpp:290] Iteration 161900 (6.197 iter/s, 16.1368s/100 iter), loss = 1.4881
I0630 10:45:17.692900 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 10:45:17.692906 29777 sgd_solver.cpp:106] Iteration 161900, lr = 0.00494062
I0630 10:45:33.553373 29777 solver.cpp:354] Sparsity after update:
I0630 10:45:33.554841 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:45:33.554848 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 10:45:33.554855 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 10:45:33.554857 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:45:33.554859 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 10:45:33.554862 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 10:45:33.554863 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 10:45:33.554865 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 10:45:33.554867 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 10:45:33.554869 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 10:45:33.554872 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 10:45:33.554873 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 10:45:33.554875 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 10:45:33.554961 29777 solver.cpp:471] Iteration 162000, Testing net (#0)
I0630 10:45:46.781530 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 10:46:21.914722 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5369
I0630 10:46:21.914858 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.778701
I0630 10:46:21.914868 29777 solver.cpp:544]     Test net output #2: loss = 1.66298 (* 1 = 1.66298 loss)
I0630 10:46:22.090438 29777 solver.cpp:290] Iteration 162000 (1.5529 iter/s, 64.3958s/100 iter), loss = 1.45238
I0630 10:46:22.090461 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 10:46:22.090467 29777 sgd_solver.cpp:106] Iteration 162000, lr = 0.0049375
I0630 10:46:38.216171 29777 solver.cpp:290] Iteration 162100 (6.20145 iter/s, 16.1253s/100 iter), loss = 1
I0630 10:46:38.216204 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 10:46:38.216213 29777 sgd_solver.cpp:106] Iteration 162100, lr = 0.00493438
I0630 10:46:54.325886 29777 solver.cpp:290] Iteration 162200 (6.20762 iter/s, 16.1092s/100 iter), loss = 1.28571
I0630 10:46:54.325968 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 10:46:54.325979 29777 sgd_solver.cpp:106] Iteration 162200, lr = 0.00493125
I0630 10:47:10.376444 29777 solver.cpp:290] Iteration 162300 (6.23052 iter/s, 16.05s/100 iter), loss = 1.13095
I0630 10:47:10.376468 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 10:47:10.376476 29777 sgd_solver.cpp:106] Iteration 162300, lr = 0.00492813
I0630 10:47:26.612850 29777 solver.cpp:290] Iteration 162400 (6.15918 iter/s, 16.2359s/100 iter), loss = 1.10714
I0630 10:47:26.612944 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 10:47:26.612957 29777 sgd_solver.cpp:106] Iteration 162400, lr = 0.004925
I0630 10:47:42.622613 29777 solver.cpp:290] Iteration 162500 (6.2464 iter/s, 16.0092s/100 iter), loss = 1.09524
I0630 10:47:42.622674 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 10:47:42.622694 29777 sgd_solver.cpp:106] Iteration 162500, lr = 0.00492187
I0630 10:47:58.731909 29777 solver.cpp:290] Iteration 162600 (6.20779 iter/s, 16.1088s/100 iter), loss = 1.25
I0630 10:47:58.732015 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 10:47:58.732023 29777 sgd_solver.cpp:106] Iteration 162600, lr = 0.00491875
I0630 10:48:14.843065 29777 solver.cpp:290] Iteration 162700 (6.20709 iter/s, 16.1106s/100 iter), loss = 1.29762
I0630 10:48:14.843089 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 10:48:14.843098 29777 sgd_solver.cpp:106] Iteration 162700, lr = 0.00491562
I0630 10:48:30.831351 29777 solver.cpp:290] Iteration 162800 (6.25476 iter/s, 15.9878s/100 iter), loss = 1.61905
I0630 10:48:30.831388 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 10:48:30.831395 29777 sgd_solver.cpp:106] Iteration 162800, lr = 0.0049125
I0630 10:48:46.932401 29777 solver.cpp:290] Iteration 162900 (6.21096 iter/s, 16.1006s/100 iter), loss = 1.39286
I0630 10:48:46.932428 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 10:48:46.932437 29777 sgd_solver.cpp:106] Iteration 162900, lr = 0.00490937
I0630 10:49:02.734346 29777 solver.cpp:354] Sparsity after update:
I0630 10:49:02.754874 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:49:02.754889 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 10:49:02.754900 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 10:49:02.754904 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:49:02.754909 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 10:49:02.754912 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 10:49:02.754915 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 10:49:02.754920 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 10:49:02.754925 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 10:49:02.754928 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 10:49:02.754931 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 10:49:02.754935 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 10:49:02.754937 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 10:49:02.914165 29777 solver.cpp:290] Iteration 163000 (6.25731 iter/s, 15.9813s/100 iter), loss = 0.940476
I0630 10:49:02.914189 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 10:49:02.914198 29777 sgd_solver.cpp:106] Iteration 163000, lr = 0.00490625
I0630 10:49:18.902956 29777 solver.cpp:290] Iteration 163100 (6.25456 iter/s, 15.9883s/100 iter), loss = 1.72619
I0630 10:49:18.902981 29777 solver.cpp:309]     Train net output #0: loss = 1.88095 (* 1 = 1.88095 loss)
I0630 10:49:18.902990 29777 sgd_solver.cpp:106] Iteration 163100, lr = 0.00490313
I0630 10:49:34.922690 29777 solver.cpp:290] Iteration 163200 (6.24248 iter/s, 16.0193s/100 iter), loss = 1.03571
I0630 10:49:34.922797 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 10:49:34.922808 29777 sgd_solver.cpp:106] Iteration 163200, lr = 0.0049
I0630 10:49:50.859683 29777 solver.cpp:290] Iteration 163300 (6.27492 iter/s, 15.9365s/100 iter), loss = 1.2619
I0630 10:49:50.859709 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 10:49:50.859721 29777 sgd_solver.cpp:106] Iteration 163300, lr = 0.00489688
I0630 10:50:06.876621 29777 solver.cpp:290] Iteration 163400 (6.24357 iter/s, 16.0165s/100 iter), loss = 1.38095
I0630 10:50:06.876691 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 10:50:06.876699 29777 sgd_solver.cpp:106] Iteration 163400, lr = 0.00489375
I0630 10:50:23.344045 29777 solver.cpp:290] Iteration 163500 (6.07279 iter/s, 16.4669s/100 iter), loss = 1.19048
I0630 10:50:23.344072 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 10:50:23.344081 29777 sgd_solver.cpp:106] Iteration 163500, lr = 0.00489062
I0630 10:50:39.696533 29777 solver.cpp:290] Iteration 163600 (6.11545 iter/s, 16.352s/100 iter), loss = 1.63095
I0630 10:50:39.696638 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 10:50:39.696648 29777 sgd_solver.cpp:106] Iteration 163600, lr = 0.0048875
I0630 10:50:55.916693 29777 solver.cpp:290] Iteration 163700 (6.16537 iter/s, 16.2196s/100 iter), loss = 1.29762
I0630 10:50:55.916723 29777 solver.cpp:309]     Train net output #0: loss = 1.88095 (* 1 = 1.88095 loss)
I0630 10:50:55.916733 29777 sgd_solver.cpp:106] Iteration 163700, lr = 0.00488437
I0630 10:51:12.017515 29777 solver.cpp:290] Iteration 163800 (6.21104 iter/s, 16.1004s/100 iter), loss = 1.25
I0630 10:51:12.017622 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 10:51:12.017632 29777 sgd_solver.cpp:106] Iteration 163800, lr = 0.00488125
I0630 10:51:28.127074 29777 solver.cpp:290] Iteration 163900 (6.20771 iter/s, 16.109s/100 iter), loss = 1.35714
I0630 10:51:28.127100 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 10:51:28.127107 29777 sgd_solver.cpp:106] Iteration 163900, lr = 0.00487813
I0630 10:51:44.174103 29777 solver.cpp:354] Sparsity after update:
I0630 10:51:44.175534 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:51:44.175540 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 10:51:44.175549 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 10:51:44.175550 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:51:44.175554 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 10:51:44.175556 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 10:51:44.175559 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 10:51:44.175561 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 10:51:44.175564 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 10:51:44.175565 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 10:51:44.175567 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 10:51:44.175570 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 10:51:44.175571 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 10:51:44.175657 29777 solver.cpp:471] Iteration 164000, Testing net (#0)
I0630 10:52:03.818078 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 10:52:51.837007 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.53778
I0630 10:52:51.837134 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.780341
I0630 10:52:51.837144 29777 solver.cpp:544]     Test net output #2: loss = 1.66192 (* 1 = 1.66192 loss)
I0630 10:52:52.018946 29777 solver.cpp:290] Iteration 164000 (1.19204 iter/s, 83.8896s/100 iter), loss = 1.14286
I0630 10:52:52.018972 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 10:52:52.018982 29777 sgd_solver.cpp:106] Iteration 164000, lr = 0.004875
I0630 10:53:08.329767 29777 solver.cpp:290] Iteration 164100 (6.13108 iter/s, 16.3103s/100 iter), loss = 1.27381
I0630 10:53:08.329790 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 10:53:08.329797 29777 sgd_solver.cpp:106] Iteration 164100, lr = 0.00487188
I0630 10:53:24.475828 29777 solver.cpp:290] Iteration 164200 (6.19364 iter/s, 16.1456s/100 iter), loss = 0.988095
I0630 10:53:24.475936 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 10:53:24.475946 29777 sgd_solver.cpp:106] Iteration 164200, lr = 0.00486875
I0630 10:53:40.551874 29777 solver.cpp:290] Iteration 164300 (6.22065 iter/s, 16.0755s/100 iter), loss = 1.16667
I0630 10:53:40.551899 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 10:53:40.551908 29777 sgd_solver.cpp:106] Iteration 164300, lr = 0.00486562
I0630 10:53:56.679663 29777 solver.cpp:290] Iteration 164400 (6.20066 iter/s, 16.1273s/100 iter), loss = 1.47619
I0630 10:53:56.679752 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 10:53:56.679762 29777 sgd_solver.cpp:106] Iteration 164400, lr = 0.0048625
I0630 10:54:12.867516 29777 solver.cpp:290] Iteration 164500 (6.17767 iter/s, 16.1873s/100 iter), loss = 1.27381
I0630 10:54:12.867544 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 10:54:12.867553 29777 sgd_solver.cpp:106] Iteration 164500, lr = 0.00485937
I0630 10:54:29.077955 29777 solver.cpp:290] Iteration 164600 (6.16904 iter/s, 16.21s/100 iter), loss = 1.34524
I0630 10:54:29.078066 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 10:54:29.078083 29777 sgd_solver.cpp:106] Iteration 164600, lr = 0.00485625
I0630 10:54:45.287645 29777 solver.cpp:290] Iteration 164700 (6.16936 iter/s, 16.2091s/100 iter), loss = 1.2381
I0630 10:54:45.287670 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 10:54:45.287678 29777 sgd_solver.cpp:106] Iteration 164700, lr = 0.00485313
I0630 10:55:01.264015 29777 solver.cpp:290] Iteration 164800 (6.25942 iter/s, 15.9759s/100 iter), loss = 1.40476
I0630 10:55:01.264063 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 10:55:01.264070 29777 sgd_solver.cpp:106] Iteration 164800, lr = 0.00485
I0630 10:55:17.312141 29777 solver.cpp:290] Iteration 164900 (6.23145 iter/s, 16.0476s/100 iter), loss = 1.16667
I0630 10:55:17.312168 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 10:55:17.312181 29777 sgd_solver.cpp:106] Iteration 164900, lr = 0.00484688
I0630 10:55:33.252025 29777 solver.cpp:354] Sparsity after update:
I0630 10:55:33.272685 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:55:33.272701 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 10:55:33.272711 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 10:55:33.272716 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:55:33.272718 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 10:55:33.272722 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 10:55:33.272724 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 10:55:33.272728 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 10:55:33.272732 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 10:55:33.272734 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 10:55:33.272743 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 10:55:33.272749 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 10:55:33.272755 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 10:55:33.429479 29777 solver.cpp:290] Iteration 165000 (6.20468 iter/s, 16.1169s/100 iter), loss = 1.54762
I0630 10:55:33.429502 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 10:55:33.429508 29777 sgd_solver.cpp:106] Iteration 165000, lr = 0.00484375
I0630 10:55:49.515846 29777 solver.cpp:290] Iteration 165100 (6.21662 iter/s, 16.0859s/100 iter), loss = 1.72619
I0630 10:55:49.515869 29777 solver.cpp:309]     Train net output #0: loss = 1.97619 (* 1 = 1.97619 loss)
I0630 10:55:49.515878 29777 sgd_solver.cpp:106] Iteration 165100, lr = 0.00484062
I0630 10:56:05.589046 29777 solver.cpp:290] Iteration 165200 (6.22172 iter/s, 16.0727s/100 iter), loss = 1.88095
I0630 10:56:05.589162 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 10:56:05.589172 29777 sgd_solver.cpp:106] Iteration 165200, lr = 0.0048375
I0630 10:56:21.643632 29777 solver.cpp:290] Iteration 165300 (6.22896 iter/s, 16.054s/100 iter), loss = 1.13095
I0630 10:56:21.643657 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 10:56:21.643667 29777 sgd_solver.cpp:106] Iteration 165300, lr = 0.00483437
I0630 10:56:37.657533 29777 solver.cpp:290] Iteration 165400 (6.24475 iter/s, 16.0134s/100 iter), loss = 1.45238
I0630 10:56:37.657606 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 10:56:37.657614 29777 sgd_solver.cpp:106] Iteration 165400, lr = 0.00483125
I0630 10:56:53.574590 29777 solver.cpp:290] Iteration 165500 (6.28277 iter/s, 15.9165s/100 iter), loss = 1.2619
I0630 10:56:53.574615 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 10:56:53.574622 29777 sgd_solver.cpp:106] Iteration 165500, lr = 0.00482813
I0630 10:57:09.522552 29777 solver.cpp:290] Iteration 165600 (6.27058 iter/s, 15.9475s/100 iter), loss = 1.27381
I0630 10:57:09.522634 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 10:57:09.522641 29777 sgd_solver.cpp:106] Iteration 165600, lr = 0.004825
I0630 10:57:25.583644 29777 solver.cpp:290] Iteration 165700 (6.22643 iter/s, 16.0606s/100 iter), loss = 1.25
I0630 10:57:25.583667 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 10:57:25.583674 29777 sgd_solver.cpp:106] Iteration 165700, lr = 0.00482188
I0630 10:57:41.569368 29777 solver.cpp:290] Iteration 165800 (6.25576 iter/s, 15.9853s/100 iter), loss = 1.54762
I0630 10:57:41.569461 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 10:57:41.569471 29777 sgd_solver.cpp:106] Iteration 165800, lr = 0.00481875
I0630 10:57:57.566126 29777 solver.cpp:290] Iteration 165900 (6.25147 iter/s, 15.9962s/100 iter), loss = 1.2619
I0630 10:57:57.566153 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 10:57:57.566161 29777 sgd_solver.cpp:106] Iteration 165900, lr = 0.00481563
I0630 10:58:13.379883 29777 solver.cpp:354] Sparsity after update:
I0630 10:58:13.381319 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 10:58:13.381326 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 10:58:13.381335 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 10:58:13.381336 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 10:58:13.381338 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 10:58:13.381340 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 10:58:13.381342 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 10:58:13.381345 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 10:58:13.381346 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 10:58:13.381348 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 10:58:13.381350 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 10:58:13.381351 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 10:58:13.381353 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 10:58:13.381440 29777 solver.cpp:471] Iteration 166000, Testing net (#0)
I0630 10:58:27.196679 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 10:59:02.996574 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54266
I0630 10:59:02.996695 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.781461
I0630 10:59:02.996706 29777 solver.cpp:544]     Test net output #2: loss = 1.63798 (* 1 = 1.63798 loss)
I0630 10:59:03.167307 29777 solver.cpp:290] Iteration 166000 (1.5244 iter/s, 65.5994s/100 iter), loss = 1.2381
I0630 10:59:03.167335 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 10:59:03.167345 29777 sgd_solver.cpp:106] Iteration 166000, lr = 0.0048125
I0630 10:59:19.101447 29777 solver.cpp:290] Iteration 166100 (6.27602 iter/s, 15.9337s/100 iter), loss = 2.07143
I0630 10:59:19.101472 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 10:59:19.101481 29777 sgd_solver.cpp:106] Iteration 166100, lr = 0.00480937
I0630 10:59:35.097266 29777 solver.cpp:290] Iteration 166200 (6.25181 iter/s, 15.9954s/100 iter), loss = 1.33333
I0630 10:59:35.097347 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 10:59:35.097358 29777 sgd_solver.cpp:106] Iteration 166200, lr = 0.00480625
I0630 10:59:50.988941 29777 solver.cpp:290] Iteration 166300 (6.29281 iter/s, 15.8912s/100 iter), loss = 1.45238
I0630 10:59:50.988963 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 10:59:50.988970 29777 sgd_solver.cpp:106] Iteration 166300, lr = 0.00480313
I0630 11:00:06.918673 29777 solver.cpp:290] Iteration 166400 (6.27775 iter/s, 15.9293s/100 iter), loss = 1.5119
I0630 11:00:06.919507 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 11:00:06.919520 29777 sgd_solver.cpp:106] Iteration 166400, lr = 0.0048
I0630 11:00:22.875365 29777 solver.cpp:290] Iteration 166500 (6.26746 iter/s, 15.9554s/100 iter), loss = 1.40476
I0630 11:00:22.875392 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 11:00:22.875401 29777 sgd_solver.cpp:106] Iteration 166500, lr = 0.00479688
I0630 11:00:38.892931 29777 solver.cpp:290] Iteration 166600 (6.24333 iter/s, 16.0171s/100 iter), loss = 1.2619
I0630 11:00:38.893004 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 11:00:38.893014 29777 sgd_solver.cpp:106] Iteration 166600, lr = 0.00479375
I0630 11:00:54.992002 29777 solver.cpp:290] Iteration 166700 (6.21174 iter/s, 16.0986s/100 iter), loss = 1.45238
I0630 11:00:54.992027 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 11:00:54.992036 29777 sgd_solver.cpp:106] Iteration 166700, lr = 0.00479063
I0630 11:01:10.939218 29777 solver.cpp:290] Iteration 166800 (6.27087 iter/s, 15.9468s/100 iter), loss = 1.54762
I0630 11:01:10.939769 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 11:01:10.939782 29777 sgd_solver.cpp:106] Iteration 166800, lr = 0.0047875
I0630 11:01:27.130517 29777 solver.cpp:290] Iteration 166900 (6.17653 iter/s, 16.1903s/100 iter), loss = 1.5119
I0630 11:01:27.130543 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 11:01:27.130551 29777 sgd_solver.cpp:106] Iteration 166900, lr = 0.00478437
I0630 11:01:43.086347 29777 solver.cpp:354] Sparsity after update:
I0630 11:01:43.106576 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:01:43.106592 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:01:43.106604 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:01:43.106608 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:01:43.106613 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:01:43.106617 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:01:43.106621 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:01:43.106626 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:01:43.106629 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:01:43.106632 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:01:43.106637 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:01:43.106640 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:01:43.106644 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:01:43.263917 29777 solver.cpp:290] Iteration 167000 (6.1985 iter/s, 16.1329s/100 iter), loss = 1.34524
I0630 11:01:43.263938 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 11:01:43.263944 29777 sgd_solver.cpp:106] Iteration 167000, lr = 0.00478125
I0630 11:01:59.411566 29777 solver.cpp:290] Iteration 167100 (6.19303 iter/s, 16.1472s/100 iter), loss = 1.39286
I0630 11:01:59.411613 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 11:01:59.411631 29777 sgd_solver.cpp:106] Iteration 167100, lr = 0.00477813
I0630 11:02:15.495504 29777 solver.cpp:290] Iteration 167200 (6.21757 iter/s, 16.0834s/100 iter), loss = 1.71429
I0630 11:02:15.495621 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 11:02:15.495649 29777 sgd_solver.cpp:106] Iteration 167200, lr = 0.004775
I0630 11:02:31.748473 29777 solver.cpp:290] Iteration 167300 (6.15293 iter/s, 16.2524s/100 iter), loss = 1.29762
I0630 11:02:31.748498 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 11:02:31.748505 29777 sgd_solver.cpp:106] Iteration 167300, lr = 0.00477188
I0630 11:02:47.940596 29777 solver.cpp:290] Iteration 167400 (6.17602 iter/s, 16.1917s/100 iter), loss = 1.32143
I0630 11:02:47.940685 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 11:02:47.940695 29777 sgd_solver.cpp:106] Iteration 167400, lr = 0.00476875
I0630 11:03:04.055630 29777 solver.cpp:290] Iteration 167500 (6.20559 iter/s, 16.1145s/100 iter), loss = 1.63095
I0630 11:03:04.055655 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 11:03:04.055662 29777 sgd_solver.cpp:106] Iteration 167500, lr = 0.00476563
I0630 11:03:20.085208 29777 solver.cpp:290] Iteration 167600 (6.23865 iter/s, 16.0291s/100 iter), loss = 1.53571
I0630 11:03:20.085299 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 11:03:20.085310 29777 sgd_solver.cpp:106] Iteration 167600, lr = 0.0047625
I0630 11:03:36.343724 29777 solver.cpp:290] Iteration 167700 (6.15083 iter/s, 16.258s/100 iter), loss = 1.16667
I0630 11:03:36.343761 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 11:03:36.343772 29777 sgd_solver.cpp:106] Iteration 167700, lr = 0.00475937
I0630 11:03:52.569447 29777 solver.cpp:290] Iteration 167800 (6.16323 iter/s, 16.2252s/100 iter), loss = 1.13095
I0630 11:03:52.569555 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 11:03:52.569566 29777 sgd_solver.cpp:106] Iteration 167800, lr = 0.00475625
I0630 11:04:09.387289 29777 solver.cpp:290] Iteration 167900 (5.94627 iter/s, 16.8173s/100 iter), loss = 1.25
I0630 11:04:09.387315 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 11:04:09.387323 29777 sgd_solver.cpp:106] Iteration 167900, lr = 0.00475312
I0630 11:04:25.319737 29777 solver.cpp:354] Sparsity after update:
I0630 11:04:25.321244 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:04:25.321256 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:04:25.321265 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:04:25.321270 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:04:25.321274 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:04:25.321277 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:04:25.321281 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:04:25.321285 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:04:25.321290 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:04:25.321292 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:04:25.321296 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:04:25.321300 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:04:25.321303 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:04:25.321401 29777 solver.cpp:471] Iteration 168000, Testing net (#0)
I0630 11:04:44.111032 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 11:05:30.587833 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54074
I0630 11:05:30.587941 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.780301
I0630 11:05:30.587951 29777 solver.cpp:544]     Test net output #2: loss = 1.6488 (* 1 = 1.6488 loss)
I0630 11:05:30.760341 29777 solver.cpp:290] Iteration 168000 (1.22894 iter/s, 81.3708s/100 iter), loss = 1.45238
I0630 11:05:30.760365 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 11:05:30.760371 29777 sgd_solver.cpp:106] Iteration 168000, lr = 0.00475
I0630 11:05:46.885792 29777 solver.cpp:290] Iteration 168100 (6.20156 iter/s, 16.125s/100 iter), loss = 1.59524
I0630 11:05:46.885818 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 11:05:46.885828 29777 sgd_solver.cpp:106] Iteration 168100, lr = 0.00474688
I0630 11:06:03.001209 29777 solver.cpp:290] Iteration 168200 (6.20542 iter/s, 16.115s/100 iter), loss = 1.41667
I0630 11:06:03.001314 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 11:06:03.001324 29777 sgd_solver.cpp:106] Iteration 168200, lr = 0.00474375
I0630 11:06:19.035120 29777 solver.cpp:290] Iteration 168300 (6.23699 iter/s, 16.0334s/100 iter), loss = 1.28571
I0630 11:06:19.035145 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 11:06:19.035159 29777 sgd_solver.cpp:106] Iteration 168300, lr = 0.00474062
I0630 11:06:35.190510 29777 solver.cpp:290] Iteration 168400 (6.19006 iter/s, 16.1549s/100 iter), loss = 1.02381
I0630 11:06:35.190608 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 11:06:35.190615 29777 sgd_solver.cpp:106] Iteration 168400, lr = 0.0047375
I0630 11:06:51.409592 29777 solver.cpp:290] Iteration 168500 (6.16578 iter/s, 16.2185s/100 iter), loss = 1.45238
I0630 11:06:51.409616 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 11:06:51.409624 29777 sgd_solver.cpp:106] Iteration 168500, lr = 0.00473437
I0630 11:07:07.618175 29777 solver.cpp:290] Iteration 168600 (6.16975 iter/s, 16.2081s/100 iter), loss = 1.14286
I0630 11:07:07.618273 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 11:07:07.618283 29777 sgd_solver.cpp:106] Iteration 168600, lr = 0.00473125
I0630 11:07:23.965733 29777 solver.cpp:290] Iteration 168700 (6.11732 iter/s, 16.347s/100 iter), loss = 1.32143
I0630 11:07:23.965761 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 11:07:23.966884 29777 sgd_solver.cpp:106] Iteration 168700, lr = 0.00472812
I0630 11:07:40.204485 29777 solver.cpp:290] Iteration 168800 (6.15829 iter/s, 16.2383s/100 iter), loss = 1.36905
I0630 11:07:40.204596 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 11:07:40.204605 29777 sgd_solver.cpp:106] Iteration 168800, lr = 0.004725
I0630 11:07:56.568235 29777 solver.cpp:290] Iteration 168900 (6.11128 iter/s, 16.3632s/100 iter), loss = 1.69048
I0630 11:07:56.568289 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 11:07:56.568310 29777 sgd_solver.cpp:106] Iteration 168900, lr = 0.00472188
I0630 11:08:12.585578 29777 solver.cpp:354] Sparsity after update:
I0630 11:08:12.605942 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:08:12.605968 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:08:12.605983 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:08:12.605990 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:08:12.605996 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:08:12.606001 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:08:12.606007 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:08:12.606014 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:08:12.606019 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:08:12.606026 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:08:12.606030 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:08:12.606036 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:08:12.606042 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:08:12.764094 29777 solver.cpp:290] Iteration 169000 (6.1746 iter/s, 16.1954s/100 iter), loss = 1.19048
I0630 11:08:12.764122 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 11:08:12.764130 29777 sgd_solver.cpp:106] Iteration 169000, lr = 0.00471875
I0630 11:08:29.005120 29777 solver.cpp:290] Iteration 169100 (6.15742 iter/s, 16.2406s/100 iter), loss = 1.22619
I0630 11:08:29.005151 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 11:08:29.005159 29777 sgd_solver.cpp:106] Iteration 169100, lr = 0.00471562
I0630 11:08:45.117099 29777 solver.cpp:290] Iteration 169200 (6.20674 iter/s, 16.1115s/100 iter), loss = 1.17857
I0630 11:08:45.117189 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 11:08:45.117200 29777 sgd_solver.cpp:106] Iteration 169200, lr = 0.0047125
I0630 11:09:01.095952 29777 solver.cpp:290] Iteration 169300 (6.25848 iter/s, 15.9783s/100 iter), loss = 1.54762
I0630 11:09:01.095971 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 11:09:01.095979 29777 sgd_solver.cpp:106] Iteration 169300, lr = 0.00470937
I0630 11:09:17.213794 29777 solver.cpp:290] Iteration 169400 (6.20448 iter/s, 16.1174s/100 iter), loss = 1.5119
I0630 11:09:17.213891 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 11:09:17.213898 29777 sgd_solver.cpp:106] Iteration 169400, lr = 0.00470625
I0630 11:09:33.392452 29777 solver.cpp:290] Iteration 169500 (6.18119 iter/s, 16.1781s/100 iter), loss = 1.22619
I0630 11:09:33.392478 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 11:09:33.392488 29777 sgd_solver.cpp:106] Iteration 169500, lr = 0.00470312
I0630 11:09:49.570617 29777 solver.cpp:290] Iteration 169600 (6.18135 iter/s, 16.1777s/100 iter), loss = 1.36905
I0630 11:09:49.570719 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 11:09:49.570730 29777 sgd_solver.cpp:106] Iteration 169600, lr = 0.0047
I0630 11:10:05.558758 29777 solver.cpp:290] Iteration 169700 (6.25484 iter/s, 15.9876s/100 iter), loss = 1.39286
I0630 11:10:05.558781 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 11:10:05.558789 29777 sgd_solver.cpp:106] Iteration 169700, lr = 0.00469687
I0630 11:10:21.636925 29777 solver.cpp:290] Iteration 169800 (6.21979 iter/s, 16.0777s/100 iter), loss = 1.27381
I0630 11:10:21.637043 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 11:10:21.637053 29777 sgd_solver.cpp:106] Iteration 169800, lr = 0.00469375
I0630 11:10:37.853076 29777 solver.cpp:290] Iteration 169900 (6.16691 iter/s, 16.2156s/100 iter), loss = 1.2381
I0630 11:10:37.853117 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 11:10:37.853132 29777 sgd_solver.cpp:106] Iteration 169900, lr = 0.00469062
I0630 11:10:53.772404 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_170000.caffemodel
I0630 11:10:53.791554 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_170000.solverstate
I0630 11:10:53.800323 29777 solver.cpp:354] Sparsity after update:
I0630 11:10:53.801352 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:10:53.801362 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:10:53.801368 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:10:53.801372 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:10:53.801373 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:10:53.801375 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:10:53.801378 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:10:53.801379 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:10:53.801381 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:10:53.801383 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:10:53.801384 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:10:53.801386 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:10:53.801388 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:10:53.801484 29777 solver.cpp:471] Iteration 170000, Testing net (#0)
I0630 11:11:10.222422 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 11:11:54.307220 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.53908
I0630 11:11:54.307309 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.780621
I0630 11:11:54.307324 29777 solver.cpp:544]     Test net output #2: loss = 1.65196 (* 1 = 1.65196 loss)
I0630 11:11:54.535126 29777 solver.cpp:290] Iteration 170000 (1.30412 iter/s, 76.6799s/100 iter), loss = 1.39286
I0630 11:11:54.535182 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 11:11:54.535200 29777 sgd_solver.cpp:106] Iteration 170000, lr = 0.0046875
I0630 11:12:10.582475 29777 solver.cpp:290] Iteration 170100 (6.23175 iter/s, 16.0469s/100 iter), loss = 1.34524
I0630 11:12:10.582497 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 11:12:10.582504 29777 sgd_solver.cpp:106] Iteration 170100, lr = 0.00468437
I0630 11:12:26.660698 29777 solver.cpp:290] Iteration 170200 (6.21977 iter/s, 16.0778s/100 iter), loss = 1.40476
I0630 11:12:26.660804 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 11:12:26.660814 29777 sgd_solver.cpp:106] Iteration 170200, lr = 0.00468125
I0630 11:12:42.701295 29777 solver.cpp:290] Iteration 170300 (6.23439 iter/s, 16.0401s/100 iter), loss = 1.05952
I0630 11:12:42.701321 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 11:12:42.701336 29777 sgd_solver.cpp:106] Iteration 170300, lr = 0.00467812
I0630 11:12:58.782986 29777 solver.cpp:290] Iteration 170400 (6.21843 iter/s, 16.0812s/100 iter), loss = 1.55952
I0630 11:12:58.783074 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 11:12:58.783084 29777 sgd_solver.cpp:106] Iteration 170400, lr = 0.004675
I0630 11:13:14.842877 29777 solver.cpp:290] Iteration 170500 (6.22689 iter/s, 16.0594s/100 iter), loss = 1.5
I0630 11:13:14.842902 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 11:13:14.842910 29777 sgd_solver.cpp:106] Iteration 170500, lr = 0.00467187
I0630 11:13:31.025617 29777 solver.cpp:290] Iteration 170600 (6.1796 iter/s, 16.1823s/100 iter), loss = 1.53571
I0630 11:13:31.025676 29777 solver.cpp:309]     Train net output #0: loss = 1.78571 (* 1 = 1.78571 loss)
I0630 11:13:31.025688 29777 sgd_solver.cpp:106] Iteration 170600, lr = 0.00466875
I0630 11:13:47.012461 29777 solver.cpp:290] Iteration 170700 (6.25533 iter/s, 15.9864s/100 iter), loss = 1.4881
I0630 11:13:47.012495 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 11:13:47.012503 29777 sgd_solver.cpp:106] Iteration 170700, lr = 0.00466562
I0630 11:14:02.977267 29777 solver.cpp:290] Iteration 170800 (6.26396 iter/s, 15.9643s/100 iter), loss = 1.16667
I0630 11:14:02.977349 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 11:14:02.977357 29777 sgd_solver.cpp:106] Iteration 170800, lr = 0.0046625
I0630 11:14:19.043007 29777 solver.cpp:290] Iteration 170900 (6.22463 iter/s, 16.0652s/100 iter), loss = 1.05952
I0630 11:14:19.043033 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 11:14:19.043042 29777 sgd_solver.cpp:106] Iteration 170900, lr = 0.00465937
I0630 11:14:34.905124 29777 solver.cpp:354] Sparsity after update:
I0630 11:14:34.925470 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:14:34.925487 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:14:34.925498 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:14:34.925501 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:14:34.925504 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:14:34.925518 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:14:34.925523 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:14:34.925529 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:14:34.925534 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:14:34.925539 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:14:34.925544 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:14:34.925549 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:14:34.925551 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:14:35.083562 29777 solver.cpp:290] Iteration 171000 (6.23438 iter/s, 16.0401s/100 iter), loss = 1.14286
I0630 11:14:35.083587 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 11:14:35.083596 29777 sgd_solver.cpp:106] Iteration 171000, lr = 0.00465625
I0630 11:14:51.192147 29777 solver.cpp:290] Iteration 171100 (6.20805 iter/s, 16.1081s/100 iter), loss = 1.13095
I0630 11:14:51.192168 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 11:14:51.192175 29777 sgd_solver.cpp:106] Iteration 171100, lr = 0.00465312
I0630 11:15:07.217164 29777 solver.cpp:290] Iteration 171200 (6.24042 iter/s, 16.0246s/100 iter), loss = 1.21429
I0630 11:15:07.217260 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 11:15:07.217286 29777 sgd_solver.cpp:106] Iteration 171200, lr = 0.00465
I0630 11:15:23.173982 29777 solver.cpp:290] Iteration 171300 (6.26712 iter/s, 15.9563s/100 iter), loss = 1.2619
I0630 11:15:23.174036 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 11:15:23.174059 29777 sgd_solver.cpp:106] Iteration 171300, lr = 0.00464687
I0630 11:15:39.548784 29777 solver.cpp:290] Iteration 171400 (6.10713 iter/s, 16.3743s/100 iter), loss = 1.70238
I0630 11:15:39.548854 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 11:15:39.548861 29777 sgd_solver.cpp:106] Iteration 171400, lr = 0.00464375
I0630 11:15:55.667531 29777 solver.cpp:290] Iteration 171500 (6.20415 iter/s, 16.1182s/100 iter), loss = 1.2381
I0630 11:15:55.667562 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 11:15:55.667572 29777 sgd_solver.cpp:106] Iteration 171500, lr = 0.00464062
I0630 11:16:11.694209 29777 solver.cpp:290] Iteration 171600 (6.23978 iter/s, 16.0262s/100 iter), loss = 1.02381
I0630 11:16:11.694298 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 11:16:11.694308 29777 sgd_solver.cpp:106] Iteration 171600, lr = 0.0046375
I0630 11:16:27.974938 29777 solver.cpp:290] Iteration 171700 (6.14243 iter/s, 16.2802s/100 iter), loss = 1.2619
I0630 11:16:27.974967 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 11:16:27.974977 29777 sgd_solver.cpp:106] Iteration 171700, lr = 0.00463437
I0630 11:16:44.043372 29777 solver.cpp:290] Iteration 171800 (6.22356 iter/s, 16.068s/100 iter), loss = 1.19048
I0630 11:16:44.043453 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 11:16:44.043462 29777 sgd_solver.cpp:106] Iteration 171800, lr = 0.00463125
I0630 11:17:00.436560 29777 solver.cpp:290] Iteration 171900 (6.10029 iter/s, 16.3927s/100 iter), loss = 1.52381
I0630 11:17:00.436589 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 11:17:00.436595 29777 sgd_solver.cpp:106] Iteration 171900, lr = 0.00462812
I0630 11:17:16.364609 29777 solver.cpp:354] Sparsity after update:
I0630 11:17:16.366996 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:17:16.367013 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:17:16.367030 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:17:16.367036 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:17:16.367041 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:17:16.367048 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:17:16.367056 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:17:16.367063 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:17:16.367067 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:17:16.367074 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:17:16.367079 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:17:16.367084 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:17:16.367090 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:17:16.367295 29777 solver.cpp:471] Iteration 172000, Testing net (#0)
I0630 11:17:36.159358 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 11:18:25.823510 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54292
I0630 11:18:25.823617 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.78236
I0630 11:18:25.823627 29777 solver.cpp:544]     Test net output #2: loss = 1.64092 (* 1 = 1.64092 loss)
I0630 11:18:25.994895 29777 solver.cpp:290] Iteration 172000 (1.16883 iter/s, 85.556s/100 iter), loss = 1.2619
I0630 11:18:25.994923 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 11:18:25.994933 29777 sgd_solver.cpp:106] Iteration 172000, lr = 0.004625
I0630 11:18:42.249028 29777 solver.cpp:290] Iteration 172100 (6.15246 iter/s, 16.2537s/100 iter), loss = 1.40476
I0630 11:18:42.249061 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 11:18:42.249071 29777 sgd_solver.cpp:106] Iteration 172100, lr = 0.00462188
I0630 11:18:58.390177 29777 solver.cpp:290] Iteration 172200 (6.19553 iter/s, 16.1407s/100 iter), loss = 1.54762
I0630 11:18:58.390278 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 11:18:58.390297 29777 sgd_solver.cpp:106] Iteration 172200, lr = 0.00461875
I0630 11:19:14.600037 29777 solver.cpp:290] Iteration 172300 (6.16929 iter/s, 16.2093s/100 iter), loss = 1.2381
I0630 11:19:14.600067 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 11:19:14.600075 29777 sgd_solver.cpp:106] Iteration 172300, lr = 0.00461562
I0630 11:19:30.740658 29777 solver.cpp:290] Iteration 172400 (6.19573 iter/s, 16.1402s/100 iter), loss = 0.952381
I0630 11:19:30.740749 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 11:19:30.740759 29777 sgd_solver.cpp:106] Iteration 172400, lr = 0.0046125
I0630 11:19:46.850600 29777 solver.cpp:290] Iteration 172500 (6.20755 iter/s, 16.1094s/100 iter), loss = 1.35714
I0630 11:19:46.850627 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 11:19:46.850636 29777 sgd_solver.cpp:106] Iteration 172500, lr = 0.00460937
I0630 11:20:03.008889 29777 solver.cpp:290] Iteration 172600 (6.18895 iter/s, 16.1578s/100 iter), loss = 1.25
I0630 11:20:03.008999 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 11:20:03.009011 29777 sgd_solver.cpp:106] Iteration 172600, lr = 0.00460625
I0630 11:20:19.150060 29777 solver.cpp:290] Iteration 172700 (6.19555 iter/s, 16.1406s/100 iter), loss = 1.25
I0630 11:20:19.150087 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 11:20:19.150094 29777 sgd_solver.cpp:106] Iteration 172700, lr = 0.00460312
I0630 11:20:35.345355 29777 solver.cpp:290] Iteration 172800 (6.17481 iter/s, 16.1948s/100 iter), loss = 1.45238
I0630 11:20:35.345484 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 11:20:35.345515 29777 sgd_solver.cpp:106] Iteration 172800, lr = 0.0046
I0630 11:20:52.074048 29777 solver.cpp:290] Iteration 172900 (5.97796 iter/s, 16.7281s/100 iter), loss = 1.29762
I0630 11:20:52.074101 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 11:20:52.074122 29777 sgd_solver.cpp:106] Iteration 172900, lr = 0.00459687
I0630 11:21:08.631024 29777 solver.cpp:354] Sparsity after update:
I0630 11:21:08.651448 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:21:08.651491 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:21:08.651507 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:21:08.651517 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:21:08.651526 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:21:08.651535 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:21:08.651545 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:21:08.651553 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:21:08.651562 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:21:08.651571 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:21:08.651581 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:21:08.651589 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:21:08.651598 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:21:08.806525 29777 solver.cpp:290] Iteration 173000 (5.97658 iter/s, 16.732s/100 iter), loss = 1.58333
I0630 11:21:08.806550 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 11:21:08.806556 29777 sgd_solver.cpp:106] Iteration 173000, lr = 0.00459375
I0630 11:21:25.077198 29777 solver.cpp:290] Iteration 173100 (6.1462 iter/s, 16.2702s/100 iter), loss = 2.05952
I0630 11:21:25.077224 29777 solver.cpp:309]     Train net output #0: loss = 2.09524 (* 1 = 2.09524 loss)
I0630 11:21:25.077239 29777 sgd_solver.cpp:106] Iteration 173100, lr = 0.00459062
I0630 11:21:41.215838 29777 solver.cpp:290] Iteration 173200 (6.19649 iter/s, 16.1382s/100 iter), loss = 1.5119
I0630 11:21:41.215955 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 11:21:41.215970 29777 sgd_solver.cpp:106] Iteration 173200, lr = 0.0045875
I0630 11:21:57.333263 29777 solver.cpp:290] Iteration 173300 (6.20468 iter/s, 16.1169s/100 iter), loss = 1.47619
I0630 11:21:57.333290 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 11:21:57.333299 29777 sgd_solver.cpp:106] Iteration 173300, lr = 0.00458437
I0630 11:22:13.528096 29777 solver.cpp:290] Iteration 173400 (6.17499 iter/s, 16.1944s/100 iter), loss = 1.44048
I0630 11:22:13.528197 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 11:22:13.528220 29777 sgd_solver.cpp:106] Iteration 173400, lr = 0.00458125
I0630 11:22:29.686723 29777 solver.cpp:290] Iteration 173500 (6.18885 iter/s, 16.1581s/100 iter), loss = 1
I0630 11:22:29.686774 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 11:22:29.686805 29777 sgd_solver.cpp:106] Iteration 173500, lr = 0.00457812
I0630 11:22:45.842494 29777 solver.cpp:290] Iteration 173600 (6.18993 iter/s, 16.1553s/100 iter), loss = 1.46429
I0630 11:22:45.842577 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 11:22:45.842586 29777 sgd_solver.cpp:106] Iteration 173600, lr = 0.004575
I0630 11:23:01.927233 29777 solver.cpp:290] Iteration 173700 (6.21727 iter/s, 16.0842s/100 iter), loss = 0.940476
I0630 11:23:01.927260 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 11:23:01.927269 29777 sgd_solver.cpp:106] Iteration 173700, lr = 0.00457187
I0630 11:23:17.994873 29777 solver.cpp:290] Iteration 173800 (6.22387 iter/s, 16.0672s/100 iter), loss = 1.57143
I0630 11:23:17.994968 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 11:23:17.994981 29777 sgd_solver.cpp:106] Iteration 173800, lr = 0.00456875
I0630 11:23:34.296128 29777 solver.cpp:290] Iteration 173900 (6.1347 iter/s, 16.3007s/100 iter), loss = 1.25
I0630 11:23:34.296154 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 11:23:34.296164 29777 sgd_solver.cpp:106] Iteration 173900, lr = 0.00456562
I0630 11:23:50.343266 29777 solver.cpp:354] Sparsity after update:
I0630 11:23:50.344727 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:23:50.344734 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:23:50.344741 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:23:50.344743 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:23:50.344745 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:23:50.344748 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:23:50.344749 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:23:50.344751 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:23:50.344753 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:23:50.344755 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:23:50.344758 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:23:50.344759 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:23:50.344761 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:23:50.344847 29777 solver.cpp:471] Iteration 174000, Testing net (#0)
I0630 11:24:08.352706 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 11:24:55.087182 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54368
I0630 11:24:55.087306 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.784941
I0630 11:24:55.087327 29777 solver.cpp:544]     Test net output #2: loss = 1.62624 (* 1 = 1.62624 loss)
I0630 11:24:55.295344 29777 solver.cpp:290] Iteration 174000 (1.23461 iter/s, 80.997s/100 iter), loss = 1.44048
I0630 11:24:55.295384 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 11:24:55.295403 29777 sgd_solver.cpp:106] Iteration 174000, lr = 0.0045625
I0630 11:25:11.636245 29777 solver.cpp:290] Iteration 174100 (6.11979 iter/s, 16.3404s/100 iter), loss = 1.36905
I0630 11:25:11.636268 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 11:25:11.636274 29777 sgd_solver.cpp:106] Iteration 174100, lr = 0.00455937
I0630 11:25:27.769809 29777 solver.cpp:290] Iteration 174200 (6.19844 iter/s, 16.1331s/100 iter), loss = 1.39286
I0630 11:25:27.769891 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 11:25:27.769901 29777 sgd_solver.cpp:106] Iteration 174200, lr = 0.00455625
I0630 11:25:43.923708 29777 solver.cpp:290] Iteration 174300 (6.19066 iter/s, 16.1534s/100 iter), loss = 1.28571
I0630 11:25:43.923748 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 11:25:43.923763 29777 sgd_solver.cpp:106] Iteration 174300, lr = 0.00455312
I0630 11:26:00.023892 29777 solver.cpp:290] Iteration 174400 (6.21129 iter/s, 16.0997s/100 iter), loss = 1.02381
I0630 11:26:00.024081 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 11:26:00.024171 29777 sgd_solver.cpp:106] Iteration 174400, lr = 0.00455
I0630 11:26:16.253459 29777 solver.cpp:290] Iteration 174500 (6.16179 iter/s, 16.229s/100 iter), loss = 0.988096
I0630 11:26:16.253484 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 11:26:16.253494 29777 sgd_solver.cpp:106] Iteration 174500, lr = 0.00454687
I0630 11:26:32.470969 29777 solver.cpp:290] Iteration 174600 (6.16635 iter/s, 16.217s/100 iter), loss = 1.21429
I0630 11:26:32.471050 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 11:26:32.471060 29777 sgd_solver.cpp:106] Iteration 174600, lr = 0.00454375
I0630 11:26:48.429163 29777 solver.cpp:290] Iteration 174700 (6.26658 iter/s, 15.9577s/100 iter), loss = 1
I0630 11:26:48.429190 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 11:26:48.429199 29777 sgd_solver.cpp:106] Iteration 174700, lr = 0.00454063
I0630 11:27:04.618135 29777 solver.cpp:290] Iteration 174800 (6.17722 iter/s, 16.1885s/100 iter), loss = 1.52381
I0630 11:27:04.618216 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 11:27:04.618227 29777 sgd_solver.cpp:106] Iteration 174800, lr = 0.0045375
I0630 11:27:20.639051 29777 solver.cpp:290] Iteration 174900 (6.24204 iter/s, 16.0204s/100 iter), loss = 1.34524
I0630 11:27:20.639076 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 11:27:20.639086 29777 sgd_solver.cpp:106] Iteration 174900, lr = 0.00453437
I0630 11:27:36.589658 29777 solver.cpp:354] Sparsity after update:
I0630 11:27:36.610195 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:27:36.610211 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:27:36.610222 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:27:36.610226 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:27:36.610229 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:27:36.610232 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:27:36.610235 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:27:36.610239 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:27:36.610242 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:27:36.610245 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:27:36.610249 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:27:36.610251 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:27:36.610255 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:27:36.764014 29777 solver.cpp:290] Iteration 175000 (6.20175 iter/s, 16.1245s/100 iter), loss = 1.60714
I0630 11:27:36.764065 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 11:27:36.764086 29777 sgd_solver.cpp:106] Iteration 175000, lr = 0.00453125
I0630 11:27:52.794682 29777 solver.cpp:290] Iteration 175100 (6.23823 iter/s, 16.0302s/100 iter), loss = 1.65476
I0630 11:27:52.794726 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 11:27:52.794736 29777 sgd_solver.cpp:106] Iteration 175100, lr = 0.00452812
I0630 11:28:08.956820 29777 solver.cpp:290] Iteration 175200 (6.18748 iter/s, 16.1617s/100 iter), loss = 1.16667
I0630 11:28:08.956894 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 11:28:08.956905 29777 sgd_solver.cpp:106] Iteration 175200, lr = 0.004525
I0630 11:28:25.106338 29777 solver.cpp:290] Iteration 175300 (6.19233 iter/s, 16.149s/100 iter), loss = 1.2381
I0630 11:28:25.106385 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 11:28:25.106403 29777 sgd_solver.cpp:106] Iteration 175300, lr = 0.00452187
I0630 11:28:41.205587 29777 solver.cpp:290] Iteration 175400 (6.21166 iter/s, 16.0988s/100 iter), loss = 0.869048
I0630 11:28:41.205687 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 11:28:41.205698 29777 sgd_solver.cpp:106] Iteration 175400, lr = 0.00451875
I0630 11:28:57.202970 29777 solver.cpp:290] Iteration 175500 (6.25123 iter/s, 15.9969s/100 iter), loss = 1.32143
I0630 11:28:57.202993 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 11:28:57.203001 29777 sgd_solver.cpp:106] Iteration 175500, lr = 0.00451563
I0630 11:29:13.223006 29777 solver.cpp:290] Iteration 175600 (6.24236 iter/s, 16.0196s/100 iter), loss = 1.71429
I0630 11:29:13.223129 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 11:29:13.223142 29777 sgd_solver.cpp:106] Iteration 175600, lr = 0.0045125
I0630 11:29:29.268057 29777 solver.cpp:290] Iteration 175700 (6.23267 iter/s, 16.0445s/100 iter), loss = 1.44048
I0630 11:29:29.268081 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 11:29:29.268088 29777 sgd_solver.cpp:106] Iteration 175700, lr = 0.00450937
I0630 11:29:45.309984 29777 solver.cpp:290] Iteration 175800 (6.23384 iter/s, 16.0415s/100 iter), loss = 1.34524
I0630 11:29:45.310065 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 11:29:45.310073 29777 sgd_solver.cpp:106] Iteration 175800, lr = 0.00450625
I0630 11:30:01.280839 29777 solver.cpp:290] Iteration 175900 (6.26161 iter/s, 15.9703s/100 iter), loss = 1.2381
I0630 11:30:01.280866 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 11:30:01.280875 29777 sgd_solver.cpp:106] Iteration 175900, lr = 0.00450312
I0630 11:30:17.155743 29777 solver.cpp:354] Sparsity after update:
I0630 11:30:17.157187 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:30:17.157194 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:30:17.157202 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:30:17.157204 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:30:17.157207 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:30:17.157209 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:30:17.157212 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:30:17.157214 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:30:17.157217 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:30:17.157219 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:30:17.157222 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:30:17.157224 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:30:17.157227 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:30:17.157312 29777 solver.cpp:471] Iteration 176000, Testing net (#0)
I0630 11:30:31.941349 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 11:31:06.793934 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54502
I0630 11:31:06.793992 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.783641
I0630 11:31:06.793999 29777 solver.cpp:544]     Test net output #2: loss = 1.63206 (* 1 = 1.63206 loss)
I0630 11:31:06.970237 29777 solver.cpp:290] Iteration 176000 (1.52236 iter/s, 65.6876s/100 iter), loss = 1.60714
I0630 11:31:06.970260 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 11:31:06.970266 29777 sgd_solver.cpp:106] Iteration 176000, lr = 0.0045
I0630 11:31:22.960647 29777 solver.cpp:290] Iteration 176100 (6.25393 iter/s, 15.9899s/100 iter), loss = 1.5
I0630 11:31:22.960670 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 11:31:22.960676 29777 sgd_solver.cpp:106] Iteration 176100, lr = 0.00449687
I0630 11:31:38.998368 29777 solver.cpp:290] Iteration 176200 (6.23548 iter/s, 16.0373s/100 iter), loss = 1.35714
I0630 11:31:39.000542 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 11:31:39.000566 29777 sgd_solver.cpp:106] Iteration 176200, lr = 0.00449375
I0630 11:31:55.032958 29777 solver.cpp:290] Iteration 176300 (6.23753 iter/s, 16.032s/100 iter), loss = 0.988096
I0630 11:31:55.033020 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 11:31:55.033049 29777 sgd_solver.cpp:106] Iteration 176300, lr = 0.00449063
I0630 11:32:11.021749 29777 solver.cpp:290] Iteration 176400 (6.25458 iter/s, 15.9883s/100 iter), loss = 1.71429
I0630 11:32:11.022395 29777 solver.cpp:309]     Train net output #0: loss = 1.71429 (* 1 = 1.71429 loss)
I0630 11:32:11.022406 29777 sgd_solver.cpp:106] Iteration 176400, lr = 0.0044875
I0630 11:32:27.125950 29777 solver.cpp:290] Iteration 176500 (6.20998 iter/s, 16.1031s/100 iter), loss = 1.14286
I0630 11:32:27.125972 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 11:32:27.125977 29777 sgd_solver.cpp:106] Iteration 176500, lr = 0.00448438
I0630 11:32:43.084329 29777 solver.cpp:290] Iteration 176600 (6.26648 iter/s, 15.9579s/100 iter), loss = 1.19048
I0630 11:32:43.084426 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 11:32:43.084439 29777 sgd_solver.cpp:106] Iteration 176600, lr = 0.00448125
I0630 11:32:59.055630 29777 solver.cpp:290] Iteration 176700 (6.26144 iter/s, 15.9708s/100 iter), loss = 1.38095
I0630 11:32:59.055655 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 11:32:59.055665 29777 sgd_solver.cpp:106] Iteration 176700, lr = 0.00447812
I0630 11:33:15.028695 29777 solver.cpp:290] Iteration 176800 (6.26072 iter/s, 15.9726s/100 iter), loss = 1.10714
I0630 11:33:15.028786 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 11:33:15.028800 29777 sgd_solver.cpp:106] Iteration 176800, lr = 0.004475
I0630 11:33:31.194072 29777 solver.cpp:290] Iteration 176900 (6.18626 iter/s, 16.1648s/100 iter), loss = 1.16667
I0630 11:33:31.194094 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 11:33:31.194102 29777 sgd_solver.cpp:106] Iteration 176900, lr = 0.00447187
I0630 11:33:47.183140 29777 solver.cpp:354] Sparsity after update:
I0630 11:33:47.203536 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:33:47.203553 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:33:47.203563 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:33:47.203567 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:33:47.203570 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:33:47.203573 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:33:47.203577 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:33:47.203580 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:33:47.203583 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:33:47.203588 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:33:47.203590 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:33:47.203593 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:33:47.203596 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:33:47.359843 29777 solver.cpp:290] Iteration 177000 (6.18609 iter/s, 16.1653s/100 iter), loss = 1.27381
I0630 11:33:47.359864 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 11:33:47.359871 29777 sgd_solver.cpp:106] Iteration 177000, lr = 0.00446875
I0630 11:34:03.396483 29777 solver.cpp:290] Iteration 177100 (6.2359 iter/s, 16.0362s/100 iter), loss = 1.78571
I0630 11:34:03.396507 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 11:34:03.396513 29777 sgd_solver.cpp:106] Iteration 177100, lr = 0.00446563
I0630 11:34:19.505506 29777 solver.cpp:290] Iteration 177200 (6.20788 iter/s, 16.1086s/100 iter), loss = 1.14286
I0630 11:34:19.505867 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 11:34:19.505878 29777 sgd_solver.cpp:106] Iteration 177200, lr = 0.0044625
I0630 11:34:35.605304 29777 solver.cpp:290] Iteration 177300 (6.21157 iter/s, 16.099s/100 iter), loss = 1.19048
I0630 11:34:35.605329 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 11:34:35.605339 29777 sgd_solver.cpp:106] Iteration 177300, lr = 0.00445938
I0630 11:34:51.616605 29777 solver.cpp:290] Iteration 177400 (6.24577 iter/s, 16.0108s/100 iter), loss = 1.60714
I0630 11:34:51.616708 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 11:34:51.616717 29777 sgd_solver.cpp:106] Iteration 177400, lr = 0.00445625
I0630 11:35:07.629328 29777 solver.cpp:290] Iteration 177500 (6.24524 iter/s, 16.0122s/100 iter), loss = 1.33333
I0630 11:35:07.629351 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 11:35:07.629359 29777 sgd_solver.cpp:106] Iteration 177500, lr = 0.00445312
I0630 11:35:23.677886 29777 solver.cpp:290] Iteration 177600 (6.23127 iter/s, 16.0481s/100 iter), loss = 1.25
I0630 11:35:23.677975 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 11:35:23.677983 29777 sgd_solver.cpp:106] Iteration 177600, lr = 0.00445
I0630 11:35:39.821091 29777 solver.cpp:290] Iteration 177700 (6.19476 iter/s, 16.1427s/100 iter), loss = 1.36905
I0630 11:35:39.821120 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 11:35:39.821128 29777 sgd_solver.cpp:106] Iteration 177700, lr = 0.00444687
I0630 11:35:55.831712 29777 solver.cpp:290] Iteration 177800 (6.24604 iter/s, 16.0102s/100 iter), loss = 1.39286
I0630 11:35:55.831804 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 11:35:55.831815 29777 sgd_solver.cpp:106] Iteration 177800, lr = 0.00444375
I0630 11:36:11.910784 29777 solver.cpp:290] Iteration 177900 (6.21947 iter/s, 16.0785s/100 iter), loss = 1.47619
I0630 11:36:11.910809 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 11:36:11.910815 29777 sgd_solver.cpp:106] Iteration 177900, lr = 0.00444062
I0630 11:36:27.739439 29777 solver.cpp:354] Sparsity after update:
I0630 11:36:27.740921 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:36:27.740929 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:36:27.740941 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:36:27.740944 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:36:27.740947 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:36:27.740952 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:36:27.740955 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:36:27.740959 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:36:27.740963 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:36:27.740967 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:36:27.740972 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:36:27.740975 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:36:27.740980 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:36:27.741117 29777 solver.cpp:471] Iteration 178000, Testing net (#0)
I0630 11:36:42.541292 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 11:37:16.974917 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.543879
I0630 11:37:16.974959 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.783221
I0630 11:37:16.974966 29777 solver.cpp:544]     Test net output #2: loss = 1.63296 (* 1 = 1.63296 loss)
I0630 11:37:17.148187 29777 solver.cpp:290] Iteration 178000 (1.53291 iter/s, 65.2356s/100 iter), loss = 1.16667
I0630 11:37:17.148211 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 11:37:17.148217 29777 sgd_solver.cpp:106] Iteration 178000, lr = 0.0044375
I0630 11:37:33.153815 29777 solver.cpp:290] Iteration 178100 (6.24798 iter/s, 16.0052s/100 iter), loss = 1.13095
I0630 11:37:33.153839 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 11:37:33.153846 29777 sgd_solver.cpp:106] Iteration 178100, lr = 0.00443438
I0630 11:37:49.100049 29777 solver.cpp:290] Iteration 178200 (6.27126 iter/s, 15.9458s/100 iter), loss = 1.09524
I0630 11:37:49.100159 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 11:37:49.100168 29777 sgd_solver.cpp:106] Iteration 178200, lr = 0.00443125
I0630 11:38:05.283841 29777 solver.cpp:290] Iteration 178300 (6.17923 iter/s, 16.1832s/100 iter), loss = 1.52381
I0630 11:38:05.283864 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 11:38:05.283870 29777 sgd_solver.cpp:106] Iteration 178300, lr = 0.00442812
I0630 11:38:21.304919 29777 solver.cpp:290] Iteration 178400 (6.24196 iter/s, 16.0206s/100 iter), loss = 1.27381
I0630 11:38:21.305022 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 11:38:21.305033 29777 sgd_solver.cpp:106] Iteration 178400, lr = 0.004425
I0630 11:38:37.357851 29777 solver.cpp:290] Iteration 178500 (6.2296 iter/s, 16.0524s/100 iter), loss = 0.869048
I0630 11:38:37.357873 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 11:38:37.357880 29777 sgd_solver.cpp:106] Iteration 178500, lr = 0.00442187
I0630 11:38:53.355039 29777 solver.cpp:290] Iteration 178600 (6.25128 iter/s, 15.9967s/100 iter), loss = 1.09524
I0630 11:38:53.355104 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 11:38:53.355116 29777 sgd_solver.cpp:106] Iteration 178600, lr = 0.00441875
I0630 11:39:09.448429 29777 solver.cpp:290] Iteration 178700 (6.21393 iter/s, 16.0929s/100 iter), loss = 1.57143
I0630 11:39:09.448456 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 11:39:09.448464 29777 sgd_solver.cpp:106] Iteration 178700, lr = 0.00441562
I0630 11:39:25.403528 29777 solver.cpp:290] Iteration 178800 (6.26777 iter/s, 15.9546s/100 iter), loss = 1.2619
I0630 11:39:25.403642 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 11:39:25.403651 29777 sgd_solver.cpp:106] Iteration 178800, lr = 0.0044125
I0630 11:39:41.393363 29777 solver.cpp:290] Iteration 178900 (6.25419 iter/s, 15.9893s/100 iter), loss = 1.35714
I0630 11:39:41.393386 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 11:39:41.393395 29777 sgd_solver.cpp:106] Iteration 178900, lr = 0.00440938
I0630 11:39:57.355361 29777 solver.cpp:354] Sparsity after update:
I0630 11:39:57.375777 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:39:57.375805 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:39:57.375838 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:39:57.375854 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:39:57.375870 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:39:57.375885 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:39:57.375900 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:39:57.375917 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:39:57.375932 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:39:57.375946 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:39:57.375964 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:39:57.375979 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:39:57.375995 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:39:57.530066 29777 solver.cpp:290] Iteration 179000 (6.19723 iter/s, 16.1362s/100 iter), loss = 1.2619
I0630 11:39:57.530120 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 11:39:57.530143 29777 sgd_solver.cpp:106] Iteration 179000, lr = 0.00440625
I0630 11:40:13.763413 29777 solver.cpp:290] Iteration 179100 (6.16035 iter/s, 16.2329s/100 iter), loss = 1.52381
I0630 11:40:13.763439 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 11:40:13.763453 29777 sgd_solver.cpp:106] Iteration 179100, lr = 0.00440313
I0630 11:40:29.843394 29777 solver.cpp:290] Iteration 179200 (6.2191 iter/s, 16.0795s/100 iter), loss = 1.72619
I0630 11:40:29.843508 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 11:40:29.843540 29777 sgd_solver.cpp:106] Iteration 179200, lr = 0.0044
I0630 11:40:45.867310 29777 solver.cpp:290] Iteration 179300 (6.24089 iter/s, 16.0234s/100 iter), loss = 1.20238
I0630 11:40:45.867373 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 11:40:45.867395 29777 sgd_solver.cpp:106] Iteration 179300, lr = 0.00439687
I0630 11:41:02.149670 29777 solver.cpp:290] Iteration 179400 (6.14181 iter/s, 16.2818s/100 iter), loss = 1.53571
I0630 11:41:02.149790 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 11:41:02.149809 29777 sgd_solver.cpp:106] Iteration 179400, lr = 0.00439375
I0630 11:41:18.146337 29777 solver.cpp:290] Iteration 179500 (6.25152 iter/s, 15.9961s/100 iter), loss = 1.46429
I0630 11:41:18.146363 29777 solver.cpp:309]     Train net output #0: loss = 1.88095 (* 1 = 1.88095 loss)
I0630 11:41:18.146373 29777 sgd_solver.cpp:106] Iteration 179500, lr = 0.00439062
I0630 11:41:34.193557 29777 solver.cpp:290] Iteration 179600 (6.23179 iter/s, 16.0468s/100 iter), loss = 1.45238
I0630 11:41:34.193672 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 11:41:34.193683 29777 sgd_solver.cpp:106] Iteration 179600, lr = 0.0043875
I0630 11:41:50.231928 29777 solver.cpp:290] Iteration 179700 (6.23526 iter/s, 16.0378s/100 iter), loss = 1.54762
I0630 11:41:50.231958 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 11:41:50.231967 29777 sgd_solver.cpp:106] Iteration 179700, lr = 0.00438438
I0630 11:42:06.398386 29777 solver.cpp:290] Iteration 179800 (6.18583 iter/s, 16.166s/100 iter), loss = 1.33333
I0630 11:42:06.398458 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 11:42:06.398465 29777 sgd_solver.cpp:106] Iteration 179800, lr = 0.00438125
I0630 11:42:22.497493 29777 solver.cpp:290] Iteration 179900 (6.21172 iter/s, 16.0986s/100 iter), loss = 1.29762
I0630 11:42:22.497516 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 11:42:22.497524 29777 sgd_solver.cpp:106] Iteration 179900, lr = 0.00437813
I0630 11:42:38.419026 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_180000.caffemodel
I0630 11:42:38.438383 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_180000.solverstate
I0630 11:42:38.447789 29777 solver.cpp:354] Sparsity after update:
I0630 11:42:38.448766 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:42:38.448776 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:42:38.448782 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:42:38.448786 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:42:38.448788 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:42:38.448791 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:42:38.448793 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:42:38.448796 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:42:38.448797 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:42:38.448799 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:42:38.448801 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:42:38.448803 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:42:38.448806 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:42:38.448902 29777 solver.cpp:471] Iteration 180000, Testing net (#0)
I0630 11:42:54.980329 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 11:43:35.511044 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54628
I0630 11:43:35.511154 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.786281
I0630 11:43:35.511165 29777 solver.cpp:544]     Test net output #2: loss = 1.6174 (* 1 = 1.6174 loss)
I0630 11:43:35.744496 29777 solver.cpp:290] Iteration 180000 (1.36528 iter/s, 73.245s/100 iter), loss = 1.13095
I0630 11:43:35.744552 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 11:43:35.744575 29777 sgd_solver.cpp:106] Iteration 180000, lr = 0.004375
I0630 11:43:51.852099 29777 solver.cpp:290] Iteration 180100 (6.20844 iter/s, 16.1071s/100 iter), loss = 1.39286
I0630 11:43:51.852136 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 11:43:51.852149 29777 sgd_solver.cpp:106] Iteration 180100, lr = 0.00437187
I0630 11:44:07.822502 29777 solver.cpp:290] Iteration 180200 (6.26177 iter/s, 15.9699s/100 iter), loss = 1.19048
I0630 11:44:07.822608 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 11:44:07.822620 29777 sgd_solver.cpp:106] Iteration 180200, lr = 0.00436875
I0630 11:44:23.875380 29777 solver.cpp:290] Iteration 180300 (6.22963 iter/s, 16.0523s/100 iter), loss = 1.4881
I0630 11:44:23.875404 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 11:44:23.875414 29777 sgd_solver.cpp:106] Iteration 180300, lr = 0.00436562
I0630 11:44:39.891939 29777 solver.cpp:290] Iteration 180400 (6.24372 iter/s, 16.0161s/100 iter), loss = 1.55952
I0630 11:44:39.891984 29777 solver.cpp:309]     Train net output #0: loss = 2.07143 (* 1 = 2.07143 loss)
I0630 11:44:39.891993 29777 sgd_solver.cpp:106] Iteration 180400, lr = 0.0043625
I0630 11:44:55.966861 29777 solver.cpp:290] Iteration 180500 (6.22106 iter/s, 16.0744s/100 iter), loss = 1.60714
I0630 11:44:55.966882 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 11:44:55.966888 29777 sgd_solver.cpp:106] Iteration 180500, lr = 0.00435938
I0630 11:45:12.018780 29777 solver.cpp:290] Iteration 180600 (6.22997 iter/s, 16.0515s/100 iter), loss = 0.916667
I0630 11:45:12.018880 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 11:45:12.018894 29777 sgd_solver.cpp:106] Iteration 180600, lr = 0.00435625
I0630 11:45:28.088238 29777 solver.cpp:290] Iteration 180700 (6.22319 iter/s, 16.0689s/100 iter), loss = 1.21429
I0630 11:45:28.088260 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 11:45:28.088270 29777 sgd_solver.cpp:106] Iteration 180700, lr = 0.00435313
I0630 11:45:44.120739 29777 solver.cpp:290] Iteration 180800 (6.23751 iter/s, 16.032s/100 iter), loss = 1.59524
I0630 11:45:44.120828 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 11:45:44.120852 29777 sgd_solver.cpp:106] Iteration 180800, lr = 0.00435
I0630 11:46:00.169137 29777 solver.cpp:290] Iteration 180900 (6.23136 iter/s, 16.0479s/100 iter), loss = 1.25
I0630 11:46:00.169183 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 11:46:00.169209 29777 sgd_solver.cpp:106] Iteration 180900, lr = 0.00434688
I0630 11:46:16.132161 29777 solver.cpp:354] Sparsity after update:
I0630 11:46:16.157310 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:46:16.157337 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:46:16.157354 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:46:16.157361 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:46:16.157367 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:46:16.157372 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:46:16.157377 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:46:16.157387 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:46:16.157392 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:46:16.157399 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:46:16.157405 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:46:16.157411 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:46:16.157418 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:46:16.316292 29777 solver.cpp:290] Iteration 181000 (6.19323 iter/s, 16.1467s/100 iter), loss = 1.34524
I0630 11:46:16.316315 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 11:46:16.316323 29777 sgd_solver.cpp:106] Iteration 181000, lr = 0.00434375
I0630 11:46:32.371177 29777 solver.cpp:290] Iteration 181100 (6.22881 iter/s, 16.0544s/100 iter), loss = 1.19048
I0630 11:46:32.371199 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 11:46:32.371225 29777 sgd_solver.cpp:106] Iteration 181100, lr = 0.00434062
I0630 11:46:48.394492 29777 solver.cpp:290] Iteration 181200 (6.24109 iter/s, 16.0229s/100 iter), loss = 1.28571
I0630 11:46:48.394596 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 11:46:48.394608 29777 sgd_solver.cpp:106] Iteration 181200, lr = 0.0043375
I0630 11:47:04.624711 29777 solver.cpp:290] Iteration 181300 (6.16155 iter/s, 16.2297s/100 iter), loss = 1.58333
I0630 11:47:04.624739 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 11:47:04.624749 29777 sgd_solver.cpp:106] Iteration 181300, lr = 0.00433438
I0630 11:47:20.897552 29777 solver.cpp:290] Iteration 181400 (6.14539 iter/s, 16.2724s/100 iter), loss = 1
I0630 11:47:20.897797 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 11:47:20.897902 29777 sgd_solver.cpp:106] Iteration 181400, lr = 0.00433125
I0630 11:47:37.033638 29777 solver.cpp:290] Iteration 181500 (6.19755 iter/s, 16.1354s/100 iter), loss = 1.46429
I0630 11:47:37.033670 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 11:47:37.033681 29777 sgd_solver.cpp:106] Iteration 181500, lr = 0.00432813
I0630 11:47:53.098098 29777 solver.cpp:290] Iteration 181600 (6.2251 iter/s, 16.064s/100 iter), loss = 1.16667
I0630 11:47:53.098192 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 11:47:53.098203 29777 sgd_solver.cpp:106] Iteration 181600, lr = 0.004325
I0630 11:48:09.166232 29777 solver.cpp:290] Iteration 181700 (6.22371 iter/s, 16.0676s/100 iter), loss = 1.2619
I0630 11:48:09.166290 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 11:48:09.166438 29777 sgd_solver.cpp:106] Iteration 181700, lr = 0.00432188
I0630 11:48:25.311166 29777 solver.cpp:290] Iteration 181800 (6.19409 iter/s, 16.1444s/100 iter), loss = 1.40476
I0630 11:48:25.311264 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 11:48:25.311280 29777 sgd_solver.cpp:106] Iteration 181800, lr = 0.00431875
I0630 11:48:41.454052 29777 solver.cpp:290] Iteration 181900 (6.19489 iter/s, 16.1423s/100 iter), loss = 1.20238
I0630 11:48:41.454082 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 11:48:41.454090 29777 sgd_solver.cpp:106] Iteration 181900, lr = 0.00431562
I0630 11:48:57.334683 29777 solver.cpp:354] Sparsity after update:
I0630 11:48:57.335964 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:48:57.335971 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:48:57.335978 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:48:57.335980 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:48:57.335983 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:48:57.335985 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:48:57.335986 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:48:57.335988 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:48:57.335990 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:48:57.335992 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:48:57.335994 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:48:57.335996 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:48:57.335999 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:48:57.336084 29777 solver.cpp:471] Iteration 182000, Testing net (#0)
I0630 11:49:18.064666 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 11:50:03.034739 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5457
I0630 11:50:03.034792 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.784201
I0630 11:50:03.034799 29777 solver.cpp:544]     Test net output #2: loss = 1.62252 (* 1 = 1.62252 loss)
I0630 11:50:03.213630 29777 solver.cpp:290] Iteration 182000 (1.22313 iter/s, 81.7573s/100 iter), loss = 1.13095
I0630 11:50:03.213654 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 11:50:03.213663 29777 sgd_solver.cpp:106] Iteration 182000, lr = 0.0043125
I0630 11:50:19.403471 29777 solver.cpp:290] Iteration 182100 (6.17689 iter/s, 16.1894s/100 iter), loss = 1.33333
I0630 11:50:19.403508 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 11:50:19.403519 29777 sgd_solver.cpp:106] Iteration 182100, lr = 0.00430938
I0630 11:50:35.632084 29777 solver.cpp:290] Iteration 182200 (6.16214 iter/s, 16.2281s/100 iter), loss = 1.02381
I0630 11:50:35.632159 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 11:50:35.632179 29777 sgd_solver.cpp:106] Iteration 182200, lr = 0.00430625
I0630 11:50:51.780714 29777 solver.cpp:290] Iteration 182300 (6.19267 iter/s, 16.1481s/100 iter), loss = 1.25
I0630 11:50:51.780740 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 11:50:51.780750 29777 sgd_solver.cpp:106] Iteration 182300, lr = 0.00430313
I0630 11:51:07.962760 29777 solver.cpp:290] Iteration 182400 (6.17987 iter/s, 16.1816s/100 iter), loss = 1.46429
I0630 11:51:07.962836 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 11:51:07.962844 29777 sgd_solver.cpp:106] Iteration 182400, lr = 0.0043
I0630 11:51:24.084724 29777 solver.cpp:290] Iteration 182500 (6.20291 iter/s, 16.1215s/100 iter), loss = 1.15476
I0630 11:51:24.084748 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 11:51:24.084754 29777 sgd_solver.cpp:106] Iteration 182500, lr = 0.00429688
I0630 11:51:40.185528 29777 solver.cpp:290] Iteration 182600 (6.21105 iter/s, 16.1003s/100 iter), loss = 1.67857
I0630 11:51:40.185614 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 11:51:40.185638 29777 sgd_solver.cpp:106] Iteration 182600, lr = 0.00429375
I0630 11:51:56.406200 29777 solver.cpp:290] Iteration 182700 (6.16517 iter/s, 16.2201s/100 iter), loss = 1.41667
I0630 11:51:56.406239 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 11:51:56.406249 29777 sgd_solver.cpp:106] Iteration 182700, lr = 0.00429062
I0630 11:52:12.474715 29777 solver.cpp:290] Iteration 182800 (6.22354 iter/s, 16.068s/100 iter), loss = 1.42857
I0630 11:52:12.474849 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 11:52:12.474880 29777 sgd_solver.cpp:106] Iteration 182800, lr = 0.0042875
I0630 11:52:28.511916 29777 solver.cpp:290] Iteration 182900 (6.23572 iter/s, 16.0366s/100 iter), loss = 1.36905
I0630 11:52:28.511942 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 11:52:28.511951 29777 sgd_solver.cpp:106] Iteration 182900, lr = 0.00428437
I0630 11:52:44.412593 29777 solver.cpp:354] Sparsity after update:
I0630 11:52:44.433193 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:52:44.433239 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:52:44.433280 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:52:44.433300 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:52:44.433310 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:52:44.433317 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:52:44.433324 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:52:44.433331 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:52:44.433338 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:52:44.433348 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:52:44.433360 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:52:44.433375 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:52:44.433392 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:52:44.595221 29777 solver.cpp:290] Iteration 183000 (6.21781 iter/s, 16.0828s/100 iter), loss = 1.40476
I0630 11:52:44.595244 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 11:52:44.595250 29777 sgd_solver.cpp:106] Iteration 183000, lr = 0.00428125
I0630 11:53:01.157851 29777 solver.cpp:290] Iteration 183100 (6.03786 iter/s, 16.5622s/100 iter), loss = 1.40476
I0630 11:53:01.157876 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 11:53:01.157884 29777 sgd_solver.cpp:106] Iteration 183100, lr = 0.00427813
I0630 11:53:17.090247 29777 solver.cpp:290] Iteration 183200 (6.27671 iter/s, 15.9319s/100 iter), loss = 1.02381
I0630 11:53:17.090528 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 11:53:17.090636 29777 sgd_solver.cpp:106] Iteration 183200, lr = 0.004275
I0630 11:53:33.088322 29777 solver.cpp:290] Iteration 183300 (6.25103 iter/s, 15.9974s/100 iter), loss = 1.20238
I0630 11:53:33.088346 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 11:53:33.088353 29777 sgd_solver.cpp:106] Iteration 183300, lr = 0.00427188
I0630 11:53:49.043234 29777 solver.cpp:290] Iteration 183400 (6.26784 iter/s, 15.9545s/100 iter), loss = 1.09524
I0630 11:53:49.043336 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 11:53:49.043346 29777 sgd_solver.cpp:106] Iteration 183400, lr = 0.00426875
I0630 11:54:05.122162 29777 solver.cpp:290] Iteration 183500 (6.21953 iter/s, 16.0784s/100 iter), loss = 1.38095
I0630 11:54:05.122189 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 11:54:05.122198 29777 sgd_solver.cpp:106] Iteration 183500, lr = 0.00426562
I0630 11:54:21.153014 29777 solver.cpp:290] Iteration 183600 (6.23815 iter/s, 16.0304s/100 iter), loss = 1.20238
I0630 11:54:21.153120 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 11:54:21.153131 29777 sgd_solver.cpp:106] Iteration 183600, lr = 0.0042625
I0630 11:54:37.336175 29777 solver.cpp:290] Iteration 183700 (6.17947 iter/s, 16.1826s/100 iter), loss = 1.27381
I0630 11:54:37.336199 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 11:54:37.336205 29777 sgd_solver.cpp:106] Iteration 183700, lr = 0.00425937
I0630 11:54:53.444193 29777 solver.cpp:290] Iteration 183800 (6.20827 iter/s, 16.1076s/100 iter), loss = 1.20238
I0630 11:54:53.444272 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 11:54:53.444283 29777 sgd_solver.cpp:106] Iteration 183800, lr = 0.00425625
I0630 11:55:09.800820 29777 solver.cpp:290] Iteration 183900 (6.11393 iter/s, 16.3561s/100 iter), loss = 1.63095
I0630 11:55:09.800843 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 11:55:09.800851 29777 sgd_solver.cpp:106] Iteration 183900, lr = 0.00425313
I0630 11:55:25.810653 29777 solver.cpp:354] Sparsity after update:
I0630 11:55:25.812077 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:55:25.812084 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:55:25.812093 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:55:25.812094 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:55:25.812096 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:55:25.812098 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:55:25.812100 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:55:25.812103 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:55:25.812104 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:55:25.812106 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:55:25.812108 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:55:25.812110 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:55:25.812113 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:55:25.812198 29777 solver.cpp:471] Iteration 184000, Testing net (#0)
I0630 11:55:42.001765 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 11:56:15.024901 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.546499
I0630 11:56:15.024976 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.783361
I0630 11:56:15.024984 29777 solver.cpp:544]     Test net output #2: loss = 1.62646 (* 1 = 1.62646 loss)
I0630 11:56:15.208169 29777 solver.cpp:290] Iteration 184000 (1.52892 iter/s, 65.4056s/100 iter), loss = 1.32143
I0630 11:56:15.208194 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 11:56:15.208201 29777 sgd_solver.cpp:106] Iteration 184000, lr = 0.00425
I0630 11:56:31.272001 29777 solver.cpp:290] Iteration 184100 (6.22535 iter/s, 16.0634s/100 iter), loss = 1.19048
I0630 11:56:31.272024 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 11:56:31.272032 29777 sgd_solver.cpp:106] Iteration 184100, lr = 0.00424688
I0630 11:56:47.342273 29777 solver.cpp:290] Iteration 184200 (6.22285 iter/s, 16.0698s/100 iter), loss = 1.14286
I0630 11:56:47.342368 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 11:56:47.342380 29777 sgd_solver.cpp:106] Iteration 184200, lr = 0.00424375
I0630 11:57:03.368043 29777 solver.cpp:290] Iteration 184300 (6.24016 iter/s, 16.0252s/100 iter), loss = 1.19048
I0630 11:57:03.368067 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 11:57:03.368074 29777 sgd_solver.cpp:106] Iteration 184300, lr = 0.00424062
I0630 11:57:19.406862 29777 solver.cpp:290] Iteration 184400 (6.23505 iter/s, 16.0384s/100 iter), loss = 1.2381
I0630 11:57:19.406975 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 11:57:19.406990 29777 sgd_solver.cpp:106] Iteration 184400, lr = 0.0042375
I0630 11:57:35.483793 29777 solver.cpp:290] Iteration 184500 (6.22031 iter/s, 16.0764s/100 iter), loss = 1.64286
I0630 11:57:35.483817 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 11:57:35.483824 29777 sgd_solver.cpp:106] Iteration 184500, lr = 0.00423437
I0630 11:57:51.484762 29777 solver.cpp:290] Iteration 184600 (6.2498 iter/s, 16.0005s/100 iter), loss = 1.41667
I0630 11:57:51.484875 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 11:57:51.484886 29777 sgd_solver.cpp:106] Iteration 184600, lr = 0.00423125
I0630 11:58:07.576694 29777 solver.cpp:290] Iteration 184700 (6.21451 iter/s, 16.0914s/100 iter), loss = 1.17857
I0630 11:58:07.576716 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 11:58:07.576723 29777 sgd_solver.cpp:106] Iteration 184700, lr = 0.00422813
I0630 11:58:23.710542 29777 solver.cpp:290] Iteration 184800 (6.19833 iter/s, 16.1334s/100 iter), loss = 1.03571
I0630 11:58:23.710590 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 11:58:23.710636 29777 sgd_solver.cpp:106] Iteration 184800, lr = 0.004225
I0630 11:58:40.053843 29777 solver.cpp:290] Iteration 184900 (6.1189 iter/s, 16.3428s/100 iter), loss = 1.4881
I0630 11:58:40.054026 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 11:58:40.054113 29777 sgd_solver.cpp:106] Iteration 184900, lr = 0.00422187
I0630 11:58:55.860914 29777 solver.cpp:354] Sparsity after update:
I0630 11:58:55.882987 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 11:58:55.883002 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 11:58:55.883013 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 11:58:55.883016 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 11:58:55.883019 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 11:58:55.883025 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 11:58:55.883029 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 11:58:55.883033 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 11:58:55.883035 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 11:58:55.883038 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 11:58:55.883043 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 11:58:55.883045 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 11:58:55.883050 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 11:58:56.042822 29777 solver.cpp:290] Iteration 185000 (6.25455 iter/s, 15.9884s/100 iter), loss = 1.32143
I0630 11:58:56.042852 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 11:58:56.042861 29777 sgd_solver.cpp:106] Iteration 185000, lr = 0.00421875
I0630 11:59:12.095744 29777 solver.cpp:290] Iteration 185100 (6.22958 iter/s, 16.0525s/100 iter), loss = 1.2381
I0630 11:59:12.095767 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 11:59:12.095774 29777 sgd_solver.cpp:106] Iteration 185100, lr = 0.00421562
I0630 11:59:28.146909 29777 solver.cpp:290] Iteration 185200 (6.23026 iter/s, 16.0507s/100 iter), loss = 1.2381
I0630 11:59:28.147029 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 11:59:28.147040 29777 sgd_solver.cpp:106] Iteration 185200, lr = 0.0042125
I0630 11:59:44.170043 29777 solver.cpp:290] Iteration 185300 (6.24119 iter/s, 16.0226s/100 iter), loss = 1.13095
I0630 11:59:44.170064 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 11:59:44.170071 29777 sgd_solver.cpp:106] Iteration 185300, lr = 0.00420937
I0630 12:00:00.105085 29777 solver.cpp:290] Iteration 185400 (6.27566 iter/s, 15.9346s/100 iter), loss = 1.40476
I0630 12:00:00.105190 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 12:00:00.105199 29777 sgd_solver.cpp:106] Iteration 185400, lr = 0.00420625
I0630 12:00:16.082502 29777 solver.cpp:290] Iteration 185500 (6.25904 iter/s, 15.9769s/100 iter), loss = 1.47619
I0630 12:00:16.082527 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 12:00:16.082535 29777 sgd_solver.cpp:106] Iteration 185500, lr = 0.00420313
I0630 12:00:32.080106 29777 solver.cpp:290] Iteration 185600 (6.25112 iter/s, 15.9971s/100 iter), loss = 1.46429
I0630 12:00:32.080204 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 12:00:32.080214 29777 sgd_solver.cpp:106] Iteration 185600, lr = 0.0042
I0630 12:00:48.167601 29777 solver.cpp:290] Iteration 185700 (6.21622 iter/s, 16.087s/100 iter), loss = 1.64286
I0630 12:00:48.167629 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 12:00:48.167642 29777 sgd_solver.cpp:106] Iteration 185700, lr = 0.00419687
I0630 12:01:04.168866 29777 solver.cpp:290] Iteration 185800 (6.24969 iter/s, 16.0008s/100 iter), loss = 0.916667
I0630 12:01:04.168918 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 12:01:04.168928 29777 sgd_solver.cpp:106] Iteration 185800, lr = 0.00419375
I0630 12:01:20.227181 29777 solver.cpp:290] Iteration 185900 (6.22749 iter/s, 16.0578s/100 iter), loss = 1.40476
I0630 12:01:20.227206 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 12:01:20.227212 29777 sgd_solver.cpp:106] Iteration 185900, lr = 0.00419062
I0630 12:01:36.207635 29777 solver.cpp:354] Sparsity after update:
I0630 12:01:36.208896 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:01:36.208904 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:01:36.208911 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:01:36.208914 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:01:36.208916 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:01:36.208919 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:01:36.208920 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:01:36.208921 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:01:36.208923 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:01:36.208925 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:01:36.208927 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:01:36.208930 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:01:36.208931 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:01:36.209017 29777 solver.cpp:471] Iteration 186000, Testing net (#0)
I0630 12:01:52.134735 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 12:02:25.441493 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.545
I0630 12:02:25.441553 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.783661
I0630 12:02:25.441560 29777 solver.cpp:544]     Test net output #2: loss = 1.62282 (* 1 = 1.62282 loss)
I0630 12:02:25.618343 29777 solver.cpp:290] Iteration 186000 (1.5293 iter/s, 65.3894s/100 iter), loss = 1.35714
I0630 12:02:25.618368 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 12:02:25.618374 29777 sgd_solver.cpp:106] Iteration 186000, lr = 0.0041875
I0630 12:02:41.023031 29777 solver.cpp:290] Iteration 186100 (6.49172 iter/s, 15.4042s/100 iter), loss = 0.928572
I0630 12:02:41.023057 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 12:02:41.023068 29777 sgd_solver.cpp:106] Iteration 186100, lr = 0.00418437
I0630 12:02:57.040208 29777 solver.cpp:290] Iteration 186200 (6.24348 iter/s, 16.0167s/100 iter), loss = 1.42857
I0630 12:02:57.040290 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 12:02:57.040298 29777 sgd_solver.cpp:106] Iteration 186200, lr = 0.00418125
I0630 12:03:13.109071 29777 solver.cpp:290] Iteration 186300 (6.22342 iter/s, 16.0683s/100 iter), loss = 1.27381
I0630 12:03:13.109092 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 12:03:13.109098 29777 sgd_solver.cpp:106] Iteration 186300, lr = 0.00417812
I0630 12:03:29.200760 29777 solver.cpp:290] Iteration 186400 (6.21457 iter/s, 16.0912s/100 iter), loss = 0.928572
I0630 12:03:29.200810 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 12:03:29.200821 29777 sgd_solver.cpp:106] Iteration 186400, lr = 0.004175
I0630 12:03:45.252784 29777 solver.cpp:290] Iteration 186500 (6.22993 iter/s, 16.0515s/100 iter), loss = 1.15476
I0630 12:03:45.252805 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 12:03:45.252812 29777 sgd_solver.cpp:106] Iteration 186500, lr = 0.00417187
I0630 12:04:01.231449 29777 solver.cpp:290] Iteration 186600 (6.25853 iter/s, 15.9782s/100 iter), loss = 1.20238
I0630 12:04:01.231578 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 12:04:01.231613 29777 sgd_solver.cpp:106] Iteration 186600, lr = 0.00416875
I0630 12:04:17.189385 29777 solver.cpp:290] Iteration 186700 (6.2667 iter/s, 15.9574s/100 iter), loss = 1.15476
I0630 12:04:17.189416 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 12:04:17.189425 29777 sgd_solver.cpp:106] Iteration 186700, lr = 0.00416562
I0630 12:04:33.244696 29777 solver.cpp:290] Iteration 186800 (6.22865 iter/s, 16.0548s/100 iter), loss = 1.38095
I0630 12:04:33.244807 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 12:04:33.244817 29777 sgd_solver.cpp:106] Iteration 186800, lr = 0.0041625
I0630 12:04:49.321211 29777 solver.cpp:290] Iteration 186900 (6.22047 iter/s, 16.076s/100 iter), loss = 1.28571
I0630 12:04:49.321234 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 12:04:49.321241 29777 sgd_solver.cpp:106] Iteration 186900, lr = 0.00415937
I0630 12:05:05.121779 29777 solver.cpp:354] Sparsity after update:
I0630 12:05:05.142472 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:05:05.142487 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:05:05.142498 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:05:05.142501 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:05:05.142505 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:05:05.142508 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:05:05.142511 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:05:05.142515 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:05:05.142527 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:05:05.142536 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:05:05.142544 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:05:05.142549 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:05:05.142554 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:05:05.297335 29777 solver.cpp:290] Iteration 187000 (6.25953 iter/s, 15.9757s/100 iter), loss = 1.29762
I0630 12:05:05.297385 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 12:05:05.297421 29777 sgd_solver.cpp:106] Iteration 187000, lr = 0.00415625
I0630 12:05:21.384862 29777 solver.cpp:290] Iteration 187100 (6.21618 iter/s, 16.087s/100 iter), loss = 0.916667
I0630 12:05:21.384884 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 12:05:21.384891 29777 sgd_solver.cpp:106] Iteration 187100, lr = 0.00415312
I0630 12:05:37.464721 29777 solver.cpp:290] Iteration 187200 (6.21914 iter/s, 16.0794s/100 iter), loss = 1.07143
I0630 12:05:37.464821 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 12:05:37.464833 29777 sgd_solver.cpp:106] Iteration 187200, lr = 0.00415
I0630 12:05:53.582921 29777 solver.cpp:290] Iteration 187300 (6.20438 iter/s, 16.1176s/100 iter), loss = 1.53571
I0630 12:05:53.583189 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 12:05:53.583341 29777 sgd_solver.cpp:106] Iteration 187300, lr = 0.00414687
I0630 12:06:09.802132 29777 solver.cpp:290] Iteration 187400 (6.16579 iter/s, 16.2185s/100 iter), loss = 1.85714
I0630 12:06:09.802232 29777 solver.cpp:309]     Train net output #0: loss = 1.71429 (* 1 = 1.71429 loss)
I0630 12:06:09.802240 29777 sgd_solver.cpp:106] Iteration 187400, lr = 0.00414375
I0630 12:06:25.795215 29777 solver.cpp:290] Iteration 187500 (6.25291 iter/s, 15.9925s/100 iter), loss = 1.4881
I0630 12:06:25.795240 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 12:06:25.795248 29777 sgd_solver.cpp:106] Iteration 187500, lr = 0.00414062
I0630 12:06:41.946360 29777 solver.cpp:290] Iteration 187600 (6.19169 iter/s, 16.1507s/100 iter), loss = 1.34524
I0630 12:06:41.946452 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 12:06:41.946463 29777 sgd_solver.cpp:106] Iteration 187600, lr = 0.0041375
I0630 12:06:58.024793 29777 solver.cpp:290] Iteration 187700 (6.21972 iter/s, 16.0779s/100 iter), loss = 1.7619
I0630 12:06:58.024835 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 12:06:58.024847 29777 sgd_solver.cpp:106] Iteration 187700, lr = 0.00413437
I0630 12:07:14.077853 29777 solver.cpp:290] Iteration 187800 (6.22953 iter/s, 16.0526s/100 iter), loss = 1.34524
I0630 12:07:14.077908 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 12:07:14.077919 29777 sgd_solver.cpp:106] Iteration 187800, lr = 0.00413125
I0630 12:07:30.197924 29777 solver.cpp:290] Iteration 187900 (6.20364 iter/s, 16.1196s/100 iter), loss = 1.72619
I0630 12:07:30.197948 29777 solver.cpp:309]     Train net output #0: loss = 2.07143 (* 1 = 2.07143 loss)
I0630 12:07:30.197957 29777 sgd_solver.cpp:106] Iteration 187900, lr = 0.00412812
I0630 12:07:46.122381 29777 solver.cpp:354] Sparsity after update:
I0630 12:07:46.123795 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:07:46.123802 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:07:46.123811 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:07:46.123816 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:07:46.123821 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:07:46.123826 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:07:46.123827 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:07:46.123829 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:07:46.123832 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:07:46.123836 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:07:46.123839 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:07:46.123843 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:07:46.123847 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:07:46.123942 29777 solver.cpp:471] Iteration 188000, Testing net (#0)
I0630 12:08:01.895462 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 12:08:35.380458 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54804
I0630 12:08:35.380575 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.787061
I0630 12:08:35.380585 29777 solver.cpp:544]     Test net output #2: loss = 1.61076 (* 1 = 1.61076 loss)
I0630 12:08:35.566038 29777 solver.cpp:290] Iteration 188000 (1.52984 iter/s, 65.3663s/100 iter), loss = 1.4881
I0630 12:08:35.566061 29777 solver.cpp:309]     Train net output #0: loss = 1.85714 (* 1 = 1.85714 loss)
I0630 12:08:35.566068 29777 sgd_solver.cpp:106] Iteration 188000, lr = 0.004125
I0630 12:08:50.958283 29777 solver.cpp:290] Iteration 188100 (6.49697 iter/s, 15.3918s/100 iter), loss = 0.976191
I0630 12:08:50.958336 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 12:08:50.958356 29777 sgd_solver.cpp:106] Iteration 188100, lr = 0.00412187
I0630 12:09:06.884492 29777 solver.cpp:290] Iteration 188200 (6.27915 iter/s, 15.9257s/100 iter), loss = 1.15476
I0630 12:09:06.884574 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 12:09:06.884585 29777 sgd_solver.cpp:106] Iteration 188200, lr = 0.00411875
I0630 12:09:22.954910 29777 solver.cpp:290] Iteration 188300 (6.22282 iter/s, 16.0699s/100 iter), loss = 1.40476
I0630 12:09:22.954941 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 12:09:22.954954 29777 sgd_solver.cpp:106] Iteration 188300, lr = 0.00411562
I0630 12:09:39.050225 29777 solver.cpp:290] Iteration 188400 (6.21317 iter/s, 16.0948s/100 iter), loss = 1.80952
I0630 12:09:39.050293 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 12:09:39.050302 29777 sgd_solver.cpp:106] Iteration 188400, lr = 0.0041125
I0630 12:09:55.048292 29777 solver.cpp:290] Iteration 188500 (6.25095 iter/s, 15.9976s/100 iter), loss = 1.39286
I0630 12:09:55.048315 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 12:09:55.048321 29777 sgd_solver.cpp:106] Iteration 188500, lr = 0.00410937
I0630 12:10:11.050294 29777 solver.cpp:290] Iteration 188600 (6.2494 iter/s, 16.0015s/100 iter), loss = 1.28571
I0630 12:10:11.050392 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 12:10:11.050400 29777 sgd_solver.cpp:106] Iteration 188600, lr = 0.00410625
I0630 12:10:27.143988 29777 solver.cpp:290] Iteration 188700 (6.21382 iter/s, 16.0932s/100 iter), loss = 1.17857
I0630 12:10:27.144013 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 12:10:27.144021 29777 sgd_solver.cpp:106] Iteration 188700, lr = 0.00410312
I0630 12:10:43.139900 29777 solver.cpp:290] Iteration 188800 (6.25178 iter/s, 15.9954s/100 iter), loss = 1.13095
I0630 12:10:43.139993 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 12:10:43.140003 29777 sgd_solver.cpp:106] Iteration 188800, lr = 0.0041
I0630 12:10:59.179679 29777 solver.cpp:290] Iteration 188900 (6.23471 iter/s, 16.0392s/100 iter), loss = 1.40476
I0630 12:10:59.179704 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 12:10:59.179710 29777 sgd_solver.cpp:106] Iteration 188900, lr = 0.00409687
I0630 12:11:15.135224 29777 solver.cpp:354] Sparsity after update:
I0630 12:11:15.156564 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:11:15.156584 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:11:15.156594 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:11:15.156596 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:11:15.156600 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:11:15.156603 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:11:15.156607 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:11:15.156611 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:11:15.156615 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:11:15.156620 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:11:15.156623 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:11:15.156627 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:11:15.156631 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:11:15.314579 29777 solver.cpp:290] Iteration 189000 (6.19793 iter/s, 16.1344s/100 iter), loss = 1.03571
I0630 12:11:15.314607 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 12:11:15.314616 29777 sgd_solver.cpp:106] Iteration 189000, lr = 0.00409375
I0630 12:11:31.384840 29777 solver.cpp:290] Iteration 189100 (6.22286 iter/s, 16.0698s/100 iter), loss = 1.54762
I0630 12:11:31.384863 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 12:11:31.384871 29777 sgd_solver.cpp:106] Iteration 189100, lr = 0.00409062
I0630 12:11:47.449165 29777 solver.cpp:290] Iteration 189200 (6.22515 iter/s, 16.0639s/100 iter), loss = 1.38095
I0630 12:11:47.449220 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 12:11:47.449230 29777 sgd_solver.cpp:106] Iteration 189200, lr = 0.0040875
I0630 12:12:03.372269 29777 solver.cpp:290] Iteration 189300 (6.28038 iter/s, 15.9226s/100 iter), loss = 0.714286
I0630 12:12:03.372297 29777 solver.cpp:309]     Train net output #0: loss = 0.47619 (* 1 = 0.47619 loss)
I0630 12:12:03.372306 29777 sgd_solver.cpp:106] Iteration 189300, lr = 0.00408437
I0630 12:12:19.361199 29777 solver.cpp:290] Iteration 189400 (6.25451 iter/s, 15.9885s/100 iter), loss = 1.07143
I0630 12:12:19.361273 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 12:12:19.361284 29777 sgd_solver.cpp:106] Iteration 189400, lr = 0.00408125
I0630 12:12:35.406184 29777 solver.cpp:290] Iteration 189500 (6.23268 iter/s, 16.0445s/100 iter), loss = 1.38095
I0630 12:12:35.406209 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 12:12:35.406219 29777 sgd_solver.cpp:106] Iteration 189500, lr = 0.00407812
I0630 12:12:51.427723 29777 solver.cpp:290] Iteration 189600 (6.24178 iter/s, 16.0211s/100 iter), loss = 2.0119
I0630 12:12:51.427834 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 12:12:51.427850 29777 sgd_solver.cpp:106] Iteration 189600, lr = 0.004075
I0630 12:13:07.424640 29777 solver.cpp:290] Iteration 189700 (6.25142 iter/s, 15.9964s/100 iter), loss = 0.952381
I0630 12:13:07.424662 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 12:13:07.424670 29777 sgd_solver.cpp:106] Iteration 189700, lr = 0.00407188
I0630 12:13:23.501313 29777 solver.cpp:290] Iteration 189800 (6.22037 iter/s, 16.0762s/100 iter), loss = 1.46429
I0630 12:13:23.501420 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 12:13:23.501430 29777 sgd_solver.cpp:106] Iteration 189800, lr = 0.00406875
I0630 12:13:39.506575 29777 solver.cpp:290] Iteration 189900 (6.24816 iter/s, 16.0047s/100 iter), loss = 1.17857
I0630 12:13:39.506599 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 12:13:39.506608 29777 sgd_solver.cpp:106] Iteration 189900, lr = 0.00406562
I0630 12:13:55.324442 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_190000.caffemodel
I0630 12:13:55.343865 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_190000.solverstate
I0630 12:13:55.353054 29777 solver.cpp:354] Sparsity after update:
I0630 12:13:55.354024 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:13:55.354033 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:13:55.354043 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:13:55.354048 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:13:55.354053 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:13:55.354058 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:13:55.354060 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:13:55.354064 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:13:55.354068 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:13:55.354073 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:13:55.354077 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:13:55.354081 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:13:55.354086 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:13:55.354187 29777 solver.cpp:471] Iteration 190000, Testing net (#0)
I0630 12:14:11.896518 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 12:14:44.519559 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5475
I0630 12:14:44.519611 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.786001
I0630 12:14:44.519618 29777 solver.cpp:544]     Test net output #2: loss = 1.61508 (* 1 = 1.61508 loss)
I0630 12:14:44.697819 29777 solver.cpp:290] Iteration 190000 (1.53399 iter/s, 65.1894s/100 iter), loss = 1.14286
I0630 12:14:44.697844 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 12:14:44.697849 29777 sgd_solver.cpp:106] Iteration 190000, lr = 0.0040625
I0630 12:14:59.980008 29777 solver.cpp:290] Iteration 190100 (6.54376 iter/s, 15.2817s/100 iter), loss = 1.07143
I0630 12:14:59.980036 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 12:14:59.980044 29777 sgd_solver.cpp:106] Iteration 190100, lr = 0.00405937
I0630 12:15:15.839448 29777 solver.cpp:290] Iteration 190200 (6.30558 iter/s, 15.859s/100 iter), loss = 1.46429
I0630 12:15:15.839540 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 12:15:15.839551 29777 sgd_solver.cpp:106] Iteration 190200, lr = 0.00405625
I0630 12:15:31.933228 29777 solver.cpp:290] Iteration 190300 (6.21379 iter/s, 16.0932s/100 iter), loss = 1.55952
I0630 12:15:31.933248 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 12:15:31.933256 29777 sgd_solver.cpp:106] Iteration 190300, lr = 0.00405312
I0630 12:15:47.850487 29777 solver.cpp:290] Iteration 190400 (6.28267 iter/s, 15.9168s/100 iter), loss = 1.02381
I0630 12:15:47.850591 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 12:15:47.850601 29777 sgd_solver.cpp:106] Iteration 190400, lr = 0.00405
I0630 12:16:03.860855 29777 solver.cpp:290] Iteration 190500 (6.24617 iter/s, 16.0098s/100 iter), loss = 1.33333
I0630 12:16:03.860877 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 12:16:03.860884 29777 sgd_solver.cpp:106] Iteration 190500, lr = 0.00404688
I0630 12:16:19.835305 29777 solver.cpp:290] Iteration 190600 (6.26018 iter/s, 15.974s/100 iter), loss = 1.47619
I0630 12:16:19.835400 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 12:16:19.835410 29777 sgd_solver.cpp:106] Iteration 190600, lr = 0.00404375
I0630 12:16:35.869060 29777 solver.cpp:290] Iteration 190700 (6.23705 iter/s, 16.0332s/100 iter), loss = 1.22619
I0630 12:16:35.869089 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 12:16:35.869098 29777 sgd_solver.cpp:106] Iteration 190700, lr = 0.00404062
I0630 12:16:51.937033 29777 solver.cpp:290] Iteration 190800 (6.22374 iter/s, 16.0675s/100 iter), loss = 1.09524
I0630 12:16:51.937117 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 12:16:51.937126 29777 sgd_solver.cpp:106] Iteration 190800, lr = 0.0040375
I0630 12:17:08.026510 29777 solver.cpp:290] Iteration 190900 (6.21545 iter/s, 16.0889s/100 iter), loss = 1.27381
I0630 12:17:08.026531 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 12:17:08.026538 29777 sgd_solver.cpp:106] Iteration 190900, lr = 0.00403437
I0630 12:17:23.875231 29777 solver.cpp:354] Sparsity after update:
I0630 12:17:23.895630 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:17:23.895648 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:17:23.895659 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:17:23.895663 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:17:23.895666 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:17:23.895670 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:17:23.895674 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:17:23.895678 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:17:23.895683 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:17:23.895685 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:17:23.895689 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:17:23.895694 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:17:23.895706 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:17:24.051235 29777 solver.cpp:290] Iteration 191000 (6.24054 iter/s, 16.0243s/100 iter), loss = 1.35714
I0630 12:17:24.051256 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 12:17:24.051265 29777 sgd_solver.cpp:106] Iteration 191000, lr = 0.00403125
I0630 12:17:39.971645 29777 solver.cpp:290] Iteration 191100 (6.28143 iter/s, 15.92s/100 iter), loss = 1.5119
I0630 12:17:39.971668 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 12:17:39.971674 29777 sgd_solver.cpp:106] Iteration 191100, lr = 0.00402812
I0630 12:17:55.976191 29777 solver.cpp:290] Iteration 191200 (6.24841 iter/s, 16.0041s/100 iter), loss = 1.22619
I0630 12:17:55.976270 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 12:17:55.976281 29777 sgd_solver.cpp:106] Iteration 191200, lr = 0.004025
I0630 12:18:11.978916 29777 solver.cpp:290] Iteration 191300 (6.24914 iter/s, 16.0022s/100 iter), loss = 1.38095
I0630 12:18:11.978940 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 12:18:11.978946 29777 sgd_solver.cpp:106] Iteration 191300, lr = 0.00402188
I0630 12:18:28.002478 29777 solver.cpp:290] Iteration 191400 (6.24099 iter/s, 16.0231s/100 iter), loss = 1.16667
I0630 12:18:28.002588 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 12:18:28.002599 29777 sgd_solver.cpp:106] Iteration 191400, lr = 0.00401875
I0630 12:18:44.048410 29777 solver.cpp:290] Iteration 191500 (6.23232 iter/s, 16.0454s/100 iter), loss = 1.09524
I0630 12:18:44.048436 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 12:18:44.048445 29777 sgd_solver.cpp:106] Iteration 191500, lr = 0.00401562
I0630 12:18:59.981504 29777 solver.cpp:290] Iteration 191600 (6.27643 iter/s, 15.9326s/100 iter), loss = 1.38095
I0630 12:18:59.981616 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 12:18:59.981624 29777 sgd_solver.cpp:106] Iteration 191600, lr = 0.0040125
I0630 12:19:16.067284 29777 solver.cpp:290] Iteration 191700 (6.21688 iter/s, 16.0852s/100 iter), loss = 1.44048
I0630 12:19:16.067307 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 12:19:16.067313 29777 sgd_solver.cpp:106] Iteration 191700, lr = 0.00400937
I0630 12:19:32.125784 29777 solver.cpp:290] Iteration 191800 (6.22741 iter/s, 16.058s/100 iter), loss = 1.57143
I0630 12:19:32.125869 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 12:19:32.125876 29777 sgd_solver.cpp:106] Iteration 191800, lr = 0.00400625
I0630 12:19:48.162432 29777 solver.cpp:290] Iteration 191900 (6.23592 iter/s, 16.0361s/100 iter), loss = 1.52381
I0630 12:19:48.162456 29777 solver.cpp:309]     Train net output #0: loss = 1.90476 (* 1 = 1.90476 loss)
I0630 12:19:48.162463 29777 sgd_solver.cpp:106] Iteration 191900, lr = 0.00400312
I0630 12:20:04.115231 29777 solver.cpp:354] Sparsity after update:
I0630 12:20:04.116827 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:20:04.116835 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:20:04.116843 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:20:04.116845 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:20:04.116847 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:20:04.116849 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:20:04.116852 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:20:04.116853 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:20:04.116855 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:20:04.116858 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:20:04.116859 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:20:04.116861 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:20:04.116863 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:20:04.116961 29777 solver.cpp:471] Iteration 192000, Testing net (#0)
I0630 12:20:20.433461 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 12:20:53.008106 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54806
I0630 12:20:53.008162 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.786541
I0630 12:20:53.008168 29777 solver.cpp:544]     Test net output #2: loss = 1.61678 (* 1 = 1.61678 loss)
I0630 12:20:53.200942 29777 solver.cpp:290] Iteration 192000 (1.53759 iter/s, 65.0367s/100 iter), loss = 1.28571
I0630 12:20:53.200964 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 12:20:53.200970 29777 sgd_solver.cpp:106] Iteration 192000, lr = 0.004
I0630 12:21:08.535524 29777 solver.cpp:290] Iteration 192100 (6.5214 iter/s, 15.3341s/100 iter), loss = 1.36905
I0630 12:21:08.535547 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 12:21:08.535553 29777 sgd_solver.cpp:106] Iteration 192100, lr = 0.00399688
I0630 12:21:24.564786 29777 solver.cpp:290] Iteration 192200 (6.23877 iter/s, 16.0288s/100 iter), loss = 1.39286
I0630 12:21:24.564915 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 12:21:24.564934 29777 sgd_solver.cpp:106] Iteration 192200, lr = 0.00399375
I0630 12:21:40.615010 29777 solver.cpp:290] Iteration 192300 (6.23066 iter/s, 16.0497s/100 iter), loss = 1.25
I0630 12:21:40.615032 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 12:21:40.615039 29777 sgd_solver.cpp:106] Iteration 192300, lr = 0.00399063
I0630 12:21:56.597358 29777 solver.cpp:290] Iteration 192400 (6.25708 iter/s, 15.9819s/100 iter), loss = 0.880953
I0630 12:21:56.597403 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 12:21:56.597414 29777 sgd_solver.cpp:106] Iteration 192400, lr = 0.0039875
I0630 12:22:12.550282 29777 solver.cpp:290] Iteration 192500 (6.26864 iter/s, 15.9524s/100 iter), loss = 1.40476
I0630 12:22:12.550334 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 12:22:12.550353 29777 sgd_solver.cpp:106] Iteration 192500, lr = 0.00398437
I0630 12:22:28.526331 29777 solver.cpp:290] Iteration 192600 (6.25956 iter/s, 15.9756s/100 iter), loss = 1.38095
I0630 12:22:28.526442 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 12:22:28.526502 29777 sgd_solver.cpp:106] Iteration 192600, lr = 0.00398125
I0630 12:22:44.515259 29777 solver.cpp:290] Iteration 192700 (6.25454 iter/s, 15.9884s/100 iter), loss = 1.28571
I0630 12:22:44.515280 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 12:22:44.515287 29777 sgd_solver.cpp:106] Iteration 192700, lr = 0.00397812
I0630 12:23:00.600080 29777 solver.cpp:290] Iteration 192800 (6.21722 iter/s, 16.0843s/100 iter), loss = 0.857143
I0630 12:23:00.600190 29777 solver.cpp:309]     Train net output #0: loss = 0.571429 (* 1 = 0.571429 loss)
I0630 12:23:00.600211 29777 sgd_solver.cpp:106] Iteration 192800, lr = 0.003975
I0630 12:23:16.653218 29777 solver.cpp:290] Iteration 192900 (6.22953 iter/s, 16.0526s/100 iter), loss = 1.57143
I0630 12:23:16.653272 29777 solver.cpp:309]     Train net output #0: loss = 1.92857 (* 1 = 1.92857 loss)
I0630 12:23:16.653285 29777 sgd_solver.cpp:106] Iteration 192900, lr = 0.00397187
I0630 12:23:32.707456 29777 solver.cpp:354] Sparsity after update:
I0630 12:23:32.727851 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:23:32.727869 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:23:32.727879 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:23:32.727882 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:23:32.727886 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:23:32.727890 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:23:32.727893 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:23:32.727897 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:23:32.727901 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:23:32.727905 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:23:32.727910 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:23:32.727912 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:23:32.727916 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:23:32.885673 29777 solver.cpp:290] Iteration 193000 (6.16069 iter/s, 16.232s/100 iter), loss = 1.05952
I0630 12:23:32.885699 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 12:23:32.885709 29777 sgd_solver.cpp:106] Iteration 193000, lr = 0.00396875
I0630 12:23:48.900730 29777 solver.cpp:290] Iteration 193100 (6.24431 iter/s, 16.0146s/100 iter), loss = 1.32143
I0630 12:23:48.900756 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 12:23:48.900765 29777 sgd_solver.cpp:106] Iteration 193100, lr = 0.00396563
I0630 12:24:04.874626 29777 solver.cpp:290] Iteration 193200 (6.2604 iter/s, 15.9734s/100 iter), loss = 1.05952
I0630 12:24:04.874734 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 12:24:04.874744 29777 sgd_solver.cpp:106] Iteration 193200, lr = 0.0039625
I0630 12:24:20.817512 29777 solver.cpp:290] Iteration 193300 (6.2726 iter/s, 15.9423s/100 iter), loss = 0.964286
I0630 12:24:20.817538 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 12:24:20.817546 29777 sgd_solver.cpp:106] Iteration 193300, lr = 0.00395937
I0630 12:24:36.946352 29777 solver.cpp:290] Iteration 193400 (6.20026 iter/s, 16.1284s/100 iter), loss = 1.40476
I0630 12:24:36.946458 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 12:24:36.946468 29777 sgd_solver.cpp:106] Iteration 193400, lr = 0.00395625
I0630 12:24:52.969713 29777 solver.cpp:290] Iteration 193500 (6.2411 iter/s, 16.0228s/100 iter), loss = 1.45238
I0630 12:24:52.969738 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 12:24:52.969748 29777 sgd_solver.cpp:106] Iteration 193500, lr = 0.00395312
I0630 12:25:09.087848 29777 solver.cpp:290] Iteration 193600 (6.20437 iter/s, 16.1177s/100 iter), loss = 1.36905
I0630 12:25:09.087895 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 12:25:09.087903 29777 sgd_solver.cpp:106] Iteration 193600, lr = 0.00395
I0630 12:25:25.120178 29777 solver.cpp:290] Iteration 193700 (6.23759 iter/s, 16.0318s/100 iter), loss = 1.34524
I0630 12:25:25.120201 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 12:25:25.120208 29777 sgd_solver.cpp:106] Iteration 193700, lr = 0.00394687
I0630 12:25:41.244523 29777 solver.cpp:290] Iteration 193800 (6.20198 iter/s, 16.1239s/100 iter), loss = 1.70238
I0630 12:25:41.244638 29777 solver.cpp:309]     Train net output #0: loss = 2 (* 1 = 2 loss)
I0630 12:25:41.244648 29777 sgd_solver.cpp:106] Iteration 193800, lr = 0.00394375
I0630 12:25:57.215366 29777 solver.cpp:290] Iteration 193900 (6.26163 iter/s, 15.9703s/100 iter), loss = 1.2619
I0630 12:25:57.215390 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 12:25:57.215396 29777 sgd_solver.cpp:106] Iteration 193900, lr = 0.00394063
I0630 12:26:13.072108 29777 solver.cpp:354] Sparsity after update:
I0630 12:26:13.074478 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:26:13.074501 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:26:13.074523 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:26:13.074534 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:26:13.074542 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:26:13.074561 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:26:13.074576 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:26:13.074590 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:26:13.074605 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:26:13.074620 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:26:13.074635 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:26:13.074645 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:26:13.074659 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:26:13.074949 29777 solver.cpp:471] Iteration 194000, Testing net (#0)
I0630 12:26:29.407784 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 12:27:09.814343 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54858
I0630 12:27:09.814450 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.786561
I0630 12:27:09.814460 29777 solver.cpp:544]     Test net output #2: loss = 1.61064 (* 1 = 1.61064 loss)
I0630 12:27:10.045747 29777 solver.cpp:290] Iteration 194000 (1.37309 iter/s, 72.8284s/100 iter), loss = 1.0119
I0630 12:27:10.045770 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 12:27:10.045778 29777 sgd_solver.cpp:106] Iteration 194000, lr = 0.0039375
I0630 12:27:25.396481 29777 solver.cpp:290] Iteration 194100 (6.51454 iter/s, 15.3503s/100 iter), loss = 1.42857
I0630 12:27:25.396503 29777 solver.cpp:309]     Train net output #0: loss = 1.85714 (* 1 = 1.85714 loss)
I0630 12:27:25.396510 29777 sgd_solver.cpp:106] Iteration 194100, lr = 0.00393438
I0630 12:27:41.307934 29777 solver.cpp:290] Iteration 194200 (6.28496 iter/s, 15.911s/100 iter), loss = 0.964286
I0630 12:27:41.308027 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 12:27:41.308037 29777 sgd_solver.cpp:106] Iteration 194200, lr = 0.00393125
I0630 12:27:57.378604 29777 solver.cpp:290] Iteration 194300 (6.22272 iter/s, 16.0701s/100 iter), loss = 1.35714
I0630 12:27:57.378630 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 12:27:57.378638 29777 sgd_solver.cpp:106] Iteration 194300, lr = 0.00392812
I0630 12:28:13.345146 29777 solver.cpp:290] Iteration 194400 (6.26328 iter/s, 15.9661s/100 iter), loss = 1.57143
I0630 12:28:13.345263 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 12:28:13.345273 29777 sgd_solver.cpp:106] Iteration 194400, lr = 0.003925
I0630 12:28:29.399219 29777 solver.cpp:290] Iteration 194500 (6.22917 iter/s, 16.0535s/100 iter), loss = 1.32143
I0630 12:28:29.399241 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 12:28:29.399248 29777 sgd_solver.cpp:106] Iteration 194500, lr = 0.00392187
I0630 12:28:45.279706 29777 solver.cpp:290] Iteration 194600 (6.29722 iter/s, 15.88s/100 iter), loss = 1.38095
I0630 12:28:45.279829 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 12:28:45.279839 29777 sgd_solver.cpp:106] Iteration 194600, lr = 0.00391875
I0630 12:29:01.258139 29777 solver.cpp:290] Iteration 194700 (6.25866 iter/s, 15.9779s/100 iter), loss = 1.39286
I0630 12:29:01.258165 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 12:29:01.258173 29777 sgd_solver.cpp:106] Iteration 194700, lr = 0.00391563
I0630 12:29:17.216478 29777 solver.cpp:290] Iteration 194800 (6.2665 iter/s, 15.9579s/100 iter), loss = 1.36905
I0630 12:29:17.216581 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 12:29:17.216591 29777 sgd_solver.cpp:106] Iteration 194800, lr = 0.0039125
I0630 12:29:33.161592 29777 solver.cpp:290] Iteration 194900 (6.27173 iter/s, 15.9446s/100 iter), loss = 1.16667
I0630 12:29:33.161619 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 12:29:33.161636 29777 sgd_solver.cpp:106] Iteration 194900, lr = 0.00390938
I0630 12:29:48.970405 29777 solver.cpp:354] Sparsity after update:
I0630 12:29:48.991094 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:29:48.991111 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:29:48.991118 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:29:48.991120 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:29:48.991123 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:29:48.991125 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:29:48.991127 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:29:48.991128 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:29:48.991130 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:29:48.991135 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:29:48.991137 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:29:48.991139 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:29:48.991142 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:29:49.150496 29777 solver.cpp:290] Iteration 195000 (6.25452 iter/s, 15.9884s/100 iter), loss = 1.32143
I0630 12:29:49.150522 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 12:29:49.150532 29777 sgd_solver.cpp:106] Iteration 195000, lr = 0.00390625
I0630 12:30:05.319962 29777 solver.cpp:290] Iteration 195100 (6.18468 iter/s, 16.169s/100 iter), loss = 1.35714
I0630 12:30:05.319988 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 12:30:05.319998 29777 sgd_solver.cpp:106] Iteration 195100, lr = 0.00390312
I0630 12:30:21.313674 29777 solver.cpp:290] Iteration 195200 (6.25264 iter/s, 15.9932s/100 iter), loss = 1.34524
I0630 12:30:21.313761 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 12:30:21.313774 29777 sgd_solver.cpp:106] Iteration 195200, lr = 0.0039
I0630 12:30:37.258419 29777 solver.cpp:290] Iteration 195300 (6.27186 iter/s, 15.9442s/100 iter), loss = 1.65476
I0630 12:30:37.258443 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 12:30:37.258450 29777 sgd_solver.cpp:106] Iteration 195300, lr = 0.00389687
I0630 12:30:53.246479 29777 solver.cpp:290] Iteration 195400 (6.25485 iter/s, 15.9876s/100 iter), loss = 1.58333
I0630 12:30:53.246563 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 12:30:53.246572 29777 sgd_solver.cpp:106] Iteration 195400, lr = 0.00389375
I0630 12:31:09.208382 29777 solver.cpp:290] Iteration 195500 (6.26512 iter/s, 15.9614s/100 iter), loss = 1
I0630 12:31:09.208458 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 12:31:09.208485 29777 sgd_solver.cpp:106] Iteration 195500, lr = 0.00389063
I0630 12:31:25.323843 29777 solver.cpp:290] Iteration 195600 (6.20542 iter/s, 16.1149s/100 iter), loss = 1.29762
I0630 12:31:25.323926 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 12:31:25.323935 29777 sgd_solver.cpp:106] Iteration 195600, lr = 0.0038875
I0630 12:31:41.381631 29777 solver.cpp:290] Iteration 195700 (6.22771 iter/s, 16.0573s/100 iter), loss = 1.11905
I0630 12:31:41.381667 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 12:31:41.381680 29777 sgd_solver.cpp:106] Iteration 195700, lr = 0.00388438
I0630 12:31:57.434921 29777 solver.cpp:290] Iteration 195800 (6.22944 iter/s, 16.0528s/100 iter), loss = 1.36905
I0630 12:31:57.434994 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 12:31:57.435004 29777 sgd_solver.cpp:106] Iteration 195800, lr = 0.00388125
I0630 12:32:13.406357 29777 solver.cpp:290] Iteration 195900 (6.26138 iter/s, 15.9709s/100 iter), loss = 1.58333
I0630 12:32:13.406383 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 12:32:13.406393 29777 sgd_solver.cpp:106] Iteration 195900, lr = 0.00387812
I0630 12:32:29.408041 29777 solver.cpp:354] Sparsity after update:
I0630 12:32:29.409471 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:32:29.409477 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:32:29.409485 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:32:29.409487 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:32:29.409489 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:32:29.409492 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:32:29.409493 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:32:29.409495 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:32:29.409497 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:32:29.409498 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:32:29.409500 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:32:29.409502 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:32:29.409505 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:32:29.409590 29777 solver.cpp:471] Iteration 196000, Testing net (#0)
I0630 12:32:45.797305 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 12:33:17.805824 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5495
I0630 12:33:17.805925 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.787361
I0630 12:33:17.805934 29777 solver.cpp:544]     Test net output #2: loss = 1.61566 (* 1 = 1.61566 loss)
I0630 12:33:17.991603 29777 solver.cpp:290] Iteration 196000 (1.54838 iter/s, 64.5835s/100 iter), loss = 1.30952
I0630 12:33:17.991626 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 12:33:17.991633 29777 sgd_solver.cpp:106] Iteration 196000, lr = 0.003875
I0630 12:33:33.333714 29777 solver.cpp:290] Iteration 196100 (6.51821 iter/s, 15.3416s/100 iter), loss = 1.25
I0630 12:33:33.333854 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 12:33:33.333890 29777 sgd_solver.cpp:106] Iteration 196100, lr = 0.00387187
I0630 12:33:49.397019 29777 solver.cpp:290] Iteration 196200 (6.22559 iter/s, 16.0627s/100 iter), loss = 1.07143
I0630 12:33:49.397121 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 12:33:49.397130 29777 sgd_solver.cpp:106] Iteration 196200, lr = 0.00386875
I0630 12:34:05.417917 29777 solver.cpp:290] Iteration 196300 (6.24206 iter/s, 16.0204s/100 iter), loss = 1.52381
I0630 12:34:05.417940 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 12:34:05.417959 29777 sgd_solver.cpp:106] Iteration 196300, lr = 0.00386563
I0630 12:34:21.365229 29777 solver.cpp:290] Iteration 196400 (6.27083 iter/s, 15.9469s/100 iter), loss = 1.29762
I0630 12:34:21.365283 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 12:34:21.365293 29777 sgd_solver.cpp:106] Iteration 196400, lr = 0.0038625
I0630 12:34:37.359230 29777 solver.cpp:290] Iteration 196500 (6.25253 iter/s, 15.9935s/100 iter), loss = 0.976191
I0630 12:34:37.359256 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 12:34:37.359264 29777 sgd_solver.cpp:106] Iteration 196500, lr = 0.00385938
I0630 12:34:53.441222 29777 solver.cpp:290] Iteration 196600 (6.21832 iter/s, 16.0815s/100 iter), loss = 1.39286
I0630 12:34:53.441301 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 12:34:53.441323 29777 sgd_solver.cpp:106] Iteration 196600, lr = 0.00385625
I0630 12:35:09.556448 29777 solver.cpp:290] Iteration 196700 (6.20551 iter/s, 16.1147s/100 iter), loss = 1.40476
I0630 12:35:09.556473 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 12:35:09.556483 29777 sgd_solver.cpp:106] Iteration 196700, lr = 0.00385312
I0630 12:35:25.476500 29777 solver.cpp:290] Iteration 196800 (6.28157 iter/s, 15.9196s/100 iter), loss = 1.27381
I0630 12:35:25.476639 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 12:35:25.476656 29777 sgd_solver.cpp:106] Iteration 196800, lr = 0.00385
I0630 12:35:41.485754 29777 solver.cpp:290] Iteration 196900 (6.24661 iter/s, 16.0087s/100 iter), loss = 1.33333
I0630 12:35:41.485780 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 12:35:41.485788 29777 sgd_solver.cpp:106] Iteration 196900, lr = 0.00384687
I0630 12:35:57.329812 29777 solver.cpp:354] Sparsity after update:
I0630 12:35:57.350358 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:35:57.350374 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:35:57.350386 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:35:57.350389 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:35:57.350394 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:35:57.350405 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:35:57.350410 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:35:57.350415 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:35:57.350420 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:35:57.350425 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:35:57.350430 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:35:57.350435 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:35:57.350437 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:35:57.520813 29777 solver.cpp:290] Iteration 197000 (6.23652 iter/s, 16.0346s/100 iter), loss = 1.27381
I0630 12:35:57.520835 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 12:35:57.520841 29777 sgd_solver.cpp:106] Iteration 197000, lr = 0.00384375
I0630 12:36:13.588266 29777 solver.cpp:290] Iteration 197100 (6.22394 iter/s, 16.067s/100 iter), loss = 1.21429
I0630 12:36:13.588290 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 12:36:13.588297 29777 sgd_solver.cpp:106] Iteration 197100, lr = 0.00384063
I0630 12:36:29.701117 29777 solver.cpp:290] Iteration 197200 (6.2064 iter/s, 16.1124s/100 iter), loss = 1.32143
I0630 12:36:29.701189 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 12:36:29.701200 29777 sgd_solver.cpp:106] Iteration 197200, lr = 0.0038375
I0630 12:36:45.755620 29777 solver.cpp:290] Iteration 197300 (6.22898 iter/s, 16.054s/100 iter), loss = 0.940476
I0630 12:36:45.755643 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 12:36:45.755650 29777 sgd_solver.cpp:106] Iteration 197300, lr = 0.00383437
I0630 12:37:01.783156 29777 solver.cpp:290] Iteration 197400 (6.23944 iter/s, 16.0271s/100 iter), loss = 1.38095
I0630 12:37:01.783233 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 12:37:01.783241 29777 sgd_solver.cpp:106] Iteration 197400, lr = 0.00383125
I0630 12:37:17.760340 29777 solver.cpp:290] Iteration 197500 (6.25913 iter/s, 15.9767s/100 iter), loss = 1.25
I0630 12:37:17.760365 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 12:37:17.760375 29777 sgd_solver.cpp:106] Iteration 197500, lr = 0.00382812
I0630 12:37:33.737946 29777 solver.cpp:290] Iteration 197600 (6.25894 iter/s, 15.9771s/100 iter), loss = 1.17857
I0630 12:37:33.738060 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 12:37:33.738072 29777 sgd_solver.cpp:106] Iteration 197600, lr = 0.003825
I0630 12:37:49.774976 29777 solver.cpp:290] Iteration 197700 (6.23578 iter/s, 16.0365s/100 iter), loss = 1.5
I0630 12:37:49.775002 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 12:37:49.775010 29777 sgd_solver.cpp:106] Iteration 197700, lr = 0.00382187
I0630 12:38:05.835855 29777 solver.cpp:290] Iteration 197800 (6.22649 iter/s, 16.0604s/100 iter), loss = 1.15476
I0630 12:38:05.835927 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 12:38:05.835935 29777 sgd_solver.cpp:106] Iteration 197800, lr = 0.00381875
I0630 12:38:21.867875 29777 solver.cpp:290] Iteration 197900 (6.23772 iter/s, 16.0315s/100 iter), loss = 1.14286
I0630 12:38:21.867898 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 12:38:21.867905 29777 sgd_solver.cpp:106] Iteration 197900, lr = 0.00381562
I0630 12:38:37.800308 29777 solver.cpp:354] Sparsity after update:
I0630 12:38:37.802464 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:38:37.802495 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:38:37.802520 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:38:37.802534 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:38:37.802546 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:38:37.802561 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:38:37.802573 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:38:37.802585 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:38:37.802597 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:38:37.802610 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:38:37.802623 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:38:37.802636 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:38:37.802649 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:38:37.802919 29777 solver.cpp:471] Iteration 198000, Testing net (#0)
I0630 12:38:54.711349 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 12:39:27.114504 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54844
I0630 12:39:27.114601 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.786641
I0630 12:39:27.114611 29777 solver.cpp:544]     Test net output #2: loss = 1.61028 (* 1 = 1.61028 loss)
I0630 12:39:27.301093 29777 solver.cpp:290] Iteration 198000 (1.52832 iter/s, 65.4314s/100 iter), loss = 1.58333
I0630 12:39:27.301116 29777 solver.cpp:309]     Train net output #0: loss = 1.71429 (* 1 = 1.71429 loss)
I0630 12:39:27.301123 29777 sgd_solver.cpp:106] Iteration 198000, lr = 0.0038125
I0630 12:39:42.717214 29777 solver.cpp:290] Iteration 198100 (6.4869 iter/s, 15.4157s/100 iter), loss = 1.39286
I0630 12:39:42.717236 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 12:39:42.717243 29777 sgd_solver.cpp:106] Iteration 198100, lr = 0.00380937
I0630 12:39:58.679603 29777 solver.cpp:290] Iteration 198200 (6.26491 iter/s, 15.9619s/100 iter), loss = 1.04762
I0630 12:39:58.679708 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 12:39:58.679718 29777 sgd_solver.cpp:106] Iteration 198200, lr = 0.00380625
I0630 12:40:14.704704 29777 solver.cpp:290] Iteration 198300 (6.24042 iter/s, 16.0246s/100 iter), loss = 0.797619
I0630 12:40:14.704726 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 12:40:14.704733 29777 sgd_solver.cpp:106] Iteration 198300, lr = 0.00380312
I0630 12:40:30.711949 29777 solver.cpp:290] Iteration 198400 (6.24735 iter/s, 16.0068s/100 iter), loss = 1.0119
I0630 12:40:30.712049 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 12:40:30.712061 29777 sgd_solver.cpp:106] Iteration 198400, lr = 0.0038
I0630 12:40:46.680680 29777 solver.cpp:290] Iteration 198500 (6.26245 iter/s, 15.9682s/100 iter), loss = 1.72619
I0630 12:40:46.680707 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 12:40:46.680722 29777 sgd_solver.cpp:106] Iteration 198500, lr = 0.00379687
I0630 12:41:02.633126 29777 solver.cpp:290] Iteration 198600 (6.26881 iter/s, 15.952s/100 iter), loss = 1.41667
I0630 12:41:02.633230 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 12:41:02.633240 29777 sgd_solver.cpp:106] Iteration 198600, lr = 0.00379375
I0630 12:41:18.792882 29777 solver.cpp:290] Iteration 198700 (6.18842 iter/s, 16.1592s/100 iter), loss = 1.36905
I0630 12:41:18.792906 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 12:41:18.792913 29777 sgd_solver.cpp:106] Iteration 198700, lr = 0.00379062
I0630 12:41:34.788806 29777 solver.cpp:290] Iteration 198800 (6.25177 iter/s, 15.9955s/100 iter), loss = 1.58333
I0630 12:41:34.788900 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 12:41:34.788911 29777 sgd_solver.cpp:106] Iteration 198800, lr = 0.0037875
I0630 12:41:50.736560 29777 solver.cpp:290] Iteration 198900 (6.27068 iter/s, 15.9472s/100 iter), loss = 1
I0630 12:41:50.736588 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 12:41:50.736596 29777 sgd_solver.cpp:106] Iteration 198900, lr = 0.00378438
I0630 12:42:06.634716 29777 solver.cpp:354] Sparsity after update:
I0630 12:42:06.655521 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:42:06.655539 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:42:06.655550 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:42:06.655553 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:42:06.655556 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:42:06.655560 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:42:06.655563 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:42:06.655566 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:42:06.655570 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:42:06.655572 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:42:06.655576 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:42:06.655580 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:42:06.655582 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:42:06.818323 29777 solver.cpp:290] Iteration 199000 (6.2184 iter/s, 16.0813s/100 iter), loss = 1.34524
I0630 12:42:06.818347 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 12:42:06.818354 29777 sgd_solver.cpp:106] Iteration 199000, lr = 0.00378125
I0630 12:42:22.872582 29777 solver.cpp:290] Iteration 199100 (6.22906 iter/s, 16.0538s/100 iter), loss = 1.25
I0630 12:42:22.872607 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 12:42:22.872613 29777 sgd_solver.cpp:106] Iteration 199100, lr = 0.00377812
I0630 12:42:38.882388 29777 solver.cpp:290] Iteration 199200 (6.24635 iter/s, 16.0093s/100 iter), loss = 1.15476
I0630 12:42:38.882472 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 12:42:38.882478 29777 sgd_solver.cpp:106] Iteration 199200, lr = 0.003775
I0630 12:42:54.898813 29777 solver.cpp:290] Iteration 199300 (6.24379 iter/s, 16.0159s/100 iter), loss = 1.09524
I0630 12:42:54.898845 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 12:42:54.898854 29777 sgd_solver.cpp:106] Iteration 199300, lr = 0.00377187
I0630 12:43:10.863035 29777 solver.cpp:290] Iteration 199400 (6.26419 iter/s, 15.9638s/100 iter), loss = 1.09524
I0630 12:43:10.863153 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 12:43:10.863163 29777 sgd_solver.cpp:106] Iteration 199400, lr = 0.00376875
I0630 12:43:26.880861 29777 solver.cpp:290] Iteration 199500 (6.24326 iter/s, 16.0173s/100 iter), loss = 1.15476
I0630 12:43:26.880884 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 12:43:26.880892 29777 sgd_solver.cpp:106] Iteration 199500, lr = 0.00376562
I0630 12:43:42.947981 29777 solver.cpp:290] Iteration 199600 (6.22407 iter/s, 16.0667s/100 iter), loss = 1.17857
I0630 12:43:42.948091 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 12:43:42.948101 29777 sgd_solver.cpp:106] Iteration 199600, lr = 0.0037625
I0630 12:43:58.942972 29777 solver.cpp:290] Iteration 199700 (6.25217 iter/s, 15.9944s/100 iter), loss = 1.11905
I0630 12:43:58.942996 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 12:43:58.943002 29777 sgd_solver.cpp:106] Iteration 199700, lr = 0.00375938
I0630 12:44:15.005084 29777 solver.cpp:290] Iteration 199800 (6.22601 iter/s, 16.0616s/100 iter), loss = 1.42857
I0630 12:44:15.005179 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 12:44:15.005190 29777 sgd_solver.cpp:106] Iteration 199800, lr = 0.00375625
I0630 12:44:31.026327 29777 solver.cpp:290] Iteration 199900 (6.24192 iter/s, 16.0207s/100 iter), loss = 1.15476
I0630 12:44:31.026352 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 12:44:31.026368 29777 sgd_solver.cpp:106] Iteration 199900, lr = 0.00375312
I0630 12:44:46.826830 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_200000.caffemodel
I0630 12:44:46.849414 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_200000.solverstate
I0630 12:44:46.858203 29777 solver.cpp:354] Sparsity after update:
I0630 12:44:46.859171 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:44:46.859180 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:44:46.859189 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:44:46.859190 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:44:46.859192 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:44:46.859194 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:44:46.859196 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:44:46.859199 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:44:46.859200 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:44:46.859202 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:44:46.859205 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:44:46.859206 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:44:46.859208 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:44:46.859303 29777 solver.cpp:471] Iteration 200000, Testing net (#0)
I0630 12:45:03.962929 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 12:45:36.472712 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55044
I0630 12:45:36.472761 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.787201
I0630 12:45:36.472767 29777 solver.cpp:544]     Test net output #2: loss = 1.60748 (* 1 = 1.60748 loss)
I0630 12:45:36.646565 29777 solver.cpp:290] Iteration 200000 (1.52396 iter/s, 65.6184s/100 iter), loss = 1.4881
I0630 12:45:36.646590 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 12:45:36.646600 29777 sgd_solver.cpp:106] Iteration 200000, lr = 0.00375
I0630 12:45:52.032799 29777 solver.cpp:290] Iteration 200100 (6.49951 iter/s, 15.3858s/100 iter), loss = 1.45238
I0630 12:45:52.032825 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 12:45:52.032835 29777 sgd_solver.cpp:106] Iteration 200100, lr = 0.00374687
I0630 12:46:08.077330 29777 solver.cpp:290] Iteration 200200 (6.23283 iter/s, 16.0441s/100 iter), loss = 1.40476
I0630 12:46:08.077446 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 12:46:08.077456 29777 sgd_solver.cpp:106] Iteration 200200, lr = 0.00374375
I0630 12:46:24.059507 29777 solver.cpp:290] Iteration 200300 (6.25719 iter/s, 15.9816s/100 iter), loss = 1.11905
I0630 12:46:24.059530 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 12:46:24.059536 29777 sgd_solver.cpp:106] Iteration 200300, lr = 0.00374062
I0630 12:46:40.026058 29777 solver.cpp:290] Iteration 200400 (6.26327 iter/s, 15.9661s/100 iter), loss = 1.41667
I0630 12:46:40.026155 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 12:46:40.026165 29777 sgd_solver.cpp:106] Iteration 200400, lr = 0.0037375
I0630 12:46:56.057092 29777 solver.cpp:290] Iteration 200500 (6.23811 iter/s, 16.0305s/100 iter), loss = 1.54762
I0630 12:46:56.057117 29777 solver.cpp:309]     Train net output #0: loss = 2.09524 (* 1 = 2.09524 loss)
I0630 12:46:56.057126 29777 sgd_solver.cpp:106] Iteration 200500, lr = 0.00373438
I0630 12:47:12.123257 29777 solver.cpp:290] Iteration 200600 (6.22444 iter/s, 16.0657s/100 iter), loss = 1.20238
I0630 12:47:12.123337 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 12:47:12.123348 29777 sgd_solver.cpp:106] Iteration 200600, lr = 0.00373125
I0630 12:47:28.214594 29777 solver.cpp:290] Iteration 200700 (6.21473 iter/s, 16.0908s/100 iter), loss = 1.10714
I0630 12:47:28.214618 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 12:47:28.214625 29777 sgd_solver.cpp:106] Iteration 200700, lr = 0.00372813
I0630 12:47:44.292896 29777 solver.cpp:290] Iteration 200800 (6.21974 iter/s, 16.0778s/100 iter), loss = 0.976191
I0630 12:47:44.299324 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 12:47:44.299355 29777 sgd_solver.cpp:106] Iteration 200800, lr = 0.003725
I0630 12:48:00.407301 29777 solver.cpp:290] Iteration 200900 (6.20827 iter/s, 16.1075s/100 iter), loss = 1.45238
I0630 12:48:00.407325 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 12:48:00.407332 29777 sgd_solver.cpp:106] Iteration 200900, lr = 0.00372187
I0630 12:48:16.304502 29777 solver.cpp:354] Sparsity after update:
I0630 12:48:16.326386 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:48:16.326438 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:48:16.326468 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:48:16.326478 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:48:16.326486 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:48:16.326494 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:48:16.326503 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:48:16.326511 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:48:16.326519 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:48:16.326527 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:48:16.326535 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:48:16.326544 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:48:16.326550 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:48:16.494441 29777 solver.cpp:290] Iteration 201000 (6.21633 iter/s, 16.0867s/100 iter), loss = 1.35714
I0630 12:48:16.494480 29777 solver.cpp:309]     Train net output #0: loss = 1.71429 (* 1 = 1.71429 loss)
I0630 12:48:16.494499 29777 sgd_solver.cpp:106] Iteration 201000, lr = 0.00371875
I0630 12:48:32.790930 29777 solver.cpp:290] Iteration 201100 (6.13647 iter/s, 16.296s/100 iter), loss = 1.22619
I0630 12:48:32.790954 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 12:48:32.790961 29777 sgd_solver.cpp:106] Iteration 201100, lr = 0.00371562
I0630 12:48:48.855867 29777 solver.cpp:290] Iteration 201200 (6.22492 iter/s, 16.0645s/100 iter), loss = 1.03571
I0630 12:48:48.855978 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 12:48:48.855985 29777 sgd_solver.cpp:106] Iteration 201200, lr = 0.0037125
I0630 12:49:04.953423 29777 solver.cpp:290] Iteration 201300 (6.21233 iter/s, 16.097s/100 iter), loss = 1.33333
I0630 12:49:04.953445 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 12:49:04.953452 29777 sgd_solver.cpp:106] Iteration 201300, lr = 0.00370938
I0630 12:49:21.048115 29777 solver.cpp:290] Iteration 201400 (6.21341 iter/s, 16.0942s/100 iter), loss = 1.41667
I0630 12:49:21.048188 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 12:49:21.048195 29777 sgd_solver.cpp:106] Iteration 201400, lr = 0.00370625
I0630 12:49:37.179419 29777 solver.cpp:290] Iteration 201500 (6.19932 iter/s, 16.1308s/100 iter), loss = 1.05952
I0630 12:49:37.179446 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 12:49:37.179455 29777 sgd_solver.cpp:106] Iteration 201500, lr = 0.00370313
I0630 12:49:53.223063 29777 solver.cpp:290] Iteration 201600 (6.23318 iter/s, 16.0432s/100 iter), loss = 1.11905
I0630 12:49:53.223168 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 12:49:53.223177 29777 sgd_solver.cpp:106] Iteration 201600, lr = 0.0037
I0630 12:50:09.339459 29777 solver.cpp:290] Iteration 201700 (6.20507 iter/s, 16.1159s/100 iter), loss = 1.5
I0630 12:50:09.339484 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 12:50:09.339493 29777 sgd_solver.cpp:106] Iteration 201700, lr = 0.00369687
I0630 12:50:25.336817 29777 solver.cpp:290] Iteration 201800 (6.25121 iter/s, 15.9969s/100 iter), loss = 1.45238
I0630 12:50:25.336921 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 12:50:25.336931 29777 sgd_solver.cpp:106] Iteration 201800, lr = 0.00369375
I0630 12:50:41.482467 29777 solver.cpp:290] Iteration 201900 (6.19383 iter/s, 16.1451s/100 iter), loss = 1.2619
I0630 12:50:41.482491 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 12:50:41.482497 29777 sgd_solver.cpp:106] Iteration 201900, lr = 0.00369062
I0630 12:50:57.431658 29777 solver.cpp:354] Sparsity after update:
I0630 12:50:57.433104 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:50:57.433111 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:50:57.433120 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:50:57.433121 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:50:57.433125 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:50:57.433126 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:50:57.433128 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:50:57.433131 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:50:57.433133 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:50:57.433135 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:50:57.433138 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:50:57.433140 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:50:57.433142 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:50:57.433234 29777 solver.cpp:471] Iteration 202000, Testing net (#0)
I0630 12:51:15.808956 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 12:51:48.107079 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54918
I0630 12:51:48.107141 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.785421
I0630 12:51:48.107146 29777 solver.cpp:544]     Test net output #2: loss = 1.61502 (* 1 = 1.61502 loss)
I0630 12:51:48.303637 29777 solver.cpp:290] Iteration 202000 (1.49657 iter/s, 66.8193s/100 iter), loss = 0.928572
I0630 12:51:48.303666 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 12:51:48.303673 29777 sgd_solver.cpp:106] Iteration 202000, lr = 0.0036875
I0630 12:52:04.198745 29777 solver.cpp:290] Iteration 202100 (6.29143 iter/s, 15.8946s/100 iter), loss = 1.58333
I0630 12:52:04.198772 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 12:52:04.198782 29777 sgd_solver.cpp:106] Iteration 202100, lr = 0.00368438
I0630 12:52:20.880825 29777 solver.cpp:290] Iteration 202200 (5.99463 iter/s, 16.6816s/100 iter), loss = 1.0119
I0630 12:52:20.880901 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 12:52:20.880911 29777 sgd_solver.cpp:106] Iteration 202200, lr = 0.00368125
I0630 12:52:36.959077 29777 solver.cpp:290] Iteration 202300 (6.21978 iter/s, 16.0777s/100 iter), loss = 1.2619
I0630 12:52:36.959125 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 12:52:36.959146 29777 sgd_solver.cpp:106] Iteration 202300, lr = 0.00367813
I0630 12:52:53.059648 29777 solver.cpp:290] Iteration 202400 (6.21115 iter/s, 16.1001s/100 iter), loss = 1.35714
I0630 12:52:53.059741 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 12:52:53.059752 29777 sgd_solver.cpp:106] Iteration 202400, lr = 0.003675
I0630 12:53:09.156069 29777 solver.cpp:290] Iteration 202500 (6.21277 iter/s, 16.0959s/100 iter), loss = 1.45238
I0630 12:53:09.156095 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 12:53:09.156103 29777 sgd_solver.cpp:106] Iteration 202500, lr = 0.00367187
I0630 12:53:25.272895 29777 solver.cpp:290] Iteration 202600 (6.20487 iter/s, 16.1164s/100 iter), loss = 1.66667
I0630 12:53:25.272976 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 12:53:25.272984 29777 sgd_solver.cpp:106] Iteration 202600, lr = 0.00366875
I0630 12:53:41.306145 29777 solver.cpp:290] Iteration 202700 (6.23724 iter/s, 16.0327s/100 iter), loss = 1.47619
I0630 12:53:41.306185 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 12:53:41.306205 29777 sgd_solver.cpp:106] Iteration 202700, lr = 0.00366562
I0630 12:53:57.539311 29777 solver.cpp:290] Iteration 202800 (6.16041 iter/s, 16.2327s/100 iter), loss = 1.11905
I0630 12:53:57.539392 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 12:53:57.539404 29777 sgd_solver.cpp:106] Iteration 202800, lr = 0.0036625
I0630 12:54:13.677984 29777 solver.cpp:290] Iteration 202900 (6.1965 iter/s, 16.1382s/100 iter), loss = 1.36905
I0630 12:54:13.678010 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 12:54:13.678020 29777 sgd_solver.cpp:106] Iteration 202900, lr = 0.00365937
I0630 12:54:29.694394 29777 solver.cpp:354] Sparsity after update:
I0630 12:54:29.718364 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:54:29.718405 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:54:29.718422 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:54:29.718431 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:54:29.718441 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:54:29.718449 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:54:29.718458 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:54:29.718467 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:54:29.718477 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:54:29.718484 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:54:29.718493 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:54:29.718502 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:54:29.718511 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:54:29.874300 29777 solver.cpp:290] Iteration 203000 (6.17442 iter/s, 16.1958s/100 iter), loss = 1.2619
I0630 12:54:29.874325 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 12:54:29.874331 29777 sgd_solver.cpp:106] Iteration 203000, lr = 0.00365625
I0630 12:54:45.848338 29777 solver.cpp:290] Iteration 203100 (6.26034 iter/s, 15.9736s/100 iter), loss = 1.30952
I0630 12:54:45.848364 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 12:54:45.848373 29777 sgd_solver.cpp:106] Iteration 203100, lr = 0.00365313
I0630 12:55:01.896085 29777 solver.cpp:290] Iteration 203200 (6.23158 iter/s, 16.0473s/100 iter), loss = 1.45238
I0630 12:55:01.896165 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 12:55:01.896173 29777 sgd_solver.cpp:106] Iteration 203200, lr = 0.00365
I0630 12:55:17.990584 29777 solver.cpp:290] Iteration 203300 (6.2135 iter/s, 16.094s/100 iter), loss = 1.28571
I0630 12:55:17.990607 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 12:55:17.990615 29777 sgd_solver.cpp:106] Iteration 203300, lr = 0.00364688
I0630 12:55:34.025266 29777 solver.cpp:290] Iteration 203400 (6.23666 iter/s, 16.0342s/100 iter), loss = 1.10714
I0630 12:55:34.025317 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 12:55:34.025326 29777 sgd_solver.cpp:106] Iteration 203400, lr = 0.00364375
I0630 12:55:49.954334 29777 solver.cpp:290] Iteration 203500 (6.27802 iter/s, 15.9286s/100 iter), loss = 1.44048
I0630 12:55:49.954355 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 12:55:49.954362 29777 sgd_solver.cpp:106] Iteration 203500, lr = 0.00364062
I0630 12:56:05.986243 29777 solver.cpp:290] Iteration 203600 (6.23774 iter/s, 16.0314s/100 iter), loss = 1.54762
I0630 12:56:05.986352 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 12:56:05.986366 29777 sgd_solver.cpp:106] Iteration 203600, lr = 0.0036375
I0630 12:56:22.160385 29777 solver.cpp:290] Iteration 203700 (6.18292 iter/s, 16.1736s/100 iter), loss = 1.69048
I0630 12:56:22.160408 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 12:56:22.160415 29777 sgd_solver.cpp:106] Iteration 203700, lr = 0.00363437
I0630 12:56:38.197368 29777 solver.cpp:290] Iteration 203800 (6.23577 iter/s, 16.0365s/100 iter), loss = 1.20238
I0630 12:56:38.197475 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 12:56:38.197485 29777 sgd_solver.cpp:106] Iteration 203800, lr = 0.00363125
I0630 12:56:54.134104 29777 solver.cpp:290] Iteration 203900 (6.27502 iter/s, 15.9362s/100 iter), loss = 1.40476
I0630 12:56:54.134129 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 12:56:54.134135 29777 sgd_solver.cpp:106] Iteration 203900, lr = 0.00362813
I0630 12:57:10.030920 29777 solver.cpp:354] Sparsity after update:
I0630 12:57:10.032337 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 12:57:10.032346 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 12:57:10.032352 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 12:57:10.032354 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 12:57:10.032356 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 12:57:10.032359 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 12:57:10.032361 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 12:57:10.032362 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 12:57:10.032364 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 12:57:10.032366 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 12:57:10.032368 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 12:57:10.032371 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 12:57:10.032372 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 12:57:10.032459 29777 solver.cpp:471] Iteration 204000, Testing net (#0)
I0630 12:57:27.345805 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 12:57:58.779801 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54924
I0630 12:57:58.779932 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.786761
I0630 12:57:58.779942 29777 solver.cpp:544]     Test net output #2: loss = 1.61164 (* 1 = 1.61164 loss)
I0630 12:57:58.958827 29777 solver.cpp:290] Iteration 204000 (1.54266 iter/s, 64.823s/100 iter), loss = 1.30952
I0630 12:57:58.958853 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 12:57:58.958860 29777 sgd_solver.cpp:106] Iteration 204000, lr = 0.003625
I0630 12:58:14.348562 29777 solver.cpp:290] Iteration 204100 (6.49803 iter/s, 15.3893s/100 iter), loss = 1.58333
I0630 12:58:14.348584 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 12:58:14.348592 29777 sgd_solver.cpp:106] Iteration 204100, lr = 0.00362188
I0630 12:58:30.333621 29777 solver.cpp:290] Iteration 204200 (6.25602 iter/s, 15.9846s/100 iter), loss = 1.45238
I0630 12:58:30.333716 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 12:58:30.333726 29777 sgd_solver.cpp:106] Iteration 204200, lr = 0.00361875
I0630 12:58:46.318222 29777 solver.cpp:290] Iteration 204300 (6.25623 iter/s, 15.9841s/100 iter), loss = 1.55952
I0630 12:58:46.318248 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 12:58:46.318256 29777 sgd_solver.cpp:106] Iteration 204300, lr = 0.00361562
I0630 12:59:02.322253 29777 solver.cpp:290] Iteration 204400 (6.24861 iter/s, 16.0036s/100 iter), loss = 0.988095
I0630 12:59:02.322382 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 12:59:02.322420 29777 sgd_solver.cpp:106] Iteration 204400, lr = 0.0036125
I0630 12:59:18.522347 29777 solver.cpp:290] Iteration 204500 (6.17302 iter/s, 16.1995s/100 iter), loss = 1.75
I0630 12:59:18.522372 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 12:59:18.522379 29777 sgd_solver.cpp:106] Iteration 204500, lr = 0.00360937
I0630 12:59:34.562994 29777 solver.cpp:290] Iteration 204600 (6.23434 iter/s, 16.0402s/100 iter), loss = 1.80952
I0630 12:59:34.563102 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 12:59:34.563117 29777 sgd_solver.cpp:106] Iteration 204600, lr = 0.00360625
I0630 12:59:50.592280 29777 solver.cpp:290] Iteration 204700 (6.23879 iter/s, 16.0287s/100 iter), loss = 1.19048
I0630 12:59:50.592306 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 12:59:50.592315 29777 sgd_solver.cpp:106] Iteration 204700, lr = 0.00360313
I0630 13:00:06.597162 29777 solver.cpp:290] Iteration 204800 (6.24827 iter/s, 16.0044s/100 iter), loss = 1
I0630 13:00:06.597234 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 13:00:06.597241 29777 sgd_solver.cpp:106] Iteration 204800, lr = 0.0036
I0630 13:00:22.686693 29777 solver.cpp:290] Iteration 204900 (6.21542 iter/s, 16.089s/100 iter), loss = 1.46429
I0630 13:00:22.686720 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 13:00:22.686730 29777 sgd_solver.cpp:106] Iteration 204900, lr = 0.00359687
I0630 13:00:38.507714 29777 solver.cpp:354] Sparsity after update:
I0630 13:00:38.528453 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:00:38.528467 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:00:38.528475 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:00:38.528477 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:00:38.528479 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:00:38.528481 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:00:38.528483 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:00:38.528486 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:00:38.528488 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:00:38.528491 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:00:38.528499 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:00:38.528501 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:00:38.528503 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:00:38.682294 29777 solver.cpp:290] Iteration 205000 (6.2519 iter/s, 15.9951s/100 iter), loss = 1.22619
I0630 13:00:38.682318 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 13:00:38.682327 29777 sgd_solver.cpp:106] Iteration 205000, lr = 0.00359375
I0630 13:00:54.763983 29777 solver.cpp:290] Iteration 205100 (6.21843 iter/s, 16.0812s/100 iter), loss = 1.14286
I0630 13:00:54.764008 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 13:00:54.764016 29777 sgd_solver.cpp:106] Iteration 205100, lr = 0.00359062
I0630 13:01:10.873378 29777 solver.cpp:290] Iteration 205200 (6.20774 iter/s, 16.1089s/100 iter), loss = 1.13095
I0630 13:01:10.873702 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 13:01:10.873711 29777 sgd_solver.cpp:106] Iteration 205200, lr = 0.0035875
I0630 13:01:26.945050 29777 solver.cpp:290] Iteration 205300 (6.22242 iter/s, 16.0709s/100 iter), loss = 1.32143
I0630 13:01:26.945077 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 13:01:26.945086 29777 sgd_solver.cpp:106] Iteration 205300, lr = 0.00358437
I0630 13:01:42.971186 29777 solver.cpp:290] Iteration 205400 (6.23999 iter/s, 16.0257s/100 iter), loss = 1.40476
I0630 13:01:42.971284 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 13:01:42.971295 29777 sgd_solver.cpp:106] Iteration 205400, lr = 0.00358125
I0630 13:01:59.003520 29777 solver.cpp:290] Iteration 205500 (6.2376 iter/s, 16.0318s/100 iter), loss = 1.25
I0630 13:01:59.003545 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 13:01:59.003554 29777 sgd_solver.cpp:106] Iteration 205500, lr = 0.00357813
I0630 13:02:15.048928 29777 solver.cpp:290] Iteration 205600 (6.23249 iter/s, 16.0449s/100 iter), loss = 1.19048
I0630 13:02:15.049010 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 13:02:15.049021 29777 sgd_solver.cpp:106] Iteration 205600, lr = 0.003575
I0630 13:02:31.152962 29777 solver.cpp:290] Iteration 205700 (6.20983 iter/s, 16.1035s/100 iter), loss = 1.45238
I0630 13:02:31.152984 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 13:02:31.152992 29777 sgd_solver.cpp:106] Iteration 205700, lr = 0.00357187
I0630 13:02:47.189075 29777 solver.cpp:290] Iteration 205800 (6.2361 iter/s, 16.0357s/100 iter), loss = 1.47619
I0630 13:02:47.189180 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 13:02:47.189190 29777 sgd_solver.cpp:106] Iteration 205800, lr = 0.00356875
I0630 13:03:03.162910 29777 solver.cpp:290] Iteration 205900 (6.26045 iter/s, 15.9733s/100 iter), loss = 1.63095
I0630 13:03:03.162935 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 13:03:03.162945 29777 sgd_solver.cpp:106] Iteration 205900, lr = 0.00356562
I0630 13:03:19.063956 29777 solver.cpp:354] Sparsity after update:
I0630 13:03:19.066224 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:03:19.066232 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:03:19.066239 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:03:19.066241 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:03:19.066244 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:03:19.066246 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:03:19.066248 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:03:19.066251 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:03:19.066251 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:03:19.066253 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:03:19.066256 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:03:19.066257 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:03:19.066259 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:03:19.066345 29777 solver.cpp:471] Iteration 206000, Testing net (#0)
I0630 13:03:36.677826 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 13:04:08.917502 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.551099
I0630 13:04:08.917631 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.787441
I0630 13:04:08.917641 29777 solver.cpp:544]     Test net output #2: loss = 1.61018 (* 1 = 1.61018 loss)
I0630 13:04:09.086093 29777 solver.cpp:290] Iteration 206000 (1.51696 iter/s, 65.9214s/100 iter), loss = 2.09524
I0630 13:04:09.086117 29777 solver.cpp:309]     Train net output #0: loss = 1.88095 (* 1 = 1.88095 loss)
I0630 13:04:09.086124 29777 sgd_solver.cpp:106] Iteration 206000, lr = 0.0035625
I0630 13:04:24.531334 29777 solver.cpp:290] Iteration 206100 (6.47468 iter/s, 15.4448s/100 iter), loss = 1.27381
I0630 13:04:24.531379 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 13:04:24.531410 29777 sgd_solver.cpp:106] Iteration 206100, lr = 0.00355937
I0630 13:04:40.499562 29777 solver.cpp:290] Iteration 206200 (6.26262 iter/s, 15.9677s/100 iter), loss = 0.952381
I0630 13:04:40.499629 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 13:04:40.499641 29777 sgd_solver.cpp:106] Iteration 206200, lr = 0.00355625
I0630 13:04:56.632375 29777 solver.cpp:290] Iteration 206300 (6.19874 iter/s, 16.1323s/100 iter), loss = 1.7381
I0630 13:04:56.632401 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 13:04:56.632411 29777 sgd_solver.cpp:106] Iteration 206300, lr = 0.00355313
I0630 13:05:12.628417 29777 solver.cpp:290] Iteration 206400 (6.25173 iter/s, 15.9956s/100 iter), loss = 1.16667
I0630 13:05:12.628463 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 13:05:12.628473 29777 sgd_solver.cpp:106] Iteration 206400, lr = 0.00355
I0630 13:05:28.660279 29777 solver.cpp:290] Iteration 206500 (6.23777 iter/s, 16.0314s/100 iter), loss = 1.34524
I0630 13:05:28.660306 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 13:05:28.660323 29777 sgd_solver.cpp:106] Iteration 206500, lr = 0.00354687
I0630 13:05:44.805356 29777 solver.cpp:290] Iteration 206600 (6.19402 iter/s, 16.1446s/100 iter), loss = 0.892857
I0630 13:05:44.805449 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 13:05:44.805460 29777 sgd_solver.cpp:106] Iteration 206600, lr = 0.00354375
I0630 13:06:00.827747 29777 solver.cpp:290] Iteration 206700 (6.24147 iter/s, 16.0219s/100 iter), loss = 1.52381
I0630 13:06:00.827775 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 13:06:00.827791 29777 sgd_solver.cpp:106] Iteration 206700, lr = 0.00354062
I0630 13:06:16.968655 29777 solver.cpp:290] Iteration 206800 (6.19562 iter/s, 16.1404s/100 iter), loss = 1.20238
I0630 13:06:16.968741 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 13:06:16.968756 29777 sgd_solver.cpp:106] Iteration 206800, lr = 0.0035375
I0630 13:06:33.024149 29777 solver.cpp:290] Iteration 206900 (6.2286 iter/s, 16.055s/100 iter), loss = 1.07143
I0630 13:06:33.024173 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 13:06:33.024181 29777 sgd_solver.cpp:106] Iteration 206900, lr = 0.00353437
I0630 13:06:48.852422 29777 solver.cpp:354] Sparsity after update:
I0630 13:06:48.873266 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:06:48.873302 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:06:48.873324 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:06:48.873343 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:06:48.873358 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:06:48.873373 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:06:48.873386 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:06:48.873409 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:06:48.873423 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:06:48.873437 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:06:48.873450 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:06:48.873464 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:06:48.873469 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:06:49.030217 29777 solver.cpp:290] Iteration 207000 (6.24781 iter/s, 16.0056s/100 iter), loss = 1.04762
I0630 13:06:49.030241 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 13:06:49.030247 29777 sgd_solver.cpp:106] Iteration 207000, lr = 0.00353125
I0630 13:07:05.025055 29777 solver.cpp:290] Iteration 207100 (6.2522 iter/s, 15.9944s/100 iter), loss = 1.17857
I0630 13:07:05.025081 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 13:07:05.025091 29777 sgd_solver.cpp:106] Iteration 207100, lr = 0.00352813
I0630 13:07:20.995906 29777 solver.cpp:290] Iteration 207200 (6.26159 iter/s, 15.9704s/100 iter), loss = 1.53571
I0630 13:07:20.996217 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 13:07:20.996346 29777 sgd_solver.cpp:106] Iteration 207200, lr = 0.003525
I0630 13:07:36.983889 29777 solver.cpp:290] Iteration 207300 (6.25499 iter/s, 15.9872s/100 iter), loss = 1.32143
I0630 13:07:36.983916 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 13:07:36.983923 29777 sgd_solver.cpp:106] Iteration 207300, lr = 0.00352188
I0630 13:07:52.919553 29777 solver.cpp:290] Iteration 207400 (6.27542 iter/s, 15.9352s/100 iter), loss = 1.29762
I0630 13:07:52.919647 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 13:07:52.919668 29777 sgd_solver.cpp:106] Iteration 207400, lr = 0.00351875
I0630 13:08:09.035076 29777 solver.cpp:290] Iteration 207500 (6.2054 iter/s, 16.115s/100 iter), loss = 1.75
I0630 13:08:09.035120 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 13:08:09.035135 29777 sgd_solver.cpp:106] Iteration 207500, lr = 0.00351562
I0630 13:08:25.078899 29777 solver.cpp:290] Iteration 207600 (6.23312 iter/s, 16.0433s/100 iter), loss = 0.880953
I0630 13:08:25.078985 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 13:08:25.078995 29777 sgd_solver.cpp:106] Iteration 207600, lr = 0.0035125
I0630 13:08:41.226209 29777 solver.cpp:290] Iteration 207700 (6.19318 iter/s, 16.1468s/100 iter), loss = 1.60714
I0630 13:08:41.226233 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 13:08:41.226239 29777 sgd_solver.cpp:106] Iteration 207700, lr = 0.00350937
I0630 13:08:57.209885 29777 solver.cpp:290] Iteration 207800 (6.25656 iter/s, 15.9832s/100 iter), loss = 1.14286
I0630 13:08:57.209934 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 13:08:57.209942 29777 sgd_solver.cpp:106] Iteration 207800, lr = 0.00350625
I0630 13:09:13.274826 29777 solver.cpp:290] Iteration 207900 (6.22493 iter/s, 16.0644s/100 iter), loss = 1.05952
I0630 13:09:13.274854 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 13:09:13.274862 29777 sgd_solver.cpp:106] Iteration 207900, lr = 0.00350312
I0630 13:09:29.176127 29777 solver.cpp:354] Sparsity after update:
I0630 13:09:29.177548 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:09:29.177556 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:09:29.177564 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:09:29.177567 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:09:29.177569 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:09:29.177572 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:09:29.177573 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:09:29.177575 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:09:29.177578 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:09:29.177580 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:09:29.177582 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:09:29.177583 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:09:29.177587 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:09:29.177675 29777 solver.cpp:471] Iteration 208000, Testing net (#0)
I0630 13:09:46.866770 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 13:10:17.985980 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.551699
I0630 13:10:17.986105 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.787901
I0630 13:10:17.986115 29777 solver.cpp:544]     Test net output #2: loss = 1.61118 (* 1 = 1.61118 loss)
I0630 13:10:18.163066 29777 solver.cpp:290] Iteration 208000 (1.54115 iter/s, 64.8865s/100 iter), loss = 1.34524
I0630 13:10:18.163090 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 13:10:18.163097 29777 sgd_solver.cpp:106] Iteration 208000, lr = 0.0035
I0630 13:10:33.689106 29777 solver.cpp:290] Iteration 208100 (6.44098 iter/s, 15.5256s/100 iter), loss = 1.32143
I0630 13:10:33.689136 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 13:10:33.689144 29777 sgd_solver.cpp:106] Iteration 208100, lr = 0.00349688
I0630 13:10:49.709329 29777 solver.cpp:290] Iteration 208200 (6.24229 iter/s, 16.0198s/100 iter), loss = 1.16667
I0630 13:10:49.709408 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 13:10:49.709419 29777 sgd_solver.cpp:106] Iteration 208200, lr = 0.00349375
I0630 13:11:05.771327 29777 solver.cpp:290] Iteration 208300 (6.22608 iter/s, 16.0615s/100 iter), loss = 1.59524
I0630 13:11:05.771351 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 13:11:05.771358 29777 sgd_solver.cpp:106] Iteration 208300, lr = 0.00349062
I0630 13:11:21.921926 29777 solver.cpp:290] Iteration 208400 (6.1919 iter/s, 16.1501s/100 iter), loss = 1.07143
I0630 13:11:21.922018 29777 solver.cpp:309]     Train net output #0: loss = 0.642857 (* 1 = 0.642857 loss)
I0630 13:11:21.922029 29777 sgd_solver.cpp:106] Iteration 208400, lr = 0.0034875
I0630 13:11:38.024235 29777 solver.cpp:290] Iteration 208500 (6.21049 iter/s, 16.1018s/100 iter), loss = 1.67857
I0630 13:11:38.024257 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 13:11:38.024266 29777 sgd_solver.cpp:106] Iteration 208500, lr = 0.00348437
I0630 13:11:54.095747 29777 solver.cpp:290] Iteration 208600 (6.22237 iter/s, 16.071s/100 iter), loss = 1.34524
I0630 13:11:54.095839 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 13:11:54.095850 29777 sgd_solver.cpp:106] Iteration 208600, lr = 0.00348125
I0630 13:12:10.025229 29777 solver.cpp:290] Iteration 208700 (6.27788 iter/s, 15.929s/100 iter), loss = 1.36905
I0630 13:12:10.025251 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 13:12:10.025259 29777 sgd_solver.cpp:106] Iteration 208700, lr = 0.00347812
I0630 13:12:26.189303 29777 solver.cpp:290] Iteration 208800 (6.18674 iter/s, 16.1636s/100 iter), loss = 0.928572
I0630 13:12:26.189360 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 13:12:26.189368 29777 sgd_solver.cpp:106] Iteration 208800, lr = 0.003475
I0630 13:12:42.083288 29777 solver.cpp:290] Iteration 208900 (6.29188 iter/s, 15.8935s/100 iter), loss = 1.53571
I0630 13:12:42.083310 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 13:12:42.083317 29777 sgd_solver.cpp:106] Iteration 208900, lr = 0.00347188
I0630 13:12:57.936303 29777 solver.cpp:354] Sparsity after update:
I0630 13:12:57.956866 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:12:57.956879 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:12:57.956890 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:12:57.956894 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:12:57.956898 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:12:57.956902 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:12:57.956904 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:12:57.956917 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:12:57.956921 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:12:57.956926 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:12:57.956931 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:12:57.956936 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:12:57.956941 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:12:58.113339 29777 solver.cpp:290] Iteration 209000 (6.23846 iter/s, 16.0296s/100 iter), loss = 0.976191
I0630 13:12:58.113368 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 13:12:58.113376 29777 sgd_solver.cpp:106] Iteration 209000, lr = 0.00346875
I0630 13:13:14.180220 29777 solver.cpp:290] Iteration 209100 (6.22417 iter/s, 16.0664s/100 iter), loss = 1.14286
I0630 13:13:14.180279 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 13:13:14.180304 29777 sgd_solver.cpp:106] Iteration 209100, lr = 0.00346562
I0630 13:13:30.285264 29777 solver.cpp:290] Iteration 209200 (6.20943 iter/s, 16.1045s/100 iter), loss = 0.964286
I0630 13:13:30.285344 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 13:13:30.285352 29777 sgd_solver.cpp:106] Iteration 209200, lr = 0.0034625
I0630 13:13:46.292665 29777 solver.cpp:290] Iteration 209300 (6.24731 iter/s, 16.0069s/100 iter), loss = 1.40476
I0630 13:13:46.292690 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 13:13:46.292699 29777 sgd_solver.cpp:106] Iteration 209300, lr = 0.00345937
I0630 13:14:02.309547 29777 solver.cpp:290] Iteration 209400 (6.24359 iter/s, 16.0164s/100 iter), loss = 0.916667
I0630 13:14:02.309658 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 13:14:02.309667 29777 sgd_solver.cpp:106] Iteration 209400, lr = 0.00345625
I0630 13:14:18.344702 29777 solver.cpp:290] Iteration 209500 (6.23651 iter/s, 16.0346s/100 iter), loss = 1.30952
I0630 13:14:18.344724 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 13:14:18.344732 29777 sgd_solver.cpp:106] Iteration 209500, lr = 0.00345312
I0630 13:14:34.391789 29777 solver.cpp:290] Iteration 209600 (6.23184 iter/s, 16.0466s/100 iter), loss = 1.64286
I0630 13:14:34.391893 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 13:14:34.391903 29777 sgd_solver.cpp:106] Iteration 209600, lr = 0.00345
I0630 13:14:50.334626 29777 solver.cpp:290] Iteration 209700 (6.27262 iter/s, 15.9423s/100 iter), loss = 1.70238
I0630 13:14:50.334651 29777 solver.cpp:309]     Train net output #0: loss = 2.04762 (* 1 = 2.04762 loss)
I0630 13:14:50.334658 29777 sgd_solver.cpp:106] Iteration 209700, lr = 0.00344688
I0630 13:15:06.362318 29777 solver.cpp:290] Iteration 209800 (6.23938 iter/s, 16.0272s/100 iter), loss = 1.02381
I0630 13:15:06.362412 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 13:15:06.362422 29777 sgd_solver.cpp:106] Iteration 209800, lr = 0.00344375
I0630 13:15:22.398598 29777 solver.cpp:290] Iteration 209900 (6.23607 iter/s, 16.0357s/100 iter), loss = 1.41667
I0630 13:15:22.398623 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 13:15:22.398633 29777 sgd_solver.cpp:106] Iteration 209900, lr = 0.00344063
I0630 13:15:38.213711 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_210000.caffemodel
I0630 13:15:38.232976 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_210000.solverstate
I0630 13:15:38.241809 29777 solver.cpp:354] Sparsity after update:
I0630 13:15:38.242822 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:15:38.242831 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:15:38.242858 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:15:38.242864 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:15:38.242868 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:15:38.242873 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:15:38.242877 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:15:38.242882 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:15:38.242887 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:15:38.242890 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:15:38.242894 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:15:38.242898 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:15:38.242902 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:15:38.243011 29777 solver.cpp:471] Iteration 210000, Testing net (#0)
I0630 13:15:56.055513 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 13:16:28.090598 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55204
I0630 13:16:28.090737 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.789001
I0630 13:16:28.090747 29777 solver.cpp:544]     Test net output #2: loss = 1.59674 (* 1 = 1.59674 loss)
I0630 13:16:28.265851 29777 solver.cpp:290] Iteration 210000 (1.51825 iter/s, 65.8654s/100 iter), loss = 1.17857
I0630 13:16:28.265874 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 13:16:28.265880 29777 sgd_solver.cpp:106] Iteration 210000, lr = 0.0034375
I0630 13:16:44.258770 29777 solver.cpp:290] Iteration 210100 (6.25295 iter/s, 15.9925s/100 iter), loss = 1.39286
I0630 13:16:44.258795 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 13:16:44.258800 29777 sgd_solver.cpp:106] Iteration 210100, lr = 0.00343437
I0630 13:17:00.313319 29777 solver.cpp:290] Iteration 210200 (6.22895 iter/s, 16.0541s/100 iter), loss = 0.845238
I0630 13:17:00.313398 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 13:17:00.313407 29777 sgd_solver.cpp:106] Iteration 210200, lr = 0.00343125
I0630 13:17:16.398792 29777 solver.cpp:290] Iteration 210300 (6.21699 iter/s, 16.0849s/100 iter), loss = 1.29762
I0630 13:17:16.398862 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 13:17:16.398886 29777 sgd_solver.cpp:106] Iteration 210300, lr = 0.00342812
I0630 13:17:32.417970 29777 solver.cpp:290] Iteration 210400 (6.24271 iter/s, 16.0187s/100 iter), loss = 1.65476
I0630 13:17:32.418012 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 13:17:32.418020 29777 sgd_solver.cpp:106] Iteration 210400, lr = 0.003425
I0630 13:17:48.476744 29777 solver.cpp:290] Iteration 210500 (6.22731 iter/s, 16.0583s/100 iter), loss = 1.2381
I0630 13:17:48.476765 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 13:17:48.476773 29777 sgd_solver.cpp:106] Iteration 210500, lr = 0.00342188
I0630 13:18:04.456634 29777 solver.cpp:290] Iteration 210600 (6.25805 iter/s, 15.9794s/100 iter), loss = 1.71429
I0630 13:18:04.456679 29777 solver.cpp:309]     Train net output #0: loss = 1.78571 (* 1 = 1.78571 loss)
I0630 13:18:04.456687 29777 sgd_solver.cpp:106] Iteration 210600, lr = 0.00341875
I0630 13:18:20.495348 29777 solver.cpp:290] Iteration 210700 (6.2351 iter/s, 16.0382s/100 iter), loss = 1.22619
I0630 13:18:20.495370 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 13:18:20.495378 29777 sgd_solver.cpp:106] Iteration 210700, lr = 0.00341563
I0630 13:18:36.516863 29777 solver.cpp:290] Iteration 210800 (6.24179 iter/s, 16.0211s/100 iter), loss = 1.47619
I0630 13:18:36.516968 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 13:18:36.516978 29777 sgd_solver.cpp:106] Iteration 210800, lr = 0.0034125
I0630 13:18:52.525533 29777 solver.cpp:290] Iteration 210900 (6.24683 iter/s, 16.0081s/100 iter), loss = 1.42857
I0630 13:18:52.525555 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 13:18:52.525563 29777 sgd_solver.cpp:106] Iteration 210900, lr = 0.00340937
I0630 13:19:08.471174 29777 solver.cpp:354] Sparsity after update:
I0630 13:19:08.491708 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:19:08.491746 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:19:08.491762 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:19:08.491771 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:19:08.491780 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:19:08.491791 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:19:08.491799 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:19:08.491808 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:19:08.491817 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:19:08.491827 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:19:08.491837 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:19:08.491847 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:19:08.491856 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:19:08.659642 29777 solver.cpp:290] Iteration 211000 (6.19823 iter/s, 16.1336s/100 iter), loss = 1.14286
I0630 13:19:08.659667 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 13:19:08.659673 29777 sgd_solver.cpp:106] Iteration 211000, lr = 0.00340625
I0630 13:19:24.675215 29777 solver.cpp:290] Iteration 211100 (6.2441 iter/s, 16.0151s/100 iter), loss = 1.30952
I0630 13:19:24.675238 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 13:19:24.675246 29777 sgd_solver.cpp:106] Iteration 211100, lr = 0.00340312
I0630 13:19:40.909955 29777 solver.cpp:290] Iteration 211200 (6.15981 iter/s, 16.2343s/100 iter), loss = 0.857143
I0630 13:19:40.910048 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 13:19:40.910061 29777 sgd_solver.cpp:106] Iteration 211200, lr = 0.0034
I0630 13:19:57.063271 29777 solver.cpp:290] Iteration 211300 (6.19089 iter/s, 16.1528s/100 iter), loss = 1.45238
I0630 13:19:57.063295 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 13:19:57.063304 29777 sgd_solver.cpp:106] Iteration 211300, lr = 0.00339688
I0630 13:20:13.206707 29777 solver.cpp:290] Iteration 211400 (6.19465 iter/s, 16.143s/100 iter), loss = 1.14286
I0630 13:20:13.206758 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 13:20:13.206766 29777 sgd_solver.cpp:106] Iteration 211400, lr = 0.00339375
I0630 13:20:29.139156 29777 solver.cpp:290] Iteration 211500 (6.27669 iter/s, 15.932s/100 iter), loss = 1.45238
I0630 13:20:29.139179 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 13:20:29.139186 29777 sgd_solver.cpp:106] Iteration 211500, lr = 0.00339063
I0630 13:20:45.326135 29777 solver.cpp:290] Iteration 211600 (6.17798 iter/s, 16.1865s/100 iter), loss = 1.42857
I0630 13:20:45.326242 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 13:20:45.326251 29777 sgd_solver.cpp:106] Iteration 211600, lr = 0.0033875
I0630 13:21:01.274344 29777 solver.cpp:290] Iteration 211700 (6.27051 iter/s, 15.9477s/100 iter), loss = 1.60714
I0630 13:21:01.274369 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 13:21:01.274379 29777 sgd_solver.cpp:106] Iteration 211700, lr = 0.00338438
I0630 13:21:17.356289 29777 solver.cpp:290] Iteration 211800 (6.21833 iter/s, 16.0815s/100 iter), loss = 1.42857
I0630 13:21:17.356385 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 13:21:17.356396 29777 sgd_solver.cpp:106] Iteration 211800, lr = 0.00338125
I0630 13:21:33.394413 29777 solver.cpp:290] Iteration 211900 (6.23535 iter/s, 16.0376s/100 iter), loss = 1.29762
I0630 13:21:33.394439 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 13:21:33.394449 29777 sgd_solver.cpp:106] Iteration 211900, lr = 0.00337812
I0630 13:21:49.237644 29777 solver.cpp:354] Sparsity after update:
I0630 13:21:49.239096 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:21:49.239104 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:21:49.239114 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:21:49.239118 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:21:49.239122 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:21:49.239125 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:21:49.239126 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:21:49.239128 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:21:49.239130 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:21:49.239132 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:21:49.239133 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:21:49.239135 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:21:49.239137 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:21:49.239233 29777 solver.cpp:471] Iteration 212000, Testing net (#0)
I0630 13:22:07.156523 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 13:22:38.208866 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5515
I0630 13:22:38.208972 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.788681
I0630 13:22:38.208981 29777 solver.cpp:544]     Test net output #2: loss = 1.60248 (* 1 = 1.60248 loss)
I0630 13:22:38.383013 29777 solver.cpp:290] Iteration 212000 (1.53877 iter/s, 64.9868s/100 iter), loss = 1.07143
I0630 13:22:38.383036 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 13:22:38.383043 29777 sgd_solver.cpp:106] Iteration 212000, lr = 0.003375
I0630 13:22:54.200453 29777 solver.cpp:290] Iteration 212100 (6.32232 iter/s, 15.817s/100 iter), loss = 1.17857
I0630 13:22:54.200475 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 13:22:54.200484 29777 sgd_solver.cpp:106] Iteration 212100, lr = 0.00337188
I0630 13:23:10.185171 29777 solver.cpp:290] Iteration 212200 (6.25616 iter/s, 15.9843s/100 iter), loss = 1.10714
I0630 13:23:10.185292 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 13:23:10.185303 29777 sgd_solver.cpp:106] Iteration 212200, lr = 0.00336875
I0630 13:23:26.209380 29777 solver.cpp:290] Iteration 212300 (6.24078 iter/s, 16.0236s/100 iter), loss = 1.59524
I0630 13:23:26.209406 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 13:23:26.209416 29777 sgd_solver.cpp:106] Iteration 212300, lr = 0.00336563
I0630 13:23:42.180919 29777 solver.cpp:290] Iteration 212400 (6.26132 iter/s, 15.9711s/100 iter), loss = 1.20238
I0630 13:23:42.180968 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 13:23:42.180979 29777 sgd_solver.cpp:106] Iteration 212400, lr = 0.0033625
I0630 13:23:58.213896 29777 solver.cpp:290] Iteration 212500 (6.23734 iter/s, 16.0325s/100 iter), loss = 1.2381
I0630 13:23:58.213924 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 13:23:58.213938 29777 sgd_solver.cpp:106] Iteration 212500, lr = 0.00335937
I0630 13:24:14.143450 29777 solver.cpp:290] Iteration 212600 (6.27782 iter/s, 15.9291s/100 iter), loss = 1.41667
I0630 13:24:14.143555 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 13:24:14.143564 29777 sgd_solver.cpp:106] Iteration 212600, lr = 0.00335625
I0630 13:24:30.055621 29777 solver.cpp:290] Iteration 212700 (6.28471 iter/s, 15.9116s/100 iter), loss = 1.2381
I0630 13:24:30.055646 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 13:24:30.055655 29777 sgd_solver.cpp:106] Iteration 212700, lr = 0.00335312
I0630 13:24:46.042230 29777 solver.cpp:290] Iteration 212800 (6.25542 iter/s, 15.9861s/100 iter), loss = 1.19048
I0630 13:24:46.042290 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 13:24:46.042299 29777 sgd_solver.cpp:106] Iteration 212800, lr = 0.00335
I0630 13:25:02.065318 29777 solver.cpp:290] Iteration 212900 (6.24119 iter/s, 16.0226s/100 iter), loss = 1.15476
I0630 13:25:02.065342 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 13:25:02.065348 29777 sgd_solver.cpp:106] Iteration 212900, lr = 0.00334687
I0630 13:25:18.052908 29777 solver.cpp:354] Sparsity after update:
I0630 13:25:18.073125 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:25:18.073159 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:25:18.073176 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:25:18.073184 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:25:18.073194 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:25:18.073201 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:25:18.073210 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:25:18.073216 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:25:18.073225 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:25:18.073232 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:25:18.073240 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:25:18.073248 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:25:18.073256 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:25:18.226958 29777 solver.cpp:290] Iteration 213000 (6.18767 iter/s, 16.1612s/100 iter), loss = 1.34524
I0630 13:25:18.226985 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 13:25:18.226991 29777 sgd_solver.cpp:106] Iteration 213000, lr = 0.00334375
I0630 13:25:34.307572 29777 solver.cpp:290] Iteration 213100 (6.21885 iter/s, 16.0801s/100 iter), loss = 1.32143
I0630 13:25:34.307595 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 13:25:34.307610 29777 sgd_solver.cpp:106] Iteration 213100, lr = 0.00334063
I0630 13:25:50.453380 29777 solver.cpp:290] Iteration 213200 (6.19374 iter/s, 16.1453s/100 iter), loss = 1.53571
I0630 13:25:50.453454 29777 solver.cpp:309]     Train net output #0: loss = 2.02381 (* 1 = 2.02381 loss)
I0630 13:25:50.453461 29777 sgd_solver.cpp:106] Iteration 213200, lr = 0.0033375
I0630 13:26:06.402784 29777 solver.cpp:290] Iteration 213300 (6.27003 iter/s, 15.9489s/100 iter), loss = 1.2619
I0630 13:26:06.402807 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 13:26:06.402813 29777 sgd_solver.cpp:106] Iteration 213300, lr = 0.00333437
I0630 13:26:22.519083 29777 solver.cpp:290] Iteration 213400 (6.20508 iter/s, 16.1158s/100 iter), loss = 1.57143
I0630 13:26:22.519150 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 13:26:22.519157 29777 sgd_solver.cpp:106] Iteration 213400, lr = 0.00333125
I0630 13:26:38.527709 29777 solver.cpp:290] Iteration 213500 (6.24683 iter/s, 16.0081s/100 iter), loss = 1.11905
I0630 13:26:38.527732 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 13:26:38.527740 29777 sgd_solver.cpp:106] Iteration 213500, lr = 0.00332812
I0630 13:26:55.350410 29777 solver.cpp:290] Iteration 213600 (5.94452 iter/s, 16.8222s/100 iter), loss = 1.41667
I0630 13:26:55.350733 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 13:26:55.350878 29777 sgd_solver.cpp:106] Iteration 213600, lr = 0.003325
I0630 13:27:11.955433 29777 solver.cpp:290] Iteration 213700 (6.02256 iter/s, 16.6042s/100 iter), loss = 1.09524
I0630 13:27:11.955459 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 13:27:11.955468 29777 sgd_solver.cpp:106] Iteration 213700, lr = 0.00332187
I0630 13:27:28.185789 29777 solver.cpp:290] Iteration 213800 (6.16147 iter/s, 16.2299s/100 iter), loss = 1.2619
I0630 13:27:28.185920 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 13:27:28.185930 29777 sgd_solver.cpp:106] Iteration 213800, lr = 0.00331875
I0630 13:27:44.160537 29777 solver.cpp:290] Iteration 213900 (6.26012 iter/s, 15.9741s/100 iter), loss = 1.60714
I0630 13:27:44.160730 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 13:27:44.160794 29777 sgd_solver.cpp:106] Iteration 213900, lr = 0.00331563
I0630 13:28:00.197993 29777 solver.cpp:354] Sparsity after update:
I0630 13:28:00.199637 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:28:00.199646 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:28:00.199654 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:28:00.199656 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:28:00.199659 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:28:00.199661 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:28:00.199663 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:28:00.199664 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:28:00.199666 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:28:00.199668 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:28:00.199671 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:28:00.199672 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:28:00.199674 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:28:00.199762 29777 solver.cpp:471] Iteration 214000, Testing net (#0)
I0630 13:28:27.538229 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 13:29:01.491847 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55192
I0630 13:29:01.491958 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.787981
I0630 13:29:01.491967 29777 solver.cpp:544]     Test net output #2: loss = 1.60606 (* 1 = 1.60606 loss)
I0630 13:29:01.674037 29777 solver.cpp:290] Iteration 214000 (1.29014 iter/s, 77.5112s/100 iter), loss = 1.2619
I0630 13:29:01.674062 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 13:29:01.674068 29777 sgd_solver.cpp:106] Iteration 214000, lr = 0.0033125
I0630 13:29:17.824748 29777 solver.cpp:290] Iteration 214100 (6.19186 iter/s, 16.1502s/100 iter), loss = 1.40476
I0630 13:29:17.824774 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 13:29:17.824784 29777 sgd_solver.cpp:106] Iteration 214100, lr = 0.00330937
I0630 13:29:33.890422 29777 solver.cpp:290] Iteration 214200 (6.22463 iter/s, 16.0652s/100 iter), loss = 1.30952
I0630 13:29:33.890517 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 13:29:33.890528 29777 sgd_solver.cpp:106] Iteration 214200, lr = 0.00330625
I0630 13:29:49.871846 29777 solver.cpp:290] Iteration 214300 (6.25747 iter/s, 15.9809s/100 iter), loss = 1.21429
I0630 13:29:49.871870 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 13:29:49.871876 29777 sgd_solver.cpp:106] Iteration 214300, lr = 0.00330312
I0630 13:30:05.980680 29777 solver.cpp:290] Iteration 214400 (6.20795 iter/s, 16.1084s/100 iter), loss = 1.34524
I0630 13:30:05.980785 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 13:30:05.980795 29777 sgd_solver.cpp:106] Iteration 214400, lr = 0.0033
I0630 13:30:21.940567 29777 solver.cpp:290] Iteration 214500 (6.26592 iter/s, 15.9593s/100 iter), loss = 1.60714
I0630 13:30:21.940593 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 13:30:21.940601 29777 sgd_solver.cpp:106] Iteration 214500, lr = 0.00329687
I0630 13:30:37.873157 29777 solver.cpp:290] Iteration 214600 (6.27663 iter/s, 15.9321s/100 iter), loss = 1.30952
I0630 13:30:37.873236 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 13:30:37.873247 29777 sgd_solver.cpp:106] Iteration 214600, lr = 0.00329375
I0630 13:30:53.963420 29777 solver.cpp:290] Iteration 214700 (6.21514 iter/s, 16.0897s/100 iter), loss = 1.33333
I0630 13:30:53.963444 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 13:30:53.963451 29777 sgd_solver.cpp:106] Iteration 214700, lr = 0.00329063
I0630 13:31:10.040329 29777 solver.cpp:290] Iteration 214800 (6.22028 iter/s, 16.0764s/100 iter), loss = 1.40476
I0630 13:31:10.040444 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 13:31:10.040455 29777 sgd_solver.cpp:106] Iteration 214800, lr = 0.0032875
I0630 13:31:26.056066 29777 solver.cpp:290] Iteration 214900 (6.24407 iter/s, 16.0152s/100 iter), loss = 0.845238
I0630 13:31:26.056092 29777 solver.cpp:309]     Train net output #0: loss = 0.690476 (* 1 = 0.690476 loss)
I0630 13:31:26.056108 29777 sgd_solver.cpp:106] Iteration 214900, lr = 0.00328437
I0630 13:31:41.989079 29777 solver.cpp:354] Sparsity after update:
I0630 13:31:42.009599 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:31:42.009615 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:31:42.009625 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:31:42.009629 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:31:42.009641 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:31:42.009646 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:31:42.009651 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:31:42.009656 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:31:42.009661 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:31:42.009666 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:31:42.009670 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:31:42.009675 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:31:42.009678 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:31:42.170007 29777 solver.cpp:290] Iteration 215000 (6.20599 iter/s, 16.1135s/100 iter), loss = 1.13095
I0630 13:31:42.170029 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 13:31:42.170035 29777 sgd_solver.cpp:106] Iteration 215000, lr = 0.00328125
I0630 13:31:58.144430 29777 solver.cpp:290] Iteration 215100 (6.26019 iter/s, 15.974s/100 iter), loss = 1.04762
I0630 13:31:58.144456 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 13:31:58.144466 29777 sgd_solver.cpp:106] Iteration 215100, lr = 0.00327812
I0630 13:32:14.229681 29777 solver.cpp:290] Iteration 215200 (6.21706 iter/s, 16.0848s/100 iter), loss = 1.40476
I0630 13:32:14.229770 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 13:32:14.229780 29777 sgd_solver.cpp:106] Iteration 215200, lr = 0.003275
I0630 13:32:30.220229 29777 solver.cpp:290] Iteration 215300 (6.2539 iter/s, 15.99s/100 iter), loss = 1.39286
I0630 13:32:30.220254 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 13:32:30.220263 29777 sgd_solver.cpp:106] Iteration 215300, lr = 0.00327187
I0630 13:32:46.193820 29777 solver.cpp:290] Iteration 215400 (6.26051 iter/s, 15.9731s/100 iter), loss = 1.36905
I0630 13:32:46.193894 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 13:32:46.193904 29777 sgd_solver.cpp:106] Iteration 215400, lr = 0.00326875
I0630 13:33:02.192960 29777 solver.cpp:290] Iteration 215500 (6.25054 iter/s, 15.9986s/100 iter), loss = 1.41667
I0630 13:33:02.192984 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 13:33:02.192991 29777 sgd_solver.cpp:106] Iteration 215500, lr = 0.00326563
I0630 13:33:18.100261 29777 solver.cpp:290] Iteration 215600 (6.2866 iter/s, 15.9068s/100 iter), loss = 1.20238
I0630 13:33:18.100307 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 13:33:18.100317 29777 sgd_solver.cpp:106] Iteration 215600, lr = 0.0032625
I0630 13:33:34.205675 29777 solver.cpp:290] Iteration 215700 (6.20928 iter/s, 16.1049s/100 iter), loss = 0.964286
I0630 13:33:34.205699 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 13:33:34.205706 29777 sgd_solver.cpp:106] Iteration 215700, lr = 0.00325937
I0630 13:33:50.216477 29777 solver.cpp:290] Iteration 215800 (6.24597 iter/s, 16.0103s/100 iter), loss = 1.33333
I0630 13:33:50.216596 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 13:33:50.216605 29777 sgd_solver.cpp:106] Iteration 215800, lr = 0.00325625
I0630 13:34:06.199223 29777 solver.cpp:290] Iteration 215900 (6.25697 iter/s, 15.9822s/100 iter), loss = 1.28571
I0630 13:34:06.199246 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 13:34:06.199254 29777 sgd_solver.cpp:106] Iteration 215900, lr = 0.00325312
I0630 13:34:22.160682 29777 solver.cpp:354] Sparsity after update:
I0630 13:34:22.161950 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:34:22.161958 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:34:22.161965 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:34:22.161967 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:34:22.161969 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:34:22.161972 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:34:22.161973 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:34:22.161975 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:34:22.161978 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:34:22.161979 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:34:22.161981 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:34:22.161983 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:34:22.161985 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:34:22.162071 29777 solver.cpp:471] Iteration 216000, Testing net (#0)
I0630 13:34:41.953014 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 13:35:23.953145 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.54992
I0630 13:35:23.953225 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.788221
I0630 13:35:23.953234 29777 solver.cpp:544]     Test net output #2: loss = 1.60762 (* 1 = 1.60762 loss)
I0630 13:35:24.143347 29777 solver.cpp:290] Iteration 216000 (1.28301 iter/s, 77.942s/100 iter), loss = 1.28571
I0630 13:35:24.143370 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 13:35:24.143378 29777 sgd_solver.cpp:106] Iteration 216000, lr = 0.00325
I0630 13:35:40.185118 29777 solver.cpp:290] Iteration 216100 (6.23391 iter/s, 16.0413s/100 iter), loss = 1.30952
I0630 13:35:40.185143 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 13:35:40.185153 29777 sgd_solver.cpp:106] Iteration 216100, lr = 0.00324687
I0630 13:35:56.178587 29777 solver.cpp:290] Iteration 216200 (6.25273 iter/s, 15.993s/100 iter), loss = 1.52381
I0630 13:35:56.178658 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 13:35:56.178665 29777 sgd_solver.cpp:106] Iteration 216200, lr = 0.00324375
I0630 13:36:12.389734 29777 solver.cpp:290] Iteration 216300 (6.16879 iter/s, 16.2106s/100 iter), loss = 1.15476
I0630 13:36:12.389785 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 13:36:12.389806 29777 sgd_solver.cpp:106] Iteration 216300, lr = 0.00324063
I0630 13:36:28.563164 29777 solver.cpp:290] Iteration 216400 (6.18317 iter/s, 16.1729s/100 iter), loss = 1.32143
I0630 13:36:28.563257 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 13:36:28.563267 29777 sgd_solver.cpp:106] Iteration 216400, lr = 0.0032375
I0630 13:36:44.567934 29777 solver.cpp:290] Iteration 216500 (6.24834 iter/s, 16.0042s/100 iter), loss = 1.22619
I0630 13:36:44.567957 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 13:36:44.567965 29777 sgd_solver.cpp:106] Iteration 216500, lr = 0.00323438
I0630 13:37:00.715360 29777 solver.cpp:290] Iteration 216600 (6.19312 iter/s, 16.147s/100 iter), loss = 1.44048
I0630 13:37:00.715474 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 13:37:00.715487 29777 sgd_solver.cpp:106] Iteration 216600, lr = 0.00323125
I0630 13:37:16.784238 29777 solver.cpp:290] Iteration 216700 (6.22343 iter/s, 16.0683s/100 iter), loss = 1.28571
I0630 13:37:16.784263 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 13:37:16.784271 29777 sgd_solver.cpp:106] Iteration 216700, lr = 0.00322812
I0630 13:37:32.825292 29777 solver.cpp:290] Iteration 216800 (6.23419 iter/s, 16.0406s/100 iter), loss = 1.60714
I0630 13:37:32.825395 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 13:37:32.825419 29777 sgd_solver.cpp:106] Iteration 216800, lr = 0.003225
I0630 13:37:48.921033 29777 solver.cpp:290] Iteration 216900 (6.21303 iter/s, 16.0952s/100 iter), loss = 1.09524
I0630 13:37:48.921106 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 13:37:48.921130 29777 sgd_solver.cpp:106] Iteration 216900, lr = 0.00322187
I0630 13:38:04.821918 29777 solver.cpp:354] Sparsity after update:
I0630 13:38:04.842531 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:38:04.842566 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:38:04.842583 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:38:04.842593 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:38:04.842602 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:38:04.842612 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:38:04.842620 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:38:04.842628 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:38:04.842636 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:38:04.842645 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:38:04.842653 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:38:04.842663 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:38:04.842671 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:38:05.000784 29777 solver.cpp:290] Iteration 217000 (6.2192 iter/s, 16.0792s/100 iter), loss = 1.09524
I0630 13:38:05.000808 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 13:38:05.000814 29777 sgd_solver.cpp:106] Iteration 217000, lr = 0.00321875
I0630 13:38:20.998370 29777 solver.cpp:290] Iteration 217100 (6.25112 iter/s, 15.9971s/100 iter), loss = 1.27381
I0630 13:38:20.998409 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 13:38:20.998421 29777 sgd_solver.cpp:106] Iteration 217100, lr = 0.00321563
I0630 13:38:37.086129 29777 solver.cpp:290] Iteration 217200 (6.21609 iter/s, 16.0873s/100 iter), loss = 1.17857
I0630 13:38:37.086231 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 13:38:37.086241 29777 sgd_solver.cpp:106] Iteration 217200, lr = 0.0032125
I0630 13:38:53.131919 29777 solver.cpp:290] Iteration 217300 (6.23237 iter/s, 16.0453s/100 iter), loss = 1.14286
I0630 13:38:53.131943 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 13:38:53.131950 29777 sgd_solver.cpp:106] Iteration 217300, lr = 0.00320938
I0630 13:39:09.309993 29777 solver.cpp:290] Iteration 217400 (6.18139 iter/s, 16.1776s/100 iter), loss = 1.28571
I0630 13:39:09.310061 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 13:39:09.310070 29777 sgd_solver.cpp:106] Iteration 217400, lr = 0.00320625
I0630 13:39:25.299649 29777 solver.cpp:290] Iteration 217500 (6.25424 iter/s, 15.9891s/100 iter), loss = 0.916667
I0630 13:39:25.299671 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 13:39:25.299679 29777 sgd_solver.cpp:106] Iteration 217500, lr = 0.00320312
I0630 13:39:41.379065 29777 solver.cpp:290] Iteration 217600 (6.21931 iter/s, 16.0789s/100 iter), loss = 1.22619
I0630 13:39:41.379179 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 13:39:41.379192 29777 sgd_solver.cpp:106] Iteration 217600, lr = 0.0032
I0630 13:39:57.507848 29777 solver.cpp:290] Iteration 217700 (6.20031 iter/s, 16.1282s/100 iter), loss = 1.29762
I0630 13:39:57.507936 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 13:39:57.507962 29777 sgd_solver.cpp:106] Iteration 217700, lr = 0.00319687
I0630 13:40:13.672430 29777 solver.cpp:290] Iteration 217800 (6.18657 iter/s, 16.1641s/100 iter), loss = 1.35714
I0630 13:40:13.672526 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 13:40:13.672534 29777 sgd_solver.cpp:106] Iteration 217800, lr = 0.00319375
I0630 13:40:29.796919 29777 solver.cpp:290] Iteration 217900 (6.20195 iter/s, 16.1239s/100 iter), loss = 1.27381
I0630 13:40:29.796942 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 13:40:29.796950 29777 sgd_solver.cpp:106] Iteration 217900, lr = 0.00319062
I0630 13:40:45.709569 29777 solver.cpp:354] Sparsity after update:
I0630 13:40:45.710857 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:40:45.710867 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:40:45.710875 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:40:45.710878 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:40:45.710881 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:40:45.710883 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:40:45.710886 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:40:45.710888 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:40:45.710891 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:40:45.710892 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:40:45.710896 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:40:45.710898 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:40:45.710901 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:40:45.710988 29777 solver.cpp:471] Iteration 218000, Testing net (#0)
I0630 13:41:08.841572 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 13:41:45.003715 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55366
I0630 13:41:45.003816 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.789081
I0630 13:41:45.003828 29777 solver.cpp:544]     Test net output #2: loss = 1.60058 (* 1 = 1.60058 loss)
I0630 13:41:45.185909 29777 solver.cpp:290] Iteration 218000 (1.32649 iter/s, 75.3869s/100 iter), loss = 1.20238
I0630 13:41:45.185933 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 13:41:45.185940 29777 sgd_solver.cpp:106] Iteration 218000, lr = 0.0031875
I0630 13:42:01.307112 29777 solver.cpp:290] Iteration 218100 (6.20319 iter/s, 16.1207s/100 iter), loss = 1.39286
I0630 13:42:01.307145 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 13:42:01.307157 29777 sgd_solver.cpp:106] Iteration 218100, lr = 0.00318438
I0630 13:42:17.381842 29777 solver.cpp:290] Iteration 218200 (6.22113 iter/s, 16.0742s/100 iter), loss = 1.42857
I0630 13:42:17.381955 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 13:42:17.381983 29777 sgd_solver.cpp:106] Iteration 218200, lr = 0.00318125
I0630 13:42:33.879449 29777 solver.cpp:290] Iteration 218300 (6.06169 iter/s, 16.497s/100 iter), loss = 1.09524
I0630 13:42:33.879483 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 13:42:33.879492 29777 sgd_solver.cpp:106] Iteration 218300, lr = 0.00317813
I0630 13:42:50.165935 29777 solver.cpp:290] Iteration 218400 (6.14024 iter/s, 16.286s/100 iter), loss = 1.52381
I0630 13:42:50.166044 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 13:42:50.166054 29777 sgd_solver.cpp:106] Iteration 218400, lr = 0.003175
I0630 13:43:06.338681 29777 solver.cpp:290] Iteration 218500 (6.18345 iter/s, 16.1722s/100 iter), loss = 0.916667
I0630 13:43:06.338706 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 13:43:06.338713 29777 sgd_solver.cpp:106] Iteration 218500, lr = 0.00317187
I0630 13:43:22.640679 29777 solver.cpp:290] Iteration 218600 (6.1344 iter/s, 16.3015s/100 iter), loss = 1.38095
I0630 13:43:22.640799 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 13:43:22.640808 29777 sgd_solver.cpp:106] Iteration 218600, lr = 0.00316875
I0630 13:43:39.965940 29777 solver.cpp:290] Iteration 218700 (5.77212 iter/s, 17.3247s/100 iter), loss = 1.63095
I0630 13:43:39.965970 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 13:43:39.965978 29777 sgd_solver.cpp:106] Iteration 218700, lr = 0.00316562
I0630 13:43:56.655720 29777 solver.cpp:290] Iteration 218800 (5.99187 iter/s, 16.6893s/100 iter), loss = 1.53571
I0630 13:43:56.655803 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 13:43:56.655813 29777 sgd_solver.cpp:106] Iteration 218800, lr = 0.0031625
I0630 13:44:12.986032 29777 solver.cpp:290] Iteration 218900 (6.12378 iter/s, 16.3298s/100 iter), loss = 1.30952
I0630 13:44:12.986055 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 13:44:12.986063 29777 sgd_solver.cpp:106] Iteration 218900, lr = 0.00315938
I0630 13:44:29.038142 29777 solver.cpp:354] Sparsity after update:
I0630 13:44:29.058724 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:44:29.058748 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:44:29.058760 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:44:29.058763 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:44:29.058768 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:44:29.058779 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:44:29.058790 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:44:29.058800 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:44:29.058805 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:44:29.058815 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:44:29.058823 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:44:29.058841 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:44:29.058853 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:44:29.218962 29777 solver.cpp:290] Iteration 219000 (6.1605 iter/s, 16.2325s/100 iter), loss = 1.29762
I0630 13:44:29.219002 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 13:44:29.219018 29777 sgd_solver.cpp:106] Iteration 219000, lr = 0.00315625
I0630 13:44:45.373040 29777 solver.cpp:290] Iteration 219100 (6.19058 iter/s, 16.1536s/100 iter), loss = 0.952381
I0630 13:44:45.373128 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 13:44:45.373158 29777 sgd_solver.cpp:106] Iteration 219100, lr = 0.00315313
I0630 13:45:01.743257 29777 solver.cpp:290] Iteration 219200 (6.10885 iter/s, 16.3697s/100 iter), loss = 1.21429
I0630 13:45:01.743350 29777 solver.cpp:309]     Train net output #0: loss = 0.690476 (* 1 = 0.690476 loss)
I0630 13:45:01.743360 29777 sgd_solver.cpp:106] Iteration 219200, lr = 0.00315
I0630 13:45:17.810592 29777 solver.cpp:290] Iteration 219300 (6.22402 iter/s, 16.0668s/100 iter), loss = 1.58333
I0630 13:45:17.810699 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 13:45:17.810739 29777 sgd_solver.cpp:106] Iteration 219300, lr = 0.00314687
I0630 13:45:33.997388 29777 solver.cpp:290] Iteration 219400 (6.17808 iter/s, 16.1862s/100 iter), loss = 1.36905
I0630 13:45:33.997483 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 13:45:33.997493 29777 sgd_solver.cpp:106] Iteration 219400, lr = 0.00314375
I0630 13:45:50.209331 29777 solver.cpp:290] Iteration 219500 (6.1685 iter/s, 16.2114s/100 iter), loss = 0.77381
I0630 13:45:50.209354 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 13:45:50.209360 29777 sgd_solver.cpp:106] Iteration 219500, lr = 0.00314062
I0630 13:46:06.505888 29777 solver.cpp:290] Iteration 219600 (6.13644 iter/s, 16.2961s/100 iter), loss = 1.40476
I0630 13:46:06.505981 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 13:46:06.505992 29777 sgd_solver.cpp:106] Iteration 219600, lr = 0.0031375
I0630 13:46:22.641075 29777 solver.cpp:290] Iteration 219700 (6.19784 iter/s, 16.1347s/100 iter), loss = 1.13095
I0630 13:46:22.641096 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 13:46:22.641105 29777 sgd_solver.cpp:106] Iteration 219700, lr = 0.00313438
I0630 13:46:38.729243 29777 solver.cpp:290] Iteration 219800 (6.21593 iter/s, 16.0877s/100 iter), loss = 1.4881
I0630 13:46:38.729352 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 13:46:38.729362 29777 sgd_solver.cpp:106] Iteration 219800, lr = 0.00313125
I0630 13:46:55.131389 29777 solver.cpp:290] Iteration 219900 (6.09697 iter/s, 16.4016s/100 iter), loss = 1.38095
I0630 13:46:55.131438 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 13:46:55.131470 29777 sgd_solver.cpp:106] Iteration 219900, lr = 0.00312813
I0630 13:47:11.296489 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_220000.caffemodel
I0630 13:47:11.318470 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_220000.solverstate
I0630 13:47:11.329566 29777 solver.cpp:354] Sparsity after update:
I0630 13:47:11.331670 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:47:11.331697 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:47:11.331723 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:47:11.331735 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:47:11.331746 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:47:11.331755 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:47:11.331761 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:47:11.331766 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:47:11.331774 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:47:11.331782 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:47:11.331790 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:47:11.331799 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:47:11.331807 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:47:11.332109 29777 solver.cpp:471] Iteration 220000, Testing net (#0)
I0630 13:47:35.500135 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 13:48:17.154284 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.552499
I0630 13:48:17.154381 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.789822
I0630 13:48:17.154389 29777 solver.cpp:544]     Test net output #2: loss = 1.59718 (* 1 = 1.59718 loss)
I0630 13:48:17.325251 29777 solver.cpp:290] Iteration 220000 (1.21667 iter/s, 82.1916s/100 iter), loss = 1.32143
I0630 13:48:17.325276 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 13:48:17.325285 29777 sgd_solver.cpp:106] Iteration 220000, lr = 0.003125
I0630 13:48:33.456585 29777 solver.cpp:290] Iteration 220100 (6.1993 iter/s, 16.1309s/100 iter), loss = 1.28571
I0630 13:48:33.456632 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 13:48:33.456645 29777 sgd_solver.cpp:106] Iteration 220100, lr = 0.00312187
I0630 13:48:49.541900 29777 solver.cpp:290] Iteration 220200 (6.21704 iter/s, 16.0848s/100 iter), loss = 1.04762
I0630 13:48:49.542001 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 13:48:49.542013 29777 sgd_solver.cpp:106] Iteration 220200, lr = 0.00311875
I0630 13:49:05.603376 29777 solver.cpp:290] Iteration 220300 (6.22629 iter/s, 16.0609s/100 iter), loss = 1.07143
I0630 13:49:05.603401 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 13:49:05.603410 29777 sgd_solver.cpp:106] Iteration 220300, lr = 0.00311562
I0630 13:49:21.883144 29777 solver.cpp:290] Iteration 220400 (6.14278 iter/s, 16.2793s/100 iter), loss = 1.29762
I0630 13:49:21.883234 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 13:49:21.883258 29777 sgd_solver.cpp:106] Iteration 220400, lr = 0.0031125
I0630 13:49:38.156378 29777 solver.cpp:290] Iteration 220500 (6.14526 iter/s, 16.2727s/100 iter), loss = 1.03571
I0630 13:49:38.156402 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 13:49:38.156410 29777 sgd_solver.cpp:106] Iteration 220500, lr = 0.00310938
I0630 13:49:54.266692 29777 solver.cpp:290] Iteration 220600 (6.20738 iter/s, 16.1098s/100 iter), loss = 1.34524
I0630 13:49:54.266767 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 13:49:54.266777 29777 sgd_solver.cpp:106] Iteration 220600, lr = 0.00310625
I0630 13:50:10.696987 29777 solver.cpp:290] Iteration 220700 (6.08651 iter/s, 16.4298s/100 iter), loss = 1.2381
I0630 13:50:10.697015 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 13:50:10.697021 29777 sgd_solver.cpp:106] Iteration 220700, lr = 0.00310313
I0630 13:50:26.951617 29777 solver.cpp:290] Iteration 220800 (6.15227 iter/s, 16.2542s/100 iter), loss = 1.42857
I0630 13:50:26.951676 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 13:50:26.951684 29777 sgd_solver.cpp:106] Iteration 220800, lr = 0.0031
I0630 13:50:43.274657 29777 solver.cpp:290] Iteration 220900 (6.1265 iter/s, 16.3225s/100 iter), loss = 1.11905
I0630 13:50:43.274680 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 13:50:43.274688 29777 sgd_solver.cpp:106] Iteration 220900, lr = 0.00309687
I0630 13:50:59.409833 29777 solver.cpp:354] Sparsity after update:
I0630 13:50:59.430179 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:50:59.430199 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:50:59.430209 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:50:59.430213 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:50:59.430217 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:50:59.430227 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:50:59.430233 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:50:59.430238 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:50:59.430241 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:50:59.430245 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:50:59.430249 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:50:59.430253 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:50:59.430258 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:50:59.591084 29777 solver.cpp:290] Iteration 221000 (6.12897 iter/s, 16.316s/100 iter), loss = 0.869048
I0630 13:50:59.591123 29777 solver.cpp:309]     Train net output #0: loss = 0.595238 (* 1 = 0.595238 loss)
I0630 13:50:59.591133 29777 sgd_solver.cpp:106] Iteration 221000, lr = 0.00309375
I0630 13:51:15.891921 29777 solver.cpp:290] Iteration 221100 (6.13484 iter/s, 16.3003s/100 iter), loss = 1.08333
I0630 13:51:15.892012 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 13:51:15.892036 29777 sgd_solver.cpp:106] Iteration 221100, lr = 0.00309062
I0630 13:51:32.004070 29777 solver.cpp:290] Iteration 221200 (6.2067 iter/s, 16.1116s/100 iter), loss = 1.47619
I0630 13:51:32.004189 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 13:51:32.004196 29777 sgd_solver.cpp:106] Iteration 221200, lr = 0.0030875
I0630 13:51:48.274154 29777 solver.cpp:290] Iteration 221300 (6.14646 iter/s, 16.2695s/100 iter), loss = 1.32143
I0630 13:51:48.274183 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 13:51:48.274193 29777 sgd_solver.cpp:106] Iteration 221300, lr = 0.00308438
I0630 13:52:04.353127 29777 solver.cpp:290] Iteration 221400 (6.21948 iter/s, 16.0785s/100 iter), loss = 1.39286
I0630 13:52:04.353202 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 13:52:04.353212 29777 sgd_solver.cpp:106] Iteration 221400, lr = 0.00308125
I0630 13:52:20.444947 29777 solver.cpp:290] Iteration 221500 (6.21454 iter/s, 16.0913s/100 iter), loss = 1.52381
I0630 13:52:20.444972 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 13:52:20.444977 29777 sgd_solver.cpp:106] Iteration 221500, lr = 0.00307812
I0630 13:52:36.667657 29777 solver.cpp:290] Iteration 221600 (6.16438 iter/s, 16.2222s/100 iter), loss = 1.46429
I0630 13:52:36.667742 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 13:52:36.667752 29777 sgd_solver.cpp:106] Iteration 221600, lr = 0.003075
I0630 13:52:52.801128 29777 solver.cpp:290] Iteration 221700 (6.1985 iter/s, 16.1329s/100 iter), loss = 1.16667
I0630 13:52:52.801167 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 13:52:52.801182 29777 sgd_solver.cpp:106] Iteration 221700, lr = 0.00307187
I0630 13:53:08.890921 29777 solver.cpp:290] Iteration 221800 (6.21531 iter/s, 16.0893s/100 iter), loss = 1.14286
I0630 13:53:08.891062 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 13:53:08.891091 29777 sgd_solver.cpp:106] Iteration 221800, lr = 0.00306875
I0630 13:53:24.964521 29777 solver.cpp:290] Iteration 221900 (6.2216 iter/s, 16.073s/100 iter), loss = 1.22619
I0630 13:53:24.964547 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 13:53:24.964555 29777 sgd_solver.cpp:106] Iteration 221900, lr = 0.00306562
I0630 13:53:41.026065 29777 solver.cpp:354] Sparsity after update:
I0630 13:53:41.027973 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:53:41.027992 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:53:41.028007 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:53:41.028012 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:53:41.028015 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:53:41.028020 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:53:41.028025 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:53:41.028030 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:53:41.028036 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:53:41.028041 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:53:41.028046 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:53:41.028053 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:53:41.028057 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:53:41.028267 29777 solver.cpp:471] Iteration 222000, Testing net (#0)
I0630 13:54:06.349931 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 13:54:44.091150 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55232
I0630 13:54:44.091277 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.789541
I0630 13:54:44.091298 29777 solver.cpp:544]     Test net output #2: loss = 1.59314 (* 1 = 1.59314 loss)
I0630 13:54:44.336486 29777 solver.cpp:290] Iteration 222000 (1.25993 iter/s, 79.3698s/100 iter), loss = 1.28571
I0630 13:54:44.336529 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 13:54:44.336539 29777 sgd_solver.cpp:106] Iteration 222000, lr = 0.0030625
I0630 13:55:00.374326 29777 solver.cpp:290] Iteration 222100 (6.23544 iter/s, 16.0374s/100 iter), loss = 1.5119
I0630 13:55:00.374351 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 13:55:00.374358 29777 sgd_solver.cpp:106] Iteration 222100, lr = 0.00305938
I0630 13:55:16.555017 29777 solver.cpp:290] Iteration 222200 (6.18039 iter/s, 16.1802s/100 iter), loss = 1.15476
I0630 13:55:16.555124 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 13:55:16.555135 29777 sgd_solver.cpp:106] Iteration 222200, lr = 0.00305625
I0630 13:55:32.653915 29777 solver.cpp:290] Iteration 222300 (6.21182 iter/s, 16.0984s/100 iter), loss = 1.32143
I0630 13:55:32.653939 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 13:55:32.653947 29777 sgd_solver.cpp:106] Iteration 222300, lr = 0.00305312
I0630 13:55:48.726963 29777 solver.cpp:290] Iteration 222400 (6.22178 iter/s, 16.0726s/100 iter), loss = 1.13095
I0630 13:55:48.727057 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 13:55:48.727068 29777 sgd_solver.cpp:106] Iteration 222400, lr = 0.00305
I0630 13:56:04.870342 29777 solver.cpp:290] Iteration 222500 (6.1947 iter/s, 16.1428s/100 iter), loss = 1.5119
I0630 13:56:04.870364 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 13:56:04.870371 29777 sgd_solver.cpp:106] Iteration 222500, lr = 0.00304687
I0630 13:56:21.287022 29777 solver.cpp:290] Iteration 222600 (6.09154 iter/s, 16.4162s/100 iter), loss = 1.11905
I0630 13:56:21.287127 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 13:56:21.287137 29777 sgd_solver.cpp:106] Iteration 222600, lr = 0.00304375
I0630 13:56:37.480618 29777 solver.cpp:290] Iteration 222700 (6.17549 iter/s, 16.193s/100 iter), loss = 1.34524
I0630 13:56:37.480640 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 13:56:37.480646 29777 sgd_solver.cpp:106] Iteration 222700, lr = 0.00304062
I0630 13:56:53.640102 29777 solver.cpp:290] Iteration 222800 (6.1885 iter/s, 16.159s/100 iter), loss = 0.988095
I0630 13:56:53.640183 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 13:56:53.640194 29777 sgd_solver.cpp:106] Iteration 222800, lr = 0.0030375
I0630 13:57:09.762192 29777 solver.cpp:290] Iteration 222900 (6.20287 iter/s, 16.1216s/100 iter), loss = 1.15476
I0630 13:57:09.762214 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 13:57:09.762221 29777 sgd_solver.cpp:106] Iteration 222900, lr = 0.00303437
I0630 13:57:25.772415 29777 solver.cpp:354] Sparsity after update:
I0630 13:57:25.792842 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 13:57:25.792856 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 13:57:25.792867 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 13:57:25.792870 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 13:57:25.792888 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 13:57:25.792898 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 13:57:25.792906 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 13:57:25.792914 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 13:57:25.792922 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 13:57:25.792929 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 13:57:25.792937 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 13:57:25.792945 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 13:57:25.792953 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 13:57:25.952538 29777 solver.cpp:290] Iteration 223000 (6.1767 iter/s, 16.1899s/100 iter), loss = 1.5
I0630 13:57:25.952599 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 13:57:25.952618 29777 sgd_solver.cpp:106] Iteration 223000, lr = 0.00303125
I0630 13:57:42.085860 29777 solver.cpp:290] Iteration 223100 (6.19854 iter/s, 16.1328s/100 iter), loss = 1.08333
I0630 13:57:42.085887 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 13:57:42.085894 29777 sgd_solver.cpp:106] Iteration 223100, lr = 0.00302813
I0630 13:57:58.206182 29777 solver.cpp:290] Iteration 223200 (6.20353 iter/s, 16.1199s/100 iter), loss = 1.33333
I0630 13:57:58.206301 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 13:57:58.206311 29777 sgd_solver.cpp:106] Iteration 223200, lr = 0.003025
I0630 13:58:14.193364 29777 solver.cpp:290] Iteration 223300 (6.25523 iter/s, 15.9866s/100 iter), loss = 1.44048
I0630 13:58:14.193393 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 13:58:14.193401 29777 sgd_solver.cpp:106] Iteration 223300, lr = 0.00302187
I0630 13:58:30.417757 29777 solver.cpp:290] Iteration 223400 (6.16374 iter/s, 16.2239s/100 iter), loss = 1.2381
I0630 13:58:30.417896 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 13:58:30.417932 29777 sgd_solver.cpp:106] Iteration 223400, lr = 0.00301875
I0630 13:58:47.090749 29777 solver.cpp:290] Iteration 223500 (5.99794 iter/s, 16.6724s/100 iter), loss = 1.36905
I0630 13:58:47.090781 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 13:58:47.090790 29777 sgd_solver.cpp:106] Iteration 223500, lr = 0.00301562
I0630 13:59:03.243475 29777 solver.cpp:290] Iteration 223600 (6.19109 iter/s, 16.1522s/100 iter), loss = 1.16667
I0630 13:59:03.243625 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 13:59:03.243669 29777 sgd_solver.cpp:106] Iteration 223600, lr = 0.0030125
I0630 13:59:19.549912 29777 solver.cpp:290] Iteration 223700 (6.13277 iter/s, 16.3059s/100 iter), loss = 1.19048
I0630 13:59:19.549935 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 13:59:19.549942 29777 sgd_solver.cpp:106] Iteration 223700, lr = 0.00300937
I0630 13:59:36.816723 29777 solver.cpp:290] Iteration 223800 (5.79163 iter/s, 17.2663s/100 iter), loss = 1.5119
I0630 13:59:36.816798 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 13:59:36.816805 29777 sgd_solver.cpp:106] Iteration 223800, lr = 0.00300625
I0630 13:59:53.341405 29777 solver.cpp:290] Iteration 223900 (6.05176 iter/s, 16.5241s/100 iter), loss = 1.32143
I0630 13:59:53.341569 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 13:59:53.341619 29777 sgd_solver.cpp:106] Iteration 223900, lr = 0.00300313
I0630 14:00:09.431151 29777 solver.cpp:354] Sparsity after update:
I0630 14:00:09.432765 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:00:09.432771 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:00:09.432778 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:00:09.432781 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:00:09.432783 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:00:09.432785 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:00:09.432787 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:00:09.432790 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:00:09.432791 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:00:09.432793 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:00:09.432796 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:00:09.432797 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:00:09.432799 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:00:09.432886 29777 solver.cpp:471] Iteration 224000, Testing net (#0)
I0630 14:00:33.393151 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 14:01:18.322438 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55412
I0630 14:01:18.322525 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.789781
I0630 14:01:18.322533 29777 solver.cpp:544]     Test net output #2: loss = 1.59584 (* 1 = 1.59584 loss)
I0630 14:01:18.503085 29777 solver.cpp:290] Iteration 224000 (1.17427 iter/s, 85.1592s/100 iter), loss = 1.5
I0630 14:01:18.503146 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 14:01:18.503171 29777 sgd_solver.cpp:106] Iteration 224000, lr = 0.003
I0630 14:01:34.963623 29777 solver.cpp:290] Iteration 224100 (6.07533 iter/s, 16.46s/100 iter), loss = 1.09524
I0630 14:01:34.963676 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 14:01:34.963698 29777 sgd_solver.cpp:106] Iteration 224100, lr = 0.00299687
I0630 14:01:51.437942 29777 solver.cpp:290] Iteration 224200 (6.07024 iter/s, 16.4738s/100 iter), loss = 1.05952
I0630 14:01:51.438118 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 14:01:51.438150 29777 sgd_solver.cpp:106] Iteration 224200, lr = 0.00299375
I0630 14:02:07.700378 29777 solver.cpp:290] Iteration 224300 (6.14937 iter/s, 16.2618s/100 iter), loss = 1.14286
I0630 14:02:07.700409 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 14:02:07.700418 29777 sgd_solver.cpp:106] Iteration 224300, lr = 0.00299062
I0630 14:02:23.861222 29777 solver.cpp:290] Iteration 224400 (6.18798 iter/s, 16.1604s/100 iter), loss = 1
I0630 14:02:23.861313 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 14:02:23.861325 29777 sgd_solver.cpp:106] Iteration 224400, lr = 0.0029875
I0630 14:02:39.885944 29777 solver.cpp:290] Iteration 224500 (6.24056 iter/s, 16.0242s/100 iter), loss = 1.13095
I0630 14:02:39.885970 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 14:02:39.886010 29777 sgd_solver.cpp:106] Iteration 224500, lr = 0.00298437
I0630 14:02:55.977427 29777 solver.cpp:290] Iteration 224600 (6.21465 iter/s, 16.091s/100 iter), loss = 1.35714
I0630 14:02:55.977536 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 14:02:55.977546 29777 sgd_solver.cpp:106] Iteration 224600, lr = 0.00298125
I0630 14:03:12.288460 29777 solver.cpp:290] Iteration 224700 (6.13103 iter/s, 16.3105s/100 iter), loss = 1.07143
I0630 14:03:12.288481 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 14:03:12.288488 29777 sgd_solver.cpp:106] Iteration 224700, lr = 0.00297813
I0630 14:03:28.737675 29777 solver.cpp:290] Iteration 224800 (6.07949 iter/s, 16.4487s/100 iter), loss = 1.38095
I0630 14:03:28.737767 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 14:03:28.737779 29777 sgd_solver.cpp:106] Iteration 224800, lr = 0.002975
I0630 14:03:44.963948 29777 solver.cpp:290] Iteration 224900 (6.16305 iter/s, 16.2257s/100 iter), loss = 1.21429
I0630 14:03:44.963973 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 14:03:44.963982 29777 sgd_solver.cpp:106] Iteration 224900, lr = 0.00297188
I0630 14:04:00.983098 29777 solver.cpp:354] Sparsity after update:
I0630 14:04:01.003566 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:04:01.003582 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:04:01.003592 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:04:01.003595 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:04:01.003598 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:04:01.003603 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:04:01.003607 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:04:01.003610 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:04:01.003613 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:04:01.003617 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:04:01.003620 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:04:01.003624 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:04:01.003628 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:04:01.161258 29777 solver.cpp:290] Iteration 225000 (6.17404 iter/s, 16.1968s/100 iter), loss = 1.28571
I0630 14:04:01.161284 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 14:04:01.161293 29777 sgd_solver.cpp:106] Iteration 225000, lr = 0.00296875
I0630 14:04:17.421329 29777 solver.cpp:290] Iteration 225100 (6.15022 iter/s, 16.2596s/100 iter), loss = 1.25
I0630 14:04:17.421367 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 14:04:17.421377 29777 sgd_solver.cpp:106] Iteration 225100, lr = 0.00296562
I0630 14:04:33.594876 29777 solver.cpp:290] Iteration 225200 (6.18313 iter/s, 16.173s/100 iter), loss = 1.61905
I0630 14:04:33.595008 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 14:04:33.595041 29777 sgd_solver.cpp:106] Iteration 225200, lr = 0.0029625
I0630 14:04:49.982837 29777 solver.cpp:290] Iteration 225300 (6.10226 iter/s, 16.3874s/100 iter), loss = 1.29762
I0630 14:04:49.982861 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 14:04:49.982870 29777 sgd_solver.cpp:106] Iteration 225300, lr = 0.00295937
I0630 14:05:06.138088 29777 solver.cpp:290] Iteration 225400 (6.19011 iter/s, 16.1548s/100 iter), loss = 1.32143
I0630 14:05:06.138182 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 14:05:06.138193 29777 sgd_solver.cpp:106] Iteration 225400, lr = 0.00295625
I0630 14:05:22.263356 29777 solver.cpp:290] Iteration 225500 (6.20165 iter/s, 16.1247s/100 iter), loss = 1.07143
I0630 14:05:22.263382 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 14:05:22.263391 29777 sgd_solver.cpp:106] Iteration 225500, lr = 0.00295313
I0630 14:05:38.371866 29777 solver.cpp:290] Iteration 225600 (6.20808 iter/s, 16.108s/100 iter), loss = 1.64286
I0630 14:05:38.371964 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 14:05:38.371990 29777 sgd_solver.cpp:106] Iteration 225600, lr = 0.00295
I0630 14:05:54.630784 29777 solver.cpp:290] Iteration 225700 (6.15067 iter/s, 16.2584s/100 iter), loss = 1.10714
I0630 14:05:54.630857 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 14:05:54.630889 29777 sgd_solver.cpp:106] Iteration 225700, lr = 0.00294688
I0630 14:06:10.965904 29777 solver.cpp:290] Iteration 225800 (6.12197 iter/s, 16.3346s/100 iter), loss = 1.4881
I0630 14:06:10.965952 29777 solver.cpp:309]     Train net output #0: loss = 1.71429 (* 1 = 1.71429 loss)
I0630 14:06:10.965960 29777 sgd_solver.cpp:106] Iteration 225800, lr = 0.00294375
I0630 14:06:27.155979 29777 solver.cpp:290] Iteration 225900 (6.17681 iter/s, 16.1896s/100 iter), loss = 1.52381
I0630 14:06:27.156011 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 14:06:27.156020 29777 sgd_solver.cpp:106] Iteration 225900, lr = 0.00294062
I0630 14:06:43.043416 29777 solver.cpp:354] Sparsity after update:
I0630 14:06:43.044865 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:06:43.044872 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:06:43.044879 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:06:43.044881 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:06:43.044883 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:06:43.044885 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:06:43.044888 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:06:43.044889 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:06:43.044891 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:06:43.044893 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:06:43.044895 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:06:43.044898 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:06:43.044899 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:06:43.044986 29777 solver.cpp:471] Iteration 226000, Testing net (#0)
I0630 14:07:10.524691 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 14:07:48.060554 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55216
I0630 14:07:48.060619 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.789581
I0630 14:07:48.060628 29777 solver.cpp:544]     Test net output #2: loss = 1.5993 (* 1 = 1.5993 loss)
I0630 14:07:48.236333 29777 solver.cpp:290] Iteration 226000 (1.23338 iter/s, 81.0781s/100 iter), loss = 1.25
I0630 14:07:48.236361 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 14:07:48.236379 29777 sgd_solver.cpp:106] Iteration 226000, lr = 0.0029375
I0630 14:08:04.358242 29777 solver.cpp:290] Iteration 226100 (6.20292 iter/s, 16.1214s/100 iter), loss = 1.32143
I0630 14:08:04.358265 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 14:08:04.358271 29777 sgd_solver.cpp:106] Iteration 226100, lr = 0.00293437
I0630 14:08:20.464841 29777 solver.cpp:290] Iteration 226200 (6.20881 iter/s, 16.1061s/100 iter), loss = 1.21429
I0630 14:08:20.464912 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 14:08:20.464920 29777 sgd_solver.cpp:106] Iteration 226200, lr = 0.00293125
I0630 14:08:36.583302 29777 solver.cpp:290] Iteration 226300 (6.20426 iter/s, 16.118s/100 iter), loss = 1.90476
I0630 14:08:36.583325 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 14:08:36.583331 29777 sgd_solver.cpp:106] Iteration 226300, lr = 0.00292813
I0630 14:08:52.637400 29777 solver.cpp:290] Iteration 226400 (6.22912 iter/s, 16.0536s/100 iter), loss = 0.940476
I0630 14:08:52.637504 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 14:08:52.637514 29777 sgd_solver.cpp:106] Iteration 226400, lr = 0.002925
I0630 14:09:08.559564 29777 solver.cpp:290] Iteration 226500 (6.28077 iter/s, 15.9216s/100 iter), loss = 1.13095
I0630 14:09:08.559590 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 14:09:08.559599 29777 sgd_solver.cpp:106] Iteration 226500, lr = 0.00292188
I0630 14:09:24.527236 29777 solver.cpp:290] Iteration 226600 (6.26283 iter/s, 15.9672s/100 iter), loss = 1.80952
I0630 14:09:24.527330 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 14:09:24.527341 29777 sgd_solver.cpp:106] Iteration 226600, lr = 0.00291875
I0630 14:09:40.577672 29777 solver.cpp:290] Iteration 226700 (6.23057 iter/s, 16.0499s/100 iter), loss = 1.41667
I0630 14:09:40.577698 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 14:09:40.577705 29777 sgd_solver.cpp:106] Iteration 226700, lr = 0.00291562
I0630 14:09:56.526296 29777 solver.cpp:290] Iteration 226800 (6.27032 iter/s, 15.9482s/100 iter), loss = 1.07143
I0630 14:09:56.526376 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 14:09:56.526386 29777 sgd_solver.cpp:106] Iteration 226800, lr = 0.0029125
I0630 14:10:12.465441 29777 solver.cpp:290] Iteration 226900 (6.27406 iter/s, 15.9386s/100 iter), loss = 0.904762
I0630 14:10:12.465464 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 14:10:12.465471 29777 sgd_solver.cpp:106] Iteration 226900, lr = 0.00290937
I0630 14:10:28.302409 29777 solver.cpp:354] Sparsity after update:
I0630 14:10:28.322885 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:10:28.322901 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:10:28.322911 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:10:28.322914 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:10:28.322918 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:10:28.322921 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:10:28.322924 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:10:28.322928 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:10:28.322932 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:10:28.322934 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:10:28.322937 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:10:28.322940 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:10:28.322943 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:10:28.480382 29777 solver.cpp:290] Iteration 227000 (6.24435 iter/s, 16.0145s/100 iter), loss = 1.46429
I0630 14:10:28.480406 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 14:10:28.480412 29777 sgd_solver.cpp:106] Iteration 227000, lr = 0.00290625
I0630 14:10:44.535065 29777 solver.cpp:290] Iteration 227100 (6.22889 iter/s, 16.0542s/100 iter), loss = 1.15476
I0630 14:10:44.535089 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 14:10:44.535094 29777 sgd_solver.cpp:106] Iteration 227100, lr = 0.00290313
I0630 14:11:00.501235 29777 solver.cpp:290] Iteration 227200 (6.26342 iter/s, 15.9657s/100 iter), loss = 1.29762
I0630 14:11:00.501350 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 14:11:00.501360 29777 sgd_solver.cpp:106] Iteration 227200, lr = 0.0029
I0630 14:11:16.609247 29777 solver.cpp:290] Iteration 227300 (6.2083 iter/s, 16.1075s/100 iter), loss = 1.20238
I0630 14:11:16.609273 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 14:11:16.609282 29777 sgd_solver.cpp:106] Iteration 227300, lr = 0.00289688
I0630 14:11:32.538069 29777 solver.cpp:290] Iteration 227400 (6.27811 iter/s, 15.9284s/100 iter), loss = 1.52381
I0630 14:11:32.538142 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 14:11:32.538151 29777 sgd_solver.cpp:106] Iteration 227400, lr = 0.00289375
I0630 14:11:48.522158 29777 solver.cpp:290] Iteration 227500 (6.25642 iter/s, 15.9836s/100 iter), loss = 1.44048
I0630 14:11:48.522181 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 14:11:48.522189 29777 sgd_solver.cpp:106] Iteration 227500, lr = 0.00289063
I0630 14:12:04.498356 29777 solver.cpp:290] Iteration 227600 (6.25949 iter/s, 15.9757s/100 iter), loss = 1.4881
I0630 14:12:04.498462 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 14:12:04.498472 29777 sgd_solver.cpp:106] Iteration 227600, lr = 0.0028875
I0630 14:12:20.475781 29777 solver.cpp:290] Iteration 227700 (6.25904 iter/s, 15.9769s/100 iter), loss = 1.53571
I0630 14:12:20.475805 29777 solver.cpp:309]     Train net output #0: loss = 1.97619 (* 1 = 1.97619 loss)
I0630 14:12:20.475811 29777 sgd_solver.cpp:106] Iteration 227700, lr = 0.00288437
I0630 14:12:36.495021 29777 solver.cpp:290] Iteration 227800 (6.24267 iter/s, 16.0188s/100 iter), loss = 1.32143
I0630 14:12:36.495128 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 14:12:36.495137 29777 sgd_solver.cpp:106] Iteration 227800, lr = 0.00288125
I0630 14:12:52.567446 29777 solver.cpp:290] Iteration 227900 (6.22204 iter/s, 16.0719s/100 iter), loss = 1.46429
I0630 14:12:52.567471 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 14:12:52.567477 29777 sgd_solver.cpp:106] Iteration 227900, lr = 0.00287812
I0630 14:13:08.387542 29777 solver.cpp:354] Sparsity after update:
I0630 14:13:08.389143 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:13:08.389152 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:13:08.389158 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:13:08.389160 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:13:08.389163 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:13:08.389164 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:13:08.389166 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:13:08.389168 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:13:08.389170 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:13:08.389173 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:13:08.389174 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:13:08.389176 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:13:08.389178 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:13:08.389266 29777 solver.cpp:471] Iteration 228000, Testing net (#0)
I0630 14:13:27.796874 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 14:13:57.913717 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55356
I0630 14:13:57.913851 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.790162
I0630 14:13:57.913861 29777 solver.cpp:544]     Test net output #2: loss = 1.58826 (* 1 = 1.58826 loss)
I0630 14:13:58.111191 29777 solver.cpp:290] Iteration 228000 (1.52574 iter/s, 65.542s/100 iter), loss = 0.821429
I0630 14:13:58.111213 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 14:13:58.111219 29777 sgd_solver.cpp:106] Iteration 228000, lr = 0.002875
I0630 14:14:14.129302 29777 solver.cpp:290] Iteration 228100 (6.24314 iter/s, 16.0176s/100 iter), loss = 1.44048
I0630 14:14:14.129353 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 14:14:14.129369 29777 sgd_solver.cpp:106] Iteration 228100, lr = 0.00287188
I0630 14:14:30.175231 29777 solver.cpp:290] Iteration 228200 (6.2323 iter/s, 16.0454s/100 iter), loss = 1.30952
I0630 14:14:30.175295 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 14:14:30.175307 29777 sgd_solver.cpp:106] Iteration 228200, lr = 0.00286875
I0630 14:14:46.209688 29777 solver.cpp:290] Iteration 228300 (6.23676 iter/s, 16.034s/100 iter), loss = 1.30952
I0630 14:14:46.209712 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 14:14:46.209718 29777 sgd_solver.cpp:106] Iteration 228300, lr = 0.00286562
I0630 14:15:02.258258 29777 solver.cpp:290] Iteration 228400 (6.23126 iter/s, 16.0481s/100 iter), loss = 1.2381
I0630 14:15:02.258329 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 14:15:02.258338 29777 sgd_solver.cpp:106] Iteration 228400, lr = 0.0028625
I0630 14:15:18.168148 29777 solver.cpp:290] Iteration 228500 (6.2856 iter/s, 15.9094s/100 iter), loss = 0.97619
I0630 14:15:18.168198 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 14:15:18.168210 29777 sgd_solver.cpp:106] Iteration 228500, lr = 0.00285937
I0630 14:15:34.096426 29777 solver.cpp:290] Iteration 228600 (6.27833 iter/s, 15.9278s/100 iter), loss = 1.2619
I0630 14:15:34.096523 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 14:15:34.096534 29777 sgd_solver.cpp:106] Iteration 228600, lr = 0.00285625
I0630 14:15:50.189019 29777 solver.cpp:290] Iteration 228700 (6.21425 iter/s, 16.0921s/100 iter), loss = 1.45238
I0630 14:15:50.189061 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 14:15:50.189074 29777 sgd_solver.cpp:106] Iteration 228700, lr = 0.00285312
I0630 14:16:06.190770 29777 solver.cpp:290] Iteration 228800 (6.2495 iter/s, 16.0013s/100 iter), loss = 1.04762
I0630 14:16:06.194934 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 14:16:06.194962 29777 sgd_solver.cpp:106] Iteration 228800, lr = 0.00285
I0630 14:16:22.155745 29777 solver.cpp:290] Iteration 228900 (6.26552 iter/s, 15.9604s/100 iter), loss = 1.10714
I0630 14:16:22.155771 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 14:16:22.155781 29777 sgd_solver.cpp:106] Iteration 228900, lr = 0.00284688
I0630 14:16:38.017102 29777 solver.cpp:354] Sparsity after update:
I0630 14:16:38.037492 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:16:38.037513 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:16:38.037523 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:16:38.037526 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:16:38.037529 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:16:38.037533 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:16:38.037539 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:16:38.037541 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:16:38.037544 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:16:38.037549 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:16:38.037552 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:16:38.037555 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:16:38.037566 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:16:38.196847 29777 solver.cpp:290] Iteration 229000 (6.23417 iter/s, 16.0406s/100 iter), loss = 1.07143
I0630 14:16:38.196873 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 14:16:38.196882 29777 sgd_solver.cpp:106] Iteration 229000, lr = 0.00284375
I0630 14:16:54.311803 29777 solver.cpp:290] Iteration 229100 (6.2056 iter/s, 16.1145s/100 iter), loss = 1.45238
I0630 14:16:54.311839 29777 solver.cpp:309]     Train net output #0: loss = 1.71429 (* 1 = 1.71429 loss)
I0630 14:16:54.311851 29777 sgd_solver.cpp:106] Iteration 229100, lr = 0.00284062
I0630 14:17:10.387051 29777 solver.cpp:290] Iteration 229200 (6.22093 iter/s, 16.0748s/100 iter), loss = 1.44048
I0630 14:17:10.387106 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 14:17:10.387115 29777 sgd_solver.cpp:106] Iteration 229200, lr = 0.0028375
I0630 14:17:26.495076 29777 solver.cpp:290] Iteration 229300 (6.20828 iter/s, 16.1075s/100 iter), loss = 1.17857
I0630 14:17:26.495100 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 14:17:26.495107 29777 sgd_solver.cpp:106] Iteration 229300, lr = 0.00283437
I0630 14:17:42.524621 29777 solver.cpp:290] Iteration 229400 (6.23866 iter/s, 16.0291s/100 iter), loss = 1.61905
I0630 14:17:42.524718 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 14:17:42.524729 29777 sgd_solver.cpp:106] Iteration 229400, lr = 0.00283125
I0630 14:17:58.398046 29777 solver.cpp:290] Iteration 229500 (6.30005 iter/s, 15.8729s/100 iter), loss = 1.02381
I0630 14:17:58.398084 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 14:17:58.398094 29777 sgd_solver.cpp:106] Iteration 229500, lr = 0.00282812
I0630 14:18:14.343631 29777 solver.cpp:290] Iteration 229600 (6.27152 iter/s, 15.9451s/100 iter), loss = 1.35714
I0630 14:18:14.343721 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 14:18:14.343747 29777 sgd_solver.cpp:106] Iteration 229600, lr = 0.002825
I0630 14:18:30.306893 29777 solver.cpp:290] Iteration 229700 (6.26459 iter/s, 15.9627s/100 iter), loss = 1.27381
I0630 14:18:30.306915 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 14:18:30.306923 29777 sgd_solver.cpp:106] Iteration 229700, lr = 0.00282188
I0630 14:18:46.347692 29777 solver.cpp:290] Iteration 229800 (6.23428 iter/s, 16.0403s/100 iter), loss = 1.07143
I0630 14:18:46.347787 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 14:18:46.347797 29777 sgd_solver.cpp:106] Iteration 229800, lr = 0.00281875
I0630 14:19:02.413331 29777 solver.cpp:290] Iteration 229900 (6.22467 iter/s, 16.0651s/100 iter), loss = 1.10714
I0630 14:19:02.413377 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 14:19:02.413388 29777 sgd_solver.cpp:106] Iteration 229900, lr = 0.00281562
I0630 14:19:18.371891 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_230000.caffemodel
I0630 14:19:18.391487 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_230000.solverstate
I0630 14:19:18.401614 29777 solver.cpp:354] Sparsity after update:
I0630 14:19:18.402814 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:19:18.402825 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:19:18.402840 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:19:18.402844 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:19:18.402848 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:19:18.402851 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:19:18.402854 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:19:18.402858 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:19:18.402860 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:19:18.402863 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:19:18.402866 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:19:18.402869 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:19:18.402873 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:19:18.403015 29777 solver.cpp:471] Iteration 230000, Testing net (#0)
I0630 14:19:37.884713 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 14:20:07.174387 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5533
I0630 14:20:07.174502 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.790641
I0630 14:20:07.174511 29777 solver.cpp:544]     Test net output #2: loss = 1.58798 (* 1 = 1.58798 loss)
I0630 14:20:07.351311 29777 solver.cpp:290] Iteration 230000 (1.53997 iter/s, 64.9362s/100 iter), loss = 1.15476
I0630 14:20:07.351335 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 14:20:07.351341 29777 sgd_solver.cpp:106] Iteration 230000, lr = 0.0028125
I0630 14:20:23.326033 29777 solver.cpp:290] Iteration 230100 (6.26007 iter/s, 15.9743s/100 iter), loss = 1.04762
I0630 14:20:23.326056 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 14:20:23.326063 29777 sgd_solver.cpp:106] Iteration 230100, lr = 0.00280937
I0630 14:20:39.287045 29777 solver.cpp:290] Iteration 230200 (6.26545 iter/s, 15.9606s/100 iter), loss = 1.5119
I0630 14:20:39.287140 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 14:20:39.287151 29777 sgd_solver.cpp:106] Iteration 230200, lr = 0.00280625
I0630 14:20:55.339983 29777 solver.cpp:290] Iteration 230300 (6.2296 iter/s, 16.0524s/100 iter), loss = 1.28571
I0630 14:20:55.340010 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 14:20:55.340016 29777 sgd_solver.cpp:106] Iteration 230300, lr = 0.00280312
I0630 14:21:11.385181 29777 solver.cpp:290] Iteration 230400 (6.23257 iter/s, 16.0447s/100 iter), loss = 1.5
I0630 14:21:11.385277 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 14:21:11.385288 29777 sgd_solver.cpp:106] Iteration 230400, lr = 0.0028
I0630 14:21:27.379279 29777 solver.cpp:290] Iteration 230500 (6.25251 iter/s, 15.9936s/100 iter), loss = 1.22619
I0630 14:21:27.379308 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 14:21:27.379318 29777 sgd_solver.cpp:106] Iteration 230500, lr = 0.00279688
I0630 14:21:43.343636 29777 solver.cpp:290] Iteration 230600 (6.26414 iter/s, 15.9639s/100 iter), loss = 1.21429
I0630 14:21:43.343742 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 14:21:43.343765 29777 sgd_solver.cpp:106] Iteration 230600, lr = 0.00279375
I0630 14:21:59.314690 29777 solver.cpp:290] Iteration 230700 (6.26154 iter/s, 15.9705s/100 iter), loss = 1.2381
I0630 14:21:59.314716 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 14:21:59.314725 29777 sgd_solver.cpp:106] Iteration 230700, lr = 0.00279062
I0630 14:22:15.280571 29777 solver.cpp:290] Iteration 230800 (6.26354 iter/s, 15.9654s/100 iter), loss = 1
I0630 14:22:15.280654 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 14:22:15.280663 29777 sgd_solver.cpp:106] Iteration 230800, lr = 0.0027875
I0630 14:22:31.295322 29777 solver.cpp:290] Iteration 230900 (6.24445 iter/s, 16.0142s/100 iter), loss = 1.10714
I0630 14:22:31.295346 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 14:22:31.295353 29777 sgd_solver.cpp:106] Iteration 230900, lr = 0.00278437
I0630 14:22:47.150323 29777 solver.cpp:354] Sparsity after update:
I0630 14:22:47.171025 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:22:47.171070 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:22:47.171085 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:22:47.171094 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:22:47.171099 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:22:47.171103 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:22:47.171106 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:22:47.171110 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:22:47.171119 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:22:47.171123 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:22:47.171128 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:22:47.171131 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:22:47.171135 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:22:47.331900 29777 solver.cpp:290] Iteration 231000 (6.23593 iter/s, 16.0361s/100 iter), loss = 1.34524
I0630 14:22:47.331928 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 14:22:47.331943 29777 sgd_solver.cpp:106] Iteration 231000, lr = 0.00278125
I0630 14:23:03.464542 29777 solver.cpp:290] Iteration 231100 (6.19879 iter/s, 16.1322s/100 iter), loss = 1.25
I0630 14:23:03.464565 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 14:23:03.464572 29777 sgd_solver.cpp:106] Iteration 231100, lr = 0.00277812
I0630 14:23:19.498975 29777 solver.cpp:290] Iteration 231200 (6.23676 iter/s, 16.034s/100 iter), loss = 1.53571
I0630 14:23:19.499054 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 14:23:19.499197 29777 sgd_solver.cpp:106] Iteration 231200, lr = 0.002775
I0630 14:23:35.583425 29777 solver.cpp:290] Iteration 231300 (6.21738 iter/s, 16.0839s/100 iter), loss = 2.04762
I0630 14:23:35.583449 29777 solver.cpp:309]     Train net output #0: loss = 2.42857 (* 1 = 2.42857 loss)
I0630 14:23:35.583458 29777 sgd_solver.cpp:106] Iteration 231300, lr = 0.00277188
I0630 14:23:51.647315 29777 solver.cpp:290] Iteration 231400 (6.22532 iter/s, 16.0634s/100 iter), loss = 1.36905
I0630 14:23:51.647421 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 14:23:51.647431 29777 sgd_solver.cpp:106] Iteration 231400, lr = 0.00276875
I0630 14:24:07.650107 29777 solver.cpp:290] Iteration 231500 (6.24912 iter/s, 16.0022s/100 iter), loss = 0.988095
I0630 14:24:07.650130 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 14:24:07.650137 29777 sgd_solver.cpp:106] Iteration 231500, lr = 0.00276563
I0630 14:24:23.729573 29777 solver.cpp:290] Iteration 231600 (6.21929 iter/s, 16.079s/100 iter), loss = 1.36905
I0630 14:24:23.729650 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 14:24:23.729656 29777 sgd_solver.cpp:106] Iteration 231600, lr = 0.0027625
I0630 14:24:39.837654 29777 solver.cpp:290] Iteration 231700 (6.20827 iter/s, 16.1076s/100 iter), loss = 1.46429
I0630 14:24:39.837715 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 14:24:39.837729 29777 sgd_solver.cpp:106] Iteration 231700, lr = 0.00275937
I0630 14:24:55.817275 29777 solver.cpp:290] Iteration 231800 (6.25817 iter/s, 15.9791s/100 iter), loss = 1.69048
I0630 14:24:55.817320 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 14:24:55.817328 29777 sgd_solver.cpp:106] Iteration 231800, lr = 0.00275625
I0630 14:25:11.968683 29777 solver.cpp:290] Iteration 231900 (6.1916 iter/s, 16.1509s/100 iter), loss = 1.22619
I0630 14:25:11.968704 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 14:25:11.968711 29777 sgd_solver.cpp:106] Iteration 231900, lr = 0.00275312
I0630 14:25:27.819828 29777 solver.cpp:354] Sparsity after update:
I0630 14:25:27.821429 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:25:27.821437 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:25:27.821447 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:25:27.821452 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:25:27.821456 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:25:27.821461 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:25:27.821465 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:25:27.821468 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:25:27.821472 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:25:27.821476 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:25:27.821480 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:25:27.821485 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:25:27.821490 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:25:27.821580 29777 solver.cpp:471] Iteration 232000, Testing net (#0)
I0630 14:25:47.597570 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 14:26:16.374560 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55464
I0630 14:26:16.374610 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.791321
I0630 14:26:16.374619 29777 solver.cpp:544]     Test net output #2: loss = 1.58766 (* 1 = 1.58766 loss)
I0630 14:26:16.545650 29777 solver.cpp:290] Iteration 232000 (1.54858 iter/s, 64.5752s/100 iter), loss = 1.27381
I0630 14:26:16.545672 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 14:26:16.545680 29777 sgd_solver.cpp:106] Iteration 232000, lr = 0.00275
I0630 14:26:31.881989 29777 solver.cpp:290] Iteration 232100 (6.52065 iter/s, 15.3359s/100 iter), loss = 1.05952
I0630 14:26:31.882014 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 14:26:31.882020 29777 sgd_solver.cpp:106] Iteration 232100, lr = 0.00274688
I0630 14:26:47.830440 29777 solver.cpp:290] Iteration 232200 (6.27038 iter/s, 15.948s/100 iter), loss = 1.29762
I0630 14:26:47.830543 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 14:26:47.830554 29777 sgd_solver.cpp:106] Iteration 232200, lr = 0.00274375
I0630 14:27:03.776324 29777 solver.cpp:290] Iteration 232300 (6.27142 iter/s, 15.9453s/100 iter), loss = 1.52381
I0630 14:27:03.776350 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 14:27:03.776360 29777 sgd_solver.cpp:106] Iteration 232300, lr = 0.00274063
I0630 14:27:19.784111 29777 solver.cpp:290] Iteration 232400 (6.24714 iter/s, 16.0073s/100 iter), loss = 1.46429
I0630 14:27:19.784185 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 14:27:19.784193 29777 sgd_solver.cpp:106] Iteration 232400, lr = 0.0027375
I0630 14:27:35.672462 29777 solver.cpp:290] Iteration 232500 (6.29412 iter/s, 15.8878s/100 iter), loss = 1.25
I0630 14:27:35.672490 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 14:27:35.672499 29777 sgd_solver.cpp:106] Iteration 232500, lr = 0.00273437
I0630 14:27:51.581817 29777 solver.cpp:290] Iteration 232600 (6.28579 iter/s, 15.9089s/100 iter), loss = 1.30952
I0630 14:27:51.581892 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 14:27:51.581900 29777 sgd_solver.cpp:106] Iteration 232600, lr = 0.00273125
I0630 14:28:07.568176 29777 solver.cpp:290] Iteration 232700 (6.25554 iter/s, 15.9858s/100 iter), loss = 1.38095
I0630 14:28:07.568200 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 14:28:07.568210 29777 sgd_solver.cpp:106] Iteration 232700, lr = 0.00272812
I0630 14:28:23.643311 29777 solver.cpp:290] Iteration 232800 (6.22097 iter/s, 16.0747s/100 iter), loss = 1.27381
I0630 14:28:23.643432 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 14:28:23.643447 29777 sgd_solver.cpp:106] Iteration 232800, lr = 0.002725
I0630 14:28:39.788221 29777 solver.cpp:290] Iteration 232900 (6.19412 iter/s, 16.1443s/100 iter), loss = 1.32143
I0630 14:28:39.788246 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 14:28:39.788252 29777 sgd_solver.cpp:106] Iteration 232900, lr = 0.00272187
I0630 14:28:55.702858 29777 solver.cpp:354] Sparsity after update:
I0630 14:28:55.722939 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:28:55.722955 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:28:55.722966 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:28:55.722970 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:28:55.722973 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:28:55.722976 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:28:55.722980 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:28:55.722983 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:28:55.722986 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:28:55.722990 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:28:55.722993 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:28:55.722996 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:28:55.723006 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:28:55.878744 29777 solver.cpp:290] Iteration 233000 (6.21502 iter/s, 16.0901s/100 iter), loss = 1.42857
I0630 14:28:55.878770 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 14:28:55.878779 29777 sgd_solver.cpp:106] Iteration 233000, lr = 0.00271875
I0630 14:29:11.883605 29777 solver.cpp:290] Iteration 233100 (6.24828 iter/s, 16.0044s/100 iter), loss = 1.04762
I0630 14:29:11.883628 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 14:29:11.883635 29777 sgd_solver.cpp:106] Iteration 233100, lr = 0.00271563
I0630 14:29:27.983646 29777 solver.cpp:290] Iteration 233200 (6.21134 iter/s, 16.0996s/100 iter), loss = 1.2381
I0630 14:29:27.983741 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 14:29:27.983752 29777 sgd_solver.cpp:106] Iteration 233200, lr = 0.0027125
I0630 14:29:43.978220 29777 solver.cpp:290] Iteration 233300 (6.25233 iter/s, 15.994s/100 iter), loss = 1.0119
I0630 14:29:43.978243 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 14:29:43.978250 29777 sgd_solver.cpp:106] Iteration 233300, lr = 0.00270937
I0630 14:29:59.952394 29777 solver.cpp:290] Iteration 233400 (6.26029 iter/s, 15.9737s/100 iter), loss = 1.21429
I0630 14:29:59.952497 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 14:29:59.952507 29777 sgd_solver.cpp:106] Iteration 233400, lr = 0.00270625
I0630 14:30:16.050669 29777 solver.cpp:290] Iteration 233500 (6.21205 iter/s, 16.0977s/100 iter), loss = 1.19048
I0630 14:30:16.050694 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 14:30:16.050703 29777 sgd_solver.cpp:106] Iteration 233500, lr = 0.00270312
I0630 14:30:32.086611 29777 solver.cpp:290] Iteration 233600 (6.23617 iter/s, 16.0355s/100 iter), loss = 1.25
I0630 14:30:32.086719 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 14:30:32.086729 29777 sgd_solver.cpp:106] Iteration 233600, lr = 0.0027
I0630 14:30:48.121155 29777 solver.cpp:290] Iteration 233700 (6.23675 iter/s, 16.034s/100 iter), loss = 1.20238
I0630 14:30:48.121182 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 14:30:48.121191 29777 sgd_solver.cpp:106] Iteration 233700, lr = 0.00269687
I0630 14:31:04.225577 29777 solver.cpp:290] Iteration 233800 (6.20965 iter/s, 16.104s/100 iter), loss = 1.08333
I0630 14:31:04.225649 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 14:31:04.225658 29777 sgd_solver.cpp:106] Iteration 233800, lr = 0.00269375
I0630 14:31:20.323796 29777 solver.cpp:290] Iteration 233900 (6.21207 iter/s, 16.0977s/100 iter), loss = 1.2619
I0630 14:31:20.323848 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 14:31:20.323869 29777 sgd_solver.cpp:106] Iteration 233900, lr = 0.00269063
I0630 14:31:36.147076 29777 solver.cpp:354] Sparsity after update:
I0630 14:31:36.148412 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:31:36.148419 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:31:36.148427 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:31:36.148429 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:31:36.148432 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:31:36.148434 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:31:36.148437 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:31:36.148438 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:31:36.148440 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:31:36.148442 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:31:36.148443 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:31:36.148445 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:31:36.148447 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:31:36.148533 29777 solver.cpp:471] Iteration 234000, Testing net (#0)
I0630 14:31:55.990288 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 14:32:28.569995 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5566
I0630 14:32:28.570132 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.790321
I0630 14:32:28.570154 29777 solver.cpp:544]     Test net output #2: loss = 1.58958 (* 1 = 1.58958 loss)
I0630 14:32:28.802414 29777 solver.cpp:290] Iteration 234000 (1.46035 iter/s, 68.4767s/100 iter), loss = 0.869048
I0630 14:32:28.802508 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 14:32:28.802546 29777 sgd_solver.cpp:106] Iteration 234000, lr = 0.0026875
I0630 14:32:44.449003 29777 solver.cpp:290] Iteration 234100 (6.39138 iter/s, 15.6461s/100 iter), loss = 1.52381
I0630 14:32:44.449039 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 14:32:44.449049 29777 sgd_solver.cpp:106] Iteration 234100, lr = 0.00268438
I0630 14:33:00.484014 29777 solver.cpp:290] Iteration 234200 (6.23654 iter/s, 16.0345s/100 iter), loss = 1.71429
I0630 14:33:00.484180 29777 solver.cpp:309]     Train net output #0: loss = 1.95238 (* 1 = 1.95238 loss)
I0630 14:33:00.484230 29777 sgd_solver.cpp:106] Iteration 234200, lr = 0.00268125
I0630 14:33:16.716459 29777 solver.cpp:290] Iteration 234300 (6.16073 iter/s, 16.2318s/100 iter), loss = 1.41667
I0630 14:33:16.716485 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 14:33:16.716493 29777 sgd_solver.cpp:106] Iteration 234300, lr = 0.00267812
I0630 14:33:32.779314 29777 solver.cpp:290] Iteration 234400 (6.22572 iter/s, 16.0624s/100 iter), loss = 1.2381
I0630 14:33:32.779388 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 14:33:32.779397 29777 sgd_solver.cpp:106] Iteration 234400, lr = 0.002675
I0630 14:33:48.848537 29777 solver.cpp:290] Iteration 234500 (6.22328 iter/s, 16.0687s/100 iter), loss = 0.988095
I0630 14:33:48.848582 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 14:33:48.848606 29777 sgd_solver.cpp:106] Iteration 234500, lr = 0.00267187
I0630 14:34:04.987484 29777 solver.cpp:290] Iteration 234600 (6.19638 iter/s, 16.1385s/100 iter), loss = 1.38095
I0630 14:34:04.987578 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 14:34:04.987589 29777 sgd_solver.cpp:106] Iteration 234600, lr = 0.00266875
I0630 14:34:21.123479 29777 solver.cpp:290] Iteration 234700 (6.19753 iter/s, 16.1355s/100 iter), loss = 1.79762
I0630 14:34:21.123502 29777 solver.cpp:309]     Train net output #0: loss = 2.14286 (* 1 = 2.14286 loss)
I0630 14:34:21.123509 29777 sgd_solver.cpp:106] Iteration 234700, lr = 0.00266563
I0630 14:34:37.139156 29777 solver.cpp:290] Iteration 234800 (6.24406 iter/s, 16.0152s/100 iter), loss = 1.44048
I0630 14:34:37.139261 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 14:34:37.139272 29777 sgd_solver.cpp:106] Iteration 234800, lr = 0.0026625
I0630 14:34:53.284138 29777 solver.cpp:290] Iteration 234900 (6.19408 iter/s, 16.1444s/100 iter), loss = 1.32143
I0630 14:34:53.284164 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 14:34:53.284178 29777 sgd_solver.cpp:106] Iteration 234900, lr = 0.00265938
I0630 14:35:09.306536 29777 solver.cpp:354] Sparsity after update:
I0630 14:35:09.326903 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:35:09.326918 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:35:09.326927 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:35:09.326930 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:35:09.326932 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:35:09.326934 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:35:09.326936 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:35:09.326938 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:35:09.326941 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:35:09.326942 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:35:09.326944 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:35:09.326948 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:35:09.326951 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:35:09.479795 29777 solver.cpp:290] Iteration 235000 (6.17467 iter/s, 16.1952s/100 iter), loss = 1.30952
I0630 14:35:09.479820 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 14:35:09.479828 29777 sgd_solver.cpp:106] Iteration 235000, lr = 0.00265625
I0630 14:35:25.864646 29777 solver.cpp:290] Iteration 235100 (6.10338 iter/s, 16.3844s/100 iter), loss = 1.41667
I0630 14:35:25.864672 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 14:35:25.864681 29777 sgd_solver.cpp:106] Iteration 235100, lr = 0.00265312
I0630 14:35:41.923863 29777 solver.cpp:290] Iteration 235200 (6.22713 iter/s, 16.0588s/100 iter), loss = 1.02381
I0630 14:35:41.923933 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 14:35:41.923941 29777 sgd_solver.cpp:106] Iteration 235200, lr = 0.00265
I0630 14:35:57.901494 29777 solver.cpp:290] Iteration 235300 (6.25895 iter/s, 15.9771s/100 iter), loss = 1.34524
I0630 14:35:57.901517 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 14:35:57.901523 29777 sgd_solver.cpp:106] Iteration 235300, lr = 0.00264687
I0630 14:36:13.973537 29777 solver.cpp:290] Iteration 235400 (6.22216 iter/s, 16.0716s/100 iter), loss = 1.09524
I0630 14:36:13.973654 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 14:36:13.973664 29777 sgd_solver.cpp:106] Iteration 235400, lr = 0.00264375
I0630 14:36:30.082515 29777 solver.cpp:290] Iteration 235500 (6.20794 iter/s, 16.1084s/100 iter), loss = 1.60714
I0630 14:36:30.082566 29777 solver.cpp:309]     Train net output #0: loss = 1.88095 (* 1 = 1.88095 loss)
I0630 14:36:30.082581 29777 sgd_solver.cpp:106] Iteration 235500, lr = 0.00264063
I0630 14:36:46.168768 29777 solver.cpp:290] Iteration 235600 (6.21668 iter/s, 16.0858s/100 iter), loss = 1.2619
I0630 14:36:46.169162 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 14:36:46.169185 29777 sgd_solver.cpp:106] Iteration 235600, lr = 0.0026375
I0630 14:37:02.187577 29777 solver.cpp:290] Iteration 235700 (6.24298 iter/s, 16.018s/100 iter), loss = 1.16667
I0630 14:37:02.187604 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 14:37:02.187614 29777 sgd_solver.cpp:106] Iteration 235700, lr = 0.00263438
I0630 14:37:18.265444 29777 solver.cpp:290] Iteration 235800 (6.21991 iter/s, 16.0774s/100 iter), loss = 1.59524
I0630 14:37:18.265529 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 14:37:18.265537 29777 sgd_solver.cpp:106] Iteration 235800, lr = 0.00263125
I0630 14:37:34.499179 29777 solver.cpp:290] Iteration 235900 (6.16021 iter/s, 16.2332s/100 iter), loss = 1.2381
I0630 14:37:34.499203 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 14:37:34.499212 29777 sgd_solver.cpp:106] Iteration 235900, lr = 0.00262812
I0630 14:37:50.411195 29777 solver.cpp:354] Sparsity after update:
I0630 14:37:50.412479 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:37:50.412487 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:37:50.412493 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:37:50.412497 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:37:50.412498 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:37:50.412500 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:37:50.412503 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:37:50.412504 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:37:50.412506 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:37:50.412508 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:37:50.412509 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:37:50.412511 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:37:50.412513 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:37:50.412607 29777 solver.cpp:471] Iteration 236000, Testing net (#0)
I0630 14:38:14.446404 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 14:38:50.121421 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55394
I0630 14:38:50.121525 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.789881
I0630 14:38:50.121534 29777 solver.cpp:544]     Test net output #2: loss = 1.58258 (* 1 = 1.58258 loss)
I0630 14:38:50.299664 29777 solver.cpp:290] Iteration 236000 (1.31929 iter/s, 75.7984s/100 iter), loss = 1.58333
I0630 14:38:50.299685 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 14:38:50.299691 29777 sgd_solver.cpp:106] Iteration 236000, lr = 0.002625
I0630 14:39:05.699781 29777 solver.cpp:290] Iteration 236100 (6.49365 iter/s, 15.3997s/100 iter), loss = 1.5
I0630 14:39:05.699808 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 14:39:05.699817 29777 sgd_solver.cpp:106] Iteration 236100, lr = 0.00262187
I0630 14:39:21.806182 29777 solver.cpp:290] Iteration 236200 (6.20889 iter/s, 16.1059s/100 iter), loss = 1.0119
I0630 14:39:21.806236 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 14:39:21.806244 29777 sgd_solver.cpp:106] Iteration 236200, lr = 0.00261875
I0630 14:39:37.771760 29777 solver.cpp:290] Iteration 236300 (6.26367 iter/s, 15.9651s/100 iter), loss = 1.65476
I0630 14:39:37.771783 29777 solver.cpp:309]     Train net output #0: loss = 1.71429 (* 1 = 1.71429 loss)
I0630 14:39:37.771790 29777 sgd_solver.cpp:106] Iteration 236300, lr = 0.00261563
I0630 14:39:53.797937 29777 solver.cpp:290] Iteration 236400 (6.23997 iter/s, 16.0257s/100 iter), loss = 1.32143
I0630 14:39:53.798032 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 14:39:53.798043 29777 sgd_solver.cpp:106] Iteration 236400, lr = 0.0026125
I0630 14:40:09.941553 29777 solver.cpp:290] Iteration 236500 (6.19461 iter/s, 16.1431s/100 iter), loss = 1.59524
I0630 14:40:09.941578 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 14:40:09.941587 29777 sgd_solver.cpp:106] Iteration 236500, lr = 0.00260938
I0630 14:40:26.005620 29777 solver.cpp:290] Iteration 236600 (6.22526 iter/s, 16.0636s/100 iter), loss = 1.04762
I0630 14:40:26.005735 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 14:40:26.005759 29777 sgd_solver.cpp:106] Iteration 236600, lr = 0.00260625
I0630 14:40:41.994613 29777 solver.cpp:290] Iteration 236700 (6.25452 iter/s, 15.9884s/100 iter), loss = 1.38095
I0630 14:40:41.994639 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 14:40:41.994648 29777 sgd_solver.cpp:106] Iteration 236700, lr = 0.00260312
I0630 14:40:58.061368 29777 solver.cpp:290] Iteration 236800 (6.22421 iter/s, 16.0663s/100 iter), loss = 1.33333
I0630 14:40:58.061462 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 14:40:58.061473 29777 sgd_solver.cpp:106] Iteration 236800, lr = 0.0026
I0630 14:41:14.146313 29777 solver.cpp:290] Iteration 236900 (6.2172 iter/s, 16.0844s/100 iter), loss = 1.44048
I0630 14:41:14.146338 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 14:41:14.146345 29777 sgd_solver.cpp:106] Iteration 236900, lr = 0.00259687
I0630 14:41:30.000206 29777 solver.cpp:354] Sparsity after update:
I0630 14:41:30.020542 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:41:30.020560 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:41:30.020568 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:41:30.020571 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:41:30.020573 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:41:30.020576 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:41:30.020577 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:41:30.020579 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:41:30.020581 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:41:30.020583 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:41:30.020586 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:41:30.020587 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:41:30.020589 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:41:30.174350 29777 solver.cpp:290] Iteration 237000 (6.23925 iter/s, 16.0276s/100 iter), loss = 1.0119
I0630 14:41:30.174372 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 14:41:30.174379 29777 sgd_solver.cpp:106] Iteration 237000, lr = 0.00259375
I0630 14:41:46.240965 29777 solver.cpp:290] Iteration 237100 (6.22427 iter/s, 16.0662s/100 iter), loss = 0.988095
I0630 14:41:46.240993 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 14:41:46.241003 29777 sgd_solver.cpp:106] Iteration 237100, lr = 0.00259063
I0630 14:42:02.265977 29777 solver.cpp:290] Iteration 237200 (6.24043 iter/s, 16.0245s/100 iter), loss = 1.04762
I0630 14:42:02.266068 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 14:42:02.266079 29777 sgd_solver.cpp:106] Iteration 237200, lr = 0.0025875
I0630 14:42:18.431689 29777 solver.cpp:290] Iteration 237300 (6.18614 iter/s, 16.1652s/100 iter), loss = 1.53571
I0630 14:42:18.431710 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 14:42:18.431717 29777 sgd_solver.cpp:106] Iteration 237300, lr = 0.00258437
I0630 14:42:34.444743 29777 solver.cpp:290] Iteration 237400 (6.24509 iter/s, 16.0126s/100 iter), loss = 0.880952
I0630 14:42:34.444824 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 14:42:34.444835 29777 sgd_solver.cpp:106] Iteration 237400, lr = 0.00258125
I0630 14:42:50.738234 29777 solver.cpp:290] Iteration 237500 (6.13762 iter/s, 16.293s/100 iter), loss = 1.10714
I0630 14:42:50.738256 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 14:42:50.738263 29777 sgd_solver.cpp:106] Iteration 237500, lr = 0.00257812
I0630 14:43:06.684914 29777 solver.cpp:290] Iteration 237600 (6.27108 iter/s, 15.9462s/100 iter), loss = 1.33333
I0630 14:43:06.685006 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 14:43:06.685016 29777 sgd_solver.cpp:106] Iteration 237600, lr = 0.002575
I0630 14:43:22.977571 29777 solver.cpp:290] Iteration 237700 (6.13794 iter/s, 16.2921s/100 iter), loss = 1.33333
I0630 14:43:22.977624 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 14:43:22.977646 29777 sgd_solver.cpp:106] Iteration 237700, lr = 0.00257187
I0630 14:43:39.155689 29777 solver.cpp:290] Iteration 237800 (6.18138 iter/s, 16.1776s/100 iter), loss = 1.13095
I0630 14:43:39.155807 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 14:43:39.155829 29777 sgd_solver.cpp:106] Iteration 237800, lr = 0.00256875
I0630 14:43:55.281255 29777 solver.cpp:290] Iteration 237900 (6.20155 iter/s, 16.125s/100 iter), loss = 1.20238
I0630 14:43:55.281275 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 14:43:55.281283 29777 sgd_solver.cpp:106] Iteration 237900, lr = 0.00256562
I0630 14:44:11.116421 29777 solver.cpp:354] Sparsity after update:
I0630 14:44:11.117875 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:44:11.117885 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:44:11.117899 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:44:11.117903 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:44:11.117908 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:44:11.117913 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:44:11.117918 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:44:11.117924 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:44:11.117929 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:44:11.117934 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:44:11.117938 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:44:11.117943 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:44:11.117946 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:44:11.118041 29777 solver.cpp:471] Iteration 238000, Testing net (#0)
I0630 14:44:36.907836 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 14:45:15.684857 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.553599
I0630 14:45:15.684913 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.791081
I0630 14:45:15.684922 29777 solver.cpp:544]     Test net output #2: loss = 1.58548 (* 1 = 1.58548 loss)
I0630 14:45:15.871109 29777 solver.cpp:290] Iteration 238000 (1.24089 iter/s, 80.5876s/100 iter), loss = 1.04762
I0630 14:45:15.871141 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 14:45:15.871152 29777 sgd_solver.cpp:106] Iteration 238000, lr = 0.0025625
I0630 14:45:31.122153 29777 solver.cpp:290] Iteration 238100 (6.55712 iter/s, 15.2506s/100 iter), loss = 1.32143
I0630 14:45:31.122177 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 14:45:31.122187 29777 sgd_solver.cpp:106] Iteration 238100, lr = 0.00255938
I0630 14:45:47.284894 29777 solver.cpp:290] Iteration 238200 (6.18725 iter/s, 16.1623s/100 iter), loss = 1.38095
I0630 14:45:47.285002 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 14:45:47.285012 29777 sgd_solver.cpp:106] Iteration 238200, lr = 0.00255625
I0630 14:46:03.638743 29777 solver.cpp:290] Iteration 238300 (6.11498 iter/s, 16.3533s/100 iter), loss = 1.57143
I0630 14:46:03.638767 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 14:46:03.638773 29777 sgd_solver.cpp:106] Iteration 238300, lr = 0.00255312
I0630 14:46:19.826336 29777 solver.cpp:290] Iteration 238400 (6.17775 iter/s, 16.1871s/100 iter), loss = 1.4881
I0630 14:46:19.826436 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 14:46:19.826445 29777 sgd_solver.cpp:106] Iteration 238400, lr = 0.00255
I0630 14:46:35.919262 29777 solver.cpp:290] Iteration 238500 (6.21412 iter/s, 16.0924s/100 iter), loss = 0.988095
I0630 14:46:35.919287 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 14:46:35.919293 29777 sgd_solver.cpp:106] Iteration 238500, lr = 0.00254687
I0630 14:46:52.234625 29777 solver.cpp:290] Iteration 238600 (6.12937 iter/s, 16.3149s/100 iter), loss = 1.42857
I0630 14:46:52.234748 29777 solver.cpp:309]     Train net output #0: loss = 1.90476 (* 1 = 1.90476 loss)
I0630 14:46:52.234762 29777 sgd_solver.cpp:106] Iteration 238600, lr = 0.00254375
I0630 14:47:08.378088 29777 solver.cpp:290] Iteration 238700 (6.19468 iter/s, 16.1429s/100 iter), loss = 1.22619
I0630 14:47:08.378181 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 14:47:08.378216 29777 sgd_solver.cpp:106] Iteration 238700, lr = 0.00254062
I0630 14:47:24.616417 29777 solver.cpp:290] Iteration 238800 (6.15847 iter/s, 16.2378s/100 iter), loss = 1.29762
I0630 14:47:24.616484 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 14:47:24.616495 29777 sgd_solver.cpp:106] Iteration 238800, lr = 0.0025375
I0630 14:47:40.681682 29777 solver.cpp:290] Iteration 238900 (6.22481 iter/s, 16.0648s/100 iter), loss = 1.17857
I0630 14:47:40.681706 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 14:47:40.681713 29777 sgd_solver.cpp:106] Iteration 238900, lr = 0.00253438
I0630 14:47:56.669863 29777 solver.cpp:354] Sparsity after update:
I0630 14:47:56.690438 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:47:56.690603 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:47:56.690688 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:47:56.690804 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:47:56.690912 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:47:56.691030 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:47:56.691141 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:47:56.691249 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:47:56.691329 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:47:56.691355 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:47:56.691380 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:47:56.691402 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:47:56.691422 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:47:56.908994 29777 solver.cpp:290] Iteration 239000 (6.16263 iter/s, 16.2268s/100 iter), loss = 1.02381
I0630 14:47:56.909019 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 14:47:56.909030 29777 sgd_solver.cpp:106] Iteration 239000, lr = 0.00253125
I0630 14:48:13.330937 29777 solver.cpp:290] Iteration 239100 (6.08959 iter/s, 16.4215s/100 iter), loss = 1.4881
I0630 14:48:13.330960 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 14:48:13.330967 29777 sgd_solver.cpp:106] Iteration 239100, lr = 0.00252812
I0630 14:48:29.502120 29777 solver.cpp:290] Iteration 239200 (6.18402 iter/s, 16.1707s/100 iter), loss = 1.07143
I0630 14:48:29.502209 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 14:48:29.502233 29777 sgd_solver.cpp:106] Iteration 239200, lr = 0.002525
I0630 14:48:45.489085 29777 solver.cpp:290] Iteration 239300 (6.2553 iter/s, 15.9865s/100 iter), loss = 1.32143
I0630 14:48:45.489107 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 14:48:45.489114 29777 sgd_solver.cpp:106] Iteration 239300, lr = 0.00252187
I0630 14:49:01.535408 29777 solver.cpp:290] Iteration 239400 (6.23214 iter/s, 16.0459s/100 iter), loss = 1.02381
I0630 14:49:01.535501 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 14:49:01.535519 29777 sgd_solver.cpp:106] Iteration 239400, lr = 0.00251875
I0630 14:49:17.793753 29777 solver.cpp:290] Iteration 239500 (6.15089 iter/s, 16.2578s/100 iter), loss = 1.2381
I0630 14:49:17.793778 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 14:49:17.793787 29777 sgd_solver.cpp:106] Iteration 239500, lr = 0.00251562
I0630 14:49:33.953625 29777 solver.cpp:290] Iteration 239600 (6.18835 iter/s, 16.1594s/100 iter), loss = 1.46429
I0630 14:49:33.953747 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 14:49:33.953771 29777 sgd_solver.cpp:106] Iteration 239600, lr = 0.0025125
I0630 14:49:50.155144 29777 solver.cpp:290] Iteration 239700 (6.17248 iter/s, 16.201s/100 iter), loss = 1.14286
I0630 14:49:50.155169 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 14:49:50.155179 29777 sgd_solver.cpp:106] Iteration 239700, lr = 0.00250938
I0630 14:50:06.205945 29777 solver.cpp:290] Iteration 239800 (6.2304 iter/s, 16.0503s/100 iter), loss = 1.45238
I0630 14:50:06.206053 29777 solver.cpp:309]     Train net output #0: loss = 1.92857 (* 1 = 1.92857 loss)
I0630 14:50:06.206063 29777 sgd_solver.cpp:106] Iteration 239800, lr = 0.00250625
I0630 14:50:22.371521 29777 solver.cpp:290] Iteration 239900 (6.18619 iter/s, 16.165s/100 iter), loss = 1.29762
I0630 14:50:22.371544 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 14:50:22.371551 29777 sgd_solver.cpp:106] Iteration 239900, lr = 0.00250312
I0630 14:50:38.490882 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_240000.caffemodel
I0630 14:50:38.510709 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_240000.solverstate
I0630 14:50:38.519436 29777 solver.cpp:354] Sparsity after update:
I0630 14:50:38.520406 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:50:38.520413 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:50:38.520422 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:50:38.520424 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:50:38.520426 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:50:38.520428 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:50:38.520431 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:50:38.520432 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:50:38.520434 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:50:38.520437 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:50:38.520438 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:50:38.520440 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:50:38.520442 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:50:38.520539 29777 solver.cpp:471] Iteration 240000, Testing net (#0)
I0630 14:51:07.245438 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 14:51:43.166494 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55654
I0630 14:51:43.166581 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.791502
I0630 14:51:43.166594 29777 solver.cpp:544]     Test net output #2: loss = 1.57972 (* 1 = 1.57972 loss)
I0630 14:51:43.362066 29777 solver.cpp:290] Iteration 240000 (1.23475 iter/s, 80.9883s/100 iter), loss = 1.15476
I0630 14:51:43.362089 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 14:51:43.362095 29777 sgd_solver.cpp:106] Iteration 240000, lr = 0.0025
I0630 14:51:58.862139 29777 solver.cpp:290] Iteration 240100 (6.45177 iter/s, 15.4996s/100 iter), loss = 1.04762
I0630 14:51:58.862160 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 14:51:58.862167 29777 sgd_solver.cpp:106] Iteration 240100, lr = 0.00249687
I0630 14:52:14.922886 29777 solver.cpp:290] Iteration 240200 (6.22654 iter/s, 16.0603s/100 iter), loss = 1.22619
I0630 14:52:14.922958 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 14:52:14.922967 29777 sgd_solver.cpp:106] Iteration 240200, lr = 0.00249375
I0630 14:52:30.994843 29777 solver.cpp:290] Iteration 240300 (6.22222 iter/s, 16.0714s/100 iter), loss = 1.14286
I0630 14:52:30.994868 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 14:52:30.994876 29777 sgd_solver.cpp:106] Iteration 240300, lr = 0.00249062
I0630 14:52:47.076318 29777 solver.cpp:290] Iteration 240400 (6.21852 iter/s, 16.081s/100 iter), loss = 0.928571
I0630 14:52:47.076427 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 14:52:47.076439 29777 sgd_solver.cpp:106] Iteration 240400, lr = 0.0024875
I0630 14:53:03.342655 29777 solver.cpp:290] Iteration 240500 (6.14787 iter/s, 16.2658s/100 iter), loss = 1.54762
I0630 14:53:03.342679 29777 solver.cpp:309]     Train net output #0: loss = 1.90476 (* 1 = 1.90476 loss)
I0630 14:53:03.342686 29777 sgd_solver.cpp:106] Iteration 240500, lr = 0.00248438
I0630 14:53:19.411567 29777 solver.cpp:290] Iteration 240600 (6.22338 iter/s, 16.0684s/100 iter), loss = 0.988095
I0630 14:53:19.411689 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 14:53:19.411702 29777 sgd_solver.cpp:106] Iteration 240600, lr = 0.00248125
I0630 14:53:35.471686 29777 solver.cpp:290] Iteration 240700 (6.22682 iter/s, 16.0596s/100 iter), loss = 1.55952
I0630 14:53:35.471710 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 14:53:35.471719 29777 sgd_solver.cpp:106] Iteration 240700, lr = 0.00247813
I0630 14:53:51.436975 29777 solver.cpp:290] Iteration 240800 (6.26377 iter/s, 15.9648s/100 iter), loss = 1.33333
I0630 14:53:51.437088 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 14:53:51.437103 29777 sgd_solver.cpp:106] Iteration 240800, lr = 0.002475
I0630 14:54:07.517187 29777 solver.cpp:290] Iteration 240900 (6.21904 iter/s, 16.0797s/100 iter), loss = 1.33333
I0630 14:54:07.517211 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 14:54:07.517220 29777 sgd_solver.cpp:106] Iteration 240900, lr = 0.00247187
I0630 14:54:23.427368 29777 solver.cpp:354] Sparsity after update:
I0630 14:54:23.447798 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:54:23.447814 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:54:23.447824 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:54:23.447827 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:54:23.447831 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:54:23.447834 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:54:23.447837 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:54:23.447841 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:54:23.447844 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:54:23.447847 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:54:23.447850 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:54:23.447854 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:54:23.447865 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:54:23.608479 29777 solver.cpp:290] Iteration 241000 (6.21472 iter/s, 16.0908s/100 iter), loss = 1
I0630 14:54:23.608502 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 14:54:23.608508 29777 sgd_solver.cpp:106] Iteration 241000, lr = 0.00246875
I0630 14:54:39.527045 29777 solver.cpp:290] Iteration 241100 (6.28215 iter/s, 15.9181s/100 iter), loss = 1.63095
I0630 14:54:39.527070 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 14:54:39.527077 29777 sgd_solver.cpp:106] Iteration 241100, lr = 0.00246562
I0630 14:54:55.565255 29777 solver.cpp:290] Iteration 241200 (6.23529 iter/s, 16.0377s/100 iter), loss = 1.54762
I0630 14:54:55.565361 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 14:54:55.565371 29777 sgd_solver.cpp:106] Iteration 241200, lr = 0.0024625
I0630 14:55:11.595424 29777 solver.cpp:290] Iteration 241300 (6.23845 iter/s, 16.0296s/100 iter), loss = 1.21429
I0630 14:55:11.595449 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 14:55:11.595469 29777 sgd_solver.cpp:106] Iteration 241300, lr = 0.00245938
I0630 14:55:27.834018 29777 solver.cpp:290] Iteration 241400 (6.15835 iter/s, 16.2381s/100 iter), loss = 1.17857
I0630 14:55:27.834123 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 14:55:27.834136 29777 sgd_solver.cpp:106] Iteration 241400, lr = 0.00245625
I0630 14:55:44.115034 29777 solver.cpp:290] Iteration 241500 (6.14233 iter/s, 16.2805s/100 iter), loss = 1.45238
I0630 14:55:44.115056 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 14:55:44.115062 29777 sgd_solver.cpp:106] Iteration 241500, lr = 0.00245313
I0630 14:56:00.104871 29777 solver.cpp:290] Iteration 241600 (6.25415 iter/s, 15.9894s/100 iter), loss = 1.14286
I0630 14:56:00.104935 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 14:56:00.104955 29777 sgd_solver.cpp:106] Iteration 241600, lr = 0.00245
I0630 14:56:16.303617 29777 solver.cpp:290] Iteration 241700 (6.17351 iter/s, 16.1982s/100 iter), loss = 1.05952
I0630 14:56:16.303649 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 14:56:16.303659 29777 sgd_solver.cpp:106] Iteration 241700, lr = 0.00244687
I0630 14:56:32.411417 29777 solver.cpp:290] Iteration 241800 (6.20835 iter/s, 16.1073s/100 iter), loss = 1.15476
I0630 14:56:32.411526 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 14:56:32.411535 29777 sgd_solver.cpp:106] Iteration 241800, lr = 0.00244375
I0630 14:56:48.462512 29777 solver.cpp:290] Iteration 241900 (6.23032 iter/s, 16.0505s/100 iter), loss = 1.11905
I0630 14:56:48.462539 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 14:56:48.462555 29777 sgd_solver.cpp:106] Iteration 241900, lr = 0.00244062
I0630 14:57:04.297482 29777 solver.cpp:354] Sparsity after update:
I0630 14:57:04.298946 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 14:57:04.298954 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 14:57:04.298964 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 14:57:04.298969 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 14:57:04.298976 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 14:57:04.298982 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 14:57:04.298986 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 14:57:04.298990 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 14:57:04.298995 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 14:57:04.298997 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 14:57:04.299002 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 14:57:04.299005 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 14:57:04.299010 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 14:57:04.299105 29777 solver.cpp:471] Iteration 242000, Testing net (#0)
I0630 14:57:28.836158 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 14:58:04.628875 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5558
I0630 14:58:04.628921 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.791421
I0630 14:58:04.628927 29777 solver.cpp:544]     Test net output #2: loss = 1.58108 (* 1 = 1.58108 loss)
I0630 14:58:04.812335 29777 solver.cpp:290] Iteration 242000 (1.3098 iter/s, 76.3477s/100 iter), loss = 1.19048
I0630 14:58:04.812357 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 14:58:04.812364 29777 sgd_solver.cpp:106] Iteration 242000, lr = 0.0024375
I0630 14:58:20.492357 29777 solver.cpp:290] Iteration 242100 (6.37773 iter/s, 15.6796s/100 iter), loss = 1.04762
I0630 14:58:20.492379 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 14:58:20.492386 29777 sgd_solver.cpp:106] Iteration 242100, lr = 0.00243438
I0630 14:58:36.456567 29777 solver.cpp:290] Iteration 242200 (6.26419 iter/s, 15.9637s/100 iter), loss = 1.21429
I0630 14:58:36.456671 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 14:58:36.456682 29777 sgd_solver.cpp:106] Iteration 242200, lr = 0.00243125
I0630 14:58:52.575175 29777 solver.cpp:290] Iteration 242300 (6.20422 iter/s, 16.1181s/100 iter), loss = 1.2381
I0630 14:58:52.575273 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 14:58:52.575311 29777 sgd_solver.cpp:106] Iteration 242300, lr = 0.00242813
I0630 14:59:08.564743 29777 solver.cpp:290] Iteration 242400 (6.25428 iter/s, 15.989s/100 iter), loss = 1.15476
I0630 14:59:08.564833 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 14:59:08.564844 29777 sgd_solver.cpp:106] Iteration 242400, lr = 0.002425
I0630 14:59:24.888545 29777 solver.cpp:290] Iteration 242500 (6.12623 iter/s, 16.3233s/100 iter), loss = 1.11905
I0630 14:59:24.888571 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 14:59:24.888579 29777 sgd_solver.cpp:106] Iteration 242500, lr = 0.00242188
I0630 14:59:40.945576 29777 solver.cpp:290] Iteration 242600 (6.22798 iter/s, 16.0566s/100 iter), loss = 1.25
I0630 14:59:40.945662 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 14:59:40.945669 29777 sgd_solver.cpp:106] Iteration 242600, lr = 0.00241875
I0630 14:59:57.225538 29777 solver.cpp:290] Iteration 242700 (6.14272 iter/s, 16.2794s/100 iter), loss = 1.66667
I0630 14:59:57.225564 29777 solver.cpp:309]     Train net output #0: loss = 2.19048 (* 1 = 2.19048 loss)
I0630 14:59:57.225572 29777 sgd_solver.cpp:106] Iteration 242700, lr = 0.00241562
I0630 15:00:13.360242 29777 solver.cpp:290] Iteration 242800 (6.198 iter/s, 16.1342s/100 iter), loss = 1.07143
I0630 15:00:13.360368 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 15:00:13.360386 29777 sgd_solver.cpp:106] Iteration 242800, lr = 0.0024125
I0630 15:00:29.547134 29777 solver.cpp:290] Iteration 242900 (6.17805 iter/s, 16.1863s/100 iter), loss = 1.33333
I0630 15:00:29.547158 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 15:00:29.547164 29777 sgd_solver.cpp:106] Iteration 242900, lr = 0.00240937
I0630 15:00:45.407768 29777 solver.cpp:354] Sparsity after update:
I0630 15:00:45.428372 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:00:45.428388 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:00:45.428398 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:00:45.428402 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:00:45.428408 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:00:45.428411 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:00:45.428414 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:00:45.428418 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:00:45.428423 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:00:45.428427 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:00:45.428432 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:00:45.428436 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:00:45.428442 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:00:45.582679 29777 solver.cpp:290] Iteration 243000 (6.23633 iter/s, 16.0351s/100 iter), loss = 1.19048
I0630 15:00:45.582705 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 15:00:45.582715 29777 sgd_solver.cpp:106] Iteration 243000, lr = 0.00240625
I0630 15:01:01.703308 29777 solver.cpp:290] Iteration 243100 (6.20341 iter/s, 16.1202s/100 iter), loss = 1.14286
I0630 15:01:01.703331 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 15:01:01.703338 29777 sgd_solver.cpp:106] Iteration 243100, lr = 0.00240313
I0630 15:01:17.782371 29777 solver.cpp:290] Iteration 243200 (6.21945 iter/s, 16.0786s/100 iter), loss = 1.29762
I0630 15:01:17.782466 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 15:01:17.782474 29777 sgd_solver.cpp:106] Iteration 243200, lr = 0.0024
I0630 15:01:33.840658 29777 solver.cpp:290] Iteration 243300 (6.22752 iter/s, 16.0578s/100 iter), loss = 1.13095
I0630 15:01:33.840693 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 15:01:33.840704 29777 sgd_solver.cpp:106] Iteration 243300, lr = 0.00239688
I0630 15:01:49.993893 29777 solver.cpp:290] Iteration 243400 (6.19089 iter/s, 16.1528s/100 iter), loss = 1.21429
I0630 15:01:49.993968 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 15:01:49.993975 29777 sgd_solver.cpp:106] Iteration 243400, lr = 0.00239375
I0630 15:02:06.010871 29777 solver.cpp:290] Iteration 243500 (6.24357 iter/s, 16.0165s/100 iter), loss = 1.15476
I0630 15:02:06.010896 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 15:02:06.010905 29777 sgd_solver.cpp:106] Iteration 243500, lr = 0.00239062
I0630 15:02:22.181170 29777 solver.cpp:290] Iteration 243600 (6.18436 iter/s, 16.1698s/100 iter), loss = 1.39286
I0630 15:02:22.181260 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 15:02:22.181272 29777 sgd_solver.cpp:106] Iteration 243600, lr = 0.0023875
I0630 15:02:38.389775 29777 solver.cpp:290] Iteration 243700 (6.16976 iter/s, 16.2081s/100 iter), loss = 1.47619
I0630 15:02:38.389818 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 15:02:38.389832 29777 sgd_solver.cpp:106] Iteration 243700, lr = 0.00238437
I0630 15:02:54.589095 29777 solver.cpp:290] Iteration 243800 (6.17328 iter/s, 16.1988s/100 iter), loss = 1.28571
I0630 15:02:54.589169 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 15:02:54.589176 29777 sgd_solver.cpp:106] Iteration 243800, lr = 0.00238125
I0630 15:03:10.604950 29777 solver.cpp:290] Iteration 243900 (6.24401 iter/s, 16.0153s/100 iter), loss = 1.33333
I0630 15:03:10.604976 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 15:03:10.604986 29777 sgd_solver.cpp:106] Iteration 243900, lr = 0.00237813
I0630 15:03:26.554603 29777 solver.cpp:354] Sparsity after update:
I0630 15:03:26.558284 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:03:26.558300 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:03:26.558320 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:03:26.558324 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:03:26.558328 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:03:26.558334 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:03:26.558339 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:03:26.558346 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:03:26.558349 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:03:26.558352 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:03:26.558357 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:03:26.558362 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:03:26.558364 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:03:26.558513 29777 solver.cpp:471] Iteration 244000, Testing net (#0)
I0630 15:03:54.312980 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 15:04:28.255213 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55512
I0630 15:04:28.255250 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.790461
I0630 15:04:28.255256 29777 solver.cpp:544]     Test net output #2: loss = 1.58336 (* 1 = 1.58336 loss)
I0630 15:04:28.437615 29777 solver.cpp:290] Iteration 244000 (1.28484 iter/s, 77.8305s/100 iter), loss = 1.05952
I0630 15:04:28.437649 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 15:04:28.437659 29777 sgd_solver.cpp:106] Iteration 244000, lr = 0.002375
I0630 15:04:44.369254 29777 solver.cpp:290] Iteration 244100 (6.277 iter/s, 15.9312s/100 iter), loss = 1.55952
I0630 15:04:44.369280 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 15:04:44.369288 29777 sgd_solver.cpp:106] Iteration 244100, lr = 0.00237188
I0630 15:05:00.500527 29777 solver.cpp:290] Iteration 244200 (6.19932 iter/s, 16.1308s/100 iter), loss = 1.04762
I0630 15:05:00.500690 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 15:05:00.500730 29777 sgd_solver.cpp:106] Iteration 244200, lr = 0.00236875
I0630 15:05:16.649865 29777 solver.cpp:290] Iteration 244300 (6.19243 iter/s, 16.1487s/100 iter), loss = 1.19048
I0630 15:05:16.649888 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 15:05:16.649895 29777 sgd_solver.cpp:106] Iteration 244300, lr = 0.00236562
I0630 15:05:32.660594 29777 solver.cpp:290] Iteration 244400 (6.24599 iter/s, 16.0103s/100 iter), loss = 1.25
I0630 15:05:32.660646 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 15:05:32.660655 29777 sgd_solver.cpp:106] Iteration 244400, lr = 0.0023625
I0630 15:05:48.817102 29777 solver.cpp:290] Iteration 244500 (6.18965 iter/s, 16.156s/100 iter), loss = 1.20238
I0630 15:05:48.817129 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 15:05:48.817138 29777 sgd_solver.cpp:106] Iteration 244500, lr = 0.00235937
I0630 15:06:04.891726 29777 solver.cpp:290] Iteration 244600 (6.22117 iter/s, 16.0742s/100 iter), loss = 1.58333
I0630 15:06:04.891832 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 15:06:04.891842 29777 sgd_solver.cpp:106] Iteration 244600, lr = 0.00235625
I0630 15:06:20.993818 29777 solver.cpp:290] Iteration 244700 (6.21058 iter/s, 16.1015s/100 iter), loss = 1.36905
I0630 15:06:20.993840 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 15:06:20.993847 29777 sgd_solver.cpp:106] Iteration 244700, lr = 0.00235313
I0630 15:06:37.058713 29777 solver.cpp:290] Iteration 244800 (6.22493 iter/s, 16.0644s/100 iter), loss = 1.40476
I0630 15:06:37.058820 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 15:06:37.058840 29777 sgd_solver.cpp:106] Iteration 244800, lr = 0.00235
I0630 15:06:53.019523 29777 solver.cpp:290] Iteration 244900 (6.26556 iter/s, 15.9603s/100 iter), loss = 0.976191
I0630 15:06:53.019551 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 15:06:53.019559 29777 sgd_solver.cpp:106] Iteration 244900, lr = 0.00234687
I0630 15:07:08.952559 29777 solver.cpp:354] Sparsity after update:
I0630 15:07:08.972908 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:07:08.972923 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:07:08.972934 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:07:08.972937 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:07:08.972941 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:07:08.972944 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:07:08.972949 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:07:08.972951 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:07:08.972954 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:07:08.972957 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:07:08.972961 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:07:08.972965 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:07:08.972968 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:07:09.131500 29777 solver.cpp:290] Iteration 245000 (6.20674 iter/s, 16.1115s/100 iter), loss = 1.54762
I0630 15:07:09.131525 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 15:07:09.131534 29777 sgd_solver.cpp:106] Iteration 245000, lr = 0.00234375
I0630 15:07:25.066159 29777 solver.cpp:290] Iteration 245100 (6.27581 iter/s, 15.9342s/100 iter), loss = 1.2381
I0630 15:07:25.066181 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 15:07:25.066189 29777 sgd_solver.cpp:106] Iteration 245100, lr = 0.00234062
I0630 15:07:41.011425 29777 solver.cpp:290] Iteration 245200 (6.27163 iter/s, 15.9448s/100 iter), loss = 1
I0630 15:07:41.011543 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 15:07:41.011554 29777 sgd_solver.cpp:106] Iteration 245200, lr = 0.0023375
I0630 15:07:57.050654 29777 solver.cpp:290] Iteration 245300 (6.23493 iter/s, 16.0387s/100 iter), loss = 0.75
I0630 15:07:57.050678 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 15:07:57.050686 29777 sgd_solver.cpp:106] Iteration 245300, lr = 0.00233437
I0630 15:08:13.049819 29777 solver.cpp:290] Iteration 245400 (6.25051 iter/s, 15.9987s/100 iter), loss = 1.75
I0630 15:08:13.049922 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 15:08:13.049932 29777 sgd_solver.cpp:106] Iteration 245400, lr = 0.00233125
I0630 15:08:29.052705 29777 solver.cpp:290] Iteration 245500 (6.24908 iter/s, 16.0023s/100 iter), loss = 1.46429
I0630 15:08:29.052726 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 15:08:29.052732 29777 sgd_solver.cpp:106] Iteration 245500, lr = 0.00232813
I0630 15:08:45.176229 29777 solver.cpp:290] Iteration 245600 (6.20229 iter/s, 16.1231s/100 iter), loss = 1.14286
I0630 15:08:45.176338 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 15:08:45.176348 29777 sgd_solver.cpp:106] Iteration 245600, lr = 0.002325
I0630 15:09:01.080462 29777 solver.cpp:290] Iteration 245700 (6.28785 iter/s, 15.9037s/100 iter), loss = 0.916667
I0630 15:09:01.080484 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 15:09:01.080492 29777 sgd_solver.cpp:106] Iteration 245700, lr = 0.00232187
I0630 15:09:17.202512 29777 solver.cpp:290] Iteration 245800 (6.20286 iter/s, 16.1216s/100 iter), loss = 1.16667
I0630 15:09:17.202569 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 15:09:17.202576 29777 sgd_solver.cpp:106] Iteration 245800, lr = 0.00231875
I0630 15:09:33.120579 29777 solver.cpp:290] Iteration 245900 (6.28236 iter/s, 15.9176s/100 iter), loss = 1.47619
I0630 15:09:33.120600 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 15:09:33.120609 29777 sgd_solver.cpp:106] Iteration 245900, lr = 0.00231562
I0630 15:09:49.036010 29777 solver.cpp:354] Sparsity after update:
I0630 15:09:49.037933 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:09:49.037951 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:09:49.037981 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:09:49.037993 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:09:49.038004 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:09:49.038014 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:09:49.038024 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:09:49.038035 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:09:49.038046 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:09:49.038058 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:09:49.038074 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:09:49.038084 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:09:49.038094 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:09:49.038300 29777 solver.cpp:471] Iteration 246000, Testing net (#0)
I0630 15:10:11.014765 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 15:10:39.311548 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55708
I0630 15:10:39.311605 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.791681
I0630 15:10:39.311611 29777 solver.cpp:544]     Test net output #2: loss = 1.58056 (* 1 = 1.58056 loss)
I0630 15:10:39.491602 29777 solver.cpp:290] Iteration 246000 (1.50672 iter/s, 66.3692s/100 iter), loss = 1.32143
I0630 15:10:39.491626 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 15:10:39.491632 29777 sgd_solver.cpp:106] Iteration 246000, lr = 0.0023125
I0630 15:10:54.918157 29777 solver.cpp:290] Iteration 246100 (6.48252 iter/s, 15.4261s/100 iter), loss = 1.08333
I0630 15:10:54.918184 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 15:10:54.918192 29777 sgd_solver.cpp:106] Iteration 246100, lr = 0.00230937
I0630 15:11:10.995131 29777 solver.cpp:290] Iteration 246200 (6.22026 iter/s, 16.0765s/100 iter), loss = 1.17857
I0630 15:11:10.995219 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 15:11:10.995230 29777 sgd_solver.cpp:106] Iteration 246200, lr = 0.00230625
I0630 15:11:26.974689 29777 solver.cpp:290] Iteration 246300 (6.2582 iter/s, 15.979s/100 iter), loss = 1.22619
I0630 15:11:26.974730 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 15:11:26.974743 29777 sgd_solver.cpp:106] Iteration 246300, lr = 0.00230313
I0630 15:11:43.012068 29777 solver.cpp:290] Iteration 246400 (6.23562 iter/s, 16.0369s/100 iter), loss = 1.2619
I0630 15:11:43.012135 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 15:11:43.012146 29777 sgd_solver.cpp:106] Iteration 246400, lr = 0.0023
I0630 15:11:59.016470 29777 solver.cpp:290] Iteration 246500 (6.24848 iter/s, 16.0039s/100 iter), loss = 1.16667
I0630 15:11:59.016494 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 15:11:59.016500 29777 sgd_solver.cpp:106] Iteration 246500, lr = 0.00229687
I0630 15:12:14.944749 29777 solver.cpp:290] Iteration 246600 (6.27832 iter/s, 15.9278s/100 iter), loss = 1.44048
I0630 15:12:14.944797 29777 solver.cpp:309]     Train net output #0: loss = 2 (* 1 = 2 loss)
I0630 15:12:14.944805 29777 sgd_solver.cpp:106] Iteration 246600, lr = 0.00229375
I0630 15:12:31.008201 29777 solver.cpp:290] Iteration 246700 (6.2255 iter/s, 16.063s/100 iter), loss = 1.2381
I0630 15:12:31.008226 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 15:12:31.008232 29777 sgd_solver.cpp:106] Iteration 246700, lr = 0.00229062
I0630 15:12:46.956996 29777 solver.cpp:290] Iteration 246800 (6.27025 iter/s, 15.9483s/100 iter), loss = 0.857143
I0630 15:12:46.957100 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 15:12:46.957110 29777 sgd_solver.cpp:106] Iteration 246800, lr = 0.0022875
I0630 15:13:03.044584 29777 solver.cpp:290] Iteration 246900 (6.21619 iter/s, 16.087s/100 iter), loss = 1.19048
I0630 15:13:03.044793 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 15:13:03.044911 29777 sgd_solver.cpp:106] Iteration 246900, lr = 0.00228437
I0630 15:13:18.898795 29777 solver.cpp:354] Sparsity after update:
I0630 15:13:18.919595 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:13:18.919611 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:13:18.919621 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:13:18.919625 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:13:18.919628 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:13:18.919639 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:13:18.919642 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:13:18.919646 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:13:18.919648 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:13:18.919654 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:13:18.919657 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:13:18.919661 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:13:18.919667 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:13:19.077872 29777 solver.cpp:290] Iteration 247000 (6.23725 iter/s, 16.0327s/100 iter), loss = 1.33333
I0630 15:13:19.077893 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 15:13:19.077900 29777 sgd_solver.cpp:106] Iteration 247000, lr = 0.00228125
I0630 15:13:35.017765 29777 solver.cpp:290] Iteration 247100 (6.27375 iter/s, 15.9394s/100 iter), loss = 1.58333
I0630 15:13:35.017791 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 15:13:35.017797 29777 sgd_solver.cpp:106] Iteration 247100, lr = 0.00227813
I0630 15:13:51.015534 29777 solver.cpp:290] Iteration 247200 (6.25105 iter/s, 15.9973s/100 iter), loss = 1.35714
I0630 15:13:51.015635 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 15:13:51.015645 29777 sgd_solver.cpp:106] Iteration 247200, lr = 0.002275
I0630 15:14:07.026319 29777 solver.cpp:290] Iteration 247300 (6.246 iter/s, 16.0103s/100 iter), loss = 1.29762
I0630 15:14:07.026345 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 15:14:07.026353 29777 sgd_solver.cpp:106] Iteration 247300, lr = 0.00227188
I0630 15:14:23.142765 29777 solver.cpp:290] Iteration 247400 (6.20502 iter/s, 16.116s/100 iter), loss = 1.11905
I0630 15:14:23.142807 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 15:14:23.142815 29777 sgd_solver.cpp:106] Iteration 247400, lr = 0.00226875
I0630 15:14:39.147349 29777 solver.cpp:290] Iteration 247500 (6.2484 iter/s, 16.0041s/100 iter), loss = 1.35714
I0630 15:14:39.147373 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 15:14:39.147382 29777 sgd_solver.cpp:106] Iteration 247500, lr = 0.00226562
I0630 15:14:55.040453 29777 solver.cpp:290] Iteration 247600 (6.29222 iter/s, 15.8926s/100 iter), loss = 0.904762
I0630 15:14:55.040535 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 15:14:55.040547 29777 sgd_solver.cpp:106] Iteration 247600, lr = 0.0022625
I0630 15:15:11.158155 29777 solver.cpp:290] Iteration 247700 (6.20456 iter/s, 16.1172s/100 iter), loss = 1.21429
I0630 15:15:11.158213 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 15:15:11.158231 29777 sgd_solver.cpp:106] Iteration 247700, lr = 0.00225937
I0630 15:15:27.173578 29777 solver.cpp:290] Iteration 247800 (6.24417 iter/s, 16.0149s/100 iter), loss = 1.35714
I0630 15:15:27.173749 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 15:15:27.173764 29777 sgd_solver.cpp:106] Iteration 247800, lr = 0.00225625
I0630 15:15:43.145578 29777 solver.cpp:290] Iteration 247900 (6.26119 iter/s, 15.9714s/100 iter), loss = 1.16667
I0630 15:15:43.145602 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 15:15:43.145611 29777 sgd_solver.cpp:106] Iteration 247900, lr = 0.00225312
I0630 15:15:58.916957 29777 solver.cpp:354] Sparsity after update:
I0630 15:15:58.918380 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:15:58.918387 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:15:58.918395 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:15:58.918400 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:15:58.918403 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:15:58.918407 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:15:58.918411 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:15:58.918416 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:15:58.918419 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:15:58.918422 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:15:58.918426 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:15:58.918431 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:15:58.918434 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:15:58.918573 29777 solver.cpp:471] Iteration 248000, Testing net (#0)
I0630 15:16:20.067695 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 15:16:48.327513 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.556839
I0630 15:16:48.327584 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.790942
I0630 15:16:48.327591 29777 solver.cpp:544]     Test net output #2: loss = 1.58104 (* 1 = 1.58104 loss)
I0630 15:16:48.507814 29777 solver.cpp:290] Iteration 248000 (1.52998 iter/s, 65.3605s/100 iter), loss = 1.41667
I0630 15:16:48.507846 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 15:16:48.507856 29777 sgd_solver.cpp:106] Iteration 248000, lr = 0.00225
I0630 15:17:03.956970 29777 solver.cpp:290] Iteration 248100 (6.47304 iter/s, 15.4487s/100 iter), loss = 1.2381
I0630 15:17:03.956993 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 15:17:03.957000 29777 sgd_solver.cpp:106] Iteration 248100, lr = 0.00224688
I0630 15:17:20.091437 29777 solver.cpp:290] Iteration 248200 (6.19809 iter/s, 16.134s/100 iter), loss = 1.5119
I0630 15:17:20.091542 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 15:17:20.091552 29777 sgd_solver.cpp:106] Iteration 248200, lr = 0.00224375
I0630 15:17:36.457223 29777 solver.cpp:290] Iteration 248300 (6.11051 iter/s, 16.3652s/100 iter), loss = 1.46429
I0630 15:17:36.457255 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 15:17:36.457264 29777 sgd_solver.cpp:106] Iteration 248300, lr = 0.00224062
I0630 15:17:52.527395 29777 solver.cpp:290] Iteration 248400 (6.22289 iter/s, 16.0697s/100 iter), loss = 1.35714
I0630 15:17:52.527698 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 15:17:52.527714 29777 sgd_solver.cpp:106] Iteration 248400, lr = 0.0022375
I0630 15:18:08.844426 29777 solver.cpp:290] Iteration 248500 (6.12885 iter/s, 16.3163s/100 iter), loss = 0.940476
I0630 15:18:08.844451 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 15:18:08.844460 29777 sgd_solver.cpp:106] Iteration 248500, lr = 0.00223437
I0630 15:18:25.013842 29777 solver.cpp:290] Iteration 248600 (6.1847 iter/s, 16.1689s/100 iter), loss = 0.75
I0630 15:18:25.013948 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 15:18:25.014123 29777 sgd_solver.cpp:106] Iteration 248600, lr = 0.00223125
I0630 15:18:41.145943 29777 solver.cpp:290] Iteration 248700 (6.19903 iter/s, 16.1316s/100 iter), loss = 1.4881
I0630 15:18:41.145967 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 15:18:41.145972 29777 sgd_solver.cpp:106] Iteration 248700, lr = 0.00222812
I0630 15:18:57.335738 29777 solver.cpp:290] Iteration 248800 (6.17691 iter/s, 16.1893s/100 iter), loss = 1.53571
I0630 15:18:57.335861 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 15:18:57.335871 29777 sgd_solver.cpp:106] Iteration 248800, lr = 0.002225
I0630 15:19:13.449295 29777 solver.cpp:290] Iteration 248900 (6.20617 iter/s, 16.113s/100 iter), loss = 1.21429
I0630 15:19:13.449327 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 15:19:13.449338 29777 sgd_solver.cpp:106] Iteration 248900, lr = 0.00222188
I0630 15:19:29.525897 29777 solver.cpp:354] Sparsity after update:
I0630 15:19:29.550369 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:19:29.550396 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:19:29.550427 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:19:29.550442 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:19:29.550459 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:19:29.550469 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:19:29.550487 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:19:29.550496 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:19:29.550513 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:19:29.550521 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:19:29.550529 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:19:29.550544 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:19:29.550552 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:19:29.708742 29777 solver.cpp:290] Iteration 249000 (6.15045 iter/s, 16.259s/100 iter), loss = 1.20238
I0630 15:19:29.708766 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 15:19:29.708775 29777 sgd_solver.cpp:106] Iteration 249000, lr = 0.00221875
I0630 15:19:45.808287 29777 solver.cpp:290] Iteration 249100 (6.21153 iter/s, 16.0991s/100 iter), loss = 1.5119
I0630 15:19:45.808311 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 15:19:45.808318 29777 sgd_solver.cpp:106] Iteration 249100, lr = 0.00221563
I0630 15:20:02.140578 29777 solver.cpp:290] Iteration 249200 (6.12301 iter/s, 16.3318s/100 iter), loss = 1.63095
I0630 15:20:02.140672 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 15:20:02.140683 29777 sgd_solver.cpp:106] Iteration 249200, lr = 0.0022125
I0630 15:20:18.637384 29777 solver.cpp:290] Iteration 249300 (6.06199 iter/s, 16.4962s/100 iter), loss = 1.35714
I0630 15:20:18.637467 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 15:20:18.638917 29777 sgd_solver.cpp:106] Iteration 249300, lr = 0.00220937
I0630 15:20:34.914152 29777 solver.cpp:290] Iteration 249400 (6.14392 iter/s, 16.2763s/100 iter), loss = 1.34524
I0630 15:20:34.914265 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 15:20:34.914275 29777 sgd_solver.cpp:106] Iteration 249400, lr = 0.00220625
I0630 15:20:50.925715 29777 solver.cpp:290] Iteration 249500 (6.2457 iter/s, 16.011s/100 iter), loss = 1.34524
I0630 15:20:50.925740 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 15:20:50.925748 29777 sgd_solver.cpp:106] Iteration 249500, lr = 0.00220312
I0630 15:21:06.972280 29777 solver.cpp:290] Iteration 249600 (6.23205 iter/s, 16.0461s/100 iter), loss = 1.28571
I0630 15:21:06.972411 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 15:21:06.972437 29777 sgd_solver.cpp:106] Iteration 249600, lr = 0.0022
I0630 15:21:23.334511 29777 solver.cpp:290] Iteration 249700 (6.11185 iter/s, 16.3617s/100 iter), loss = 1.67857
I0630 15:21:23.334537 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 15:21:23.334545 29777 sgd_solver.cpp:106] Iteration 249700, lr = 0.00219688
I0630 15:21:39.454362 29777 solver.cpp:290] Iteration 249800 (6.20371 iter/s, 16.1194s/100 iter), loss = 1.54762
I0630 15:21:39.454455 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 15:21:39.454466 29777 sgd_solver.cpp:106] Iteration 249800, lr = 0.00219375
I0630 15:21:55.619889 29777 solver.cpp:290] Iteration 249900 (6.18621 iter/s, 16.165s/100 iter), loss = 1.33333
I0630 15:21:55.619981 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 15:21:55.620013 29777 sgd_solver.cpp:106] Iteration 249900, lr = 0.00219063
I0630 15:22:11.509621 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_250000.caffemodel
I0630 15:22:11.529258 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_250000.solverstate
I0630 15:22:11.538048 29777 solver.cpp:354] Sparsity after update:
I0630 15:22:11.538993 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:22:11.539001 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:22:11.539010 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:22:11.539011 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:22:11.539013 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:22:11.539016 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:22:11.539017 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:22:11.539019 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:22:11.539021 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:22:11.539023 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:22:11.539026 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:22:11.539027 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:22:11.539029 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:22:11.539124 29777 solver.cpp:471] Iteration 250000, Testing net (#0)
I0630 15:22:39.509423 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 15:23:18.557313 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5552
I0630 15:23:18.557413 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.791362
I0630 15:23:18.557423 29777 solver.cpp:544]     Test net output #2: loss = 1.57802 (* 1 = 1.57802 loss)
I0630 15:23:18.746023 29777 solver.cpp:290] Iteration 250000 (1.20302 iter/s, 83.1238s/100 iter), loss = 1.28571
I0630 15:23:18.746043 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 15:23:18.746050 29777 sgd_solver.cpp:106] Iteration 250000, lr = 0.0021875
I0630 15:23:34.088002 29777 solver.cpp:290] Iteration 250100 (6.51825 iter/s, 15.3415s/100 iter), loss = 1.44048
I0630 15:23:34.088029 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 15:23:34.088038 29777 sgd_solver.cpp:106] Iteration 250100, lr = 0.00218437
I0630 15:23:50.250442 29777 solver.cpp:290] Iteration 250200 (6.18736 iter/s, 16.162s/100 iter), loss = 1.44048
I0630 15:23:50.250557 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 15:23:50.250572 29777 sgd_solver.cpp:106] Iteration 250200, lr = 0.00218125
I0630 15:24:06.356670 29777 solver.cpp:290] Iteration 250300 (6.20899 iter/s, 16.1057s/100 iter), loss = 1.15476
I0630 15:24:06.356694 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 15:24:06.356701 29777 sgd_solver.cpp:106] Iteration 250300, lr = 0.00217812
I0630 15:24:22.432922 29777 solver.cpp:290] Iteration 250400 (6.22053 iter/s, 16.0758s/100 iter), loss = 1.28571
I0630 15:24:22.433032 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 15:24:22.433043 29777 sgd_solver.cpp:106] Iteration 250400, lr = 0.002175
I0630 15:24:38.581184 29777 solver.cpp:290] Iteration 250500 (6.19283 iter/s, 16.1477s/100 iter), loss = 1.17857
I0630 15:24:38.581209 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 15:24:38.581218 29777 sgd_solver.cpp:106] Iteration 250500, lr = 0.00217188
I0630 15:24:54.708983 29777 solver.cpp:290] Iteration 250600 (6.20065 iter/s, 16.1273s/100 iter), loss = 1.70238
I0630 15:24:54.709055 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 15:24:54.709064 29777 sgd_solver.cpp:106] Iteration 250600, lr = 0.00216875
I0630 15:25:10.819033 29777 solver.cpp:290] Iteration 250700 (6.20752 iter/s, 16.1095s/100 iter), loss = 1.08333
I0630 15:25:10.819216 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 15:25:10.819289 29777 sgd_solver.cpp:106] Iteration 250700, lr = 0.00216563
I0630 15:25:26.936451 29777 solver.cpp:290] Iteration 250800 (6.2047 iter/s, 16.1168s/100 iter), loss = 1.34524
I0630 15:25:26.936537 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 15:25:26.936547 29777 sgd_solver.cpp:106] Iteration 250800, lr = 0.0021625
I0630 15:25:42.906175 29777 solver.cpp:290] Iteration 250900 (6.26207 iter/s, 15.9692s/100 iter), loss = 1.30952
I0630 15:25:42.906355 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 15:25:42.906420 29777 sgd_solver.cpp:106] Iteration 250900, lr = 0.00215937
I0630 15:25:59.182185 29777 solver.cpp:354] Sparsity after update:
I0630 15:25:59.202579 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:25:59.202595 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:25:59.202606 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:25:59.202610 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:25:59.202615 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:25:59.202620 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:25:59.202623 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:25:59.202626 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:25:59.202630 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:25:59.202635 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:25:59.202638 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:25:59.202651 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:25:59.202656 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:25:59.359585 29777 solver.cpp:290] Iteration 251000 (6.07799 iter/s, 16.4528s/100 iter), loss = 1.52381
I0630 15:25:59.359607 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 15:25:59.359614 29777 sgd_solver.cpp:106] Iteration 251000, lr = 0.00215625
I0630 15:26:15.646344 29777 solver.cpp:290] Iteration 251100 (6.14014 iter/s, 16.2863s/100 iter), loss = 0.964286
I0630 15:26:15.646389 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 15:26:15.646406 29777 sgd_solver.cpp:106] Iteration 251100, lr = 0.00215312
I0630 15:26:31.841173 29777 solver.cpp:290] Iteration 251200 (6.17499 iter/s, 16.1943s/100 iter), loss = 1.19048
I0630 15:26:31.841295 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 15:26:31.841308 29777 sgd_solver.cpp:106] Iteration 251200, lr = 0.00215
I0630 15:26:47.784798 29777 solver.cpp:290] Iteration 251300 (6.27232 iter/s, 15.9431s/100 iter), loss = 1.13095
I0630 15:26:47.784822 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 15:26:47.784829 29777 sgd_solver.cpp:106] Iteration 251300, lr = 0.00214688
I0630 15:27:03.853195 29777 solver.cpp:290] Iteration 251400 (6.22357 iter/s, 16.0679s/100 iter), loss = 1.03571
I0630 15:27:03.853665 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 15:27:03.853674 29777 sgd_solver.cpp:106] Iteration 251400, lr = 0.00214375
I0630 15:27:19.997928 29777 solver.cpp:290] Iteration 251500 (6.19432 iter/s, 16.1438s/100 iter), loss = 1.2381
I0630 15:27:19.997953 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 15:27:19.997961 29777 sgd_solver.cpp:106] Iteration 251500, lr = 0.00214063
I0630 15:27:36.170356 29777 solver.cpp:290] Iteration 251600 (6.18354 iter/s, 16.172s/100 iter), loss = 1.21429
I0630 15:27:36.170459 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 15:27:36.170478 29777 sgd_solver.cpp:106] Iteration 251600, lr = 0.0021375
I0630 15:27:52.133726 29777 solver.cpp:290] Iteration 251700 (6.26455 iter/s, 15.9628s/100 iter), loss = 1.08333
I0630 15:27:52.133769 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 15:27:52.133797 29777 sgd_solver.cpp:106] Iteration 251700, lr = 0.00213438
I0630 15:28:08.237123 29777 solver.cpp:290] Iteration 251800 (6.21005 iter/s, 16.1029s/100 iter), loss = 1.40476
I0630 15:28:08.237195 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 15:28:08.237202 29777 sgd_solver.cpp:106] Iteration 251800, lr = 0.00213125
I0630 15:28:24.373260 29777 solver.cpp:290] Iteration 251900 (6.19747 iter/s, 16.1356s/100 iter), loss = 1.2619
I0630 15:28:24.373383 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 15:28:24.373505 29777 sgd_solver.cpp:106] Iteration 251900, lr = 0.00212812
I0630 15:28:40.348826 29777 solver.cpp:354] Sparsity after update:
I0630 15:28:40.354501 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:28:40.354516 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:28:40.354526 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:28:40.354529 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:28:40.354532 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:28:40.354535 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:28:40.354539 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:28:40.354544 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:28:40.354548 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:28:40.354552 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:28:40.354557 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:28:40.354560 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:28:40.354565 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:28:40.354663 29777 solver.cpp:471] Iteration 252000, Testing net (#0)
I0630 15:29:11.004412 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 15:29:51.669005 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55632
I0630 15:29:51.669121 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.793141
I0630 15:29:51.669132 29777 solver.cpp:544]     Test net output #2: loss = 1.57364 (* 1 = 1.57364 loss)
I0630 15:29:51.848747 29777 solver.cpp:290] Iteration 252000 (1.14321 iter/s, 87.473s/100 iter), loss = 1.13095
I0630 15:29:51.848770 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 15:29:51.848778 29777 sgd_solver.cpp:106] Iteration 252000, lr = 0.002125
I0630 15:30:07.394568 29777 solver.cpp:290] Iteration 252100 (6.43278 iter/s, 15.5454s/100 iter), loss = 1.38095
I0630 15:30:07.394592 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 15:30:07.394598 29777 sgd_solver.cpp:106] Iteration 252100, lr = 0.00212188
I0630 15:30:23.451524 29777 solver.cpp:290] Iteration 252200 (6.22801 iter/s, 16.0565s/100 iter), loss = 1.25
I0630 15:30:23.451628 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 15:30:23.451638 29777 sgd_solver.cpp:106] Iteration 252200, lr = 0.00211875
I0630 15:30:39.689488 29777 solver.cpp:290] Iteration 252300 (6.15861 iter/s, 16.2374s/100 iter), loss = 1.45238
I0630 15:30:39.689512 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 15:30:39.689518 29777 sgd_solver.cpp:106] Iteration 252300, lr = 0.00211563
I0630 15:30:56.243593 29777 solver.cpp:290] Iteration 252400 (6.04097 iter/s, 16.5536s/100 iter), loss = 1
I0630 15:30:56.243752 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 15:30:56.243770 29777 sgd_solver.cpp:106] Iteration 252400, lr = 0.0021125
I0630 15:31:12.378381 29777 solver.cpp:290] Iteration 252500 (6.19802 iter/s, 16.1342s/100 iter), loss = 1.65476
I0630 15:31:12.378432 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 15:31:12.378448 29777 sgd_solver.cpp:106] Iteration 252500, lr = 0.00210937
I0630 15:31:28.543354 29777 solver.cpp:290] Iteration 252600 (6.1864 iter/s, 16.1645s/100 iter), loss = 1.41667
I0630 15:31:28.543454 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 15:31:28.543464 29777 sgd_solver.cpp:106] Iteration 252600, lr = 0.00210625
I0630 15:31:44.600363 29777 solver.cpp:290] Iteration 252700 (6.22802 iter/s, 16.0565s/100 iter), loss = 1.40476
I0630 15:31:44.600391 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 15:31:44.600401 29777 sgd_solver.cpp:106] Iteration 252700, lr = 0.00210312
I0630 15:32:00.804839 29777 solver.cpp:290] Iteration 252800 (6.17131 iter/s, 16.204s/100 iter), loss = 0.916667
I0630 15:32:00.804932 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 15:32:00.804944 29777 sgd_solver.cpp:106] Iteration 252800, lr = 0.0021
I0630 15:32:17.113024 29777 solver.cpp:290] Iteration 252900 (6.13209 iter/s, 16.3076s/100 iter), loss = 1.2619
I0630 15:32:17.113049 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 15:32:17.113057 29777 sgd_solver.cpp:106] Iteration 252900, lr = 0.00209687
I0630 15:32:33.218890 29777 solver.cpp:354] Sparsity after update:
I0630 15:32:33.239336 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:32:33.239349 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:32:33.239357 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:32:33.239359 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:32:33.239362 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:32:33.239363 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:32:33.239365 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:32:33.239367 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:32:33.239369 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:32:33.239372 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:32:33.239373 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:32:33.239375 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:32:33.239377 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:32:33.408120 29777 solver.cpp:290] Iteration 253000 (6.13699 iter/s, 16.2946s/100 iter), loss = 1.22619
I0630 15:32:33.408145 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 15:32:33.408154 29777 sgd_solver.cpp:106] Iteration 253000, lr = 0.00209375
I0630 15:32:49.634726 29777 solver.cpp:290] Iteration 253100 (6.1629 iter/s, 16.2261s/100 iter), loss = 1.32143
I0630 15:32:49.634749 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 15:32:49.634757 29777 sgd_solver.cpp:106] Iteration 253100, lr = 0.00209063
I0630 15:33:05.908680 29777 solver.cpp:290] Iteration 253200 (6.14496 iter/s, 16.2735s/100 iter), loss = 1.57143
I0630 15:33:05.908769 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 15:33:05.908782 29777 sgd_solver.cpp:106] Iteration 253200, lr = 0.0020875
I0630 15:33:22.185606 29777 solver.cpp:290] Iteration 253300 (6.14387 iter/s, 16.2764s/100 iter), loss = 1.63095
I0630 15:33:22.185631 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 15:33:22.185641 29777 sgd_solver.cpp:106] Iteration 253300, lr = 0.00208437
I0630 15:33:38.404901 29777 solver.cpp:290] Iteration 253400 (6.16568 iter/s, 16.2188s/100 iter), loss = 0.892857
I0630 15:33:38.405129 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 15:33:38.405239 29777 sgd_solver.cpp:106] Iteration 253400, lr = 0.00208125
I0630 15:33:54.929572 29777 solver.cpp:290] Iteration 253500 (6.05178 iter/s, 16.524s/100 iter), loss = 1.32143
I0630 15:33:54.929596 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 15:33:54.929605 29777 sgd_solver.cpp:106] Iteration 253500, lr = 0.00207812
I0630 15:34:11.257833 29777 solver.cpp:290] Iteration 253600 (6.12453 iter/s, 16.3278s/100 iter), loss = 1.40476
I0630 15:34:11.257925 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 15:34:11.257941 29777 sgd_solver.cpp:106] Iteration 253600, lr = 0.002075
I0630 15:34:27.595573 29777 solver.cpp:290] Iteration 253700 (6.121 iter/s, 16.3372s/100 iter), loss = 1.55952
I0630 15:34:27.595598 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 15:34:27.595607 29777 sgd_solver.cpp:106] Iteration 253700, lr = 0.00207187
I0630 15:34:43.963559 29777 solver.cpp:290] Iteration 253800 (6.10966 iter/s, 16.3675s/100 iter), loss = 1.20238
I0630 15:34:43.963656 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 15:34:43.963666 29777 sgd_solver.cpp:106] Iteration 253800, lr = 0.00206875
I0630 15:35:00.088942 29777 solver.cpp:290] Iteration 253900 (6.20161 iter/s, 16.1248s/100 iter), loss = 1.13095
I0630 15:35:00.088966 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 15:35:00.088974 29777 sgd_solver.cpp:106] Iteration 253900, lr = 0.00206563
I0630 15:35:16.194032 29777 solver.cpp:354] Sparsity after update:
I0630 15:35:16.197434 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:35:16.197445 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:35:16.197455 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:35:16.197459 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:35:16.197463 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:35:16.197465 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:35:16.197469 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:35:16.197473 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:35:16.197475 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:35:16.197479 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:35:16.197484 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:35:16.197489 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:35:16.197492 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:35:16.197628 29777 solver.cpp:471] Iteration 254000, Testing net (#0)
I0630 15:35:44.944841 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 15:36:24.725229 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55742
I0630 15:36:24.725273 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.793121
I0630 15:36:24.725281 29777 solver.cpp:544]     Test net output #2: loss = 1.57578 (* 1 = 1.57578 loss)
I0630 15:36:24.918277 29777 solver.cpp:290] Iteration 254000 (1.17887 iter/s, 84.827s/100 iter), loss = 1.04762
I0630 15:36:24.918303 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 15:36:24.918313 29777 sgd_solver.cpp:106] Iteration 254000, lr = 0.0020625
I0630 15:36:40.391065 29777 solver.cpp:290] Iteration 254100 (6.46315 iter/s, 15.4723s/100 iter), loss = 1.04762
I0630 15:36:40.391155 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 15:36:40.391185 29777 sgd_solver.cpp:106] Iteration 254100, lr = 0.00205937
I0630 15:36:56.532610 29777 solver.cpp:290] Iteration 254200 (6.1954 iter/s, 16.141s/100 iter), loss = 1.08333
I0630 15:36:56.532717 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 15:36:56.532737 29777 sgd_solver.cpp:106] Iteration 254200, lr = 0.00205625
I0630 15:37:13.325111 29777 solver.cpp:290] Iteration 254300 (5.95524 iter/s, 16.7919s/100 iter), loss = 1.15476
I0630 15:37:13.325134 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 15:37:13.325140 29777 sgd_solver.cpp:106] Iteration 254300, lr = 0.00205312
I0630 15:37:29.607538 29777 solver.cpp:290] Iteration 254400 (6.14177 iter/s, 16.282s/100 iter), loss = 1.44048
I0630 15:37:29.607642 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 15:37:29.607652 29777 sgd_solver.cpp:106] Iteration 254400, lr = 0.00205
I0630 15:37:45.664497 29777 solver.cpp:290] Iteration 254500 (6.22804 iter/s, 16.0564s/100 iter), loss = 1.44048
I0630 15:37:45.664553 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 15:37:45.664599 29777 sgd_solver.cpp:106] Iteration 254500, lr = 0.00204687
I0630 15:38:01.917445 29777 solver.cpp:290] Iteration 254600 (6.15292 iter/s, 16.2524s/100 iter), loss = 1.44048
I0630 15:38:01.917528 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 15:38:01.917539 29777 sgd_solver.cpp:106] Iteration 254600, lr = 0.00204375
I0630 15:38:18.132956 29777 solver.cpp:290] Iteration 254700 (6.16714 iter/s, 16.215s/100 iter), loss = 1.33333
I0630 15:38:18.132997 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 15:38:18.133010 29777 sgd_solver.cpp:106] Iteration 254700, lr = 0.00204063
I0630 15:38:34.416740 29777 solver.cpp:290] Iteration 254800 (6.14126 iter/s, 16.2833s/100 iter), loss = 1.34524
I0630 15:38:34.416862 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 15:38:34.416872 29777 sgd_solver.cpp:106] Iteration 254800, lr = 0.0020375
I0630 15:38:50.731081 29777 solver.cpp:290] Iteration 254900 (6.12979 iter/s, 16.3138s/100 iter), loss = 1.30952
I0630 15:38:50.731111 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 15:38:50.731123 29777 sgd_solver.cpp:106] Iteration 254900, lr = 0.00203437
I0630 15:39:06.600706 29777 solver.cpp:354] Sparsity after update:
I0630 15:39:06.621060 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:39:06.621074 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:39:06.621084 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:39:06.621088 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:39:06.621093 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:39:06.621098 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:39:06.621100 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:39:06.621104 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:39:06.621107 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:39:06.621111 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:39:06.621114 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:39:06.621119 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:39:06.621124 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:39:06.783799 29777 solver.cpp:290] Iteration 255000 (6.22966 iter/s, 16.0522s/100 iter), loss = 0.857143
I0630 15:39:06.783833 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 15:39:06.783841 29777 sgd_solver.cpp:106] Iteration 255000, lr = 0.00203125
I0630 15:39:22.884285 29777 solver.cpp:290] Iteration 255100 (6.21117 iter/s, 16.1s/100 iter), loss = 1.02381
I0630 15:39:22.884308 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 15:39:22.884315 29777 sgd_solver.cpp:106] Iteration 255100, lr = 0.00202812
I0630 15:39:38.961735 29777 solver.cpp:290] Iteration 255200 (6.22007 iter/s, 16.077s/100 iter), loss = 1.28571
I0630 15:39:38.961832 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 15:39:38.961843 29777 sgd_solver.cpp:106] Iteration 255200, lr = 0.002025
I0630 15:39:54.941509 29777 solver.cpp:290] Iteration 255300 (6.25812 iter/s, 15.9792s/100 iter), loss = 1
I0630 15:39:54.941563 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 15:39:54.941584 29777 sgd_solver.cpp:106] Iteration 255300, lr = 0.00202187
I0630 15:40:10.952944 29777 solver.cpp:290] Iteration 255400 (6.24572 iter/s, 16.011s/100 iter), loss = 0.916667
I0630 15:40:10.953021 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 15:40:10.953029 29777 sgd_solver.cpp:106] Iteration 255400, lr = 0.00201875
I0630 15:40:27.108242 29777 solver.cpp:290] Iteration 255500 (6.19012 iter/s, 16.1548s/100 iter), loss = 1.58333
I0630 15:40:27.108268 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 15:40:27.108276 29777 sgd_solver.cpp:106] Iteration 255500, lr = 0.00201563
I0630 15:40:43.289672 29777 solver.cpp:290] Iteration 255600 (6.1801 iter/s, 16.181s/100 iter), loss = 1.30952
I0630 15:40:43.289775 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 15:40:43.289783 29777 sgd_solver.cpp:106] Iteration 255600, lr = 0.0020125
I0630 15:40:59.477193 29777 solver.cpp:290] Iteration 255700 (6.17781 iter/s, 16.187s/100 iter), loss = 1.4881
I0630 15:40:59.477263 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 15:40:59.477286 29777 sgd_solver.cpp:106] Iteration 255700, lr = 0.00200938
I0630 15:41:15.461904 29777 solver.cpp:290] Iteration 255800 (6.25618 iter/s, 15.9842s/100 iter), loss = 1.08333
I0630 15:41:15.462016 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 15:41:15.462030 29777 sgd_solver.cpp:106] Iteration 255800, lr = 0.00200625
I0630 15:41:31.456094 29777 solver.cpp:290] Iteration 255900 (6.25248 iter/s, 15.9936s/100 iter), loss = 1.5119
I0630 15:41:31.456120 29777 solver.cpp:309]     Train net output #0: loss = 1.71429 (* 1 = 1.71429 loss)
I0630 15:41:31.456138 29777 sgd_solver.cpp:106] Iteration 255900, lr = 0.00200312
I0630 15:41:47.348697 29777 solver.cpp:354] Sparsity after update:
I0630 15:41:47.350126 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:41:47.350133 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:41:47.350142 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:41:47.350143 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:41:47.350147 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:41:47.350148 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:41:47.350152 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:41:47.350153 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:41:47.350155 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:41:47.350158 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:41:47.350160 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:41:47.350162 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:41:47.350164 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:41:47.350252 29777 solver.cpp:471] Iteration 256000, Testing net (#0)
I0630 15:42:14.113710 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 15:42:46.847990 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55804
I0630 15:42:46.848098 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.792542
I0630 15:42:46.848109 29777 solver.cpp:544]     Test net output #2: loss = 1.57508 (* 1 = 1.57508 loss)
I0630 15:42:47.081357 29777 solver.cpp:290] Iteration 256000 (1.32235 iter/s, 75.6232s/100 iter), loss = 1.58333
I0630 15:42:47.081409 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 15:42:47.081430 29777 sgd_solver.cpp:106] Iteration 256000, lr = 0.002
I0630 15:43:03.406507 29777 solver.cpp:290] Iteration 256100 (6.1257 iter/s, 16.3247s/100 iter), loss = 1.2381
I0630 15:43:03.406534 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 15:43:03.406543 29777 sgd_solver.cpp:106] Iteration 256100, lr = 0.00199687
I0630 15:43:19.496361 29777 solver.cpp:290] Iteration 256200 (6.21528 iter/s, 16.0894s/100 iter), loss = 0.988095
I0630 15:43:19.496433 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 15:43:19.496443 29777 sgd_solver.cpp:106] Iteration 256200, lr = 0.00199375
I0630 15:43:35.580618 29777 solver.cpp:290] Iteration 256300 (6.21746 iter/s, 16.0837s/100 iter), loss = 1.13095
I0630 15:43:35.580643 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 15:43:35.580651 29777 sgd_solver.cpp:106] Iteration 256300, lr = 0.00199063
I0630 15:43:51.791316 29777 solver.cpp:290] Iteration 256400 (6.16894 iter/s, 16.2102s/100 iter), loss = 1.47619
I0630 15:43:51.791386 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 15:43:51.791394 29777 sgd_solver.cpp:106] Iteration 256400, lr = 0.0019875
I0630 15:44:08.020476 29777 solver.cpp:290] Iteration 256500 (6.16194 iter/s, 16.2287s/100 iter), loss = 1.46429
I0630 15:44:08.020514 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 15:44:08.020522 29777 sgd_solver.cpp:106] Iteration 256500, lr = 0.00198438
I0630 15:44:24.194913 29777 solver.cpp:290] Iteration 256600 (6.18277 iter/s, 16.174s/100 iter), loss = 1.32143
I0630 15:44:24.194994 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 15:44:24.195009 29777 sgd_solver.cpp:106] Iteration 256600, lr = 0.00198125
I0630 15:44:40.268066 29777 solver.cpp:290] Iteration 256700 (6.22175 iter/s, 16.0726s/100 iter), loss = 1.69048
I0630 15:44:40.268090 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 15:44:40.268097 29777 sgd_solver.cpp:106] Iteration 256700, lr = 0.00197812
I0630 15:44:56.260305 29777 solver.cpp:290] Iteration 256800 (6.25321 iter/s, 15.9918s/100 iter), loss = 1.41667
I0630 15:44:56.260401 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 15:44:56.260418 29777 sgd_solver.cpp:106] Iteration 256800, lr = 0.001975
I0630 15:45:12.381429 29777 solver.cpp:290] Iteration 256900 (6.20324 iter/s, 16.1206s/100 iter), loss = 1.39286
I0630 15:45:12.381453 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 15:45:12.381459 29777 sgd_solver.cpp:106] Iteration 256900, lr = 0.00197187
I0630 15:45:28.652824 29777 solver.cpp:354] Sparsity after update:
I0630 15:45:28.673185 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:45:28.673197 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:45:28.673204 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:45:28.673207 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:45:28.673208 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:45:28.673210 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:45:28.673213 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:45:28.673214 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:45:28.673216 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:45:28.673218 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:45:28.673226 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:45:28.673228 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:45:28.673230 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:45:28.829107 29777 solver.cpp:290] Iteration 257000 (6.08006 iter/s, 16.4472s/100 iter), loss = 1.17857
I0630 15:45:28.829130 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 15:45:28.829138 29777 sgd_solver.cpp:106] Iteration 257000, lr = 0.00196875
I0630 15:45:45.108896 29777 solver.cpp:290] Iteration 257100 (6.14276 iter/s, 16.2793s/100 iter), loss = 1.27381
I0630 15:45:45.108937 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 15:45:45.108948 29777 sgd_solver.cpp:106] Iteration 257100, lr = 0.00196563
I0630 15:46:01.512224 29777 solver.cpp:290] Iteration 257200 (6.0965 iter/s, 16.4029s/100 iter), loss = 0.642857
I0630 15:46:01.512307 29777 solver.cpp:309]     Train net output #0: loss = 0.595238 (* 1 = 0.595238 loss)
I0630 15:46:01.512328 29777 sgd_solver.cpp:106] Iteration 257200, lr = 0.0019625
I0630 15:46:18.256892 29777 solver.cpp:290] Iteration 257300 (5.97224 iter/s, 16.7441s/100 iter), loss = 0.964286
I0630 15:46:18.256937 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 15:46:18.256949 29777 sgd_solver.cpp:106] Iteration 257300, lr = 0.00195938
I0630 15:46:34.471421 29777 solver.cpp:290] Iteration 257400 (6.16749 iter/s, 16.2141s/100 iter), loss = 1.11905
I0630 15:46:34.471511 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 15:46:34.471527 29777 sgd_solver.cpp:106] Iteration 257400, lr = 0.00195625
I0630 15:46:51.104604 29777 solver.cpp:290] Iteration 257500 (6.01227 iter/s, 16.6327s/100 iter), loss = 1.33333
I0630 15:46:51.104629 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 15:46:51.104638 29777 sgd_solver.cpp:106] Iteration 257500, lr = 0.00195312
I0630 15:47:07.491976 29777 solver.cpp:290] Iteration 257600 (6.10243 iter/s, 16.3869s/100 iter), loss = 1.30952
I0630 15:47:07.492069 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 15:47:07.492079 29777 sgd_solver.cpp:106] Iteration 257600, lr = 0.00195
I0630 15:47:24.394891 29777 solver.cpp:290] Iteration 257700 (5.91633 iter/s, 16.9024s/100 iter), loss = 1.03571
I0630 15:47:24.394973 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 15:47:24.395001 29777 sgd_solver.cpp:106] Iteration 257700, lr = 0.00194687
I0630 15:47:40.773247 29777 solver.cpp:290] Iteration 257800 (6.10581 iter/s, 16.3778s/100 iter), loss = 1.2381
I0630 15:47:40.773355 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 15:47:40.773365 29777 sgd_solver.cpp:106] Iteration 257800, lr = 0.00194375
I0630 15:47:57.068112 29777 solver.cpp:290] Iteration 257900 (6.13711 iter/s, 16.2943s/100 iter), loss = 1.78571
I0630 15:47:57.068135 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 15:47:57.068141 29777 sgd_solver.cpp:106] Iteration 257900, lr = 0.00194062
I0630 15:48:13.250954 29777 solver.cpp:354] Sparsity after update:
I0630 15:48:13.252423 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:48:13.252434 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:48:13.252447 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:48:13.252452 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:48:13.252457 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:48:13.252462 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:48:13.252468 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:48:13.252473 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:48:13.252480 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:48:13.252486 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:48:13.252493 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:48:13.252498 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:48:13.252506 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:48:13.252627 29777 solver.cpp:471] Iteration 258000, Testing net (#0)
I0630 15:48:43.528952 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 15:49:22.237642 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.558779
I0630 15:49:22.237679 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.792581
I0630 15:49:22.237685 29777 solver.cpp:544]     Test net output #2: loss = 1.5753 (* 1 = 1.5753 loss)
I0630 15:49:22.476742 29777 solver.cpp:290] Iteration 258000 (1.17087 iter/s, 85.4063s/100 iter), loss = 2.08333
I0630 15:49:22.476840 29777 solver.cpp:309]     Train net output #0: loss = 2.33333 (* 1 = 2.33333 loss)
I0630 15:49:22.476877 29777 sgd_solver.cpp:106] Iteration 258000, lr = 0.0019375
I0630 15:49:38.991432 29777 solver.cpp:290] Iteration 258100 (6.05541 iter/s, 16.5142s/100 iter), loss = 1.4881
I0630 15:49:38.991472 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 15:49:38.991487 29777 sgd_solver.cpp:106] Iteration 258100, lr = 0.00193438
I0630 15:49:55.141630 29777 solver.cpp:290] Iteration 258200 (6.19205 iter/s, 16.1497s/100 iter), loss = 1.04762
I0630 15:49:55.141683 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 15:49:55.141691 29777 sgd_solver.cpp:106] Iteration 258200, lr = 0.00193125
I0630 15:50:11.510462 29777 solver.cpp:290] Iteration 258300 (6.10935 iter/s, 16.3683s/100 iter), loss = 1.60714
I0630 15:50:11.510486 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 15:50:11.510494 29777 sgd_solver.cpp:106] Iteration 258300, lr = 0.00192812
I0630 15:50:27.988457 29777 solver.cpp:290] Iteration 258400 (6.06887 iter/s, 16.4775s/100 iter), loss = 1.11905
I0630 15:50:27.988549 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 15:50:27.988561 29777 sgd_solver.cpp:106] Iteration 258400, lr = 0.001925
I0630 15:50:44.326650 29777 solver.cpp:290] Iteration 258500 (6.12082 iter/s, 16.3377s/100 iter), loss = 1.44048
I0630 15:50:44.326674 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 15:50:44.326683 29777 sgd_solver.cpp:106] Iteration 258500, lr = 0.00192187
I0630 15:51:00.508365 29777 solver.cpp:290] Iteration 258600 (6.17999 iter/s, 16.1812s/100 iter), loss = 1.2619
I0630 15:51:00.508535 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 15:51:00.508568 29777 sgd_solver.cpp:106] Iteration 258600, lr = 0.00191875
I0630 15:51:16.644847 29777 solver.cpp:290] Iteration 258700 (6.19736 iter/s, 16.1359s/100 iter), loss = 1.45238
I0630 15:51:16.644870 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 15:51:16.644876 29777 sgd_solver.cpp:106] Iteration 258700, lr = 0.00191562
I0630 15:51:32.843067 29777 solver.cpp:290] Iteration 258800 (6.17369 iter/s, 16.1978s/100 iter), loss = 1.25
I0630 15:51:32.843175 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 15:51:32.843185 29777 sgd_solver.cpp:106] Iteration 258800, lr = 0.0019125
I0630 15:51:49.019193 29777 solver.cpp:290] Iteration 258900 (6.18215 iter/s, 16.1756s/100 iter), loss = 1.21429
I0630 15:51:49.019218 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 15:51:49.019224 29777 sgd_solver.cpp:106] Iteration 258900, lr = 0.00190938
I0630 15:52:05.058890 29777 solver.cpp:354] Sparsity after update:
I0630 15:52:05.079272 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:52:05.079288 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:52:05.079298 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:52:05.079301 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:52:05.079304 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:52:05.079308 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:52:05.079311 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:52:05.079314 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:52:05.079318 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:52:05.079321 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:52:05.079324 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:52:05.079327 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:52:05.079341 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:52:05.237354 29777 solver.cpp:290] Iteration 259000 (6.1661 iter/s, 16.2177s/100 iter), loss = 1.7619
I0630 15:52:05.237376 29777 solver.cpp:309]     Train net output #0: loss = 1.88095 (* 1 = 1.88095 loss)
I0630 15:52:05.237383 29777 sgd_solver.cpp:106] Iteration 259000, lr = 0.00190625
I0630 15:52:21.384455 29777 solver.cpp:290] Iteration 259100 (6.19324 iter/s, 16.1466s/100 iter), loss = 1.30952
I0630 15:52:21.384481 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 15:52:21.384490 29777 sgd_solver.cpp:106] Iteration 259100, lr = 0.00190312
I0630 15:52:37.763417 29777 solver.cpp:290] Iteration 259200 (6.10556 iter/s, 16.3785s/100 iter), loss = 1.38095
I0630 15:52:37.763523 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 15:52:37.763533 29777 sgd_solver.cpp:106] Iteration 259200, lr = 0.0019
I0630 15:52:53.997654 29777 solver.cpp:290] Iteration 259300 (6.16002 iter/s, 16.2337s/100 iter), loss = 1.65476
I0630 15:52:53.997678 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 15:52:53.997684 29777 sgd_solver.cpp:106] Iteration 259300, lr = 0.00189687
I0630 15:53:10.697499 29777 solver.cpp:290] Iteration 259400 (5.98825 iter/s, 16.6994s/100 iter), loss = 1.16667
I0630 15:53:10.697616 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 15:53:10.697646 29777 sgd_solver.cpp:106] Iteration 259400, lr = 0.00189375
I0630 15:53:26.910004 29777 solver.cpp:290] Iteration 259500 (6.16828 iter/s, 16.212s/100 iter), loss = 1.04762
I0630 15:53:26.910030 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 15:53:26.910038 29777 sgd_solver.cpp:106] Iteration 259500, lr = 0.00189062
I0630 15:53:43.346637 29777 solver.cpp:290] Iteration 259600 (6.08414 iter/s, 16.4362s/100 iter), loss = 1.02381
I0630 15:53:43.346998 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 15:53:43.347010 29777 sgd_solver.cpp:106] Iteration 259600, lr = 0.0018875
I0630 15:53:59.757474 29777 solver.cpp:290] Iteration 259700 (6.09383 iter/s, 16.41s/100 iter), loss = 1.61905
I0630 15:53:59.757524 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 15:53:59.757537 29777 sgd_solver.cpp:106] Iteration 259700, lr = 0.00188438
I0630 15:54:16.099812 29777 solver.cpp:290] Iteration 259800 (6.11926 iter/s, 16.3419s/100 iter), loss = 1.15476
I0630 15:54:16.099891 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 15:54:16.099903 29777 sgd_solver.cpp:106] Iteration 259800, lr = 0.00188125
I0630 15:54:32.211781 29777 solver.cpp:290] Iteration 259900 (6.20676 iter/s, 16.1115s/100 iter), loss = 1.33333
I0630 15:54:32.211805 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 15:54:32.211812 29777 sgd_solver.cpp:106] Iteration 259900, lr = 0.00187813
I0630 15:54:48.303933 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_260000.caffemodel
I0630 15:54:48.323426 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_260000.solverstate
I0630 15:54:48.332121 29777 solver.cpp:354] Sparsity after update:
I0630 15:54:48.333070 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:54:48.333078 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:54:48.333086 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:54:48.333087 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:54:48.333089 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:54:48.333091 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:54:48.333093 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:54:48.333096 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:54:48.333097 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:54:48.333099 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:54:48.333101 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:54:48.333103 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:54:48.333106 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:54:48.333209 29777 solver.cpp:471] Iteration 260000, Testing net (#0)
I0630 15:55:17.416335 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 15:55:51.572190 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55716
I0630 15:55:51.572285 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.792722
I0630 15:55:51.572295 29777 solver.cpp:544]     Test net output #2: loss = 1.56832 (* 1 = 1.56832 loss)
I0630 15:55:51.749517 29777 solver.cpp:290] Iteration 260000 (1.2573 iter/s, 79.5356s/100 iter), loss = 1.05952
I0630 15:55:51.749543 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 15:55:51.749552 29777 sgd_solver.cpp:106] Iteration 260000, lr = 0.001875
I0630 15:56:07.919903 29777 solver.cpp:290] Iteration 260100 (6.18434 iter/s, 16.1699s/100 iter), loss = 1.38095
I0630 15:56:07.920039 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 15:56:07.920065 29777 sgd_solver.cpp:106] Iteration 260100, lr = 0.00187187
I0630 15:56:24.094065 29777 solver.cpp:290] Iteration 260200 (6.18292 iter/s, 16.1736s/100 iter), loss = 1.07143
I0630 15:56:24.094158 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 15:56:24.094169 29777 sgd_solver.cpp:106] Iteration 260200, lr = 0.00186875
I0630 15:56:40.191532 29777 solver.cpp:290] Iteration 260300 (6.21236 iter/s, 16.0969s/100 iter), loss = 1.02381
I0630 15:56:40.191555 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 15:56:40.191562 29777 sgd_solver.cpp:106] Iteration 260300, lr = 0.00186562
I0630 15:56:56.233181 29777 solver.cpp:290] Iteration 260400 (6.23395 iter/s, 16.0412s/100 iter), loss = 1.5
I0630 15:56:56.233273 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 15:56:56.233283 29777 sgd_solver.cpp:106] Iteration 260400, lr = 0.0018625
I0630 15:57:12.365967 29777 solver.cpp:290] Iteration 260500 (6.19876 iter/s, 16.1323s/100 iter), loss = 1.08333
I0630 15:57:12.365991 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 15:57:12.366000 29777 sgd_solver.cpp:106] Iteration 260500, lr = 0.00185938
I0630 15:57:28.541085 29777 solver.cpp:290] Iteration 260600 (6.18251 iter/s, 16.1747s/100 iter), loss = 0.857143
I0630 15:57:28.541193 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 15:57:28.541203 29777 sgd_solver.cpp:106] Iteration 260600, lr = 0.00185625
I0630 15:57:44.737468 29777 solver.cpp:290] Iteration 260700 (6.17442 iter/s, 16.1958s/100 iter), loss = 0.833333
I0630 15:57:44.737491 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 15:57:44.737500 29777 sgd_solver.cpp:106] Iteration 260700, lr = 0.00185313
I0630 15:58:00.794750 29777 solver.cpp:290] Iteration 260800 (6.22788 iter/s, 16.0568s/100 iter), loss = 1.15476
I0630 15:58:00.794857 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 15:58:00.794867 29777 sgd_solver.cpp:106] Iteration 260800, lr = 0.00185
I0630 15:58:16.799566 29777 solver.cpp:290] Iteration 260900 (6.24833 iter/s, 16.0043s/100 iter), loss = 1.20238
I0630 15:58:16.799599 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 15:58:16.799612 29777 sgd_solver.cpp:106] Iteration 260900, lr = 0.00184687
I0630 15:58:32.791018 29777 solver.cpp:354] Sparsity after update:
I0630 15:58:32.811933 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 15:58:32.811947 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 15:58:32.811959 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 15:58:32.811962 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 15:58:32.811975 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 15:58:32.811985 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 15:58:32.811993 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 15:58:32.812002 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 15:58:32.812016 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 15:58:32.812022 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 15:58:32.812031 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 15:58:32.812041 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 15:58:32.812046 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 15:58:32.974967 29777 solver.cpp:290] Iteration 261000 (6.18241 iter/s, 16.1749s/100 iter), loss = 1.32143
I0630 15:58:32.975011 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 15:58:32.975025 29777 sgd_solver.cpp:106] Iteration 261000, lr = 0.00184375
I0630 15:58:49.112702 29777 solver.cpp:290] Iteration 261100 (6.19684 iter/s, 16.1373s/100 iter), loss = 1.79762
I0630 15:58:49.112722 29777 solver.cpp:309]     Train net output #0: loss = 2.07143 (* 1 = 2.07143 loss)
I0630 15:58:49.112730 29777 sgd_solver.cpp:106] Iteration 261100, lr = 0.00184062
I0630 15:59:05.106362 29777 solver.cpp:290] Iteration 261200 (6.25265 iter/s, 15.9932s/100 iter), loss = 1.2619
I0630 15:59:05.106464 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 15:59:05.106474 29777 sgd_solver.cpp:106] Iteration 261200, lr = 0.0018375
I0630 15:59:21.236981 29777 solver.cpp:290] Iteration 261300 (6.19959 iter/s, 16.1301s/100 iter), loss = 0.880952
I0630 15:59:21.237004 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 15:59:21.237012 29777 sgd_solver.cpp:106] Iteration 261300, lr = 0.00183438
I0630 15:59:37.481923 29777 solver.cpp:290] Iteration 261400 (6.15594 iter/s, 16.2445s/100 iter), loss = 0.892857
I0630 15:59:37.482034 29777 solver.cpp:309]     Train net output #0: loss = 0.595238 (* 1 = 0.595238 loss)
I0630 15:59:37.482046 29777 sgd_solver.cpp:106] Iteration 261400, lr = 0.00183125
I0630 15:59:53.748235 29777 solver.cpp:290] Iteration 261500 (6.14788 iter/s, 16.2658s/100 iter), loss = 1.27381
I0630 15:59:53.748260 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 15:59:53.748266 29777 sgd_solver.cpp:106] Iteration 261500, lr = 0.00182813
I0630 16:00:09.924247 29777 solver.cpp:290] Iteration 261600 (6.18217 iter/s, 16.1755s/100 iter), loss = 1.34524
I0630 16:00:09.924383 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 16:00:09.924415 29777 sgd_solver.cpp:106] Iteration 261600, lr = 0.001825
I0630 16:00:25.990659 29777 solver.cpp:290] Iteration 261700 (6.22438 iter/s, 16.0658s/100 iter), loss = 1.41667
I0630 16:00:25.990684 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 16:00:25.990694 29777 sgd_solver.cpp:106] Iteration 261700, lr = 0.00182187
I0630 16:00:42.081673 29777 solver.cpp:290] Iteration 261800 (6.21483 iter/s, 16.0905s/100 iter), loss = 1.30952
I0630 16:00:42.081787 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 16:00:42.081804 29777 sgd_solver.cpp:106] Iteration 261800, lr = 0.00181875
I0630 16:00:58.170698 29777 solver.cpp:290] Iteration 261900 (6.21563 iter/s, 16.0885s/100 iter), loss = 1.35714
I0630 16:00:58.170747 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 16:00:58.170760 29777 sgd_solver.cpp:106] Iteration 261900, lr = 0.00181562
I0630 16:01:14.120252 29777 solver.cpp:354] Sparsity after update:
I0630 16:01:14.122079 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:01:14.122097 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:01:14.122118 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:01:14.122129 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:01:14.122140 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:01:14.122150 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:01:14.122162 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:01:14.122171 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:01:14.122181 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:01:14.122190 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:01:14.122200 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:01:14.122207 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:01:14.122217 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:01:14.122416 29777 solver.cpp:471] Iteration 262000, Testing net (#0)
I0630 16:01:38.919246 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 16:02:19.521041 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55768
I0630 16:02:19.521150 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.792342
I0630 16:02:19.521160 29777 solver.cpp:544]     Test net output #2: loss = 1.56872 (* 1 = 1.56872 loss)
I0630 16:02:19.713402 29777 solver.cpp:290] Iteration 262000 (1.22638 iter/s, 81.5405s/100 iter), loss = 1.16667
I0630 16:02:19.713454 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 16:02:19.713474 29777 sgd_solver.cpp:106] Iteration 262000, lr = 0.0018125
I0630 16:02:36.193545 29777 solver.cpp:290] Iteration 262100 (6.06809 iter/s, 16.4796s/100 iter), loss = 1.27381
I0630 16:02:36.193634 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 16:02:36.193666 29777 sgd_solver.cpp:106] Iteration 262100, lr = 0.00180938
I0630 16:02:52.446241 29777 solver.cpp:290] Iteration 262200 (6.15302 iter/s, 16.2522s/100 iter), loss = 1.27381
I0630 16:02:52.446312 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 16:02:52.446321 29777 sgd_solver.cpp:106] Iteration 262200, lr = 0.00180625
I0630 16:03:09.033488 29777 solver.cpp:290] Iteration 262300 (6.02893 iter/s, 16.5867s/100 iter), loss = 1.38095
I0630 16:03:09.033814 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 16:03:09.033991 29777 sgd_solver.cpp:106] Iteration 262300, lr = 0.00180313
I0630 16:03:25.626636 29777 solver.cpp:290] Iteration 262400 (6.02684 iter/s, 16.5924s/100 iter), loss = 1.11905
I0630 16:03:25.626744 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 16:03:25.626751 29777 sgd_solver.cpp:106] Iteration 262400, lr = 0.0018
I0630 16:03:41.847774 29777 solver.cpp:290] Iteration 262500 (6.165 iter/s, 16.2206s/100 iter), loss = 1.09524
I0630 16:03:41.847800 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 16:03:41.847808 29777 sgd_solver.cpp:106] Iteration 262500, lr = 0.00179687
I0630 16:03:58.029654 29777 solver.cpp:290] Iteration 262600 (6.17993 iter/s, 16.1814s/100 iter), loss = 1.38095
I0630 16:03:58.029747 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 16:03:58.029757 29777 sgd_solver.cpp:106] Iteration 262600, lr = 0.00179375
I0630 16:04:14.500898 29777 solver.cpp:290] Iteration 262700 (6.07139 iter/s, 16.4707s/100 iter), loss = 1.30952
I0630 16:04:14.500953 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 16:04:14.500978 29777 sgd_solver.cpp:106] Iteration 262700, lr = 0.00179062
I0630 16:04:30.961112 29777 solver.cpp:290] Iteration 262800 (6.07544 iter/s, 16.4597s/100 iter), loss = 1.09524
I0630 16:04:30.961205 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 16:04:30.961215 29777 sgd_solver.cpp:106] Iteration 262800, lr = 0.0017875
I0630 16:04:47.458186 29777 solver.cpp:290] Iteration 262900 (6.06188 iter/s, 16.4965s/100 iter), loss = 1.32143
I0630 16:04:47.458232 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 16:04:47.458253 29777 sgd_solver.cpp:106] Iteration 262900, lr = 0.00178437
I0630 16:05:03.576735 29777 solver.cpp:354] Sparsity after update:
I0630 16:05:03.597043 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:05:03.597095 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:05:03.597121 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:05:03.597132 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:05:03.597143 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:05:03.597157 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:05:03.597168 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:05:03.597177 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:05:03.597187 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:05:03.597195 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:05:03.597204 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:05:03.597213 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:05:03.597223 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:05:03.761271 29777 solver.cpp:290] Iteration 263000 (6.13399 iter/s, 16.3026s/100 iter), loss = 1.28571
I0630 16:05:03.761314 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 16:05:03.761332 29777 sgd_solver.cpp:106] Iteration 263000, lr = 0.00178125
I0630 16:05:20.068538 29777 solver.cpp:290] Iteration 263100 (6.13242 iter/s, 16.3068s/100 iter), loss = 1.25
I0630 16:05:20.068564 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 16:05:20.068573 29777 sgd_solver.cpp:106] Iteration 263100, lr = 0.00177813
I0630 16:05:36.241583 29777 solver.cpp:290] Iteration 263200 (6.1833 iter/s, 16.1726s/100 iter), loss = 1.41667
I0630 16:05:36.241653 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 16:05:36.241663 29777 sgd_solver.cpp:106] Iteration 263200, lr = 0.001775
I0630 16:05:52.400758 29777 solver.cpp:290] Iteration 263300 (6.18863 iter/s, 16.1587s/100 iter), loss = 1.21429
I0630 16:05:52.400806 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 16:05:52.400912 29777 sgd_solver.cpp:106] Iteration 263300, lr = 0.00177187
I0630 16:06:08.479953 29777 solver.cpp:290] Iteration 263400 (6.2194 iter/s, 16.0787s/100 iter), loss = 0.97619
I0630 16:06:08.480034 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 16:06:08.480042 29777 sgd_solver.cpp:106] Iteration 263400, lr = 0.00176875
I0630 16:06:24.776469 29777 solver.cpp:290] Iteration 263500 (6.13647 iter/s, 16.296s/100 iter), loss = 1.4881
I0630 16:06:24.776491 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 16:06:24.776497 29777 sgd_solver.cpp:106] Iteration 263500, lr = 0.00176562
I0630 16:06:41.374871 29777 solver.cpp:290] Iteration 263600 (6.02485 iter/s, 16.5979s/100 iter), loss = 1.66667
I0630 16:06:41.375383 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 16:06:41.375401 29777 sgd_solver.cpp:106] Iteration 263600, lr = 0.0017625
I0630 16:06:57.806583 29777 solver.cpp:290] Iteration 263700 (6.08615 iter/s, 16.4308s/100 iter), loss = 1.65476
I0630 16:06:57.806608 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 16:06:57.806617 29777 sgd_solver.cpp:106] Iteration 263700, lr = 0.00175937
I0630 16:07:13.833518 29777 solver.cpp:290] Iteration 263800 (6.23967 iter/s, 16.0265s/100 iter), loss = 1.35714
I0630 16:07:13.833602 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 16:07:13.833609 29777 sgd_solver.cpp:106] Iteration 263800, lr = 0.00175625
I0630 16:07:30.043263 29777 solver.cpp:290] Iteration 263900 (6.16932 iter/s, 16.2092s/100 iter), loss = 1.5
I0630 16:07:30.043285 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 16:07:30.043292 29777 sgd_solver.cpp:106] Iteration 263900, lr = 0.00175313
I0630 16:07:45.880270 29777 solver.cpp:354] Sparsity after update:
I0630 16:07:45.885915 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:07:45.885928 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:07:45.885936 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:07:45.885938 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:07:45.885941 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:07:45.885943 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:07:45.885946 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:07:45.885947 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:07:45.885951 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:07:45.885956 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:07:45.885958 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:07:45.885962 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:07:45.885964 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:07:45.886108 29777 solver.cpp:471] Iteration 264000, Testing net (#0)
I0630 16:08:17.888103 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 16:08:54.759668 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55652
I0630 16:08:54.759770 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.792321
I0630 16:08:54.759779 29777 solver.cpp:544]     Test net output #2: loss = 1.57794 (* 1 = 1.57794 loss)
I0630 16:08:54.932385 29777 solver.cpp:290] Iteration 264000 (1.17804 iter/s, 84.8868s/100 iter), loss = 1.16667
I0630 16:08:54.932412 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 16:08:54.932430 29777 sgd_solver.cpp:106] Iteration 264000, lr = 0.00175
I0630 16:09:11.500831 29777 solver.cpp:290] Iteration 264100 (6.03574 iter/s, 16.568s/100 iter), loss = 1.29762
I0630 16:09:11.500862 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 16:09:11.500869 29777 sgd_solver.cpp:106] Iteration 264100, lr = 0.00174688
I0630 16:09:27.608811 29777 solver.cpp:290] Iteration 264200 (6.20829 iter/s, 16.1075s/100 iter), loss = 1.05952
I0630 16:09:27.608964 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 16:09:27.608986 29777 sgd_solver.cpp:106] Iteration 264200, lr = 0.00174375
I0630 16:09:43.702558 29777 solver.cpp:290] Iteration 264300 (6.21382 iter/s, 16.0932s/100 iter), loss = 1.25
I0630 16:09:43.702611 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 16:09:43.702626 29777 sgd_solver.cpp:106] Iteration 264300, lr = 0.00174062
I0630 16:09:59.965559 29777 solver.cpp:290] Iteration 264400 (6.14911 iter/s, 16.2625s/100 iter), loss = 1.14286
I0630 16:09:59.965610 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 16:09:59.965617 29777 sgd_solver.cpp:106] Iteration 264400, lr = 0.0017375
I0630 16:10:16.626904 29777 solver.cpp:290] Iteration 264500 (6.0021 iter/s, 16.6608s/100 iter), loss = 1.4881
I0630 16:10:16.627110 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 16:10:16.627214 29777 sgd_solver.cpp:106] Iteration 264500, lr = 0.00173437
I0630 16:10:32.898144 29777 solver.cpp:290] Iteration 264600 (6.14604 iter/s, 16.2706s/100 iter), loss = 1.4881
I0630 16:10:32.898231 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 16:10:32.898241 29777 sgd_solver.cpp:106] Iteration 264600, lr = 0.00173125
I0630 16:10:49.159587 29777 solver.cpp:290] Iteration 264700 (6.14971 iter/s, 16.2609s/100 iter), loss = 0.988095
I0630 16:10:49.159613 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 16:10:49.159623 29777 sgd_solver.cpp:106] Iteration 264700, lr = 0.00172813
I0630 16:11:05.377550 29777 solver.cpp:290] Iteration 264800 (6.16618 iter/s, 16.2175s/100 iter), loss = 1.02381
I0630 16:11:05.377677 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 16:11:05.377698 29777 sgd_solver.cpp:106] Iteration 264800, lr = 0.001725
I0630 16:11:21.486783 29777 solver.cpp:290] Iteration 264900 (6.20783 iter/s, 16.1087s/100 iter), loss = 1.54762
I0630 16:11:21.486809 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 16:11:21.486817 29777 sgd_solver.cpp:106] Iteration 264900, lr = 0.00172188
I0630 16:11:37.408314 29777 solver.cpp:354] Sparsity after update:
I0630 16:11:37.428702 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:11:37.428715 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:11:37.428726 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:11:37.428730 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:11:37.428733 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:11:37.428736 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:11:37.428740 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:11:37.428743 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:11:37.428746 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:11:37.428750 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:11:37.428752 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:11:37.428763 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:11:37.428769 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:11:37.589720 29777 solver.cpp:290] Iteration 265000 (6.21023 iter/s, 16.1025s/100 iter), loss = 1.17857
I0630 16:11:37.589751 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 16:11:37.589759 29777 sgd_solver.cpp:106] Iteration 265000, lr = 0.00171875
I0630 16:11:53.717018 29777 solver.cpp:290] Iteration 265100 (6.20085 iter/s, 16.1268s/100 iter), loss = 1.2381
I0630 16:11:53.717044 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 16:11:53.717053 29777 sgd_solver.cpp:106] Iteration 265100, lr = 0.00171562
I0630 16:12:09.793929 29777 solver.cpp:290] Iteration 265200 (6.22028 iter/s, 16.0765s/100 iter), loss = 1.21429
I0630 16:12:09.794036 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 16:12:09.794049 29777 sgd_solver.cpp:106] Iteration 265200, lr = 0.0017125
I0630 16:12:25.962712 29777 solver.cpp:290] Iteration 265300 (6.18496 iter/s, 16.1682s/100 iter), loss = 1.36905
I0630 16:12:25.962738 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 16:12:25.962743 29777 sgd_solver.cpp:106] Iteration 265300, lr = 0.00170937
I0630 16:12:42.117558 29777 solver.cpp:290] Iteration 265400 (6.19027 iter/s, 16.1544s/100 iter), loss = 1.19048
I0630 16:12:42.117640 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 16:12:42.117648 29777 sgd_solver.cpp:106] Iteration 265400, lr = 0.00170625
I0630 16:12:58.209756 29777 solver.cpp:290] Iteration 265500 (6.21439 iter/s, 16.0917s/100 iter), loss = 1.39286
I0630 16:12:58.209789 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 16:12:58.209875 29777 sgd_solver.cpp:106] Iteration 265500, lr = 0.00170313
I0630 16:13:14.430405 29777 solver.cpp:290] Iteration 265600 (6.16516 iter/s, 16.2202s/100 iter), loss = 1.65476
I0630 16:13:14.430480 29777 solver.cpp:309]     Train net output #0: loss = 1.71429 (* 1 = 1.71429 loss)
I0630 16:13:14.430491 29777 sgd_solver.cpp:106] Iteration 265600, lr = 0.0017
I0630 16:13:30.436769 29777 solver.cpp:290] Iteration 265700 (6.24771 iter/s, 16.0059s/100 iter), loss = 1.41667
I0630 16:13:30.436795 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 16:13:30.436805 29777 sgd_solver.cpp:106] Iteration 265700, lr = 0.00169688
I0630 16:13:46.444794 29777 solver.cpp:290] Iteration 265800 (6.24705 iter/s, 16.0076s/100 iter), loss = 1.22619
I0630 16:13:46.444878 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 16:13:46.444900 29777 sgd_solver.cpp:106] Iteration 265800, lr = 0.00169375
I0630 16:14:02.595690 29777 solver.cpp:290] Iteration 265900 (6.1918 iter/s, 16.1504s/100 iter), loss = 1.05952
I0630 16:14:02.595715 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 16:14:02.595723 29777 sgd_solver.cpp:106] Iteration 265900, lr = 0.00169062
I0630 16:14:18.593852 29777 solver.cpp:354] Sparsity after update:
I0630 16:14:18.595301 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:14:18.595310 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:14:18.595317 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:14:18.595320 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:14:18.595322 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:14:18.595324 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:14:18.595326 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:14:18.595329 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:14:18.595329 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:14:18.595331 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:14:18.595333 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:14:18.595335 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:14:18.595337 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:14:18.595433 29777 solver.cpp:471] Iteration 266000, Testing net (#0)
I0630 16:14:45.176678 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 16:15:19.173346 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55696
I0630 16:15:19.173491 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.793241
I0630 16:15:19.173509 29777 solver.cpp:544]     Test net output #2: loss = 1.5675 (* 1 = 1.5675 loss)
I0630 16:15:19.350677 29777 solver.cpp:290] Iteration 266000 (1.30288 iter/s, 76.7529s/100 iter), loss = 1.33333
I0630 16:15:19.350754 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 16:15:19.350787 29777 sgd_solver.cpp:106] Iteration 266000, lr = 0.0016875
I0630 16:15:35.528805 29777 solver.cpp:290] Iteration 266100 (6.18138 iter/s, 16.1776s/100 iter), loss = 1.15476
I0630 16:15:35.528831 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 16:15:35.528839 29777 sgd_solver.cpp:106] Iteration 266100, lr = 0.00168437
I0630 16:15:51.660513 29777 solver.cpp:290] Iteration 266200 (6.19915 iter/s, 16.1312s/100 iter), loss = 1.39286
I0630 16:15:51.660598 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 16:15:51.660604 29777 sgd_solver.cpp:106] Iteration 266200, lr = 0.00168125
I0630 16:16:07.863515 29777 solver.cpp:290] Iteration 266300 (6.17189 iter/s, 16.2025s/100 iter), loss = 1.21429
I0630 16:16:07.863539 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 16:16:07.863546 29777 sgd_solver.cpp:106] Iteration 266300, lr = 0.00167813
I0630 16:16:24.250897 29777 solver.cpp:290] Iteration 266400 (6.10243 iter/s, 16.3869s/100 iter), loss = 0.97619
I0630 16:16:24.250959 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 16:16:24.250970 29777 sgd_solver.cpp:106] Iteration 266400, lr = 0.001675
I0630 16:16:40.375716 29777 solver.cpp:290] Iteration 266500 (6.20181 iter/s, 16.1243s/100 iter), loss = 1.42857
I0630 16:16:40.375741 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 16:16:40.375746 29777 sgd_solver.cpp:106] Iteration 266500, lr = 0.00167188
I0630 16:16:56.514878 29777 solver.cpp:290] Iteration 266600 (6.19629 iter/s, 16.1387s/100 iter), loss = 1.70238
I0630 16:16:56.514986 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 16:16:56.514997 29777 sgd_solver.cpp:106] Iteration 266600, lr = 0.00166875
I0630 16:17:12.602115 29777 solver.cpp:290] Iteration 266700 (6.21632 iter/s, 16.0867s/100 iter), loss = 1.55952
I0630 16:17:12.602141 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 16:17:12.602150 29777 sgd_solver.cpp:106] Iteration 266700, lr = 0.00166562
I0630 16:17:28.731165 29777 solver.cpp:290] Iteration 266800 (6.20017 iter/s, 16.1286s/100 iter), loss = 1.40476
I0630 16:17:28.731261 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 16:17:28.731271 29777 sgd_solver.cpp:106] Iteration 266800, lr = 0.0016625
I0630 16:17:44.930889 29777 solver.cpp:290] Iteration 266900 (6.17315 iter/s, 16.1992s/100 iter), loss = 1.27381
I0630 16:17:44.930912 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 16:17:44.930918 29777 sgd_solver.cpp:106] Iteration 266900, lr = 0.00165937
I0630 16:18:00.783980 29777 solver.cpp:354] Sparsity after update:
I0630 16:18:00.804349 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:18:00.804363 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:18:00.804373 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:18:00.804376 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:18:00.804379 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:18:00.804383 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:18:00.804386 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:18:00.804389 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:18:00.804392 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:18:00.804396 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:18:00.804399 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:18:00.804402 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:18:00.804409 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:18:00.958015 29777 solver.cpp:290] Iteration 267000 (6.2396 iter/s, 16.0267s/100 iter), loss = 0.928571
I0630 16:18:00.958041 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 16:18:00.958050 29777 sgd_solver.cpp:106] Iteration 267000, lr = 0.00165625
I0630 16:18:17.158956 29777 solver.cpp:290] Iteration 267100 (6.17266 iter/s, 16.2005s/100 iter), loss = 1.07143
I0630 16:18:17.159026 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 16:18:17.167744 29777 sgd_solver.cpp:106] Iteration 267100, lr = 0.00165313
I0630 16:18:33.643970 29777 solver.cpp:290] Iteration 267200 (6.0663 iter/s, 16.4845s/100 iter), loss = 1.55952
I0630 16:18:33.644086 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 16:18:33.644100 29777 sgd_solver.cpp:106] Iteration 267200, lr = 0.00165
I0630 16:18:50.011517 29777 solver.cpp:290] Iteration 267300 (6.10986 iter/s, 16.367s/100 iter), loss = 1.29762
I0630 16:18:50.011540 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 16:18:50.011549 29777 sgd_solver.cpp:106] Iteration 267300, lr = 0.00164688
I0630 16:19:06.295717 29777 solver.cpp:290] Iteration 267400 (6.1411 iter/s, 16.2837s/100 iter), loss = 1.25
I0630 16:19:06.295799 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 16:19:06.295810 29777 sgd_solver.cpp:106] Iteration 267400, lr = 0.00164375
I0630 16:19:22.427153 29777 solver.cpp:290] Iteration 267500 (6.19928 iter/s, 16.1309s/100 iter), loss = 1.34524
I0630 16:19:22.427178 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 16:19:22.427184 29777 sgd_solver.cpp:106] Iteration 267500, lr = 0.00164062
I0630 16:19:38.789873 29777 solver.cpp:290] Iteration 267600 (6.11163 iter/s, 16.3622s/100 iter), loss = 1.27381
I0630 16:19:38.789961 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 16:19:38.789973 29777 sgd_solver.cpp:106] Iteration 267600, lr = 0.0016375
I0630 16:19:54.988876 29777 solver.cpp:290] Iteration 267700 (6.17342 iter/s, 16.1985s/100 iter), loss = 1.15476
I0630 16:19:54.988905 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 16:19:54.988914 29777 sgd_solver.cpp:106] Iteration 267700, lr = 0.00163437
I0630 16:20:11.372541 29777 solver.cpp:290] Iteration 267800 (6.10382 iter/s, 16.3832s/100 iter), loss = 1.35714
I0630 16:20:11.372653 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 16:20:11.372681 29777 sgd_solver.cpp:106] Iteration 267800, lr = 0.00163125
I0630 16:20:27.632442 29777 solver.cpp:290] Iteration 267900 (6.1503 iter/s, 16.2594s/100 iter), loss = 1.04762
I0630 16:20:27.632465 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 16:20:27.632472 29777 sgd_solver.cpp:106] Iteration 267900, lr = 0.00162812
I0630 16:20:43.923125 29777 solver.cpp:354] Sparsity after update:
I0630 16:20:43.924558 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:20:43.924564 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:20:43.924571 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:20:43.924573 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:20:43.924576 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:20:43.924578 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:20:43.924579 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:20:43.924582 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:20:43.924583 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:20:43.924585 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:20:43.924587 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:20:43.924589 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:20:43.924592 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:20:43.924675 29777 solver.cpp:471] Iteration 268000, Testing net (#0)
I0630 16:21:17.011164 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 16:21:53.567647 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5587
I0630 16:21:53.567769 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.792721
I0630 16:21:53.567786 29777 solver.cpp:544]     Test net output #2: loss = 1.56964 (* 1 = 1.56964 loss)
I0630 16:21:53.743578 29777 solver.cpp:290] Iteration 268000 (1.16132 iter/s, 86.1088s/100 iter), loss = 1.19048
I0630 16:21:53.743602 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 16:21:53.743609 29777 sgd_solver.cpp:106] Iteration 268000, lr = 0.001625
I0630 16:22:10.387081 29777 solver.cpp:290] Iteration 268100 (6.00853 iter/s, 16.643s/100 iter), loss = 1.38095
I0630 16:22:10.387316 29777 solver.cpp:309]     Train net output #0: loss = 1.61905 (* 1 = 1.61905 loss)
I0630 16:22:10.387347 29777 sgd_solver.cpp:106] Iteration 268100, lr = 0.00162188
I0630 16:22:26.652627 29777 solver.cpp:290] Iteration 268200 (6.14822 iter/s, 16.2649s/100 iter), loss = 1.20238
I0630 16:22:26.652737 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 16:22:26.652756 29777 sgd_solver.cpp:106] Iteration 268200, lr = 0.00161875
I0630 16:22:42.868201 29777 solver.cpp:290] Iteration 268300 (6.16712 iter/s, 16.215s/100 iter), loss = 1.44048
I0630 16:22:42.868222 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 16:22:42.868229 29777 sgd_solver.cpp:106] Iteration 268300, lr = 0.00161562
I0630 16:22:58.956550 29777 solver.cpp:290] Iteration 268400 (6.21586 iter/s, 16.0879s/100 iter), loss = 1.22619
I0630 16:22:58.956643 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 16:22:58.956655 29777 sgd_solver.cpp:106] Iteration 268400, lr = 0.0016125
I0630 16:23:15.399881 29777 solver.cpp:290] Iteration 268500 (6.08169 iter/s, 16.4428s/100 iter), loss = 1.35714
I0630 16:23:15.399921 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 16:23:15.399931 29777 sgd_solver.cpp:106] Iteration 268500, lr = 0.00160937
I0630 16:23:31.669833 29777 solver.cpp:290] Iteration 268600 (6.14648 iter/s, 16.2695s/100 iter), loss = 1.10714
I0630 16:23:31.669891 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 16:23:31.669899 29777 sgd_solver.cpp:106] Iteration 268600, lr = 0.00160625
I0630 16:23:48.026887 29777 solver.cpp:290] Iteration 268700 (6.11376 iter/s, 16.3566s/100 iter), loss = 1.25
I0630 16:23:48.026912 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 16:23:48.026918 29777 sgd_solver.cpp:106] Iteration 268700, lr = 0.00160312
I0630 16:24:04.221962 29777 solver.cpp:290] Iteration 268800 (6.17489 iter/s, 16.1946s/100 iter), loss = 1.30952
I0630 16:24:04.222064 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 16:24:04.222072 29777 sgd_solver.cpp:106] Iteration 268800, lr = 0.0016
I0630 16:24:20.561739 29777 solver.cpp:290] Iteration 268900 (6.12024 iter/s, 16.3392s/100 iter), loss = 1.39286
I0630 16:24:20.561784 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 16:24:20.561796 29777 sgd_solver.cpp:106] Iteration 268900, lr = 0.00159688
I0630 16:24:36.563786 29777 solver.cpp:354] Sparsity after update:
I0630 16:24:36.585129 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:24:36.585353 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:24:36.585458 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:24:36.585548 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:24:36.585638 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:24:36.585732 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:24:36.585824 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:24:36.585916 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:24:36.586007 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:24:36.586107 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:24:36.586207 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:24:36.586300 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:24:36.586391 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:24:36.750720 29777 solver.cpp:290] Iteration 269000 (6.17723 iter/s, 16.1885s/100 iter), loss = 1.70238
I0630 16:24:36.751024 29777 solver.cpp:309]     Train net output #0: loss = 2.09524 (* 1 = 2.09524 loss)
I0630 16:24:36.751152 29777 sgd_solver.cpp:106] Iteration 269000, lr = 0.00159375
I0630 16:24:52.798568 29777 solver.cpp:290] Iteration 269100 (6.23166 iter/s, 16.0471s/100 iter), loss = 1.17857
I0630 16:24:52.798629 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 16:24:52.798663 29777 sgd_solver.cpp:106] Iteration 269100, lr = 0.00159063
I0630 16:25:09.187773 29777 solver.cpp:290] Iteration 269200 (6.10176 iter/s, 16.3887s/100 iter), loss = 1.09524
I0630 16:25:09.187883 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 16:25:09.187894 29777 sgd_solver.cpp:106] Iteration 269200, lr = 0.0015875
I0630 16:25:25.483898 29777 solver.cpp:290] Iteration 269300 (6.13663 iter/s, 16.2956s/100 iter), loss = 1.19048
I0630 16:25:25.483924 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 16:25:25.483932 29777 sgd_solver.cpp:106] Iteration 269300, lr = 0.00158437
I0630 16:25:41.590593 29777 solver.cpp:290] Iteration 269400 (6.20878 iter/s, 16.1062s/100 iter), loss = 1.16667
I0630 16:25:41.590678 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 16:25:41.590688 29777 sgd_solver.cpp:106] Iteration 269400, lr = 0.00158125
I0630 16:25:58.058115 29777 solver.cpp:290] Iteration 269500 (6.07276 iter/s, 16.467s/100 iter), loss = 1.03571
I0630 16:25:58.058167 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 16:25:58.058199 29777 sgd_solver.cpp:106] Iteration 269500, lr = 0.00157812
I0630 16:26:14.357267 29777 solver.cpp:290] Iteration 269600 (6.13547 iter/s, 16.2987s/100 iter), loss = 1.21429
I0630 16:26:14.357341 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 16:26:14.357347 29777 sgd_solver.cpp:106] Iteration 269600, lr = 0.001575
I0630 16:26:30.400074 29777 solver.cpp:290] Iteration 269700 (6.23352 iter/s, 16.0423s/100 iter), loss = 1.25
I0630 16:26:30.400096 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 16:26:30.400104 29777 sgd_solver.cpp:106] Iteration 269700, lr = 0.00157188
I0630 16:26:46.688630 29777 solver.cpp:290] Iteration 269800 (6.13946 iter/s, 16.2881s/100 iter), loss = 1.72619
I0630 16:26:46.688704 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 16:26:46.688740 29777 sgd_solver.cpp:106] Iteration 269800, lr = 0.00156875
I0630 16:27:02.964231 29777 solver.cpp:290] Iteration 269900 (6.14436 iter/s, 16.2751s/100 iter), loss = 1.15476
I0630 16:27:02.964257 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 16:27:02.964267 29777 sgd_solver.cpp:106] Iteration 269900, lr = 0.00156563
I0630 16:27:19.030539 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_270000.caffemodel
I0630 16:27:19.050632 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_270000.solverstate
I0630 16:27:19.059579 29777 solver.cpp:354] Sparsity after update:
I0630 16:27:19.060547 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:27:19.060555 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:27:19.060564 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:27:19.060565 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:27:19.060567 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:27:19.060570 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:27:19.060571 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:27:19.060573 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:27:19.060575 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:27:19.060577 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:27:19.060580 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:27:19.060581 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:27:19.060583 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:27:19.060680 29777 solver.cpp:471] Iteration 270000, Testing net (#0)
I0630 16:27:45.558688 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 16:28:22.592118 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55802
I0630 16:28:22.592247 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.793841
I0630 16:28:22.592257 29777 solver.cpp:544]     Test net output #2: loss = 1.56084 (* 1 = 1.56084 loss)
I0630 16:28:22.775820 29777 solver.cpp:290] Iteration 270000 (1.25298 iter/s, 79.8094s/100 iter), loss = 1.36905
I0630 16:28:22.775841 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 16:28:22.775851 29777 sgd_solver.cpp:106] Iteration 270000, lr = 0.0015625
I0630 16:28:38.812268 29777 solver.cpp:290] Iteration 270100 (6.23608 iter/s, 16.0357s/100 iter), loss = 1.59524
I0630 16:28:38.812307 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 16:28:38.812319 29777 sgd_solver.cpp:106] Iteration 270100, lr = 0.00155937
I0630 16:28:55.139308 29777 solver.cpp:290] Iteration 270200 (6.12499 iter/s, 16.3266s/100 iter), loss = 1.30952
I0630 16:28:55.139430 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 16:28:55.139452 29777 sgd_solver.cpp:106] Iteration 270200, lr = 0.00155625
I0630 16:29:11.291501 29777 solver.cpp:290] Iteration 270300 (6.19132 iter/s, 16.1516s/100 iter), loss = 1.54762
I0630 16:29:11.291528 29777 solver.cpp:309]     Train net output #0: loss = 1.71429 (* 1 = 1.71429 loss)
I0630 16:29:11.291538 29777 sgd_solver.cpp:106] Iteration 270300, lr = 0.00155312
I0630 16:29:27.826089 29777 solver.cpp:290] Iteration 270400 (6.04811 iter/s, 16.5341s/100 iter), loss = 1.21429
I0630 16:29:27.826231 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 16:29:27.826268 29777 sgd_solver.cpp:106] Iteration 270400, lr = 0.00155
I0630 16:29:44.097486 29777 solver.cpp:290] Iteration 270500 (6.14597 iter/s, 16.2708s/100 iter), loss = 1.22619
I0630 16:29:44.097510 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 16:29:44.097519 29777 sgd_solver.cpp:106] Iteration 270500, lr = 0.00154688
I0630 16:30:00.305975 29777 solver.cpp:290] Iteration 270600 (6.16978 iter/s, 16.208s/100 iter), loss = 1.04762
I0630 16:30:00.306069 29777 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 16:30:00.306080 29777 sgd_solver.cpp:106] Iteration 270600, lr = 0.00154375
I0630 16:30:16.442446 29777 solver.cpp:290] Iteration 270700 (6.19734 iter/s, 16.136s/100 iter), loss = 1.19048
I0630 16:30:16.442469 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 16:30:16.442476 29777 sgd_solver.cpp:106] Iteration 270700, lr = 0.00154063
I0630 16:30:32.635998 29777 solver.cpp:290] Iteration 270800 (6.17547 iter/s, 16.1931s/100 iter), loss = 1.09524
I0630 16:30:32.636057 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 16:30:32.636067 29777 sgd_solver.cpp:106] Iteration 270800, lr = 0.0015375
I0630 16:30:48.632436 29777 solver.cpp:290] Iteration 270900 (6.25158 iter/s, 15.996s/100 iter), loss = 1.04762
I0630 16:30:48.632459 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 16:30:48.632465 29777 sgd_solver.cpp:106] Iteration 270900, lr = 0.00153437
I0630 16:31:04.648823 29777 solver.cpp:354] Sparsity after update:
I0630 16:31:04.669540 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:31:04.669558 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:31:04.669569 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:31:04.669574 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:31:04.669577 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:31:04.669580 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:31:04.669584 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:31:04.669587 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:31:04.669591 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:31:04.669595 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:31:04.669598 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:31:04.669601 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:31:04.669611 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:31:04.831120 29777 solver.cpp:290] Iteration 271000 (6.17352 iter/s, 16.1982s/100 iter), loss = 1.55952
I0630 16:31:04.831157 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 16:31:04.831166 29777 sgd_solver.cpp:106] Iteration 271000, lr = 0.00153125
I0630 16:31:21.068657 29777 solver.cpp:290] Iteration 271100 (6.15875 iter/s, 16.2371s/100 iter), loss = 1.44048
I0630 16:31:21.068683 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 16:31:21.068692 29777 sgd_solver.cpp:106] Iteration 271100, lr = 0.00152812
I0630 16:31:37.208669 29777 solver.cpp:290] Iteration 271200 (6.19596 iter/s, 16.1396s/100 iter), loss = 1.40476
I0630 16:31:37.208796 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 16:31:37.208809 29777 sgd_solver.cpp:106] Iteration 271200, lr = 0.001525
I0630 16:31:53.344542 29777 solver.cpp:290] Iteration 271300 (6.19759 iter/s, 16.1353s/100 iter), loss = 1.17857
I0630 16:31:53.344599 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 16:31:53.344621 29777 sgd_solver.cpp:106] Iteration 271300, lr = 0.00152188
I0630 16:32:09.497283 29777 solver.cpp:290] Iteration 271400 (6.19108 iter/s, 16.1523s/100 iter), loss = 0.952381
I0630 16:32:09.497378 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 16:32:09.497388 29777 sgd_solver.cpp:106] Iteration 271400, lr = 0.00151875
I0630 16:32:25.679538 29777 solver.cpp:290] Iteration 271500 (6.17981 iter/s, 16.1817s/100 iter), loss = 1.46429
I0630 16:32:25.679563 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 16:32:25.679571 29777 sgd_solver.cpp:106] Iteration 271500, lr = 0.00151563
I0630 16:32:41.746062 29777 solver.cpp:290] Iteration 271600 (6.2243 iter/s, 16.0661s/100 iter), loss = 1.35714
I0630 16:32:41.746170 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 16:32:41.746178 29777 sgd_solver.cpp:106] Iteration 271600, lr = 0.0015125
I0630 16:32:57.825119 29777 solver.cpp:290] Iteration 271700 (6.21948 iter/s, 16.0785s/100 iter), loss = 1.13095
I0630 16:32:57.825141 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 16:32:57.825147 29777 sgd_solver.cpp:106] Iteration 271700, lr = 0.00150937
I0630 16:33:13.938997 29777 solver.cpp:290] Iteration 271800 (6.206 iter/s, 16.1134s/100 iter), loss = 0.904762
I0630 16:33:13.939051 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 16:33:13.939062 29777 sgd_solver.cpp:106] Iteration 271800, lr = 0.00150625
I0630 16:33:30.096323 29777 solver.cpp:290] Iteration 271900 (6.18933 iter/s, 16.1568s/100 iter), loss = 1.32143
I0630 16:33:30.096487 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 16:33:30.096572 29777 sgd_solver.cpp:106] Iteration 271900, lr = 0.00150312
I0630 16:33:46.142004 29777 solver.cpp:354] Sparsity after update:
I0630 16:33:46.143457 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:33:46.143466 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:33:46.143474 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:33:46.143477 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:33:46.143479 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:33:46.143481 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:33:46.143483 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:33:46.143486 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:33:46.143487 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:33:46.143489 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:33:46.143491 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:33:46.143493 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:33:46.143496 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:33:46.143584 29777 solver.cpp:471] Iteration 272000, Testing net (#0)
I0630 16:34:14.549052 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 16:34:44.583405 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55764
I0630 16:34:44.583561 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.793321
I0630 16:34:44.583581 29777 solver.cpp:544]     Test net output #2: loss = 1.56842 (* 1 = 1.56842 loss)
I0630 16:34:44.786919 29777 solver.cpp:290] Iteration 272000 (1.33889 iter/s, 74.6885s/100 iter), loss = 1.30952
I0630 16:34:44.786942 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 16:34:44.786948 29777 sgd_solver.cpp:106] Iteration 272000, lr = 0.0015
I0630 16:35:01.100699 29777 solver.cpp:290] Iteration 272100 (6.12997 iter/s, 16.3133s/100 iter), loss = 1.21429
I0630 16:35:01.100754 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 16:35:01.100785 29777 sgd_solver.cpp:106] Iteration 272100, lr = 0.00149688
I0630 16:35:17.633711 29777 solver.cpp:290] Iteration 272200 (6.04868 iter/s, 16.5325s/100 iter), loss = 1.52381
I0630 16:35:17.633805 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 16:35:17.633816 29777 sgd_solver.cpp:106] Iteration 272200, lr = 0.00149375
I0630 16:35:34.094935 29777 solver.cpp:290] Iteration 272300 (6.07508 iter/s, 16.4607s/100 iter), loss = 1.5
I0630 16:35:34.094969 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 16:35:34.094975 29777 sgd_solver.cpp:106] Iteration 272300, lr = 0.00149063
I0630 16:35:50.235216 29777 solver.cpp:290] Iteration 272400 (6.19586 iter/s, 16.1398s/100 iter), loss = 1.02381
I0630 16:35:50.235321 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 16:35:50.235339 29777 sgd_solver.cpp:106] Iteration 272400, lr = 0.0014875
I0630 16:36:06.444191 29777 solver.cpp:290] Iteration 272500 (6.16962 iter/s, 16.2084s/100 iter), loss = 1.09524
I0630 16:36:06.444213 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 16:36:06.444221 29777 sgd_solver.cpp:106] Iteration 272500, lr = 0.00148437
I0630 16:36:22.893465 29777 solver.cpp:290] Iteration 272600 (6.07947 iter/s, 16.4488s/100 iter), loss = 1.17857
I0630 16:36:22.893555 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 16:36:22.893566 29777 sgd_solver.cpp:106] Iteration 272600, lr = 0.00148125
I0630 16:36:39.197896 29777 solver.cpp:290] Iteration 272700 (6.1335 iter/s, 16.3039s/100 iter), loss = 1.40476
I0630 16:36:39.197931 29777 solver.cpp:309]     Train net output #0: loss = 1.78571 (* 1 = 1.78571 loss)
I0630 16:36:39.197940 29777 sgd_solver.cpp:106] Iteration 272700, lr = 0.00147812
I0630 16:36:55.223722 29777 solver.cpp:290] Iteration 272800 (6.24011 iter/s, 16.0254s/100 iter), loss = 1.4881
I0630 16:36:55.223817 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 16:36:55.223829 29777 sgd_solver.cpp:106] Iteration 272800, lr = 0.001475
I0630 16:37:11.612382 29777 solver.cpp:290] Iteration 272900 (6.10198 iter/s, 16.3881s/100 iter), loss = 1.35714
I0630 16:37:11.612426 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 16:37:11.612447 29777 sgd_solver.cpp:106] Iteration 272900, lr = 0.00147187
I0630 16:37:27.770084 29777 solver.cpp:354] Sparsity after update:
I0630 16:37:27.790515 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:37:27.790531 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:37:27.790542 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:37:27.790546 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:37:27.790557 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:37:27.790563 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:37:27.790568 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:37:27.790573 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:37:27.790578 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:37:27.790582 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:37:27.790587 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:37:27.790591 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:37:27.790594 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:37:27.950963 29777 solver.cpp:290] Iteration 273000 (6.12066 iter/s, 16.3381s/100 iter), loss = 1.20238
I0630 16:37:27.950986 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 16:37:27.950992 29777 sgd_solver.cpp:106] Iteration 273000, lr = 0.00146875
I0630 16:37:44.243206 29777 solver.cpp:290] Iteration 273100 (6.13807 iter/s, 16.2918s/100 iter), loss = 1
I0630 16:37:44.243330 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 16:37:44.243373 29777 sgd_solver.cpp:106] Iteration 273100, lr = 0.00146563
I0630 16:38:00.599009 29777 solver.cpp:290] Iteration 273200 (6.11425 iter/s, 16.3552s/100 iter), loss = 1.33333
I0630 16:38:00.599153 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 16:38:00.599175 29777 sgd_solver.cpp:106] Iteration 273200, lr = 0.0014625
I0630 16:38:16.906347 29777 solver.cpp:290] Iteration 273300 (6.13242 iter/s, 16.3068s/100 iter), loss = 1.11905
I0630 16:38:16.906370 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 16:38:16.906378 29777 sgd_solver.cpp:106] Iteration 273300, lr = 0.00145938
I0630 16:38:33.308181 29777 solver.cpp:290] Iteration 273400 (6.09705 iter/s, 16.4014s/100 iter), loss = 1.52381
I0630 16:38:33.308269 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 16:38:33.308290 29777 sgd_solver.cpp:106] Iteration 273400, lr = 0.00145625
I0630 16:38:49.555624 29777 solver.cpp:290] Iteration 273500 (6.15501 iter/s, 16.2469s/100 iter), loss = 1.7619
I0630 16:38:49.555660 29777 solver.cpp:309]     Train net output #0: loss = 1.90476 (* 1 = 1.90476 loss)
I0630 16:38:49.555670 29777 sgd_solver.cpp:106] Iteration 273500, lr = 0.00145312
I0630 16:39:05.819142 29777 solver.cpp:290] Iteration 273600 (6.14891 iter/s, 16.263s/100 iter), loss = 1.30952
I0630 16:39:05.819236 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 16:39:05.819247 29777 sgd_solver.cpp:106] Iteration 273600, lr = 0.00145
I0630 16:39:22.027103 29777 solver.cpp:290] Iteration 273700 (6.17001 iter/s, 16.2074s/100 iter), loss = 1.40476
I0630 16:39:22.027124 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 16:39:22.027132 29777 sgd_solver.cpp:106] Iteration 273700, lr = 0.00144687
I0630 16:39:38.512483 29777 solver.cpp:290] Iteration 273800 (6.06615 iter/s, 16.4849s/100 iter), loss = 1.14286
I0630 16:39:38.512588 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 16:39:38.512596 29777 sgd_solver.cpp:106] Iteration 273800, lr = 0.00144375
I0630 16:39:54.610622 29777 solver.cpp:290] Iteration 273900 (6.2121 iter/s, 16.0976s/100 iter), loss = 1.40476
I0630 16:39:54.610649 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 16:39:54.610664 29777 sgd_solver.cpp:106] Iteration 273900, lr = 0.00144063
I0630 16:40:10.796180 29777 solver.cpp:354] Sparsity after update:
I0630 16:40:10.797436 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:40:10.797444 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:40:10.797451 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:40:10.797453 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:40:10.797456 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:40:10.797457 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:40:10.797459 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:40:10.797461 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:40:10.797463 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:40:10.797464 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:40:10.797466 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:40:10.797468 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:40:10.797471 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:40:10.797557 29777 solver.cpp:471] Iteration 274000, Testing net (#0)
I0630 16:40:37.945215 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 16:41:09.476315 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.55884
I0630 16:41:09.476439 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.793601
I0630 16:41:09.476449 29777 solver.cpp:544]     Test net output #2: loss = 1.56662 (* 1 = 1.56662 loss)
I0630 16:41:09.656054 29777 solver.cpp:290] Iteration 274000 (1.33256 iter/s, 75.0434s/100 iter), loss = 1.10714
I0630 16:41:09.656076 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 16:41:09.656083 29777 sgd_solver.cpp:106] Iteration 274000, lr = 0.0014375
I0630 16:41:25.730001 29777 solver.cpp:290] Iteration 274100 (6.22142 iter/s, 16.0735s/100 iter), loss = 1.35714
I0630 16:41:25.730026 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 16:41:25.730034 29777 sgd_solver.cpp:106] Iteration 274100, lr = 0.00143438
I0630 16:41:41.766072 29777 solver.cpp:290] Iteration 274200 (6.23612 iter/s, 16.0356s/100 iter), loss = 1.09524
I0630 16:41:41.766181 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 16:41:41.766191 29777 sgd_solver.cpp:106] Iteration 274200, lr = 0.00143125
I0630 16:41:57.856390 29777 solver.cpp:290] Iteration 274300 (6.21513 iter/s, 16.0898s/100 iter), loss = 1.25
I0630 16:41:57.856417 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 16:41:57.856426 29777 sgd_solver.cpp:106] Iteration 274300, lr = 0.00142812
I0630 16:42:13.788930 29777 solver.cpp:290] Iteration 274400 (6.27664 iter/s, 15.9321s/100 iter), loss = 1.07143
I0630 16:42:13.789014 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 16:42:13.789026 29777 sgd_solver.cpp:106] Iteration 274400, lr = 0.001425
I0630 16:42:29.748309 29777 solver.cpp:290] Iteration 274500 (6.26611 iter/s, 15.9589s/100 iter), loss = 1.10714
I0630 16:42:29.748335 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 16:42:29.748347 29777 sgd_solver.cpp:106] Iteration 274500, lr = 0.00142187
I0630 16:42:45.714867 29777 solver.cpp:290] Iteration 274600 (6.26327 iter/s, 15.9661s/100 iter), loss = 1.46429
I0630 16:42:45.714915 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 16:42:45.714922 29777 sgd_solver.cpp:106] Iteration 274600, lr = 0.00141875
I0630 16:43:01.652400 29777 solver.cpp:290] Iteration 274700 (6.27468 iter/s, 15.9371s/100 iter), loss = 1.29762
I0630 16:43:01.652423 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 16:43:01.652431 29777 sgd_solver.cpp:106] Iteration 274700, lr = 0.00141563
I0630 16:43:17.591188 29777 solver.cpp:290] Iteration 274800 (6.27418 iter/s, 15.9383s/100 iter), loss = 0.869048
I0630 16:43:17.591272 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 16:43:17.591284 29777 sgd_solver.cpp:106] Iteration 274800, lr = 0.0014125
I0630 16:43:33.787267 29777 solver.cpp:290] Iteration 274900 (6.17453 iter/s, 16.1956s/100 iter), loss = 1.2381
I0630 16:43:33.787386 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 16:43:33.787431 29777 sgd_solver.cpp:106] Iteration 274900, lr = 0.00140937
I0630 16:43:49.563340 29777 solver.cpp:354] Sparsity after update:
I0630 16:43:49.583725 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:43:49.583746 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:43:49.583757 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:43:49.583760 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:43:49.583765 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:43:49.583770 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:43:49.583775 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:43:49.583777 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:43:49.583781 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:43:49.583783 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:43:49.583787 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:43:49.583791 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:43:49.583793 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:43:49.748886 29777 solver.cpp:290] Iteration 275000 (6.26524 iter/s, 15.9611s/100 iter), loss = 1.41667
I0630 16:43:49.748909 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 16:43:49.748915 29777 sgd_solver.cpp:106] Iteration 275000, lr = 0.00140625
I0630 16:44:05.738831 29777 solver.cpp:290] Iteration 275100 (6.25411 iter/s, 15.9895s/100 iter), loss = 1.32143
I0630 16:44:05.738857 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 16:44:05.738863 29777 sgd_solver.cpp:106] Iteration 275100, lr = 0.00140312
I0630 16:44:21.861691 29777 solver.cpp:290] Iteration 275200 (6.20255 iter/s, 16.1224s/100 iter), loss = 1.07143
I0630 16:44:21.861788 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 16:44:21.861799 29777 sgd_solver.cpp:106] Iteration 275200, lr = 0.0014
I0630 16:44:37.853838 29777 solver.cpp:290] Iteration 275300 (6.25327 iter/s, 15.9916s/100 iter), loss = 1.69048
I0630 16:44:37.853864 29777 solver.cpp:309]     Train net output #0: loss = 1.88095 (* 1 = 1.88095 loss)
I0630 16:44:37.853873 29777 sgd_solver.cpp:106] Iteration 275300, lr = 0.00139687
I0630 16:44:53.848438 29777 solver.cpp:290] Iteration 275400 (6.25229 iter/s, 15.9941s/100 iter), loss = 1.61905
I0630 16:44:53.848526 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 16:44:53.848536 29777 sgd_solver.cpp:106] Iteration 275400, lr = 0.00139375
I0630 16:45:09.862996 29777 solver.cpp:290] Iteration 275500 (6.24452 iter/s, 16.014s/100 iter), loss = 1.44048
I0630 16:45:09.863021 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 16:45:09.863030 29777 sgd_solver.cpp:106] Iteration 275500, lr = 0.00139063
I0630 16:45:25.870237 29777 solver.cpp:290] Iteration 275600 (6.24735 iter/s, 16.0068s/100 iter), loss = 1.39286
I0630 16:45:25.870329 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 16:45:25.870337 29777 sgd_solver.cpp:106] Iteration 275600, lr = 0.0013875
I0630 16:45:41.910820 29777 solver.cpp:290] Iteration 275700 (6.23439 iter/s, 16.0401s/100 iter), loss = 1.04762
I0630 16:45:41.910886 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 16:45:41.910902 29777 sgd_solver.cpp:106] Iteration 275700, lr = 0.00138438
I0630 16:45:57.935986 29777 solver.cpp:290] Iteration 275800 (6.24038 iter/s, 16.0247s/100 iter), loss = 1.34524
I0630 16:45:57.936074 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 16:45:57.936085 29777 sgd_solver.cpp:106] Iteration 275800, lr = 0.00138125
I0630 16:46:13.963121 29777 solver.cpp:290] Iteration 275900 (6.23962 iter/s, 16.0266s/100 iter), loss = 1.35714
I0630 16:46:13.963148 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 16:46:13.963163 29777 sgd_solver.cpp:106] Iteration 275900, lr = 0.00137812
I0630 16:46:29.852841 29777 solver.cpp:354] Sparsity after update:
I0630 16:46:29.854109 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:46:29.854117 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:46:29.854125 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:46:29.854127 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:46:29.854130 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:46:29.854131 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:46:29.854133 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:46:29.854135 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:46:29.854137 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:46:29.854140 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:46:29.854141 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:46:29.854143 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:46:29.854146 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:46:29.854234 29777 solver.cpp:471] Iteration 276000, Testing net (#0)
I0630 16:46:56.736564 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 16:47:29.768332 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.559339
I0630 16:47:29.768384 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.793861
I0630 16:47:29.768391 29777 solver.cpp:544]     Test net output #2: loss = 1.56672 (* 1 = 1.56672 loss)
I0630 16:47:29.946707 29777 solver.cpp:290] Iteration 276000 (1.31611 iter/s, 75.9815s/100 iter), loss = 1.2381
I0630 16:47:29.946732 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 16:47:29.946738 29777 sgd_solver.cpp:106] Iteration 276000, lr = 0.001375
I0630 16:47:45.901664 29777 solver.cpp:290] Iteration 276100 (6.26782 iter/s, 15.9545s/100 iter), loss = 1.27381
I0630 16:47:45.901708 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 16:47:45.901721 29777 sgd_solver.cpp:106] Iteration 276100, lr = 0.00137187
I0630 16:48:01.912650 29777 solver.cpp:290] Iteration 276200 (6.2459 iter/s, 16.0105s/100 iter), loss = 1.17857
I0630 16:48:01.912719 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 16:48:01.912729 29777 sgd_solver.cpp:106] Iteration 276200, lr = 0.00136875
I0630 16:48:17.959359 29777 solver.cpp:290] Iteration 276300 (6.232 iter/s, 16.0462s/100 iter), loss = 1.38095
I0630 16:48:17.959383 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 16:48:17.959393 29777 sgd_solver.cpp:106] Iteration 276300, lr = 0.00136563
I0630 16:48:33.964589 29777 solver.cpp:290] Iteration 276400 (6.24814 iter/s, 16.0048s/100 iter), loss = 1.52381
I0630 16:48:33.964668 29777 solver.cpp:309]     Train net output #0: loss = 1.85714 (* 1 = 1.85714 loss)
I0630 16:48:33.964679 29777 sgd_solver.cpp:106] Iteration 276400, lr = 0.0013625
I0630 16:48:49.971174 29777 solver.cpp:290] Iteration 276500 (6.24763 iter/s, 16.0061s/100 iter), loss = 1.66667
I0630 16:48:49.971228 29777 solver.cpp:309]     Train net output #0: loss = 1.92857 (* 1 = 1.92857 loss)
I0630 16:48:49.971261 29777 sgd_solver.cpp:106] Iteration 276500, lr = 0.00135938
I0630 16:49:05.963835 29777 solver.cpp:290] Iteration 276600 (6.25305 iter/s, 15.9922s/100 iter), loss = 1.11905
I0630 16:49:05.963908 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 16:49:05.963918 29777 sgd_solver.cpp:106] Iteration 276600, lr = 0.00135625
I0630 16:49:22.272341 29777 solver.cpp:290] Iteration 276700 (6.13196 iter/s, 16.308s/100 iter), loss = 1.2381
I0630 16:49:22.272368 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 16:49:22.272377 29777 sgd_solver.cpp:106] Iteration 276700, lr = 0.00135312
I0630 16:49:38.358007 29777 solver.cpp:290] Iteration 276800 (6.21689 iter/s, 16.0852s/100 iter), loss = 1.33333
I0630 16:49:38.358105 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 16:49:38.358116 29777 sgd_solver.cpp:106] Iteration 276800, lr = 0.00135
I0630 16:49:54.233314 29777 solver.cpp:290] Iteration 276900 (6.2993 iter/s, 15.8748s/100 iter), loss = 1.07143
I0630 16:49:54.233338 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 16:49:54.233346 29777 sgd_solver.cpp:106] Iteration 276900, lr = 0.00134687
I0630 16:50:10.119701 29777 solver.cpp:354] Sparsity after update:
I0630 16:50:10.140064 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:50:10.140077 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:50:10.140085 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:50:10.140089 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:50:10.140090 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:50:10.140092 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:50:10.140094 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:50:10.140096 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:50:10.140101 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:50:10.140103 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:50:10.140105 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:50:10.140106 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:50:10.140108 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:50:10.298290 29777 solver.cpp:290] Iteration 277000 (6.2249 iter/s, 16.0645s/100 iter), loss = 1.47619
I0630 16:50:10.298312 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 16:50:10.298319 29777 sgd_solver.cpp:106] Iteration 277000, lr = 0.00134375
I0630 16:50:26.498237 29777 solver.cpp:290] Iteration 277100 (6.17303 iter/s, 16.1995s/100 iter), loss = 1.34524
I0630 16:50:26.498263 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 16:50:26.498278 29777 sgd_solver.cpp:106] Iteration 277100, lr = 0.00134063
I0630 16:50:42.606186 29777 solver.cpp:290] Iteration 277200 (6.20829 iter/s, 16.1075s/100 iter), loss = 1.54762
I0630 16:50:42.606258 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 16:50:42.606266 29777 sgd_solver.cpp:106] Iteration 277200, lr = 0.0013375
I0630 16:50:58.965338 29777 solver.cpp:290] Iteration 277300 (6.11298 iter/s, 16.3586s/100 iter), loss = 1.33333
I0630 16:50:58.965363 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 16:50:58.965368 29777 sgd_solver.cpp:106] Iteration 277300, lr = 0.00133438
I0630 16:51:15.205674 29777 solver.cpp:290] Iteration 277400 (6.15768 iter/s, 16.2399s/100 iter), loss = 1.15476
I0630 16:51:15.205750 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 16:51:15.205763 29777 sgd_solver.cpp:106] Iteration 277400, lr = 0.00133125
I0630 16:51:31.732604 29777 solver.cpp:290] Iteration 277500 (6.05092 iter/s, 16.5264s/100 iter), loss = 1.14286
I0630 16:51:31.732626 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 16:51:31.732635 29777 sgd_solver.cpp:106] Iteration 277500, lr = 0.00132813
I0630 16:51:48.134209 29777 solver.cpp:290] Iteration 277600 (6.09714 iter/s, 16.4011s/100 iter), loss = 0.904762
I0630 16:51:48.134904 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 16:51:48.134925 29777 sgd_solver.cpp:106] Iteration 277600, lr = 0.001325
I0630 16:52:04.313410 29777 solver.cpp:290] Iteration 277700 (6.18121 iter/s, 16.1781s/100 iter), loss = 1.29762
I0630 16:52:04.313536 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 16:52:04.313582 29777 sgd_solver.cpp:106] Iteration 277700, lr = 0.00132187
I0630 16:52:20.575610 29777 solver.cpp:290] Iteration 277800 (6.14944 iter/s, 16.2616s/100 iter), loss = 1.52381
I0630 16:52:20.575704 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 16:52:20.575716 29777 sgd_solver.cpp:106] Iteration 277800, lr = 0.00131875
I0630 16:52:36.672644 29777 solver.cpp:290] Iteration 277900 (6.21253 iter/s, 16.0965s/100 iter), loss = 1.22619
I0630 16:52:36.672667 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 16:52:36.672674 29777 sgd_solver.cpp:106] Iteration 277900, lr = 0.00131562
I0630 16:52:52.708391 29777 solver.cpp:354] Sparsity after update:
I0630 16:52:52.710436 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:52:52.710458 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:52:52.710477 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:52:52.710484 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:52:52.710490 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:52:52.710496 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:52:52.710501 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:52:52.710505 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:52:52.710510 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:52:52.710515 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:52:52.710520 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:52:52.710525 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:52:52.710527 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:52:52.710731 29777 solver.cpp:471] Iteration 278000, Testing net (#0)
I0630 16:53:24.795035 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 16:53:55.946665 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5606
I0630 16:53:55.946728 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.794941
I0630 16:53:55.946738 29777 solver.cpp:544]     Test net output #2: loss = 1.56158 (* 1 = 1.56158 loss)
I0630 16:53:56.138885 29777 solver.cpp:290] Iteration 278000 (1.25843 iter/s, 79.4641s/100 iter), loss = 1.36905
I0630 16:53:56.138908 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 16:53:56.138916 29777 sgd_solver.cpp:106] Iteration 278000, lr = 0.0013125
I0630 16:54:12.425042 29777 solver.cpp:290] Iteration 278100 (6.14036 iter/s, 16.2857s/100 iter), loss = 1.57143
I0630 16:54:12.425084 29777 solver.cpp:309]     Train net output #0: loss = 1.90476 (* 1 = 1.90476 loss)
I0630 16:54:12.425099 29777 sgd_solver.cpp:106] Iteration 278100, lr = 0.00130938
I0630 16:54:28.857920 29777 solver.cpp:290] Iteration 278200 (6.08554 iter/s, 16.4324s/100 iter), loss = 1.05952
I0630 16:54:28.857990 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 16:54:28.857998 29777 sgd_solver.cpp:106] Iteration 278200, lr = 0.00130625
I0630 16:54:44.962005 29777 solver.cpp:290] Iteration 278300 (6.2098 iter/s, 16.1036s/100 iter), loss = 1.34524
I0630 16:54:44.962028 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 16:54:44.962035 29777 sgd_solver.cpp:106] Iteration 278300, lr = 0.00130312
I0630 16:55:00.939384 29777 solver.cpp:290] Iteration 278400 (6.25903 iter/s, 15.9769s/100 iter), loss = 0.928571
I0630 16:55:00.939450 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 16:55:00.939466 29777 sgd_solver.cpp:106] Iteration 278400, lr = 0.0013
I0630 16:55:16.965695 29777 solver.cpp:290] Iteration 278500 (6.23993 iter/s, 16.0258s/100 iter), loss = 1.11905
I0630 16:55:16.965721 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 16:55:16.965729 29777 sgd_solver.cpp:106] Iteration 278500, lr = 0.00129687
I0630 16:55:33.044785 29777 solver.cpp:290] Iteration 278600 (6.21943 iter/s, 16.0786s/100 iter), loss = 1.03571
I0630 16:55:33.044854 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 16:55:33.044862 29777 sgd_solver.cpp:106] Iteration 278600, lr = 0.00129375
I0630 16:55:49.287850 29777 solver.cpp:290] Iteration 278700 (6.15667 iter/s, 16.2426s/100 iter), loss = 1.57143
I0630 16:55:49.287878 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 16:55:49.287888 29777 sgd_solver.cpp:106] Iteration 278700, lr = 0.00129062
I0630 16:56:05.331868 29777 solver.cpp:290] Iteration 278800 (6.23303 iter/s, 16.0436s/100 iter), loss = 1.10714
I0630 16:56:05.331974 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 16:56:05.331984 29777 sgd_solver.cpp:106] Iteration 278800, lr = 0.0012875
I0630 16:56:21.537645 29777 solver.cpp:290] Iteration 278900 (6.17085 iter/s, 16.2052s/100 iter), loss = 1.38095
I0630 16:56:21.537677 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 16:56:21.537685 29777 sgd_solver.cpp:106] Iteration 278900, lr = 0.00128438
I0630 16:56:37.509292 29777 solver.cpp:354] Sparsity after update:
I0630 16:56:37.538650 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:56:37.538673 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:56:37.538684 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:56:37.538687 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:56:37.538691 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:56:37.538694 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:56:37.538699 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:56:37.538712 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:56:37.538717 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:56:37.538720 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:56:37.538724 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:56:37.538727 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:56:37.538746 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:56:37.689826 29777 solver.cpp:290] Iteration 279000 (6.19129 iter/s, 16.1517s/100 iter), loss = 1.42857
I0630 16:56:37.689852 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 16:56:37.689862 29777 sgd_solver.cpp:106] Iteration 279000, lr = 0.00128125
I0630 16:56:53.917697 29777 solver.cpp:290] Iteration 279100 (6.16241 iter/s, 16.2274s/100 iter), loss = 1.10714
I0630 16:56:53.917719 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 16:56:53.917726 29777 sgd_solver.cpp:106] Iteration 279100, lr = 0.00127812
I0630 16:57:09.938506 29777 solver.cpp:290] Iteration 279200 (6.24206 iter/s, 16.0204s/100 iter), loss = 0.880952
I0630 16:57:09.938580 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 16:57:09.938587 29777 sgd_solver.cpp:106] Iteration 279200, lr = 0.001275
I0630 16:57:26.074738 29777 solver.cpp:290] Iteration 279300 (6.19743 iter/s, 16.1357s/100 iter), loss = 1.16667
I0630 16:57:26.074764 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 16:57:26.074772 29777 sgd_solver.cpp:106] Iteration 279300, lr = 0.00127187
I0630 16:57:42.152009 29777 solver.cpp:290] Iteration 279400 (6.22014 iter/s, 16.0768s/100 iter), loss = 1.54762
I0630 16:57:42.152096 29777 solver.cpp:309]     Train net output #0: loss = 1.78571 (* 1 = 1.78571 loss)
I0630 16:57:42.152106 29777 sgd_solver.cpp:106] Iteration 279400, lr = 0.00126875
I0630 16:57:58.420785 29777 solver.cpp:290] Iteration 279500 (6.14694 iter/s, 16.2683s/100 iter), loss = 1.47619
I0630 16:57:58.420809 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 16:57:58.420815 29777 sgd_solver.cpp:106] Iteration 279500, lr = 0.00126562
I0630 16:58:14.472399 29777 solver.cpp:290] Iteration 279600 (6.23008 iter/s, 16.0512s/100 iter), loss = 1.41667
I0630 16:58:14.472477 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 16:58:14.472487 29777 sgd_solver.cpp:106] Iteration 279600, lr = 0.0012625
I0630 16:58:30.779343 29777 solver.cpp:290] Iteration 279700 (6.13255 iter/s, 16.3064s/100 iter), loss = 1.34524
I0630 16:58:30.779403 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 16:58:30.779422 29777 sgd_solver.cpp:106] Iteration 279700, lr = 0.00125938
I0630 16:58:47.013258 29777 solver.cpp:290] Iteration 279800 (6.16013 iter/s, 16.2334s/100 iter), loss = 1.41667
I0630 16:58:47.013365 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 16:58:47.013375 29777 sgd_solver.cpp:106] Iteration 279800, lr = 0.00125625
I0630 16:59:03.197070 29777 solver.cpp:290] Iteration 279900 (6.17922 iter/s, 16.1833s/100 iter), loss = 1.40476
I0630 16:59:03.197099 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 16:59:03.197108 29777 sgd_solver.cpp:106] Iteration 279900, lr = 0.00125313
I0630 16:59:19.053048 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_280000.caffemodel
I0630 16:59:19.072835 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_280000.solverstate
I0630 16:59:19.083107 29777 solver.cpp:354] Sparsity after update:
I0630 16:59:19.084408 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 16:59:19.084419 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 16:59:19.084430 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 16:59:19.084434 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 16:59:19.084437 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 16:59:19.084441 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 16:59:19.084444 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 16:59:19.084447 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 16:59:19.084451 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 16:59:19.084460 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 16:59:19.084463 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 16:59:19.084467 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 16:59:19.084472 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 16:59:19.084615 29777 solver.cpp:471] Iteration 280000, Testing net (#0)
I0630 16:59:47.363186 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 17:00:20.999557 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.559359
I0630 17:00:20.999634 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.793581
I0630 17:00:20.999642 29777 solver.cpp:544]     Test net output #2: loss = 1.56788 (* 1 = 1.56788 loss)
I0630 17:00:21.182617 29777 solver.cpp:290] Iteration 280000 (1.28232 iter/s, 77.9834s/100 iter), loss = 1.28571
I0630 17:00:21.182643 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 17:00:21.182653 29777 sgd_solver.cpp:106] Iteration 280000, lr = 0.00125
I0630 17:00:37.150815 29777 solver.cpp:290] Iteration 280100 (6.26263 iter/s, 15.9677s/100 iter), loss = 1.4881
I0630 17:00:37.150842 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 17:00:37.150851 29777 sgd_solver.cpp:106] Iteration 280100, lr = 0.00124687
I0630 17:00:53.162376 29777 solver.cpp:290] Iteration 280200 (6.24567 iter/s, 16.0111s/100 iter), loss = 1.40476
I0630 17:00:53.162484 29777 solver.cpp:309]     Train net output #0: loss = 1.85714 (* 1 = 1.85714 loss)
I0630 17:00:53.162494 29777 sgd_solver.cpp:106] Iteration 280200, lr = 0.00124375
I0630 17:01:09.249219 29777 solver.cpp:290] Iteration 280300 (6.21646 iter/s, 16.0863s/100 iter), loss = 1.19048
I0630 17:01:09.249243 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 17:01:09.249249 29777 sgd_solver.cpp:106] Iteration 280300, lr = 0.00124062
I0630 17:01:25.305990 29777 solver.cpp:290] Iteration 280400 (6.22807 iter/s, 16.0563s/100 iter), loss = 0.940476
I0630 17:01:25.306062 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 17:01:25.306071 29777 sgd_solver.cpp:106] Iteration 280400, lr = 0.0012375
I0630 17:01:41.388325 29777 solver.cpp:290] Iteration 280500 (6.21819 iter/s, 16.0819s/100 iter), loss = 1.13095
I0630 17:01:41.388348 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 17:01:41.388355 29777 sgd_solver.cpp:106] Iteration 280500, lr = 0.00123438
I0630 17:01:57.432045 29777 solver.cpp:290] Iteration 280600 (6.23314 iter/s, 16.0433s/100 iter), loss = 1.2381
I0630 17:01:57.432150 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 17:01:57.432162 29777 sgd_solver.cpp:106] Iteration 280600, lr = 0.00123125
I0630 17:02:13.650142 29777 solver.cpp:290] Iteration 280700 (6.16615 iter/s, 16.2176s/100 iter), loss = 1.60714
I0630 17:02:13.650168 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 17:02:13.650177 29777 sgd_solver.cpp:106] Iteration 280700, lr = 0.00122813
I0630 17:02:29.831900 29777 solver.cpp:290] Iteration 280800 (6.17997 iter/s, 16.1813s/100 iter), loss = 1.2381
I0630 17:02:29.832016 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 17:02:29.832043 29777 sgd_solver.cpp:106] Iteration 280800, lr = 0.001225
I0630 17:02:45.963198 29777 solver.cpp:290] Iteration 280900 (6.19933 iter/s, 16.1308s/100 iter), loss = 1.44048
I0630 17:02:45.963222 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 17:02:45.963229 29777 sgd_solver.cpp:106] Iteration 280900, lr = 0.00122187
I0630 17:03:01.784296 29777 solver.cpp:354] Sparsity after update:
I0630 17:03:01.804709 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:03:01.804730 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:03:01.804742 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:03:01.804746 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:03:01.804749 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:03:01.804774 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:03:01.804783 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:03:01.804791 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:03:01.804800 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:03:01.804807 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:03:01.804816 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:03:01.804824 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:03:01.804832 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:03:01.964717 29777 solver.cpp:290] Iteration 281000 (6.24958 iter/s, 16.0011s/100 iter), loss = 1.22619
I0630 17:03:01.964742 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 17:03:01.964751 29777 sgd_solver.cpp:106] Iteration 281000, lr = 0.00121875
I0630 17:03:18.016782 29777 solver.cpp:290] Iteration 281100 (6.2299 iter/s, 16.0516s/100 iter), loss = 1.19048
I0630 17:03:18.016806 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 17:03:18.016824 29777 sgd_solver.cpp:106] Iteration 281100, lr = 0.00121562
I0630 17:03:34.212791 29777 solver.cpp:290] Iteration 281200 (6.17453 iter/s, 16.1956s/100 iter), loss = 1.30952
I0630 17:03:34.212893 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 17:03:34.212909 29777 sgd_solver.cpp:106] Iteration 281200, lr = 0.0012125
I0630 17:03:50.322293 29777 solver.cpp:290] Iteration 281300 (6.20771 iter/s, 16.109s/100 iter), loss = 0.928571
I0630 17:03:50.322314 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 17:03:50.322398 29777 sgd_solver.cpp:106] Iteration 281300, lr = 0.00120938
I0630 17:04:06.386071 29777 solver.cpp:290] Iteration 281400 (6.22536 iter/s, 16.0633s/100 iter), loss = 1.60714
I0630 17:04:06.386175 29777 solver.cpp:309]     Train net output #0: loss = 2 (* 1 = 2 loss)
I0630 17:04:06.386186 29777 sgd_solver.cpp:106] Iteration 281400, lr = 0.00120625
I0630 17:04:22.343489 29777 solver.cpp:290] Iteration 281500 (6.26688 iter/s, 15.9569s/100 iter), loss = 1.32143
I0630 17:04:22.343513 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 17:04:22.343519 29777 sgd_solver.cpp:106] Iteration 281500, lr = 0.00120313
I0630 17:04:38.349198 29777 solver.cpp:290] Iteration 281600 (6.24794 iter/s, 16.0053s/100 iter), loss = 1.10714
I0630 17:04:38.349282 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 17:04:38.349289 29777 sgd_solver.cpp:106] Iteration 281600, lr = 0.0012
I0630 17:04:54.474395 29777 solver.cpp:290] Iteration 281700 (6.20167 iter/s, 16.1247s/100 iter), loss = 1.25
I0630 17:04:54.474427 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 17:04:54.474438 29777 sgd_solver.cpp:106] Iteration 281700, lr = 0.00119688
I0630 17:05:10.648337 29777 solver.cpp:290] Iteration 281800 (6.18297 iter/s, 16.1735s/100 iter), loss = 1.27381
I0630 17:05:10.648439 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 17:05:10.648449 29777 sgd_solver.cpp:106] Iteration 281800, lr = 0.00119375
I0630 17:05:26.600831 29777 solver.cpp:290] Iteration 281900 (6.26883 iter/s, 15.952s/100 iter), loss = 1.39286
I0630 17:05:26.600852 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 17:05:26.600859 29777 sgd_solver.cpp:106] Iteration 281900, lr = 0.00119062
I0630 17:05:42.466426 29777 solver.cpp:354] Sparsity after update:
I0630 17:05:42.467876 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:05:42.467885 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:05:42.467892 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:05:42.467895 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:05:42.467896 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:05:42.467898 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:05:42.467900 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:05:42.467902 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:05:42.467905 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:05:42.467906 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:05:42.467908 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:05:42.467911 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:05:42.467912 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:05:42.468046 29777 solver.cpp:471] Iteration 282000, Testing net (#0)
I0630 17:06:10.328379 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 17:06:42.278558 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.559559
I0630 17:06:42.278641 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.794682
I0630 17:06:42.278650 29777 solver.cpp:544]     Test net output #2: loss = 1.56782 (* 1 = 1.56782 loss)
I0630 17:06:42.454619 29777 solver.cpp:290] Iteration 282000 (1.31836 iter/s, 75.8517s/100 iter), loss = 1
I0630 17:06:42.454643 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 17:06:42.454650 29777 sgd_solver.cpp:106] Iteration 282000, lr = 0.0011875
I0630 17:06:58.665565 29777 solver.cpp:290] Iteration 282100 (6.16885 iter/s, 16.2105s/100 iter), loss = 1.63095
I0630 17:06:58.665588 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 17:06:58.665606 29777 sgd_solver.cpp:106] Iteration 282100, lr = 0.00118438
I0630 17:07:14.904050 29777 solver.cpp:290] Iteration 282200 (6.15839 iter/s, 16.238s/100 iter), loss = 0.857143
I0630 17:07:14.904139 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 17:07:14.904146 29777 sgd_solver.cpp:106] Iteration 282200, lr = 0.00118125
I0630 17:07:31.169255 29777 solver.cpp:290] Iteration 282300 (6.1483 iter/s, 16.2647s/100 iter), loss = 1.36905
I0630 17:07:31.169296 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 17:07:31.169311 29777 sgd_solver.cpp:106] Iteration 282300, lr = 0.00117813
I0630 17:07:47.848453 29777 solver.cpp:290] Iteration 282400 (5.99567 iter/s, 16.6787s/100 iter), loss = 1.47619
I0630 17:07:47.848578 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 17:07:47.848589 29777 sgd_solver.cpp:106] Iteration 282400, lr = 0.001175
I0630 17:08:04.030392 29777 solver.cpp:290] Iteration 282500 (6.17995 iter/s, 16.1814s/100 iter), loss = 1.52381
I0630 17:08:04.030418 29777 solver.cpp:309]     Train net output #0: loss = 1.78571 (* 1 = 1.78571 loss)
I0630 17:08:04.030426 29777 sgd_solver.cpp:106] Iteration 282500, lr = 0.00117187
I0630 17:08:20.173286 29777 solver.cpp:290] Iteration 282600 (6.19487 iter/s, 16.1424s/100 iter), loss = 1.34524
I0630 17:08:20.173548 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 17:08:20.173638 29777 sgd_solver.cpp:106] Iteration 282600, lr = 0.00116875
I0630 17:08:36.437882 29777 solver.cpp:290] Iteration 282700 (6.14858 iter/s, 16.2639s/100 iter), loss = 1.55952
I0630 17:08:36.437908 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 17:08:36.437917 29777 sgd_solver.cpp:106] Iteration 282700, lr = 0.00116562
I0630 17:08:52.603209 29777 solver.cpp:290] Iteration 282800 (6.18626 iter/s, 16.1649s/100 iter), loss = 0.916667
I0630 17:08:52.603310 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 17:08:52.603320 29777 sgd_solver.cpp:106] Iteration 282800, lr = 0.0011625
I0630 17:09:08.810308 29777 solver.cpp:290] Iteration 282900 (6.17034 iter/s, 16.2066s/100 iter), loss = 1.08333
I0630 17:09:08.810333 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 17:09:08.810339 29777 sgd_solver.cpp:106] Iteration 282900, lr = 0.00115937
I0630 17:09:24.843513 29777 solver.cpp:354] Sparsity after update:
I0630 17:09:24.863724 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:09:24.863764 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:09:24.863780 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:09:24.863790 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:09:24.863797 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:09:24.863806 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:09:24.863814 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:09:24.863821 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:09:24.863829 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:09:24.863837 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:09:24.863845 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:09:24.863853 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:09:24.863862 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:09:25.019034 29777 solver.cpp:290] Iteration 283000 (6.1697 iter/s, 16.2083s/100 iter), loss = 0.964286
I0630 17:09:25.019060 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 17:09:25.019069 29777 sgd_solver.cpp:106] Iteration 283000, lr = 0.00115625
I0630 17:09:41.283560 29777 solver.cpp:290] Iteration 283100 (6.14853 iter/s, 16.264s/100 iter), loss = 1.28571
I0630 17:09:41.283638 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 17:09:41.283663 29777 sgd_solver.cpp:106] Iteration 283100, lr = 0.00115313
I0630 17:09:57.384878 29777 solver.cpp:290] Iteration 283200 (6.21087 iter/s, 16.1008s/100 iter), loss = 1.35714
I0630 17:09:57.384982 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 17:09:57.384994 29777 sgd_solver.cpp:106] Iteration 283200, lr = 0.00115
I0630 17:10:13.322868 29777 solver.cpp:290] Iteration 283300 (6.27454 iter/s, 15.9374s/100 iter), loss = 1.04762
I0630 17:10:13.322918 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 17:10:13.322943 29777 sgd_solver.cpp:106] Iteration 283300, lr = 0.00114687
I0630 17:10:29.445705 29777 solver.cpp:290] Iteration 283400 (6.20257 iter/s, 16.1223s/100 iter), loss = 1.0119
I0630 17:10:29.445801 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 17:10:29.445812 29777 sgd_solver.cpp:106] Iteration 283400, lr = 0.00114375
I0630 17:10:45.446838 29777 solver.cpp:290] Iteration 283500 (6.24977 iter/s, 16.0006s/100 iter), loss = 1.29762
I0630 17:10:45.446887 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 17:10:45.446900 29777 sgd_solver.cpp:106] Iteration 283500, lr = 0.00114062
I0630 17:11:01.798624 29777 solver.cpp:290] Iteration 283600 (6.11573 iter/s, 16.3513s/100 iter), loss = 1.47619
I0630 17:11:01.798719 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 17:11:01.798729 29777 sgd_solver.cpp:106] Iteration 283600, lr = 0.0011375
I0630 17:11:17.814823 29777 solver.cpp:290] Iteration 283700 (6.24389 iter/s, 16.0157s/100 iter), loss = 1.57143
I0630 17:11:17.814853 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 17:11:17.814862 29777 sgd_solver.cpp:106] Iteration 283700, lr = 0.00113437
I0630 17:11:33.924600 29777 solver.cpp:290] Iteration 283800 (6.20759 iter/s, 16.1093s/100 iter), loss = 1.29762
I0630 17:11:33.924707 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 17:11:33.924717 29777 sgd_solver.cpp:106] Iteration 283800, lr = 0.00113125
I0630 17:11:50.002185 29777 solver.cpp:290] Iteration 283900 (6.22005 iter/s, 16.077s/100 iter), loss = 0.97619
I0630 17:11:50.002212 29777 solver.cpp:309]     Train net output #0: loss = 0.690476 (* 1 = 0.690476 loss)
I0630 17:11:50.002220 29777 sgd_solver.cpp:106] Iteration 283900, lr = 0.00112813
I0630 17:12:05.875097 29777 solver.cpp:354] Sparsity after update:
I0630 17:12:05.876514 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:12:05.876523 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:12:05.876533 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:12:05.876538 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:12:05.876543 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:12:05.876549 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:12:05.876552 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:12:05.876557 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:12:05.876562 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:12:05.876566 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:12:05.876571 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:12:05.876575 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:12:05.876580 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:12:05.876673 29777 solver.cpp:471] Iteration 284000, Testing net (#0)
I0630 17:12:34.065768 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 17:13:09.121268 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.560219
I0630 17:13:09.121361 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.794562
I0630 17:13:09.121377 29777 solver.cpp:544]     Test net output #2: loss = 1.56022 (* 1 = 1.56022 loss)
I0630 17:13:09.310082 29777 solver.cpp:290] Iteration 284000 (1.26094 iter/s, 79.3057s/100 iter), loss = 1.0119
I0630 17:13:09.310135 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 17:13:09.310302 29777 sgd_solver.cpp:106] Iteration 284000, lr = 0.001125
I0630 17:13:25.528134 29777 solver.cpp:290] Iteration 284100 (6.16616 iter/s, 16.2176s/100 iter), loss = 1.36905
I0630 17:13:25.528161 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 17:13:25.528169 29777 sgd_solver.cpp:106] Iteration 284100, lr = 0.00112188
I0630 17:13:41.489471 29777 solver.cpp:290] Iteration 284200 (6.26532 iter/s, 15.9609s/100 iter), loss = 0.952381
I0630 17:13:41.489542 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 17:13:41.489553 29777 sgd_solver.cpp:106] Iteration 284200, lr = 0.00111875
I0630 17:13:57.651161 29777 solver.cpp:290] Iteration 284300 (6.18767 iter/s, 16.1612s/100 iter), loss = 1.29762
I0630 17:13:57.651185 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 17:13:57.651191 29777 sgd_solver.cpp:106] Iteration 284300, lr = 0.00111562
I0630 17:14:13.812708 29777 solver.cpp:290] Iteration 284400 (6.18771 iter/s, 16.1611s/100 iter), loss = 1.20238
I0630 17:14:13.812777 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 17:14:13.812788 29777 sgd_solver.cpp:106] Iteration 284400, lr = 0.0011125
I0630 17:14:29.972671 29777 solver.cpp:290] Iteration 284500 (6.18833 iter/s, 16.1594s/100 iter), loss = 0.976191
I0630 17:14:29.972694 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 17:14:29.972700 29777 sgd_solver.cpp:106] Iteration 284500, lr = 0.00110937
I0630 17:14:46.031225 29777 solver.cpp:290] Iteration 284600 (6.22739 iter/s, 16.0581s/100 iter), loss = 1.5119
I0630 17:14:46.031635 29777 solver.cpp:309]     Train net output #0: loss = 1.64286 (* 1 = 1.64286 loss)
I0630 17:14:46.031646 29777 sgd_solver.cpp:106] Iteration 284600, lr = 0.00110625
I0630 17:15:02.115922 29777 solver.cpp:290] Iteration 284700 (6.21742 iter/s, 16.0838s/100 iter), loss = 1.09524
I0630 17:15:02.115945 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 17:15:02.115952 29777 sgd_solver.cpp:106] Iteration 284700, lr = 0.00110313
I0630 17:15:18.783980 29777 solver.cpp:290] Iteration 284800 (5.99967 iter/s, 16.6676s/100 iter), loss = 1.25
I0630 17:15:18.784045 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 17:15:18.784056 29777 sgd_solver.cpp:106] Iteration 284800, lr = 0.0011
I0630 17:15:35.062820 29777 solver.cpp:290] Iteration 284900 (6.14314 iter/s, 16.2783s/100 iter), loss = 1.34524
I0630 17:15:35.062930 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 17:15:35.062961 29777 sgd_solver.cpp:106] Iteration 284900, lr = 0.00109688
I0630 17:15:50.879458 29777 solver.cpp:354] Sparsity after update:
I0630 17:15:50.899850 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:15:50.899866 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:15:50.899878 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:15:50.899880 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:15:50.899883 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:15:50.899888 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:15:50.899893 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:15:50.899896 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:15:50.899899 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:15:50.899902 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:15:50.899905 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:15:50.899909 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:15:50.899914 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:15:51.055974 29777 solver.cpp:290] Iteration 285000 (6.25289 iter/s, 15.9926s/100 iter), loss = 1.02381
I0630 17:15:51.055995 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 17:15:51.056016 29777 sgd_solver.cpp:106] Iteration 285000, lr = 0.00109375
I0630 17:16:07.058696 29777 solver.cpp:290] Iteration 285100 (6.24912 iter/s, 16.0023s/100 iter), loss = 1.17857
I0630 17:16:07.058790 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 17:16:07.058826 29777 sgd_solver.cpp:106] Iteration 285100, lr = 0.00109062
I0630 17:16:23.183198 29777 solver.cpp:290] Iteration 285200 (6.20195 iter/s, 16.124s/100 iter), loss = 1.38095
I0630 17:16:23.183305 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 17:16:23.183315 29777 sgd_solver.cpp:106] Iteration 285200, lr = 0.0010875
I0630 17:16:39.280849 29777 solver.cpp:290] Iteration 285300 (6.2123 iter/s, 16.0971s/100 iter), loss = 1.14286
I0630 17:16:39.280870 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 17:16:39.280879 29777 sgd_solver.cpp:106] Iteration 285300, lr = 0.00108437
I0630 17:16:55.347234 29777 solver.cpp:290] Iteration 285400 (6.22436 iter/s, 16.0659s/100 iter), loss = 1.07143
I0630 17:16:55.347303 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 17:16:55.347311 29777 sgd_solver.cpp:106] Iteration 285400, lr = 0.00108125
I0630 17:17:11.487680 29777 solver.cpp:290] Iteration 285500 (6.19581 iter/s, 16.1399s/100 iter), loss = 1.02381
I0630 17:17:11.487706 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 17:17:11.487715 29777 sgd_solver.cpp:106] Iteration 285500, lr = 0.00107813
I0630 17:17:27.520227 29777 solver.cpp:290] Iteration 285600 (6.2375 iter/s, 16.0321s/100 iter), loss = 0.976191
I0630 17:17:27.520279 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 17:17:27.520290 29777 sgd_solver.cpp:106] Iteration 285600, lr = 0.001075
I0630 17:17:43.583160 29777 solver.cpp:290] Iteration 285700 (6.2257 iter/s, 16.0624s/100 iter), loss = 1.2381
I0630 17:17:43.583184 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 17:17:43.583191 29777 sgd_solver.cpp:106] Iteration 285700, lr = 0.00107188
I0630 17:17:59.658637 29777 solver.cpp:290] Iteration 285800 (6.22084 iter/s, 16.075s/100 iter), loss = 1.59524
I0630 17:17:59.658733 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 17:17:59.658745 29777 sgd_solver.cpp:106] Iteration 285800, lr = 0.00106875
I0630 17:18:15.624510 29777 solver.cpp:290] Iteration 285900 (6.26357 iter/s, 15.9653s/100 iter), loss = 1.30952
I0630 17:18:15.624533 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 17:18:15.624542 29777 sgd_solver.cpp:106] Iteration 285900, lr = 0.00106562
I0630 17:18:31.548679 29777 solver.cpp:354] Sparsity after update:
I0630 17:18:31.549973 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:18:31.549979 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:18:31.549988 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:18:31.549989 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:18:31.549991 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:18:31.549993 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:18:31.549995 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:18:31.549998 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:18:31.549999 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:18:31.550001 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:18:31.550004 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:18:31.550005 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:18:31.550007 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:18:31.550091 29777 solver.cpp:471] Iteration 286000, Testing net (#0)
I0630 17:19:01.436851 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 17:19:30.697374 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.560059
I0630 17:19:30.697427 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.794461
I0630 17:19:30.697433 29777 solver.cpp:544]     Test net output #2: loss = 1.559 (* 1 = 1.559 loss)
I0630 17:19:30.881933 29777 solver.cpp:290] Iteration 286000 (1.32881 iter/s, 75.2553s/100 iter), loss = 1.04762
I0630 17:19:30.881970 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 17:19:30.881992 29777 sgd_solver.cpp:106] Iteration 286000, lr = 0.0010625
I0630 17:19:46.828047 29777 solver.cpp:290] Iteration 286100 (6.27131 iter/s, 15.9456s/100 iter), loss = 1.22619
I0630 17:19:46.828070 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 17:19:46.828076 29777 sgd_solver.cpp:106] Iteration 286100, lr = 0.00105937
I0630 17:20:03.002068 29777 solver.cpp:290] Iteration 286200 (6.18294 iter/s, 16.1735s/100 iter), loss = 1.20238
I0630 17:20:03.002188 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 17:20:03.002198 29777 sgd_solver.cpp:106] Iteration 286200, lr = 0.00105625
I0630 17:20:19.147915 29777 solver.cpp:290] Iteration 286300 (6.19376 iter/s, 16.1453s/100 iter), loss = 1.39286
I0630 17:20:19.147940 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 17:20:19.147948 29777 sgd_solver.cpp:106] Iteration 286300, lr = 0.00105313
I0630 17:20:35.188217 29777 solver.cpp:290] Iteration 286400 (6.23448 iter/s, 16.0398s/100 iter), loss = 1.53571
I0630 17:20:35.188313 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 17:20:35.188323 29777 sgd_solver.cpp:106] Iteration 286400, lr = 0.00105
I0630 17:20:51.137120 29777 solver.cpp:290] Iteration 286500 (6.27024 iter/s, 15.9484s/100 iter), loss = 1.30952
I0630 17:20:51.137164 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 17:20:51.137177 29777 sgd_solver.cpp:106] Iteration 286500, lr = 0.00104688
I0630 17:21:07.227602 29777 solver.cpp:290] Iteration 286600 (6.21505 iter/s, 16.09s/100 iter), loss = 1.04762
I0630 17:21:07.227717 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 17:21:07.227737 29777 sgd_solver.cpp:106] Iteration 286600, lr = 0.00104375
I0630 17:21:23.384256 29777 solver.cpp:290] Iteration 286700 (6.18961 iter/s, 16.1561s/100 iter), loss = 1.22619
I0630 17:21:23.384284 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 17:21:23.384290 29777 sgd_solver.cpp:106] Iteration 286700, lr = 0.00104062
I0630 17:21:39.515103 29777 solver.cpp:290] Iteration 286800 (6.19949 iter/s, 16.1304s/100 iter), loss = 1.17857
I0630 17:21:39.515202 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 17:21:39.515213 29777 sgd_solver.cpp:106] Iteration 286800, lr = 0.0010375
I0630 17:21:55.614341 29777 solver.cpp:290] Iteration 286900 (6.21169 iter/s, 16.0987s/100 iter), loss = 1.34524
I0630 17:21:55.614365 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 17:21:55.614373 29777 sgd_solver.cpp:106] Iteration 286900, lr = 0.00103437
I0630 17:22:11.560190 29777 solver.cpp:354] Sparsity after update:
I0630 17:22:11.580288 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:22:11.580315 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:22:11.580335 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:22:11.580343 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:22:11.580348 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:22:11.580353 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:22:11.580358 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:22:11.580361 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:22:11.580368 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:22:11.580373 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:22:11.580379 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:22:11.580384 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:22:11.580390 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:22:11.741456 29777 solver.cpp:290] Iteration 287000 (6.20092 iter/s, 16.1266s/100 iter), loss = 1.32143
I0630 17:22:11.741482 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 17:22:11.741490 29777 sgd_solver.cpp:106] Iteration 287000, lr = 0.00103125
I0630 17:22:27.916051 29777 solver.cpp:290] Iteration 287100 (6.18272 iter/s, 16.1741s/100 iter), loss = 1.59524
I0630 17:22:27.916074 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 17:22:27.916081 29777 sgd_solver.cpp:106] Iteration 287100, lr = 0.00102813
I0630 17:22:44.120811 29777 solver.cpp:290] Iteration 287200 (6.17121 iter/s, 16.2043s/100 iter), loss = 1.47619
I0630 17:22:44.120919 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 17:22:44.120934 29777 sgd_solver.cpp:106] Iteration 287200, lr = 0.001025
I0630 17:23:00.330063 29777 solver.cpp:290] Iteration 287300 (6.16953 iter/s, 16.2087s/100 iter), loss = 1.35714
I0630 17:23:00.330143 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 17:23:00.330174 29777 sgd_solver.cpp:106] Iteration 287300, lr = 0.00102188
I0630 17:23:16.608460 29777 solver.cpp:290] Iteration 287400 (6.14331 iter/s, 16.2779s/100 iter), loss = 1.39286
I0630 17:23:16.608568 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 17:23:16.608578 29777 sgd_solver.cpp:106] Iteration 287400, lr = 0.00101875
I0630 17:23:32.705376 29777 solver.cpp:290] Iteration 287500 (6.21258 iter/s, 16.0964s/100 iter), loss = 1.41667
I0630 17:23:32.705400 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 17:23:32.705406 29777 sgd_solver.cpp:106] Iteration 287500, lr = 0.00101562
I0630 17:23:49.050240 29777 solver.cpp:290] Iteration 287600 (6.11831 iter/s, 16.3444s/100 iter), loss = 1.42857
I0630 17:23:49.050329 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 17:23:49.050340 29777 sgd_solver.cpp:106] Iteration 287600, lr = 0.0010125
I0630 17:24:05.322429 29777 solver.cpp:290] Iteration 287700 (6.14566 iter/s, 16.2716s/100 iter), loss = 1.04762
I0630 17:24:05.322474 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 17:24:05.322487 29777 sgd_solver.cpp:106] Iteration 287700, lr = 0.00100937
I0630 17:24:21.468595 29777 solver.cpp:290] Iteration 287800 (6.19361 iter/s, 16.1457s/100 iter), loss = 1.0119
I0630 17:24:21.468729 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 17:24:21.468739 29777 sgd_solver.cpp:106] Iteration 287800, lr = 0.00100625
I0630 17:24:37.687618 29777 solver.cpp:290] Iteration 287900 (6.16582 iter/s, 16.2184s/100 iter), loss = 0.809524
I0630 17:24:37.687643 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 17:24:37.687651 29777 sgd_solver.cpp:106] Iteration 287900, lr = 0.00100312
I0630 17:24:53.582113 29777 solver.cpp:354] Sparsity after update:
I0630 17:24:53.583384 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:24:53.583392 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:24:53.583400 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:24:53.583403 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:24:53.583405 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:24:53.583407 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:24:53.583410 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:24:53.583412 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:24:53.583415 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:24:53.583416 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:24:53.583420 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:24:53.583421 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:24:53.583423 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:24:53.583509 29777 solver.cpp:471] Iteration 288000, Testing net (#0)
I0630 17:25:23.614511 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 17:25:58.031829 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56056
I0630 17:25:58.031916 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.795061
I0630 17:25:58.031927 29777 solver.cpp:544]     Test net output #2: loss = 1.55818 (* 1 = 1.55818 loss)
I0630 17:25:58.218803 29777 solver.cpp:290] Iteration 288000 (1.24179 iter/s, 80.5289s/100 iter), loss = 1.2619
I0630 17:25:58.218829 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 17:25:58.218842 29777 sgd_solver.cpp:106] Iteration 288000, lr = 0.001
I0630 17:26:14.286845 29777 solver.cpp:290] Iteration 288100 (6.22373 iter/s, 16.0675s/100 iter), loss = 1.28571
I0630 17:26:14.287076 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 17:26:14.287231 29777 sgd_solver.cpp:106] Iteration 288100, lr = 0.000996875
I0630 17:26:30.585925 29777 solver.cpp:290] Iteration 288200 (6.13557 iter/s, 16.2984s/100 iter), loss = 1.2619
I0630 17:26:30.586066 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 17:26:30.586092 29777 sgd_solver.cpp:106] Iteration 288200, lr = 0.00099375
I0630 17:26:46.739785 29777 solver.cpp:290] Iteration 288300 (6.1907 iter/s, 16.1533s/100 iter), loss = 1.41667
I0630 17:26:46.739812 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 17:26:46.739821 29777 sgd_solver.cpp:106] Iteration 288300, lr = 0.000990625
I0630 17:27:02.854013 29777 solver.cpp:290] Iteration 288400 (6.20588 iter/s, 16.1138s/100 iter), loss = 1.29762
I0630 17:27:02.854123 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 17:27:02.854133 29777 sgd_solver.cpp:106] Iteration 288400, lr = 0.0009875
I0630 17:27:19.086715 29777 solver.cpp:290] Iteration 288500 (6.16062 iter/s, 16.2321s/100 iter), loss = 1.29762
I0630 17:27:19.086740 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 17:27:19.086750 29777 sgd_solver.cpp:106] Iteration 288500, lr = 0.000984375
I0630 17:27:35.400409 29777 solver.cpp:290] Iteration 288600 (6.13 iter/s, 16.3132s/100 iter), loss = 1.45238
I0630 17:27:35.400457 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 17:27:35.400466 29777 sgd_solver.cpp:106] Iteration 288600, lr = 0.00098125
I0630 17:27:51.461545 29777 solver.cpp:290] Iteration 288700 (6.2264 iter/s, 16.0606s/100 iter), loss = 1.09524
I0630 17:27:51.461578 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 17:27:51.461587 29777 sgd_solver.cpp:106] Iteration 288700, lr = 0.000978125
I0630 17:28:07.565199 29777 solver.cpp:290] Iteration 288800 (6.20996 iter/s, 16.1032s/100 iter), loss = 1.17857
I0630 17:28:07.565289 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 17:28:07.565306 29777 sgd_solver.cpp:106] Iteration 288800, lr = 0.000975
I0630 17:28:23.788097 29777 solver.cpp:290] Iteration 288900 (6.16433 iter/s, 16.2224s/100 iter), loss = 1.20238
I0630 17:28:23.788188 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 17:28:23.788219 29777 sgd_solver.cpp:106] Iteration 288900, lr = 0.000971875
I0630 17:28:39.750427 29777 solver.cpp:354] Sparsity after update:
I0630 17:28:39.770886 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:28:39.770903 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:28:39.770915 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:28:39.770917 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:28:39.770920 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:28:39.770936 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:28:39.770946 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:28:39.770954 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:28:39.770962 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:28:39.770970 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:28:39.770977 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:28:39.770985 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:28:39.770993 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:28:39.932538 29777 solver.cpp:290] Iteration 289000 (6.19429 iter/s, 16.1439s/100 iter), loss = 0.964286
I0630 17:28:39.932559 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 17:28:39.932565 29777 sgd_solver.cpp:106] Iteration 289000, lr = 0.00096875
I0630 17:28:55.974465 29777 solver.cpp:290] Iteration 289100 (6.23385 iter/s, 16.0415s/100 iter), loss = 0.940476
I0630 17:28:55.974491 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 17:28:55.974500 29777 sgd_solver.cpp:106] Iteration 289100, lr = 0.000965625
I0630 17:29:12.132303 29777 solver.cpp:290] Iteration 289200 (6.18913 iter/s, 16.1574s/100 iter), loss = 1.41667
I0630 17:29:12.132371 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 17:29:12.132381 29777 sgd_solver.cpp:106] Iteration 289200, lr = 0.0009625
I0630 17:29:28.430519 29777 solver.cpp:290] Iteration 289300 (6.13584 iter/s, 16.2977s/100 iter), loss = 0.964286
I0630 17:29:28.430548 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 17:29:28.430557 29777 sgd_solver.cpp:106] Iteration 289300, lr = 0.000959375
I0630 17:29:44.701161 29777 solver.cpp:290] Iteration 289400 (6.14622 iter/s, 16.2702s/100 iter), loss = 1.07143
I0630 17:29:44.701277 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 17:29:44.701292 29777 sgd_solver.cpp:106] Iteration 289400, lr = 0.00095625
I0630 17:30:00.817814 29777 solver.cpp:290] Iteration 289500 (6.20498 iter/s, 16.1161s/100 iter), loss = 1.5
I0630 17:30:00.817838 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 17:30:00.817844 29777 sgd_solver.cpp:106] Iteration 289500, lr = 0.000953125
I0630 17:30:16.818025 29777 solver.cpp:290] Iteration 289600 (6.2501 iter/s, 15.9997s/100 iter), loss = 1.4881
I0630 17:30:16.818096 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 17:30:16.818105 29777 sgd_solver.cpp:106] Iteration 289600, lr = 0.00095
I0630 17:30:32.997741 29777 solver.cpp:290] Iteration 289700 (6.18078 iter/s, 16.1792s/100 iter), loss = 1.02381
I0630 17:30:32.997766 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 17:30:32.997776 29777 sgd_solver.cpp:106] Iteration 289700, lr = 0.000946875
I0630 17:30:49.130882 29777 solver.cpp:290] Iteration 289800 (6.1986 iter/s, 16.1327s/100 iter), loss = 1.11905
I0630 17:30:49.130964 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 17:30:49.130975 29777 sgd_solver.cpp:106] Iteration 289800, lr = 0.00094375
I0630 17:31:05.722362 29777 solver.cpp:290] Iteration 289900 (6.02739 iter/s, 16.5909s/100 iter), loss = 1.21429
I0630 17:31:05.722389 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 17:31:05.722398 29777 sgd_solver.cpp:106] Iteration 289900, lr = 0.000940625
I0630 17:31:21.602392 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_290000.caffemodel
I0630 17:31:21.625623 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_290000.solverstate
I0630 17:31:21.635192 29777 solver.cpp:354] Sparsity after update:
I0630 17:31:21.636317 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:31:21.636342 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:31:21.636355 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:31:21.636358 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:31:21.636363 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:31:21.636366 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:31:21.636399 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:31:21.636423 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:31:21.636445 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:31:21.636466 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:31:21.636485 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:31:21.636505 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:31:21.636533 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:31:21.636787 29777 solver.cpp:471] Iteration 290000, Testing net (#0)
I0630 17:31:53.926623 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 17:32:24.782502 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.561339
I0630 17:32:24.782552 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.794881
I0630 17:32:24.782557 29777 solver.cpp:544]     Test net output #2: loss = 1.5572 (* 1 = 1.5572 loss)
I0630 17:32:24.960434 29777 solver.cpp:290] Iteration 290000 (1.26205 iter/s, 79.2359s/100 iter), loss = 1.09524
I0630 17:32:24.960460 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 17:32:24.960469 29777 sgd_solver.cpp:106] Iteration 290000, lr = 0.0009375
I0630 17:32:40.976974 29777 solver.cpp:290] Iteration 290100 (6.24373 iter/s, 16.0161s/100 iter), loss = 1.07143
I0630 17:32:40.976995 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 17:32:40.977002 29777 sgd_solver.cpp:106] Iteration 290100, lr = 0.000934375
I0630 17:32:57.129739 29777 solver.cpp:290] Iteration 290200 (6.19107 iter/s, 16.1523s/100 iter), loss = 1.27381
I0630 17:32:57.129848 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 17:32:57.129858 29777 sgd_solver.cpp:106] Iteration 290200, lr = 0.00093125
I0630 17:33:13.125516 29777 solver.cpp:290] Iteration 290300 (6.25187 iter/s, 15.9952s/100 iter), loss = 1.33333
I0630 17:33:13.125540 29777 solver.cpp:309]     Train net output #0: loss = 1.71429 (* 1 = 1.71429 loss)
I0630 17:33:13.125546 29777 sgd_solver.cpp:106] Iteration 290300, lr = 0.000928125
I0630 17:33:29.149704 29777 solver.cpp:290] Iteration 290400 (6.24075 iter/s, 16.0237s/100 iter), loss = 1.25
I0630 17:33:29.149811 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 17:33:29.149826 29777 sgd_solver.cpp:106] Iteration 290400, lr = 0.000925
I0630 17:33:45.125416 29777 solver.cpp:290] Iteration 290500 (6.25972 iter/s, 15.9752s/100 iter), loss = 1.2619
I0630 17:33:45.125442 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 17:33:45.125450 29777 sgd_solver.cpp:106] Iteration 290500, lr = 0.000921875
I0630 17:34:01.391939 29777 solver.cpp:290] Iteration 290600 (6.14778 iter/s, 16.266s/100 iter), loss = 1.41667
I0630 17:34:01.392047 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 17:34:01.392057 29777 sgd_solver.cpp:106] Iteration 290600, lr = 0.00091875
I0630 17:34:17.514559 29777 solver.cpp:290] Iteration 290700 (6.20268 iter/s, 16.1221s/100 iter), loss = 1.19048
I0630 17:34:17.514581 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 17:34:17.514588 29777 sgd_solver.cpp:106] Iteration 290700, lr = 0.000915625
I0630 17:34:33.644093 29777 solver.cpp:290] Iteration 290800 (6.19999 iter/s, 16.1291s/100 iter), loss = 1.08333
I0630 17:34:33.644170 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 17:34:33.644181 29777 sgd_solver.cpp:106] Iteration 290800, lr = 0.0009125
I0630 17:34:49.688630 29777 solver.cpp:290] Iteration 290900 (6.23285 iter/s, 16.044s/100 iter), loss = 1.59524
I0630 17:34:49.688653 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 17:34:49.688660 29777 sgd_solver.cpp:106] Iteration 290900, lr = 0.000909375
I0630 17:35:05.504011 29777 solver.cpp:354] Sparsity after update:
I0630 17:35:05.524543 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:35:05.524559 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:35:05.524567 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:35:05.524569 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:35:05.524571 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:35:05.524574 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:35:05.524575 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:35:05.524577 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:35:05.524580 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:35:05.524581 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:35:05.524583 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:35:05.524585 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:35:05.524587 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:35:05.682700 29777 solver.cpp:290] Iteration 291000 (6.2525 iter/s, 15.9936s/100 iter), loss = 1.16667
I0630 17:35:05.682723 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 17:35:05.682729 29777 sgd_solver.cpp:106] Iteration 291000, lr = 0.00090625
I0630 17:35:21.715629 29777 solver.cpp:290] Iteration 291100 (6.23735 iter/s, 16.0325s/100 iter), loss = 0.797619
I0630 17:35:21.715657 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 17:35:21.715667 29777 sgd_solver.cpp:106] Iteration 291100, lr = 0.000903125
I0630 17:35:37.749711 29777 solver.cpp:290] Iteration 291200 (6.2369 iter/s, 16.0336s/100 iter), loss = 1.02381
I0630 17:35:37.749795 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 17:35:37.749804 29777 sgd_solver.cpp:106] Iteration 291200, lr = 0.0009
I0630 17:35:53.855823 29777 solver.cpp:290] Iteration 291300 (6.20903 iter/s, 16.1056s/100 iter), loss = 1.27381
I0630 17:35:53.855845 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 17:35:53.855854 29777 sgd_solver.cpp:106] Iteration 291300, lr = 0.000896875
I0630 17:36:09.876195 29777 solver.cpp:290] Iteration 291400 (6.24223 iter/s, 16.0199s/100 iter), loss = 1.61905
I0630 17:36:09.876268 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 17:36:09.876278 29777 sgd_solver.cpp:106] Iteration 291400, lr = 0.00089375
I0630 17:36:26.005679 29777 solver.cpp:290] Iteration 291500 (6.20003 iter/s, 16.129s/100 iter), loss = 0.928571
I0630 17:36:26.005724 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 17:36:26.005739 29777 sgd_solver.cpp:106] Iteration 291500, lr = 0.000890625
I0630 17:36:42.057188 29777 solver.cpp:290] Iteration 291600 (6.23013 iter/s, 16.051s/100 iter), loss = 0.845238
I0630 17:36:42.057283 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 17:36:42.057294 29777 sgd_solver.cpp:106] Iteration 291600, lr = 0.0008875
I0630 17:36:58.154192 29777 solver.cpp:290] Iteration 291700 (6.21255 iter/s, 16.0965s/100 iter), loss = 1.09524
I0630 17:36:58.154214 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 17:36:58.154222 29777 sgd_solver.cpp:106] Iteration 291700, lr = 0.000884375
I0630 17:37:14.189749 29777 solver.cpp:290] Iteration 291800 (6.23632 iter/s, 16.0351s/100 iter), loss = 1.30952
I0630 17:37:14.189846 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 17:37:14.189853 29777 sgd_solver.cpp:106] Iteration 291800, lr = 0.00088125
I0630 17:37:30.289474 29777 solver.cpp:290] Iteration 291900 (6.2115 iter/s, 16.0992s/100 iter), loss = 1.11905
I0630 17:37:30.289495 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 17:37:30.289502 29777 sgd_solver.cpp:106] Iteration 291900, lr = 0.000878125
I0630 17:37:46.207043 29777 solver.cpp:354] Sparsity after update:
I0630 17:37:46.208864 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:37:46.208881 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:37:46.208901 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:37:46.208909 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:37:46.208917 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:37:46.208925 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:37:46.208935 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:37:46.208942 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:37:46.208950 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:37:46.208961 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:37:46.208969 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:37:46.208977 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:37:46.208986 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:37:46.209197 29777 solver.cpp:471] Iteration 292000, Testing net (#0)
I0630 17:38:22.796255 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 17:38:56.727989 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5604
I0630 17:38:56.728104 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.795281
I0630 17:38:56.728117 29777 solver.cpp:544]     Test net output #2: loss = 1.55776 (* 1 = 1.55776 loss)
I0630 17:38:56.910038 29777 solver.cpp:290] Iteration 292000 (1.15449 iter/s, 86.6181s/100 iter), loss = 1.08333
I0630 17:38:56.910066 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 17:38:56.910075 29777 sgd_solver.cpp:106] Iteration 292000, lr = 0.000875
I0630 17:39:13.100045 29777 solver.cpp:290] Iteration 292100 (6.17683 iter/s, 16.1895s/100 iter), loss = 1.08333
I0630 17:39:13.100083 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 17:39:13.100102 29777 sgd_solver.cpp:106] Iteration 292100, lr = 0.000871875
I0630 17:39:29.258440 29777 solver.cpp:290] Iteration 292200 (6.18892 iter/s, 16.1579s/100 iter), loss = 1.32143
I0630 17:39:29.258492 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 17:39:29.258499 29777 sgd_solver.cpp:106] Iteration 292200, lr = 0.00086875
I0630 17:39:45.360733 29777 solver.cpp:290] Iteration 292300 (6.21049 iter/s, 16.1018s/100 iter), loss = 1.21429
I0630 17:39:45.360756 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 17:39:45.360764 29777 sgd_solver.cpp:106] Iteration 292300, lr = 0.000865625
I0630 17:40:01.384099 29777 solver.cpp:290] Iteration 292400 (6.24107 iter/s, 16.0229s/100 iter), loss = 1.45238
I0630 17:40:01.384459 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 17:40:01.384472 29777 sgd_solver.cpp:106] Iteration 292400, lr = 0.0008625
I0630 17:40:17.411423 29777 solver.cpp:290] Iteration 292500 (6.23966 iter/s, 16.0265s/100 iter), loss = 1.22619
I0630 17:40:17.411447 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 17:40:17.411453 29777 sgd_solver.cpp:106] Iteration 292500, lr = 0.000859375
I0630 17:40:33.862148 29777 solver.cpp:290] Iteration 292600 (6.07894 iter/s, 16.4502s/100 iter), loss = 0.845238
I0630 17:40:33.862242 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 17:40:33.862251 29777 sgd_solver.cpp:106] Iteration 292600, lr = 0.00085625
I0630 17:40:50.043678 29777 solver.cpp:290] Iteration 292700 (6.18009 iter/s, 16.181s/100 iter), loss = 1.15476
I0630 17:40:50.043701 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 17:40:50.043709 29777 sgd_solver.cpp:106] Iteration 292700, lr = 0.000853125
I0630 17:41:06.107460 29777 solver.cpp:290] Iteration 292800 (6.22537 iter/s, 16.0633s/100 iter), loss = 1.28571
I0630 17:41:06.107856 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 17:41:06.107866 29777 sgd_solver.cpp:106] Iteration 292800, lr = 0.00085
I0630 17:41:22.418046 29777 solver.cpp:290] Iteration 292900 (6.13131 iter/s, 16.3097s/100 iter), loss = 1.14286
I0630 17:41:22.418079 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 17:41:22.418102 29777 sgd_solver.cpp:106] Iteration 292900, lr = 0.000846875
I0630 17:41:38.480417 29777 solver.cpp:354] Sparsity after update:
I0630 17:41:38.504784 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:41:38.504855 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:41:38.504896 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:41:38.504915 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:41:38.504935 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:41:38.504952 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:41:38.504971 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:41:38.504987 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:41:38.505005 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:41:38.505023 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:41:38.505039 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:41:38.505059 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:41:38.505079 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:41:38.657433 29777 solver.cpp:290] Iteration 293000 (6.15805 iter/s, 16.2389s/100 iter), loss = 1.03571
I0630 17:41:38.657655 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 17:41:38.657795 29777 sgd_solver.cpp:106] Iteration 293000, lr = 0.00084375
I0630 17:41:54.941942 29777 solver.cpp:290] Iteration 293100 (6.14106 iter/s, 16.2838s/100 iter), loss = 1.13095
I0630 17:41:54.941967 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 17:41:54.941972 29777 sgd_solver.cpp:106] Iteration 293100, lr = 0.000840625
I0630 17:42:10.985244 29777 solver.cpp:290] Iteration 293200 (6.23331 iter/s, 16.0428s/100 iter), loss = 1.29762
I0630 17:42:10.986250 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 17:42:10.986260 29777 sgd_solver.cpp:106] Iteration 293200, lr = 0.0008375
I0630 17:42:27.048847 29777 solver.cpp:290] Iteration 293300 (6.22582 iter/s, 16.0621s/100 iter), loss = 1
I0630 17:42:27.048877 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 17:42:27.048887 29777 sgd_solver.cpp:106] Iteration 293300, lr = 0.000834375
I0630 17:42:43.273017 29777 solver.cpp:290] Iteration 293400 (6.16383 iter/s, 16.2237s/100 iter), loss = 1.02381
I0630 17:42:43.273130 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 17:42:43.273140 29777 sgd_solver.cpp:106] Iteration 293400, lr = 0.00083125
I0630 17:42:59.303803 29777 solver.cpp:290] Iteration 293500 (6.23821 iter/s, 16.0302s/100 iter), loss = 1.2381
I0630 17:42:59.303827 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 17:42:59.303833 29777 sgd_solver.cpp:106] Iteration 293500, lr = 0.000828125
I0630 17:43:15.522938 29777 solver.cpp:290] Iteration 293600 (6.16574 iter/s, 16.2186s/100 iter), loss = 0.833333
I0630 17:43:15.523072 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 17:43:15.523100 29777 sgd_solver.cpp:106] Iteration 293600, lr = 0.000825
I0630 17:43:31.708559 29777 solver.cpp:290] Iteration 293700 (6.17854 iter/s, 16.185s/100 iter), loss = 1.27381
I0630 17:43:31.708586 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 17:43:31.708600 29777 sgd_solver.cpp:106] Iteration 293700, lr = 0.000821875
I0630 17:43:47.748509 29777 solver.cpp:290] Iteration 293800 (6.23462 iter/s, 16.0395s/100 iter), loss = 1.14286
I0630 17:43:47.748611 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 17:43:47.748625 29777 sgd_solver.cpp:106] Iteration 293800, lr = 0.00081875
I0630 17:44:03.890223 29777 solver.cpp:290] Iteration 293900 (6.19534 iter/s, 16.1412s/100 iter), loss = 1.29762
I0630 17:44:03.890250 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 17:44:03.890259 29777 sgd_solver.cpp:106] Iteration 293900, lr = 0.000815625
I0630 17:44:19.750598 29777 solver.cpp:354] Sparsity after update:
I0630 17:44:19.752055 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:44:19.752063 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:44:19.752070 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:44:19.752073 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:44:19.752074 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:44:19.752076 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:44:19.752079 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:44:19.752080 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:44:19.752082 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:44:19.752084 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:44:19.752086 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:44:19.752089 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:44:19.752090 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:44:19.752179 29777 solver.cpp:471] Iteration 294000, Testing net (#0)
I0630 17:44:51.188599 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 17:45:21.425086 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56114
I0630 17:45:21.425161 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.796082
I0630 17:45:21.425170 29777 solver.cpp:544]     Test net output #2: loss = 1.55782 (* 1 = 1.55782 loss)
I0630 17:45:21.661008 29777 solver.cpp:290] Iteration 294000 (1.28587 iter/s, 77.7686s/100 iter), loss = 0.880952
I0630 17:45:21.661048 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 17:45:21.661058 29777 sgd_solver.cpp:106] Iteration 294000, lr = 0.0008125
I0630 17:45:37.678767 29777 solver.cpp:290] Iteration 294100 (6.24326 iter/s, 16.0173s/100 iter), loss = 1.14286
I0630 17:45:37.678791 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 17:45:37.678798 29777 sgd_solver.cpp:106] Iteration 294100, lr = 0.000809375
I0630 17:45:53.835072 29777 solver.cpp:290] Iteration 294200 (6.18972 iter/s, 16.1558s/100 iter), loss = 1.19048
I0630 17:45:53.835115 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 17:45:53.835124 29777 sgd_solver.cpp:106] Iteration 294200, lr = 0.00080625
I0630 17:46:09.881464 29777 solver.cpp:290] Iteration 294300 (6.23212 iter/s, 16.0459s/100 iter), loss = 1.16667
I0630 17:46:09.881487 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 17:46:09.881494 29777 sgd_solver.cpp:106] Iteration 294300, lr = 0.000803125
I0630 17:46:25.969101 29777 solver.cpp:290] Iteration 294400 (6.21614 iter/s, 16.0872s/100 iter), loss = 1.41667
I0630 17:46:25.969183 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 17:46:25.969192 29777 sgd_solver.cpp:106] Iteration 294400, lr = 0.0008
I0630 17:46:42.215883 29777 solver.cpp:290] Iteration 294500 (6.15527 iter/s, 16.2462s/100 iter), loss = 1.13095
I0630 17:46:42.215910 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 17:46:42.215919 29777 sgd_solver.cpp:106] Iteration 294500, lr = 0.000796875
I0630 17:46:58.233736 29777 solver.cpp:290] Iteration 294600 (6.24322 iter/s, 16.0174s/100 iter), loss = 1.7381
I0630 17:46:58.233841 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 17:46:58.233851 29777 sgd_solver.cpp:106] Iteration 294600, lr = 0.00079375
I0630 17:47:14.308810 29777 solver.cpp:290] Iteration 294700 (6.22102 iter/s, 16.0745s/100 iter), loss = 1.08333
I0630 17:47:14.308835 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 17:47:14.308841 29777 sgd_solver.cpp:106] Iteration 294700, lr = 0.000790625
I0630 17:47:30.398787 29777 solver.cpp:290] Iteration 294800 (6.21523 iter/s, 16.0895s/100 iter), loss = 1.13095
I0630 17:47:30.398841 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 17:47:30.398852 29777 sgd_solver.cpp:106] Iteration 294800, lr = 0.0007875
I0630 17:47:46.399969 29777 solver.cpp:290] Iteration 294900 (6.24973 iter/s, 16.0007s/100 iter), loss = 1.2619
I0630 17:47:46.399992 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 17:47:46.399999 29777 sgd_solver.cpp:106] Iteration 294900, lr = 0.000784375
I0630 17:48:02.863251 29777 solver.cpp:354] Sparsity after update:
I0630 17:48:02.883680 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:48:02.883703 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:48:02.883715 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:48:02.883718 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:48:02.883723 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:48:02.883725 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:48:02.883730 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:48:02.883733 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:48:02.883736 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:48:02.883740 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:48:02.883744 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:48:02.883747 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:48:02.883750 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:48:03.040372 29777 solver.cpp:290] Iteration 295000 (6.00965 iter/s, 16.6399s/100 iter), loss = 1.29762
I0630 17:48:03.040407 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 17:48:03.040416 29777 sgd_solver.cpp:106] Iteration 295000, lr = 0.00078125
I0630 17:48:19.116148 29777 solver.cpp:290] Iteration 295100 (6.22073 iter/s, 16.0753s/100 iter), loss = 1.09524
I0630 17:48:19.116173 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 17:48:19.116183 29777 sgd_solver.cpp:106] Iteration 295100, lr = 0.000778125
I0630 17:48:35.197944 29777 solver.cpp:290] Iteration 295200 (6.21839 iter/s, 16.0813s/100 iter), loss = 1.36905
I0630 17:48:35.198036 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 17:48:35.198047 29777 sgd_solver.cpp:106] Iteration 295200, lr = 0.000775
I0630 17:48:51.217810 29777 solver.cpp:290] Iteration 295300 (6.24246 iter/s, 16.0193s/100 iter), loss = 1
I0630 17:48:51.217833 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 17:48:51.217840 29777 sgd_solver.cpp:106] Iteration 295300, lr = 0.000771875
I0630 17:49:07.250471 29777 solver.cpp:290] Iteration 295400 (6.23745 iter/s, 16.0322s/100 iter), loss = 1.10714
I0630 17:49:07.250571 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 17:49:07.250581 29777 sgd_solver.cpp:106] Iteration 295400, lr = 0.00076875
I0630 17:49:23.350294 29777 solver.cpp:290] Iteration 295500 (6.21146 iter/s, 16.0993s/100 iter), loss = 1.19048
I0630 17:49:23.350317 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 17:49:23.350323 29777 sgd_solver.cpp:106] Iteration 295500, lr = 0.000765625
I0630 17:49:39.367588 29777 solver.cpp:290] Iteration 295600 (6.24343 iter/s, 16.0168s/100 iter), loss = 1.11905
I0630 17:49:39.367682 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 17:49:39.367693 29777 sgd_solver.cpp:106] Iteration 295600, lr = 0.0007625
I0630 17:49:55.320020 29777 solver.cpp:290] Iteration 295700 (6.26885 iter/s, 15.9519s/100 iter), loss = 1.35714
I0630 17:49:55.320045 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 17:49:55.320053 29777 sgd_solver.cpp:106] Iteration 295700, lr = 0.000759375
I0630 17:50:11.360900 29777 solver.cpp:290] Iteration 295800 (6.23425 iter/s, 16.0404s/100 iter), loss = 1.4881
I0630 17:50:11.360986 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 17:50:11.360996 29777 sgd_solver.cpp:106] Iteration 295800, lr = 0.00075625
I0630 17:50:27.420449 29777 solver.cpp:290] Iteration 295900 (6.22703 iter/s, 16.059s/100 iter), loss = 1.39286
I0630 17:50:27.420473 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 17:50:27.420482 29777 sgd_solver.cpp:106] Iteration 295900, lr = 0.000753125
I0630 17:50:43.332746 29777 solver.cpp:354] Sparsity after update:
I0630 17:50:43.334342 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:50:43.334350 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:50:43.334357 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:50:43.334359 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:50:43.334362 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:50:43.334364 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:50:43.334367 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:50:43.334367 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:50:43.334369 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:50:43.334372 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:50:43.334373 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:50:43.334375 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:50:43.334378 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:50:43.334468 29777 solver.cpp:471] Iteration 296000, Testing net (#0)
I0630 17:51:12.999989 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 17:51:44.202541 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56016
I0630 17:51:44.202635 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.794501
I0630 17:51:44.202657 29777 solver.cpp:544]     Test net output #2: loss = 1.55962 (* 1 = 1.55962 loss)
I0630 17:51:44.453775 29777 solver.cpp:290] Iteration 296000 (1.29818 iter/s, 77.0312s/100 iter), loss = 1.15476
I0630 17:51:44.453821 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 17:51:44.453840 29777 sgd_solver.cpp:106] Iteration 296000, lr = 0.00075
I0630 17:52:00.394922 29777 solver.cpp:290] Iteration 296100 (6.27327 iter/s, 15.9407s/100 iter), loss = 1.59524
I0630 17:52:00.394946 29777 solver.cpp:309]     Train net output #0: loss = 1.83333 (* 1 = 1.83333 loss)
I0630 17:52:00.394953 29777 sgd_solver.cpp:106] Iteration 296100, lr = 0.000746875
I0630 17:52:16.388047 29777 solver.cpp:290] Iteration 296200 (6.25287 iter/s, 15.9927s/100 iter), loss = 1.29762
I0630 17:52:16.388118 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 17:52:16.388125 29777 sgd_solver.cpp:106] Iteration 296200, lr = 0.00074375
I0630 17:52:32.501762 29777 solver.cpp:290] Iteration 296300 (6.20609 iter/s, 16.1132s/100 iter), loss = 1.03571
I0630 17:52:32.501788 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 17:52:32.501797 29777 sgd_solver.cpp:106] Iteration 296300, lr = 0.000740625
I0630 17:52:48.566247 29777 solver.cpp:290] Iteration 296400 (6.22509 iter/s, 16.064s/100 iter), loss = 1.44048
I0630 17:52:48.566320 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 17:52:48.566329 29777 sgd_solver.cpp:106] Iteration 296400, lr = 0.0007375
I0630 17:53:04.567050 29777 solver.cpp:290] Iteration 296500 (6.24989 iter/s, 16.0003s/100 iter), loss = 1.33333
I0630 17:53:04.567078 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 17:53:04.567087 29777 sgd_solver.cpp:106] Iteration 296500, lr = 0.000734375
I0630 17:53:20.614104 29777 solver.cpp:290] Iteration 296600 (6.23186 iter/s, 16.0466s/100 iter), loss = 1.46429
I0630 17:53:20.614195 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 17:53:20.614203 29777 sgd_solver.cpp:106] Iteration 296600, lr = 0.00073125
I0630 17:53:36.910050 29777 solver.cpp:290] Iteration 296700 (6.1367 iter/s, 16.2954s/100 iter), loss = 1.32143
I0630 17:53:36.910079 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 17:53:36.910089 29777 sgd_solver.cpp:106] Iteration 296700, lr = 0.000728125
I0630 17:53:53.104265 29777 solver.cpp:290] Iteration 296800 (6.17523 iter/s, 16.1937s/100 iter), loss = 1.0119
I0630 17:53:53.104394 29777 solver.cpp:309]     Train net output #0: loss = 0.547619 (* 1 = 0.547619 loss)
I0630 17:53:53.104434 29777 sgd_solver.cpp:106] Iteration 296800, lr = 0.000725
I0630 17:54:09.187799 29777 solver.cpp:290] Iteration 296900 (6.21776 iter/s, 16.083s/100 iter), loss = 1.42857
I0630 17:54:09.187822 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 17:54:09.187829 29777 sgd_solver.cpp:106] Iteration 296900, lr = 0.000721875
I0630 17:54:25.140435 29777 solver.cpp:354] Sparsity after update:
I0630 17:54:25.160660 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:54:25.160686 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:54:25.160718 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:54:25.160733 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:54:25.160748 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:54:25.160760 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:54:25.160779 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:54:25.160790 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:54:25.160806 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:54:25.160823 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:54:25.160836 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:54:25.160853 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:54:25.160862 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:54:25.331431 29777 solver.cpp:290] Iteration 297000 (6.19457 iter/s, 16.1432s/100 iter), loss = 1.09524
I0630 17:54:25.331454 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 17:54:25.331460 29777 sgd_solver.cpp:106] Iteration 297000, lr = 0.00071875
I0630 17:54:41.487928 29777 solver.cpp:290] Iteration 297100 (6.18964 iter/s, 16.156s/100 iter), loss = 1.46429
I0630 17:54:41.487952 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 17:54:41.487958 29777 sgd_solver.cpp:106] Iteration 297100, lr = 0.000715625
I0630 17:54:57.692994 29777 solver.cpp:290] Iteration 297200 (6.17109 iter/s, 16.2046s/100 iter), loss = 1.35714
I0630 17:54:57.693086 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 17:54:57.693097 29777 sgd_solver.cpp:106] Iteration 297200, lr = 0.0007125
I0630 17:55:13.752595 29777 solver.cpp:290] Iteration 297300 (6.22701 iter/s, 16.0591s/100 iter), loss = 1.32143
I0630 17:55:13.752616 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 17:55:13.752624 29777 sgd_solver.cpp:106] Iteration 297300, lr = 0.000709375
I0630 17:55:29.854094 29777 solver.cpp:290] Iteration 297400 (6.21078 iter/s, 16.101s/100 iter), loss = 1
I0630 17:55:29.854182 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 17:55:29.854192 29777 sgd_solver.cpp:106] Iteration 297400, lr = 0.00070625
I0630 17:55:45.993770 29777 solver.cpp:290] Iteration 297500 (6.19612 iter/s, 16.1391s/100 iter), loss = 1.13095
I0630 17:55:45.993795 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 17:55:45.993804 29777 sgd_solver.cpp:106] Iteration 297500, lr = 0.000703125
I0630 17:56:02.211547 29777 solver.cpp:290] Iteration 297600 (6.16626 iter/s, 16.2173s/100 iter), loss = 1.32143
I0630 17:56:02.211658 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 17:56:02.211688 29777 sgd_solver.cpp:106] Iteration 297600, lr = 0.0007
I0630 17:56:18.454188 29777 solver.cpp:290] Iteration 297700 (6.15684 iter/s, 16.2421s/100 iter), loss = 0.97619
I0630 17:56:18.454306 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 17:56:18.454320 29777 sgd_solver.cpp:106] Iteration 297700, lr = 0.000696875
I0630 17:56:34.701936 29777 solver.cpp:290] Iteration 297800 (6.15491 iter/s, 16.2472s/100 iter), loss = 0.940476
I0630 17:56:34.702033 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 17:56:34.702044 29777 sgd_solver.cpp:106] Iteration 297800, lr = 0.00069375
I0630 17:56:50.857287 29777 solver.cpp:290] Iteration 297900 (6.19011 iter/s, 16.1548s/100 iter), loss = 1.39286
I0630 17:56:50.857316 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 17:56:50.857331 29777 sgd_solver.cpp:106] Iteration 297900, lr = 0.000690625
I0630 17:57:06.912855 29777 solver.cpp:354] Sparsity after update:
I0630 17:57:06.914461 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 17:57:06.914469 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 17:57:06.914475 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 17:57:06.914479 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 17:57:06.914480 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 17:57:06.914482 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 17:57:06.914484 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 17:57:06.914486 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 17:57:06.914489 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 17:57:06.914490 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 17:57:06.914492 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 17:57:06.914494 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 17:57:06.914495 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 17:57:06.914582 29777 solver.cpp:471] Iteration 298000, Testing net (#0)
I0630 17:57:39.548760 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 17:58:08.131953 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56106
I0630 17:58:08.131978 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.795442
I0630 17:58:08.131986 29777 solver.cpp:544]     Test net output #2: loss = 1.55388 (* 1 = 1.55388 loss)
I0630 17:58:08.307731 29777 solver.cpp:290] Iteration 298000 (1.29118 iter/s, 77.4483s/100 iter), loss = 1.44048
I0630 17:58:08.307752 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 17:58:08.307760 29777 sgd_solver.cpp:106] Iteration 298000, lr = 0.0006875
I0630 17:58:24.480840 29777 solver.cpp:290] Iteration 298100 (6.18328 iter/s, 16.1726s/100 iter), loss = 1.63095
I0630 17:58:24.480969 29777 solver.cpp:309]     Train net output #0: loss = 1.71429 (* 1 = 1.71429 loss)
I0630 17:58:24.481003 29777 sgd_solver.cpp:106] Iteration 298100, lr = 0.000684375
I0630 17:58:40.586434 29777 solver.cpp:290] Iteration 298200 (6.20924 iter/s, 16.105s/100 iter), loss = 0.964285
I0630 17:58:40.586459 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 17:58:40.586467 29777 sgd_solver.cpp:106] Iteration 298200, lr = 0.00068125
I0630 17:58:56.583739 29777 solver.cpp:290] Iteration 298300 (6.25124 iter/s, 15.9968s/100 iter), loss = 1.17857
I0630 17:58:56.583847 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 17:58:56.583869 29777 sgd_solver.cpp:106] Iteration 298300, lr = 0.000678125
I0630 17:59:12.736435 29777 solver.cpp:290] Iteration 298400 (6.19113 iter/s, 16.1521s/100 iter), loss = 1.14286
I0630 17:59:12.736460 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 17:59:12.736466 29777 sgd_solver.cpp:106] Iteration 298400, lr = 0.000675
I0630 17:59:28.926200 29777 solver.cpp:290] Iteration 298500 (6.17693 iter/s, 16.1893s/100 iter), loss = 1.20238
I0630 17:59:28.926303 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 17:59:28.926327 29777 sgd_solver.cpp:106] Iteration 298500, lr = 0.000671875
I0630 17:59:44.999788 29777 solver.cpp:290] Iteration 298600 (6.22159 iter/s, 16.0731s/100 iter), loss = 0.928571
I0630 17:59:44.999815 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 17:59:44.999825 29777 sgd_solver.cpp:106] Iteration 298600, lr = 0.00066875
I0630 18:00:01.158108 29777 solver.cpp:290] Iteration 298700 (6.18894 iter/s, 16.1579s/100 iter), loss = 1.41667
I0630 18:00:01.158210 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 18:00:01.158219 29777 sgd_solver.cpp:106] Iteration 298700, lr = 0.000665625
I0630 18:00:17.073683 29777 solver.cpp:290] Iteration 298800 (6.28337 iter/s, 15.915s/100 iter), loss = 1.16667
I0630 18:00:17.073704 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 18:00:17.073712 29777 sgd_solver.cpp:106] Iteration 298800, lr = 0.0006625
I0630 18:00:33.385638 29777 solver.cpp:290] Iteration 298900 (6.13066 iter/s, 16.3115s/100 iter), loss = 1.14286
I0630 18:00:33.385803 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 18:00:33.389030 29777 sgd_solver.cpp:106] Iteration 298900, lr = 0.000659375
I0630 18:00:49.424887 29777 solver.cpp:354] Sparsity after update:
I0630 18:00:49.445271 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:00:49.445289 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:00:49.445300 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:00:49.445303 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:00:49.445307 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:00:49.445310 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:00:49.445313 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:00:49.445317 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:00:49.445319 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:00:49.445322 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:00:49.445325 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:00:49.445329 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:00:49.445333 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:00:49.602418 29777 solver.cpp:290] Iteration 299000 (6.16791 iter/s, 16.2129s/100 iter), loss = 1.13095
I0630 18:00:49.602469 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 18:00:49.602491 29777 sgd_solver.cpp:106] Iteration 299000, lr = 0.00065625
I0630 18:01:05.787582 29777 solver.cpp:290] Iteration 299100 (6.17869 iter/s, 16.1847s/100 iter), loss = 1.02381
I0630 18:01:05.787686 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 18:01:05.787695 29777 sgd_solver.cpp:106] Iteration 299100, lr = 0.000653125
I0630 18:01:21.848136 29777 solver.cpp:290] Iteration 299200 (6.22665 iter/s, 16.06s/100 iter), loss = 0.869047
I0630 18:01:21.848163 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 18:01:21.848173 29777 sgd_solver.cpp:106] Iteration 299200, lr = 0.00065
I0630 18:01:37.865962 29777 solver.cpp:290] Iteration 299300 (6.24323 iter/s, 16.0174s/100 iter), loss = 0.988095
I0630 18:01:37.866072 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 18:01:37.866082 29777 sgd_solver.cpp:106] Iteration 299300, lr = 0.000646875
I0630 18:01:54.004521 29777 solver.cpp:290] Iteration 299400 (6.19655 iter/s, 16.138s/100 iter), loss = 1.67857
I0630 18:01:54.004544 29777 solver.cpp:309]     Train net output #0: loss = 2.04762 (* 1 = 2.04762 loss)
I0630 18:01:54.004551 29777 sgd_solver.cpp:106] Iteration 299400, lr = 0.00064375
I0630 18:02:10.068337 29777 solver.cpp:290] Iteration 299500 (6.22535 iter/s, 16.0633s/100 iter), loss = 1.17857
I0630 18:02:10.068476 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 18:02:10.068518 29777 sgd_solver.cpp:106] Iteration 299500, lr = 0.000640625
I0630 18:02:26.299629 29777 solver.cpp:290] Iteration 299600 (6.16116 iter/s, 16.2307s/100 iter), loss = 1.03571
I0630 18:02:26.299652 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 18:02:26.299659 29777 sgd_solver.cpp:106] Iteration 299600, lr = 0.0006375
I0630 18:02:42.334800 29777 solver.cpp:290] Iteration 299700 (6.23647 iter/s, 16.0347s/100 iter), loss = 1.34524
I0630 18:02:42.334908 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 18:02:42.334918 29777 sgd_solver.cpp:106] Iteration 299700, lr = 0.000634375
I0630 18:02:58.477773 29777 solver.cpp:290] Iteration 299800 (6.19486 iter/s, 16.1424s/100 iter), loss = 1.08333
I0630 18:02:58.477797 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 18:02:58.477803 29777 sgd_solver.cpp:106] Iteration 299800, lr = 0.00063125
I0630 18:03:14.441915 29777 solver.cpp:290] Iteration 299900 (6.26422 iter/s, 15.9637s/100 iter), loss = 1.60714
I0630 18:03:14.442029 29777 solver.cpp:309]     Train net output #0: loss = 1.80952 (* 1 = 1.80952 loss)
I0630 18:03:14.442039 29777 sgd_solver.cpp:106] Iteration 299900, lr = 0.000628125
I0630 18:03:30.616818 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_300000.caffemodel
I0630 18:03:30.636335 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_300000.solverstate
I0630 18:03:30.645140 29777 solver.cpp:354] Sparsity after update:
I0630 18:03:30.646103 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:03:30.646113 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:03:30.646124 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:03:30.646128 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:03:30.646132 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:03:30.646137 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:03:30.646142 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:03:30.646147 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:03:30.646152 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:03:30.646155 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:03:30.646159 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:03:30.646163 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:03:30.646167 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:03:30.646268 29777 solver.cpp:471] Iteration 300000, Testing net (#0)
I0630 18:04:02.522979 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 18:04:32.709554 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.561739
I0630 18:04:32.709659 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.795701
I0630 18:04:32.709668 29777 solver.cpp:544]     Test net output #2: loss = 1.55358 (* 1 = 1.55358 loss)
I0630 18:04:32.880309 29777 solver.cpp:290] Iteration 300000 (1.27492 iter/s, 78.4362s/100 iter), loss = 0.916666
I0630 18:04:32.880333 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 18:04:32.880339 29777 sgd_solver.cpp:106] Iteration 300000, lr = 0.000625
I0630 18:04:48.943444 29777 solver.cpp:290] Iteration 300100 (6.22562 iter/s, 16.0627s/100 iter), loss = 1.28571
I0630 18:04:48.943516 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 18:04:48.943536 29777 sgd_solver.cpp:106] Iteration 300100, lr = 0.000621875
I0630 18:05:04.768913 29777 solver.cpp:290] Iteration 300200 (6.31913 iter/s, 15.825s/100 iter), loss = 1.29762
I0630 18:05:04.768995 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 18:05:04.769007 29777 sgd_solver.cpp:106] Iteration 300200, lr = 0.00061875
I0630 18:05:21.011246 29777 solver.cpp:290] Iteration 300300 (6.15695 iter/s, 16.2418s/100 iter), loss = 1.05952
I0630 18:05:21.011287 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 18:05:21.011298 29777 sgd_solver.cpp:106] Iteration 300300, lr = 0.000615625
I0630 18:05:37.127701 29777 solver.cpp:290] Iteration 300400 (6.20502 iter/s, 16.116s/100 iter), loss = 1.4881
I0630 18:05:37.127801 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 18:05:37.127812 29777 sgd_solver.cpp:106] Iteration 300400, lr = 0.0006125
I0630 18:05:53.237344 29777 solver.cpp:290] Iteration 300500 (6.20767 iter/s, 16.1091s/100 iter), loss = 1.41667
I0630 18:05:53.237365 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 18:05:53.237371 29777 sgd_solver.cpp:106] Iteration 300500, lr = 0.000609375
I0630 18:06:09.324841 29777 solver.cpp:290] Iteration 300600 (6.21619 iter/s, 16.087s/100 iter), loss = 1.33333
I0630 18:06:09.324947 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 18:06:09.324959 29777 sgd_solver.cpp:106] Iteration 300600, lr = 0.00060625
I0630 18:06:25.463397 29777 solver.cpp:290] Iteration 300700 (6.19655 iter/s, 16.138s/100 iter), loss = 1
I0630 18:06:25.463425 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 18:06:25.463434 29777 sgd_solver.cpp:106] Iteration 300700, lr = 0.000603125
I0630 18:06:41.492734 29777 solver.cpp:290] Iteration 300800 (6.23874 iter/s, 16.0289s/100 iter), loss = 1.03571
I0630 18:06:41.492807 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 18:06:41.492815 29777 sgd_solver.cpp:106] Iteration 300800, lr = 0.0006
I0630 18:06:57.551543 29777 solver.cpp:290] Iteration 300900 (6.22731 iter/s, 16.0583s/100 iter), loss = 1.42857
I0630 18:06:57.551565 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 18:06:57.551573 29777 sgd_solver.cpp:106] Iteration 300900, lr = 0.000596875
I0630 18:07:13.468242 29777 solver.cpp:354] Sparsity after update:
I0630 18:07:13.488598 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:07:13.488613 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:07:13.488625 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:07:13.488627 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:07:13.488639 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:07:13.488649 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:07:13.488657 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:07:13.488667 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:07:13.488674 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:07:13.488690 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:07:13.488695 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:07:13.488704 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:07:13.488714 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:07:13.643486 29777 solver.cpp:290] Iteration 301000 (6.21447 iter/s, 16.0915s/100 iter), loss = 1.46429
I0630 18:07:13.643509 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 18:07:13.643518 29777 sgd_solver.cpp:106] Iteration 301000, lr = 0.00059375
I0630 18:07:29.725893 29777 solver.cpp:290] Iteration 301100 (6.21815 iter/s, 16.0819s/100 iter), loss = 1.03571
I0630 18:07:29.725915 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 18:07:29.725924 29777 sgd_solver.cpp:106] Iteration 301100, lr = 0.000590625
I0630 18:07:45.865913 29777 solver.cpp:290] Iteration 301200 (6.19596 iter/s, 16.1396s/100 iter), loss = 1.44048
I0630 18:07:45.865984 29777 solver.cpp:309]     Train net output #0: loss = 2.07143 (* 1 = 2.07143 loss)
I0630 18:07:45.865991 29777 sgd_solver.cpp:106] Iteration 301200, lr = 0.0005875
I0630 18:08:01.966498 29777 solver.cpp:290] Iteration 301300 (6.21115 iter/s, 16.1001s/100 iter), loss = 1.28571
I0630 18:08:01.966536 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 18:08:01.966547 29777 sgd_solver.cpp:106] Iteration 301300, lr = 0.000584375
I0630 18:08:18.017927 29777 solver.cpp:290] Iteration 301400 (6.23016 iter/s, 16.051s/100 iter), loss = 1.22619
I0630 18:08:18.017974 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 18:08:18.017982 29777 sgd_solver.cpp:106] Iteration 301400, lr = 0.00058125
I0630 18:08:34.174926 29777 solver.cpp:290] Iteration 301500 (6.18946 iter/s, 16.1565s/100 iter), loss = 1.33333
I0630 18:08:34.174969 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 18:08:34.174983 29777 sgd_solver.cpp:106] Iteration 301500, lr = 0.000578125
I0630 18:08:50.153005 29777 solver.cpp:290] Iteration 301600 (6.25876 iter/s, 15.9776s/100 iter), loss = 1.4881
I0630 18:08:50.153090 29777 solver.cpp:309]     Train net output #0: loss = 1.88095 (* 1 = 1.88095 loss)
I0630 18:08:50.153097 29777 sgd_solver.cpp:106] Iteration 301600, lr = 0.000575
I0630 18:09:06.286329 29777 solver.cpp:290] Iteration 301700 (6.19855 iter/s, 16.1328s/100 iter), loss = 1.35714
I0630 18:09:06.286384 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 18:09:06.286406 29777 sgd_solver.cpp:106] Iteration 301700, lr = 0.000571875
I0630 18:09:22.665072 29777 solver.cpp:290] Iteration 301800 (6.10566 iter/s, 16.3783s/100 iter), loss = 0.97619
I0630 18:09:22.665151 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 18:09:22.665163 29777 sgd_solver.cpp:106] Iteration 301800, lr = 0.00056875
I0630 18:09:38.856696 29777 solver.cpp:290] Iteration 301900 (6.17623 iter/s, 16.1911s/100 iter), loss = 1.28571
I0630 18:09:38.856720 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 18:09:38.856729 29777 sgd_solver.cpp:106] Iteration 301900, lr = 0.000565625
I0630 18:09:55.003743 29777 solver.cpp:354] Sparsity after update:
I0630 18:09:55.005182 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:09:55.005190 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:09:55.005198 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:09:55.005199 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:09:55.005201 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:09:55.005203 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:09:55.005205 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:09:55.005208 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:09:55.005209 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:09:55.005211 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:09:55.005213 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:09:55.005215 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:09:55.005218 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:09:55.005306 29777 solver.cpp:471] Iteration 302000, Testing net (#0)
I0630 18:10:30.919648 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 18:10:58.948318 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56196
I0630 18:10:58.948343 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.795541
I0630 18:10:58.948348 29777 solver.cpp:544]     Test net output #2: loss = 1.5534 (* 1 = 1.5534 loss)
I0630 18:10:59.126555 29777 solver.cpp:290] Iteration 302000 (1.24583 iter/s, 80.2677s/100 iter), loss = 1.59524
I0630 18:10:59.126579 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 18:10:59.126585 29777 sgd_solver.cpp:106] Iteration 302000, lr = 0.0005625
I0630 18:11:14.663869 29777 solver.cpp:290] Iteration 302100 (6.43631 iter/s, 15.5369s/100 iter), loss = 1.19048
I0630 18:11:14.663967 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 18:11:14.663980 29777 sgd_solver.cpp:106] Iteration 302100, lr = 0.000559375
I0630 18:11:30.987031 29777 solver.cpp:290] Iteration 302200 (6.12647 iter/s, 16.3226s/100 iter), loss = 1.25
I0630 18:11:30.987059 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 18:11:30.987066 29777 sgd_solver.cpp:106] Iteration 302200, lr = 0.00055625
I0630 18:11:47.141424 29777 solver.cpp:290] Iteration 302300 (6.19045 iter/s, 16.1539s/100 iter), loss = 1.35714
I0630 18:11:47.141517 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 18:11:47.141528 29777 sgd_solver.cpp:106] Iteration 302300, lr = 0.000553125
I0630 18:12:03.488590 29777 solver.cpp:290] Iteration 302400 (6.11747 iter/s, 16.3466s/100 iter), loss = 1.27381
I0630 18:12:03.488612 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 18:12:03.488620 29777 sgd_solver.cpp:106] Iteration 302400, lr = 0.00055
I0630 18:12:19.635521 29777 solver.cpp:290] Iteration 302500 (6.1933 iter/s, 16.1465s/100 iter), loss = 1.27381
I0630 18:12:19.635609 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 18:12:19.635620 29777 sgd_solver.cpp:106] Iteration 302500, lr = 0.000546875
I0630 18:12:35.929595 29777 solver.cpp:290] Iteration 302600 (6.1374 iter/s, 16.2935s/100 iter), loss = 1.61905
I0630 18:12:35.929617 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 18:12:35.929625 29777 sgd_solver.cpp:106] Iteration 302600, lr = 0.00054375
I0630 18:12:52.140811 29777 solver.cpp:290] Iteration 302700 (6.16875 iter/s, 16.2108s/100 iter), loss = 1.38095
I0630 18:12:52.140910 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 18:12:52.140921 29777 sgd_solver.cpp:106] Iteration 302700, lr = 0.000540625
I0630 18:13:08.292147 29777 solver.cpp:290] Iteration 302800 (6.19165 iter/s, 16.1508s/100 iter), loss = 1.07143
I0630 18:13:08.292212 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 18:13:08.292235 29777 sgd_solver.cpp:106] Iteration 302800, lr = 0.0005375
I0630 18:13:24.808022 29777 solver.cpp:290] Iteration 302900 (6.05497 iter/s, 16.5154s/100 iter), loss = 0.892857
I0630 18:13:24.808111 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 18:13:24.808122 29777 sgd_solver.cpp:106] Iteration 302900, lr = 0.000534375
I0630 18:13:40.790220 29777 solver.cpp:354] Sparsity after update:
I0630 18:13:40.810227 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:13:40.810253 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:13:40.810266 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:13:40.810269 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:13:40.810272 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:13:40.810277 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:13:40.810281 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:13:40.810284 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:13:40.810287 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:13:40.810292 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:13:40.810294 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:13:40.810298 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:13:40.810302 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:13:40.979362 29777 solver.cpp:290] Iteration 303000 (6.18398 iter/s, 16.1708s/100 iter), loss = 0.940476
I0630 18:13:40.979389 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 18:13:40.979398 29777 sgd_solver.cpp:106] Iteration 303000, lr = 0.00053125
I0630 18:13:57.098321 29777 solver.cpp:290] Iteration 303100 (6.20405 iter/s, 16.1185s/100 iter), loss = 0.833333
I0630 18:13:57.098439 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 18:13:57.098454 29777 sgd_solver.cpp:106] Iteration 303100, lr = 0.000528125
I0630 18:14:13.149806 29777 solver.cpp:290] Iteration 303200 (6.23017 iter/s, 16.0509s/100 iter), loss = 1.38095
I0630 18:14:13.149833 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 18:14:13.149842 29777 sgd_solver.cpp:106] Iteration 303200, lr = 0.000525
I0630 18:14:29.248299 29777 solver.cpp:290] Iteration 303300 (6.21194 iter/s, 16.098s/100 iter), loss = 1.38095
I0630 18:14:29.248416 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 18:14:29.248430 29777 sgd_solver.cpp:106] Iteration 303300, lr = 0.000521875
I0630 18:14:45.393438 29777 solver.cpp:290] Iteration 303400 (6.19403 iter/s, 16.1446s/100 iter), loss = 1.0119
I0630 18:14:45.393537 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 18:14:45.393565 29777 sgd_solver.cpp:106] Iteration 303400, lr = 0.00051875
I0630 18:15:01.618389 29777 solver.cpp:290] Iteration 303500 (6.16355 iter/s, 16.2244s/100 iter), loss = 1.2619
I0630 18:15:01.618500 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 18:15:01.618511 29777 sgd_solver.cpp:106] Iteration 303500, lr = 0.000515625
I0630 18:15:17.977466 29777 solver.cpp:290] Iteration 303600 (6.11302 iter/s, 16.3585s/100 iter), loss = 1.4881
I0630 18:15:17.977496 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 18:15:17.977505 29777 sgd_solver.cpp:106] Iteration 303600, lr = 0.0005125
I0630 18:15:34.091676 29777 solver.cpp:290] Iteration 303700 (6.20588 iter/s, 16.1137s/100 iter), loss = 1.2381
I0630 18:15:34.091747 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 18:15:34.091754 29777 sgd_solver.cpp:106] Iteration 303700, lr = 0.000509375
I0630 18:15:50.280081 29777 solver.cpp:290] Iteration 303800 (6.17745 iter/s, 16.1879s/100 iter), loss = 0.880952
I0630 18:15:50.280105 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 18:15:50.280112 29777 sgd_solver.cpp:106] Iteration 303800, lr = 0.00050625
I0630 18:16:06.452337 29777 solver.cpp:290] Iteration 303900 (6.18361 iter/s, 16.1718s/100 iter), loss = 1.46429
I0630 18:16:06.452425 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 18:16:06.452435 29777 sgd_solver.cpp:106] Iteration 303900, lr = 0.000503125
I0630 18:16:22.450608 29777 solver.cpp:354] Sparsity after update:
I0630 18:16:22.452040 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:16:22.452049 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:16:22.452056 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:16:22.452059 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:16:22.452061 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:16:22.452064 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:16:22.452065 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:16:22.452069 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:16:22.452070 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:16:22.452072 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:16:22.452075 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:16:22.452077 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:16:22.452080 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:16:22.452217 29777 solver.cpp:471] Iteration 304000, Testing net (#0)
I0630 18:16:53.876369 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 18:17:24.146639 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56212
I0630 18:17:24.146751 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.795982
I0630 18:17:24.146761 29777 solver.cpp:544]     Test net output #2: loss = 1.5516 (* 1 = 1.5516 loss)
I0630 18:17:24.321475 29777 solver.cpp:290] Iteration 304000 (1.28424 iter/s, 77.867s/100 iter), loss = 1.70238
I0630 18:17:24.321497 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 18:17:24.321504 29777 sgd_solver.cpp:106] Iteration 304000, lr = 0.0005
I0630 18:17:39.891906 29777 solver.cpp:290] Iteration 304100 (6.42262 iter/s, 15.57s/100 iter), loss = 1.28571
I0630 18:17:39.891970 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 18:17:39.891993 29777 sgd_solver.cpp:106] Iteration 304100, lr = 0.000496875
I0630 18:17:55.889930 29777 solver.cpp:290] Iteration 304200 (6.25097 iter/s, 15.9975s/100 iter), loss = 0.940476
I0630 18:17:55.890018 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 18:17:55.890027 29777 sgd_solver.cpp:106] Iteration 304200, lr = 0.00049375
I0630 18:18:12.031931 29777 solver.cpp:290] Iteration 304300 (6.19522 iter/s, 16.1415s/100 iter), loss = 1.14286
I0630 18:18:12.031954 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 18:18:12.031960 29777 sgd_solver.cpp:106] Iteration 304300, lr = 0.000490625
I0630 18:18:28.083312 29777 solver.cpp:290] Iteration 304400 (6.23017 iter/s, 16.0509s/100 iter), loss = 1.13095
I0630 18:18:28.083371 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 18:18:28.083379 29777 sgd_solver.cpp:106] Iteration 304400, lr = 0.0004875
I0630 18:18:44.124704 29777 solver.cpp:290] Iteration 304500 (6.23406 iter/s, 16.0409s/100 iter), loss = 1.15476
I0630 18:18:44.124727 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 18:18:44.124734 29777 sgd_solver.cpp:106] Iteration 304500, lr = 0.000484375
I0630 18:19:00.154285 29777 solver.cpp:290] Iteration 304600 (6.23864 iter/s, 16.0291s/100 iter), loss = 1.42857
I0630 18:19:00.154362 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 18:19:00.154372 29777 sgd_solver.cpp:106] Iteration 304600, lr = 0.00048125
I0630 18:19:16.239392 29777 solver.cpp:290] Iteration 304700 (6.21713 iter/s, 16.0846s/100 iter), loss = 1.02381
I0630 18:19:16.239434 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 18:19:16.239497 29777 sgd_solver.cpp:106] Iteration 304700, lr = 0.000478125
I0630 18:19:32.278779 29777 solver.cpp:290] Iteration 304800 (6.23484 iter/s, 16.0389s/100 iter), loss = 1.28571
I0630 18:19:32.278867 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 18:19:32.278877 29777 sgd_solver.cpp:106] Iteration 304800, lr = 0.000475
I0630 18:19:48.308703 29777 solver.cpp:290] Iteration 304900 (6.23853 iter/s, 16.0294s/100 iter), loss = 1.44048
I0630 18:19:48.308727 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 18:19:48.308732 29777 sgd_solver.cpp:106] Iteration 304900, lr = 0.000471875
I0630 18:20:04.204231 29777 solver.cpp:354] Sparsity after update:
I0630 18:20:04.224629 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:20:04.224647 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:20:04.224658 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:20:04.224661 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:20:04.224664 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:20:04.224673 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:20:04.224678 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:20:04.224683 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:20:04.224689 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:20:04.224694 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:20:04.224699 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:20:04.224704 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:20:04.224707 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:20:04.384378 29777 solver.cpp:290] Iteration 305000 (6.22076 iter/s, 16.0752s/100 iter), loss = 1.36905
I0630 18:20:04.384407 29777 solver.cpp:309]     Train net output #0: loss = 1.71429 (* 1 = 1.71429 loss)
I0630 18:20:04.384416 29777 sgd_solver.cpp:106] Iteration 305000, lr = 0.00046875
I0630 18:20:20.668160 29777 solver.cpp:290] Iteration 305100 (6.14126 iter/s, 16.2833s/100 iter), loss = 1.42857
I0630 18:20:20.668184 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 18:20:20.668193 29777 sgd_solver.cpp:106] Iteration 305100, lr = 0.000465625
I0630 18:20:36.987215 29777 solver.cpp:290] Iteration 305200 (6.12798 iter/s, 16.3186s/100 iter), loss = 1.5
I0630 18:20:36.987337 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 18:20:36.987361 29777 sgd_solver.cpp:106] Iteration 305200, lr = 0.0004625
I0630 18:20:53.256808 29777 solver.cpp:290] Iteration 305300 (6.14665 iter/s, 16.269s/100 iter), loss = 1.19048
I0630 18:20:53.256832 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 18:20:53.256839 29777 sgd_solver.cpp:106] Iteration 305300, lr = 0.000459375
I0630 18:21:09.304924 29777 solver.cpp:290] Iteration 305400 (6.23144 iter/s, 16.0477s/100 iter), loss = 1.35714
I0630 18:21:09.305042 29777 solver.cpp:309]     Train net output #0: loss = 1.78571 (* 1 = 1.78571 loss)
I0630 18:21:09.305052 29777 sgd_solver.cpp:106] Iteration 305400, lr = 0.00045625
I0630 18:21:25.421131 29777 solver.cpp:290] Iteration 305500 (6.20515 iter/s, 16.1157s/100 iter), loss = 1.63095
I0630 18:21:25.421154 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 18:21:25.421161 29777 sgd_solver.cpp:106] Iteration 305500, lr = 0.000453125
I0630 18:21:41.449990 29777 solver.cpp:290] Iteration 305600 (6.23893 iter/s, 16.0284s/100 iter), loss = 1.5
I0630 18:21:41.450093 29777 solver.cpp:309]     Train net output #0: loss = 1.71429 (* 1 = 1.71429 loss)
I0630 18:21:41.450103 29777 sgd_solver.cpp:106] Iteration 305600, lr = 0.00045
I0630 18:21:57.581367 29777 solver.cpp:290] Iteration 305700 (6.19931 iter/s, 16.1308s/100 iter), loss = 1.20238
I0630 18:21:57.581431 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 18:21:57.581451 29777 sgd_solver.cpp:106] Iteration 305700, lr = 0.000446875
I0630 18:22:13.687748 29777 solver.cpp:290] Iteration 305800 (6.20892 iter/s, 16.1059s/100 iter), loss = 1.32143
I0630 18:22:13.687860 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 18:22:13.687871 29777 sgd_solver.cpp:106] Iteration 305800, lr = 0.00044375
I0630 18:22:29.715507 29777 solver.cpp:290] Iteration 305900 (6.23939 iter/s, 16.0272s/100 iter), loss = 1.44048
I0630 18:22:29.715535 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 18:22:29.715544 29777 sgd_solver.cpp:106] Iteration 305900, lr = 0.000440625
I0630 18:22:45.688619 29777 solver.cpp:354] Sparsity after update:
I0630 18:22:45.690304 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:22:45.690312 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:22:45.690323 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:22:45.690328 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:22:45.690333 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:22:45.690338 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:22:45.690342 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:22:45.690346 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:22:45.690351 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:22:45.690354 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:22:45.690358 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:22:45.690362 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:22:45.690366 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:22:45.690472 29777 solver.cpp:471] Iteration 306000, Testing net (#0)
I0630 18:23:17.135666 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 18:23:46.182137 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.5617
I0630 18:23:46.182216 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.795901
I0630 18:23:46.182234 29777 solver.cpp:544]     Test net output #2: loss = 1.55208 (* 1 = 1.55208 loss)
I0630 18:23:46.404659 29777 solver.cpp:290] Iteration 306000 (1.304 iter/s, 76.687s/100 iter), loss = 1.2619
I0630 18:23:46.404682 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 18:23:46.404692 29777 sgd_solver.cpp:106] Iteration 306000, lr = 0.0004375
I0630 18:24:01.765732 29777 solver.cpp:290] Iteration 306100 (6.51015 iter/s, 15.3606s/100 iter), loss = 1.4881
I0630 18:24:01.765810 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 18:24:01.765820 29777 sgd_solver.cpp:106] Iteration 306100, lr = 0.000434375
I0630 18:24:17.647200 29777 solver.cpp:290] Iteration 306200 (6.29685 iter/s, 15.881s/100 iter), loss = 1.70238
I0630 18:24:17.647222 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 18:24:17.647229 29777 sgd_solver.cpp:106] Iteration 306200, lr = 0.00043125
I0630 18:24:33.600026 29777 solver.cpp:290] Iteration 306300 (6.26866 iter/s, 15.9524s/100 iter), loss = 1.17857
I0630 18:24:33.600108 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 18:24:33.600116 29777 sgd_solver.cpp:106] Iteration 306300, lr = 0.000428125
I0630 18:24:49.596458 29777 solver.cpp:290] Iteration 306400 (6.2516 iter/s, 15.9959s/100 iter), loss = 1.07143
I0630 18:24:49.596506 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 18:24:49.596518 29777 sgd_solver.cpp:106] Iteration 306400, lr = 0.000425
I0630 18:25:05.710918 29777 solver.cpp:290] Iteration 306500 (6.2058 iter/s, 16.114s/100 iter), loss = 1.10714
I0630 18:25:05.711024 29777 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 18:25:05.711035 29777 sgd_solver.cpp:106] Iteration 306500, lr = 0.000421875
I0630 18:25:21.969485 29777 solver.cpp:290] Iteration 306600 (6.15081 iter/s, 16.258s/100 iter), loss = 1.41667
I0630 18:25:21.969511 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 18:25:21.969521 29777 sgd_solver.cpp:106] Iteration 306600, lr = 0.00041875
I0630 18:25:38.174243 29777 solver.cpp:290] Iteration 306700 (6.17121 iter/s, 16.2043s/100 iter), loss = 1.2619
I0630 18:25:38.174347 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 18:25:38.174355 29777 sgd_solver.cpp:106] Iteration 306700, lr = 0.000415625
I0630 18:25:54.418213 29777 solver.cpp:290] Iteration 306800 (6.15634 iter/s, 16.2434s/100 iter), loss = 1.33333
I0630 18:25:54.418236 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 18:25:54.418242 29777 sgd_solver.cpp:106] Iteration 306800, lr = 0.0004125
I0630 18:26:10.540685 29777 solver.cpp:290] Iteration 306900 (6.2027 iter/s, 16.122s/100 iter), loss = 1.29762
I0630 18:26:10.540765 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 18:26:10.540776 29777 sgd_solver.cpp:106] Iteration 306900, lr = 0.000409375
I0630 18:26:26.575690 29777 solver.cpp:354] Sparsity after update:
I0630 18:26:26.596266 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:26:26.596282 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:26:26.596292 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:26:26.596294 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:26:26.596298 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:26:26.596302 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:26:26.596304 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:26:26.596307 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:26:26.596310 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:26:26.596313 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:26:26.596318 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:26:26.596320 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:26:26.596323 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:26:26.756466 29777 solver.cpp:290] Iteration 307000 (6.16703 iter/s, 16.2153s/100 iter), loss = 1.42857
I0630 18:26:26.756490 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 18:26:26.756500 29777 sgd_solver.cpp:106] Iteration 307000, lr = 0.00040625
I0630 18:26:43.067719 29777 solver.cpp:290] Iteration 307100 (6.13091 iter/s, 16.3108s/100 iter), loss = 1.2381
I0630 18:26:43.067811 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 18:26:43.067822 29777 sgd_solver.cpp:106] Iteration 307100, lr = 0.000403125
I0630 18:26:59.110915 29777 solver.cpp:290] Iteration 307200 (6.23338 iter/s, 16.0427s/100 iter), loss = 1.60714
I0630 18:26:59.110942 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 18:26:59.110951 29777 sgd_solver.cpp:106] Iteration 307200, lr = 0.0004
I0630 18:27:15.214874 29777 solver.cpp:290] Iteration 307300 (6.20984 iter/s, 16.1035s/100 iter), loss = 1.46429
I0630 18:27:15.214941 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 18:27:15.214948 29777 sgd_solver.cpp:106] Iteration 307300, lr = 0.000396875
I0630 18:27:31.310170 29777 solver.cpp:290] Iteration 307400 (6.2132 iter/s, 16.0948s/100 iter), loss = 1.09524
I0630 18:27:31.310220 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 18:27:31.310247 29777 sgd_solver.cpp:106] Iteration 307400, lr = 0.00039375
I0630 18:27:47.551131 29777 solver.cpp:290] Iteration 307500 (6.15746 iter/s, 16.2405s/100 iter), loss = 0.97619
I0630 18:27:47.551221 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 18:27:47.551231 29777 sgd_solver.cpp:106] Iteration 307500, lr = 0.000390625
I0630 18:28:03.560253 29777 solver.cpp:290] Iteration 307600 (6.24665 iter/s, 16.0086s/100 iter), loss = 1.17857
I0630 18:28:03.560276 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 18:28:03.560283 29777 sgd_solver.cpp:106] Iteration 307600, lr = 0.0003875
I0630 18:28:19.581893 29777 solver.cpp:290] Iteration 307700 (6.24174 iter/s, 16.0212s/100 iter), loss = 1.11905
I0630 18:28:19.581981 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 18:28:19.581997 29777 sgd_solver.cpp:106] Iteration 307700, lr = 0.000384375
I0630 18:28:35.606482 29777 solver.cpp:290] Iteration 307800 (6.24062 iter/s, 16.0241s/100 iter), loss = 1.20238
I0630 18:28:35.606509 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 18:28:35.606518 29777 sgd_solver.cpp:106] Iteration 307800, lr = 0.00038125
I0630 18:28:51.868137 29777 solver.cpp:290] Iteration 307900 (6.14961 iter/s, 16.2612s/100 iter), loss = 1.39286
I0630 18:28:51.868206 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 18:28:51.868213 29777 sgd_solver.cpp:106] Iteration 307900, lr = 0.000378125
I0630 18:29:07.896229 29777 solver.cpp:354] Sparsity after update:
I0630 18:29:07.897274 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:29:07.897281 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:29:07.897289 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:29:07.897291 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:29:07.897294 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:29:07.897295 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:29:07.897297 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:29:07.897300 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:29:07.897301 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:29:07.897303 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:29:07.897305 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:29:07.897306 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:29:07.897308 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:29:07.897395 29777 solver.cpp:471] Iteration 308000, Testing net (#0)
I0630 18:29:39.092839 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 18:30:11.233497 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.563199
I0630 18:30:11.233588 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.795461
I0630 18:30:11.233598 29777 solver.cpp:544]     Test net output #2: loss = 1.54864 (* 1 = 1.54864 loss)
I0630 18:30:11.413558 29777 solver.cpp:290] Iteration 308000 (1.25718 iter/s, 79.5432s/100 iter), loss = 1.39286
I0630 18:30:11.413581 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 18:30:11.413589 29777 sgd_solver.cpp:106] Iteration 308000, lr = 0.000375
I0630 18:30:26.880447 29777 solver.cpp:290] Iteration 308100 (6.46561 iter/s, 15.4664s/100 iter), loss = 1.19048
I0630 18:30:26.880475 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 18:30:26.880486 29777 sgd_solver.cpp:106] Iteration 308100, lr = 0.000371875
I0630 18:30:42.850268 29777 solver.cpp:290] Iteration 308200 (6.26199 iter/s, 15.9694s/100 iter), loss = 1
I0630 18:30:42.850354 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 18:30:42.850364 29777 sgd_solver.cpp:106] Iteration 308200, lr = 0.00036875
I0630 18:30:58.915382 29777 solver.cpp:290] Iteration 308300 (6.22487 iter/s, 16.0646s/100 iter), loss = 1.5
I0630 18:30:58.915408 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 18:30:58.915415 29777 sgd_solver.cpp:106] Iteration 308300, lr = 0.000365625
I0630 18:31:15.321871 29777 solver.cpp:290] Iteration 308400 (6.09533 iter/s, 16.406s/100 iter), loss = 1
I0630 18:31:15.321980 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 18:31:15.321990 29777 sgd_solver.cpp:106] Iteration 308400, lr = 0.0003625
I0630 18:31:31.554877 29777 solver.cpp:290] Iteration 308500 (6.1605 iter/s, 16.2324s/100 iter), loss = 1.44048
I0630 18:31:31.555096 29777 solver.cpp:309]     Train net output #0: loss = 1.69048 (* 1 = 1.69048 loss)
I0630 18:31:31.555207 29777 sgd_solver.cpp:106] Iteration 308500, lr = 0.000359375
I0630 18:31:47.707978 29777 solver.cpp:290] Iteration 308600 (6.19101 iter/s, 16.1524s/100 iter), loss = 1.22619
I0630 18:31:47.708056 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 18:31:47.708068 29777 sgd_solver.cpp:106] Iteration 308600, lr = 0.00035625
I0630 18:32:03.758970 29777 solver.cpp:290] Iteration 308700 (6.23035 iter/s, 16.0505s/100 iter), loss = 1.25
I0630 18:32:03.758997 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 18:32:03.759006 29777 sgd_solver.cpp:106] Iteration 308700, lr = 0.000353125
I0630 18:32:19.866988 29777 solver.cpp:290] Iteration 308800 (6.20827 iter/s, 16.1075s/100 iter), loss = 1.0119
I0630 18:32:19.867090 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 18:32:19.867100 29777 sgd_solver.cpp:106] Iteration 308800, lr = 0.00035
I0630 18:32:36.185422 29777 solver.cpp:290] Iteration 308900 (6.12825 iter/s, 16.3179s/100 iter), loss = 1.22619
I0630 18:32:36.185448 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 18:32:36.185457 29777 sgd_solver.cpp:106] Iteration 308900, lr = 0.000346875
I0630 18:32:52.273315 29777 solver.cpp:354] Sparsity after update:
I0630 18:32:52.293965 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:32:52.293982 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:32:52.293992 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:32:52.293993 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:32:52.293995 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:32:52.293998 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:32:52.293999 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:32:52.294003 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:32:52.294008 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:32:52.294010 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:32:52.294014 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:32:52.294018 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:32:52.294021 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:32:52.452903 29777 solver.cpp:290] Iteration 309000 (6.14741 iter/s, 16.267s/100 iter), loss = 1.08333
I0630 18:32:52.452926 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 18:32:52.452932 29777 sgd_solver.cpp:106] Iteration 309000, lr = 0.00034375
I0630 18:33:08.434273 29777 solver.cpp:290] Iteration 309100 (6.25747 iter/s, 15.9809s/100 iter), loss = 1.10714
I0630 18:33:08.434298 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 18:33:08.434307 29777 sgd_solver.cpp:106] Iteration 309100, lr = 0.000340625
I0630 18:33:24.551996 29777 solver.cpp:290] Iteration 309200 (6.20453 iter/s, 16.1173s/100 iter), loss = 1.13095
I0630 18:33:24.552110 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 18:33:24.552121 29777 sgd_solver.cpp:106] Iteration 309200, lr = 0.0003375
I0630 18:33:40.573665 29777 solver.cpp:290] Iteration 309300 (6.24176 iter/s, 16.0211s/100 iter), loss = 1.40476
I0630 18:33:40.573689 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 18:33:40.573695 29777 sgd_solver.cpp:106] Iteration 309300, lr = 0.000334375
I0630 18:33:56.756903 29777 solver.cpp:290] Iteration 309400 (6.17941 iter/s, 16.1828s/100 iter), loss = 1.2381
I0630 18:33:56.756975 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 18:33:56.756984 29777 sgd_solver.cpp:106] Iteration 309400, lr = 0.00033125
I0630 18:34:12.927049 29777 solver.cpp:290] Iteration 309500 (6.18443 iter/s, 16.1696s/100 iter), loss = 1.57143
I0630 18:34:12.927073 29777 solver.cpp:309]     Train net output #0: loss = 1.88095 (* 1 = 1.88095 loss)
I0630 18:34:12.927078 29777 sgd_solver.cpp:106] Iteration 309500, lr = 0.000328125
I0630 18:34:29.095804 29777 solver.cpp:290] Iteration 309600 (6.18495 iter/s, 16.1683s/100 iter), loss = 1.20238
I0630 18:34:29.096346 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 18:34:29.096356 29777 sgd_solver.cpp:106] Iteration 309600, lr = 0.000325
I0630 18:34:45.467846 29777 solver.cpp:290] Iteration 309700 (6.10834 iter/s, 16.371s/100 iter), loss = 1.40476
I0630 18:34:45.467869 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 18:34:45.467876 29777 sgd_solver.cpp:106] Iteration 309700, lr = 0.000321875
I0630 18:35:01.465138 29777 solver.cpp:290] Iteration 309800 (6.25124 iter/s, 15.9968s/100 iter), loss = 1.15476
I0630 18:35:01.465252 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 18:35:01.465275 29777 sgd_solver.cpp:106] Iteration 309800, lr = 0.00031875
I0630 18:35:17.571010 29777 solver.cpp:290] Iteration 309900 (6.20913 iter/s, 16.1053s/100 iter), loss = 1.22619
I0630 18:35:17.571035 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 18:35:17.571044 29777 sgd_solver.cpp:106] Iteration 309900, lr = 0.000315625
I0630 18:35:33.456791 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_310000.caffemodel
I0630 18:35:33.498013 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_310000.solverstate
I0630 18:35:33.506937 29777 solver.cpp:354] Sparsity after update:
I0630 18:35:33.507906 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:35:33.507915 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:35:33.507921 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:35:33.507923 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:35:33.507925 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:35:33.507927 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:35:33.507930 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:35:33.507931 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:35:33.507933 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:35:33.507936 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:35:33.507937 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:35:33.507939 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:35:33.507941 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:35:33.508036 29777 solver.cpp:471] Iteration 310000, Testing net (#0)
I0630 18:36:04.163959 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 18:36:34.144042 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56254
I0630 18:36:34.144069 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.796162
I0630 18:36:34.144076 29777 solver.cpp:544]     Test net output #2: loss = 1.55118 (* 1 = 1.55118 loss)
I0630 18:36:34.321046 29777 solver.cpp:290] Iteration 310000 (1.30297 iter/s, 76.7479s/100 iter), loss = 1.28571
I0630 18:36:34.321156 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 18:36:34.321166 29777 sgd_solver.cpp:106] Iteration 310000, lr = 0.0003125
I0630 18:36:49.579769 29777 solver.cpp:290] Iteration 310100 (6.55386 iter/s, 15.2582s/100 iter), loss = 1.11905
I0630 18:36:49.579794 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 18:36:49.579803 29777 sgd_solver.cpp:106] Iteration 310100, lr = 0.000309375
I0630 18:37:05.908581 29777 solver.cpp:290] Iteration 310200 (6.12433 iter/s, 16.3283s/100 iter), loss = 1.20238
I0630 18:37:05.908704 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 18:37:05.908728 29777 sgd_solver.cpp:106] Iteration 310200, lr = 0.00030625
I0630 18:37:22.304648 29777 solver.cpp:290] Iteration 310300 (6.09923 iter/s, 16.3955s/100 iter), loss = 1.4881
I0630 18:37:22.304672 29777 solver.cpp:309]     Train net output #0: loss = 1.7619 (* 1 = 1.7619 loss)
I0630 18:37:22.304678 29777 sgd_solver.cpp:106] Iteration 310300, lr = 0.000303125
I0630 18:37:38.400524 29777 solver.cpp:290] Iteration 310400 (6.21295 iter/s, 16.0954s/100 iter), loss = 1.28571
I0630 18:37:38.400635 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 18:37:38.400645 29777 sgd_solver.cpp:106] Iteration 310400, lr = 0.0003
I0630 18:37:54.369987 29777 solver.cpp:290] Iteration 310500 (6.26217 iter/s, 15.9689s/100 iter), loss = 1.09524
I0630 18:37:54.370010 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 18:37:54.370020 29777 sgd_solver.cpp:106] Iteration 310500, lr = 0.000296875
I0630 18:38:10.404911 29777 solver.cpp:290] Iteration 310600 (6.23657 iter/s, 16.0345s/100 iter), loss = 0.880952
I0630 18:38:10.404983 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 18:38:10.404989 29777 sgd_solver.cpp:106] Iteration 310600, lr = 0.00029375
I0630 18:38:26.394877 29777 solver.cpp:290] Iteration 310700 (6.25412 iter/s, 15.9895s/100 iter), loss = 1.54762
I0630 18:38:26.394904 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 18:38:26.394913 29777 sgd_solver.cpp:106] Iteration 310700, lr = 0.000290625
I0630 18:38:42.380879 29777 solver.cpp:290] Iteration 310800 (6.25566 iter/s, 15.9855s/100 iter), loss = 1.21429
I0630 18:38:42.380970 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 18:38:42.380981 29777 sgd_solver.cpp:106] Iteration 310800, lr = 0.0002875
I0630 18:38:58.409296 29777 solver.cpp:290] Iteration 310900 (6.23913 iter/s, 16.0279s/100 iter), loss = 1.65476
I0630 18:38:58.409330 29777 solver.cpp:309]     Train net output #0: loss = 1.54762 (* 1 = 1.54762 loss)
I0630 18:38:58.409342 29777 sgd_solver.cpp:106] Iteration 310900, lr = 0.000284375
I0630 18:39:14.296926 29777 solver.cpp:354] Sparsity after update:
I0630 18:39:14.317648 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:39:14.317663 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:39:14.317674 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:39:14.317678 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:39:14.317682 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:39:14.317692 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:39:14.317698 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:39:14.317703 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:39:14.317708 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:39:14.317713 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:39:14.317718 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:39:14.317720 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:39:14.317723 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:39:14.476863 29777 solver.cpp:290] Iteration 311000 (6.2239 iter/s, 16.0671s/100 iter), loss = 1.2619
I0630 18:39:14.476891 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 18:39:14.476907 29777 sgd_solver.cpp:106] Iteration 311000, lr = 0.00028125
I0630 18:39:30.515630 29777 solver.cpp:290] Iteration 311100 (6.23508 iter/s, 16.0383s/100 iter), loss = 1.2619
I0630 18:39:30.515655 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 18:39:30.515663 29777 sgd_solver.cpp:106] Iteration 311100, lr = 0.000278125
I0630 18:39:46.507894 29777 solver.cpp:290] Iteration 311200 (6.25321 iter/s, 15.9918s/100 iter), loss = 1.2619
I0630 18:39:46.507988 29777 solver.cpp:309]     Train net output #0: loss = 0.904762 (* 1 = 0.904762 loss)
I0630 18:39:46.507999 29777 sgd_solver.cpp:106] Iteration 311200, lr = 0.000275
I0630 18:40:02.740038 29777 solver.cpp:290] Iteration 311300 (6.16082 iter/s, 16.2316s/100 iter), loss = 1.47619
I0630 18:40:02.740061 29777 solver.cpp:309]     Train net output #0: loss = 1.66667 (* 1 = 1.66667 loss)
I0630 18:40:02.740068 29777 sgd_solver.cpp:106] Iteration 311300, lr = 0.000271875
I0630 18:40:18.644572 29777 solver.cpp:290] Iteration 311400 (6.2877 iter/s, 15.9041s/100 iter), loss = 1.02381
I0630 18:40:18.644644 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 18:40:18.644655 29777 sgd_solver.cpp:106] Iteration 311400, lr = 0.00026875
I0630 18:40:34.935684 29777 solver.cpp:290] Iteration 311500 (6.13851 iter/s, 16.2906s/100 iter), loss = 1.36905
I0630 18:40:34.935705 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 18:40:34.935714 29777 sgd_solver.cpp:106] Iteration 311500, lr = 0.000265625
I0630 18:40:51.209291 29777 solver.cpp:290] Iteration 311600 (6.1451 iter/s, 16.2731s/100 iter), loss = 1.5119
I0630 18:40:51.209381 29777 solver.cpp:309]     Train net output #0: loss = 2.09524 (* 1 = 2.09524 loss)
I0630 18:40:51.209403 29777 sgd_solver.cpp:106] Iteration 311600, lr = 0.0002625
I0630 18:41:07.367911 29777 solver.cpp:290] Iteration 311700 (6.18885 iter/s, 16.1581s/100 iter), loss = 1.22619
I0630 18:41:07.367933 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 18:41:07.367939 29777 sgd_solver.cpp:106] Iteration 311700, lr = 0.000259375
I0630 18:41:23.579325 29777 solver.cpp:290] Iteration 311800 (6.16867 iter/s, 16.2109s/100 iter), loss = 0.821428
I0630 18:41:23.579412 29777 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 18:41:23.579421 29777 sgd_solver.cpp:106] Iteration 311800, lr = 0.00025625
I0630 18:41:39.662416 29777 solver.cpp:290] Iteration 311900 (6.21792 iter/s, 16.0826s/100 iter), loss = 0.857143
I0630 18:41:39.662437 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 18:41:39.662444 29777 sgd_solver.cpp:106] Iteration 311900, lr = 0.000253125
I0630 18:41:55.816081 29777 solver.cpp:354] Sparsity after update:
I0630 18:41:55.817693 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:41:55.817701 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:41:55.817708 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:41:55.817710 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:41:55.817713 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:41:55.817714 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:41:55.817716 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:41:55.817718 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:41:55.817720 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:41:55.817723 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:41:55.817724 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:41:55.817726 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:41:55.817728 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:41:55.817813 29777 solver.cpp:471] Iteration 312000, Testing net (#0)
I0630 18:42:29.441920 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 18:42:57.359827 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56226
I0630 18:42:57.359859 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.796261
I0630 18:42:57.359868 29777 solver.cpp:544]     Test net output #2: loss = 1.54894 (* 1 = 1.54894 loss)
I0630 18:42:57.544755 29777 solver.cpp:290] Iteration 312000 (1.28402 iter/s, 77.8802s/100 iter), loss = 1.34524
I0630 18:42:57.544780 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 18:42:57.544790 29777 sgd_solver.cpp:106] Iteration 312000, lr = 0.00025
I0630 18:43:13.103616 29777 solver.cpp:290] Iteration 312100 (6.42739 iter/s, 15.5584s/100 iter), loss = 1.04762
I0630 18:43:13.103688 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 18:43:13.103698 29777 sgd_solver.cpp:106] Iteration 312100, lr = 0.000246875
I0630 18:43:29.286809 29777 solver.cpp:290] Iteration 312200 (6.17945 iter/s, 16.1827s/100 iter), loss = 1.32143
I0630 18:43:29.286871 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 18:43:29.286893 29777 sgd_solver.cpp:106] Iteration 312200, lr = 0.00024375
I0630 18:43:45.528520 29777 solver.cpp:290] Iteration 312300 (6.15718 iter/s, 16.2412s/100 iter), loss = 1.36905
I0630 18:43:45.528626 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 18:43:45.528636 29777 sgd_solver.cpp:106] Iteration 312300, lr = 0.000240625
I0630 18:44:01.628659 29777 solver.cpp:290] Iteration 312400 (6.21134 iter/s, 16.0996s/100 iter), loss = 0.964285
I0630 18:44:01.628715 29777 solver.cpp:309]     Train net output #0: loss = 0.738095 (* 1 = 0.738095 loss)
I0630 18:44:01.628733 29777 sgd_solver.cpp:106] Iteration 312400, lr = 0.0002375
I0630 18:44:17.684772 29777 solver.cpp:290] Iteration 312500 (6.22835 iter/s, 16.0556s/100 iter), loss = 1.14286
I0630 18:44:17.684847 29777 solver.cpp:309]     Train net output #0: loss = 1.14286 (* 1 = 1.14286 loss)
I0630 18:44:17.684859 29777 sgd_solver.cpp:106] Iteration 312500, lr = 0.000234375
I0630 18:44:33.927431 29777 solver.cpp:290] Iteration 312600 (6.15683 iter/s, 16.2421s/100 iter), loss = 1.40476
I0630 18:44:33.927454 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 18:44:33.927461 29777 sgd_solver.cpp:106] Iteration 312600, lr = 0.00023125
I0630 18:44:50.040076 29777 solver.cpp:290] Iteration 312700 (6.20649 iter/s, 16.1122s/100 iter), loss = 1.07143
I0630 18:44:50.040161 29777 solver.cpp:309]     Train net output #0: loss = 1.2619 (* 1 = 1.2619 loss)
I0630 18:44:50.040170 29777 sgd_solver.cpp:106] Iteration 312700, lr = 0.000228125
I0630 18:45:06.015960 29777 solver.cpp:290] Iteration 312800 (6.25964 iter/s, 15.9754s/100 iter), loss = 1
I0630 18:45:06.015986 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 18:45:06.015993 29777 sgd_solver.cpp:106] Iteration 312800, lr = 0.000225
I0630 18:45:22.329058 29777 solver.cpp:290] Iteration 312900 (6.13022 iter/s, 16.3126s/100 iter), loss = 1.07143
I0630 18:45:22.329107 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 18:45:22.329115 29777 sgd_solver.cpp:106] Iteration 312900, lr = 0.000221875
I0630 18:45:38.261442 29777 solver.cpp:354] Sparsity after update:
I0630 18:45:38.281843 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:45:38.281857 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:45:38.281868 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:45:38.281872 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:45:38.281875 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:45:38.281878 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:45:38.281883 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:45:38.281885 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:45:38.281890 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:45:38.281893 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:45:38.281896 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:45:38.281899 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:45:38.281903 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:45:38.440925 29777 solver.cpp:290] Iteration 313000 (6.2068 iter/s, 16.1114s/100 iter), loss = 1.10714
I0630 18:45:38.441040 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 18:45:38.441084 29777 sgd_solver.cpp:106] Iteration 313000, lr = 0.00021875
I0630 18:45:54.446918 29777 solver.cpp:290] Iteration 313100 (6.24788 iter/s, 16.0054s/100 iter), loss = 1.21429
I0630 18:45:54.447031 29777 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 18:45:54.447042 29777 sgd_solver.cpp:106] Iteration 313100, lr = 0.000215625
I0630 18:46:10.430660 29777 solver.cpp:290] Iteration 313200 (6.25657 iter/s, 15.9832s/100 iter), loss = 1.27381
I0630 18:46:10.430683 29777 solver.cpp:309]     Train net output #0: loss = 1.5 (* 1 = 1.5 loss)
I0630 18:46:10.430691 29777 sgd_solver.cpp:106] Iteration 313200, lr = 0.0002125
I0630 18:46:26.651829 29777 solver.cpp:290] Iteration 313300 (6.16496 iter/s, 16.2207s/100 iter), loss = 1.11905
I0630 18:46:26.651942 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 18:46:26.651952 29777 sgd_solver.cpp:106] Iteration 313300, lr = 0.000209375
I0630 18:46:42.748646 29777 solver.cpp:290] Iteration 313400 (6.21262 iter/s, 16.0963s/100 iter), loss = 0.988095
I0630 18:46:42.748669 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 18:46:42.748677 29777 sgd_solver.cpp:106] Iteration 313400, lr = 0.00020625
I0630 18:46:58.925981 29777 solver.cpp:290] Iteration 313500 (6.18167 iter/s, 16.1769s/100 iter), loss = 1.40476
I0630 18:46:58.926079 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 18:46:58.926091 29777 sgd_solver.cpp:106] Iteration 313500, lr = 0.000203125
I0630 18:47:14.977782 29777 solver.cpp:290] Iteration 313600 (6.23004 iter/s, 16.0513s/100 iter), loss = 1.0119
I0630 18:47:14.977808 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 18:47:14.977821 29777 sgd_solver.cpp:106] Iteration 313600, lr = 0.0002
I0630 18:47:31.074937 29777 solver.cpp:290] Iteration 313700 (6.21246 iter/s, 16.0967s/100 iter), loss = 1.40476
I0630 18:47:31.075044 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 18:47:31.075054 29777 sgd_solver.cpp:106] Iteration 313700, lr = 0.000196875
I0630 18:47:47.226397 29777 solver.cpp:290] Iteration 313800 (6.1916 iter/s, 16.1509s/100 iter), loss = 1.33333
I0630 18:47:47.226421 29777 solver.cpp:309]     Train net output #0: loss = 1.47619 (* 1 = 1.47619 loss)
I0630 18:47:47.226428 29777 sgd_solver.cpp:106] Iteration 313800, lr = 0.00019375
I0630 18:48:03.476140 29777 solver.cpp:290] Iteration 313900 (6.15412 iter/s, 16.2493s/100 iter), loss = 1.38095
I0630 18:48:03.476207 29777 solver.cpp:309]     Train net output #0: loss = 1.40476 (* 1 = 1.40476 loss)
I0630 18:48:03.476217 29777 sgd_solver.cpp:106] Iteration 313900, lr = 0.000190625
I0630 18:48:19.379379 29777 solver.cpp:354] Sparsity after update:
I0630 18:48:19.380822 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:48:19.380831 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:48:19.380838 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:48:19.380843 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:48:19.380848 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:48:19.380852 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:48:19.380856 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:48:19.380861 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:48:19.380864 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:48:19.380869 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:48:19.380873 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:48:19.380877 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:48:19.380882 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:48:19.380975 29777 solver.cpp:471] Iteration 314000, Testing net (#0)
I0630 18:48:50.599023 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 18:49:20.887429 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56268
I0630 18:49:20.887481 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.796222
I0630 18:49:20.887488 29777 solver.cpp:544]     Test net output #2: loss = 1.54902 (* 1 = 1.54902 loss)
I0630 18:49:21.056749 29777 solver.cpp:290] Iteration 314000 (1.28902 iter/s, 77.5784s/100 iter), loss = 1.27381
I0630 18:49:21.056776 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 18:49:21.056784 29777 sgd_solver.cpp:106] Iteration 314000, lr = 0.0001875
I0630 18:49:36.438316 29777 solver.cpp:290] Iteration 314100 (6.50148 iter/s, 15.3811s/100 iter), loss = 1.30952
I0630 18:49:36.438345 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 18:49:36.438354 29777 sgd_solver.cpp:106] Iteration 314100, lr = 0.000184375
I0630 18:49:52.385802 29777 solver.cpp:290] Iteration 314200 (6.27077 iter/s, 15.947s/100 iter), loss = 1.03571
I0630 18:49:52.385882 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 18:49:52.385892 29777 sgd_solver.cpp:106] Iteration 314200, lr = 0.00018125
I0630 18:50:08.557054 29777 solver.cpp:290] Iteration 314300 (6.18401 iter/s, 16.1707s/100 iter), loss = 1.35714
I0630 18:50:08.557078 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 18:50:08.557087 29777 sgd_solver.cpp:106] Iteration 314300, lr = 0.000178125
I0630 18:50:24.645826 29777 solver.cpp:290] Iteration 314400 (6.2157 iter/s, 16.0883s/100 iter), loss = 1.20238
I0630 18:50:24.645938 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 18:50:24.645948 29777 sgd_solver.cpp:106] Iteration 314400, lr = 0.000175
I0630 18:50:40.667273 29777 solver.cpp:290] Iteration 314500 (6.24185 iter/s, 16.0209s/100 iter), loss = 1.63095
I0630 18:50:40.667297 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 18:50:40.667304 29777 sgd_solver.cpp:106] Iteration 314500, lr = 0.000171875
I0630 18:50:56.678047 29777 solver.cpp:290] Iteration 314600 (6.24598 iter/s, 16.0103s/100 iter), loss = 1.20238
I0630 18:50:56.678143 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 18:50:56.678153 29777 sgd_solver.cpp:106] Iteration 314600, lr = 0.00016875
I0630 18:51:12.803457 29777 solver.cpp:290] Iteration 314700 (6.2016 iter/s, 16.1249s/100 iter), loss = 1.19048
I0630 18:51:12.803511 29777 solver.cpp:309]     Train net output #0: loss = 1.02381 (* 1 = 1.02381 loss)
I0630 18:51:12.803529 29777 sgd_solver.cpp:106] Iteration 314700, lr = 0.000165625
I0630 18:51:30.187762 29777 solver.cpp:290] Iteration 314800 (5.75249 iter/s, 17.3838s/100 iter), loss = 1.09524
I0630 18:51:30.187868 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 18:51:30.187877 29777 sgd_solver.cpp:106] Iteration 314800, lr = 0.0001625
I0630 18:51:46.607745 29777 solver.cpp:290] Iteration 314900 (6.09035 iter/s, 16.4194s/100 iter), loss = 1.13095
I0630 18:51:46.607771 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 18:51:46.607780 29777 sgd_solver.cpp:106] Iteration 314900, lr = 0.000159375
I0630 18:52:02.970026 29777 solver.cpp:354] Sparsity after update:
I0630 18:52:02.990408 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:52:02.990430 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:52:02.990442 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:52:02.990445 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:52:02.990448 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:52:02.990453 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:52:02.990463 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:52:02.990473 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:52:02.990481 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:52:02.990496 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:52:02.990501 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:52:02.990509 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:52:02.990520 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:52:03.147049 29777 solver.cpp:290] Iteration 315000 (6.04638 iter/s, 16.5388s/100 iter), loss = 1.63095
I0630 18:52:03.147081 29777 solver.cpp:309]     Train net output #0: loss = 2.09524 (* 1 = 2.09524 loss)
I0630 18:52:03.147091 29777 sgd_solver.cpp:106] Iteration 315000, lr = 0.00015625
I0630 18:52:19.275383 29777 solver.cpp:290] Iteration 315100 (6.20045 iter/s, 16.1279s/100 iter), loss = 1.19048
I0630 18:52:19.275410 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 18:52:19.275419 29777 sgd_solver.cpp:106] Iteration 315100, lr = 0.000153125
I0630 18:52:35.302212 29777 solver.cpp:290] Iteration 315200 (6.23972 iter/s, 16.0264s/100 iter), loss = 1.40476
I0630 18:52:35.302320 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 18:52:35.302331 29777 sgd_solver.cpp:106] Iteration 315200, lr = 0.00015
I0630 18:52:52.022287 29777 solver.cpp:290] Iteration 315300 (5.98104 iter/s, 16.7195s/100 iter), loss = 1.33333
I0630 18:52:52.022308 29777 solver.cpp:309]     Train net output #0: loss = 1.7381 (* 1 = 1.7381 loss)
I0630 18:52:52.022316 29777 sgd_solver.cpp:106] Iteration 315300, lr = 0.000146875
I0630 18:53:08.265226 29777 solver.cpp:290] Iteration 315400 (6.1567 iter/s, 16.2425s/100 iter), loss = 1.21429
I0630 18:53:08.265635 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 18:53:08.265645 29777 sgd_solver.cpp:106] Iteration 315400, lr = 0.00014375
I0630 18:53:24.251302 29777 solver.cpp:290] Iteration 315500 (6.25578 iter/s, 15.9852s/100 iter), loss = 0.869047
I0630 18:53:24.251325 29777 solver.cpp:309]     Train net output #0: loss = 0.833333 (* 1 = 0.833333 loss)
I0630 18:53:24.251332 29777 sgd_solver.cpp:106] Iteration 315500, lr = 0.000140625
I0630 18:53:40.348618 29777 solver.cpp:290] Iteration 315600 (6.2124 iter/s, 16.0968s/100 iter), loss = 1.42857
I0630 18:53:40.348696 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 18:53:40.348706 29777 sgd_solver.cpp:106] Iteration 315600, lr = 0.0001375
I0630 18:53:56.689486 29777 solver.cpp:290] Iteration 315700 (6.11982 iter/s, 16.3403s/100 iter), loss = 1.34524
I0630 18:53:56.689515 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 18:53:56.689522 29777 sgd_solver.cpp:106] Iteration 315700, lr = 0.000134375
I0630 18:54:12.817323 29777 solver.cpp:290] Iteration 315800 (6.20064 iter/s, 16.1274s/100 iter), loss = 1.20238
I0630 18:54:12.817396 29777 solver.cpp:309]     Train net output #0: loss = 0.952381 (* 1 = 0.952381 loss)
I0630 18:54:12.817404 29777 sgd_solver.cpp:106] Iteration 315800, lr = 0.00013125
I0630 18:54:28.818830 29777 solver.cpp:290] Iteration 315900 (6.24961 iter/s, 16.001s/100 iter), loss = 1.33333
I0630 18:54:28.818857 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 18:54:28.818864 29777 sgd_solver.cpp:106] Iteration 315900, lr = 0.000128125
I0630 18:54:44.736696 29777 solver.cpp:354] Sparsity after update:
I0630 18:54:44.737980 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:54:44.737988 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:54:44.737996 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:54:44.737998 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:54:44.738001 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:54:44.738004 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:54:44.738006 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:54:44.738009 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:54:44.738011 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:54:44.738014 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:54:44.738016 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:54:44.738018 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:54:44.738020 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:54:44.738107 29777 solver.cpp:471] Iteration 316000, Testing net (#0)
I0630 18:55:17.286839 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 18:55:42.842175 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56186
I0630 18:55:42.842219 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.797121
I0630 18:55:42.842231 29777 solver.cpp:544]     Test net output #2: loss = 1.54824 (* 1 = 1.54824 loss)
I0630 18:55:43.046141 29777 solver.cpp:290] Iteration 316000 (1.34725 iter/s, 74.2253s/100 iter), loss = 1.2619
I0630 18:55:43.046182 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 18:55:43.050048 29777 sgd_solver.cpp:106] Iteration 316000, lr = 0.000125
I0630 18:55:58.488785 29777 solver.cpp:290] Iteration 316100 (6.47577 iter/s, 15.4422s/100 iter), loss = 0.845238
I0630 18:55:58.488881 29777 solver.cpp:309]     Train net output #0: loss = 0.785714 (* 1 = 0.785714 loss)
I0630 18:55:58.488893 29777 sgd_solver.cpp:106] Iteration 316100, lr = 0.000121875
I0630 18:56:14.741091 29777 solver.cpp:290] Iteration 316200 (6.15318 iter/s, 16.2518s/100 iter), loss = 1.28571
I0630 18:56:14.741117 29777 solver.cpp:309]     Train net output #0: loss = 1.30952 (* 1 = 1.30952 loss)
I0630 18:56:14.741127 29777 sgd_solver.cpp:106] Iteration 316200, lr = 0.00011875
I0630 18:56:31.126323 29777 solver.cpp:290] Iteration 316300 (6.10324 iter/s, 16.3847s/100 iter), loss = 1.10714
I0630 18:56:31.126411 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 18:56:31.126423 29777 sgd_solver.cpp:106] Iteration 316300, lr = 0.000115625
I0630 18:56:47.172971 29777 solver.cpp:290] Iteration 316400 (6.23205 iter/s, 16.0461s/100 iter), loss = 1.07143
I0630 18:56:47.173022 29777 solver.cpp:309]     Train net output #0: loss = 0.690476 (* 1 = 0.690476 loss)
I0630 18:56:47.173036 29777 sgd_solver.cpp:106] Iteration 316400, lr = 0.0001125
I0630 18:57:03.439646 29777 solver.cpp:290] Iteration 316500 (6.14774 iter/s, 16.2661s/100 iter), loss = 1.11905
I0630 18:57:03.439929 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 18:57:03.440058 29777 sgd_solver.cpp:106] Iteration 316500, lr = 0.000109375
I0630 18:57:19.750836 29777 solver.cpp:290] Iteration 316600 (6.13105 iter/s, 16.3104s/100 iter), loss = 1.16667
I0630 18:57:19.750859 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 18:57:19.750865 29777 sgd_solver.cpp:106] Iteration 316600, lr = 0.00010625
I0630 18:57:35.958523 29777 solver.cpp:290] Iteration 316700 (6.17011 iter/s, 16.2072s/100 iter), loss = 1.46429
I0630 18:57:35.958629 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 18:57:35.958643 29777 sgd_solver.cpp:106] Iteration 316700, lr = 0.000103125
I0630 18:57:51.961105 29777 solver.cpp:290] Iteration 316800 (6.24922 iter/s, 16.002s/100 iter), loss = 0.97619
I0630 18:57:51.961130 29777 solver.cpp:309]     Train net output #0: loss = 1.11905 (* 1 = 1.11905 loss)
I0630 18:57:51.961138 29777 sgd_solver.cpp:106] Iteration 316800, lr = 9.99999e-05
I0630 18:58:08.179890 29777 solver.cpp:290] Iteration 316900 (6.16589 iter/s, 16.2183s/100 iter), loss = 1.09524
I0630 18:58:08.180011 29777 solver.cpp:309]     Train net output #0: loss = 0.880952 (* 1 = 0.880952 loss)
I0630 18:58:08.180033 29777 sgd_solver.cpp:106] Iteration 316900, lr = 9.68748e-05
I0630 18:58:24.356744 29777 solver.cpp:354] Sparsity after update:
I0630 18:58:24.377187 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 18:58:24.377202 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 18:58:24.377213 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 18:58:24.377216 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 18:58:24.377220 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 18:58:24.377225 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 18:58:24.377229 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 18:58:24.377233 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 18:58:24.377235 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 18:58:24.377238 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 18:58:24.377243 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 18:58:24.377248 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 18:58:24.377250 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 18:58:24.535718 29777 solver.cpp:290] Iteration 317000 (6.11425 iter/s, 16.3552s/100 iter), loss = 1.15476
I0630 18:58:24.535743 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 18:58:24.535753 29777 sgd_solver.cpp:106] Iteration 317000, lr = 9.37498e-05
I0630 18:58:40.948765 29777 solver.cpp:290] Iteration 317100 (6.0929 iter/s, 16.4125s/100 iter), loss = 1.09524
I0630 18:58:40.948873 29777 solver.cpp:309]     Train net output #0: loss = 1.16667 (* 1 = 1.16667 loss)
I0630 18:58:40.948882 29777 sgd_solver.cpp:106] Iteration 317100, lr = 9.06253e-05
I0630 18:58:57.168030 29777 solver.cpp:290] Iteration 317200 (6.16573 iter/s, 16.2187s/100 iter), loss = 1.34524
I0630 18:58:57.168053 29777 solver.cpp:309]     Train net output #0: loss = 1.42857 (* 1 = 1.42857 loss)
I0630 18:58:57.168059 29777 sgd_solver.cpp:106] Iteration 317200, lr = 8.75002e-05
I0630 18:59:13.341240 29777 solver.cpp:290] Iteration 317300 (6.18326 iter/s, 16.1727s/100 iter), loss = 1.61905
I0630 18:59:13.341493 29777 solver.cpp:309]     Train net output #0: loss = 1.78571 (* 1 = 1.78571 loss)
I0630 18:59:13.341572 29777 sgd_solver.cpp:106] Iteration 317300, lr = 8.43751e-05
I0630 18:59:29.669384 29777 solver.cpp:290] Iteration 317400 (6.12467 iter/s, 16.3274s/100 iter), loss = 1.2619
I0630 18:59:29.669435 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 18:59:29.669447 29777 sgd_solver.cpp:106] Iteration 317400, lr = 8.12501e-05
I0630 18:59:45.999409 29777 solver.cpp:290] Iteration 317500 (6.12389 iter/s, 16.3295s/100 iter), loss = 1.10714
I0630 18:59:45.999518 29777 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 18:59:45.999527 29777 sgd_solver.cpp:106] Iteration 317500, lr = 7.8125e-05
I0630 19:00:02.202239 29777 solver.cpp:290] Iteration 317600 (6.17199 iter/s, 16.2022s/100 iter), loss = 1.53571
I0630 19:00:02.202262 29777 solver.cpp:309]     Train net output #0: loss = 1.52381 (* 1 = 1.52381 loss)
I0630 19:00:02.202270 29777 sgd_solver.cpp:106] Iteration 317600, lr = 7.49999e-05
I0630 19:00:18.170130 29777 solver.cpp:290] Iteration 317700 (6.26276 iter/s, 15.9674s/100 iter), loss = 1.35714
I0630 19:00:18.170204 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 19:00:18.170212 29777 sgd_solver.cpp:106] Iteration 317700, lr = 7.18749e-05
I0630 19:00:34.457340 29777 solver.cpp:290] Iteration 317800 (6.14 iter/s, 16.2866s/100 iter), loss = 1.22619
I0630 19:00:34.457412 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 19:00:34.457440 29777 sgd_solver.cpp:106] Iteration 317800, lr = 6.87498e-05
I0630 19:00:50.689833 29777 solver.cpp:290] Iteration 317900 (6.16069 iter/s, 16.2319s/100 iter), loss = 0.97619
I0630 19:00:50.689947 29777 solver.cpp:309]     Train net output #0: loss = 0.97619 (* 1 = 0.97619 loss)
I0630 19:00:50.689956 29777 sgd_solver.cpp:106] Iteration 317900, lr = 6.56247e-05
I0630 19:01:06.849702 29777 solver.cpp:354] Sparsity after update:
I0630 19:01:06.850973 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 19:01:06.850980 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 19:01:06.850988 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 19:01:06.850991 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 19:01:06.850994 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 19:01:06.850997 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 19:01:06.850999 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 19:01:06.851002 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 19:01:06.851004 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 19:01:06.851007 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 19:01:06.851009 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 19:01:06.851012 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 19:01:06.851014 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 19:01:06.851104 29777 solver.cpp:471] Iteration 318000, Testing net (#0)
I0630 19:01:37.848850 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 19:02:08.268831 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.56284
I0630 19:02:08.268883 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.796462
I0630 19:02:08.268890 29777 solver.cpp:544]     Test net output #2: loss = 1.54846 (* 1 = 1.54846 loss)
I0630 19:02:08.438630 29777 solver.cpp:290] Iteration 318000 (1.28623 iter/s, 77.7464s/100 iter), loss = 1.30952
I0630 19:02:08.438657 29777 solver.cpp:309]     Train net output #0: loss = 1.59524 (* 1 = 1.59524 loss)
I0630 19:02:08.438664 29777 sgd_solver.cpp:106] Iteration 318000, lr = 6.25002e-05
I0630 19:02:23.714841 29777 solver.cpp:290] Iteration 318100 (6.54633 iter/s, 15.2757s/100 iter), loss = 1.11905
I0630 19:02:23.714867 29777 solver.cpp:309]     Train net output #0: loss = 1.2381 (* 1 = 1.2381 loss)
I0630 19:02:23.714876 29777 sgd_solver.cpp:106] Iteration 318100, lr = 5.93752e-05
I0630 19:02:39.643331 29777 solver.cpp:290] Iteration 318200 (6.27826 iter/s, 15.928s/100 iter), loss = 1.14286
I0630 19:02:39.643404 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 19:02:39.643415 29777 sgd_solver.cpp:106] Iteration 318200, lr = 5.62501e-05
I0630 19:02:55.769297 29777 solver.cpp:290] Iteration 318300 (6.20139 iter/s, 16.1254s/100 iter), loss = 1.0119
I0630 19:02:55.769320 29777 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 19:02:55.769327 29777 sgd_solver.cpp:106] Iteration 318300, lr = 5.3125e-05
I0630 19:03:12.100435 29777 solver.cpp:290] Iteration 318400 (6.12346 iter/s, 16.3306s/100 iter), loss = 1.03571
I0630 19:03:12.100510 29777 solver.cpp:309]     Train net output #0: loss = 1.28571 (* 1 = 1.28571 loss)
I0630 19:03:12.100518 29777 sgd_solver.cpp:106] Iteration 318400, lr = 5e-05
I0630 19:03:28.309232 29777 solver.cpp:290] Iteration 318500 (6.1697 iter/s, 16.2082s/100 iter), loss = 1.33333
I0630 19:03:28.309294 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 19:03:28.309311 29777 sgd_solver.cpp:106] Iteration 318500, lr = 4.68749e-05
I0630 19:03:44.282394 29777 solver.cpp:290] Iteration 318600 (6.26071 iter/s, 15.9726s/100 iter), loss = 1.36905
I0630 19:03:44.282485 29777 solver.cpp:309]     Train net output #0: loss = 1.35714 (* 1 = 1.35714 loss)
I0630 19:03:44.282495 29777 sgd_solver.cpp:106] Iteration 318600, lr = 4.37498e-05
I0630 19:04:00.491417 29777 solver.cpp:290] Iteration 318700 (6.16962 iter/s, 16.2085s/100 iter), loss = 1.15476
I0630 19:04:00.491442 29777 solver.cpp:309]     Train net output #0: loss = 1.33333 (* 1 = 1.33333 loss)
I0630 19:04:00.491451 29777 sgd_solver.cpp:106] Iteration 318700, lr = 4.06247e-05
I0630 19:04:16.621531 29777 solver.cpp:290] Iteration 318800 (6.19979 iter/s, 16.1296s/100 iter), loss = 1.19048
I0630 19:04:16.621870 29777 solver.cpp:309]     Train net output #0: loss = 0.928571 (* 1 = 0.928571 loss)
I0630 19:04:16.621953 29777 sgd_solver.cpp:106] Iteration 318800, lr = 3.75003e-05
I0630 19:04:32.882120 29777 solver.cpp:290] Iteration 318900 (6.15014 iter/s, 16.2598s/100 iter), loss = 1.10714
I0630 19:04:32.882164 29777 solver.cpp:309]     Train net output #0: loss = 1.09524 (* 1 = 1.09524 loss)
I0630 19:04:32.882179 29777 sgd_solver.cpp:106] Iteration 318900, lr = 3.43752e-05
I0630 19:04:48.786305 29777 solver.cpp:354] Sparsity after update:
I0630 19:04:48.806515 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 19:04:48.806551 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 19:04:48.806569 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 19:04:48.806578 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 19:04:48.806586 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 19:04:48.806594 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 19:04:48.806602 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 19:04:48.806610 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 19:04:48.806618 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 19:04:48.806627 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 19:04:48.806634 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 19:04:48.806643 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 19:04:48.806650 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 19:04:48.959774 29777 solver.cpp:290] Iteration 319000 (6.22001 iter/s, 16.0771s/100 iter), loss = 0.952381
I0630 19:04:48.959800 29777 solver.cpp:309]     Train net output #0: loss = 1 (* 1 = 1 loss)
I0630 19:04:48.959810 29777 sgd_solver.cpp:106] Iteration 319000, lr = 3.12501e-05
I0630 19:05:04.986763 29777 solver.cpp:290] Iteration 319100 (6.23967 iter/s, 16.0265s/100 iter), loss = 1.10714
I0630 19:05:04.986788 29777 solver.cpp:309]     Train net output #0: loss = 1.38095 (* 1 = 1.38095 loss)
I0630 19:05:04.986795 29777 sgd_solver.cpp:106] Iteration 319100, lr = 2.8125e-05
I0630 19:05:21.170980 29777 solver.cpp:290] Iteration 319200 (6.17905 iter/s, 16.1837s/100 iter), loss = 1.13095
I0630 19:05:21.171088 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 19:05:21.171113 29777 sgd_solver.cpp:106] Iteration 319200, lr = 2.5e-05
I0630 19:05:37.269181 29777 solver.cpp:290] Iteration 319300 (6.2121 iter/s, 16.0976s/100 iter), loss = 1.58333
I0630 19:05:37.269204 29777 solver.cpp:309]     Train net output #0: loss = 1.45238 (* 1 = 1.45238 loss)
I0630 19:05:37.269213 29777 sgd_solver.cpp:106] Iteration 319300, lr = 2.18749e-05
I0630 19:05:53.399384 29777 solver.cpp:290] Iteration 319400 (6.19974 iter/s, 16.1297s/100 iter), loss = 1.57143
I0630 19:05:53.399457 29777 solver.cpp:309]     Train net output #0: loss = 1.92857 (* 1 = 1.92857 loss)
I0630 19:05:53.399468 29777 sgd_solver.cpp:106] Iteration 319400, lr = 1.87498e-05
I0630 19:06:09.489996 29777 solver.cpp:290] Iteration 319500 (6.21502 iter/s, 16.0901s/100 iter), loss = 0.928571
I0630 19:06:09.490020 29777 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 19:06:09.490026 29777 sgd_solver.cpp:106] Iteration 319500, lr = 1.56248e-05
I0630 19:06:25.680627 29777 solver.cpp:290] Iteration 319600 (6.1766 iter/s, 16.1901s/100 iter), loss = 0.928571
I0630 19:06:25.680721 29777 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 19:06:25.680730 29777 sgd_solver.cpp:106] Iteration 319600, lr = 1.25003e-05
I0630 19:06:41.789871 29777 solver.cpp:290] Iteration 319700 (6.20784 iter/s, 16.1087s/100 iter), loss = 0.797619
I0630 19:06:41.789902 29777 solver.cpp:309]     Train net output #0: loss = 0.857143 (* 1 = 0.857143 loss)
I0630 19:06:41.789912 29777 sgd_solver.cpp:106] Iteration 319700, lr = 9.37521e-06
I0630 19:06:58.019001 29777 solver.cpp:290] Iteration 319800 (6.16195 iter/s, 16.2286s/100 iter), loss = 1.25
I0630 19:06:58.019127 29777 solver.cpp:309]     Train net output #0: loss = 1.21429 (* 1 = 1.21429 loss)
I0630 19:06:58.019138 29777 sgd_solver.cpp:106] Iteration 319800, lr = 6.25014e-06
I0630 19:07:14.117492 29777 solver.cpp:290] Iteration 319900 (6.21199 iter/s, 16.0979s/100 iter), loss = 1.13095
I0630 19:07:14.117517 29777 solver.cpp:309]     Train net output #0: loss = 1.07143 (* 1 = 1.07143 loss)
I0630 19:07:14.117527 29777 sgd_solver.cpp:106] Iteration 319900, lr = 3.12507e-06
I0630 19:07:29.993223 29777 solver.cpp:598] Snapshotting to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_320000.caffemodel
I0630 19:07:30.012816 29777 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/imagenet_jacintonet11v2_2017-06-30_02-08-23/sparse/imagenet_jacintonet11v2_iter_320000.solverstate
I0630 19:07:30.021663 29777 solver.cpp:354] Sparsity after update:
I0630 19:07:30.022619 29777 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 19:07:30.022629 29777 net.cpp:1851] conv1a_param_0(0.405) 
I0630 19:07:30.022636 29777 net.cpp:1851] conv1b_param_0(0.725) 
I0630 19:07:30.022639 29777 net.cpp:1851] fc1000_param_0(0) 
I0630 19:07:30.022641 29777 net.cpp:1851] res2a_branch2a_param_0(0.81) 
I0630 19:07:30.022644 29777 net.cpp:1851] res2a_branch2b_param_0(0.81) 
I0630 19:07:30.022645 29777 net.cpp:1851] res3a_branch2a_param_0(0.81) 
I0630 19:07:30.022647 29777 net.cpp:1851] res3a_branch2b_param_0(0.81) 
I0630 19:07:30.022650 29777 net.cpp:1851] res4a_branch2a_param_0(0.81) 
I0630 19:07:30.022651 29777 net.cpp:1851] res4a_branch2b_param_0(0.81) 
I0630 19:07:30.022653 29777 net.cpp:1851] res5a_branch2a_param_0(0.81) 
I0630 19:07:30.022655 29777 net.cpp:1851] res5a_branch2b_param_0(0.81) 
I0630 19:07:30.022656 29777 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.9062e+06/2.86678e+06) 0.665
I0630 19:07:30.042140 29777 solver.cpp:451] Iteration 320000, loss = 1.7619
I0630 19:07:30.042160 29777 solver.cpp:471] Iteration 320000, Testing net (#0)
I0630 19:08:03.403156 29777 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 19:08:32.725651 29777 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.563039
I0630 19:08:32.725674 29777 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.796621
I0630 19:08:32.725680 29777 solver.cpp:544]     Test net output #2: loss = 1.5468 (* 1 = 1.5468 loss)
I0630 19:08:32.725683 29777 solver.cpp:456] Optimization Done.
I0630 19:08:32.835914 29777 caffe.cpp:246] Optimization Done.
training/imagenet_jacintonet11v2_2017-06-30_02-08-23/test
WARNING: gnome-keyring:: couldn't connect to: /run/user/30409/keyring-KJvviu/pkcs11: Connection refused
p11-kit: skipping module 'gnome-keyring' whose initialization failed: An error occurred on the device
I0630 19:08:39.011869 17469 caffe.cpp:264] Not using GPU #2 for single-GPU function
I0630 19:08:39.011992 17469 caffe.cpp:264] Not using GPU #1 for single-GPU function
I0630 19:08:39.596110 17469 caffe.cpp:273] Use GPU with device ID 0
I0630 19:08:39.596468 17469 caffe.cpp:277] GPU device name: GeForce GTX 1080
I0630 19:08:40.088613 17469 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 224
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1000"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc1000"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc1000"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0630 19:08:40.088737 17469 layer_factory.hpp:77] Creating layer data
I0630 19:08:40.089100 17469 net.cpp:98] Creating Layer data
I0630 19:08:40.089108 17469 net.cpp:413] data -> data
I0630 19:08:40.089125 17469 net.cpp:413] data -> label
I0630 19:08:40.089931 17508 db_lmdb.cpp:35] Opened lmdb ./data/ilsvrc12_val_lmdb
I0630 19:08:40.091754 17469 data_layer.cpp:78] ReshapePrefetch 50, 3, 224, 224
I0630 19:08:40.091794 17469 data_layer.cpp:83] output data size: 50,3,224,224
I0630 19:08:40.119148 17469 net.cpp:148] Setting up data
I0630 19:08:40.119174 17469 net.cpp:155] Top shape: 50 3 224 224 (7526400)
I0630 19:08:40.119179 17469 net.cpp:155] Top shape: 50 (50)
I0630 19:08:40.119182 17469 net.cpp:163] Memory required for data: 30105800
I0630 19:08:40.119192 17469 layer_factory.hpp:77] Creating layer label_data_1_split
I0630 19:08:40.119202 17469 net.cpp:98] Creating Layer label_data_1_split
I0630 19:08:40.119209 17469 net.cpp:439] label_data_1_split <- label
I0630 19:08:40.119218 17469 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0630 19:08:40.119227 17469 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0630 19:08:40.119233 17469 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0630 19:08:40.119280 17469 net.cpp:148] Setting up label_data_1_split
I0630 19:08:40.119287 17469 net.cpp:155] Top shape: 50 (50)
I0630 19:08:40.119290 17469 net.cpp:155] Top shape: 50 (50)
I0630 19:08:40.119294 17469 net.cpp:155] Top shape: 50 (50)
I0630 19:08:40.119297 17469 net.cpp:163] Memory required for data: 30106400
I0630 19:08:40.119300 17469 layer_factory.hpp:77] Creating layer data/bias
I0630 19:08:40.119307 17469 net.cpp:98] Creating Layer data/bias
I0630 19:08:40.119310 17469 net.cpp:439] data/bias <- data
I0630 19:08:40.119315 17469 net.cpp:413] data/bias -> data/bias
I0630 19:08:40.120271 17469 net.cpp:148] Setting up data/bias
I0630 19:08:40.120285 17469 net.cpp:155] Top shape: 50 3 224 224 (7526400)
I0630 19:08:40.120288 17469 net.cpp:163] Memory required for data: 60212000
I0630 19:08:40.120308 17469 layer_factory.hpp:77] Creating layer conv1a
I0630 19:08:40.120324 17469 net.cpp:98] Creating Layer conv1a
I0630 19:08:40.120329 17469 net.cpp:439] conv1a <- data/bias
I0630 19:08:40.120333 17469 net.cpp:413] conv1a -> conv1a
I0630 19:08:40.121731 17469 net.cpp:148] Setting up conv1a
I0630 19:08:40.121740 17469 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 19:08:40.121743 17469 net.cpp:163] Memory required for data: 140493600
I0630 19:08:40.121748 17469 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 19:08:40.121755 17469 net.cpp:98] Creating Layer conv1a/bn
I0630 19:08:40.121758 17469 net.cpp:439] conv1a/bn <- conv1a
I0630 19:08:40.121762 17469 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 19:08:40.122120 17469 net.cpp:148] Setting up conv1a/bn
I0630 19:08:40.122126 17469 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 19:08:40.122128 17469 net.cpp:163] Memory required for data: 220775200
I0630 19:08:40.122134 17469 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 19:08:40.122138 17469 net.cpp:98] Creating Layer conv1a/relu
I0630 19:08:40.122140 17469 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 19:08:40.122143 17469 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 19:08:40.122153 17469 net.cpp:148] Setting up conv1a/relu
I0630 19:08:40.122155 17469 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 19:08:40.122158 17469 net.cpp:163] Memory required for data: 301056800
I0630 19:08:40.122159 17469 layer_factory.hpp:77] Creating layer conv1b
I0630 19:08:40.122174 17469 net.cpp:98] Creating Layer conv1b
I0630 19:08:40.122176 17469 net.cpp:439] conv1b <- conv1a/bn
I0630 19:08:40.122179 17469 net.cpp:413] conv1b -> conv1b
I0630 19:08:40.122357 17469 net.cpp:148] Setting up conv1b
I0630 19:08:40.122362 17469 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 19:08:40.122364 17469 net.cpp:163] Memory required for data: 381338400
I0630 19:08:40.122369 17469 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 19:08:40.122372 17469 net.cpp:98] Creating Layer conv1b/bn
I0630 19:08:40.122375 17469 net.cpp:439] conv1b/bn <- conv1b
I0630 19:08:40.122377 17469 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 19:08:40.124981 17469 net.cpp:148] Setting up conv1b/bn
I0630 19:08:40.124989 17469 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 19:08:40.124992 17469 net.cpp:163] Memory required for data: 461620000
I0630 19:08:40.125002 17469 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 19:08:40.125008 17469 net.cpp:98] Creating Layer conv1b/relu
I0630 19:08:40.125011 17469 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 19:08:40.125015 17469 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 19:08:40.125020 17469 net.cpp:148] Setting up conv1b/relu
I0630 19:08:40.125025 17469 net.cpp:155] Top shape: 50 32 112 112 (20070400)
I0630 19:08:40.125028 17469 net.cpp:163] Memory required for data: 541901600
I0630 19:08:40.125036 17469 layer_factory.hpp:77] Creating layer pool1
I0630 19:08:40.125043 17469 net.cpp:98] Creating Layer pool1
I0630 19:08:40.125047 17469 net.cpp:439] pool1 <- conv1b/bn
I0630 19:08:40.125057 17469 net.cpp:413] pool1 -> pool1
I0630 19:08:40.125084 17469 net.cpp:148] Setting up pool1
I0630 19:08:40.125089 17469 net.cpp:155] Top shape: 50 32 56 56 (5017600)
I0630 19:08:40.125092 17469 net.cpp:163] Memory required for data: 561972000
I0630 19:08:40.125097 17469 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 19:08:40.125103 17469 net.cpp:98] Creating Layer res2a_branch2a
I0630 19:08:40.125108 17469 net.cpp:439] res2a_branch2a <- pool1
I0630 19:08:40.125113 17469 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 19:08:40.125603 17469 net.cpp:148] Setting up res2a_branch2a
I0630 19:08:40.125609 17469 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 19:08:40.125612 17469 net.cpp:163] Memory required for data: 602112800
I0630 19:08:40.125620 17469 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 19:08:40.125625 17469 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 19:08:40.125629 17469 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 19:08:40.125635 17469 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 19:08:40.125926 17469 net.cpp:148] Setting up res2a_branch2a/bn
I0630 19:08:40.125931 17469 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 19:08:40.125936 17469 net.cpp:163] Memory required for data: 642253600
I0630 19:08:40.125943 17469 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 19:08:40.125947 17469 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 19:08:40.125952 17469 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 19:08:40.125957 17469 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 19:08:40.125962 17469 net.cpp:148] Setting up res2a_branch2a/relu
I0630 19:08:40.125967 17469 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 19:08:40.125969 17469 net.cpp:163] Memory required for data: 682394400
I0630 19:08:40.125973 17469 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 19:08:40.125980 17469 net.cpp:98] Creating Layer res2a_branch2b
I0630 19:08:40.125984 17469 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 19:08:40.125988 17469 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 19:08:40.126296 17469 net.cpp:148] Setting up res2a_branch2b
I0630 19:08:40.126302 17469 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 19:08:40.126305 17469 net.cpp:163] Memory required for data: 722535200
I0630 19:08:40.126310 17469 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 19:08:40.126317 17469 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 19:08:40.126325 17469 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 19:08:40.126332 17469 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 19:08:40.126633 17469 net.cpp:148] Setting up res2a_branch2b/bn
I0630 19:08:40.126639 17469 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 19:08:40.126642 17469 net.cpp:163] Memory required for data: 762676000
I0630 19:08:40.126652 17469 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 19:08:40.126655 17469 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 19:08:40.126659 17469 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 19:08:40.126664 17469 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 19:08:40.126670 17469 net.cpp:148] Setting up res2a_branch2b/relu
I0630 19:08:40.126674 17469 net.cpp:155] Top shape: 50 64 56 56 (10035200)
I0630 19:08:40.126677 17469 net.cpp:163] Memory required for data: 802816800
I0630 19:08:40.126682 17469 layer_factory.hpp:77] Creating layer pool2
I0630 19:08:40.126687 17469 net.cpp:98] Creating Layer pool2
I0630 19:08:40.126690 17469 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 19:08:40.126695 17469 net.cpp:413] pool2 -> pool2
I0630 19:08:40.126715 17469 net.cpp:148] Setting up pool2
I0630 19:08:40.126720 17469 net.cpp:155] Top shape: 50 64 28 28 (2508800)
I0630 19:08:40.126724 17469 net.cpp:163] Memory required for data: 812852000
I0630 19:08:40.126727 17469 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 19:08:40.126739 17469 net.cpp:98] Creating Layer res3a_branch2a
I0630 19:08:40.126741 17469 net.cpp:439] res3a_branch2a <- pool2
I0630 19:08:40.126746 17469 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 19:08:40.129357 17469 net.cpp:148] Setting up res3a_branch2a
I0630 19:08:40.129369 17469 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 19:08:40.129371 17469 net.cpp:163] Memory required for data: 832922400
I0630 19:08:40.129377 17469 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 19:08:40.129384 17469 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 19:08:40.129389 17469 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 19:08:40.129395 17469 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 19:08:40.129670 17469 net.cpp:148] Setting up res3a_branch2a/bn
I0630 19:08:40.129676 17469 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 19:08:40.129680 17469 net.cpp:163] Memory required for data: 852992800
I0630 19:08:40.129689 17469 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 19:08:40.129694 17469 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 19:08:40.129698 17469 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 19:08:40.129704 17469 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 19:08:40.129710 17469 net.cpp:148] Setting up res3a_branch2a/relu
I0630 19:08:40.129714 17469 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 19:08:40.129719 17469 net.cpp:163] Memory required for data: 873063200
I0630 19:08:40.129722 17469 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 19:08:40.129729 17469 net.cpp:98] Creating Layer res3a_branch2b
I0630 19:08:40.129732 17469 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 19:08:40.129737 17469 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 19:08:40.130600 17469 net.cpp:148] Setting up res3a_branch2b
I0630 19:08:40.130606 17469 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 19:08:40.130609 17469 net.cpp:163] Memory required for data: 893133600
I0630 19:08:40.130615 17469 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 19:08:40.130622 17469 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 19:08:40.130626 17469 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 19:08:40.130631 17469 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 19:08:40.130915 17469 net.cpp:148] Setting up res3a_branch2b/bn
I0630 19:08:40.130923 17469 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 19:08:40.130925 17469 net.cpp:163] Memory required for data: 913204000
I0630 19:08:40.130942 17469 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 19:08:40.130946 17469 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 19:08:40.130951 17469 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 19:08:40.130956 17469 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 19:08:40.130962 17469 net.cpp:148] Setting up res3a_branch2b/relu
I0630 19:08:40.130966 17469 net.cpp:155] Top shape: 50 128 28 28 (5017600)
I0630 19:08:40.130970 17469 net.cpp:163] Memory required for data: 933274400
I0630 19:08:40.130973 17469 layer_factory.hpp:77] Creating layer pool3
I0630 19:08:40.130978 17469 net.cpp:98] Creating Layer pool3
I0630 19:08:40.130982 17469 net.cpp:439] pool3 <- res3a_branch2b/bn
I0630 19:08:40.130986 17469 net.cpp:413] pool3 -> pool3
I0630 19:08:40.131008 17469 net.cpp:148] Setting up pool3
I0630 19:08:40.131012 17469 net.cpp:155] Top shape: 50 128 14 14 (1254400)
I0630 19:08:40.131016 17469 net.cpp:163] Memory required for data: 938292000
I0630 19:08:40.131021 17469 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 19:08:40.131026 17469 net.cpp:98] Creating Layer res4a_branch2a
I0630 19:08:40.131029 17469 net.cpp:439] res4a_branch2a <- pool3
I0630 19:08:40.131034 17469 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 19:08:40.136967 17469 net.cpp:148] Setting up res4a_branch2a
I0630 19:08:40.136975 17469 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 19:08:40.136977 17469 net.cpp:163] Memory required for data: 948327200
I0630 19:08:40.136983 17469 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 19:08:40.136988 17469 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 19:08:40.136992 17469 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 19:08:40.136997 17469 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 19:08:40.137275 17469 net.cpp:148] Setting up res4a_branch2a/bn
I0630 19:08:40.137286 17469 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 19:08:40.137290 17469 net.cpp:163] Memory required for data: 958362400
I0630 19:08:40.137297 17469 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 19:08:40.137301 17469 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 19:08:40.137305 17469 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 19:08:40.137310 17469 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 19:08:40.137315 17469 net.cpp:148] Setting up res4a_branch2a/relu
I0630 19:08:40.137318 17469 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 19:08:40.137321 17469 net.cpp:163] Memory required for data: 968397600
I0630 19:08:40.137326 17469 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 19:08:40.137332 17469 net.cpp:98] Creating Layer res4a_branch2b
I0630 19:08:40.137336 17469 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 19:08:40.137341 17469 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 19:08:40.140383 17469 net.cpp:148] Setting up res4a_branch2b
I0630 19:08:40.140390 17469 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 19:08:40.140394 17469 net.cpp:163] Memory required for data: 978432800
I0630 19:08:40.140400 17469 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 19:08:40.140405 17469 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 19:08:40.140410 17469 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 19:08:40.140415 17469 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 19:08:40.140694 17469 net.cpp:148] Setting up res4a_branch2b/bn
I0630 19:08:40.140700 17469 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 19:08:40.140704 17469 net.cpp:163] Memory required for data: 988468000
I0630 19:08:40.140712 17469 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 19:08:40.140717 17469 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 19:08:40.140720 17469 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 19:08:40.140724 17469 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 19:08:40.140730 17469 net.cpp:148] Setting up res4a_branch2b/relu
I0630 19:08:40.140740 17469 net.cpp:155] Top shape: 50 256 14 14 (2508800)
I0630 19:08:40.140743 17469 net.cpp:163] Memory required for data: 998503200
I0630 19:08:40.140748 17469 layer_factory.hpp:77] Creating layer pool4
I0630 19:08:40.140753 17469 net.cpp:98] Creating Layer pool4
I0630 19:08:40.140755 17469 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 19:08:40.140759 17469 net.cpp:413] pool4 -> pool4
I0630 19:08:40.140785 17469 net.cpp:148] Setting up pool4
I0630 19:08:40.140790 17469 net.cpp:155] Top shape: 50 256 7 7 (627200)
I0630 19:08:40.140794 17469 net.cpp:163] Memory required for data: 1001012000
I0630 19:08:40.140797 17469 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 19:08:40.140805 17469 net.cpp:98] Creating Layer res5a_branch2a
I0630 19:08:40.140807 17469 net.cpp:439] res5a_branch2a <- pool4
I0630 19:08:40.140812 17469 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 19:08:40.165145 17469 net.cpp:148] Setting up res5a_branch2a
I0630 19:08:40.165168 17469 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 19:08:40.165171 17469 net.cpp:163] Memory required for data: 1006029600
I0630 19:08:40.165179 17469 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 19:08:40.165187 17469 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 19:08:40.165192 17469 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 19:08:40.165199 17469 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 19:08:40.165496 17469 net.cpp:148] Setting up res5a_branch2a/bn
I0630 19:08:40.165503 17469 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 19:08:40.165508 17469 net.cpp:163] Memory required for data: 1011047200
I0630 19:08:40.165515 17469 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 19:08:40.165520 17469 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 19:08:40.165524 17469 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 19:08:40.165529 17469 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 19:08:40.165536 17469 net.cpp:148] Setting up res5a_branch2a/relu
I0630 19:08:40.165541 17469 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 19:08:40.165544 17469 net.cpp:163] Memory required for data: 1016064800
I0630 19:08:40.165549 17469 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 19:08:40.165555 17469 net.cpp:98] Creating Layer res5a_branch2b
I0630 19:08:40.165558 17469 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 19:08:40.165563 17469 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 19:08:40.177917 17469 net.cpp:148] Setting up res5a_branch2b
I0630 19:08:40.177932 17469 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 19:08:40.177937 17469 net.cpp:163] Memory required for data: 1021082400
I0630 19:08:40.177945 17469 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 19:08:40.177954 17469 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 19:08:40.177960 17469 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 19:08:40.177968 17469 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 19:08:40.178350 17469 net.cpp:148] Setting up res5a_branch2b/bn
I0630 19:08:40.178359 17469 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 19:08:40.178361 17469 net.cpp:163] Memory required for data: 1026100000
I0630 19:08:40.178366 17469 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 19:08:40.178371 17469 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 19:08:40.178375 17469 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 19:08:40.178376 17469 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 19:08:40.178381 17469 net.cpp:148] Setting up res5a_branch2b/relu
I0630 19:08:40.178383 17469 net.cpp:155] Top shape: 50 512 7 7 (1254400)
I0630 19:08:40.178385 17469 net.cpp:163] Memory required for data: 1031117600
I0630 19:08:40.178387 17469 layer_factory.hpp:77] Creating layer pool5
I0630 19:08:40.178393 17469 net.cpp:98] Creating Layer pool5
I0630 19:08:40.178395 17469 net.cpp:439] pool5 <- res5a_branch2b/bn
I0630 19:08:40.178398 17469 net.cpp:413] pool5 -> pool5
I0630 19:08:40.178436 17469 net.cpp:148] Setting up pool5
I0630 19:08:40.178441 17469 net.cpp:155] Top shape: 50 512 1 1 (25600)
I0630 19:08:40.178442 17469 net.cpp:163] Memory required for data: 1031220000
I0630 19:08:40.178444 17469 layer_factory.hpp:77] Creating layer fc1000
I0630 19:08:40.178449 17469 net.cpp:98] Creating Layer fc1000
I0630 19:08:40.178452 17469 net.cpp:439] fc1000 <- pool5
I0630 19:08:40.178454 17469 net.cpp:413] fc1000 -> fc1000
I0630 19:08:40.189321 17469 net.cpp:148] Setting up fc1000
I0630 19:08:40.189339 17469 net.cpp:155] Top shape: 50 1000 (50000)
I0630 19:08:40.189342 17469 net.cpp:163] Memory required for data: 1031420000
I0630 19:08:40.189347 17469 layer_factory.hpp:77] Creating layer fc1000_fc1000_0_split
I0630 19:08:40.189352 17469 net.cpp:98] Creating Layer fc1000_fc1000_0_split
I0630 19:08:40.189354 17469 net.cpp:439] fc1000_fc1000_0_split <- fc1000
I0630 19:08:40.189359 17469 net.cpp:413] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_0
I0630 19:08:40.189368 17469 net.cpp:413] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_1
I0630 19:08:40.189373 17469 net.cpp:413] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_2
I0630 19:08:40.189407 17469 net.cpp:148] Setting up fc1000_fc1000_0_split
I0630 19:08:40.189411 17469 net.cpp:155] Top shape: 50 1000 (50000)
I0630 19:08:40.189414 17469 net.cpp:155] Top shape: 50 1000 (50000)
I0630 19:08:40.189415 17469 net.cpp:155] Top shape: 50 1000 (50000)
I0630 19:08:40.189417 17469 net.cpp:163] Memory required for data: 1032020000
I0630 19:08:40.189419 17469 layer_factory.hpp:77] Creating layer loss
I0630 19:08:40.189427 17469 net.cpp:98] Creating Layer loss
I0630 19:08:40.189429 17469 net.cpp:439] loss <- fc1000_fc1000_0_split_0
I0630 19:08:40.189431 17469 net.cpp:439] loss <- label_data_1_split_0
I0630 19:08:40.189435 17469 net.cpp:413] loss -> loss
I0630 19:08:40.189440 17469 layer_factory.hpp:77] Creating layer loss
I0630 19:08:40.189534 17469 net.cpp:148] Setting up loss
I0630 19:08:40.189538 17469 net.cpp:155] Top shape: (1)
I0630 19:08:40.189540 17469 net.cpp:158]     with loss weight 1
I0630 19:08:40.189553 17469 net.cpp:163] Memory required for data: 1032020004
I0630 19:08:40.189554 17469 layer_factory.hpp:77] Creating layer accuracy/top1
I0630 19:08:40.189561 17469 net.cpp:98] Creating Layer accuracy/top1
I0630 19:08:40.189563 17469 net.cpp:439] accuracy/top1 <- fc1000_fc1000_0_split_1
I0630 19:08:40.189565 17469 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0630 19:08:40.189568 17469 net.cpp:413] accuracy/top1 -> accuracy/top1
I0630 19:08:40.189574 17469 net.cpp:148] Setting up accuracy/top1
I0630 19:08:40.189577 17469 net.cpp:155] Top shape: (1)
I0630 19:08:40.189579 17469 net.cpp:163] Memory required for data: 1032020008
I0630 19:08:40.189580 17469 layer_factory.hpp:77] Creating layer accuracy/top5
I0630 19:08:40.189584 17469 net.cpp:98] Creating Layer accuracy/top5
I0630 19:08:40.189586 17469 net.cpp:439] accuracy/top5 <- fc1000_fc1000_0_split_2
I0630 19:08:40.189589 17469 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0630 19:08:40.189591 17469 net.cpp:413] accuracy/top5 -> accuracy/top5
I0630 19:08:40.189596 17469 net.cpp:148] Setting up accuracy/top5
I0630 19:08:40.189599 17469 net.cpp:155] Top shape: (1)
I0630 19:08:40.189601 17469 net.cpp:163] Memory required for data: 1032020012
I0630 19:08:40.189604 17469 net.cpp:226] accuracy/top5 does not need backward computation.
I0630 19:08:40.189606 17469 net.cpp:226] accuracy/top1 does not need backward computation.
I0630 19:08:40.189610 17469 net.cpp:224] loss needs backward computation.
I0630 19:08:40.189611 17469 net.cpp:224] fc1000_fc1000_0_split needs backward computation.
I0630 19:08:40.189613 17469 net.cpp:224] fc1000 needs backward computation.
I0630 19:08:40.189615 17469 net.cpp:224] pool5 needs backward computation.
I0630 19:08:40.189618 17469 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 19:08:40.189620 17469 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 19:08:40.189622 17469 net.cpp:224] res5a_branch2b needs backward computation.
I0630 19:08:40.189633 17469 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 19:08:40.189636 17469 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 19:08:40.189638 17469 net.cpp:224] res5a_branch2a needs backward computation.
I0630 19:08:40.189641 17469 net.cpp:224] pool4 needs backward computation.
I0630 19:08:40.189643 17469 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 19:08:40.189646 17469 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 19:08:40.189648 17469 net.cpp:224] res4a_branch2b needs backward computation.
I0630 19:08:40.189651 17469 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 19:08:40.189653 17469 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 19:08:40.189656 17469 net.cpp:224] res4a_branch2a needs backward computation.
I0630 19:08:40.189658 17469 net.cpp:224] pool3 needs backward computation.
I0630 19:08:40.189661 17469 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 19:08:40.189662 17469 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 19:08:40.189666 17469 net.cpp:224] res3a_branch2b needs backward computation.
I0630 19:08:40.189668 17469 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 19:08:40.189671 17469 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 19:08:40.189672 17469 net.cpp:224] res3a_branch2a needs backward computation.
I0630 19:08:40.189674 17469 net.cpp:224] pool2 needs backward computation.
I0630 19:08:40.189677 17469 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 19:08:40.189678 17469 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 19:08:40.189682 17469 net.cpp:224] res2a_branch2b needs backward computation.
I0630 19:08:40.189683 17469 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 19:08:40.189687 17469 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 19:08:40.189687 17469 net.cpp:224] res2a_branch2a needs backward computation.
I0630 19:08:40.189690 17469 net.cpp:224] pool1 needs backward computation.
I0630 19:08:40.189692 17469 net.cpp:224] conv1b/relu needs backward computation.
I0630 19:08:40.189694 17469 net.cpp:224] conv1b/bn needs backward computation.
I0630 19:08:40.189697 17469 net.cpp:224] conv1b needs backward computation.
I0630 19:08:40.189699 17469 net.cpp:224] conv1a/relu needs backward computation.
I0630 19:08:40.189702 17469 net.cpp:224] conv1a/bn needs backward computation.
I0630 19:08:40.189704 17469 net.cpp:224] conv1a needs backward computation.
I0630 19:08:40.189707 17469 net.cpp:226] data/bias does not need backward computation.
I0630 19:08:40.189710 17469 net.cpp:226] label_data_1_split does not need backward computation.
I0630 19:08:40.189713 17469 net.cpp:226] data does not need backward computation.
I0630 19:08:40.189715 17469 net.cpp:268] This network produces output accuracy/top1
I0630 19:08:40.189718 17469 net.cpp:268] This network produces output accuracy/top5
I0630 19:08:40.189719 17469 net.cpp:268] This network produces output loss
I0630 19:08:40.189738 17469 net.cpp:288] Network initialization done.
I0630 19:08:40.202214 17469 caffe.cpp:289] Running for 1000 iterations.
I0630 19:08:40.244678 17469 caffe.cpp:312] Batch 0, accuracy/top1 = 0.56
I0630 19:08:40.244706 17469 caffe.cpp:312] Batch 0, accuracy/top5 = 0.8
I0630 19:08:40.244709 17469 caffe.cpp:312] Batch 0, loss = 1.6
I0630 19:08:40.266147 17469 caffe.cpp:312] Batch 1, accuracy/top1 = 0.64
I0630 19:08:40.266161 17469 caffe.cpp:312] Batch 1, accuracy/top5 = 0.74
I0630 19:08:40.266165 17469 caffe.cpp:312] Batch 1, loss = 1.54
I0630 19:08:40.266168 17469 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 19:08:40.301669 17469 caffe.cpp:312] Batch 2, accuracy/top1 = 0.64
I0630 19:08:40.301692 17469 caffe.cpp:312] Batch 2, accuracy/top5 = 0.72
I0630 19:08:40.301694 17469 caffe.cpp:312] Batch 2, loss = 1.96
I0630 19:08:40.349798 17469 caffe.cpp:312] Batch 3, accuracy/top1 = 0.68
I0630 19:08:40.349823 17469 caffe.cpp:312] Batch 3, accuracy/top5 = 0.82
I0630 19:08:40.349840 17469 caffe.cpp:312] Batch 3, loss = 1.14
I0630 19:08:40.397503 17469 caffe.cpp:312] Batch 4, accuracy/top1 = 0.56
I0630 19:08:40.397526 17469 caffe.cpp:312] Batch 4, accuracy/top5 = 0.82
I0630 19:08:40.397528 17469 caffe.cpp:312] Batch 4, loss = 1.52
I0630 19:08:40.445988 17469 caffe.cpp:312] Batch 5, accuracy/top1 = 0.7
I0630 19:08:40.446012 17469 caffe.cpp:312] Batch 5, accuracy/top5 = 0.88
I0630 19:08:40.446015 17469 caffe.cpp:312] Batch 5, loss = 1.1
I0630 19:08:40.495039 17469 caffe.cpp:312] Batch 6, accuracy/top1 = 0.56
I0630 19:08:40.495060 17469 caffe.cpp:312] Batch 6, accuracy/top5 = 0.86
I0630 19:08:40.495064 17469 caffe.cpp:312] Batch 6, loss = 1.34
I0630 19:08:40.544193 17469 caffe.cpp:312] Batch 7, accuracy/top1 = 0.56
I0630 19:08:40.544215 17469 caffe.cpp:312] Batch 7, accuracy/top5 = 0.82
I0630 19:08:40.544219 17469 caffe.cpp:312] Batch 7, loss = 1.68
I0630 19:08:40.593129 17469 caffe.cpp:312] Batch 8, accuracy/top1 = 0.7
I0630 19:08:40.593150 17469 caffe.cpp:312] Batch 8, accuracy/top5 = 0.9
I0630 19:08:40.593153 17469 caffe.cpp:312] Batch 8, loss = 0.9
I0630 19:08:40.641949 17469 caffe.cpp:312] Batch 9, accuracy/top1 = 0.46
I0630 19:08:40.641971 17469 caffe.cpp:312] Batch 9, accuracy/top5 = 0.8
I0630 19:08:40.641975 17469 caffe.cpp:312] Batch 9, loss = 1.46
I0630 19:08:40.690959 17469 caffe.cpp:312] Batch 10, accuracy/top1 = 0.54
I0630 19:08:40.690981 17469 caffe.cpp:312] Batch 10, accuracy/top5 = 0.84
I0630 19:08:40.690984 17469 caffe.cpp:312] Batch 10, loss = 1.38
I0630 19:08:40.741235 17469 caffe.cpp:312] Batch 11, accuracy/top1 = 0.62
I0630 19:08:40.741261 17469 caffe.cpp:312] Batch 11, accuracy/top5 = 0.88
I0630 19:08:40.741263 17469 caffe.cpp:312] Batch 11, loss = 1.26
I0630 19:08:40.788991 17469 caffe.cpp:312] Batch 12, accuracy/top1 = 0.62
I0630 19:08:40.789013 17469 caffe.cpp:312] Batch 12, accuracy/top5 = 0.88
I0630 19:08:40.789016 17469 caffe.cpp:312] Batch 12, loss = 1.32
I0630 19:08:40.839210 17469 caffe.cpp:312] Batch 13, accuracy/top1 = 0.62
I0630 19:08:40.839231 17469 caffe.cpp:312] Batch 13, accuracy/top5 = 0.82
I0630 19:08:40.839236 17469 caffe.cpp:312] Batch 13, loss = 1.4
I0630 19:08:40.888234 17469 caffe.cpp:312] Batch 14, accuracy/top1 = 0.5
I0630 19:08:40.888255 17469 caffe.cpp:312] Batch 14, accuracy/top5 = 0.78
I0630 19:08:40.888259 17469 caffe.cpp:312] Batch 14, loss = 1.76
I0630 19:08:40.939169 17469 caffe.cpp:312] Batch 15, accuracy/top1 = 0.46
I0630 19:08:40.939191 17469 caffe.cpp:312] Batch 15, accuracy/top5 = 0.64
I0630 19:08:40.939196 17469 caffe.cpp:312] Batch 15, loss = 2.68
I0630 19:08:40.990077 17469 caffe.cpp:312] Batch 16, accuracy/top1 = 0.48
I0630 19:08:40.990097 17469 caffe.cpp:312] Batch 16, accuracy/top5 = 0.72
I0630 19:08:40.990101 17469 caffe.cpp:312] Batch 16, loss = 2
I0630 19:08:41.041025 17469 caffe.cpp:312] Batch 17, accuracy/top1 = 0.48
I0630 19:08:41.041049 17469 caffe.cpp:312] Batch 17, accuracy/top5 = 0.78
I0630 19:08:41.041054 17469 caffe.cpp:312] Batch 17, loss = 1.7
I0630 19:08:41.090518 17469 caffe.cpp:312] Batch 18, accuracy/top1 = 0.52
I0630 19:08:41.090539 17469 caffe.cpp:312] Batch 18, accuracy/top5 = 0.78
I0630 19:08:41.090544 17469 caffe.cpp:312] Batch 18, loss = 1.84
I0630 19:08:41.140123 17469 caffe.cpp:312] Batch 19, accuracy/top1 = 0.6
I0630 19:08:41.140146 17469 caffe.cpp:312] Batch 19, accuracy/top5 = 0.84
I0630 19:08:41.140151 17469 caffe.cpp:312] Batch 19, loss = 1.32
I0630 19:08:41.189074 17469 caffe.cpp:312] Batch 20, accuracy/top1 = 0.5
I0630 19:08:41.189095 17469 caffe.cpp:312] Batch 20, accuracy/top5 = 0.74
I0630 19:08:41.189100 17469 caffe.cpp:312] Batch 20, loss = 2.02
I0630 19:08:41.238345 17469 caffe.cpp:312] Batch 21, accuracy/top1 = 0.62
I0630 19:08:41.238370 17469 caffe.cpp:312] Batch 21, accuracy/top5 = 0.8
I0630 19:08:41.238374 17469 caffe.cpp:312] Batch 21, loss = 1.62
I0630 19:08:41.289345 17469 caffe.cpp:312] Batch 22, accuracy/top1 = 0.5
I0630 19:08:41.289366 17469 caffe.cpp:312] Batch 22, accuracy/top5 = 0.8
I0630 19:08:41.289371 17469 caffe.cpp:312] Batch 22, loss = 1.96
I0630 19:08:41.339869 17469 caffe.cpp:312] Batch 23, accuracy/top1 = 0.56
I0630 19:08:41.339907 17469 caffe.cpp:312] Batch 23, accuracy/top5 = 0.82
I0630 19:08:41.339912 17469 caffe.cpp:312] Batch 23, loss = 1.26
I0630 19:08:41.391023 17469 caffe.cpp:312] Batch 24, accuracy/top1 = 0.56
I0630 19:08:41.391044 17469 caffe.cpp:312] Batch 24, accuracy/top5 = 0.86
I0630 19:08:41.391048 17469 caffe.cpp:312] Batch 24, loss = 1.26
I0630 19:08:41.441539 17469 caffe.cpp:312] Batch 25, accuracy/top1 = 0.62
I0630 19:08:41.441562 17469 caffe.cpp:312] Batch 25, accuracy/top5 = 0.82
I0630 19:08:41.441566 17469 caffe.cpp:312] Batch 25, loss = 1.42
I0630 19:08:41.492012 17469 caffe.cpp:312] Batch 26, accuracy/top1 = 0.62
I0630 19:08:41.492034 17469 caffe.cpp:312] Batch 26, accuracy/top5 = 0.82
I0630 19:08:41.492038 17469 caffe.cpp:312] Batch 26, loss = 1.7
I0630 19:08:41.540422 17469 caffe.cpp:312] Batch 27, accuracy/top1 = 0.44
I0630 19:08:41.540446 17469 caffe.cpp:312] Batch 27, accuracy/top5 = 0.66
I0630 19:08:41.540451 17469 caffe.cpp:312] Batch 27, loss = 2.74
I0630 19:08:41.590898 17469 caffe.cpp:312] Batch 28, accuracy/top1 = 0.6
I0630 19:08:41.590920 17469 caffe.cpp:312] Batch 28, accuracy/top5 = 0.82
I0630 19:08:41.590924 17469 caffe.cpp:312] Batch 28, loss = 1.52
I0630 19:08:41.641832 17469 caffe.cpp:312] Batch 29, accuracy/top1 = 0.54
I0630 19:08:41.641855 17469 caffe.cpp:312] Batch 29, accuracy/top5 = 0.8
I0630 19:08:41.641860 17469 caffe.cpp:312] Batch 29, loss = 1.7
I0630 19:08:41.690521 17469 caffe.cpp:312] Batch 30, accuracy/top1 = 0.6
I0630 19:08:41.690543 17469 caffe.cpp:312] Batch 30, accuracy/top5 = 0.78
I0630 19:08:41.690548 17469 caffe.cpp:312] Batch 30, loss = 1.72
I0630 19:08:41.740180 17469 caffe.cpp:312] Batch 31, accuracy/top1 = 0.6
I0630 19:08:41.740200 17469 caffe.cpp:312] Batch 31, accuracy/top5 = 0.82
I0630 19:08:41.740206 17469 caffe.cpp:312] Batch 31, loss = 1.6
I0630 19:08:41.788614 17469 caffe.cpp:312] Batch 32, accuracy/top1 = 0.54
I0630 19:08:41.788635 17469 caffe.cpp:312] Batch 32, accuracy/top5 = 0.72
I0630 19:08:41.788638 17469 caffe.cpp:312] Batch 32, loss = 1.62
I0630 19:08:41.837862 17469 caffe.cpp:312] Batch 33, accuracy/top1 = 0.48
I0630 19:08:41.837887 17469 caffe.cpp:312] Batch 33, accuracy/top5 = 0.82
I0630 19:08:41.837890 17469 caffe.cpp:312] Batch 33, loss = 1.5
I0630 19:08:41.887758 17469 caffe.cpp:312] Batch 34, accuracy/top1 = 0.6
I0630 19:08:41.887778 17469 caffe.cpp:312] Batch 34, accuracy/top5 = 0.78
I0630 19:08:41.887783 17469 caffe.cpp:312] Batch 34, loss = 1.64
I0630 19:08:41.938735 17469 caffe.cpp:312] Batch 35, accuracy/top1 = 0.58
I0630 19:08:41.938756 17469 caffe.cpp:312] Batch 35, accuracy/top5 = 0.78
I0630 19:08:41.938760 17469 caffe.cpp:312] Batch 35, loss = 1.34
I0630 19:08:41.989260 17469 caffe.cpp:312] Batch 36, accuracy/top1 = 0.6
I0630 19:08:41.989281 17469 caffe.cpp:312] Batch 36, accuracy/top5 = 0.74
I0630 19:08:41.989285 17469 caffe.cpp:312] Batch 36, loss = 1.72
I0630 19:08:42.040127 17469 caffe.cpp:312] Batch 37, accuracy/top1 = 0.62
I0630 19:08:42.040150 17469 caffe.cpp:312] Batch 37, accuracy/top5 = 0.86
I0630 19:08:42.040155 17469 caffe.cpp:312] Batch 37, loss = 1.14
I0630 19:08:42.089323 17469 caffe.cpp:312] Batch 38, accuracy/top1 = 0.46
I0630 19:08:42.089344 17469 caffe.cpp:312] Batch 38, accuracy/top5 = 0.72
I0630 19:08:42.089349 17469 caffe.cpp:312] Batch 38, loss = 2.08
I0630 19:08:42.139235 17469 caffe.cpp:312] Batch 39, accuracy/top1 = 0.64
I0630 19:08:42.139257 17469 caffe.cpp:312] Batch 39, accuracy/top5 = 0.86
I0630 19:08:42.139261 17469 caffe.cpp:312] Batch 39, loss = 1.22
I0630 19:08:42.189127 17469 caffe.cpp:312] Batch 40, accuracy/top1 = 0.44
I0630 19:08:42.189149 17469 caffe.cpp:312] Batch 40, accuracy/top5 = 0.7
I0630 19:08:42.189154 17469 caffe.cpp:312] Batch 40, loss = 2
I0630 19:08:42.239305 17469 caffe.cpp:312] Batch 41, accuracy/top1 = 0.58
I0630 19:08:42.239327 17469 caffe.cpp:312] Batch 41, accuracy/top5 = 0.8
I0630 19:08:42.239331 17469 caffe.cpp:312] Batch 41, loss = 1.4
I0630 19:08:42.289508 17469 caffe.cpp:312] Batch 42, accuracy/top1 = 0.66
I0630 19:08:42.289530 17469 caffe.cpp:312] Batch 42, accuracy/top5 = 0.8
I0630 19:08:42.289551 17469 caffe.cpp:312] Batch 42, loss = 1.7
I0630 19:08:42.339998 17469 caffe.cpp:312] Batch 43, accuracy/top1 = 0.64
I0630 19:08:42.340020 17469 caffe.cpp:312] Batch 43, accuracy/top5 = 0.88
I0630 19:08:42.340024 17469 caffe.cpp:312] Batch 43, loss = 1.08
I0630 19:08:42.390497 17469 caffe.cpp:312] Batch 44, accuracy/top1 = 0.58
I0630 19:08:42.390519 17469 caffe.cpp:312] Batch 44, accuracy/top5 = 0.7
I0630 19:08:42.390524 17469 caffe.cpp:312] Batch 44, loss = 1.52
I0630 19:08:42.440784 17469 caffe.cpp:312] Batch 45, accuracy/top1 = 0.54
I0630 19:08:42.440807 17469 caffe.cpp:312] Batch 45, accuracy/top5 = 0.74
I0630 19:08:42.440812 17469 caffe.cpp:312] Batch 45, loss = 1.82
I0630 19:08:42.491088 17469 caffe.cpp:312] Batch 46, accuracy/top1 = 0.62
I0630 19:08:42.491111 17469 caffe.cpp:312] Batch 46, accuracy/top5 = 0.86
I0630 19:08:42.491116 17469 caffe.cpp:312] Batch 46, loss = 1.18
I0630 19:08:42.540572 17469 caffe.cpp:312] Batch 47, accuracy/top1 = 0.5
I0630 19:08:42.540596 17469 caffe.cpp:312] Batch 47, accuracy/top5 = 0.8
I0630 19:08:42.540599 17469 caffe.cpp:312] Batch 47, loss = 1.64
I0630 19:08:42.590757 17469 caffe.cpp:312] Batch 48, accuracy/top1 = 0.54
I0630 19:08:42.590778 17469 caffe.cpp:312] Batch 48, accuracy/top5 = 0.78
I0630 19:08:42.590782 17469 caffe.cpp:312] Batch 48, loss = 1.64
I0630 19:08:42.640641 17469 caffe.cpp:312] Batch 49, accuracy/top1 = 0.6
I0630 19:08:42.640664 17469 caffe.cpp:312] Batch 49, accuracy/top5 = 0.82
I0630 19:08:42.640668 17469 caffe.cpp:312] Batch 49, loss = 1.34
I0630 19:08:42.691726 17469 caffe.cpp:312] Batch 50, accuracy/top1 = 0.62
I0630 19:08:42.691747 17469 caffe.cpp:312] Batch 50, accuracy/top5 = 0.78
I0630 19:08:42.691752 17469 caffe.cpp:312] Batch 50, loss = 1.36
I0630 19:08:42.742645 17469 caffe.cpp:312] Batch 51, accuracy/top1 = 0.44
I0630 19:08:42.742669 17469 caffe.cpp:312] Batch 51, accuracy/top5 = 0.82
I0630 19:08:42.742673 17469 caffe.cpp:312] Batch 51, loss = 1.58
I0630 19:08:42.792547 17469 caffe.cpp:312] Batch 52, accuracy/top1 = 0.62
I0630 19:08:42.792570 17469 caffe.cpp:312] Batch 52, accuracy/top5 = 0.76
I0630 19:08:42.792574 17469 caffe.cpp:312] Batch 52, loss = 1.56
I0630 19:08:42.843261 17469 caffe.cpp:312] Batch 53, accuracy/top1 = 0.5
I0630 19:08:42.843286 17469 caffe.cpp:312] Batch 53, accuracy/top5 = 0.66
I0630 19:08:42.843291 17469 caffe.cpp:312] Batch 53, loss = 2
I0630 19:08:42.892985 17469 caffe.cpp:312] Batch 54, accuracy/top1 = 0.64
I0630 19:08:42.893007 17469 caffe.cpp:312] Batch 54, accuracy/top5 = 0.9
I0630 19:08:42.893012 17469 caffe.cpp:312] Batch 54, loss = 1.06
I0630 19:08:42.941460 17469 caffe.cpp:312] Batch 55, accuracy/top1 = 0.68
I0630 19:08:42.941484 17469 caffe.cpp:312] Batch 55, accuracy/top5 = 0.9
I0630 19:08:42.941488 17469 caffe.cpp:312] Batch 55, loss = 1.1
I0630 19:08:42.990401 17469 caffe.cpp:312] Batch 56, accuracy/top1 = 0.58
I0630 19:08:42.990422 17469 caffe.cpp:312] Batch 56, accuracy/top5 = 0.82
I0630 19:08:42.990427 17469 caffe.cpp:312] Batch 56, loss = 1.26
I0630 19:08:43.038861 17469 caffe.cpp:312] Batch 57, accuracy/top1 = 0.6
I0630 19:08:43.038883 17469 caffe.cpp:312] Batch 57, accuracy/top5 = 0.8
I0630 19:08:43.038887 17469 caffe.cpp:312] Batch 57, loss = 1.54
I0630 19:08:43.088472 17469 caffe.cpp:312] Batch 58, accuracy/top1 = 0.6
I0630 19:08:43.088495 17469 caffe.cpp:312] Batch 58, accuracy/top5 = 0.82
I0630 19:08:43.088500 17469 caffe.cpp:312] Batch 58, loss = 1.32
I0630 19:08:43.137936 17469 caffe.cpp:312] Batch 59, accuracy/top1 = 0.7
I0630 19:08:43.137959 17469 caffe.cpp:312] Batch 59, accuracy/top5 = 0.82
I0630 19:08:43.137964 17469 caffe.cpp:312] Batch 59, loss = 1.12
I0630 19:08:43.187772 17469 caffe.cpp:312] Batch 60, accuracy/top1 = 0.58
I0630 19:08:43.187794 17469 caffe.cpp:312] Batch 60, accuracy/top5 = 0.8
I0630 19:08:43.187798 17469 caffe.cpp:312] Batch 60, loss = 1.22
I0630 19:08:43.235985 17469 caffe.cpp:312] Batch 61, accuracy/top1 = 0.54
I0630 19:08:43.236008 17469 caffe.cpp:312] Batch 61, accuracy/top5 = 0.84
I0630 19:08:43.236030 17469 caffe.cpp:312] Batch 61, loss = 1.54
I0630 19:08:43.286031 17469 caffe.cpp:312] Batch 62, accuracy/top1 = 0.52
I0630 19:08:43.286054 17469 caffe.cpp:312] Batch 62, accuracy/top5 = 0.8
I0630 19:08:43.286058 17469 caffe.cpp:312] Batch 62, loss = 1.5
I0630 19:08:43.335613 17469 caffe.cpp:312] Batch 63, accuracy/top1 = 0.52
I0630 19:08:43.335634 17469 caffe.cpp:312] Batch 63, accuracy/top5 = 0.76
I0630 19:08:43.335639 17469 caffe.cpp:312] Batch 63, loss = 1.5
I0630 19:08:43.387480 17469 caffe.cpp:312] Batch 64, accuracy/top1 = 0.48
I0630 19:08:43.387503 17469 caffe.cpp:312] Batch 64, accuracy/top5 = 0.72
I0630 19:08:43.387507 17469 caffe.cpp:312] Batch 64, loss = 1.94
I0630 19:08:43.436051 17469 caffe.cpp:312] Batch 65, accuracy/top1 = 0.54
I0630 19:08:43.436074 17469 caffe.cpp:312] Batch 65, accuracy/top5 = 0.8
I0630 19:08:43.436077 17469 caffe.cpp:312] Batch 65, loss = 1.74
I0630 19:08:43.484372 17469 caffe.cpp:312] Batch 66, accuracy/top1 = 0.64
I0630 19:08:43.484395 17469 caffe.cpp:312] Batch 66, accuracy/top5 = 0.9
I0630 19:08:43.484400 17469 caffe.cpp:312] Batch 66, loss = 1.26
I0630 19:08:43.533715 17469 caffe.cpp:312] Batch 67, accuracy/top1 = 0.54
I0630 19:08:43.533737 17469 caffe.cpp:312] Batch 67, accuracy/top5 = 0.82
I0630 19:08:43.533742 17469 caffe.cpp:312] Batch 67, loss = 1.32
I0630 19:08:43.582223 17469 caffe.cpp:312] Batch 68, accuracy/top1 = 0.66
I0630 19:08:43.582245 17469 caffe.cpp:312] Batch 68, accuracy/top5 = 0.86
I0630 19:08:43.582249 17469 caffe.cpp:312] Batch 68, loss = 0.96
I0630 19:08:43.631646 17469 caffe.cpp:312] Batch 69, accuracy/top1 = 0.56
I0630 19:08:43.631669 17469 caffe.cpp:312] Batch 69, accuracy/top5 = 0.72
I0630 19:08:43.631674 17469 caffe.cpp:312] Batch 69, loss = 1.8
I0630 19:08:43.680447 17469 caffe.cpp:312] Batch 70, accuracy/top1 = 0.52
I0630 19:08:43.680469 17469 caffe.cpp:312] Batch 70, accuracy/top5 = 0.74
I0630 19:08:43.680474 17469 caffe.cpp:312] Batch 70, loss = 2.04
I0630 19:08:43.729465 17469 caffe.cpp:312] Batch 71, accuracy/top1 = 0.44
I0630 19:08:43.729488 17469 caffe.cpp:312] Batch 71, accuracy/top5 = 0.8
I0630 19:08:43.729492 17469 caffe.cpp:312] Batch 71, loss = 1.96
I0630 19:08:43.777081 17469 caffe.cpp:312] Batch 72, accuracy/top1 = 0.6
I0630 19:08:43.777102 17469 caffe.cpp:312] Batch 72, accuracy/top5 = 0.8
I0630 19:08:43.777107 17469 caffe.cpp:312] Batch 72, loss = 1.46
I0630 19:08:43.825970 17469 caffe.cpp:312] Batch 73, accuracy/top1 = 0.58
I0630 19:08:43.825994 17469 caffe.cpp:312] Batch 73, accuracy/top5 = 0.82
I0630 19:08:43.825999 17469 caffe.cpp:312] Batch 73, loss = 1.3
I0630 19:08:43.875488 17469 caffe.cpp:312] Batch 74, accuracy/top1 = 0.62
I0630 19:08:43.875512 17469 caffe.cpp:312] Batch 74, accuracy/top5 = 0.84
I0630 19:08:43.875515 17469 caffe.cpp:312] Batch 74, loss = 1.26
I0630 19:08:43.924757 17469 caffe.cpp:312] Batch 75, accuracy/top1 = 0.52
I0630 19:08:43.924779 17469 caffe.cpp:312] Batch 75, accuracy/top5 = 0.72
I0630 19:08:43.924783 17469 caffe.cpp:312] Batch 75, loss = 1.84
I0630 19:08:43.972579 17469 caffe.cpp:312] Batch 76, accuracy/top1 = 0.54
I0630 19:08:43.972599 17469 caffe.cpp:312] Batch 76, accuracy/top5 = 0.74
I0630 19:08:43.972602 17469 caffe.cpp:312] Batch 76, loss = 1.78
I0630 19:08:44.021811 17469 caffe.cpp:312] Batch 77, accuracy/top1 = 0.58
I0630 19:08:44.021833 17469 caffe.cpp:312] Batch 77, accuracy/top5 = 0.84
I0630 19:08:44.021836 17469 caffe.cpp:312] Batch 77, loss = 1.36
I0630 19:08:44.069958 17469 caffe.cpp:312] Batch 78, accuracy/top1 = 0.5
I0630 19:08:44.069981 17469 caffe.cpp:312] Batch 78, accuracy/top5 = 0.74
I0630 19:08:44.069984 17469 caffe.cpp:312] Batch 78, loss = 2.14
I0630 19:08:44.119601 17469 caffe.cpp:312] Batch 79, accuracy/top1 = 0.68
I0630 19:08:44.119623 17469 caffe.cpp:312] Batch 79, accuracy/top5 = 0.84
I0630 19:08:44.119626 17469 caffe.cpp:312] Batch 79, loss = 1.3
I0630 19:08:44.168742 17469 caffe.cpp:312] Batch 80, accuracy/top1 = 0.42
I0630 19:08:44.168764 17469 caffe.cpp:312] Batch 80, accuracy/top5 = 0.68
I0630 19:08:44.168767 17469 caffe.cpp:312] Batch 80, loss = 2.04
I0630 19:08:44.218271 17469 caffe.cpp:312] Batch 81, accuracy/top1 = 0.64
I0630 19:08:44.218292 17469 caffe.cpp:312] Batch 81, accuracy/top5 = 0.82
I0630 19:08:44.218297 17469 caffe.cpp:312] Batch 81, loss = 1.4
I0630 19:08:44.267485 17469 caffe.cpp:312] Batch 82, accuracy/top1 = 0.56
I0630 19:08:44.267506 17469 caffe.cpp:312] Batch 82, accuracy/top5 = 0.82
I0630 19:08:44.267509 17469 caffe.cpp:312] Batch 82, loss = 1.64
I0630 19:08:44.316617 17469 caffe.cpp:312] Batch 83, accuracy/top1 = 0.44
I0630 19:08:44.316640 17469 caffe.cpp:312] Batch 83, accuracy/top5 = 0.7
I0630 19:08:44.316643 17469 caffe.cpp:312] Batch 83, loss = 2.1
I0630 19:08:44.365662 17469 caffe.cpp:312] Batch 84, accuracy/top1 = 0.52
I0630 19:08:44.365684 17469 caffe.cpp:312] Batch 84, accuracy/top5 = 0.8
I0630 19:08:44.365687 17469 caffe.cpp:312] Batch 84, loss = 1.64
I0630 19:08:44.416122 17469 caffe.cpp:312] Batch 85, accuracy/top1 = 0.6
I0630 19:08:44.416144 17469 caffe.cpp:312] Batch 85, accuracy/top5 = 0.88
I0630 19:08:44.416147 17469 caffe.cpp:312] Batch 85, loss = 1.16
I0630 19:08:44.465275 17469 caffe.cpp:312] Batch 86, accuracy/top1 = 0.52
I0630 19:08:44.465296 17469 caffe.cpp:312] Batch 86, accuracy/top5 = 0.68
I0630 19:08:44.465299 17469 caffe.cpp:312] Batch 86, loss = 2
I0630 19:08:44.514447 17469 caffe.cpp:312] Batch 87, accuracy/top1 = 0.62
I0630 19:08:44.514469 17469 caffe.cpp:312] Batch 87, accuracy/top5 = 0.82
I0630 19:08:44.514472 17469 caffe.cpp:312] Batch 87, loss = 1.5
I0630 19:08:44.564054 17469 caffe.cpp:312] Batch 88, accuracy/top1 = 0.7
I0630 19:08:44.564074 17469 caffe.cpp:312] Batch 88, accuracy/top5 = 0.78
I0630 19:08:44.564079 17469 caffe.cpp:312] Batch 88, loss = 1.2
I0630 19:08:44.612886 17469 caffe.cpp:312] Batch 89, accuracy/top1 = 0.56
I0630 19:08:44.612910 17469 caffe.cpp:312] Batch 89, accuracy/top5 = 0.74
I0630 19:08:44.612912 17469 caffe.cpp:312] Batch 89, loss = 1.72
I0630 19:08:44.661694 17469 caffe.cpp:312] Batch 90, accuracy/top1 = 0.68
I0630 19:08:44.661715 17469 caffe.cpp:312] Batch 90, accuracy/top5 = 0.88
I0630 19:08:44.661718 17469 caffe.cpp:312] Batch 90, loss = 0.98
I0630 19:08:44.710563 17469 caffe.cpp:312] Batch 91, accuracy/top1 = 0.5
I0630 19:08:44.710587 17469 caffe.cpp:312] Batch 91, accuracy/top5 = 0.68
I0630 19:08:44.710589 17469 caffe.cpp:312] Batch 91, loss = 2.24
I0630 19:08:44.761155 17469 caffe.cpp:312] Batch 92, accuracy/top1 = 0.58
I0630 19:08:44.761178 17469 caffe.cpp:312] Batch 92, accuracy/top5 = 0.82
I0630 19:08:44.761181 17469 caffe.cpp:312] Batch 92, loss = 1.48
I0630 19:08:44.811373 17469 caffe.cpp:312] Batch 93, accuracy/top1 = 0.6
I0630 19:08:44.811395 17469 caffe.cpp:312] Batch 93, accuracy/top5 = 0.82
I0630 19:08:44.811399 17469 caffe.cpp:312] Batch 93, loss = 1.4
I0630 19:08:44.862560 17469 caffe.cpp:312] Batch 94, accuracy/top1 = 0.7
I0630 19:08:44.862581 17469 caffe.cpp:312] Batch 94, accuracy/top5 = 0.92
I0630 19:08:44.862584 17469 caffe.cpp:312] Batch 94, loss = 0.92
I0630 19:08:44.912339 17469 caffe.cpp:312] Batch 95, accuracy/top1 = 0.62
I0630 19:08:44.912360 17469 caffe.cpp:312] Batch 95, accuracy/top5 = 0.82
I0630 19:08:44.912364 17469 caffe.cpp:312] Batch 95, loss = 1.48
I0630 19:08:44.961717 17469 caffe.cpp:312] Batch 96, accuracy/top1 = 0.48
I0630 19:08:44.961740 17469 caffe.cpp:312] Batch 96, accuracy/top5 = 0.76
I0630 19:08:44.961743 17469 caffe.cpp:312] Batch 96, loss = 1.76
I0630 19:08:45.011050 17469 caffe.cpp:312] Batch 97, accuracy/top1 = 0.58
I0630 19:08:45.011072 17469 caffe.cpp:312] Batch 97, accuracy/top5 = 0.82
I0630 19:08:45.011075 17469 caffe.cpp:312] Batch 97, loss = 1.46
I0630 19:08:45.060307 17469 caffe.cpp:312] Batch 98, accuracy/top1 = 0.48
I0630 19:08:45.060328 17469 caffe.cpp:312] Batch 98, accuracy/top5 = 0.8
I0630 19:08:45.060331 17469 caffe.cpp:312] Batch 98, loss = 1.72
I0630 19:08:45.110209 17469 caffe.cpp:312] Batch 99, accuracy/top1 = 0.48
I0630 19:08:45.110231 17469 caffe.cpp:312] Batch 99, accuracy/top5 = 0.78
I0630 19:08:45.110235 17469 caffe.cpp:312] Batch 99, loss = 1.6
I0630 19:08:45.158864 17469 caffe.cpp:312] Batch 100, accuracy/top1 = 0.56
I0630 19:08:45.158903 17469 caffe.cpp:312] Batch 100, accuracy/top5 = 0.88
I0630 19:08:45.158907 17469 caffe.cpp:312] Batch 100, loss = 0.98
I0630 19:08:45.207075 17469 caffe.cpp:312] Batch 101, accuracy/top1 = 0.6
I0630 19:08:45.207098 17469 caffe.cpp:312] Batch 101, accuracy/top5 = 0.76
I0630 19:08:45.207100 17469 caffe.cpp:312] Batch 101, loss = 1.52
I0630 19:08:45.255884 17469 caffe.cpp:312] Batch 102, accuracy/top1 = 0.36
I0630 19:08:45.255908 17469 caffe.cpp:312] Batch 102, accuracy/top5 = 0.66
I0630 19:08:45.255911 17469 caffe.cpp:312] Batch 102, loss = 2.58
I0630 19:08:45.305104 17469 caffe.cpp:312] Batch 103, accuracy/top1 = 0.64
I0630 19:08:45.305124 17469 caffe.cpp:312] Batch 103, accuracy/top5 = 0.78
I0630 19:08:45.305127 17469 caffe.cpp:312] Batch 103, loss = 1.16
I0630 19:08:45.354971 17469 caffe.cpp:312] Batch 104, accuracy/top1 = 0.64
I0630 19:08:45.354995 17469 caffe.cpp:312] Batch 104, accuracy/top5 = 0.86
I0630 19:08:45.354998 17469 caffe.cpp:312] Batch 104, loss = 1.2
I0630 19:08:45.403862 17469 caffe.cpp:312] Batch 105, accuracy/top1 = 0.62
I0630 19:08:45.403883 17469 caffe.cpp:312] Batch 105, accuracy/top5 = 0.68
I0630 19:08:45.403887 17469 caffe.cpp:312] Batch 105, loss = 1.92
I0630 19:08:45.454030 17469 caffe.cpp:312] Batch 106, accuracy/top1 = 0.52
I0630 19:08:45.454051 17469 caffe.cpp:312] Batch 106, accuracy/top5 = 0.76
I0630 19:08:45.454054 17469 caffe.cpp:312] Batch 106, loss = 1.68
I0630 19:08:45.502282 17469 caffe.cpp:312] Batch 107, accuracy/top1 = 0.54
I0630 19:08:45.502302 17469 caffe.cpp:312] Batch 107, accuracy/top5 = 0.72
I0630 19:08:45.502306 17469 caffe.cpp:312] Batch 107, loss = 1.7
I0630 19:08:45.551424 17469 caffe.cpp:312] Batch 108, accuracy/top1 = 0.52
I0630 19:08:45.551448 17469 caffe.cpp:312] Batch 108, accuracy/top5 = 0.78
I0630 19:08:45.551451 17469 caffe.cpp:312] Batch 108, loss = 1.78
I0630 19:08:45.601312 17469 caffe.cpp:312] Batch 109, accuracy/top1 = 0.64
I0630 19:08:45.601333 17469 caffe.cpp:312] Batch 109, accuracy/top5 = 0.76
I0630 19:08:45.601336 17469 caffe.cpp:312] Batch 109, loss = 1.26
I0630 19:08:45.651494 17469 caffe.cpp:312] Batch 110, accuracy/top1 = 0.46
I0630 19:08:45.651516 17469 caffe.cpp:312] Batch 110, accuracy/top5 = 0.62
I0630 19:08:45.651520 17469 caffe.cpp:312] Batch 110, loss = 2.14
I0630 19:08:45.701369 17469 caffe.cpp:312] Batch 111, accuracy/top1 = 0.54
I0630 19:08:45.701390 17469 caffe.cpp:312] Batch 111, accuracy/top5 = 0.8
I0630 19:08:45.701392 17469 caffe.cpp:312] Batch 111, loss = 1.58
I0630 19:08:45.750340 17469 caffe.cpp:312] Batch 112, accuracy/top1 = 0.62
I0630 19:08:45.750362 17469 caffe.cpp:312] Batch 112, accuracy/top5 = 0.68
I0630 19:08:45.750365 17469 caffe.cpp:312] Batch 112, loss = 1.8
I0630 19:08:45.800567 17469 caffe.cpp:312] Batch 113, accuracy/top1 = 0.62
I0630 19:08:45.800588 17469 caffe.cpp:312] Batch 113, accuracy/top5 = 0.84
I0630 19:08:45.800592 17469 caffe.cpp:312] Batch 113, loss = 1.08
I0630 19:08:45.849611 17469 caffe.cpp:312] Batch 114, accuracy/top1 = 0.52
I0630 19:08:45.849632 17469 caffe.cpp:312] Batch 114, accuracy/top5 = 0.76
I0630 19:08:45.849635 17469 caffe.cpp:312] Batch 114, loss = 1.68
I0630 19:08:45.898705 17469 caffe.cpp:312] Batch 115, accuracy/top1 = 0.58
I0630 19:08:45.898725 17469 caffe.cpp:312] Batch 115, accuracy/top5 = 0.72
I0630 19:08:45.898727 17469 caffe.cpp:312] Batch 115, loss = 1.9
I0630 19:08:45.948921 17469 caffe.cpp:312] Batch 116, accuracy/top1 = 0.5
I0630 19:08:45.948945 17469 caffe.cpp:312] Batch 116, accuracy/top5 = 0.8
I0630 19:08:45.948948 17469 caffe.cpp:312] Batch 116, loss = 1.66
I0630 19:08:45.997856 17469 caffe.cpp:312] Batch 117, accuracy/top1 = 0.46
I0630 19:08:45.997879 17469 caffe.cpp:312] Batch 117, accuracy/top5 = 0.86
I0630 19:08:45.997882 17469 caffe.cpp:312] Batch 117, loss = 1.8
I0630 19:08:46.046499 17469 caffe.cpp:312] Batch 118, accuracy/top1 = 0.44
I0630 19:08:46.046521 17469 caffe.cpp:312] Batch 118, accuracy/top5 = 0.74
I0630 19:08:46.046525 17469 caffe.cpp:312] Batch 118, loss = 2.26
I0630 19:08:46.100112 17469 caffe.cpp:312] Batch 119, accuracy/top1 = 0.58
I0630 19:08:46.100147 17469 caffe.cpp:312] Batch 119, accuracy/top5 = 0.82
I0630 19:08:46.100152 17469 caffe.cpp:312] Batch 119, loss = 1.5
I0630 19:08:46.148653 17469 caffe.cpp:312] Batch 120, accuracy/top1 = 0.58
I0630 19:08:46.148676 17469 caffe.cpp:312] Batch 120, accuracy/top5 = 0.8
I0630 19:08:46.148679 17469 caffe.cpp:312] Batch 120, loss = 1.44
I0630 19:08:46.196950 17469 caffe.cpp:312] Batch 121, accuracy/top1 = 0.56
I0630 19:08:46.196972 17469 caffe.cpp:312] Batch 121, accuracy/top5 = 0.82
I0630 19:08:46.196975 17469 caffe.cpp:312] Batch 121, loss = 1.28
I0630 19:08:46.245580 17469 caffe.cpp:312] Batch 122, accuracy/top1 = 0.52
I0630 19:08:46.245604 17469 caffe.cpp:312] Batch 122, accuracy/top5 = 0.68
I0630 19:08:46.245606 17469 caffe.cpp:312] Batch 122, loss = 2.18
I0630 19:08:46.294286 17469 caffe.cpp:312] Batch 123, accuracy/top1 = 0.44
I0630 19:08:46.294306 17469 caffe.cpp:312] Batch 123, accuracy/top5 = 0.78
I0630 19:08:46.294309 17469 caffe.cpp:312] Batch 123, loss = 1.62
I0630 19:08:46.344118 17469 caffe.cpp:312] Batch 124, accuracy/top1 = 0.56
I0630 19:08:46.344142 17469 caffe.cpp:312] Batch 124, accuracy/top5 = 0.72
I0630 19:08:46.344146 17469 caffe.cpp:312] Batch 124, loss = 1.72
I0630 19:08:46.392597 17469 caffe.cpp:312] Batch 125, accuracy/top1 = 0.6
I0630 19:08:46.392618 17469 caffe.cpp:312] Batch 125, accuracy/top5 = 0.84
I0630 19:08:46.392622 17469 caffe.cpp:312] Batch 125, loss = 1.22
I0630 19:08:46.442098 17469 caffe.cpp:312] Batch 126, accuracy/top1 = 0.44
I0630 19:08:46.442121 17469 caffe.cpp:312] Batch 126, accuracy/top5 = 0.9
I0630 19:08:46.442126 17469 caffe.cpp:312] Batch 126, loss = 1.76
I0630 19:08:46.490473 17469 caffe.cpp:312] Batch 127, accuracy/top1 = 0.54
I0630 19:08:46.490495 17469 caffe.cpp:312] Batch 127, accuracy/top5 = 0.82
I0630 19:08:46.490499 17469 caffe.cpp:312] Batch 127, loss = 1.68
I0630 19:08:46.540577 17469 caffe.cpp:312] Batch 128, accuracy/top1 = 0.52
I0630 19:08:46.540599 17469 caffe.cpp:312] Batch 128, accuracy/top5 = 0.88
I0630 19:08:46.540602 17469 caffe.cpp:312] Batch 128, loss = 1.34
I0630 19:08:46.589903 17469 caffe.cpp:312] Batch 129, accuracy/top1 = 0.58
I0630 19:08:46.589923 17469 caffe.cpp:312] Batch 129, accuracy/top5 = 0.84
I0630 19:08:46.589926 17469 caffe.cpp:312] Batch 129, loss = 1.24
I0630 19:08:46.639302 17469 caffe.cpp:312] Batch 130, accuracy/top1 = 0.5
I0630 19:08:46.639322 17469 caffe.cpp:312] Batch 130, accuracy/top5 = 0.8
I0630 19:08:46.639325 17469 caffe.cpp:312] Batch 130, loss = 1.4
I0630 19:08:46.686939 17469 caffe.cpp:312] Batch 131, accuracy/top1 = 0.64
I0630 19:08:46.686959 17469 caffe.cpp:312] Batch 131, accuracy/top5 = 0.8
I0630 19:08:46.686962 17469 caffe.cpp:312] Batch 131, loss = 1.34
I0630 19:08:46.735237 17469 caffe.cpp:312] Batch 132, accuracy/top1 = 0.58
I0630 19:08:46.735260 17469 caffe.cpp:312] Batch 132, accuracy/top5 = 0.78
I0630 19:08:46.735263 17469 caffe.cpp:312] Batch 132, loss = 1.48
I0630 19:08:46.783915 17469 caffe.cpp:312] Batch 133, accuracy/top1 = 0.6
I0630 19:08:46.783936 17469 caffe.cpp:312] Batch 133, accuracy/top5 = 0.78
I0630 19:08:46.783938 17469 caffe.cpp:312] Batch 133, loss = 1.66
I0630 19:08:46.833150 17469 caffe.cpp:312] Batch 134, accuracy/top1 = 0.6
I0630 19:08:46.833171 17469 caffe.cpp:312] Batch 134, accuracy/top5 = 0.86
I0630 19:08:46.833174 17469 caffe.cpp:312] Batch 134, loss = 1.22
I0630 19:08:46.881672 17469 caffe.cpp:312] Batch 135, accuracy/top1 = 0.54
I0630 19:08:46.881693 17469 caffe.cpp:312] Batch 135, accuracy/top5 = 0.78
I0630 19:08:46.881696 17469 caffe.cpp:312] Batch 135, loss = 1.48
I0630 19:08:46.930682 17469 caffe.cpp:312] Batch 136, accuracy/top1 = 0.52
I0630 19:08:46.930706 17469 caffe.cpp:312] Batch 136, accuracy/top5 = 0.72
I0630 19:08:46.930711 17469 caffe.cpp:312] Batch 136, loss = 1.72
I0630 19:08:46.979382 17469 caffe.cpp:312] Batch 137, accuracy/top1 = 0.62
I0630 19:08:46.979403 17469 caffe.cpp:312] Batch 137, accuracy/top5 = 0.78
I0630 19:08:46.979408 17469 caffe.cpp:312] Batch 137, loss = 1.36
I0630 19:08:47.028934 17469 caffe.cpp:312] Batch 138, accuracy/top1 = 0.62
I0630 19:08:47.028970 17469 caffe.cpp:312] Batch 138, accuracy/top5 = 0.84
I0630 19:08:47.028973 17469 caffe.cpp:312] Batch 138, loss = 1.14
I0630 19:08:47.076992 17469 caffe.cpp:312] Batch 139, accuracy/top1 = 0.64
I0630 19:08:47.077014 17469 caffe.cpp:312] Batch 139, accuracy/top5 = 0.74
I0630 19:08:47.077018 17469 caffe.cpp:312] Batch 139, loss = 1.98
I0630 19:08:47.126179 17469 caffe.cpp:312] Batch 140, accuracy/top1 = 0.48
I0630 19:08:47.126204 17469 caffe.cpp:312] Batch 140, accuracy/top5 = 0.78
I0630 19:08:47.126206 17469 caffe.cpp:312] Batch 140, loss = 1.72
I0630 19:08:47.175326 17469 caffe.cpp:312] Batch 141, accuracy/top1 = 0.58
I0630 19:08:47.175348 17469 caffe.cpp:312] Batch 141, accuracy/top5 = 0.78
I0630 19:08:47.175351 17469 caffe.cpp:312] Batch 141, loss = 1.44
I0630 19:08:47.225286 17469 caffe.cpp:312] Batch 142, accuracy/top1 = 0.48
I0630 19:08:47.225308 17469 caffe.cpp:312] Batch 142, accuracy/top5 = 0.78
I0630 19:08:47.225311 17469 caffe.cpp:312] Batch 142, loss = 1.78
I0630 19:08:47.274878 17469 caffe.cpp:312] Batch 143, accuracy/top1 = 0.48
I0630 19:08:47.274899 17469 caffe.cpp:312] Batch 143, accuracy/top5 = 0.72
I0630 19:08:47.274904 17469 caffe.cpp:312] Batch 143, loss = 1.98
I0630 19:08:47.324120 17469 caffe.cpp:312] Batch 144, accuracy/top1 = 0.64
I0630 19:08:47.324143 17469 caffe.cpp:312] Batch 144, accuracy/top5 = 0.82
I0630 19:08:47.324146 17469 caffe.cpp:312] Batch 144, loss = 1.4
I0630 19:08:47.372804 17469 caffe.cpp:312] Batch 145, accuracy/top1 = 0.62
I0630 19:08:47.372826 17469 caffe.cpp:312] Batch 145, accuracy/top5 = 0.82
I0630 19:08:47.372829 17469 caffe.cpp:312] Batch 145, loss = 1.58
I0630 19:08:47.422441 17469 caffe.cpp:312] Batch 146, accuracy/top1 = 0.68
I0630 19:08:47.422464 17469 caffe.cpp:312] Batch 146, accuracy/top5 = 0.88
I0630 19:08:47.422467 17469 caffe.cpp:312] Batch 146, loss = 1.08
I0630 19:08:47.471283 17469 caffe.cpp:312] Batch 147, accuracy/top1 = 0.64
I0630 19:08:47.471305 17469 caffe.cpp:312] Batch 147, accuracy/top5 = 0.86
I0630 19:08:47.471308 17469 caffe.cpp:312] Batch 147, loss = 1.28
I0630 19:08:47.520681 17469 caffe.cpp:312] Batch 148, accuracy/top1 = 0.5
I0630 19:08:47.520704 17469 caffe.cpp:312] Batch 148, accuracy/top5 = 0.88
I0630 19:08:47.520707 17469 caffe.cpp:312] Batch 148, loss = 1.72
I0630 19:08:47.570039 17469 caffe.cpp:312] Batch 149, accuracy/top1 = 0.6
I0630 19:08:47.570060 17469 caffe.cpp:312] Batch 149, accuracy/top5 = 0.76
I0630 19:08:47.570065 17469 caffe.cpp:312] Batch 149, loss = 1.34
I0630 19:08:47.619729 17469 caffe.cpp:312] Batch 150, accuracy/top1 = 0.56
I0630 19:08:47.619752 17469 caffe.cpp:312] Batch 150, accuracy/top5 = 0.78
I0630 19:08:47.619755 17469 caffe.cpp:312] Batch 150, loss = 1.7
I0630 19:08:47.668922 17469 caffe.cpp:312] Batch 151, accuracy/top1 = 0.62
I0630 19:08:47.668944 17469 caffe.cpp:312] Batch 151, accuracy/top5 = 0.84
I0630 19:08:47.668948 17469 caffe.cpp:312] Batch 151, loss = 1.3
I0630 19:08:47.718050 17469 caffe.cpp:312] Batch 152, accuracy/top1 = 0.56
I0630 19:08:47.718075 17469 caffe.cpp:312] Batch 152, accuracy/top5 = 0.76
I0630 19:08:47.718078 17469 caffe.cpp:312] Batch 152, loss = 1.76
I0630 19:08:47.766963 17469 caffe.cpp:312] Batch 153, accuracy/top1 = 0.48
I0630 19:08:47.766986 17469 caffe.cpp:312] Batch 153, accuracy/top5 = 0.78
I0630 19:08:47.766990 17469 caffe.cpp:312] Batch 153, loss = 1.76
I0630 19:08:47.816495 17469 caffe.cpp:312] Batch 154, accuracy/top1 = 0.58
I0630 19:08:47.816519 17469 caffe.cpp:312] Batch 154, accuracy/top5 = 0.82
I0630 19:08:47.816522 17469 caffe.cpp:312] Batch 154, loss = 1.26
I0630 19:08:47.866997 17469 caffe.cpp:312] Batch 155, accuracy/top1 = 0.46
I0630 19:08:47.867017 17469 caffe.cpp:312] Batch 155, accuracy/top5 = 0.7
I0630 19:08:47.867020 17469 caffe.cpp:312] Batch 155, loss = 1.84
I0630 19:08:47.915349 17469 caffe.cpp:312] Batch 156, accuracy/top1 = 0.58
I0630 19:08:47.915369 17469 caffe.cpp:312] Batch 156, accuracy/top5 = 0.74
I0630 19:08:47.915374 17469 caffe.cpp:312] Batch 156, loss = 1.84
I0630 19:08:47.966405 17469 caffe.cpp:312] Batch 157, accuracy/top1 = 0.54
I0630 19:08:47.966442 17469 caffe.cpp:312] Batch 157, accuracy/top5 = 0.82
I0630 19:08:47.966445 17469 caffe.cpp:312] Batch 157, loss = 1.5
I0630 19:08:48.016836 17469 caffe.cpp:312] Batch 158, accuracy/top1 = 0.58
I0630 19:08:48.016858 17469 caffe.cpp:312] Batch 158, accuracy/top5 = 0.86
I0630 19:08:48.016861 17469 caffe.cpp:312] Batch 158, loss = 1.24
I0630 19:08:48.066417 17469 caffe.cpp:312] Batch 159, accuracy/top1 = 0.52
I0630 19:08:48.066439 17469 caffe.cpp:312] Batch 159, accuracy/top5 = 0.8
I0630 19:08:48.066442 17469 caffe.cpp:312] Batch 159, loss = 1.58
I0630 19:08:48.115887 17469 caffe.cpp:312] Batch 160, accuracy/top1 = 0.6
I0630 19:08:48.115913 17469 caffe.cpp:312] Batch 160, accuracy/top5 = 0.88
I0630 19:08:48.115918 17469 caffe.cpp:312] Batch 160, loss = 1.36
I0630 19:08:48.164968 17469 caffe.cpp:312] Batch 161, accuracy/top1 = 0.56
I0630 19:08:48.164993 17469 caffe.cpp:312] Batch 161, accuracy/top5 = 0.84
I0630 19:08:48.164995 17469 caffe.cpp:312] Batch 161, loss = 1.94
I0630 19:08:48.214335 17469 caffe.cpp:312] Batch 162, accuracy/top1 = 0.52
I0630 19:08:48.214356 17469 caffe.cpp:312] Batch 162, accuracy/top5 = 0.8
I0630 19:08:48.214360 17469 caffe.cpp:312] Batch 162, loss = 1.42
I0630 19:08:48.264441 17469 caffe.cpp:312] Batch 163, accuracy/top1 = 0.58
I0630 19:08:48.264463 17469 caffe.cpp:312] Batch 163, accuracy/top5 = 0.82
I0630 19:08:48.264467 17469 caffe.cpp:312] Batch 163, loss = 1.44
I0630 19:08:48.314350 17469 caffe.cpp:312] Batch 164, accuracy/top1 = 0.7
I0630 19:08:48.314371 17469 caffe.cpp:312] Batch 164, accuracy/top5 = 0.78
I0630 19:08:48.314374 17469 caffe.cpp:312] Batch 164, loss = 1.36
I0630 19:08:48.363921 17469 caffe.cpp:312] Batch 165, accuracy/top1 = 0.7
I0630 19:08:48.363945 17469 caffe.cpp:312] Batch 165, accuracy/top5 = 0.88
I0630 19:08:48.363948 17469 caffe.cpp:312] Batch 165, loss = 0.98
I0630 19:08:48.412647 17469 caffe.cpp:312] Batch 166, accuracy/top1 = 0.46
I0630 19:08:48.412668 17469 caffe.cpp:312] Batch 166, accuracy/top5 = 0.7
I0630 19:08:48.412672 17469 caffe.cpp:312] Batch 166, loss = 2
I0630 19:08:48.461881 17469 caffe.cpp:312] Batch 167, accuracy/top1 = 0.56
I0630 19:08:48.461905 17469 caffe.cpp:312] Batch 167, accuracy/top5 = 0.84
I0630 19:08:48.461908 17469 caffe.cpp:312] Batch 167, loss = 1.28
I0630 19:08:48.510313 17469 caffe.cpp:312] Batch 168, accuracy/top1 = 0.62
I0630 19:08:48.510331 17469 caffe.cpp:312] Batch 168, accuracy/top5 = 0.84
I0630 19:08:48.510335 17469 caffe.cpp:312] Batch 168, loss = 1.32
I0630 19:08:48.559543 17469 caffe.cpp:312] Batch 169, accuracy/top1 = 0.66
I0630 19:08:48.559566 17469 caffe.cpp:312] Batch 169, accuracy/top5 = 0.86
I0630 19:08:48.559568 17469 caffe.cpp:312] Batch 169, loss = 1.2
I0630 19:08:48.607528 17469 caffe.cpp:312] Batch 170, accuracy/top1 = 0.62
I0630 19:08:48.607549 17469 caffe.cpp:312] Batch 170, accuracy/top5 = 0.76
I0630 19:08:48.607553 17469 caffe.cpp:312] Batch 170, loss = 1.68
I0630 19:08:48.656066 17469 caffe.cpp:312] Batch 171, accuracy/top1 = 0.42
I0630 19:08:48.656088 17469 caffe.cpp:312] Batch 171, accuracy/top5 = 0.76
I0630 19:08:48.656091 17469 caffe.cpp:312] Batch 171, loss = 2
I0630 19:08:48.704978 17469 caffe.cpp:312] Batch 172, accuracy/top1 = 0.58
I0630 19:08:48.704998 17469 caffe.cpp:312] Batch 172, accuracy/top5 = 0.84
I0630 19:08:48.705001 17469 caffe.cpp:312] Batch 172, loss = 1.52
I0630 19:08:48.754480 17469 caffe.cpp:312] Batch 173, accuracy/top1 = 0.6
I0630 19:08:48.754503 17469 caffe.cpp:312] Batch 173, accuracy/top5 = 0.82
I0630 19:08:48.754505 17469 caffe.cpp:312] Batch 173, loss = 1.48
I0630 19:08:48.803079 17469 caffe.cpp:312] Batch 174, accuracy/top1 = 0.5
I0630 19:08:48.803100 17469 caffe.cpp:312] Batch 174, accuracy/top5 = 0.76
I0630 19:08:48.803104 17469 caffe.cpp:312] Batch 174, loss = 1.5
I0630 19:08:48.852185 17469 caffe.cpp:312] Batch 175, accuracy/top1 = 0.52
I0630 19:08:48.852205 17469 caffe.cpp:312] Batch 175, accuracy/top5 = 0.74
I0630 19:08:48.852208 17469 caffe.cpp:312] Batch 175, loss = 1.8
I0630 19:08:48.902289 17469 caffe.cpp:312] Batch 176, accuracy/top1 = 0.54
I0630 19:08:48.902325 17469 caffe.cpp:312] Batch 176, accuracy/top5 = 0.76
I0630 19:08:48.902329 17469 caffe.cpp:312] Batch 176, loss = 1.62
I0630 19:08:48.949828 17469 caffe.cpp:312] Batch 177, accuracy/top1 = 0.56
I0630 19:08:48.949851 17469 caffe.cpp:312] Batch 177, accuracy/top5 = 0.74
I0630 19:08:48.949853 17469 caffe.cpp:312] Batch 177, loss = 1.64
I0630 19:08:48.998473 17469 caffe.cpp:312] Batch 178, accuracy/top1 = 0.44
I0630 19:08:48.998495 17469 caffe.cpp:312] Batch 178, accuracy/top5 = 0.78
I0630 19:08:48.998498 17469 caffe.cpp:312] Batch 178, loss = 1.82
I0630 19:08:49.049208 17469 caffe.cpp:312] Batch 179, accuracy/top1 = 0.58
I0630 19:08:49.049230 17469 caffe.cpp:312] Batch 179, accuracy/top5 = 0.82
I0630 19:08:49.049234 17469 caffe.cpp:312] Batch 179, loss = 1.58
I0630 19:08:49.097821 17469 caffe.cpp:312] Batch 180, accuracy/top1 = 0.66
I0630 19:08:49.097842 17469 caffe.cpp:312] Batch 180, accuracy/top5 = 0.8
I0630 19:08:49.097846 17469 caffe.cpp:312] Batch 180, loss = 1.46
I0630 19:08:49.147655 17469 caffe.cpp:312] Batch 181, accuracy/top1 = 0.48
I0630 19:08:49.147677 17469 caffe.cpp:312] Batch 181, accuracy/top5 = 0.72
I0630 19:08:49.147680 17469 caffe.cpp:312] Batch 181, loss = 2.06
I0630 19:08:49.196787 17469 caffe.cpp:312] Batch 182, accuracy/top1 = 0.58
I0630 19:08:49.196810 17469 caffe.cpp:312] Batch 182, accuracy/top5 = 0.74
I0630 19:08:49.196813 17469 caffe.cpp:312] Batch 182, loss = 1.88
I0630 19:08:49.246418 17469 caffe.cpp:312] Batch 183, accuracy/top1 = 0.54
I0630 19:08:49.246440 17469 caffe.cpp:312] Batch 183, accuracy/top5 = 0.82
I0630 19:08:49.246443 17469 caffe.cpp:312] Batch 183, loss = 1.68
I0630 19:08:49.294513 17469 caffe.cpp:312] Batch 184, accuracy/top1 = 0.52
I0630 19:08:49.294536 17469 caffe.cpp:312] Batch 184, accuracy/top5 = 0.84
I0630 19:08:49.294539 17469 caffe.cpp:312] Batch 184, loss = 1.58
I0630 19:08:49.342994 17469 caffe.cpp:312] Batch 185, accuracy/top1 = 0.6
I0630 19:08:49.343017 17469 caffe.cpp:312] Batch 185, accuracy/top5 = 0.84
I0630 19:08:49.343021 17469 caffe.cpp:312] Batch 185, loss = 1.44
I0630 19:08:49.392108 17469 caffe.cpp:312] Batch 186, accuracy/top1 = 0.54
I0630 19:08:49.392129 17469 caffe.cpp:312] Batch 186, accuracy/top5 = 0.8
I0630 19:08:49.392133 17469 caffe.cpp:312] Batch 186, loss = 1.3
I0630 19:08:49.442445 17469 caffe.cpp:312] Batch 187, accuracy/top1 = 0.64
I0630 19:08:49.442466 17469 caffe.cpp:312] Batch 187, accuracy/top5 = 0.88
I0630 19:08:49.442469 17469 caffe.cpp:312] Batch 187, loss = 1.16
I0630 19:08:49.490696 17469 caffe.cpp:312] Batch 188, accuracy/top1 = 0.6
I0630 19:08:49.490718 17469 caffe.cpp:312] Batch 188, accuracy/top5 = 0.82
I0630 19:08:49.490722 17469 caffe.cpp:312] Batch 188, loss = 1.42
I0630 19:08:49.540489 17469 caffe.cpp:312] Batch 189, accuracy/top1 = 0.48
I0630 19:08:49.540513 17469 caffe.cpp:312] Batch 189, accuracy/top5 = 0.78
I0630 19:08:49.540515 17469 caffe.cpp:312] Batch 189, loss = 1.78
I0630 19:08:49.589010 17469 caffe.cpp:312] Batch 190, accuracy/top1 = 0.58
I0630 19:08:49.589032 17469 caffe.cpp:312] Batch 190, accuracy/top5 = 0.86
I0630 19:08:49.589035 17469 caffe.cpp:312] Batch 190, loss = 1.3
I0630 19:08:49.638943 17469 caffe.cpp:312] Batch 191, accuracy/top1 = 0.52
I0630 19:08:49.638965 17469 caffe.cpp:312] Batch 191, accuracy/top5 = 0.74
I0630 19:08:49.638968 17469 caffe.cpp:312] Batch 191, loss = 1.86
I0630 19:08:49.687646 17469 caffe.cpp:312] Batch 192, accuracy/top1 = 0.5
I0630 19:08:49.687669 17469 caffe.cpp:312] Batch 192, accuracy/top5 = 0.82
I0630 19:08:49.687671 17469 caffe.cpp:312] Batch 192, loss = 1.7
I0630 19:08:49.736800 17469 caffe.cpp:312] Batch 193, accuracy/top1 = 0.58
I0630 19:08:49.736824 17469 caffe.cpp:312] Batch 193, accuracy/top5 = 0.76
I0630 19:08:49.736827 17469 caffe.cpp:312] Batch 193, loss = 1.38
I0630 19:08:49.785348 17469 caffe.cpp:312] Batch 194, accuracy/top1 = 0.64
I0630 19:08:49.785369 17469 caffe.cpp:312] Batch 194, accuracy/top5 = 0.88
I0630 19:08:49.785372 17469 caffe.cpp:312] Batch 194, loss = 1.26
I0630 19:08:49.834967 17469 caffe.cpp:312] Batch 195, accuracy/top1 = 0.52
I0630 19:08:49.835005 17469 caffe.cpp:312] Batch 195, accuracy/top5 = 0.8
I0630 19:08:49.835009 17469 caffe.cpp:312] Batch 195, loss = 1.36
I0630 19:08:49.884451 17469 caffe.cpp:312] Batch 196, accuracy/top1 = 0.68
I0630 19:08:49.884472 17469 caffe.cpp:312] Batch 196, accuracy/top5 = 0.82
I0630 19:08:49.884475 17469 caffe.cpp:312] Batch 196, loss = 1.26
I0630 19:08:49.933503 17469 caffe.cpp:312] Batch 197, accuracy/top1 = 0.66
I0630 19:08:49.933527 17469 caffe.cpp:312] Batch 197, accuracy/top5 = 0.9
I0630 19:08:49.933531 17469 caffe.cpp:312] Batch 197, loss = 1.26
I0630 19:08:49.983359 17469 caffe.cpp:312] Batch 198, accuracy/top1 = 0.52
I0630 19:08:49.983381 17469 caffe.cpp:312] Batch 198, accuracy/top5 = 0.86
I0630 19:08:49.983386 17469 caffe.cpp:312] Batch 198, loss = 1.52
I0630 19:08:50.032465 17469 caffe.cpp:312] Batch 199, accuracy/top1 = 0.68
I0630 19:08:50.032488 17469 caffe.cpp:312] Batch 199, accuracy/top5 = 0.86
I0630 19:08:50.032491 17469 caffe.cpp:312] Batch 199, loss = 1.26
I0630 19:08:50.081562 17469 caffe.cpp:312] Batch 200, accuracy/top1 = 0.54
I0630 19:08:50.081584 17469 caffe.cpp:312] Batch 200, accuracy/top5 = 0.76
I0630 19:08:50.081588 17469 caffe.cpp:312] Batch 200, loss = 1.76
I0630 19:08:50.131847 17469 caffe.cpp:312] Batch 201, accuracy/top1 = 0.46
I0630 19:08:50.131870 17469 caffe.cpp:312] Batch 201, accuracy/top5 = 0.76
I0630 19:08:50.131873 17469 caffe.cpp:312] Batch 201, loss = 1.78
I0630 19:08:50.181112 17469 caffe.cpp:312] Batch 202, accuracy/top1 = 0.48
I0630 19:08:50.181134 17469 caffe.cpp:312] Batch 202, accuracy/top5 = 0.84
I0630 19:08:50.181138 17469 caffe.cpp:312] Batch 202, loss = 1.52
I0630 19:08:50.229796 17469 caffe.cpp:312] Batch 203, accuracy/top1 = 0.62
I0630 19:08:50.229817 17469 caffe.cpp:312] Batch 203, accuracy/top5 = 0.86
I0630 19:08:50.229820 17469 caffe.cpp:312] Batch 203, loss = 1.32
I0630 19:08:50.278954 17469 caffe.cpp:312] Batch 204, accuracy/top1 = 0.74
I0630 19:08:50.278982 17469 caffe.cpp:312] Batch 204, accuracy/top5 = 0.86
I0630 19:08:50.278986 17469 caffe.cpp:312] Batch 204, loss = 0.92
I0630 19:08:50.327847 17469 caffe.cpp:312] Batch 205, accuracy/top1 = 0.56
I0630 19:08:50.327870 17469 caffe.cpp:312] Batch 205, accuracy/top5 = 0.78
I0630 19:08:50.327873 17469 caffe.cpp:312] Batch 205, loss = 1.62
I0630 19:08:50.376509 17469 caffe.cpp:312] Batch 206, accuracy/top1 = 0.6
I0630 19:08:50.376529 17469 caffe.cpp:312] Batch 206, accuracy/top5 = 0.84
I0630 19:08:50.376533 17469 caffe.cpp:312] Batch 206, loss = 1.08
I0630 19:08:50.425245 17469 caffe.cpp:312] Batch 207, accuracy/top1 = 0.54
I0630 19:08:50.425267 17469 caffe.cpp:312] Batch 207, accuracy/top5 = 0.82
I0630 19:08:50.425271 17469 caffe.cpp:312] Batch 207, loss = 1.62
I0630 19:08:50.474974 17469 caffe.cpp:312] Batch 208, accuracy/top1 = 0.5
I0630 19:08:50.474994 17469 caffe.cpp:312] Batch 208, accuracy/top5 = 0.72
I0630 19:08:50.474998 17469 caffe.cpp:312] Batch 208, loss = 2.06
I0630 19:08:50.524513 17469 caffe.cpp:312] Batch 209, accuracy/top1 = 0.58
I0630 19:08:50.524535 17469 caffe.cpp:312] Batch 209, accuracy/top5 = 0.84
I0630 19:08:50.524539 17469 caffe.cpp:312] Batch 209, loss = 1.52
I0630 19:08:50.573846 17469 caffe.cpp:312] Batch 210, accuracy/top1 = 0.38
I0630 19:08:50.573868 17469 caffe.cpp:312] Batch 210, accuracy/top5 = 0.66
I0630 19:08:50.573873 17469 caffe.cpp:312] Batch 210, loss = 2.56
I0630 19:08:50.623004 17469 caffe.cpp:312] Batch 211, accuracy/top1 = 0.56
I0630 19:08:50.623024 17469 caffe.cpp:312] Batch 211, accuracy/top5 = 0.76
I0630 19:08:50.623028 17469 caffe.cpp:312] Batch 211, loss = 1.68
I0630 19:08:50.671618 17469 caffe.cpp:312] Batch 212, accuracy/top1 = 0.66
I0630 19:08:50.671639 17469 caffe.cpp:312] Batch 212, accuracy/top5 = 0.82
I0630 19:08:50.671643 17469 caffe.cpp:312] Batch 212, loss = 1.4
I0630 19:08:50.720278 17469 caffe.cpp:312] Batch 213, accuracy/top1 = 0.66
I0630 19:08:50.720301 17469 caffe.cpp:312] Batch 213, accuracy/top5 = 0.78
I0630 19:08:50.720305 17469 caffe.cpp:312] Batch 213, loss = 1.52
I0630 19:08:50.768970 17469 caffe.cpp:312] Batch 214, accuracy/top1 = 0.52
I0630 19:08:50.769006 17469 caffe.cpp:312] Batch 214, accuracy/top5 = 0.84
I0630 19:08:50.769009 17469 caffe.cpp:312] Batch 214, loss = 1.58
I0630 19:08:50.816360 17469 caffe.cpp:312] Batch 215, accuracy/top1 = 0.46
I0630 19:08:50.816382 17469 caffe.cpp:312] Batch 215, accuracy/top5 = 0.8
I0630 19:08:50.816386 17469 caffe.cpp:312] Batch 215, loss = 1.6
I0630 19:08:50.865002 17469 caffe.cpp:312] Batch 216, accuracy/top1 = 0.64
I0630 19:08:50.865022 17469 caffe.cpp:312] Batch 216, accuracy/top5 = 0.84
I0630 19:08:50.865025 17469 caffe.cpp:312] Batch 216, loss = 1.24
I0630 19:08:50.913671 17469 caffe.cpp:312] Batch 217, accuracy/top1 = 0.64
I0630 19:08:50.913696 17469 caffe.cpp:312] Batch 217, accuracy/top5 = 0.84
I0630 19:08:50.913698 17469 caffe.cpp:312] Batch 217, loss = 1.44
I0630 19:08:50.966168 17469 caffe.cpp:312] Batch 218, accuracy/top1 = 0.6
I0630 19:08:50.966190 17469 caffe.cpp:312] Batch 218, accuracy/top5 = 0.82
I0630 19:08:50.966194 17469 caffe.cpp:312] Batch 218, loss = 1.54
I0630 19:08:51.015051 17469 caffe.cpp:312] Batch 219, accuracy/top1 = 0.6
I0630 19:08:51.015077 17469 caffe.cpp:312] Batch 219, accuracy/top5 = 0.92
I0630 19:08:51.015084 17469 caffe.cpp:312] Batch 219, loss = 1.2
I0630 19:08:51.064556 17469 caffe.cpp:312] Batch 220, accuracy/top1 = 0.46
I0630 19:08:51.064577 17469 caffe.cpp:312] Batch 220, accuracy/top5 = 0.68
I0630 19:08:51.064580 17469 caffe.cpp:312] Batch 220, loss = 2
I0630 19:08:51.113351 17469 caffe.cpp:312] Batch 221, accuracy/top1 = 0.6
I0630 19:08:51.113373 17469 caffe.cpp:312] Batch 221, accuracy/top5 = 0.88
I0630 19:08:51.113378 17469 caffe.cpp:312] Batch 221, loss = 1.22
I0630 19:08:51.162082 17469 caffe.cpp:312] Batch 222, accuracy/top1 = 0.66
I0630 19:08:51.162104 17469 caffe.cpp:312] Batch 222, accuracy/top5 = 0.78
I0630 19:08:51.162107 17469 caffe.cpp:312] Batch 222, loss = 1.78
I0630 19:08:51.210820 17469 caffe.cpp:312] Batch 223, accuracy/top1 = 0.56
I0630 19:08:51.210850 17469 caffe.cpp:312] Batch 223, accuracy/top5 = 0.8
I0630 19:08:51.210855 17469 caffe.cpp:312] Batch 223, loss = 1.54
I0630 19:08:51.263466 17469 caffe.cpp:312] Batch 224, accuracy/top1 = 0.46
I0630 19:08:51.263490 17469 caffe.cpp:312] Batch 224, accuracy/top5 = 0.76
I0630 19:08:51.263495 17469 caffe.cpp:312] Batch 224, loss = 1.78
I0630 19:08:51.314853 17469 caffe.cpp:312] Batch 225, accuracy/top1 = 0.58
I0630 19:08:51.314878 17469 caffe.cpp:312] Batch 225, accuracy/top5 = 0.78
I0630 19:08:51.314882 17469 caffe.cpp:312] Batch 225, loss = 1.5
I0630 19:08:51.367251 17469 caffe.cpp:312] Batch 226, accuracy/top1 = 0.64
I0630 19:08:51.367276 17469 caffe.cpp:312] Batch 226, accuracy/top5 = 0.82
I0630 19:08:51.367280 17469 caffe.cpp:312] Batch 226, loss = 1.34
I0630 19:08:51.419983 17469 caffe.cpp:312] Batch 227, accuracy/top1 = 0.6
I0630 19:08:51.420006 17469 caffe.cpp:312] Batch 227, accuracy/top5 = 0.88
I0630 19:08:51.420008 17469 caffe.cpp:312] Batch 227, loss = 1.08
I0630 19:08:51.471155 17469 caffe.cpp:312] Batch 228, accuracy/top1 = 0.64
I0630 19:08:51.471177 17469 caffe.cpp:312] Batch 228, accuracy/top5 = 0.84
I0630 19:08:51.471180 17469 caffe.cpp:312] Batch 228, loss = 1.26
I0630 19:08:51.520516 17469 caffe.cpp:312] Batch 229, accuracy/top1 = 0.58
I0630 19:08:51.520542 17469 caffe.cpp:312] Batch 229, accuracy/top5 = 0.84
I0630 19:08:51.520546 17469 caffe.cpp:312] Batch 229, loss = 1.22
I0630 19:08:51.573123 17469 caffe.cpp:312] Batch 230, accuracy/top1 = 0.74
I0630 19:08:51.573148 17469 caffe.cpp:312] Batch 230, accuracy/top5 = 0.9
I0630 19:08:51.573153 17469 caffe.cpp:312] Batch 230, loss = 0.94
I0630 19:08:51.625116 17469 caffe.cpp:312] Batch 231, accuracy/top1 = 0.48
I0630 19:08:51.625141 17469 caffe.cpp:312] Batch 231, accuracy/top5 = 0.72
I0630 19:08:51.625146 17469 caffe.cpp:312] Batch 231, loss = 1.96
I0630 19:08:51.677287 17469 caffe.cpp:312] Batch 232, accuracy/top1 = 0.54
I0630 19:08:51.677311 17469 caffe.cpp:312] Batch 232, accuracy/top5 = 0.82
I0630 19:08:51.677316 17469 caffe.cpp:312] Batch 232, loss = 1.46
I0630 19:08:51.729238 17469 caffe.cpp:312] Batch 233, accuracy/top1 = 0.5
I0630 19:08:51.729279 17469 caffe.cpp:312] Batch 233, accuracy/top5 = 0.84
I0630 19:08:51.729285 17469 caffe.cpp:312] Batch 233, loss = 1.56
I0630 19:08:51.781195 17469 caffe.cpp:312] Batch 234, accuracy/top1 = 0.62
I0630 19:08:51.781219 17469 caffe.cpp:312] Batch 234, accuracy/top5 = 0.78
I0630 19:08:51.781221 17469 caffe.cpp:312] Batch 234, loss = 1.62
I0630 19:08:51.832633 17469 caffe.cpp:312] Batch 235, accuracy/top1 = 0.62
I0630 19:08:51.832657 17469 caffe.cpp:312] Batch 235, accuracy/top5 = 0.84
I0630 19:08:51.832662 17469 caffe.cpp:312] Batch 235, loss = 1.12
I0630 19:08:51.885272 17469 caffe.cpp:312] Batch 236, accuracy/top1 = 0.52
I0630 19:08:51.885295 17469 caffe.cpp:312] Batch 236, accuracy/top5 = 0.78
I0630 19:08:51.885299 17469 caffe.cpp:312] Batch 236, loss = 1.8
I0630 19:08:51.937624 17469 caffe.cpp:312] Batch 237, accuracy/top1 = 0.58
I0630 19:08:51.937649 17469 caffe.cpp:312] Batch 237, accuracy/top5 = 0.86
I0630 19:08:51.937654 17469 caffe.cpp:312] Batch 237, loss = 1.24
I0630 19:08:51.989904 17469 caffe.cpp:312] Batch 238, accuracy/top1 = 0.58
I0630 19:08:51.989928 17469 caffe.cpp:312] Batch 238, accuracy/top5 = 0.8
I0630 19:08:51.989933 17469 caffe.cpp:312] Batch 238, loss = 1.26
I0630 19:08:52.042943 17469 caffe.cpp:312] Batch 239, accuracy/top1 = 0.6
I0630 19:08:52.042968 17469 caffe.cpp:312] Batch 239, accuracy/top5 = 0.84
I0630 19:08:52.042973 17469 caffe.cpp:312] Batch 239, loss = 1.14
I0630 19:08:52.095312 17469 caffe.cpp:312] Batch 240, accuracy/top1 = 0.62
I0630 19:08:52.095337 17469 caffe.cpp:312] Batch 240, accuracy/top5 = 0.82
I0630 19:08:52.095341 17469 caffe.cpp:312] Batch 240, loss = 1.36
I0630 19:08:52.147339 17469 caffe.cpp:312] Batch 241, accuracy/top1 = 0.5
I0630 19:08:52.147364 17469 caffe.cpp:312] Batch 241, accuracy/top5 = 0.76
I0630 19:08:52.147368 17469 caffe.cpp:312] Batch 241, loss = 1.68
I0630 19:08:52.200477 17469 caffe.cpp:312] Batch 242, accuracy/top1 = 0.68
I0630 19:08:52.200501 17469 caffe.cpp:312] Batch 242, accuracy/top5 = 0.86
I0630 19:08:52.200506 17469 caffe.cpp:312] Batch 242, loss = 1.24
I0630 19:08:52.252794 17469 caffe.cpp:312] Batch 243, accuracy/top1 = 0.52
I0630 19:08:52.252818 17469 caffe.cpp:312] Batch 243, accuracy/top5 = 0.68
I0630 19:08:52.252823 17469 caffe.cpp:312] Batch 243, loss = 1.78
I0630 19:08:52.305125 17469 caffe.cpp:312] Batch 244, accuracy/top1 = 0.42
I0630 19:08:52.305150 17469 caffe.cpp:312] Batch 244, accuracy/top5 = 0.76
I0630 19:08:52.305155 17469 caffe.cpp:312] Batch 244, loss = 1.92
I0630 19:08:52.357734 17469 caffe.cpp:312] Batch 245, accuracy/top1 = 0.36
I0630 19:08:52.357759 17469 caffe.cpp:312] Batch 245, accuracy/top5 = 0.72
I0630 19:08:52.357764 17469 caffe.cpp:312] Batch 245, loss = 2.1
I0630 19:08:52.410466 17469 caffe.cpp:312] Batch 246, accuracy/top1 = 0.46
I0630 19:08:52.410492 17469 caffe.cpp:312] Batch 246, accuracy/top5 = 0.78
I0630 19:08:52.410496 17469 caffe.cpp:312] Batch 246, loss = 1.96
I0630 19:08:52.463696 17469 caffe.cpp:312] Batch 247, accuracy/top1 = 0.42
I0630 19:08:52.463721 17469 caffe.cpp:312] Batch 247, accuracy/top5 = 0.72
I0630 19:08:52.463724 17469 caffe.cpp:312] Batch 247, loss = 2.06
I0630 19:08:52.516080 17469 caffe.cpp:312] Batch 248, accuracy/top1 = 0.56
I0630 19:08:52.516103 17469 caffe.cpp:312] Batch 248, accuracy/top5 = 0.68
I0630 19:08:52.516108 17469 caffe.cpp:312] Batch 248, loss = 2.04
I0630 19:08:52.568717 17469 caffe.cpp:312] Batch 249, accuracy/top1 = 0.44
I0630 19:08:52.568743 17469 caffe.cpp:312] Batch 249, accuracy/top5 = 0.76
I0630 19:08:52.568747 17469 caffe.cpp:312] Batch 249, loss = 1.62
I0630 19:08:52.621553 17469 caffe.cpp:312] Batch 250, accuracy/top1 = 0.66
I0630 19:08:52.621577 17469 caffe.cpp:312] Batch 250, accuracy/top5 = 0.84
I0630 19:08:52.621582 17469 caffe.cpp:312] Batch 250, loss = 0.96
I0630 19:08:52.673805 17469 caffe.cpp:312] Batch 251, accuracy/top1 = 0.56
I0630 19:08:52.673830 17469 caffe.cpp:312] Batch 251, accuracy/top5 = 0.8
I0630 19:08:52.673833 17469 caffe.cpp:312] Batch 251, loss = 1.46
I0630 19:08:52.726136 17469 caffe.cpp:312] Batch 252, accuracy/top1 = 0.56
I0630 19:08:52.726176 17469 caffe.cpp:312] Batch 252, accuracy/top5 = 0.76
I0630 19:08:52.726181 17469 caffe.cpp:312] Batch 252, loss = 1.9
I0630 19:08:52.778228 17469 caffe.cpp:312] Batch 253, accuracy/top1 = 0.46
I0630 19:08:52.778254 17469 caffe.cpp:312] Batch 253, accuracy/top5 = 0.8
I0630 19:08:52.778259 17469 caffe.cpp:312] Batch 253, loss = 1.82
I0630 19:08:52.830463 17469 caffe.cpp:312] Batch 254, accuracy/top1 = 0.4
I0630 19:08:52.830488 17469 caffe.cpp:312] Batch 254, accuracy/top5 = 0.78
I0630 19:08:52.830492 17469 caffe.cpp:312] Batch 254, loss = 1.94
I0630 19:08:52.882663 17469 caffe.cpp:312] Batch 255, accuracy/top1 = 0.5
I0630 19:08:52.882688 17469 caffe.cpp:312] Batch 255, accuracy/top5 = 0.72
I0630 19:08:52.882692 17469 caffe.cpp:312] Batch 255, loss = 2
I0630 19:08:52.935503 17469 caffe.cpp:312] Batch 256, accuracy/top1 = 0.58
I0630 19:08:52.935528 17469 caffe.cpp:312] Batch 256, accuracy/top5 = 0.8
I0630 19:08:52.935533 17469 caffe.cpp:312] Batch 256, loss = 1.46
I0630 19:08:52.988193 17469 caffe.cpp:312] Batch 257, accuracy/top1 = 0.6
I0630 19:08:52.988219 17469 caffe.cpp:312] Batch 257, accuracy/top5 = 0.78
I0630 19:08:52.988224 17469 caffe.cpp:312] Batch 257, loss = 1.72
I0630 19:08:53.039952 17469 caffe.cpp:312] Batch 258, accuracy/top1 = 0.56
I0630 19:08:53.039976 17469 caffe.cpp:312] Batch 258, accuracy/top5 = 0.76
I0630 19:08:53.039981 17469 caffe.cpp:312] Batch 258, loss = 1.74
I0630 19:08:53.091512 17469 caffe.cpp:312] Batch 259, accuracy/top1 = 0.48
I0630 19:08:53.091536 17469 caffe.cpp:312] Batch 259, accuracy/top5 = 0.72
I0630 19:08:53.091542 17469 caffe.cpp:312] Batch 259, loss = 1.78
I0630 19:08:53.143050 17469 caffe.cpp:312] Batch 260, accuracy/top1 = 0.5
I0630 19:08:53.143075 17469 caffe.cpp:312] Batch 260, accuracy/top5 = 0.76
I0630 19:08:53.143080 17469 caffe.cpp:312] Batch 260, loss = 1.64
I0630 19:08:53.195304 17469 caffe.cpp:312] Batch 261, accuracy/top1 = 0.58
I0630 19:08:53.195327 17469 caffe.cpp:312] Batch 261, accuracy/top5 = 0.78
I0630 19:08:53.195329 17469 caffe.cpp:312] Batch 261, loss = 1.74
I0630 19:08:53.245966 17469 caffe.cpp:312] Batch 262, accuracy/top1 = 0.48
I0630 19:08:53.245987 17469 caffe.cpp:312] Batch 262, accuracy/top5 = 0.8
I0630 19:08:53.245991 17469 caffe.cpp:312] Batch 262, loss = 1.68
I0630 19:08:53.294296 17469 caffe.cpp:312] Batch 263, accuracy/top1 = 0.6
I0630 19:08:53.294319 17469 caffe.cpp:312] Batch 263, accuracy/top5 = 0.8
I0630 19:08:53.294322 17469 caffe.cpp:312] Batch 263, loss = 1.4
I0630 19:08:53.343849 17469 caffe.cpp:312] Batch 264, accuracy/top1 = 0.56
I0630 19:08:53.343870 17469 caffe.cpp:312] Batch 264, accuracy/top5 = 0.74
I0630 19:08:53.343873 17469 caffe.cpp:312] Batch 264, loss = 1.48
I0630 19:08:53.392112 17469 caffe.cpp:312] Batch 265, accuracy/top1 = 0.68
I0630 19:08:53.392135 17469 caffe.cpp:312] Batch 265, accuracy/top5 = 0.88
I0630 19:08:53.392138 17469 caffe.cpp:312] Batch 265, loss = 1.12
I0630 19:08:53.440834 17469 caffe.cpp:312] Batch 266, accuracy/top1 = 0.56
I0630 19:08:53.440855 17469 caffe.cpp:312] Batch 266, accuracy/top5 = 0.76
I0630 19:08:53.440857 17469 caffe.cpp:312] Batch 266, loss = 1.6
I0630 19:08:53.488626 17469 caffe.cpp:312] Batch 267, accuracy/top1 = 0.66
I0630 19:08:53.488647 17469 caffe.cpp:312] Batch 267, accuracy/top5 = 0.84
I0630 19:08:53.488651 17469 caffe.cpp:312] Batch 267, loss = 1.22
I0630 19:08:53.537479 17469 caffe.cpp:312] Batch 268, accuracy/top1 = 0.62
I0630 19:08:53.537500 17469 caffe.cpp:312] Batch 268, accuracy/top5 = 0.78
I0630 19:08:53.537503 17469 caffe.cpp:312] Batch 268, loss = 1.72
I0630 19:08:53.586087 17469 caffe.cpp:312] Batch 269, accuracy/top1 = 0.6
I0630 19:08:53.586112 17469 caffe.cpp:312] Batch 269, accuracy/top5 = 0.8
I0630 19:08:53.586114 17469 caffe.cpp:312] Batch 269, loss = 1.28
I0630 19:08:53.635162 17469 caffe.cpp:312] Batch 270, accuracy/top1 = 0.5
I0630 19:08:53.635184 17469 caffe.cpp:312] Batch 270, accuracy/top5 = 0.82
I0630 19:08:53.635186 17469 caffe.cpp:312] Batch 270, loss = 1.76
I0630 19:08:53.684151 17469 caffe.cpp:312] Batch 271, accuracy/top1 = 0.5
I0630 19:08:53.684175 17469 caffe.cpp:312] Batch 271, accuracy/top5 = 0.7
I0630 19:08:53.684192 17469 caffe.cpp:312] Batch 271, loss = 2.14
I0630 19:08:53.732687 17469 caffe.cpp:312] Batch 272, accuracy/top1 = 0.48
I0630 19:08:53.732708 17469 caffe.cpp:312] Batch 272, accuracy/top5 = 0.82
I0630 19:08:53.732712 17469 caffe.cpp:312] Batch 272, loss = 1.98
I0630 19:08:53.782200 17469 caffe.cpp:312] Batch 273, accuracy/top1 = 0.62
I0630 19:08:53.782222 17469 caffe.cpp:312] Batch 273, accuracy/top5 = 0.8
I0630 19:08:53.782227 17469 caffe.cpp:312] Batch 273, loss = 1.44
I0630 19:08:53.830893 17469 caffe.cpp:312] Batch 274, accuracy/top1 = 0.5
I0630 19:08:53.830914 17469 caffe.cpp:312] Batch 274, accuracy/top5 = 0.78
I0630 19:08:53.830917 17469 caffe.cpp:312] Batch 274, loss = 1.82
I0630 19:08:53.880182 17469 caffe.cpp:312] Batch 275, accuracy/top1 = 0.64
I0630 19:08:53.880208 17469 caffe.cpp:312] Batch 275, accuracy/top5 = 0.8
I0630 19:08:53.880211 17469 caffe.cpp:312] Batch 275, loss = 1.4
I0630 19:08:53.932649 17469 caffe.cpp:312] Batch 276, accuracy/top1 = 0.64
I0630 19:08:53.932673 17469 caffe.cpp:312] Batch 276, accuracy/top5 = 0.82
I0630 19:08:53.932677 17469 caffe.cpp:312] Batch 276, loss = 1.26
I0630 19:08:53.985283 17469 caffe.cpp:312] Batch 277, accuracy/top1 = 0.64
I0630 19:08:53.985308 17469 caffe.cpp:312] Batch 277, accuracy/top5 = 0.78
I0630 19:08:53.985313 17469 caffe.cpp:312] Batch 277, loss = 1.26
I0630 19:08:54.038213 17469 caffe.cpp:312] Batch 278, accuracy/top1 = 0.44
I0630 19:08:54.038239 17469 caffe.cpp:312] Batch 278, accuracy/top5 = 0.64
I0630 19:08:54.038244 17469 caffe.cpp:312] Batch 278, loss = 2.28
I0630 19:08:54.090662 17469 caffe.cpp:312] Batch 279, accuracy/top1 = 0.56
I0630 19:08:54.090687 17469 caffe.cpp:312] Batch 279, accuracy/top5 = 0.74
I0630 19:08:54.090690 17469 caffe.cpp:312] Batch 279, loss = 1.88
I0630 19:08:54.142597 17469 caffe.cpp:312] Batch 280, accuracy/top1 = 0.56
I0630 19:08:54.142623 17469 caffe.cpp:312] Batch 280, accuracy/top5 = 0.82
I0630 19:08:54.142628 17469 caffe.cpp:312] Batch 280, loss = 1.4
I0630 19:08:54.194389 17469 caffe.cpp:312] Batch 281, accuracy/top1 = 0.62
I0630 19:08:54.194411 17469 caffe.cpp:312] Batch 281, accuracy/top5 = 0.88
I0630 19:08:54.194414 17469 caffe.cpp:312] Batch 281, loss = 1.56
I0630 19:08:54.245915 17469 caffe.cpp:312] Batch 282, accuracy/top1 = 0.58
I0630 19:08:54.245940 17469 caffe.cpp:312] Batch 282, accuracy/top5 = 0.8
I0630 19:08:54.245945 17469 caffe.cpp:312] Batch 282, loss = 1.36
I0630 19:08:54.296752 17469 caffe.cpp:312] Batch 283, accuracy/top1 = 0.54
I0630 19:08:54.296773 17469 caffe.cpp:312] Batch 283, accuracy/top5 = 0.72
I0630 19:08:54.296777 17469 caffe.cpp:312] Batch 283, loss = 1.68
I0630 19:08:54.348857 17469 caffe.cpp:312] Batch 284, accuracy/top1 = 0.66
I0630 19:08:54.348882 17469 caffe.cpp:312] Batch 284, accuracy/top5 = 0.76
I0630 19:08:54.348887 17469 caffe.cpp:312] Batch 284, loss = 1.58
I0630 19:08:54.400699 17469 caffe.cpp:312] Batch 285, accuracy/top1 = 0.56
I0630 19:08:54.400722 17469 caffe.cpp:312] Batch 285, accuracy/top5 = 0.86
I0630 19:08:54.400725 17469 caffe.cpp:312] Batch 285, loss = 1.36
I0630 19:08:54.452019 17469 caffe.cpp:312] Batch 286, accuracy/top1 = 0.56
I0630 19:08:54.452045 17469 caffe.cpp:312] Batch 286, accuracy/top5 = 0.82
I0630 19:08:54.452050 17469 caffe.cpp:312] Batch 286, loss = 1.42
I0630 19:08:54.503989 17469 caffe.cpp:312] Batch 287, accuracy/top1 = 0.5
I0630 19:08:54.504017 17469 caffe.cpp:312] Batch 287, accuracy/top5 = 0.68
I0630 19:08:54.504022 17469 caffe.cpp:312] Batch 287, loss = 1.96
I0630 19:08:54.555701 17469 caffe.cpp:312] Batch 288, accuracy/top1 = 0.54
I0630 19:08:54.555727 17469 caffe.cpp:312] Batch 288, accuracy/top5 = 0.78
I0630 19:08:54.555732 17469 caffe.cpp:312] Batch 288, loss = 1.88
I0630 19:08:54.608443 17469 caffe.cpp:312] Batch 289, accuracy/top1 = 0.54
I0630 19:08:54.608467 17469 caffe.cpp:312] Batch 289, accuracy/top5 = 0.8
I0630 19:08:54.608472 17469 caffe.cpp:312] Batch 289, loss = 1.48
I0630 19:08:54.661054 17469 caffe.cpp:312] Batch 290, accuracy/top1 = 0.52
I0630 19:08:54.661078 17469 caffe.cpp:312] Batch 290, accuracy/top5 = 0.74
I0630 19:08:54.661099 17469 caffe.cpp:312] Batch 290, loss = 1.7
I0630 19:08:54.713255 17469 caffe.cpp:312] Batch 291, accuracy/top1 = 0.58
I0630 19:08:54.713275 17469 caffe.cpp:312] Batch 291, accuracy/top5 = 0.74
I0630 19:08:54.713279 17469 caffe.cpp:312] Batch 291, loss = 1.82
I0630 19:08:54.762539 17469 caffe.cpp:312] Batch 292, accuracy/top1 = 0.66
I0630 19:08:54.762562 17469 caffe.cpp:312] Batch 292, accuracy/top5 = 0.82
I0630 19:08:54.762565 17469 caffe.cpp:312] Batch 292, loss = 1.46
I0630 19:08:54.812237 17469 caffe.cpp:312] Batch 293, accuracy/top1 = 0.64
I0630 19:08:54.812258 17469 caffe.cpp:312] Batch 293, accuracy/top5 = 0.8
I0630 19:08:54.812261 17469 caffe.cpp:312] Batch 293, loss = 1.3
I0630 19:08:54.861105 17469 caffe.cpp:312] Batch 294, accuracy/top1 = 0.72
I0630 19:08:54.861129 17469 caffe.cpp:312] Batch 294, accuracy/top5 = 0.96
I0630 19:08:54.861133 17469 caffe.cpp:312] Batch 294, loss = 0.56
I0630 19:08:54.914331 17469 caffe.cpp:312] Batch 295, accuracy/top1 = 0.5
I0630 19:08:54.914355 17469 caffe.cpp:312] Batch 295, accuracy/top5 = 0.7
I0630 19:08:54.914360 17469 caffe.cpp:312] Batch 295, loss = 1.84
I0630 19:08:54.966182 17469 caffe.cpp:312] Batch 296, accuracy/top1 = 0.64
I0630 19:08:54.966207 17469 caffe.cpp:312] Batch 296, accuracy/top5 = 0.88
I0630 19:08:54.966212 17469 caffe.cpp:312] Batch 296, loss = 1.16
I0630 19:08:55.018342 17469 caffe.cpp:312] Batch 297, accuracy/top1 = 0.62
I0630 19:08:55.018366 17469 caffe.cpp:312] Batch 297, accuracy/top5 = 0.86
I0630 19:08:55.018370 17469 caffe.cpp:312] Batch 297, loss = 1.14
I0630 19:08:55.070186 17469 caffe.cpp:312] Batch 298, accuracy/top1 = 0.56
I0630 19:08:55.070211 17469 caffe.cpp:312] Batch 298, accuracy/top5 = 0.8
I0630 19:08:55.070216 17469 caffe.cpp:312] Batch 298, loss = 1.74
I0630 19:08:55.122004 17469 caffe.cpp:312] Batch 299, accuracy/top1 = 0.46
I0630 19:08:55.122030 17469 caffe.cpp:312] Batch 299, accuracy/top5 = 0.7
I0630 19:08:55.122033 17469 caffe.cpp:312] Batch 299, loss = 2
I0630 19:08:55.174234 17469 caffe.cpp:312] Batch 300, accuracy/top1 = 0.52
I0630 19:08:55.174259 17469 caffe.cpp:312] Batch 300, accuracy/top5 = 0.7
I0630 19:08:55.174264 17469 caffe.cpp:312] Batch 300, loss = 2.02
I0630 19:08:55.226763 17469 caffe.cpp:312] Batch 301, accuracy/top1 = 0.44
I0630 19:08:55.226786 17469 caffe.cpp:312] Batch 301, accuracy/top5 = 0.66
I0630 19:08:55.226791 17469 caffe.cpp:312] Batch 301, loss = 2.04
I0630 19:08:55.278784 17469 caffe.cpp:312] Batch 302, accuracy/top1 = 0.6
I0630 19:08:55.278810 17469 caffe.cpp:312] Batch 302, accuracy/top5 = 0.84
I0630 19:08:55.278815 17469 caffe.cpp:312] Batch 302, loss = 1.36
I0630 19:08:55.331430 17469 caffe.cpp:312] Batch 303, accuracy/top1 = 0.58
I0630 19:08:55.331455 17469 caffe.cpp:312] Batch 303, accuracy/top5 = 0.84
I0630 19:08:55.331459 17469 caffe.cpp:312] Batch 303, loss = 1.34
I0630 19:08:55.383091 17469 caffe.cpp:312] Batch 304, accuracy/top1 = 0.62
I0630 19:08:55.383116 17469 caffe.cpp:312] Batch 304, accuracy/top5 = 0.8
I0630 19:08:55.383121 17469 caffe.cpp:312] Batch 304, loss = 1.2
I0630 19:08:55.436509 17469 caffe.cpp:312] Batch 305, accuracy/top1 = 0.64
I0630 19:08:55.436533 17469 caffe.cpp:312] Batch 305, accuracy/top5 = 0.84
I0630 19:08:55.436537 17469 caffe.cpp:312] Batch 305, loss = 1.1
I0630 19:08:55.488219 17469 caffe.cpp:312] Batch 306, accuracy/top1 = 0.66
I0630 19:08:55.488243 17469 caffe.cpp:312] Batch 306, accuracy/top5 = 0.84
I0630 19:08:55.488248 17469 caffe.cpp:312] Batch 306, loss = 1.22
I0630 19:08:55.540272 17469 caffe.cpp:312] Batch 307, accuracy/top1 = 0.58
I0630 19:08:55.540297 17469 caffe.cpp:312] Batch 307, accuracy/top5 = 0.8
I0630 19:08:55.540302 17469 caffe.cpp:312] Batch 307, loss = 1.48
I0630 19:08:55.591701 17469 caffe.cpp:312] Batch 308, accuracy/top1 = 0.74
I0630 19:08:55.591727 17469 caffe.cpp:312] Batch 308, accuracy/top5 = 0.86
I0630 19:08:55.591730 17469 caffe.cpp:312] Batch 308, loss = 1.16
I0630 19:08:55.645109 17469 caffe.cpp:312] Batch 309, accuracy/top1 = 0.52
I0630 19:08:55.645133 17469 caffe.cpp:312] Batch 309, accuracy/top5 = 0.7
I0630 19:08:55.645161 17469 caffe.cpp:312] Batch 309, loss = 1.6
I0630 19:08:55.697823 17469 caffe.cpp:312] Batch 310, accuracy/top1 = 0.6
I0630 19:08:55.697844 17469 caffe.cpp:312] Batch 310, accuracy/top5 = 0.88
I0630 19:08:55.697849 17469 caffe.cpp:312] Batch 310, loss = 1.14
I0630 19:08:55.748571 17469 caffe.cpp:312] Batch 311, accuracy/top1 = 0.5
I0630 19:08:55.748594 17469 caffe.cpp:312] Batch 311, accuracy/top5 = 0.76
I0630 19:08:55.748597 17469 caffe.cpp:312] Batch 311, loss = 2.04
I0630 19:08:55.797998 17469 caffe.cpp:312] Batch 312, accuracy/top1 = 0.62
I0630 19:08:55.798022 17469 caffe.cpp:312] Batch 312, accuracy/top5 = 0.82
I0630 19:08:55.798024 17469 caffe.cpp:312] Batch 312, loss = 1.44
I0630 19:08:55.847188 17469 caffe.cpp:312] Batch 313, accuracy/top1 = 0.58
I0630 19:08:55.847214 17469 caffe.cpp:312] Batch 313, accuracy/top5 = 0.84
I0630 19:08:55.847218 17469 caffe.cpp:312] Batch 313, loss = 1.5
I0630 19:08:55.895740 17469 caffe.cpp:312] Batch 314, accuracy/top1 = 0.58
I0630 19:08:55.895761 17469 caffe.cpp:312] Batch 314, accuracy/top5 = 0.84
I0630 19:08:55.895766 17469 caffe.cpp:312] Batch 314, loss = 1.18
I0630 19:08:55.943598 17469 caffe.cpp:312] Batch 315, accuracy/top1 = 0.52
I0630 19:08:55.943619 17469 caffe.cpp:312] Batch 315, accuracy/top5 = 0.8
I0630 19:08:55.943622 17469 caffe.cpp:312] Batch 315, loss = 1.6
I0630 19:08:55.991973 17469 caffe.cpp:312] Batch 316, accuracy/top1 = 0.52
I0630 19:08:55.991997 17469 caffe.cpp:312] Batch 316, accuracy/top5 = 0.76
I0630 19:08:55.991999 17469 caffe.cpp:312] Batch 316, loss = 1.76
I0630 19:08:56.040391 17469 caffe.cpp:312] Batch 317, accuracy/top1 = 0.62
I0630 19:08:56.040416 17469 caffe.cpp:312] Batch 317, accuracy/top5 = 0.84
I0630 19:08:56.040418 17469 caffe.cpp:312] Batch 317, loss = 1.22
I0630 19:08:56.088104 17469 caffe.cpp:312] Batch 318, accuracy/top1 = 0.56
I0630 19:08:56.088126 17469 caffe.cpp:312] Batch 318, accuracy/top5 = 0.76
I0630 19:08:56.088129 17469 caffe.cpp:312] Batch 318, loss = 1.62
I0630 19:08:56.136570 17469 caffe.cpp:312] Batch 319, accuracy/top1 = 0.54
I0630 19:08:56.136591 17469 caffe.cpp:312] Batch 319, accuracy/top5 = 0.72
I0630 19:08:56.136595 17469 caffe.cpp:312] Batch 319, loss = 1.62
I0630 19:08:56.184437 17469 caffe.cpp:312] Batch 320, accuracy/top1 = 0.48
I0630 19:08:56.184461 17469 caffe.cpp:312] Batch 320, accuracy/top5 = 0.76
I0630 19:08:56.184464 17469 caffe.cpp:312] Batch 320, loss = 1.66
I0630 19:08:56.233495 17469 caffe.cpp:312] Batch 321, accuracy/top1 = 0.56
I0630 19:08:56.233516 17469 caffe.cpp:312] Batch 321, accuracy/top5 = 0.78
I0630 19:08:56.233520 17469 caffe.cpp:312] Batch 321, loss = 1.76
I0630 19:08:56.281860 17469 caffe.cpp:312] Batch 322, accuracy/top1 = 0.62
I0630 19:08:56.281883 17469 caffe.cpp:312] Batch 322, accuracy/top5 = 0.9
I0630 19:08:56.281888 17469 caffe.cpp:312] Batch 322, loss = 0.96
I0630 19:08:56.330996 17469 caffe.cpp:312] Batch 323, accuracy/top1 = 0.64
I0630 19:08:56.331017 17469 caffe.cpp:312] Batch 323, accuracy/top5 = 0.76
I0630 19:08:56.331020 17469 caffe.cpp:312] Batch 323, loss = 1.68
I0630 19:08:56.378463 17469 caffe.cpp:312] Batch 324, accuracy/top1 = 0.48
I0630 19:08:56.378489 17469 caffe.cpp:312] Batch 324, accuracy/top5 = 0.66
I0630 19:08:56.378491 17469 caffe.cpp:312] Batch 324, loss = 1.82
I0630 19:08:56.426977 17469 caffe.cpp:312] Batch 325, accuracy/top1 = 0.62
I0630 19:08:56.426998 17469 caffe.cpp:312] Batch 325, accuracy/top5 = 0.78
I0630 19:08:56.427001 17469 caffe.cpp:312] Batch 325, loss = 1.56
I0630 19:08:56.474969 17469 caffe.cpp:312] Batch 326, accuracy/top1 = 0.52
I0630 19:08:56.474992 17469 caffe.cpp:312] Batch 326, accuracy/top5 = 0.84
I0630 19:08:56.474995 17469 caffe.cpp:312] Batch 326, loss = 1.56
I0630 19:08:56.523687 17469 caffe.cpp:312] Batch 327, accuracy/top1 = 0.52
I0630 19:08:56.523707 17469 caffe.cpp:312] Batch 327, accuracy/top5 = 0.74
I0630 19:08:56.523710 17469 caffe.cpp:312] Batch 327, loss = 1.74
I0630 19:08:56.571959 17469 caffe.cpp:312] Batch 328, accuracy/top1 = 0.54
I0630 19:08:56.571980 17469 caffe.cpp:312] Batch 328, accuracy/top5 = 0.78
I0630 19:08:56.572000 17469 caffe.cpp:312] Batch 328, loss = 1.44
I0630 19:08:56.620085 17469 caffe.cpp:312] Batch 329, accuracy/top1 = 0.54
I0630 19:08:56.620106 17469 caffe.cpp:312] Batch 329, accuracy/top5 = 0.76
I0630 19:08:56.620110 17469 caffe.cpp:312] Batch 329, loss = 1.56
I0630 19:08:56.666998 17469 caffe.cpp:312] Batch 330, accuracy/top1 = 0.56
I0630 19:08:56.667019 17469 caffe.cpp:312] Batch 330, accuracy/top5 = 0.76
I0630 19:08:56.667023 17469 caffe.cpp:312] Batch 330, loss = 1.64
I0630 19:08:56.714850 17469 caffe.cpp:312] Batch 331, accuracy/top1 = 0.58
I0630 19:08:56.714870 17469 caffe.cpp:312] Batch 331, accuracy/top5 = 0.72
I0630 19:08:56.714874 17469 caffe.cpp:312] Batch 331, loss = 1.74
I0630 19:08:56.763432 17469 caffe.cpp:312] Batch 332, accuracy/top1 = 0.58
I0630 19:08:56.763454 17469 caffe.cpp:312] Batch 332, accuracy/top5 = 0.84
I0630 19:08:56.763458 17469 caffe.cpp:312] Batch 332, loss = 1.52
I0630 19:08:56.811295 17469 caffe.cpp:312] Batch 333, accuracy/top1 = 0.5
I0630 19:08:56.811317 17469 caffe.cpp:312] Batch 333, accuracy/top5 = 0.76
I0630 19:08:56.811321 17469 caffe.cpp:312] Batch 333, loss = 1.74
I0630 19:08:56.859951 17469 caffe.cpp:312] Batch 334, accuracy/top1 = 0.68
I0630 19:08:56.859971 17469 caffe.cpp:312] Batch 334, accuracy/top5 = 0.92
I0630 19:08:56.859974 17469 caffe.cpp:312] Batch 334, loss = 0.86
I0630 19:08:56.910331 17469 caffe.cpp:312] Batch 335, accuracy/top1 = 0.58
I0630 19:08:56.910352 17469 caffe.cpp:312] Batch 335, accuracy/top5 = 0.74
I0630 19:08:56.910356 17469 caffe.cpp:312] Batch 335, loss = 1.6
I0630 19:08:56.959919 17469 caffe.cpp:312] Batch 336, accuracy/top1 = 0.6
I0630 19:08:56.959949 17469 caffe.cpp:312] Batch 336, accuracy/top5 = 0.86
I0630 19:08:56.959954 17469 caffe.cpp:312] Batch 336, loss = 1.06
I0630 19:08:57.014330 17469 caffe.cpp:312] Batch 337, accuracy/top1 = 0.68
I0630 19:08:57.014358 17469 caffe.cpp:312] Batch 337, accuracy/top5 = 0.86
I0630 19:08:57.014361 17469 caffe.cpp:312] Batch 337, loss = 1.34
I0630 19:08:57.065407 17469 caffe.cpp:312] Batch 338, accuracy/top1 = 0.54
I0630 19:08:57.065428 17469 caffe.cpp:312] Batch 338, accuracy/top5 = 0.82
I0630 19:08:57.065431 17469 caffe.cpp:312] Batch 338, loss = 1.56
I0630 19:08:57.113023 17469 caffe.cpp:312] Batch 339, accuracy/top1 = 0.64
I0630 19:08:57.113044 17469 caffe.cpp:312] Batch 339, accuracy/top5 = 0.84
I0630 19:08:57.113046 17469 caffe.cpp:312] Batch 339, loss = 1.32
I0630 19:08:57.161067 17469 caffe.cpp:312] Batch 340, accuracy/top1 = 0.54
I0630 19:08:57.161089 17469 caffe.cpp:312] Batch 340, accuracy/top5 = 0.78
I0630 19:08:57.161092 17469 caffe.cpp:312] Batch 340, loss = 2.02
I0630 19:08:57.209719 17469 caffe.cpp:312] Batch 341, accuracy/top1 = 0.58
I0630 19:08:57.209741 17469 caffe.cpp:312] Batch 341, accuracy/top5 = 0.8
I0630 19:08:57.209744 17469 caffe.cpp:312] Batch 341, loss = 1.36
I0630 19:08:57.257256 17469 caffe.cpp:312] Batch 342, accuracy/top1 = 0.54
I0630 19:08:57.257280 17469 caffe.cpp:312] Batch 342, accuracy/top5 = 0.8
I0630 19:08:57.257283 17469 caffe.cpp:312] Batch 342, loss = 1.6
I0630 19:08:57.305575 17469 caffe.cpp:312] Batch 343, accuracy/top1 = 0.52
I0630 19:08:57.305598 17469 caffe.cpp:312] Batch 343, accuracy/top5 = 0.8
I0630 19:08:57.305601 17469 caffe.cpp:312] Batch 343, loss = 1.64
I0630 19:08:57.354005 17469 caffe.cpp:312] Batch 344, accuracy/top1 = 0.6
I0630 19:08:57.354029 17469 caffe.cpp:312] Batch 344, accuracy/top5 = 0.84
I0630 19:08:57.354032 17469 caffe.cpp:312] Batch 344, loss = 1.32
I0630 19:08:57.402118 17469 caffe.cpp:312] Batch 345, accuracy/top1 = 0.54
I0630 19:08:57.402140 17469 caffe.cpp:312] Batch 345, accuracy/top5 = 0.88
I0630 19:08:57.402144 17469 caffe.cpp:312] Batch 345, loss = 1.3
I0630 19:08:57.450239 17469 caffe.cpp:312] Batch 346, accuracy/top1 = 0.44
I0630 19:08:57.450263 17469 caffe.cpp:312] Batch 346, accuracy/top5 = 0.64
I0630 19:08:57.450265 17469 caffe.cpp:312] Batch 346, loss = 2.38
I0630 19:08:57.498729 17469 caffe.cpp:312] Batch 347, accuracy/top1 = 0.66
I0630 19:08:57.498751 17469 caffe.cpp:312] Batch 347, accuracy/top5 = 0.84
I0630 19:08:57.498769 17469 caffe.cpp:312] Batch 347, loss = 1.16
I0630 19:08:57.547658 17469 caffe.cpp:312] Batch 348, accuracy/top1 = 0.7
I0630 19:08:57.547682 17469 caffe.cpp:312] Batch 348, accuracy/top5 = 0.86
I0630 19:08:57.547684 17469 caffe.cpp:312] Batch 348, loss = 0.88
I0630 19:08:57.595477 17469 caffe.cpp:312] Batch 349, accuracy/top1 = 0.52
I0630 19:08:57.595499 17469 caffe.cpp:312] Batch 349, accuracy/top5 = 0.78
I0630 19:08:57.595501 17469 caffe.cpp:312] Batch 349, loss = 1.94
I0630 19:08:57.643435 17469 caffe.cpp:312] Batch 350, accuracy/top1 = 0.66
I0630 19:08:57.643460 17469 caffe.cpp:312] Batch 350, accuracy/top5 = 0.88
I0630 19:08:57.643462 17469 caffe.cpp:312] Batch 350, loss = 1.18
I0630 19:08:57.691017 17469 caffe.cpp:312] Batch 351, accuracy/top1 = 0.54
I0630 19:08:57.691038 17469 caffe.cpp:312] Batch 351, accuracy/top5 = 0.9
I0630 19:08:57.691041 17469 caffe.cpp:312] Batch 351, loss = 1.28
I0630 19:08:57.739545 17469 caffe.cpp:312] Batch 352, accuracy/top1 = 0.6
I0630 19:08:57.739568 17469 caffe.cpp:312] Batch 352, accuracy/top5 = 0.68
I0630 19:08:57.739572 17469 caffe.cpp:312] Batch 352, loss = 2.1
I0630 19:08:57.787986 17469 caffe.cpp:312] Batch 353, accuracy/top1 = 0.52
I0630 19:08:57.788007 17469 caffe.cpp:312] Batch 353, accuracy/top5 = 0.8
I0630 19:08:57.788012 17469 caffe.cpp:312] Batch 353, loss = 1.58
I0630 19:08:57.836419 17469 caffe.cpp:312] Batch 354, accuracy/top1 = 0.48
I0630 19:08:57.836442 17469 caffe.cpp:312] Batch 354, accuracy/top5 = 0.76
I0630 19:08:57.836446 17469 caffe.cpp:312] Batch 354, loss = 1.98
I0630 19:08:57.885905 17469 caffe.cpp:312] Batch 355, accuracy/top1 = 0.6
I0630 19:08:57.885924 17469 caffe.cpp:312] Batch 355, accuracy/top5 = 0.8
I0630 19:08:57.885928 17469 caffe.cpp:312] Batch 355, loss = 1.32
I0630 19:08:57.934312 17469 caffe.cpp:312] Batch 356, accuracy/top1 = 0.6
I0630 19:08:57.934335 17469 caffe.cpp:312] Batch 356, accuracy/top5 = 0.8
I0630 19:08:57.934339 17469 caffe.cpp:312] Batch 356, loss = 1.42
I0630 19:08:57.982389 17469 caffe.cpp:312] Batch 357, accuracy/top1 = 0.56
I0630 19:08:57.982410 17469 caffe.cpp:312] Batch 357, accuracy/top5 = 0.86
I0630 19:08:57.982414 17469 caffe.cpp:312] Batch 357, loss = 1.14
I0630 19:08:58.029953 17469 caffe.cpp:312] Batch 358, accuracy/top1 = 0.5
I0630 19:08:58.029974 17469 caffe.cpp:312] Batch 358, accuracy/top5 = 0.76
I0630 19:08:58.029978 17469 caffe.cpp:312] Batch 358, loss = 1.74
I0630 19:08:58.080312 17469 caffe.cpp:312] Batch 359, accuracy/top1 = 0.56
I0630 19:08:58.080334 17469 caffe.cpp:312] Batch 359, accuracy/top5 = 0.8
I0630 19:08:58.080338 17469 caffe.cpp:312] Batch 359, loss = 1.88
I0630 19:08:58.128913 17469 caffe.cpp:312] Batch 360, accuracy/top1 = 0.56
I0630 19:08:58.128937 17469 caffe.cpp:312] Batch 360, accuracy/top5 = 0.86
I0630 19:08:58.128939 17469 caffe.cpp:312] Batch 360, loss = 1.58
I0630 19:08:58.177271 17469 caffe.cpp:312] Batch 361, accuracy/top1 = 0.58
I0630 19:08:58.177292 17469 caffe.cpp:312] Batch 361, accuracy/top5 = 0.84
I0630 19:08:58.177295 17469 caffe.cpp:312] Batch 361, loss = 1.46
I0630 19:08:58.225980 17469 caffe.cpp:312] Batch 362, accuracy/top1 = 0.52
I0630 19:08:58.226003 17469 caffe.cpp:312] Batch 362, accuracy/top5 = 0.72
I0630 19:08:58.226006 17469 caffe.cpp:312] Batch 362, loss = 2.04
I0630 19:08:58.274054 17469 caffe.cpp:312] Batch 363, accuracy/top1 = 0.58
I0630 19:08:58.274075 17469 caffe.cpp:312] Batch 363, accuracy/top5 = 0.8
I0630 19:08:58.274078 17469 caffe.cpp:312] Batch 363, loss = 1.66
I0630 19:08:58.323222 17469 caffe.cpp:312] Batch 364, accuracy/top1 = 0.58
I0630 19:08:58.323246 17469 caffe.cpp:312] Batch 364, accuracy/top5 = 0.8
I0630 19:08:58.323249 17469 caffe.cpp:312] Batch 364, loss = 1.28
I0630 19:08:58.372195 17469 caffe.cpp:312] Batch 365, accuracy/top1 = 0.54
I0630 19:08:58.372215 17469 caffe.cpp:312] Batch 365, accuracy/top5 = 0.8
I0630 19:08:58.372220 17469 caffe.cpp:312] Batch 365, loss = 1.72
I0630 19:08:58.421200 17469 caffe.cpp:312] Batch 366, accuracy/top1 = 0.5
I0630 19:08:58.421222 17469 caffe.cpp:312] Batch 366, accuracy/top5 = 0.82
I0630 19:08:58.421241 17469 caffe.cpp:312] Batch 366, loss = 1.72
I0630 19:08:58.470523 17469 caffe.cpp:312] Batch 367, accuracy/top1 = 0.64
I0630 19:08:58.470544 17469 caffe.cpp:312] Batch 367, accuracy/top5 = 0.78
I0630 19:08:58.470548 17469 caffe.cpp:312] Batch 367, loss = 1.32
I0630 19:08:58.519533 17469 caffe.cpp:312] Batch 368, accuracy/top1 = 0.46
I0630 19:08:58.519556 17469 caffe.cpp:312] Batch 368, accuracy/top5 = 0.82
I0630 19:08:58.519558 17469 caffe.cpp:312] Batch 368, loss = 1.72
I0630 19:08:58.567818 17469 caffe.cpp:312] Batch 369, accuracy/top1 = 0.54
I0630 19:08:58.567840 17469 caffe.cpp:312] Batch 369, accuracy/top5 = 0.78
I0630 19:08:58.567843 17469 caffe.cpp:312] Batch 369, loss = 1.46
I0630 19:08:58.615350 17469 caffe.cpp:312] Batch 370, accuracy/top1 = 0.6
I0630 19:08:58.615375 17469 caffe.cpp:312] Batch 370, accuracy/top5 = 0.8
I0630 19:08:58.615377 17469 caffe.cpp:312] Batch 370, loss = 1.68
I0630 19:08:58.664073 17469 caffe.cpp:312] Batch 371, accuracy/top1 = 0.64
I0630 19:08:58.664094 17469 caffe.cpp:312] Batch 371, accuracy/top5 = 0.74
I0630 19:08:58.664098 17469 caffe.cpp:312] Batch 371, loss = 1.5
I0630 19:08:58.712854 17469 caffe.cpp:312] Batch 372, accuracy/top1 = 0.62
I0630 19:08:58.712877 17469 caffe.cpp:312] Batch 372, accuracy/top5 = 0.76
I0630 19:08:58.712879 17469 caffe.cpp:312] Batch 372, loss = 1.56
I0630 19:08:58.760641 17469 caffe.cpp:312] Batch 373, accuracy/top1 = 0.44
I0630 19:08:58.760663 17469 caffe.cpp:312] Batch 373, accuracy/top5 = 0.74
I0630 19:08:58.760666 17469 caffe.cpp:312] Batch 373, loss = 2
I0630 19:08:58.809257 17469 caffe.cpp:312] Batch 374, accuracy/top1 = 0.6
I0630 19:08:58.809281 17469 caffe.cpp:312] Batch 374, accuracy/top5 = 0.82
I0630 19:08:58.809284 17469 caffe.cpp:312] Batch 374, loss = 1.28
I0630 19:08:58.857393 17469 caffe.cpp:312] Batch 375, accuracy/top1 = 0.58
I0630 19:08:58.857416 17469 caffe.cpp:312] Batch 375, accuracy/top5 = 0.84
I0630 19:08:58.857419 17469 caffe.cpp:312] Batch 375, loss = 1.38
I0630 19:08:58.905412 17469 caffe.cpp:312] Batch 376, accuracy/top1 = 0.6
I0630 19:08:58.905434 17469 caffe.cpp:312] Batch 376, accuracy/top5 = 0.86
I0630 19:08:58.905437 17469 caffe.cpp:312] Batch 376, loss = 1.16
I0630 19:08:58.953483 17469 caffe.cpp:312] Batch 377, accuracy/top1 = 0.52
I0630 19:08:58.953505 17469 caffe.cpp:312] Batch 377, accuracy/top5 = 0.8
I0630 19:08:58.953508 17469 caffe.cpp:312] Batch 377, loss = 1.48
I0630 19:08:59.001886 17469 caffe.cpp:312] Batch 378, accuracy/top1 = 0.68
I0630 19:08:59.001910 17469 caffe.cpp:312] Batch 378, accuracy/top5 = 0.86
I0630 19:08:59.001914 17469 caffe.cpp:312] Batch 378, loss = 1.38
I0630 19:08:59.049368 17469 caffe.cpp:312] Batch 379, accuracy/top1 = 0.64
I0630 19:08:59.049388 17469 caffe.cpp:312] Batch 379, accuracy/top5 = 0.9
I0630 19:08:59.049391 17469 caffe.cpp:312] Batch 379, loss = 1.02
I0630 19:08:59.097276 17469 caffe.cpp:312] Batch 380, accuracy/top1 = 0.6
I0630 19:08:59.097299 17469 caffe.cpp:312] Batch 380, accuracy/top5 = 0.8
I0630 19:08:59.097302 17469 caffe.cpp:312] Batch 380, loss = 1.44
I0630 19:08:59.144872 17469 caffe.cpp:312] Batch 381, accuracy/top1 = 0.4
I0630 19:08:59.144893 17469 caffe.cpp:312] Batch 381, accuracy/top5 = 0.72
I0630 19:08:59.144896 17469 caffe.cpp:312] Batch 381, loss = 2.02
I0630 19:08:59.193212 17469 caffe.cpp:312] Batch 382, accuracy/top1 = 0.6
I0630 19:08:59.193234 17469 caffe.cpp:312] Batch 382, accuracy/top5 = 0.74
I0630 19:08:59.193238 17469 caffe.cpp:312] Batch 382, loss = 1.86
I0630 19:08:59.240747 17469 caffe.cpp:312] Batch 383, accuracy/top1 = 0.6
I0630 19:08:59.240768 17469 caffe.cpp:312] Batch 383, accuracy/top5 = 0.88
I0630 19:08:59.240772 17469 caffe.cpp:312] Batch 383, loss = 1.18
I0630 19:08:59.288321 17469 caffe.cpp:312] Batch 384, accuracy/top1 = 0.48
I0630 19:08:59.288343 17469 caffe.cpp:312] Batch 384, accuracy/top5 = 0.74
I0630 19:08:59.288347 17469 caffe.cpp:312] Batch 384, loss = 1.76
I0630 19:08:59.337055 17469 caffe.cpp:312] Batch 385, accuracy/top1 = 0.62
I0630 19:08:59.337079 17469 caffe.cpp:312] Batch 385, accuracy/top5 = 0.82
I0630 19:08:59.337096 17469 caffe.cpp:312] Batch 385, loss = 1.3
I0630 19:08:59.385159 17469 caffe.cpp:312] Batch 386, accuracy/top1 = 0.6
I0630 19:08:59.385184 17469 caffe.cpp:312] Batch 386, accuracy/top5 = 0.82
I0630 19:08:59.385186 17469 caffe.cpp:312] Batch 386, loss = 1.54
I0630 19:08:59.432597 17469 caffe.cpp:312] Batch 387, accuracy/top1 = 0.62
I0630 19:08:59.432621 17469 caffe.cpp:312] Batch 387, accuracy/top5 = 0.86
I0630 19:08:59.432623 17469 caffe.cpp:312] Batch 387, loss = 1.2
I0630 19:08:59.480504 17469 caffe.cpp:312] Batch 388, accuracy/top1 = 0.58
I0630 19:08:59.480525 17469 caffe.cpp:312] Batch 388, accuracy/top5 = 0.84
I0630 19:08:59.480530 17469 caffe.cpp:312] Batch 388, loss = 1.18
I0630 19:08:59.527609 17469 caffe.cpp:312] Batch 389, accuracy/top1 = 0.5
I0630 19:08:59.527631 17469 caffe.cpp:312] Batch 389, accuracy/top5 = 0.7
I0630 19:08:59.527634 17469 caffe.cpp:312] Batch 389, loss = 1.9
I0630 19:08:59.575274 17469 caffe.cpp:312] Batch 390, accuracy/top1 = 0.62
I0630 19:08:59.575299 17469 caffe.cpp:312] Batch 390, accuracy/top5 = 0.82
I0630 19:08:59.575302 17469 caffe.cpp:312] Batch 390, loss = 1.2
I0630 19:08:59.623288 17469 caffe.cpp:312] Batch 391, accuracy/top1 = 0.54
I0630 19:08:59.623311 17469 caffe.cpp:312] Batch 391, accuracy/top5 = 0.84
I0630 19:08:59.623313 17469 caffe.cpp:312] Batch 391, loss = 1.62
I0630 19:08:59.671058 17469 caffe.cpp:312] Batch 392, accuracy/top1 = 0.56
I0630 19:08:59.671083 17469 caffe.cpp:312] Batch 392, accuracy/top5 = 0.88
I0630 19:08:59.671087 17469 caffe.cpp:312] Batch 392, loss = 1.3
I0630 19:08:59.719307 17469 caffe.cpp:312] Batch 393, accuracy/top1 = 0.5
I0630 19:08:59.719328 17469 caffe.cpp:312] Batch 393, accuracy/top5 = 0.68
I0630 19:08:59.719331 17469 caffe.cpp:312] Batch 393, loss = 2.06
I0630 19:08:59.767019 17469 caffe.cpp:312] Batch 394, accuracy/top1 = 0.48
I0630 19:08:59.767042 17469 caffe.cpp:312] Batch 394, accuracy/top5 = 0.78
I0630 19:08:59.767046 17469 caffe.cpp:312] Batch 394, loss = 1.62
I0630 19:08:59.814788 17469 caffe.cpp:312] Batch 395, accuracy/top1 = 0.6
I0630 19:08:59.814810 17469 caffe.cpp:312] Batch 395, accuracy/top5 = 0.78
I0630 19:08:59.814813 17469 caffe.cpp:312] Batch 395, loss = 1.62
I0630 19:08:59.862948 17469 caffe.cpp:312] Batch 396, accuracy/top1 = 0.54
I0630 19:08:59.862969 17469 caffe.cpp:312] Batch 396, accuracy/top5 = 0.76
I0630 19:08:59.862972 17469 caffe.cpp:312] Batch 396, loss = 1.78
I0630 19:08:59.912400 17469 caffe.cpp:312] Batch 397, accuracy/top1 = 0.44
I0630 19:08:59.912422 17469 caffe.cpp:312] Batch 397, accuracy/top5 = 0.74
I0630 19:08:59.912425 17469 caffe.cpp:312] Batch 397, loss = 1.98
I0630 19:08:59.959492 17469 caffe.cpp:312] Batch 398, accuracy/top1 = 0.56
I0630 19:08:59.959514 17469 caffe.cpp:312] Batch 398, accuracy/top5 = 0.78
I0630 19:08:59.959518 17469 caffe.cpp:312] Batch 398, loss = 1.46
I0630 19:09:00.006531 17469 caffe.cpp:312] Batch 399, accuracy/top1 = 0.62
I0630 19:09:00.006552 17469 caffe.cpp:312] Batch 399, accuracy/top5 = 0.82
I0630 19:09:00.006556 17469 caffe.cpp:312] Batch 399, loss = 1.3
I0630 19:09:00.055094 17469 caffe.cpp:312] Batch 400, accuracy/top1 = 0.44
I0630 19:09:00.055117 17469 caffe.cpp:312] Batch 400, accuracy/top5 = 0.74
I0630 19:09:00.055121 17469 caffe.cpp:312] Batch 400, loss = 1.88
I0630 19:09:00.102684 17469 caffe.cpp:312] Batch 401, accuracy/top1 = 0.58
I0630 19:09:00.102707 17469 caffe.cpp:312] Batch 401, accuracy/top5 = 0.76
I0630 19:09:00.102710 17469 caffe.cpp:312] Batch 401, loss = 1.34
I0630 19:09:00.150499 17469 caffe.cpp:312] Batch 402, accuracy/top1 = 0.54
I0630 19:09:00.150523 17469 caffe.cpp:312] Batch 402, accuracy/top5 = 0.74
I0630 19:09:00.150527 17469 caffe.cpp:312] Batch 402, loss = 1.82
I0630 19:09:00.198819 17469 caffe.cpp:312] Batch 403, accuracy/top1 = 0.42
I0630 19:09:00.198844 17469 caffe.cpp:312] Batch 403, accuracy/top5 = 0.8
I0630 19:09:00.198848 17469 caffe.cpp:312] Batch 403, loss = 1.66
I0630 19:09:00.247412 17469 caffe.cpp:312] Batch 404, accuracy/top1 = 0.64
I0630 19:09:00.247434 17469 caffe.cpp:312] Batch 404, accuracy/top5 = 0.88
I0630 19:09:00.247452 17469 caffe.cpp:312] Batch 404, loss = 1.2
I0630 19:09:00.294983 17469 caffe.cpp:312] Batch 405, accuracy/top1 = 0.58
I0630 19:09:00.295006 17469 caffe.cpp:312] Batch 405, accuracy/top5 = 0.82
I0630 19:09:00.295009 17469 caffe.cpp:312] Batch 405, loss = 1.28
I0630 19:09:00.342149 17469 caffe.cpp:312] Batch 406, accuracy/top1 = 0.66
I0630 19:09:00.342172 17469 caffe.cpp:312] Batch 406, accuracy/top5 = 0.86
I0630 19:09:00.342175 17469 caffe.cpp:312] Batch 406, loss = 1.26
I0630 19:09:00.389878 17469 caffe.cpp:312] Batch 407, accuracy/top1 = 0.64
I0630 19:09:00.389900 17469 caffe.cpp:312] Batch 407, accuracy/top5 = 0.88
I0630 19:09:00.389904 17469 caffe.cpp:312] Batch 407, loss = 1.26
I0630 19:09:00.437144 17469 caffe.cpp:312] Batch 408, accuracy/top1 = 0.42
I0630 19:09:00.437168 17469 caffe.cpp:312] Batch 408, accuracy/top5 = 0.7
I0630 19:09:00.437172 17469 caffe.cpp:312] Batch 408, loss = 2.66
I0630 19:09:00.484899 17469 caffe.cpp:312] Batch 409, accuracy/top1 = 0.46
I0630 19:09:00.484921 17469 caffe.cpp:312] Batch 409, accuracy/top5 = 0.68
I0630 19:09:00.484925 17469 caffe.cpp:312] Batch 409, loss = 2.12
I0630 19:09:00.532356 17469 caffe.cpp:312] Batch 410, accuracy/top1 = 0.62
I0630 19:09:00.532379 17469 caffe.cpp:312] Batch 410, accuracy/top5 = 0.82
I0630 19:09:00.532382 17469 caffe.cpp:312] Batch 410, loss = 1.62
I0630 19:09:00.579746 17469 caffe.cpp:312] Batch 411, accuracy/top1 = 0.58
I0630 19:09:00.579768 17469 caffe.cpp:312] Batch 411, accuracy/top5 = 0.78
I0630 19:09:00.579771 17469 caffe.cpp:312] Batch 411, loss = 1.38
I0630 19:09:00.627002 17469 caffe.cpp:312] Batch 412, accuracy/top1 = 0.62
I0630 19:09:00.627024 17469 caffe.cpp:312] Batch 412, accuracy/top5 = 0.84
I0630 19:09:00.627028 17469 caffe.cpp:312] Batch 412, loss = 1.08
I0630 19:09:00.675073 17469 caffe.cpp:312] Batch 413, accuracy/top1 = 0.54
I0630 19:09:00.675096 17469 caffe.cpp:312] Batch 413, accuracy/top5 = 0.82
I0630 19:09:00.675099 17469 caffe.cpp:312] Batch 413, loss = 1.48
I0630 19:09:00.723453 17469 caffe.cpp:312] Batch 414, accuracy/top1 = 0.42
I0630 19:09:00.723475 17469 caffe.cpp:312] Batch 414, accuracy/top5 = 0.8
I0630 19:09:00.723479 17469 caffe.cpp:312] Batch 414, loss = 1.66
I0630 19:09:00.771028 17469 caffe.cpp:312] Batch 415, accuracy/top1 = 0.58
I0630 19:09:00.771049 17469 caffe.cpp:312] Batch 415, accuracy/top5 = 0.8
I0630 19:09:00.771051 17469 caffe.cpp:312] Batch 415, loss = 1.44
I0630 19:09:00.818081 17469 caffe.cpp:312] Batch 416, accuracy/top1 = 0.5
I0630 19:09:00.818104 17469 caffe.cpp:312] Batch 416, accuracy/top5 = 0.84
I0630 19:09:00.818106 17469 caffe.cpp:312] Batch 416, loss = 1.24
I0630 19:09:00.866785 17469 caffe.cpp:312] Batch 417, accuracy/top1 = 0.6
I0630 19:09:00.866806 17469 caffe.cpp:312] Batch 417, accuracy/top5 = 0.76
I0630 19:09:00.866811 17469 caffe.cpp:312] Batch 417, loss = 1.54
I0630 19:09:00.915525 17469 caffe.cpp:312] Batch 418, accuracy/top1 = 0.58
I0630 19:09:00.915549 17469 caffe.cpp:312] Batch 418, accuracy/top5 = 0.86
I0630 19:09:00.915552 17469 caffe.cpp:312] Batch 418, loss = 1.38
I0630 19:09:00.963881 17469 caffe.cpp:312] Batch 419, accuracy/top1 = 0.58
I0630 19:09:00.963902 17469 caffe.cpp:312] Batch 419, accuracy/top5 = 0.78
I0630 19:09:00.963906 17469 caffe.cpp:312] Batch 419, loss = 1.5
I0630 19:09:01.012187 17469 caffe.cpp:312] Batch 420, accuracy/top1 = 0.5
I0630 19:09:01.012212 17469 caffe.cpp:312] Batch 420, accuracy/top5 = 0.82
I0630 19:09:01.012214 17469 caffe.cpp:312] Batch 420, loss = 1.38
I0630 19:09:01.060071 17469 caffe.cpp:312] Batch 421, accuracy/top1 = 0.6
I0630 19:09:01.060092 17469 caffe.cpp:312] Batch 421, accuracy/top5 = 0.78
I0630 19:09:01.060096 17469 caffe.cpp:312] Batch 421, loss = 1.38
I0630 19:09:01.107782 17469 caffe.cpp:312] Batch 422, accuracy/top1 = 0.54
I0630 19:09:01.107805 17469 caffe.cpp:312] Batch 422, accuracy/top5 = 0.76
I0630 19:09:01.107810 17469 caffe.cpp:312] Batch 422, loss = 1.5
I0630 19:09:01.155933 17469 caffe.cpp:312] Batch 423, accuracy/top1 = 0.54
I0630 19:09:01.155956 17469 caffe.cpp:312] Batch 423, accuracy/top5 = 0.76
I0630 19:09:01.155973 17469 caffe.cpp:312] Batch 423, loss = 1.94
I0630 19:09:01.203486 17469 caffe.cpp:312] Batch 424, accuracy/top1 = 0.64
I0630 19:09:01.203510 17469 caffe.cpp:312] Batch 424, accuracy/top5 = 0.84
I0630 19:09:01.203512 17469 caffe.cpp:312] Batch 424, loss = 1.02
I0630 19:09:01.251164 17469 caffe.cpp:312] Batch 425, accuracy/top1 = 0.56
I0630 19:09:01.251186 17469 caffe.cpp:312] Batch 425, accuracy/top5 = 0.78
I0630 19:09:01.251189 17469 caffe.cpp:312] Batch 425, loss = 1.44
I0630 19:09:01.299722 17469 caffe.cpp:312] Batch 426, accuracy/top1 = 0.5
I0630 19:09:01.299744 17469 caffe.cpp:312] Batch 426, accuracy/top5 = 0.82
I0630 19:09:01.299748 17469 caffe.cpp:312] Batch 426, loss = 1.44
I0630 19:09:01.346679 17469 caffe.cpp:312] Batch 427, accuracy/top1 = 0.56
I0630 19:09:01.346701 17469 caffe.cpp:312] Batch 427, accuracy/top5 = 0.74
I0630 19:09:01.346704 17469 caffe.cpp:312] Batch 427, loss = 1.6
I0630 19:09:01.393918 17469 caffe.cpp:312] Batch 428, accuracy/top1 = 0.68
I0630 19:09:01.393941 17469 caffe.cpp:312] Batch 428, accuracy/top5 = 0.88
I0630 19:09:01.393944 17469 caffe.cpp:312] Batch 428, loss = 1.02
I0630 19:09:01.441335 17469 caffe.cpp:312] Batch 429, accuracy/top1 = 0.68
I0630 19:09:01.441357 17469 caffe.cpp:312] Batch 429, accuracy/top5 = 0.86
I0630 19:09:01.441360 17469 caffe.cpp:312] Batch 429, loss = 1.24
I0630 19:09:01.489398 17469 caffe.cpp:312] Batch 430, accuracy/top1 = 0.6
I0630 19:09:01.489421 17469 caffe.cpp:312] Batch 430, accuracy/top5 = 0.84
I0630 19:09:01.489424 17469 caffe.cpp:312] Batch 430, loss = 0.96
I0630 19:09:01.536949 17469 caffe.cpp:312] Batch 431, accuracy/top1 = 0.7
I0630 19:09:01.536970 17469 caffe.cpp:312] Batch 431, accuracy/top5 = 0.86
I0630 19:09:01.536973 17469 caffe.cpp:312] Batch 431, loss = 1.4
I0630 19:09:01.584537 17469 caffe.cpp:312] Batch 432, accuracy/top1 = 0.52
I0630 19:09:01.584561 17469 caffe.cpp:312] Batch 432, accuracy/top5 = 0.72
I0630 19:09:01.584564 17469 caffe.cpp:312] Batch 432, loss = 1.96
I0630 19:09:01.632730 17469 caffe.cpp:312] Batch 433, accuracy/top1 = 0.7
I0630 19:09:01.632751 17469 caffe.cpp:312] Batch 433, accuracy/top5 = 0.88
I0630 19:09:01.632755 17469 caffe.cpp:312] Batch 433, loss = 1.18
I0630 19:09:01.680680 17469 caffe.cpp:312] Batch 434, accuracy/top1 = 0.54
I0630 19:09:01.680702 17469 caffe.cpp:312] Batch 434, accuracy/top5 = 0.78
I0630 19:09:01.680706 17469 caffe.cpp:312] Batch 434, loss = 1.84
I0630 19:09:01.728543 17469 caffe.cpp:312] Batch 435, accuracy/top1 = 0.62
I0630 19:09:01.728564 17469 caffe.cpp:312] Batch 435, accuracy/top5 = 0.88
I0630 19:09:01.728569 17469 caffe.cpp:312] Batch 435, loss = 1.2
I0630 19:09:01.776401 17469 caffe.cpp:312] Batch 436, accuracy/top1 = 0.5
I0630 19:09:01.776425 17469 caffe.cpp:312] Batch 436, accuracy/top5 = 0.74
I0630 19:09:01.776428 17469 caffe.cpp:312] Batch 436, loss = 2.02
I0630 19:09:01.824249 17469 caffe.cpp:312] Batch 437, accuracy/top1 = 0.52
I0630 19:09:01.824270 17469 caffe.cpp:312] Batch 437, accuracy/top5 = 0.76
I0630 19:09:01.824273 17469 caffe.cpp:312] Batch 437, loss = 1.74
I0630 19:09:01.873946 17469 caffe.cpp:312] Batch 438, accuracy/top1 = 0.64
I0630 19:09:01.873968 17469 caffe.cpp:312] Batch 438, accuracy/top5 = 0.82
I0630 19:09:01.873971 17469 caffe.cpp:312] Batch 438, loss = 1.66
I0630 19:09:01.920835 17469 caffe.cpp:312] Batch 439, accuracy/top1 = 0.5
I0630 19:09:01.920859 17469 caffe.cpp:312] Batch 439, accuracy/top5 = 0.82
I0630 19:09:01.920862 17469 caffe.cpp:312] Batch 439, loss = 2.1
I0630 19:09:01.968694 17469 caffe.cpp:312] Batch 440, accuracy/top1 = 0.6
I0630 19:09:01.968717 17469 caffe.cpp:312] Batch 440, accuracy/top5 = 0.76
I0630 19:09:01.968720 17469 caffe.cpp:312] Batch 440, loss = 1.52
I0630 19:09:02.015457 17469 caffe.cpp:312] Batch 441, accuracy/top1 = 0.58
I0630 19:09:02.015481 17469 caffe.cpp:312] Batch 441, accuracy/top5 = 0.88
I0630 19:09:02.015486 17469 caffe.cpp:312] Batch 441, loss = 1.2
I0630 19:09:02.064219 17469 caffe.cpp:312] Batch 442, accuracy/top1 = 0.64
I0630 19:09:02.064241 17469 caffe.cpp:312] Batch 442, accuracy/top5 = 0.86
I0630 19:09:02.064258 17469 caffe.cpp:312] Batch 442, loss = 1.38
I0630 19:09:02.111938 17469 caffe.cpp:312] Batch 443, accuracy/top1 = 0.72
I0630 19:09:02.111961 17469 caffe.cpp:312] Batch 443, accuracy/top5 = 0.8
I0630 19:09:02.111965 17469 caffe.cpp:312] Batch 443, loss = 1.38
I0630 19:09:02.159391 17469 caffe.cpp:312] Batch 444, accuracy/top1 = 0.66
I0630 19:09:02.159413 17469 caffe.cpp:312] Batch 444, accuracy/top5 = 0.88
I0630 19:09:02.159416 17469 caffe.cpp:312] Batch 444, loss = 1.18
I0630 19:09:02.207710 17469 caffe.cpp:312] Batch 445, accuracy/top1 = 0.64
I0630 19:09:02.207733 17469 caffe.cpp:312] Batch 445, accuracy/top5 = 0.84
I0630 19:09:02.207736 17469 caffe.cpp:312] Batch 445, loss = 1.06
I0630 19:09:02.256145 17469 caffe.cpp:312] Batch 446, accuracy/top1 = 0.64
I0630 19:09:02.256166 17469 caffe.cpp:312] Batch 446, accuracy/top5 = 0.92
I0630 19:09:02.256170 17469 caffe.cpp:312] Batch 446, loss = 0.98
I0630 19:09:02.303831 17469 caffe.cpp:312] Batch 447, accuracy/top1 = 0.58
I0630 19:09:02.303854 17469 caffe.cpp:312] Batch 447, accuracy/top5 = 0.76
I0630 19:09:02.303858 17469 caffe.cpp:312] Batch 447, loss = 1.38
I0630 19:09:02.352236 17469 caffe.cpp:312] Batch 448, accuracy/top1 = 0.58
I0630 19:09:02.352257 17469 caffe.cpp:312] Batch 448, accuracy/top5 = 0.8
I0630 19:09:02.352262 17469 caffe.cpp:312] Batch 448, loss = 1.24
I0630 19:09:02.400174 17469 caffe.cpp:312] Batch 449, accuracy/top1 = 0.56
I0630 19:09:02.400197 17469 caffe.cpp:312] Batch 449, accuracy/top5 = 0.76
I0630 19:09:02.400202 17469 caffe.cpp:312] Batch 449, loss = 1.48
I0630 19:09:02.447906 17469 caffe.cpp:312] Batch 450, accuracy/top1 = 0.58
I0630 19:09:02.447927 17469 caffe.cpp:312] Batch 450, accuracy/top5 = 0.78
I0630 19:09:02.447931 17469 caffe.cpp:312] Batch 450, loss = 1.44
I0630 19:09:02.494666 17469 caffe.cpp:312] Batch 451, accuracy/top1 = 0.54
I0630 19:09:02.494688 17469 caffe.cpp:312] Batch 451, accuracy/top5 = 0.86
I0630 19:09:02.494693 17469 caffe.cpp:312] Batch 451, loss = 1.44
I0630 19:09:02.542209 17469 caffe.cpp:312] Batch 452, accuracy/top1 = 0.5
I0630 19:09:02.542232 17469 caffe.cpp:312] Batch 452, accuracy/top5 = 0.74
I0630 19:09:02.542234 17469 caffe.cpp:312] Batch 452, loss = 1.46
I0630 19:09:02.590414 17469 caffe.cpp:312] Batch 453, accuracy/top1 = 0.5
I0630 19:09:02.590436 17469 caffe.cpp:312] Batch 453, accuracy/top5 = 0.88
I0630 19:09:02.590440 17469 caffe.cpp:312] Batch 453, loss = 1.42
I0630 19:09:02.639366 17469 caffe.cpp:312] Batch 454, accuracy/top1 = 0.58
I0630 19:09:02.639387 17469 caffe.cpp:312] Batch 454, accuracy/top5 = 0.74
I0630 19:09:02.639390 17469 caffe.cpp:312] Batch 454, loss = 1.82
I0630 19:09:02.687490 17469 caffe.cpp:312] Batch 455, accuracy/top1 = 0.56
I0630 19:09:02.687513 17469 caffe.cpp:312] Batch 455, accuracy/top5 = 0.76
I0630 19:09:02.687516 17469 caffe.cpp:312] Batch 455, loss = 1.54
I0630 19:09:02.734743 17469 caffe.cpp:312] Batch 456, accuracy/top1 = 0.5
I0630 19:09:02.734764 17469 caffe.cpp:312] Batch 456, accuracy/top5 = 0.84
I0630 19:09:02.734768 17469 caffe.cpp:312] Batch 456, loss = 1.72
I0630 19:09:02.782572 17469 caffe.cpp:312] Batch 457, accuracy/top1 = 0.56
I0630 19:09:02.782594 17469 caffe.cpp:312] Batch 457, accuracy/top5 = 0.74
I0630 19:09:02.782598 17469 caffe.cpp:312] Batch 457, loss = 1.82
I0630 19:09:02.829999 17469 caffe.cpp:312] Batch 458, accuracy/top1 = 0.46
I0630 19:09:02.830019 17469 caffe.cpp:312] Batch 458, accuracy/top5 = 0.72
I0630 19:09:02.830023 17469 caffe.cpp:312] Batch 458, loss = 2.22
I0630 19:09:02.878489 17469 caffe.cpp:312] Batch 459, accuracy/top1 = 0.74
I0630 19:09:02.878509 17469 caffe.cpp:312] Batch 459, accuracy/top5 = 0.9
I0630 19:09:02.878513 17469 caffe.cpp:312] Batch 459, loss = 0.74
I0630 19:09:02.925873 17469 caffe.cpp:312] Batch 460, accuracy/top1 = 0.6
I0630 19:09:02.925894 17469 caffe.cpp:312] Batch 460, accuracy/top5 = 0.76
I0630 19:09:02.925896 17469 caffe.cpp:312] Batch 460, loss = 1.64
I0630 19:09:02.973367 17469 caffe.cpp:312] Batch 461, accuracy/top1 = 0.66
I0630 19:09:02.973390 17469 caffe.cpp:312] Batch 461, accuracy/top5 = 0.84
I0630 19:09:02.973407 17469 caffe.cpp:312] Batch 461, loss = 1.24
I0630 19:09:03.022197 17469 caffe.cpp:312] Batch 462, accuracy/top1 = 0.5
I0630 19:09:03.022217 17469 caffe.cpp:312] Batch 462, accuracy/top5 = 0.78
I0630 19:09:03.022222 17469 caffe.cpp:312] Batch 462, loss = 1.34
I0630 19:09:03.070992 17469 caffe.cpp:312] Batch 463, accuracy/top1 = 0.5
I0630 19:09:03.071013 17469 caffe.cpp:312] Batch 463, accuracy/top5 = 0.68
I0630 19:09:03.071017 17469 caffe.cpp:312] Batch 463, loss = 1.9
I0630 19:09:03.119554 17469 caffe.cpp:312] Batch 464, accuracy/top1 = 0.44
I0630 19:09:03.119575 17469 caffe.cpp:312] Batch 464, accuracy/top5 = 0.84
I0630 19:09:03.119578 17469 caffe.cpp:312] Batch 464, loss = 1.72
I0630 19:09:03.167361 17469 caffe.cpp:312] Batch 465, accuracy/top1 = 0.56
I0630 19:09:03.167383 17469 caffe.cpp:312] Batch 465, accuracy/top5 = 0.76
I0630 19:09:03.167387 17469 caffe.cpp:312] Batch 465, loss = 1.66
I0630 19:09:03.214962 17469 caffe.cpp:312] Batch 466, accuracy/top1 = 0.58
I0630 19:09:03.214984 17469 caffe.cpp:312] Batch 466, accuracy/top5 = 0.76
I0630 19:09:03.214987 17469 caffe.cpp:312] Batch 466, loss = 1.64
I0630 19:09:03.261472 17469 caffe.cpp:312] Batch 467, accuracy/top1 = 0.6
I0630 19:09:03.261497 17469 caffe.cpp:312] Batch 467, accuracy/top5 = 0.78
I0630 19:09:03.261500 17469 caffe.cpp:312] Batch 467, loss = 1.44
I0630 19:09:03.309444 17469 caffe.cpp:312] Batch 468, accuracy/top1 = 0.54
I0630 19:09:03.309466 17469 caffe.cpp:312] Batch 468, accuracy/top5 = 0.8
I0630 19:09:03.309469 17469 caffe.cpp:312] Batch 468, loss = 1.5
I0630 19:09:03.357880 17469 caffe.cpp:312] Batch 469, accuracy/top1 = 0.6
I0630 19:09:03.357903 17469 caffe.cpp:312] Batch 469, accuracy/top5 = 0.76
I0630 19:09:03.357906 17469 caffe.cpp:312] Batch 469, loss = 1.68
I0630 19:09:03.405504 17469 caffe.cpp:312] Batch 470, accuracy/top1 = 0.5
I0630 19:09:03.405526 17469 caffe.cpp:312] Batch 470, accuracy/top5 = 0.84
I0630 19:09:03.405530 17469 caffe.cpp:312] Batch 470, loss = 1.38
I0630 19:09:03.453289 17469 caffe.cpp:312] Batch 471, accuracy/top1 = 0.52
I0630 19:09:03.453312 17469 caffe.cpp:312] Batch 471, accuracy/top5 = 0.8
I0630 19:09:03.453315 17469 caffe.cpp:312] Batch 471, loss = 1.38
I0630 19:09:03.500694 17469 caffe.cpp:312] Batch 472, accuracy/top1 = 0.6
I0630 19:09:03.500716 17469 caffe.cpp:312] Batch 472, accuracy/top5 = 0.8
I0630 19:09:03.500720 17469 caffe.cpp:312] Batch 472, loss = 1.32
I0630 19:09:03.548921 17469 caffe.cpp:312] Batch 473, accuracy/top1 = 0.68
I0630 19:09:03.548945 17469 caffe.cpp:312] Batch 473, accuracy/top5 = 0.82
I0630 19:09:03.548949 17469 caffe.cpp:312] Batch 473, loss = 1.34
I0630 19:09:03.596685 17469 caffe.cpp:312] Batch 474, accuracy/top1 = 0.58
I0630 19:09:03.596706 17469 caffe.cpp:312] Batch 474, accuracy/top5 = 0.78
I0630 19:09:03.596709 17469 caffe.cpp:312] Batch 474, loss = 1.42
I0630 19:09:03.644426 17469 caffe.cpp:312] Batch 475, accuracy/top1 = 0.58
I0630 19:09:03.644450 17469 caffe.cpp:312] Batch 475, accuracy/top5 = 0.8
I0630 19:09:03.644454 17469 caffe.cpp:312] Batch 475, loss = 1.6
I0630 19:09:03.691655 17469 caffe.cpp:312] Batch 476, accuracy/top1 = 0.46
I0630 19:09:03.691678 17469 caffe.cpp:312] Batch 476, accuracy/top5 = 0.8
I0630 19:09:03.691680 17469 caffe.cpp:312] Batch 476, loss = 1.64
I0630 19:09:03.738694 17469 caffe.cpp:312] Batch 477, accuracy/top1 = 0.5
I0630 19:09:03.738718 17469 caffe.cpp:312] Batch 477, accuracy/top5 = 0.74
I0630 19:09:03.738721 17469 caffe.cpp:312] Batch 477, loss = 1.62
I0630 19:09:03.787072 17469 caffe.cpp:312] Batch 478, accuracy/top1 = 0.5
I0630 19:09:03.787093 17469 caffe.cpp:312] Batch 478, accuracy/top5 = 0.84
I0630 19:09:03.787097 17469 caffe.cpp:312] Batch 478, loss = 1.46
I0630 19:09:03.834789 17469 caffe.cpp:312] Batch 479, accuracy/top1 = 0.56
I0630 19:09:03.834811 17469 caffe.cpp:312] Batch 479, accuracy/top5 = 0.72
I0630 19:09:03.834815 17469 caffe.cpp:312] Batch 479, loss = 1.82
I0630 19:09:03.882794 17469 caffe.cpp:312] Batch 480, accuracy/top1 = 0.6
I0630 19:09:03.882817 17469 caffe.cpp:312] Batch 480, accuracy/top5 = 0.84
I0630 19:09:03.882838 17469 caffe.cpp:312] Batch 480, loss = 1.4
I0630 19:09:03.930505 17469 caffe.cpp:312] Batch 481, accuracy/top1 = 0.52
I0630 19:09:03.930527 17469 caffe.cpp:312] Batch 481, accuracy/top5 = 0.78
I0630 19:09:03.930531 17469 caffe.cpp:312] Batch 481, loss = 1.72
I0630 19:09:03.978350 17469 caffe.cpp:312] Batch 482, accuracy/top1 = 0.56
I0630 19:09:03.978371 17469 caffe.cpp:312] Batch 482, accuracy/top5 = 0.74
I0630 19:09:03.978374 17469 caffe.cpp:312] Batch 482, loss = 1.98
I0630 19:09:04.026952 17469 caffe.cpp:312] Batch 483, accuracy/top1 = 0.64
I0630 19:09:04.026975 17469 caffe.cpp:312] Batch 483, accuracy/top5 = 0.8
I0630 19:09:04.026981 17469 caffe.cpp:312] Batch 483, loss = 1.16
I0630 19:09:04.074636 17469 caffe.cpp:312] Batch 484, accuracy/top1 = 0.52
I0630 19:09:04.074659 17469 caffe.cpp:312] Batch 484, accuracy/top5 = 0.74
I0630 19:09:04.074662 17469 caffe.cpp:312] Batch 484, loss = 1.9
I0630 19:09:04.123136 17469 caffe.cpp:312] Batch 485, accuracy/top1 = 0.54
I0630 19:09:04.123158 17469 caffe.cpp:312] Batch 485, accuracy/top5 = 0.76
I0630 19:09:04.123162 17469 caffe.cpp:312] Batch 485, loss = 1.72
I0630 19:09:04.171382 17469 caffe.cpp:312] Batch 486, accuracy/top1 = 0.62
I0630 19:09:04.171404 17469 caffe.cpp:312] Batch 486, accuracy/top5 = 0.94
I0630 19:09:04.171408 17469 caffe.cpp:312] Batch 486, loss = 0.88
I0630 19:09:04.218914 17469 caffe.cpp:312] Batch 487, accuracy/top1 = 0.54
I0630 19:09:04.218936 17469 caffe.cpp:312] Batch 487, accuracy/top5 = 0.78
I0630 19:09:04.218940 17469 caffe.cpp:312] Batch 487, loss = 1.8
I0630 19:09:04.266429 17469 caffe.cpp:312] Batch 488, accuracy/top1 = 0.6
I0630 19:09:04.266451 17469 caffe.cpp:312] Batch 488, accuracy/top5 = 0.82
I0630 19:09:04.266455 17469 caffe.cpp:312] Batch 488, loss = 1.5
I0630 19:09:04.313938 17469 caffe.cpp:312] Batch 489, accuracy/top1 = 0.52
I0630 19:09:04.313961 17469 caffe.cpp:312] Batch 489, accuracy/top5 = 0.82
I0630 19:09:04.313966 17469 caffe.cpp:312] Batch 489, loss = 1.46
I0630 19:09:04.362757 17469 caffe.cpp:312] Batch 490, accuracy/top1 = 0.5
I0630 19:09:04.362781 17469 caffe.cpp:312] Batch 490, accuracy/top5 = 0.78
I0630 19:09:04.362783 17469 caffe.cpp:312] Batch 490, loss = 1.42
I0630 19:09:04.411155 17469 caffe.cpp:312] Batch 491, accuracy/top1 = 0.56
I0630 19:09:04.411178 17469 caffe.cpp:312] Batch 491, accuracy/top5 = 0.8
I0630 19:09:04.411181 17469 caffe.cpp:312] Batch 491, loss = 1.5
I0630 19:09:04.458561 17469 caffe.cpp:312] Batch 492, accuracy/top1 = 0.58
I0630 19:09:04.458583 17469 caffe.cpp:312] Batch 492, accuracy/top5 = 0.84
I0630 19:09:04.458587 17469 caffe.cpp:312] Batch 492, loss = 1.56
I0630 19:09:04.506568 17469 caffe.cpp:312] Batch 493, accuracy/top1 = 0.54
I0630 19:09:04.506592 17469 caffe.cpp:312] Batch 493, accuracy/top5 = 0.74
I0630 19:09:04.506594 17469 caffe.cpp:312] Batch 493, loss = 1.46
I0630 19:09:04.554847 17469 caffe.cpp:312] Batch 494, accuracy/top1 = 0.52
I0630 19:09:04.554869 17469 caffe.cpp:312] Batch 494, accuracy/top5 = 0.78
I0630 19:09:04.554872 17469 caffe.cpp:312] Batch 494, loss = 1.58
I0630 19:09:04.603528 17469 caffe.cpp:312] Batch 495, accuracy/top1 = 0.66
I0630 19:09:04.603550 17469 caffe.cpp:312] Batch 495, accuracy/top5 = 0.9
I0630 19:09:04.603554 17469 caffe.cpp:312] Batch 495, loss = 1.08
I0630 19:09:04.651190 17469 caffe.cpp:312] Batch 496, accuracy/top1 = 0.54
I0630 19:09:04.651212 17469 caffe.cpp:312] Batch 496, accuracy/top5 = 0.86
I0630 19:09:04.651216 17469 caffe.cpp:312] Batch 496, loss = 1.4
I0630 19:09:04.699133 17469 caffe.cpp:312] Batch 497, accuracy/top1 = 0.58
I0630 19:09:04.699156 17469 caffe.cpp:312] Batch 497, accuracy/top5 = 0.7
I0630 19:09:04.699160 17469 caffe.cpp:312] Batch 497, loss = 1.94
I0630 19:09:04.746835 17469 caffe.cpp:312] Batch 498, accuracy/top1 = 0.64
I0630 19:09:04.746858 17469 caffe.cpp:312] Batch 498, accuracy/top5 = 0.86
I0630 19:09:04.746861 17469 caffe.cpp:312] Batch 498, loss = 1.14
I0630 19:09:04.794658 17469 caffe.cpp:312] Batch 499, accuracy/top1 = 0.56
I0630 19:09:04.794682 17469 caffe.cpp:312] Batch 499, accuracy/top5 = 0.76
I0630 19:09:04.794700 17469 caffe.cpp:312] Batch 499, loss = 1.52
I0630 19:09:04.842130 17469 caffe.cpp:312] Batch 500, accuracy/top1 = 0.66
I0630 19:09:04.842152 17469 caffe.cpp:312] Batch 500, accuracy/top5 = 0.84
I0630 19:09:04.842155 17469 caffe.cpp:312] Batch 500, loss = 1.7
I0630 19:09:04.891261 17469 caffe.cpp:312] Batch 501, accuracy/top1 = 0.56
I0630 19:09:04.891283 17469 caffe.cpp:312] Batch 501, accuracy/top5 = 0.84
I0630 19:09:04.891286 17469 caffe.cpp:312] Batch 501, loss = 1.46
I0630 19:09:04.939038 17469 caffe.cpp:312] Batch 502, accuracy/top1 = 0.5
I0630 19:09:04.939059 17469 caffe.cpp:312] Batch 502, accuracy/top5 = 0.86
I0630 19:09:04.939062 17469 caffe.cpp:312] Batch 502, loss = 1.64
I0630 19:09:04.987148 17469 caffe.cpp:312] Batch 503, accuracy/top1 = 0.58
I0630 19:09:04.987170 17469 caffe.cpp:312] Batch 503, accuracy/top5 = 0.82
I0630 19:09:04.987174 17469 caffe.cpp:312] Batch 503, loss = 1.36
I0630 19:09:05.034534 17469 caffe.cpp:312] Batch 504, accuracy/top1 = 0.66
I0630 19:09:05.034556 17469 caffe.cpp:312] Batch 504, accuracy/top5 = 0.86
I0630 19:09:05.034559 17469 caffe.cpp:312] Batch 504, loss = 1.22
I0630 19:09:05.082854 17469 caffe.cpp:312] Batch 505, accuracy/top1 = 0.58
I0630 19:09:05.082877 17469 caffe.cpp:312] Batch 505, accuracy/top5 = 0.74
I0630 19:09:05.082881 17469 caffe.cpp:312] Batch 505, loss = 1.54
I0630 19:09:05.130831 17469 caffe.cpp:312] Batch 506, accuracy/top1 = 0.58
I0630 19:09:05.130861 17469 caffe.cpp:312] Batch 506, accuracy/top5 = 0.8
I0630 19:09:05.130864 17469 caffe.cpp:312] Batch 506, loss = 1.3
I0630 19:09:05.178887 17469 caffe.cpp:312] Batch 507, accuracy/top1 = 0.48
I0630 19:09:05.178913 17469 caffe.cpp:312] Batch 507, accuracy/top5 = 0.72
I0630 19:09:05.178916 17469 caffe.cpp:312] Batch 507, loss = 2.34
I0630 19:09:05.226719 17469 caffe.cpp:312] Batch 508, accuracy/top1 = 0.48
I0630 19:09:05.226742 17469 caffe.cpp:312] Batch 508, accuracy/top5 = 0.88
I0630 19:09:05.226744 17469 caffe.cpp:312] Batch 508, loss = 1.6
I0630 19:09:05.275141 17469 caffe.cpp:312] Batch 509, accuracy/top1 = 0.6
I0630 19:09:05.275164 17469 caffe.cpp:312] Batch 509, accuracy/top5 = 0.82
I0630 19:09:05.275167 17469 caffe.cpp:312] Batch 509, loss = 1.52
I0630 19:09:05.323839 17469 caffe.cpp:312] Batch 510, accuracy/top1 = 0.5
I0630 19:09:05.323861 17469 caffe.cpp:312] Batch 510, accuracy/top5 = 0.74
I0630 19:09:05.323864 17469 caffe.cpp:312] Batch 510, loss = 1.6
I0630 19:09:05.370818 17469 caffe.cpp:312] Batch 511, accuracy/top1 = 0.68
I0630 19:09:05.370843 17469 caffe.cpp:312] Batch 511, accuracy/top5 = 0.84
I0630 19:09:05.370846 17469 caffe.cpp:312] Batch 511, loss = 1.38
I0630 19:09:05.418432 17469 caffe.cpp:312] Batch 512, accuracy/top1 = 0.56
I0630 19:09:05.418455 17469 caffe.cpp:312] Batch 512, accuracy/top5 = 0.84
I0630 19:09:05.418459 17469 caffe.cpp:312] Batch 512, loss = 1.32
I0630 19:09:05.466825 17469 caffe.cpp:312] Batch 513, accuracy/top1 = 0.5
I0630 19:09:05.466853 17469 caffe.cpp:312] Batch 513, accuracy/top5 = 0.78
I0630 19:09:05.466856 17469 caffe.cpp:312] Batch 513, loss = 1.52
I0630 19:09:05.514477 17469 caffe.cpp:312] Batch 514, accuracy/top1 = 0.54
I0630 19:09:05.514500 17469 caffe.cpp:312] Batch 514, accuracy/top5 = 0.78
I0630 19:09:05.514503 17469 caffe.cpp:312] Batch 514, loss = 1.78
I0630 19:09:05.562639 17469 caffe.cpp:312] Batch 515, accuracy/top1 = 0.58
I0630 19:09:05.562664 17469 caffe.cpp:312] Batch 515, accuracy/top5 = 0.72
I0630 19:09:05.562666 17469 caffe.cpp:312] Batch 515, loss = 1.7
I0630 19:09:05.610023 17469 caffe.cpp:312] Batch 516, accuracy/top1 = 0.52
I0630 19:09:05.610044 17469 caffe.cpp:312] Batch 516, accuracy/top5 = 0.76
I0630 19:09:05.610047 17469 caffe.cpp:312] Batch 516, loss = 2.08
I0630 19:09:05.656719 17469 caffe.cpp:312] Batch 517, accuracy/top1 = 0.66
I0630 19:09:05.656744 17469 caffe.cpp:312] Batch 517, accuracy/top5 = 0.84
I0630 19:09:05.656746 17469 caffe.cpp:312] Batch 517, loss = 1.24
I0630 19:09:05.704547 17469 caffe.cpp:312] Batch 518, accuracy/top1 = 0.56
I0630 19:09:05.704569 17469 caffe.cpp:312] Batch 518, accuracy/top5 = 0.86
I0630 19:09:05.704587 17469 caffe.cpp:312] Batch 518, loss = 1.48
I0630 19:09:05.751907 17469 caffe.cpp:312] Batch 519, accuracy/top1 = 0.6
I0630 19:09:05.751931 17469 caffe.cpp:312] Batch 519, accuracy/top5 = 0.78
I0630 19:09:05.751935 17469 caffe.cpp:312] Batch 519, loss = 1.6
I0630 19:09:05.799536 17469 caffe.cpp:312] Batch 520, accuracy/top1 = 0.4
I0630 19:09:05.799559 17469 caffe.cpp:312] Batch 520, accuracy/top5 = 0.82
I0630 19:09:05.799563 17469 caffe.cpp:312] Batch 520, loss = 1.9
I0630 19:09:05.847010 17469 caffe.cpp:312] Batch 521, accuracy/top1 = 0.52
I0630 19:09:05.847044 17469 caffe.cpp:312] Batch 521, accuracy/top5 = 0.78
I0630 19:09:05.847049 17469 caffe.cpp:312] Batch 521, loss = 1.74
I0630 19:09:05.895426 17469 caffe.cpp:312] Batch 522, accuracy/top1 = 0.64
I0630 19:09:05.895447 17469 caffe.cpp:312] Batch 522, accuracy/top5 = 0.86
I0630 19:09:05.895452 17469 caffe.cpp:312] Batch 522, loss = 1.2
I0630 19:09:05.943341 17469 caffe.cpp:312] Batch 523, accuracy/top1 = 0.6
I0630 19:09:05.943364 17469 caffe.cpp:312] Batch 523, accuracy/top5 = 0.8
I0630 19:09:05.943368 17469 caffe.cpp:312] Batch 523, loss = 1.4
I0630 19:09:05.991111 17469 caffe.cpp:312] Batch 524, accuracy/top1 = 0.5
I0630 19:09:05.991132 17469 caffe.cpp:312] Batch 524, accuracy/top5 = 0.82
I0630 19:09:05.991137 17469 caffe.cpp:312] Batch 524, loss = 1.66
I0630 19:09:06.038388 17469 caffe.cpp:312] Batch 525, accuracy/top1 = 0.58
I0630 19:09:06.038410 17469 caffe.cpp:312] Batch 525, accuracy/top5 = 0.8
I0630 19:09:06.038414 17469 caffe.cpp:312] Batch 525, loss = 1.82
I0630 19:09:06.086062 17469 caffe.cpp:312] Batch 526, accuracy/top1 = 0.5
I0630 19:09:06.086086 17469 caffe.cpp:312] Batch 526, accuracy/top5 = 0.68
I0630 19:09:06.086089 17469 caffe.cpp:312] Batch 526, loss = 1.98
I0630 19:09:06.134133 17469 caffe.cpp:312] Batch 527, accuracy/top1 = 0.54
I0630 19:09:06.134156 17469 caffe.cpp:312] Batch 527, accuracy/top5 = 0.84
I0630 19:09:06.134161 17469 caffe.cpp:312] Batch 527, loss = 1.56
I0630 19:09:06.182066 17469 caffe.cpp:312] Batch 528, accuracy/top1 = 0.66
I0630 19:09:06.182088 17469 caffe.cpp:312] Batch 528, accuracy/top5 = 0.86
I0630 19:09:06.182092 17469 caffe.cpp:312] Batch 528, loss = 1.08
I0630 19:09:06.230092 17469 caffe.cpp:312] Batch 529, accuracy/top1 = 0.54
I0630 19:09:06.230114 17469 caffe.cpp:312] Batch 529, accuracy/top5 = 0.86
I0630 19:09:06.230118 17469 caffe.cpp:312] Batch 529, loss = 1.42
I0630 19:09:06.277741 17469 caffe.cpp:312] Batch 530, accuracy/top1 = 0.48
I0630 19:09:06.277765 17469 caffe.cpp:312] Batch 530, accuracy/top5 = 0.82
I0630 19:09:06.277768 17469 caffe.cpp:312] Batch 530, loss = 1.4
I0630 19:09:06.326119 17469 caffe.cpp:312] Batch 531, accuracy/top1 = 0.46
I0630 19:09:06.326143 17469 caffe.cpp:312] Batch 531, accuracy/top5 = 0.76
I0630 19:09:06.326146 17469 caffe.cpp:312] Batch 531, loss = 1.84
I0630 19:09:06.374938 17469 caffe.cpp:312] Batch 532, accuracy/top1 = 0.62
I0630 19:09:06.374960 17469 caffe.cpp:312] Batch 532, accuracy/top5 = 0.86
I0630 19:09:06.374964 17469 caffe.cpp:312] Batch 532, loss = 1.24
I0630 19:09:06.422742 17469 caffe.cpp:312] Batch 533, accuracy/top1 = 0.34
I0630 19:09:06.422767 17469 caffe.cpp:312] Batch 533, accuracy/top5 = 0.68
I0630 19:09:06.422772 17469 caffe.cpp:312] Batch 533, loss = 2.3
I0630 19:09:06.470013 17469 caffe.cpp:312] Batch 534, accuracy/top1 = 0.54
I0630 19:09:06.470036 17469 caffe.cpp:312] Batch 534, accuracy/top5 = 0.84
I0630 19:09:06.470041 17469 caffe.cpp:312] Batch 534, loss = 1.32
I0630 19:09:06.516898 17469 caffe.cpp:312] Batch 535, accuracy/top1 = 0.58
I0630 19:09:06.516921 17469 caffe.cpp:312] Batch 535, accuracy/top5 = 0.8
I0630 19:09:06.516926 17469 caffe.cpp:312] Batch 535, loss = 1.22
I0630 19:09:06.564400 17469 caffe.cpp:312] Batch 536, accuracy/top1 = 0.66
I0630 19:09:06.564424 17469 caffe.cpp:312] Batch 536, accuracy/top5 = 0.88
I0630 19:09:06.564427 17469 caffe.cpp:312] Batch 536, loss = 0.94
I0630 19:09:06.612665 17469 caffe.cpp:312] Batch 537, accuracy/top1 = 0.66
I0630 19:09:06.612689 17469 caffe.cpp:312] Batch 537, accuracy/top5 = 0.76
I0630 19:09:06.612715 17469 caffe.cpp:312] Batch 537, loss = 1.3
I0630 19:09:06.661475 17469 caffe.cpp:312] Batch 538, accuracy/top1 = 0.58
I0630 19:09:06.661497 17469 caffe.cpp:312] Batch 538, accuracy/top5 = 0.8
I0630 19:09:06.661500 17469 caffe.cpp:312] Batch 538, loss = 1.26
I0630 19:09:06.710371 17469 caffe.cpp:312] Batch 539, accuracy/top1 = 0.4
I0630 19:09:06.710392 17469 caffe.cpp:312] Batch 539, accuracy/top5 = 0.6
I0630 19:09:06.710397 17469 caffe.cpp:312] Batch 539, loss = 2.2
I0630 19:09:06.758333 17469 caffe.cpp:312] Batch 540, accuracy/top1 = 0.6
I0630 19:09:06.758354 17469 caffe.cpp:312] Batch 540, accuracy/top5 = 0.74
I0630 19:09:06.758358 17469 caffe.cpp:312] Batch 540, loss = 1.68
I0630 19:09:06.806473 17469 caffe.cpp:312] Batch 541, accuracy/top1 = 0.54
I0630 19:09:06.806496 17469 caffe.cpp:312] Batch 541, accuracy/top5 = 0.78
I0630 19:09:06.806499 17469 caffe.cpp:312] Batch 541, loss = 1.64
I0630 19:09:06.854156 17469 caffe.cpp:312] Batch 542, accuracy/top1 = 0.6
I0630 19:09:06.854178 17469 caffe.cpp:312] Batch 542, accuracy/top5 = 0.86
I0630 19:09:06.854183 17469 caffe.cpp:312] Batch 542, loss = 1.24
I0630 19:09:06.902694 17469 caffe.cpp:312] Batch 543, accuracy/top1 = 0.66
I0630 19:09:06.902717 17469 caffe.cpp:312] Batch 543, accuracy/top5 = 0.84
I0630 19:09:06.902721 17469 caffe.cpp:312] Batch 543, loss = 1.06
I0630 19:09:06.950012 17469 caffe.cpp:312] Batch 544, accuracy/top1 = 0.72
I0630 19:09:06.950036 17469 caffe.cpp:312] Batch 544, accuracy/top5 = 0.82
I0630 19:09:06.950039 17469 caffe.cpp:312] Batch 544, loss = 1.18
I0630 19:09:06.999387 17469 caffe.cpp:312] Batch 545, accuracy/top1 = 0.56
I0630 19:09:06.999410 17469 caffe.cpp:312] Batch 545, accuracy/top5 = 0.78
I0630 19:09:06.999414 17469 caffe.cpp:312] Batch 545, loss = 1.44
I0630 19:09:07.047013 17469 caffe.cpp:312] Batch 546, accuracy/top1 = 0.66
I0630 19:09:07.047034 17469 caffe.cpp:312] Batch 546, accuracy/top5 = 0.88
I0630 19:09:07.047039 17469 caffe.cpp:312] Batch 546, loss = 1.28
I0630 19:09:07.095016 17469 caffe.cpp:312] Batch 547, accuracy/top1 = 0.52
I0630 19:09:07.095042 17469 caffe.cpp:312] Batch 547, accuracy/top5 = 0.76
I0630 19:09:07.095046 17469 caffe.cpp:312] Batch 547, loss = 1.48
I0630 19:09:07.142309 17469 caffe.cpp:312] Batch 548, accuracy/top1 = 0.56
I0630 19:09:07.142331 17469 caffe.cpp:312] Batch 548, accuracy/top5 = 0.8
I0630 19:09:07.142335 17469 caffe.cpp:312] Batch 548, loss = 1.56
I0630 19:09:07.190507 17469 caffe.cpp:312] Batch 549, accuracy/top1 = 0.54
I0630 19:09:07.190531 17469 caffe.cpp:312] Batch 549, accuracy/top5 = 0.76
I0630 19:09:07.190534 17469 caffe.cpp:312] Batch 549, loss = 1.88
I0630 19:09:07.238828 17469 caffe.cpp:312] Batch 550, accuracy/top1 = 0.54
I0630 19:09:07.238862 17469 caffe.cpp:312] Batch 550, accuracy/top5 = 0.84
I0630 19:09:07.238867 17469 caffe.cpp:312] Batch 550, loss = 1.46
I0630 19:09:07.286998 17469 caffe.cpp:312] Batch 551, accuracy/top1 = 0.62
I0630 19:09:07.287019 17469 caffe.cpp:312] Batch 551, accuracy/top5 = 0.84
I0630 19:09:07.287022 17469 caffe.cpp:312] Batch 551, loss = 1.34
I0630 19:09:07.334583 17469 caffe.cpp:312] Batch 552, accuracy/top1 = 0.58
I0630 19:09:07.334605 17469 caffe.cpp:312] Batch 552, accuracy/top5 = 0.8
I0630 19:09:07.334609 17469 caffe.cpp:312] Batch 552, loss = 1.32
I0630 19:09:07.382565 17469 caffe.cpp:312] Batch 553, accuracy/top1 = 0.52
I0630 19:09:07.382589 17469 caffe.cpp:312] Batch 553, accuracy/top5 = 0.84
I0630 19:09:07.382592 17469 caffe.cpp:312] Batch 553, loss = 1.52
I0630 19:09:07.429934 17469 caffe.cpp:312] Batch 554, accuracy/top1 = 0.54
I0630 19:09:07.429955 17469 caffe.cpp:312] Batch 554, accuracy/top5 = 0.86
I0630 19:09:07.429960 17469 caffe.cpp:312] Batch 554, loss = 1.38
I0630 19:09:07.477993 17469 caffe.cpp:312] Batch 555, accuracy/top1 = 0.68
I0630 19:09:07.478015 17469 caffe.cpp:312] Batch 555, accuracy/top5 = 0.92
I0630 19:09:07.478018 17469 caffe.cpp:312] Batch 555, loss = 0.98
I0630 19:09:07.526103 17469 caffe.cpp:312] Batch 556, accuracy/top1 = 0.62
I0630 19:09:07.526124 17469 caffe.cpp:312] Batch 556, accuracy/top5 = 0.76
I0630 19:09:07.526141 17469 caffe.cpp:312] Batch 556, loss = 1.24
I0630 19:09:07.574375 17469 caffe.cpp:312] Batch 557, accuracy/top1 = 0.5
I0630 19:09:07.574398 17469 caffe.cpp:312] Batch 557, accuracy/top5 = 0.82
I0630 19:09:07.574401 17469 caffe.cpp:312] Batch 557, loss = 1.42
I0630 19:09:07.621654 17469 caffe.cpp:312] Batch 558, accuracy/top1 = 0.48
I0630 19:09:07.621676 17469 caffe.cpp:312] Batch 558, accuracy/top5 = 0.72
I0630 19:09:07.621680 17469 caffe.cpp:312] Batch 558, loss = 2.06
I0630 19:09:07.669823 17469 caffe.cpp:312] Batch 559, accuracy/top1 = 0.54
I0630 19:09:07.669845 17469 caffe.cpp:312] Batch 559, accuracy/top5 = 0.78
I0630 19:09:07.669848 17469 caffe.cpp:312] Batch 559, loss = 1.74
I0630 19:09:07.716758 17469 caffe.cpp:312] Batch 560, accuracy/top1 = 0.6
I0630 19:09:07.716780 17469 caffe.cpp:312] Batch 560, accuracy/top5 = 0.8
I0630 19:09:07.716784 17469 caffe.cpp:312] Batch 560, loss = 1.66
I0630 19:09:07.764626 17469 caffe.cpp:312] Batch 561, accuracy/top1 = 0.66
I0630 19:09:07.764649 17469 caffe.cpp:312] Batch 561, accuracy/top5 = 0.74
I0630 19:09:07.764653 17469 caffe.cpp:312] Batch 561, loss = 1.92
I0630 19:09:07.811596 17469 caffe.cpp:312] Batch 562, accuracy/top1 = 0.54
I0630 19:09:07.811619 17469 caffe.cpp:312] Batch 562, accuracy/top5 = 0.78
I0630 19:09:07.811622 17469 caffe.cpp:312] Batch 562, loss = 1.46
I0630 19:09:07.859403 17469 caffe.cpp:312] Batch 563, accuracy/top1 = 0.58
I0630 19:09:07.859433 17469 caffe.cpp:312] Batch 563, accuracy/top5 = 0.82
I0630 19:09:07.859437 17469 caffe.cpp:312] Batch 563, loss = 1.46
I0630 19:09:07.907878 17469 caffe.cpp:312] Batch 564, accuracy/top1 = 0.58
I0630 19:09:07.907901 17469 caffe.cpp:312] Batch 564, accuracy/top5 = 0.82
I0630 19:09:07.907903 17469 caffe.cpp:312] Batch 564, loss = 1.54
I0630 19:09:07.956159 17469 caffe.cpp:312] Batch 565, accuracy/top1 = 0.6
I0630 19:09:07.956182 17469 caffe.cpp:312] Batch 565, accuracy/top5 = 0.84
I0630 19:09:07.956184 17469 caffe.cpp:312] Batch 565, loss = 1.36
I0630 19:09:08.003921 17469 caffe.cpp:312] Batch 566, accuracy/top1 = 0.46
I0630 19:09:08.003943 17469 caffe.cpp:312] Batch 566, accuracy/top5 = 0.66
I0630 19:09:08.003947 17469 caffe.cpp:312] Batch 566, loss = 1.98
I0630 19:09:08.051815 17469 caffe.cpp:312] Batch 567, accuracy/top1 = 0.52
I0630 19:09:08.051837 17469 caffe.cpp:312] Batch 567, accuracy/top5 = 0.78
I0630 19:09:08.051841 17469 caffe.cpp:312] Batch 567, loss = 1.58
I0630 19:09:08.099818 17469 caffe.cpp:312] Batch 568, accuracy/top1 = 0.5
I0630 19:09:08.099840 17469 caffe.cpp:312] Batch 568, accuracy/top5 = 0.7
I0630 19:09:08.099843 17469 caffe.cpp:312] Batch 568, loss = 1.94
I0630 19:09:08.146944 17469 caffe.cpp:312] Batch 569, accuracy/top1 = 0.42
I0630 19:09:08.146970 17469 caffe.cpp:312] Batch 569, accuracy/top5 = 0.62
I0630 19:09:08.146972 17469 caffe.cpp:312] Batch 569, loss = 2.54
I0630 19:09:08.195199 17469 caffe.cpp:312] Batch 570, accuracy/top1 = 0.6
I0630 19:09:08.195221 17469 caffe.cpp:312] Batch 570, accuracy/top5 = 0.78
I0630 19:09:08.195225 17469 caffe.cpp:312] Batch 570, loss = 1.28
I0630 19:09:08.242653 17469 caffe.cpp:312] Batch 571, accuracy/top1 = 0.46
I0630 19:09:08.242677 17469 caffe.cpp:312] Batch 571, accuracy/top5 = 0.7
I0630 19:09:08.242681 17469 caffe.cpp:312] Batch 571, loss = 2
I0630 19:09:08.290701 17469 caffe.cpp:312] Batch 572, accuracy/top1 = 0.48
I0630 19:09:08.290724 17469 caffe.cpp:312] Batch 572, accuracy/top5 = 0.8
I0630 19:09:08.290727 17469 caffe.cpp:312] Batch 572, loss = 1.58
I0630 19:09:08.338505 17469 caffe.cpp:312] Batch 573, accuracy/top1 = 0.68
I0630 19:09:08.338527 17469 caffe.cpp:312] Batch 573, accuracy/top5 = 0.86
I0630 19:09:08.338531 17469 caffe.cpp:312] Batch 573, loss = 1.18
I0630 19:09:08.386327 17469 caffe.cpp:312] Batch 574, accuracy/top1 = 0.62
I0630 19:09:08.386348 17469 caffe.cpp:312] Batch 574, accuracy/top5 = 0.68
I0630 19:09:08.386350 17469 caffe.cpp:312] Batch 574, loss = 2.16
I0630 19:09:08.433869 17469 caffe.cpp:312] Batch 575, accuracy/top1 = 0.52
I0630 19:09:08.433893 17469 caffe.cpp:312] Batch 575, accuracy/top5 = 0.84
I0630 19:09:08.433910 17469 caffe.cpp:312] Batch 575, loss = 1.72
I0630 19:09:08.481029 17469 caffe.cpp:312] Batch 576, accuracy/top1 = 0.66
I0630 19:09:08.481050 17469 caffe.cpp:312] Batch 576, accuracy/top5 = 0.88
I0630 19:09:08.481055 17469 caffe.cpp:312] Batch 576, loss = 1.06
I0630 19:09:08.528592 17469 caffe.cpp:312] Batch 577, accuracy/top1 = 0.56
I0630 19:09:08.528614 17469 caffe.cpp:312] Batch 577, accuracy/top5 = 0.78
I0630 19:09:08.528619 17469 caffe.cpp:312] Batch 577, loss = 1.3
I0630 19:09:08.576956 17469 caffe.cpp:312] Batch 578, accuracy/top1 = 0.58
I0630 19:09:08.576978 17469 caffe.cpp:312] Batch 578, accuracy/top5 = 0.76
I0630 19:09:08.576982 17469 caffe.cpp:312] Batch 578, loss = 1.7
I0630 19:09:08.624528 17469 caffe.cpp:312] Batch 579, accuracy/top1 = 0.54
I0630 19:09:08.624552 17469 caffe.cpp:312] Batch 579, accuracy/top5 = 0.78
I0630 19:09:08.624554 17469 caffe.cpp:312] Batch 579, loss = 1.56
I0630 19:09:08.672621 17469 caffe.cpp:312] Batch 580, accuracy/top1 = 0.64
I0630 19:09:08.672642 17469 caffe.cpp:312] Batch 580, accuracy/top5 = 0.8
I0630 19:09:08.672646 17469 caffe.cpp:312] Batch 580, loss = 1.54
I0630 19:09:08.721014 17469 caffe.cpp:312] Batch 581, accuracy/top1 = 0.52
I0630 19:09:08.721037 17469 caffe.cpp:312] Batch 581, accuracy/top5 = 0.82
I0630 19:09:08.721040 17469 caffe.cpp:312] Batch 581, loss = 1.44
I0630 19:09:08.768672 17469 caffe.cpp:312] Batch 582, accuracy/top1 = 0.56
I0630 19:09:08.768695 17469 caffe.cpp:312] Batch 582, accuracy/top5 = 0.76
I0630 19:09:08.768698 17469 caffe.cpp:312] Batch 582, loss = 1.54
I0630 19:09:08.816601 17469 caffe.cpp:312] Batch 583, accuracy/top1 = 0.52
I0630 19:09:08.816624 17469 caffe.cpp:312] Batch 583, accuracy/top5 = 0.8
I0630 19:09:08.816627 17469 caffe.cpp:312] Batch 583, loss = 1.82
I0630 19:09:08.864636 17469 caffe.cpp:312] Batch 584, accuracy/top1 = 0.68
I0630 19:09:08.864657 17469 caffe.cpp:312] Batch 584, accuracy/top5 = 0.9
I0630 19:09:08.864660 17469 caffe.cpp:312] Batch 584, loss = 0.96
I0630 19:09:08.913461 17469 caffe.cpp:312] Batch 585, accuracy/top1 = 0.74
I0630 19:09:08.913483 17469 caffe.cpp:312] Batch 585, accuracy/top5 = 0.9
I0630 19:09:08.913486 17469 caffe.cpp:312] Batch 585, loss = 1.14
I0630 19:09:08.960558 17469 caffe.cpp:312] Batch 586, accuracy/top1 = 0.58
I0630 19:09:08.960580 17469 caffe.cpp:312] Batch 586, accuracy/top5 = 0.78
I0630 19:09:08.960583 17469 caffe.cpp:312] Batch 586, loss = 1.62
I0630 19:09:09.007072 17469 caffe.cpp:312] Batch 587, accuracy/top1 = 0.56
I0630 19:09:09.007097 17469 caffe.cpp:312] Batch 587, accuracy/top5 = 0.84
I0630 19:09:09.007100 17469 caffe.cpp:312] Batch 587, loss = 1.38
I0630 19:09:09.054313 17469 caffe.cpp:312] Batch 588, accuracy/top1 = 0.68
I0630 19:09:09.054409 17469 caffe.cpp:312] Batch 588, accuracy/top5 = 0.88
I0630 19:09:09.054414 17469 caffe.cpp:312] Batch 588, loss = 1.08
I0630 19:09:09.102355 17469 caffe.cpp:312] Batch 589, accuracy/top1 = 0.58
I0630 19:09:09.102378 17469 caffe.cpp:312] Batch 589, accuracy/top5 = 0.84
I0630 19:09:09.102381 17469 caffe.cpp:312] Batch 589, loss = 1.5
I0630 19:09:09.150555 17469 caffe.cpp:312] Batch 590, accuracy/top1 = 0.56
I0630 19:09:09.150578 17469 caffe.cpp:312] Batch 590, accuracy/top5 = 0.76
I0630 19:09:09.150581 17469 caffe.cpp:312] Batch 590, loss = 1.8
I0630 19:09:09.198395 17469 caffe.cpp:312] Batch 591, accuracy/top1 = 0.48
I0630 19:09:09.198417 17469 caffe.cpp:312] Batch 591, accuracy/top5 = 0.74
I0630 19:09:09.198421 17469 caffe.cpp:312] Batch 591, loss = 1.8
I0630 19:09:09.246876 17469 caffe.cpp:312] Batch 592, accuracy/top1 = 0.54
I0630 19:09:09.246898 17469 caffe.cpp:312] Batch 592, accuracy/top5 = 0.84
I0630 19:09:09.246901 17469 caffe.cpp:312] Batch 592, loss = 1.36
I0630 19:09:09.295269 17469 caffe.cpp:312] Batch 593, accuracy/top1 = 0.54
I0630 19:09:09.295290 17469 caffe.cpp:312] Batch 593, accuracy/top5 = 0.68
I0630 19:09:09.295295 17469 caffe.cpp:312] Batch 593, loss = 1.8
I0630 19:09:09.342696 17469 caffe.cpp:312] Batch 594, accuracy/top1 = 0.6
I0630 19:09:09.342720 17469 caffe.cpp:312] Batch 594, accuracy/top5 = 0.86
I0630 19:09:09.342723 17469 caffe.cpp:312] Batch 594, loss = 1.42
I0630 19:09:09.390696 17469 caffe.cpp:312] Batch 595, accuracy/top1 = 0.62
I0630 19:09:09.390717 17469 caffe.cpp:312] Batch 595, accuracy/top5 = 0.88
I0630 19:09:09.390722 17469 caffe.cpp:312] Batch 595, loss = 1.14
I0630 19:09:09.438664 17469 caffe.cpp:312] Batch 596, accuracy/top1 = 0.44
I0630 19:09:09.438688 17469 caffe.cpp:312] Batch 596, accuracy/top5 = 0.66
I0630 19:09:09.438690 17469 caffe.cpp:312] Batch 596, loss = 2.36
I0630 19:09:09.486284 17469 caffe.cpp:312] Batch 597, accuracy/top1 = 0.68
I0630 19:09:09.486307 17469 caffe.cpp:312] Batch 597, accuracy/top5 = 0.84
I0630 19:09:09.486310 17469 caffe.cpp:312] Batch 597, loss = 1.04
I0630 19:09:09.534452 17469 caffe.cpp:312] Batch 598, accuracy/top1 = 0.44
I0630 19:09:09.534474 17469 caffe.cpp:312] Batch 598, accuracy/top5 = 0.76
I0630 19:09:09.534477 17469 caffe.cpp:312] Batch 598, loss = 1.72
I0630 19:09:09.582682 17469 caffe.cpp:312] Batch 599, accuracy/top1 = 0.52
I0630 19:09:09.582705 17469 caffe.cpp:312] Batch 599, accuracy/top5 = 0.82
I0630 19:09:09.582708 17469 caffe.cpp:312] Batch 599, loss = 1.54
I0630 19:09:09.629786 17469 caffe.cpp:312] Batch 600, accuracy/top1 = 0.54
I0630 19:09:09.629807 17469 caffe.cpp:312] Batch 600, accuracy/top5 = 0.74
I0630 19:09:09.629811 17469 caffe.cpp:312] Batch 600, loss = 1.78
I0630 19:09:09.677803 17469 caffe.cpp:312] Batch 601, accuracy/top1 = 0.5
I0630 19:09:09.677824 17469 caffe.cpp:312] Batch 601, accuracy/top5 = 0.9
I0630 19:09:09.677827 17469 caffe.cpp:312] Batch 601, loss = 1.04
I0630 19:09:09.726444 17469 caffe.cpp:312] Batch 602, accuracy/top1 = 0.5
I0630 19:09:09.726467 17469 caffe.cpp:312] Batch 602, accuracy/top5 = 0.76
I0630 19:09:09.726470 17469 caffe.cpp:312] Batch 602, loss = 1.8
I0630 19:09:09.774827 17469 caffe.cpp:312] Batch 603, accuracy/top1 = 0.64
I0630 19:09:09.774850 17469 caffe.cpp:312] Batch 603, accuracy/top5 = 0.86
I0630 19:09:09.774853 17469 caffe.cpp:312] Batch 603, loss = 0.88
I0630 19:09:09.821990 17469 caffe.cpp:312] Batch 604, accuracy/top1 = 0.82
I0630 19:09:09.822011 17469 caffe.cpp:312] Batch 604, accuracy/top5 = 0.96
I0630 19:09:09.822015 17469 caffe.cpp:312] Batch 604, loss = 0.54
I0630 19:09:09.869542 17469 caffe.cpp:312] Batch 605, accuracy/top1 = 0.5
I0630 19:09:09.869562 17469 caffe.cpp:312] Batch 605, accuracy/top5 = 0.72
I0630 19:09:09.869566 17469 caffe.cpp:312] Batch 605, loss = 1.94
I0630 19:09:09.918332 17469 caffe.cpp:312] Batch 606, accuracy/top1 = 0.56
I0630 19:09:09.918354 17469 caffe.cpp:312] Batch 606, accuracy/top5 = 0.78
I0630 19:09:09.918357 17469 caffe.cpp:312] Batch 606, loss = 1.8
I0630 19:09:09.966434 17469 caffe.cpp:312] Batch 607, accuracy/top1 = 0.52
I0630 19:09:09.966457 17469 caffe.cpp:312] Batch 607, accuracy/top5 = 0.78
I0630 19:09:09.966475 17469 caffe.cpp:312] Batch 607, loss = 1.7
I0630 19:09:10.015401 17469 caffe.cpp:312] Batch 608, accuracy/top1 = 0.62
I0630 19:09:10.015422 17469 caffe.cpp:312] Batch 608, accuracy/top5 = 0.84
I0630 19:09:10.015425 17469 caffe.cpp:312] Batch 608, loss = 1.34
I0630 19:09:10.064365 17469 caffe.cpp:312] Batch 609, accuracy/top1 = 0.54
I0630 19:09:10.064388 17469 caffe.cpp:312] Batch 609, accuracy/top5 = 0.76
I0630 19:09:10.064391 17469 caffe.cpp:312] Batch 609, loss = 1.78
I0630 19:09:10.112521 17469 caffe.cpp:312] Batch 610, accuracy/top1 = 0.52
I0630 19:09:10.112543 17469 caffe.cpp:312] Batch 610, accuracy/top5 = 0.76
I0630 19:09:10.112546 17469 caffe.cpp:312] Batch 610, loss = 1.74
I0630 19:09:10.160611 17469 caffe.cpp:312] Batch 611, accuracy/top1 = 0.52
I0630 19:09:10.160634 17469 caffe.cpp:312] Batch 611, accuracy/top5 = 0.74
I0630 19:09:10.160639 17469 caffe.cpp:312] Batch 611, loss = 1.72
I0630 19:09:10.208631 17469 caffe.cpp:312] Batch 612, accuracy/top1 = 0.6
I0630 19:09:10.208650 17469 caffe.cpp:312] Batch 612, accuracy/top5 = 0.86
I0630 19:09:10.208653 17469 caffe.cpp:312] Batch 612, loss = 1.26
I0630 19:09:10.255504 17469 caffe.cpp:312] Batch 613, accuracy/top1 = 0.54
I0630 19:09:10.255527 17469 caffe.cpp:312] Batch 613, accuracy/top5 = 0.74
I0630 19:09:10.255530 17469 caffe.cpp:312] Batch 613, loss = 1.82
I0630 19:09:10.304632 17469 caffe.cpp:312] Batch 614, accuracy/top1 = 0.44
I0630 19:09:10.304654 17469 caffe.cpp:312] Batch 614, accuracy/top5 = 0.64
I0630 19:09:10.304657 17469 caffe.cpp:312] Batch 614, loss = 2.04
I0630 19:09:10.353353 17469 caffe.cpp:312] Batch 615, accuracy/top1 = 0.46
I0630 19:09:10.353374 17469 caffe.cpp:312] Batch 615, accuracy/top5 = 0.76
I0630 19:09:10.353379 17469 caffe.cpp:312] Batch 615, loss = 1.72
I0630 19:09:10.402406 17469 caffe.cpp:312] Batch 616, accuracy/top1 = 0.56
I0630 19:09:10.402428 17469 caffe.cpp:312] Batch 616, accuracy/top5 = 0.74
I0630 19:09:10.402431 17469 caffe.cpp:312] Batch 616, loss = 1.66
I0630 19:09:10.451020 17469 caffe.cpp:312] Batch 617, accuracy/top1 = 0.54
I0630 19:09:10.451041 17469 caffe.cpp:312] Batch 617, accuracy/top5 = 0.78
I0630 19:09:10.451045 17469 caffe.cpp:312] Batch 617, loss = 1.66
I0630 19:09:10.498950 17469 caffe.cpp:312] Batch 618, accuracy/top1 = 0.5
I0630 19:09:10.498972 17469 caffe.cpp:312] Batch 618, accuracy/top5 = 0.82
I0630 19:09:10.498975 17469 caffe.cpp:312] Batch 618, loss = 1.68
I0630 19:09:10.547412 17469 caffe.cpp:312] Batch 619, accuracy/top1 = 0.62
I0630 19:09:10.547435 17469 caffe.cpp:312] Batch 619, accuracy/top5 = 0.84
I0630 19:09:10.547438 17469 caffe.cpp:312] Batch 619, loss = 1.22
I0630 19:09:10.596184 17469 caffe.cpp:312] Batch 620, accuracy/top1 = 0.48
I0630 19:09:10.596205 17469 caffe.cpp:312] Batch 620, accuracy/top5 = 0.8
I0630 19:09:10.596210 17469 caffe.cpp:312] Batch 620, loss = 1.4
I0630 19:09:10.644482 17469 caffe.cpp:312] Batch 621, accuracy/top1 = 0.52
I0630 19:09:10.644505 17469 caffe.cpp:312] Batch 621, accuracy/top5 = 0.74
I0630 19:09:10.644510 17469 caffe.cpp:312] Batch 621, loss = 1.96
I0630 19:09:10.692255 17469 caffe.cpp:312] Batch 622, accuracy/top1 = 0.64
I0630 19:09:10.692276 17469 caffe.cpp:312] Batch 622, accuracy/top5 = 0.8
I0630 19:09:10.692279 17469 caffe.cpp:312] Batch 622, loss = 1.6
I0630 19:09:10.739899 17469 caffe.cpp:312] Batch 623, accuracy/top1 = 0.48
I0630 19:09:10.739923 17469 caffe.cpp:312] Batch 623, accuracy/top5 = 0.8
I0630 19:09:10.739926 17469 caffe.cpp:312] Batch 623, loss = 1.96
I0630 19:09:10.788137 17469 caffe.cpp:312] Batch 624, accuracy/top1 = 0.52
I0630 19:09:10.788158 17469 caffe.cpp:312] Batch 624, accuracy/top5 = 0.88
I0630 19:09:10.788162 17469 caffe.cpp:312] Batch 624, loss = 1.34
I0630 19:09:10.837005 17469 caffe.cpp:312] Batch 625, accuracy/top1 = 0.58
I0630 19:09:10.837030 17469 caffe.cpp:312] Batch 625, accuracy/top5 = 0.78
I0630 19:09:10.837033 17469 caffe.cpp:312] Batch 625, loss = 1.76
I0630 19:09:10.887490 17469 caffe.cpp:312] Batch 626, accuracy/top1 = 0.44
I0630 19:09:10.887512 17469 caffe.cpp:312] Batch 626, accuracy/top5 = 0.66
I0630 19:09:10.887527 17469 caffe.cpp:312] Batch 626, loss = 2.02
I0630 19:09:10.935940 17469 caffe.cpp:312] Batch 627, accuracy/top1 = 0.46
I0630 19:09:10.935961 17469 caffe.cpp:312] Batch 627, accuracy/top5 = 0.74
I0630 19:09:10.935963 17469 caffe.cpp:312] Batch 627, loss = 1.9
I0630 19:09:10.984251 17469 caffe.cpp:312] Batch 628, accuracy/top1 = 0.54
I0630 19:09:10.984273 17469 caffe.cpp:312] Batch 628, accuracy/top5 = 0.78
I0630 19:09:10.984277 17469 caffe.cpp:312] Batch 628, loss = 1.8
I0630 19:09:11.033262 17469 caffe.cpp:312] Batch 629, accuracy/top1 = 0.54
I0630 19:09:11.033284 17469 caffe.cpp:312] Batch 629, accuracy/top5 = 0.74
I0630 19:09:11.033288 17469 caffe.cpp:312] Batch 629, loss = 1.84
I0630 19:09:11.082195 17469 caffe.cpp:312] Batch 630, accuracy/top1 = 0.48
I0630 19:09:11.082216 17469 caffe.cpp:312] Batch 630, accuracy/top5 = 0.66
I0630 19:09:11.082218 17469 caffe.cpp:312] Batch 630, loss = 2.06
I0630 19:09:11.130064 17469 caffe.cpp:312] Batch 631, accuracy/top1 = 0.58
I0630 19:09:11.130087 17469 caffe.cpp:312] Batch 631, accuracy/top5 = 0.84
I0630 19:09:11.130091 17469 caffe.cpp:312] Batch 631, loss = 1.26
I0630 19:09:11.179277 17469 caffe.cpp:312] Batch 632, accuracy/top1 = 0.5
I0630 19:09:11.179299 17469 caffe.cpp:312] Batch 632, accuracy/top5 = 0.8
I0630 19:09:11.179303 17469 caffe.cpp:312] Batch 632, loss = 1.6
I0630 19:09:11.227861 17469 caffe.cpp:312] Batch 633, accuracy/top1 = 0.48
I0630 19:09:11.227885 17469 caffe.cpp:312] Batch 633, accuracy/top5 = 0.78
I0630 19:09:11.227887 17469 caffe.cpp:312] Batch 633, loss = 1.78
I0630 19:09:11.275653 17469 caffe.cpp:312] Batch 634, accuracy/top1 = 0.58
I0630 19:09:11.275676 17469 caffe.cpp:312] Batch 634, accuracy/top5 = 0.72
I0630 19:09:11.275678 17469 caffe.cpp:312] Batch 634, loss = 2.22
I0630 19:09:11.324825 17469 caffe.cpp:312] Batch 635, accuracy/top1 = 0.6
I0630 19:09:11.324847 17469 caffe.cpp:312] Batch 635, accuracy/top5 = 0.82
I0630 19:09:11.324851 17469 caffe.cpp:312] Batch 635, loss = 1.36
I0630 19:09:11.373020 17469 caffe.cpp:312] Batch 636, accuracy/top1 = 0.62
I0630 19:09:11.373042 17469 caffe.cpp:312] Batch 636, accuracy/top5 = 0.76
I0630 19:09:11.373045 17469 caffe.cpp:312] Batch 636, loss = 1.72
I0630 19:09:11.421118 17469 caffe.cpp:312] Batch 637, accuracy/top1 = 0.56
I0630 19:09:11.421140 17469 caffe.cpp:312] Batch 637, accuracy/top5 = 0.82
I0630 19:09:11.421144 17469 caffe.cpp:312] Batch 637, loss = 1.42
I0630 19:09:11.469730 17469 caffe.cpp:312] Batch 638, accuracy/top1 = 0.5
I0630 19:09:11.469756 17469 caffe.cpp:312] Batch 638, accuracy/top5 = 0.78
I0630 19:09:11.469759 17469 caffe.cpp:312] Batch 638, loss = 1.4
I0630 19:09:11.517839 17469 caffe.cpp:312] Batch 639, accuracy/top1 = 0.62
I0630 19:09:11.517863 17469 caffe.cpp:312] Batch 639, accuracy/top5 = 0.8
I0630 19:09:11.517865 17469 caffe.cpp:312] Batch 639, loss = 1.42
I0630 19:09:11.566509 17469 caffe.cpp:312] Batch 640, accuracy/top1 = 0.46
I0630 19:09:11.566532 17469 caffe.cpp:312] Batch 640, accuracy/top5 = 0.68
I0630 19:09:11.566535 17469 caffe.cpp:312] Batch 640, loss = 2.24
I0630 19:09:11.615896 17469 caffe.cpp:312] Batch 641, accuracy/top1 = 0.54
I0630 19:09:11.615918 17469 caffe.cpp:312] Batch 641, accuracy/top5 = 0.8
I0630 19:09:11.615921 17469 caffe.cpp:312] Batch 641, loss = 1.54
I0630 19:09:11.664350 17469 caffe.cpp:312] Batch 642, accuracy/top1 = 0.56
I0630 19:09:11.664371 17469 caffe.cpp:312] Batch 642, accuracy/top5 = 0.76
I0630 19:09:11.664374 17469 caffe.cpp:312] Batch 642, loss = 1.78
I0630 19:09:11.713271 17469 caffe.cpp:312] Batch 643, accuracy/top1 = 0.44
I0630 19:09:11.713291 17469 caffe.cpp:312] Batch 643, accuracy/top5 = 0.84
I0630 19:09:11.713294 17469 caffe.cpp:312] Batch 643, loss = 1.56
I0630 19:09:11.761068 17469 caffe.cpp:312] Batch 644, accuracy/top1 = 0.56
I0630 19:09:11.761086 17469 caffe.cpp:312] Batch 644, accuracy/top5 = 0.7
I0630 19:09:11.761090 17469 caffe.cpp:312] Batch 644, loss = 1.6
I0630 19:09:11.809494 17469 caffe.cpp:312] Batch 645, accuracy/top1 = 0.54
I0630 19:09:11.809517 17469 caffe.cpp:312] Batch 645, accuracy/top5 = 0.82
I0630 19:09:11.809556 17469 caffe.cpp:312] Batch 645, loss = 1.72
I0630 19:09:11.858461 17469 caffe.cpp:312] Batch 646, accuracy/top1 = 0.56
I0630 19:09:11.858486 17469 caffe.cpp:312] Batch 646, accuracy/top5 = 0.9
I0630 19:09:11.858489 17469 caffe.cpp:312] Batch 646, loss = 1.16
I0630 19:09:11.906955 17469 caffe.cpp:312] Batch 647, accuracy/top1 = 0.54
I0630 19:09:11.906975 17469 caffe.cpp:312] Batch 647, accuracy/top5 = 0.78
I0630 19:09:11.906980 17469 caffe.cpp:312] Batch 647, loss = 1.92
I0630 19:09:11.955166 17469 caffe.cpp:312] Batch 648, accuracy/top1 = 0.58
I0630 19:09:11.955188 17469 caffe.cpp:312] Batch 648, accuracy/top5 = 0.84
I0630 19:09:11.955191 17469 caffe.cpp:312] Batch 648, loss = 1.26
I0630 19:09:12.003504 17469 caffe.cpp:312] Batch 649, accuracy/top1 = 0.52
I0630 19:09:12.003526 17469 caffe.cpp:312] Batch 649, accuracy/top5 = 0.8
I0630 19:09:12.003530 17469 caffe.cpp:312] Batch 649, loss = 1.3
I0630 19:09:12.051878 17469 caffe.cpp:312] Batch 650, accuracy/top1 = 0.56
I0630 19:09:12.051900 17469 caffe.cpp:312] Batch 650, accuracy/top5 = 0.74
I0630 19:09:12.051903 17469 caffe.cpp:312] Batch 650, loss = 1.54
I0630 19:09:12.101275 17469 caffe.cpp:312] Batch 651, accuracy/top1 = 0.5
I0630 19:09:12.101297 17469 caffe.cpp:312] Batch 651, accuracy/top5 = 0.74
I0630 19:09:12.101300 17469 caffe.cpp:312] Batch 651, loss = 2
I0630 19:09:12.149313 17469 caffe.cpp:312] Batch 652, accuracy/top1 = 0.48
I0630 19:09:12.149334 17469 caffe.cpp:312] Batch 652, accuracy/top5 = 0.68
I0630 19:09:12.149338 17469 caffe.cpp:312] Batch 652, loss = 2.02
I0630 19:09:12.198568 17469 caffe.cpp:312] Batch 653, accuracy/top1 = 0.46
I0630 19:09:12.198591 17469 caffe.cpp:312] Batch 653, accuracy/top5 = 0.7
I0630 19:09:12.198595 17469 caffe.cpp:312] Batch 653, loss = 1.88
I0630 19:09:12.246115 17469 caffe.cpp:312] Batch 654, accuracy/top1 = 0.52
I0630 19:09:12.246136 17469 caffe.cpp:312] Batch 654, accuracy/top5 = 0.84
I0630 19:09:12.246140 17469 caffe.cpp:312] Batch 654, loss = 1.74
I0630 19:09:12.294607 17469 caffe.cpp:312] Batch 655, accuracy/top1 = 0.66
I0630 19:09:12.294630 17469 caffe.cpp:312] Batch 655, accuracy/top5 = 0.94
I0630 19:09:12.294632 17469 caffe.cpp:312] Batch 655, loss = 0.9
I0630 19:09:12.342638 17469 caffe.cpp:312] Batch 656, accuracy/top1 = 0.54
I0630 19:09:12.342660 17469 caffe.cpp:312] Batch 656, accuracy/top5 = 0.88
I0630 19:09:12.342664 17469 caffe.cpp:312] Batch 656, loss = 1.18
I0630 19:09:12.391288 17469 caffe.cpp:312] Batch 657, accuracy/top1 = 0.6
I0630 19:09:12.391309 17469 caffe.cpp:312] Batch 657, accuracy/top5 = 0.82
I0630 19:09:12.391314 17469 caffe.cpp:312] Batch 657, loss = 1.56
I0630 19:09:12.438904 17469 caffe.cpp:312] Batch 658, accuracy/top1 = 0.64
I0630 19:09:12.438925 17469 caffe.cpp:312] Batch 658, accuracy/top5 = 0.84
I0630 19:09:12.438927 17469 caffe.cpp:312] Batch 658, loss = 1.44
I0630 19:09:12.487648 17469 caffe.cpp:312] Batch 659, accuracy/top1 = 0.46
I0630 19:09:12.487671 17469 caffe.cpp:312] Batch 659, accuracy/top5 = 0.74
I0630 19:09:12.487674 17469 caffe.cpp:312] Batch 659, loss = 1.88
I0630 19:09:12.534874 17469 caffe.cpp:312] Batch 660, accuracy/top1 = 0.64
I0630 19:09:12.534894 17469 caffe.cpp:312] Batch 660, accuracy/top5 = 0.78
I0630 19:09:12.534898 17469 caffe.cpp:312] Batch 660, loss = 1.26
I0630 19:09:12.583024 17469 caffe.cpp:312] Batch 661, accuracy/top1 = 0.48
I0630 19:09:12.583046 17469 caffe.cpp:312] Batch 661, accuracy/top5 = 0.8
I0630 19:09:12.583050 17469 caffe.cpp:312] Batch 661, loss = 1.6
I0630 19:09:12.631021 17469 caffe.cpp:312] Batch 662, accuracy/top1 = 0.52
I0630 19:09:12.631042 17469 caffe.cpp:312] Batch 662, accuracy/top5 = 0.7
I0630 19:09:12.631044 17469 caffe.cpp:312] Batch 662, loss = 2.02
I0630 19:09:12.679021 17469 caffe.cpp:312] Batch 663, accuracy/top1 = 0.6
I0630 19:09:12.679044 17469 caffe.cpp:312] Batch 663, accuracy/top5 = 0.76
I0630 19:09:12.679047 17469 caffe.cpp:312] Batch 663, loss = 1.94
I0630 19:09:12.726797 17469 caffe.cpp:312] Batch 664, accuracy/top1 = 0.5
I0630 19:09:12.726819 17469 caffe.cpp:312] Batch 664, accuracy/top5 = 0.8
I0630 19:09:12.726840 17469 caffe.cpp:312] Batch 664, loss = 1.66
I0630 19:09:12.774888 17469 caffe.cpp:312] Batch 665, accuracy/top1 = 0.46
I0630 19:09:12.774910 17469 caffe.cpp:312] Batch 665, accuracy/top5 = 0.76
I0630 19:09:12.774914 17469 caffe.cpp:312] Batch 665, loss = 1.76
I0630 19:09:12.823324 17469 caffe.cpp:312] Batch 666, accuracy/top1 = 0.58
I0630 19:09:12.823346 17469 caffe.cpp:312] Batch 666, accuracy/top5 = 0.76
I0630 19:09:12.823349 17469 caffe.cpp:312] Batch 666, loss = 1.6
I0630 19:09:12.872802 17469 caffe.cpp:312] Batch 667, accuracy/top1 = 0.46
I0630 19:09:12.872822 17469 caffe.cpp:312] Batch 667, accuracy/top5 = 0.78
I0630 19:09:12.872825 17469 caffe.cpp:312] Batch 667, loss = 1.76
I0630 19:09:12.922693 17469 caffe.cpp:312] Batch 668, accuracy/top1 = 0.58
I0630 19:09:12.922714 17469 caffe.cpp:312] Batch 668, accuracy/top5 = 0.78
I0630 19:09:12.922719 17469 caffe.cpp:312] Batch 668, loss = 1.64
I0630 19:09:12.970762 17469 caffe.cpp:312] Batch 669, accuracy/top1 = 0.64
I0630 19:09:12.970785 17469 caffe.cpp:312] Batch 669, accuracy/top5 = 0.84
I0630 19:09:12.970789 17469 caffe.cpp:312] Batch 669, loss = 1.16
I0630 19:09:13.018594 17469 caffe.cpp:312] Batch 670, accuracy/top1 = 0.58
I0630 19:09:13.018616 17469 caffe.cpp:312] Batch 670, accuracy/top5 = 0.78
I0630 19:09:13.018620 17469 caffe.cpp:312] Batch 670, loss = 1.32
I0630 19:09:13.066352 17469 caffe.cpp:312] Batch 671, accuracy/top1 = 0.54
I0630 19:09:13.066375 17469 caffe.cpp:312] Batch 671, accuracy/top5 = 0.88
I0630 19:09:13.066378 17469 caffe.cpp:312] Batch 671, loss = 1.38
I0630 19:09:13.115595 17469 caffe.cpp:312] Batch 672, accuracy/top1 = 0.58
I0630 19:09:13.115617 17469 caffe.cpp:312] Batch 672, accuracy/top5 = 0.82
I0630 19:09:13.115619 17469 caffe.cpp:312] Batch 672, loss = 1.68
I0630 19:09:13.164381 17469 caffe.cpp:312] Batch 673, accuracy/top1 = 0.68
I0630 19:09:13.164404 17469 caffe.cpp:312] Batch 673, accuracy/top5 = 0.9
I0630 19:09:13.164407 17469 caffe.cpp:312] Batch 673, loss = 1.02
I0630 19:09:13.212694 17469 caffe.cpp:312] Batch 674, accuracy/top1 = 0.72
I0630 19:09:13.212716 17469 caffe.cpp:312] Batch 674, accuracy/top5 = 0.88
I0630 19:09:13.212719 17469 caffe.cpp:312] Batch 674, loss = 1.08
I0630 19:09:13.261581 17469 caffe.cpp:312] Batch 675, accuracy/top1 = 0.54
I0630 19:09:13.261602 17469 caffe.cpp:312] Batch 675, accuracy/top5 = 0.82
I0630 19:09:13.261605 17469 caffe.cpp:312] Batch 675, loss = 1.72
I0630 19:09:13.308995 17469 caffe.cpp:312] Batch 676, accuracy/top1 = 0.5
I0630 19:09:13.309017 17469 caffe.cpp:312] Batch 676, accuracy/top5 = 0.78
I0630 19:09:13.309020 17469 caffe.cpp:312] Batch 676, loss = 1.82
I0630 19:09:13.357288 17469 caffe.cpp:312] Batch 677, accuracy/top1 = 0.6
I0630 19:09:13.357311 17469 caffe.cpp:312] Batch 677, accuracy/top5 = 0.82
I0630 19:09:13.357314 17469 caffe.cpp:312] Batch 677, loss = 1.24
I0630 19:09:13.406010 17469 caffe.cpp:312] Batch 678, accuracy/top1 = 0.5
I0630 19:09:13.406031 17469 caffe.cpp:312] Batch 678, accuracy/top5 = 0.82
I0630 19:09:13.406035 17469 caffe.cpp:312] Batch 678, loss = 1.64
I0630 19:09:13.453357 17469 caffe.cpp:312] Batch 679, accuracy/top1 = 0.66
I0630 19:09:13.453380 17469 caffe.cpp:312] Batch 679, accuracy/top5 = 0.84
I0630 19:09:13.453383 17469 caffe.cpp:312] Batch 679, loss = 1.12
I0630 19:09:13.501917 17469 caffe.cpp:312] Batch 680, accuracy/top1 = 0.62
I0630 19:09:13.501940 17469 caffe.cpp:312] Batch 680, accuracy/top5 = 0.82
I0630 19:09:13.501942 17469 caffe.cpp:312] Batch 680, loss = 1.32
I0630 19:09:13.550415 17469 caffe.cpp:312] Batch 681, accuracy/top1 = 0.64
I0630 19:09:13.550436 17469 caffe.cpp:312] Batch 681, accuracy/top5 = 0.86
I0630 19:09:13.550441 17469 caffe.cpp:312] Batch 681, loss = 1.32
I0630 19:09:13.598536 17469 caffe.cpp:312] Batch 682, accuracy/top1 = 0.5
I0630 19:09:13.598557 17469 caffe.cpp:312] Batch 682, accuracy/top5 = 0.8
I0630 19:09:13.598561 17469 caffe.cpp:312] Batch 682, loss = 1.56
I0630 19:09:13.647073 17469 caffe.cpp:312] Batch 683, accuracy/top1 = 0.42
I0630 19:09:13.647100 17469 caffe.cpp:312] Batch 683, accuracy/top5 = 0.72
I0630 19:09:13.647123 17469 caffe.cpp:312] Batch 683, loss = 2.18
I0630 19:09:13.699818 17469 caffe.cpp:312] Batch 684, accuracy/top1 = 0.5
I0630 19:09:13.699842 17469 caffe.cpp:312] Batch 684, accuracy/top5 = 0.82
I0630 19:09:13.699846 17469 caffe.cpp:312] Batch 684, loss = 1.76
I0630 19:09:13.748487 17469 caffe.cpp:312] Batch 685, accuracy/top1 = 0.56
I0630 19:09:13.748510 17469 caffe.cpp:312] Batch 685, accuracy/top5 = 0.72
I0630 19:09:13.748513 17469 caffe.cpp:312] Batch 685, loss = 1.8
I0630 19:09:13.801090 17469 caffe.cpp:312] Batch 686, accuracy/top1 = 0.46
I0630 19:09:13.801112 17469 caffe.cpp:312] Batch 686, accuracy/top5 = 0.7
I0630 19:09:13.801115 17469 caffe.cpp:312] Batch 686, loss = 1.9
I0630 19:09:13.848970 17469 caffe.cpp:312] Batch 687, accuracy/top1 = 0.54
I0630 19:09:13.848992 17469 caffe.cpp:312] Batch 687, accuracy/top5 = 0.78
I0630 19:09:13.848995 17469 caffe.cpp:312] Batch 687, loss = 1.98
I0630 19:09:13.901870 17469 caffe.cpp:312] Batch 688, accuracy/top1 = 0.5
I0630 19:09:13.901890 17469 caffe.cpp:312] Batch 688, accuracy/top5 = 0.72
I0630 19:09:13.901893 17469 caffe.cpp:312] Batch 688, loss = 2.08
I0630 19:09:13.949591 17469 caffe.cpp:312] Batch 689, accuracy/top1 = 0.68
I0630 19:09:13.949615 17469 caffe.cpp:312] Batch 689, accuracy/top5 = 0.76
I0630 19:09:13.949617 17469 caffe.cpp:312] Batch 689, loss = 1.32
I0630 19:09:14.001869 17469 caffe.cpp:312] Batch 690, accuracy/top1 = 0.48
I0630 19:09:14.001890 17469 caffe.cpp:312] Batch 690, accuracy/top5 = 0.8
I0630 19:09:14.001893 17469 caffe.cpp:312] Batch 690, loss = 1.46
I0630 19:09:14.050424 17469 caffe.cpp:312] Batch 691, accuracy/top1 = 0.52
I0630 19:09:14.050447 17469 caffe.cpp:312] Batch 691, accuracy/top5 = 0.78
I0630 19:09:14.050451 17469 caffe.cpp:312] Batch 691, loss = 1.8
I0630 19:09:14.103101 17469 caffe.cpp:312] Batch 692, accuracy/top1 = 0.52
I0630 19:09:14.103122 17469 caffe.cpp:312] Batch 692, accuracy/top5 = 0.74
I0630 19:09:14.103127 17469 caffe.cpp:312] Batch 692, loss = 1.64
I0630 19:09:14.151947 17469 caffe.cpp:312] Batch 693, accuracy/top1 = 0.66
I0630 19:09:14.151968 17469 caffe.cpp:312] Batch 693, accuracy/top5 = 0.8
I0630 19:09:14.151971 17469 caffe.cpp:312] Batch 693, loss = 1.64
I0630 19:09:14.204784 17469 caffe.cpp:312] Batch 694, accuracy/top1 = 0.58
I0630 19:09:14.204807 17469 caffe.cpp:312] Batch 694, accuracy/top5 = 0.7
I0630 19:09:14.204809 17469 caffe.cpp:312] Batch 694, loss = 1.6
I0630 19:09:14.253494 17469 caffe.cpp:312] Batch 695, accuracy/top1 = 0.58
I0630 19:09:14.253517 17469 caffe.cpp:312] Batch 695, accuracy/top5 = 0.78
I0630 19:09:14.253520 17469 caffe.cpp:312] Batch 695, loss = 1.5
I0630 19:09:14.306049 17469 caffe.cpp:312] Batch 696, accuracy/top1 = 0.4
I0630 19:09:14.306073 17469 caffe.cpp:312] Batch 696, accuracy/top5 = 0.74
I0630 19:09:14.306077 17469 caffe.cpp:312] Batch 696, loss = 1.6
I0630 19:09:14.355002 17469 caffe.cpp:312] Batch 697, accuracy/top1 = 0.62
I0630 19:09:14.355024 17469 caffe.cpp:312] Batch 697, accuracy/top5 = 0.9
I0630 19:09:14.355027 17469 caffe.cpp:312] Batch 697, loss = 1.28
I0630 19:09:14.403904 17469 caffe.cpp:312] Batch 698, accuracy/top1 = 0.54
I0630 19:09:14.403924 17469 caffe.cpp:312] Batch 698, accuracy/top5 = 0.76
I0630 19:09:14.403928 17469 caffe.cpp:312] Batch 698, loss = 1.82
I0630 19:09:14.451375 17469 caffe.cpp:312] Batch 699, accuracy/top1 = 0.7
I0630 19:09:14.451396 17469 caffe.cpp:312] Batch 699, accuracy/top5 = 0.92
I0630 19:09:14.451400 17469 caffe.cpp:312] Batch 699, loss = 0.98
I0630 19:09:14.499742 17469 caffe.cpp:312] Batch 700, accuracy/top1 = 0.54
I0630 19:09:14.499763 17469 caffe.cpp:312] Batch 700, accuracy/top5 = 0.68
I0630 19:09:14.499768 17469 caffe.cpp:312] Batch 700, loss = 2.14
I0630 19:09:14.547822 17469 caffe.cpp:312] Batch 701, accuracy/top1 = 0.5
I0630 19:09:14.547844 17469 caffe.cpp:312] Batch 701, accuracy/top5 = 0.72
I0630 19:09:14.547848 17469 caffe.cpp:312] Batch 701, loss = 1.88
I0630 19:09:14.596422 17469 caffe.cpp:312] Batch 702, accuracy/top1 = 0.48
I0630 19:09:14.596443 17469 caffe.cpp:312] Batch 702, accuracy/top5 = 0.76
I0630 19:09:14.596460 17469 caffe.cpp:312] Batch 702, loss = 1.72
I0630 19:09:14.645237 17469 caffe.cpp:312] Batch 703, accuracy/top1 = 0.68
I0630 19:09:14.645261 17469 caffe.cpp:312] Batch 703, accuracy/top5 = 0.88
I0630 19:09:14.645263 17469 caffe.cpp:312] Batch 703, loss = 0.96
I0630 19:09:14.693795 17469 caffe.cpp:312] Batch 704, accuracy/top1 = 0.54
I0630 19:09:14.693817 17469 caffe.cpp:312] Batch 704, accuracy/top5 = 0.82
I0630 19:09:14.693820 17469 caffe.cpp:312] Batch 704, loss = 1.8
I0630 19:09:14.742746 17469 caffe.cpp:312] Batch 705, accuracy/top1 = 0.64
I0630 19:09:14.742769 17469 caffe.cpp:312] Batch 705, accuracy/top5 = 0.88
I0630 19:09:14.742772 17469 caffe.cpp:312] Batch 705, loss = 1.18
I0630 19:09:14.790868 17469 caffe.cpp:312] Batch 706, accuracy/top1 = 0.54
I0630 19:09:14.790889 17469 caffe.cpp:312] Batch 706, accuracy/top5 = 0.82
I0630 19:09:14.790891 17469 caffe.cpp:312] Batch 706, loss = 1.48
I0630 19:09:14.839823 17469 caffe.cpp:312] Batch 707, accuracy/top1 = 0.6
I0630 19:09:14.839846 17469 caffe.cpp:312] Batch 707, accuracy/top5 = 0.82
I0630 19:09:14.839849 17469 caffe.cpp:312] Batch 707, loss = 1.26
I0630 19:09:14.888296 17469 caffe.cpp:312] Batch 708, accuracy/top1 = 0.58
I0630 19:09:14.888317 17469 caffe.cpp:312] Batch 708, accuracy/top5 = 0.78
I0630 19:09:14.888320 17469 caffe.cpp:312] Batch 708, loss = 1.18
I0630 19:09:14.936611 17469 caffe.cpp:312] Batch 709, accuracy/top1 = 0.54
I0630 19:09:14.936632 17469 caffe.cpp:312] Batch 709, accuracy/top5 = 0.84
I0630 19:09:14.936636 17469 caffe.cpp:312] Batch 709, loss = 1.46
I0630 19:09:14.984714 17469 caffe.cpp:312] Batch 710, accuracy/top1 = 0.58
I0630 19:09:14.984735 17469 caffe.cpp:312] Batch 710, accuracy/top5 = 0.8
I0630 19:09:14.984738 17469 caffe.cpp:312] Batch 710, loss = 1.94
I0630 19:09:15.035806 17469 caffe.cpp:312] Batch 711, accuracy/top1 = 0.54
I0630 19:09:15.035828 17469 caffe.cpp:312] Batch 711, accuracy/top5 = 0.7
I0630 19:09:15.035832 17469 caffe.cpp:312] Batch 711, loss = 1.96
I0630 19:09:15.084439 17469 caffe.cpp:312] Batch 712, accuracy/top1 = 0.48
I0630 19:09:15.084461 17469 caffe.cpp:312] Batch 712, accuracy/top5 = 0.74
I0630 19:09:15.084465 17469 caffe.cpp:312] Batch 712, loss = 1.78
I0630 19:09:15.132622 17469 caffe.cpp:312] Batch 713, accuracy/top1 = 0.52
I0630 19:09:15.132644 17469 caffe.cpp:312] Batch 713, accuracy/top5 = 0.72
I0630 19:09:15.132647 17469 caffe.cpp:312] Batch 713, loss = 2.14
I0630 19:09:15.180420 17469 caffe.cpp:312] Batch 714, accuracy/top1 = 0.6
I0630 19:09:15.180441 17469 caffe.cpp:312] Batch 714, accuracy/top5 = 0.78
I0630 19:09:15.180444 17469 caffe.cpp:312] Batch 714, loss = 1.58
I0630 19:09:15.229164 17469 caffe.cpp:312] Batch 715, accuracy/top1 = 0.52
I0630 19:09:15.229188 17469 caffe.cpp:312] Batch 715, accuracy/top5 = 0.84
I0630 19:09:15.229192 17469 caffe.cpp:312] Batch 715, loss = 1.78
I0630 19:09:15.278348 17469 caffe.cpp:312] Batch 716, accuracy/top1 = 0.6
I0630 19:09:15.278372 17469 caffe.cpp:312] Batch 716, accuracy/top5 = 0.74
I0630 19:09:15.278374 17469 caffe.cpp:312] Batch 716, loss = 2
I0630 19:09:15.327864 17469 caffe.cpp:312] Batch 717, accuracy/top1 = 0.62
I0630 19:09:15.327888 17469 caffe.cpp:312] Batch 717, accuracy/top5 = 0.84
I0630 19:09:15.327893 17469 caffe.cpp:312] Batch 717, loss = 1.46
I0630 19:09:15.375391 17469 caffe.cpp:312] Batch 718, accuracy/top1 = 0.4
I0630 19:09:15.375412 17469 caffe.cpp:312] Batch 718, accuracy/top5 = 0.68
I0630 19:09:15.375416 17469 caffe.cpp:312] Batch 718, loss = 2.6
I0630 19:09:15.423079 17469 caffe.cpp:312] Batch 719, accuracy/top1 = 0.58
I0630 19:09:15.423101 17469 caffe.cpp:312] Batch 719, accuracy/top5 = 0.82
I0630 19:09:15.423105 17469 caffe.cpp:312] Batch 719, loss = 1.48
I0630 19:09:15.471496 17469 caffe.cpp:312] Batch 720, accuracy/top1 = 0.5
I0630 19:09:15.471518 17469 caffe.cpp:312] Batch 720, accuracy/top5 = 0.86
I0630 19:09:15.471521 17469 caffe.cpp:312] Batch 720, loss = 1.28
I0630 19:09:15.520480 17469 caffe.cpp:312] Batch 721, accuracy/top1 = 0.62
I0630 19:09:15.520503 17469 caffe.cpp:312] Batch 721, accuracy/top5 = 0.78
I0630 19:09:15.520521 17469 caffe.cpp:312] Batch 721, loss = 1.7
I0630 19:09:15.569034 17469 caffe.cpp:312] Batch 722, accuracy/top1 = 0.6
I0630 19:09:15.569056 17469 caffe.cpp:312] Batch 722, accuracy/top5 = 0.78
I0630 19:09:15.569059 17469 caffe.cpp:312] Batch 722, loss = 1.66
I0630 19:09:15.617805 17469 caffe.cpp:312] Batch 723, accuracy/top1 = 0.52
I0630 19:09:15.617830 17469 caffe.cpp:312] Batch 723, accuracy/top5 = 0.78
I0630 19:09:15.617832 17469 caffe.cpp:312] Batch 723, loss = 1.66
I0630 19:09:15.665024 17469 caffe.cpp:312] Batch 724, accuracy/top1 = 0.62
I0630 19:09:15.665046 17469 caffe.cpp:312] Batch 724, accuracy/top5 = 0.9
I0630 19:09:15.665050 17469 caffe.cpp:312] Batch 724, loss = 1
I0630 19:09:15.713331 17469 caffe.cpp:312] Batch 725, accuracy/top1 = 0.6
I0630 19:09:15.713354 17469 caffe.cpp:312] Batch 725, accuracy/top5 = 0.86
I0630 19:09:15.713357 17469 caffe.cpp:312] Batch 725, loss = 1.12
I0630 19:09:15.773941 17469 caffe.cpp:312] Batch 726, accuracy/top1 = 0.6
I0630 19:09:15.773962 17469 caffe.cpp:312] Batch 726, accuracy/top5 = 0.82
I0630 19:09:15.773965 17469 caffe.cpp:312] Batch 726, loss = 1.42
I0630 19:09:15.822222 17469 caffe.cpp:312] Batch 727, accuracy/top1 = 0.48
I0630 19:09:15.822247 17469 caffe.cpp:312] Batch 727, accuracy/top5 = 0.76
I0630 19:09:15.822249 17469 caffe.cpp:312] Batch 727, loss = 1.82
I0630 19:09:15.875326 17469 caffe.cpp:312] Batch 728, accuracy/top1 = 0.54
I0630 19:09:15.875347 17469 caffe.cpp:312] Batch 728, accuracy/top5 = 0.8
I0630 19:09:15.875350 17469 caffe.cpp:312] Batch 728, loss = 1.48
I0630 19:09:15.925519 17469 caffe.cpp:312] Batch 729, accuracy/top1 = 0.56
I0630 19:09:15.925540 17469 caffe.cpp:312] Batch 729, accuracy/top5 = 0.84
I0630 19:09:15.925544 17469 caffe.cpp:312] Batch 729, loss = 1.46
I0630 19:09:15.973979 17469 caffe.cpp:312] Batch 730, accuracy/top1 = 0.56
I0630 19:09:15.974002 17469 caffe.cpp:312] Batch 730, accuracy/top5 = 0.74
I0630 19:09:15.974005 17469 caffe.cpp:312] Batch 730, loss = 1.7
I0630 19:09:16.025748 17469 caffe.cpp:312] Batch 731, accuracy/top1 = 0.48
I0630 19:09:16.025768 17469 caffe.cpp:312] Batch 731, accuracy/top5 = 0.68
I0630 19:09:16.025771 17469 caffe.cpp:312] Batch 731, loss = 1.88
I0630 19:09:16.074475 17469 caffe.cpp:312] Batch 732, accuracy/top1 = 0.66
I0630 19:09:16.074498 17469 caffe.cpp:312] Batch 732, accuracy/top5 = 0.84
I0630 19:09:16.074501 17469 caffe.cpp:312] Batch 732, loss = 1.22
I0630 19:09:16.126821 17469 caffe.cpp:312] Batch 733, accuracy/top1 = 0.36
I0630 19:09:16.126847 17469 caffe.cpp:312] Batch 733, accuracy/top5 = 0.8
I0630 19:09:16.126849 17469 caffe.cpp:312] Batch 733, loss = 1.84
I0630 19:09:16.175691 17469 caffe.cpp:312] Batch 734, accuracy/top1 = 0.62
I0630 19:09:16.175715 17469 caffe.cpp:312] Batch 734, accuracy/top5 = 0.76
I0630 19:09:16.175719 17469 caffe.cpp:312] Batch 734, loss = 1.46
I0630 19:09:16.228382 17469 caffe.cpp:312] Batch 735, accuracy/top1 = 0.52
I0630 19:09:16.228404 17469 caffe.cpp:312] Batch 735, accuracy/top5 = 0.76
I0630 19:09:16.228407 17469 caffe.cpp:312] Batch 735, loss = 1.76
I0630 19:09:16.277081 17469 caffe.cpp:312] Batch 736, accuracy/top1 = 0.5
I0630 19:09:16.277103 17469 caffe.cpp:312] Batch 736, accuracy/top5 = 0.8
I0630 19:09:16.277107 17469 caffe.cpp:312] Batch 736, loss = 1.58
I0630 19:09:16.330163 17469 caffe.cpp:312] Batch 737, accuracy/top1 = 0.54
I0630 19:09:16.330183 17469 caffe.cpp:312] Batch 737, accuracy/top5 = 0.78
I0630 19:09:16.330188 17469 caffe.cpp:312] Batch 737, loss = 1.54
I0630 19:09:16.379417 17469 caffe.cpp:312] Batch 738, accuracy/top1 = 0.58
I0630 19:09:16.379441 17469 caffe.cpp:312] Batch 738, accuracy/top5 = 0.86
I0630 19:09:16.379444 17469 caffe.cpp:312] Batch 738, loss = 1.24
I0630 19:09:16.430151 17469 caffe.cpp:312] Batch 739, accuracy/top1 = 0.6
I0630 19:09:16.430172 17469 caffe.cpp:312] Batch 739, accuracy/top5 = 0.84
I0630 19:09:16.430176 17469 caffe.cpp:312] Batch 739, loss = 1.3
I0630 19:09:16.478421 17469 caffe.cpp:312] Batch 740, accuracy/top1 = 0.66
I0630 19:09:16.478446 17469 caffe.cpp:312] Batch 740, accuracy/top5 = 0.86
I0630 19:09:16.478462 17469 caffe.cpp:312] Batch 740, loss = 1.3
I0630 19:09:16.530458 17469 caffe.cpp:312] Batch 741, accuracy/top1 = 0.68
I0630 19:09:16.530479 17469 caffe.cpp:312] Batch 741, accuracy/top5 = 0.8
I0630 19:09:16.530483 17469 caffe.cpp:312] Batch 741, loss = 1.4
I0630 19:09:16.579443 17469 caffe.cpp:312] Batch 742, accuracy/top1 = 0.58
I0630 19:09:16.579466 17469 caffe.cpp:312] Batch 742, accuracy/top5 = 0.82
I0630 19:09:16.579470 17469 caffe.cpp:312] Batch 742, loss = 1.38
I0630 19:09:16.631839 17469 caffe.cpp:312] Batch 743, accuracy/top1 = 0.44
I0630 19:09:16.631863 17469 caffe.cpp:312] Batch 743, accuracy/top5 = 0.76
I0630 19:09:16.631866 17469 caffe.cpp:312] Batch 743, loss = 1.84
I0630 19:09:16.680203 17469 caffe.cpp:312] Batch 744, accuracy/top1 = 0.56
I0630 19:09:16.680225 17469 caffe.cpp:312] Batch 744, accuracy/top5 = 0.7
I0630 19:09:16.680229 17469 caffe.cpp:312] Batch 744, loss = 1.84
I0630 19:09:16.732836 17469 caffe.cpp:312] Batch 745, accuracy/top1 = 0.62
I0630 19:09:16.732857 17469 caffe.cpp:312] Batch 745, accuracy/top5 = 0.8
I0630 19:09:16.732861 17469 caffe.cpp:312] Batch 745, loss = 1.34
I0630 19:09:16.781450 17469 caffe.cpp:312] Batch 746, accuracy/top1 = 0.62
I0630 19:09:16.781472 17469 caffe.cpp:312] Batch 746, accuracy/top5 = 0.84
I0630 19:09:16.781476 17469 caffe.cpp:312] Batch 746, loss = 1.44
I0630 19:09:16.834022 17469 caffe.cpp:312] Batch 747, accuracy/top1 = 0.62
I0630 19:09:16.834044 17469 caffe.cpp:312] Batch 747, accuracy/top5 = 0.86
I0630 19:09:16.834048 17469 caffe.cpp:312] Batch 747, loss = 1.36
I0630 19:09:16.884378 17469 caffe.cpp:312] Batch 748, accuracy/top1 = 0.56
I0630 19:09:16.884399 17469 caffe.cpp:312] Batch 748, accuracy/top5 = 0.88
I0630 19:09:16.884402 17469 caffe.cpp:312] Batch 748, loss = 1.06
I0630 19:09:16.935652 17469 caffe.cpp:312] Batch 749, accuracy/top1 = 0.64
I0630 19:09:16.935674 17469 caffe.cpp:312] Batch 749, accuracy/top5 = 0.82
I0630 19:09:16.935678 17469 caffe.cpp:312] Batch 749, loss = 1.2
I0630 19:09:16.984840 17469 caffe.cpp:312] Batch 750, accuracy/top1 = 0.58
I0630 19:09:16.984863 17469 caffe.cpp:312] Batch 750, accuracy/top5 = 0.82
I0630 19:09:16.984865 17469 caffe.cpp:312] Batch 750, loss = 1.32
I0630 19:09:17.036542 17469 caffe.cpp:312] Batch 751, accuracy/top1 = 0.62
I0630 19:09:17.036564 17469 caffe.cpp:312] Batch 751, accuracy/top5 = 0.76
I0630 19:09:17.036567 17469 caffe.cpp:312] Batch 751, loss = 1.5
I0630 19:09:17.084656 17469 caffe.cpp:312] Batch 752, accuracy/top1 = 0.68
I0630 19:09:17.084681 17469 caffe.cpp:312] Batch 752, accuracy/top5 = 0.92
I0630 19:09:17.084683 17469 caffe.cpp:312] Batch 752, loss = 0.9
I0630 19:09:17.137737 17469 caffe.cpp:312] Batch 753, accuracy/top1 = 0.4
I0630 19:09:17.137758 17469 caffe.cpp:312] Batch 753, accuracy/top5 = 0.66
I0630 19:09:17.137761 17469 caffe.cpp:312] Batch 753, loss = 1.8
I0630 19:09:17.186341 17469 caffe.cpp:312] Batch 754, accuracy/top1 = 0.52
I0630 19:09:17.186364 17469 caffe.cpp:312] Batch 754, accuracy/top5 = 0.84
I0630 19:09:17.186369 17469 caffe.cpp:312] Batch 754, loss = 1.54
I0630 19:09:17.238312 17469 caffe.cpp:312] Batch 755, accuracy/top1 = 0.56
I0630 19:09:17.238333 17469 caffe.cpp:312] Batch 755, accuracy/top5 = 0.76
I0630 19:09:17.238337 17469 caffe.cpp:312] Batch 755, loss = 2
I0630 19:09:17.287142 17469 caffe.cpp:312] Batch 756, accuracy/top1 = 0.6
I0630 19:09:17.287166 17469 caffe.cpp:312] Batch 756, accuracy/top5 = 0.82
I0630 19:09:17.287171 17469 caffe.cpp:312] Batch 756, loss = 1.44
I0630 19:09:17.339406 17469 caffe.cpp:312] Batch 757, accuracy/top1 = 0.68
I0630 19:09:17.339428 17469 caffe.cpp:312] Batch 757, accuracy/top5 = 0.92
I0630 19:09:17.339433 17469 caffe.cpp:312] Batch 757, loss = 0.96
I0630 19:09:17.388142 17469 caffe.cpp:312] Batch 758, accuracy/top1 = 0.62
I0630 19:09:17.388166 17469 caffe.cpp:312] Batch 758, accuracy/top5 = 0.86
I0630 19:09:17.388170 17469 caffe.cpp:312] Batch 758, loss = 1.32
I0630 19:09:17.440326 17469 caffe.cpp:312] Batch 759, accuracy/top1 = 0.56
I0630 19:09:17.440347 17469 caffe.cpp:312] Batch 759, accuracy/top5 = 0.8
I0630 19:09:17.440366 17469 caffe.cpp:312] Batch 759, loss = 2.04
I0630 19:09:17.490854 17469 caffe.cpp:312] Batch 760, accuracy/top1 = 0.64
I0630 19:09:17.490876 17469 caffe.cpp:312] Batch 760, accuracy/top5 = 0.88
I0630 19:09:17.490880 17469 caffe.cpp:312] Batch 760, loss = 1.04
I0630 19:09:17.538358 17469 caffe.cpp:312] Batch 761, accuracy/top1 = 0.56
I0630 19:09:17.538381 17469 caffe.cpp:312] Batch 761, accuracy/top5 = 0.88
I0630 19:09:17.538384 17469 caffe.cpp:312] Batch 761, loss = 1.52
I0630 19:09:17.591169 17469 caffe.cpp:312] Batch 762, accuracy/top1 = 0.58
I0630 19:09:17.591192 17469 caffe.cpp:312] Batch 762, accuracy/top5 = 0.82
I0630 19:09:17.591194 17469 caffe.cpp:312] Batch 762, loss = 1.44
I0630 19:09:17.638545 17469 caffe.cpp:312] Batch 763, accuracy/top1 = 0.52
I0630 19:09:17.638568 17469 caffe.cpp:312] Batch 763, accuracy/top5 = 0.82
I0630 19:09:17.638571 17469 caffe.cpp:312] Batch 763, loss = 1.52
I0630 19:09:17.692030 17469 caffe.cpp:312] Batch 764, accuracy/top1 = 0.4
I0630 19:09:17.692052 17469 caffe.cpp:312] Batch 764, accuracy/top5 = 0.64
I0630 19:09:17.692055 17469 caffe.cpp:312] Batch 764, loss = 2.4
I0630 19:09:17.740933 17469 caffe.cpp:312] Batch 765, accuracy/top1 = 0.52
I0630 19:09:17.740955 17469 caffe.cpp:312] Batch 765, accuracy/top5 = 0.82
I0630 19:09:17.740958 17469 caffe.cpp:312] Batch 765, loss = 1.72
I0630 19:09:17.793113 17469 caffe.cpp:312] Batch 766, accuracy/top1 = 0.6
I0630 19:09:17.793134 17469 caffe.cpp:312] Batch 766, accuracy/top5 = 0.8
I0630 19:09:17.793138 17469 caffe.cpp:312] Batch 766, loss = 1.42
I0630 19:09:17.841936 17469 caffe.cpp:312] Batch 767, accuracy/top1 = 0.58
I0630 19:09:17.841958 17469 caffe.cpp:312] Batch 767, accuracy/top5 = 0.84
I0630 19:09:17.841961 17469 caffe.cpp:312] Batch 767, loss = 1.34
I0630 19:09:17.895340 17469 caffe.cpp:312] Batch 768, accuracy/top1 = 0.5
I0630 19:09:17.895364 17469 caffe.cpp:312] Batch 768, accuracy/top5 = 0.78
I0630 19:09:17.895367 17469 caffe.cpp:312] Batch 768, loss = 1.5
I0630 19:09:17.946341 17469 caffe.cpp:312] Batch 769, accuracy/top1 = 0.56
I0630 19:09:17.946363 17469 caffe.cpp:312] Batch 769, accuracy/top5 = 0.84
I0630 19:09:17.946367 17469 caffe.cpp:312] Batch 769, loss = 1.44
I0630 19:09:17.995303 17469 caffe.cpp:312] Batch 770, accuracy/top1 = 0.56
I0630 19:09:17.995326 17469 caffe.cpp:312] Batch 770, accuracy/top5 = 0.82
I0630 19:09:17.995329 17469 caffe.cpp:312] Batch 770, loss = 1.74
I0630 19:09:18.048130 17469 caffe.cpp:312] Batch 771, accuracy/top1 = 0.5
I0630 19:09:18.048152 17469 caffe.cpp:312] Batch 771, accuracy/top5 = 0.72
I0630 19:09:18.048156 17469 caffe.cpp:312] Batch 771, loss = 1.96
I0630 19:09:18.095976 17469 caffe.cpp:312] Batch 772, accuracy/top1 = 0.6
I0630 19:09:18.096000 17469 caffe.cpp:312] Batch 772, accuracy/top5 = 0.74
I0630 19:09:18.096004 17469 caffe.cpp:312] Batch 772, loss = 1.9
I0630 19:09:18.148080 17469 caffe.cpp:312] Batch 773, accuracy/top1 = 0.54
I0630 19:09:18.148102 17469 caffe.cpp:312] Batch 773, accuracy/top5 = 0.7
I0630 19:09:18.148105 17469 caffe.cpp:312] Batch 773, loss = 2.14
I0630 19:09:18.196719 17469 caffe.cpp:312] Batch 774, accuracy/top1 = 0.5
I0630 19:09:18.196741 17469 caffe.cpp:312] Batch 774, accuracy/top5 = 0.76
I0630 19:09:18.196744 17469 caffe.cpp:312] Batch 774, loss = 1.68
I0630 19:09:18.248939 17469 caffe.cpp:312] Batch 775, accuracy/top1 = 0.54
I0630 19:09:18.248961 17469 caffe.cpp:312] Batch 775, accuracy/top5 = 0.8
I0630 19:09:18.248965 17469 caffe.cpp:312] Batch 775, loss = 1.24
I0630 19:09:18.298151 17469 caffe.cpp:312] Batch 776, accuracy/top1 = 0.66
I0630 19:09:18.298173 17469 caffe.cpp:312] Batch 776, accuracy/top5 = 0.8
I0630 19:09:18.298177 17469 caffe.cpp:312] Batch 776, loss = 1.54
I0630 19:09:18.350916 17469 caffe.cpp:312] Batch 777, accuracy/top1 = 0.5
I0630 19:09:18.350937 17469 caffe.cpp:312] Batch 777, accuracy/top5 = 0.82
I0630 19:09:18.350941 17469 caffe.cpp:312] Batch 777, loss = 1.62
I0630 19:09:18.400055 17469 caffe.cpp:312] Batch 778, accuracy/top1 = 0.62
I0630 19:09:18.400079 17469 caffe.cpp:312] Batch 778, accuracy/top5 = 0.84
I0630 19:09:18.400095 17469 caffe.cpp:312] Batch 778, loss = 1.22
I0630 19:09:18.451750 17469 caffe.cpp:312] Batch 779, accuracy/top1 = 0.54
I0630 19:09:18.451772 17469 caffe.cpp:312] Batch 779, accuracy/top5 = 0.74
I0630 19:09:18.451776 17469 caffe.cpp:312] Batch 779, loss = 1.7
I0630 19:09:18.500540 17469 caffe.cpp:312] Batch 780, accuracy/top1 = 0.52
I0630 19:09:18.500562 17469 caffe.cpp:312] Batch 780, accuracy/top5 = 0.78
I0630 19:09:18.500566 17469 caffe.cpp:312] Batch 780, loss = 1.6
I0630 19:09:18.552719 17469 caffe.cpp:312] Batch 781, accuracy/top1 = 0.66
I0630 19:09:18.552741 17469 caffe.cpp:312] Batch 781, accuracy/top5 = 0.78
I0630 19:09:18.552744 17469 caffe.cpp:312] Batch 781, loss = 1.42
I0630 19:09:18.601101 17469 caffe.cpp:312] Batch 782, accuracy/top1 = 0.64
I0630 19:09:18.601125 17469 caffe.cpp:312] Batch 782, accuracy/top5 = 0.88
I0630 19:09:18.601128 17469 caffe.cpp:312] Batch 782, loss = 1.04
I0630 19:09:18.653612 17469 caffe.cpp:312] Batch 783, accuracy/top1 = 0.58
I0630 19:09:18.653632 17469 caffe.cpp:312] Batch 783, accuracy/top5 = 0.82
I0630 19:09:18.653635 17469 caffe.cpp:312] Batch 783, loss = 1.54
I0630 19:09:18.702543 17469 caffe.cpp:312] Batch 784, accuracy/top1 = 0.58
I0630 19:09:18.702567 17469 caffe.cpp:312] Batch 784, accuracy/top5 = 0.78
I0630 19:09:18.702570 17469 caffe.cpp:312] Batch 784, loss = 1.62
I0630 19:09:18.754896 17469 caffe.cpp:312] Batch 785, accuracy/top1 = 0.46
I0630 19:09:18.754917 17469 caffe.cpp:312] Batch 785, accuracy/top5 = 0.78
I0630 19:09:18.754920 17469 caffe.cpp:312] Batch 785, loss = 1.62
I0630 19:09:18.803115 17469 caffe.cpp:312] Batch 786, accuracy/top1 = 0.72
I0630 19:09:18.803140 17469 caffe.cpp:312] Batch 786, accuracy/top5 = 0.9
I0630 19:09:18.803143 17469 caffe.cpp:312] Batch 786, loss = 0.92
I0630 19:09:18.855999 17469 caffe.cpp:312] Batch 787, accuracy/top1 = 0.52
I0630 19:09:18.856036 17469 caffe.cpp:312] Batch 787, accuracy/top5 = 0.78
I0630 19:09:18.856040 17469 caffe.cpp:312] Batch 787, loss = 1.54
I0630 19:09:18.905071 17469 caffe.cpp:312] Batch 788, accuracy/top1 = 0.48
I0630 19:09:18.905093 17469 caffe.cpp:312] Batch 788, accuracy/top5 = 0.74
I0630 19:09:18.905097 17469 caffe.cpp:312] Batch 788, loss = 1.78
I0630 19:09:18.953279 17469 caffe.cpp:312] Batch 789, accuracy/top1 = 0.66
I0630 19:09:18.953299 17469 caffe.cpp:312] Batch 789, accuracy/top5 = 0.82
I0630 19:09:18.953302 17469 caffe.cpp:312] Batch 789, loss = 1.28
I0630 19:09:19.001225 17469 caffe.cpp:312] Batch 790, accuracy/top1 = 0.6
I0630 19:09:19.001248 17469 caffe.cpp:312] Batch 790, accuracy/top5 = 0.84
I0630 19:09:19.001251 17469 caffe.cpp:312] Batch 790, loss = 1.24
I0630 19:09:19.054548 17469 caffe.cpp:312] Batch 791, accuracy/top1 = 0.52
I0630 19:09:19.054567 17469 caffe.cpp:312] Batch 791, accuracy/top5 = 0.68
I0630 19:09:19.054570 17469 caffe.cpp:312] Batch 791, loss = 1.88
I0630 19:09:19.101531 17469 caffe.cpp:312] Batch 792, accuracy/top1 = 0.54
I0630 19:09:19.101553 17469 caffe.cpp:312] Batch 792, accuracy/top5 = 0.76
I0630 19:09:19.101557 17469 caffe.cpp:312] Batch 792, loss = 1.28
I0630 19:09:19.154800 17469 caffe.cpp:312] Batch 793, accuracy/top1 = 0.44
I0630 19:09:19.154822 17469 caffe.cpp:312] Batch 793, accuracy/top5 = 0.76
I0630 19:09:19.154825 17469 caffe.cpp:312] Batch 793, loss = 1.82
I0630 19:09:19.203949 17469 caffe.cpp:312] Batch 794, accuracy/top1 = 0.58
I0630 19:09:19.203974 17469 caffe.cpp:312] Batch 794, accuracy/top5 = 0.78
I0630 19:09:19.203976 17469 caffe.cpp:312] Batch 794, loss = 1.56
I0630 19:09:19.255301 17469 caffe.cpp:312] Batch 795, accuracy/top1 = 0.52
I0630 19:09:19.255323 17469 caffe.cpp:312] Batch 795, accuracy/top5 = 0.78
I0630 19:09:19.255326 17469 caffe.cpp:312] Batch 795, loss = 1.92
I0630 19:09:19.303531 17469 caffe.cpp:312] Batch 796, accuracy/top1 = 0.5
I0630 19:09:19.303555 17469 caffe.cpp:312] Batch 796, accuracy/top5 = 0.74
I0630 19:09:19.303558 17469 caffe.cpp:312] Batch 796, loss = 2
I0630 19:09:19.356072 17469 caffe.cpp:312] Batch 797, accuracy/top1 = 0.5
I0630 19:09:19.356094 17469 caffe.cpp:312] Batch 797, accuracy/top5 = 0.74
I0630 19:09:19.356112 17469 caffe.cpp:312] Batch 797, loss = 1.72
I0630 19:09:19.405412 17469 caffe.cpp:312] Batch 798, accuracy/top1 = 0.7
I0630 19:09:19.405436 17469 caffe.cpp:312] Batch 798, accuracy/top5 = 0.88
I0630 19:09:19.405439 17469 caffe.cpp:312] Batch 798, loss = 1.02
I0630 19:09:19.457129 17469 caffe.cpp:312] Batch 799, accuracy/top1 = 0.52
I0630 19:09:19.457150 17469 caffe.cpp:312] Batch 799, accuracy/top5 = 0.78
I0630 19:09:19.457154 17469 caffe.cpp:312] Batch 799, loss = 1.74
I0630 19:09:19.505802 17469 caffe.cpp:312] Batch 800, accuracy/top1 = 0.46
I0630 19:09:19.505825 17469 caffe.cpp:312] Batch 800, accuracy/top5 = 0.62
I0630 19:09:19.505827 17469 caffe.cpp:312] Batch 800, loss = 2.92
I0630 19:09:19.558161 17469 caffe.cpp:312] Batch 801, accuracy/top1 = 0.54
I0630 19:09:19.558182 17469 caffe.cpp:312] Batch 801, accuracy/top5 = 0.74
I0630 19:09:19.558185 17469 caffe.cpp:312] Batch 801, loss = 1.48
I0630 19:09:19.606047 17469 caffe.cpp:312] Batch 802, accuracy/top1 = 0.56
I0630 19:09:19.606070 17469 caffe.cpp:312] Batch 802, accuracy/top5 = 0.84
I0630 19:09:19.606073 17469 caffe.cpp:312] Batch 802, loss = 1.36
I0630 19:09:19.659127 17469 caffe.cpp:312] Batch 803, accuracy/top1 = 0.68
I0630 19:09:19.659149 17469 caffe.cpp:312] Batch 803, accuracy/top5 = 0.76
I0630 19:09:19.659153 17469 caffe.cpp:312] Batch 803, loss = 1.52
I0630 19:09:19.707399 17469 caffe.cpp:312] Batch 804, accuracy/top1 = 0.52
I0630 19:09:19.707422 17469 caffe.cpp:312] Batch 804, accuracy/top5 = 0.88
I0630 19:09:19.707425 17469 caffe.cpp:312] Batch 804, loss = 1.38
I0630 19:09:19.759692 17469 caffe.cpp:312] Batch 805, accuracy/top1 = 0.56
I0630 19:09:19.759714 17469 caffe.cpp:312] Batch 805, accuracy/top5 = 0.76
I0630 19:09:19.759717 17469 caffe.cpp:312] Batch 805, loss = 1.62
I0630 19:09:19.807943 17469 caffe.cpp:312] Batch 806, accuracy/top1 = 0.62
I0630 19:09:19.807966 17469 caffe.cpp:312] Batch 806, accuracy/top5 = 0.84
I0630 19:09:19.807970 17469 caffe.cpp:312] Batch 806, loss = 1.22
I0630 19:09:19.860460 17469 caffe.cpp:312] Batch 807, accuracy/top1 = 0.54
I0630 19:09:19.860508 17469 caffe.cpp:312] Batch 807, accuracy/top5 = 0.74
I0630 19:09:19.860512 17469 caffe.cpp:312] Batch 807, loss = 1.52
I0630 19:09:19.912861 17469 caffe.cpp:312] Batch 808, accuracy/top1 = 0.68
I0630 19:09:19.912883 17469 caffe.cpp:312] Batch 808, accuracy/top5 = 0.88
I0630 19:09:19.912885 17469 caffe.cpp:312] Batch 808, loss = 1.44
I0630 19:09:19.961683 17469 caffe.cpp:312] Batch 809, accuracy/top1 = 0.54
I0630 19:09:19.961705 17469 caffe.cpp:312] Batch 809, accuracy/top5 = 0.72
I0630 19:09:19.961709 17469 caffe.cpp:312] Batch 809, loss = 2.3
I0630 19:09:20.014175 17469 caffe.cpp:312] Batch 810, accuracy/top1 = 0.52
I0630 19:09:20.014197 17469 caffe.cpp:312] Batch 810, accuracy/top5 = 0.74
I0630 19:09:20.014200 17469 caffe.cpp:312] Batch 810, loss = 1.8
I0630 19:09:20.064106 17469 caffe.cpp:312] Batch 811, accuracy/top1 = 0.6
I0630 19:09:20.064129 17469 caffe.cpp:312] Batch 811, accuracy/top5 = 0.78
I0630 19:09:20.064132 17469 caffe.cpp:312] Batch 811, loss = 1.9
I0630 19:09:20.116132 17469 caffe.cpp:312] Batch 812, accuracy/top1 = 0.56
I0630 19:09:20.116153 17469 caffe.cpp:312] Batch 812, accuracy/top5 = 0.82
I0630 19:09:20.116156 17469 caffe.cpp:312] Batch 812, loss = 1.42
I0630 19:09:20.164166 17469 caffe.cpp:312] Batch 813, accuracy/top1 = 0.5
I0630 19:09:20.164189 17469 caffe.cpp:312] Batch 813, accuracy/top5 = 0.72
I0630 19:09:20.164192 17469 caffe.cpp:312] Batch 813, loss = 1.7
I0630 19:09:20.216735 17469 caffe.cpp:312] Batch 814, accuracy/top1 = 0.44
I0630 19:09:20.216758 17469 caffe.cpp:312] Batch 814, accuracy/top5 = 0.7
I0630 19:09:20.216760 17469 caffe.cpp:312] Batch 814, loss = 2.2
I0630 19:09:20.265367 17469 caffe.cpp:312] Batch 815, accuracy/top1 = 0.46
I0630 19:09:20.265389 17469 caffe.cpp:312] Batch 815, accuracy/top5 = 0.8
I0630 19:09:20.265393 17469 caffe.cpp:312] Batch 815, loss = 1.36
I0630 19:09:20.318202 17469 caffe.cpp:312] Batch 816, accuracy/top1 = 0.56
I0630 19:09:20.318222 17469 caffe.cpp:312] Batch 816, accuracy/top5 = 0.74
I0630 19:09:20.318238 17469 caffe.cpp:312] Batch 816, loss = 1.9
I0630 19:09:20.367041 17469 caffe.cpp:312] Batch 817, accuracy/top1 = 0.52
I0630 19:09:20.367065 17469 caffe.cpp:312] Batch 817, accuracy/top5 = 0.78
I0630 19:09:20.367069 17469 caffe.cpp:312] Batch 817, loss = 1.56
I0630 19:09:20.419665 17469 caffe.cpp:312] Batch 818, accuracy/top1 = 0.56
I0630 19:09:20.419687 17469 caffe.cpp:312] Batch 818, accuracy/top5 = 0.94
I0630 19:09:20.419690 17469 caffe.cpp:312] Batch 818, loss = 1.26
I0630 19:09:20.467804 17469 caffe.cpp:312] Batch 819, accuracy/top1 = 0.64
I0630 19:09:20.467828 17469 caffe.cpp:312] Batch 819, accuracy/top5 = 0.82
I0630 19:09:20.467831 17469 caffe.cpp:312] Batch 819, loss = 1.32
I0630 19:09:20.520514 17469 caffe.cpp:312] Batch 820, accuracy/top1 = 0.56
I0630 19:09:20.520535 17469 caffe.cpp:312] Batch 820, accuracy/top5 = 0.82
I0630 19:09:20.520539 17469 caffe.cpp:312] Batch 820, loss = 1.6
I0630 19:09:20.568328 17469 caffe.cpp:312] Batch 821, accuracy/top1 = 0.6
I0630 19:09:20.568351 17469 caffe.cpp:312] Batch 821, accuracy/top5 = 0.84
I0630 19:09:20.568354 17469 caffe.cpp:312] Batch 821, loss = 1.3
I0630 19:09:20.621561 17469 caffe.cpp:312] Batch 822, accuracy/top1 = 0.58
I0630 19:09:20.621582 17469 caffe.cpp:312] Batch 822, accuracy/top5 = 0.82
I0630 19:09:20.621584 17469 caffe.cpp:312] Batch 822, loss = 1.56
I0630 19:09:20.670161 17469 caffe.cpp:312] Batch 823, accuracy/top1 = 0.6
I0630 19:09:20.670184 17469 caffe.cpp:312] Batch 823, accuracy/top5 = 0.86
I0630 19:09:20.670187 17469 caffe.cpp:312] Batch 823, loss = 1.24
I0630 19:09:20.722497 17469 caffe.cpp:312] Batch 824, accuracy/top1 = 0.52
I0630 19:09:20.722519 17469 caffe.cpp:312] Batch 824, accuracy/top5 = 0.82
I0630 19:09:20.722523 17469 caffe.cpp:312] Batch 824, loss = 1.64
I0630 19:09:20.772565 17469 caffe.cpp:312] Batch 825, accuracy/top1 = 0.6
I0630 19:09:20.772589 17469 caffe.cpp:312] Batch 825, accuracy/top5 = 0.82
I0630 19:09:20.772593 17469 caffe.cpp:312] Batch 825, loss = 1.14
I0630 19:09:20.823493 17469 caffe.cpp:312] Batch 826, accuracy/top1 = 0.56
I0630 19:09:20.823520 17469 caffe.cpp:312] Batch 826, accuracy/top5 = 0.82
I0630 19:09:20.823523 17469 caffe.cpp:312] Batch 826, loss = 1.48
I0630 19:09:20.872328 17469 caffe.cpp:312] Batch 827, accuracy/top1 = 0.68
I0630 19:09:20.872351 17469 caffe.cpp:312] Batch 827, accuracy/top5 = 0.88
I0630 19:09:20.872354 17469 caffe.cpp:312] Batch 827, loss = 0.98
I0630 19:09:20.925472 17469 caffe.cpp:312] Batch 828, accuracy/top1 = 0.58
I0630 19:09:20.925493 17469 caffe.cpp:312] Batch 828, accuracy/top5 = 0.78
I0630 19:09:20.925496 17469 caffe.cpp:312] Batch 828, loss = 1.32
I0630 19:09:20.974300 17469 caffe.cpp:312] Batch 829, accuracy/top1 = 0.4
I0630 19:09:20.974323 17469 caffe.cpp:312] Batch 829, accuracy/top5 = 0.74
I0630 19:09:20.974326 17469 caffe.cpp:312] Batch 829, loss = 2.5
I0630 19:09:21.026640 17469 caffe.cpp:312] Batch 830, accuracy/top1 = 0.66
I0630 19:09:21.026662 17469 caffe.cpp:312] Batch 830, accuracy/top5 = 0.8
I0630 19:09:21.026665 17469 caffe.cpp:312] Batch 830, loss = 1.3
I0630 19:09:21.075878 17469 caffe.cpp:312] Batch 831, accuracy/top1 = 0.56
I0630 19:09:21.075899 17469 caffe.cpp:312] Batch 831, accuracy/top5 = 0.8
I0630 19:09:21.075902 17469 caffe.cpp:312] Batch 831, loss = 1.46
I0630 19:09:21.129604 17469 caffe.cpp:312] Batch 832, accuracy/top1 = 0.54
I0630 19:09:21.129626 17469 caffe.cpp:312] Batch 832, accuracy/top5 = 0.8
I0630 19:09:21.129631 17469 caffe.cpp:312] Batch 832, loss = 1.4
I0630 19:09:21.178261 17469 caffe.cpp:312] Batch 833, accuracy/top1 = 0.54
I0630 19:09:21.178285 17469 caffe.cpp:312] Batch 833, accuracy/top5 = 0.84
I0630 19:09:21.178288 17469 caffe.cpp:312] Batch 833, loss = 1.4
I0630 19:09:21.229717 17469 caffe.cpp:312] Batch 834, accuracy/top1 = 0.62
I0630 19:09:21.229739 17469 caffe.cpp:312] Batch 834, accuracy/top5 = 0.84
I0630 19:09:21.229743 17469 caffe.cpp:312] Batch 834, loss = 1.42
I0630 19:09:21.278118 17469 caffe.cpp:312] Batch 835, accuracy/top1 = 0.48
I0630 19:09:21.278141 17469 caffe.cpp:312] Batch 835, accuracy/top5 = 0.78
I0630 19:09:21.278159 17469 caffe.cpp:312] Batch 835, loss = 1.72
I0630 19:09:21.329537 17469 caffe.cpp:312] Batch 836, accuracy/top1 = 0.52
I0630 19:09:21.329560 17469 caffe.cpp:312] Batch 836, accuracy/top5 = 0.78
I0630 19:09:21.329562 17469 caffe.cpp:312] Batch 836, loss = 1.48
I0630 19:09:21.378176 17469 caffe.cpp:312] Batch 837, accuracy/top1 = 0.54
I0630 19:09:21.378198 17469 caffe.cpp:312] Batch 837, accuracy/top5 = 0.88
I0630 19:09:21.378201 17469 caffe.cpp:312] Batch 837, loss = 1.18
I0630 19:09:21.430233 17469 caffe.cpp:312] Batch 838, accuracy/top1 = 0.5
I0630 19:09:21.430253 17469 caffe.cpp:312] Batch 838, accuracy/top5 = 0.82
I0630 19:09:21.430256 17469 caffe.cpp:312] Batch 838, loss = 1.58
I0630 19:09:21.478062 17469 caffe.cpp:312] Batch 839, accuracy/top1 = 0.5
I0630 19:09:21.478085 17469 caffe.cpp:312] Batch 839, accuracy/top5 = 0.78
I0630 19:09:21.478088 17469 caffe.cpp:312] Batch 839, loss = 1.78
I0630 19:09:21.530711 17469 caffe.cpp:312] Batch 840, accuracy/top1 = 0.64
I0630 19:09:21.530732 17469 caffe.cpp:312] Batch 840, accuracy/top5 = 0.78
I0630 19:09:21.530735 17469 caffe.cpp:312] Batch 840, loss = 1.58
I0630 19:09:21.580286 17469 caffe.cpp:312] Batch 841, accuracy/top1 = 0.52
I0630 19:09:21.580308 17469 caffe.cpp:312] Batch 841, accuracy/top5 = 0.78
I0630 19:09:21.580312 17469 caffe.cpp:312] Batch 841, loss = 1.84
I0630 19:09:21.632002 17469 caffe.cpp:312] Batch 842, accuracy/top1 = 0.52
I0630 19:09:21.632024 17469 caffe.cpp:312] Batch 842, accuracy/top5 = 0.86
I0630 19:09:21.632027 17469 caffe.cpp:312] Batch 842, loss = 1.26
I0630 19:09:21.680018 17469 caffe.cpp:312] Batch 843, accuracy/top1 = 0.5
I0630 19:09:21.680042 17469 caffe.cpp:312] Batch 843, accuracy/top5 = 0.8
I0630 19:09:21.680045 17469 caffe.cpp:312] Batch 843, loss = 1.64
I0630 19:09:21.732213 17469 caffe.cpp:312] Batch 844, accuracy/top1 = 0.64
I0630 19:09:21.732234 17469 caffe.cpp:312] Batch 844, accuracy/top5 = 0.9
I0630 19:09:21.732237 17469 caffe.cpp:312] Batch 844, loss = 1.16
I0630 19:09:21.780143 17469 caffe.cpp:312] Batch 845, accuracy/top1 = 0.56
I0630 19:09:21.780166 17469 caffe.cpp:312] Batch 845, accuracy/top5 = 0.76
I0630 19:09:21.780169 17469 caffe.cpp:312] Batch 845, loss = 1.66
I0630 19:09:21.832656 17469 caffe.cpp:312] Batch 846, accuracy/top1 = 0.46
I0630 19:09:21.832677 17469 caffe.cpp:312] Batch 846, accuracy/top5 = 0.78
I0630 19:09:21.832680 17469 caffe.cpp:312] Batch 846, loss = 1.66
I0630 19:09:21.882628 17469 caffe.cpp:312] Batch 847, accuracy/top1 = 0.48
I0630 19:09:21.882652 17469 caffe.cpp:312] Batch 847, accuracy/top5 = 0.76
I0630 19:09:21.882654 17469 caffe.cpp:312] Batch 847, loss = 1.88
I0630 19:09:21.935570 17469 caffe.cpp:312] Batch 848, accuracy/top1 = 0.58
I0630 19:09:21.935591 17469 caffe.cpp:312] Batch 848, accuracy/top5 = 0.82
I0630 19:09:21.935595 17469 caffe.cpp:312] Batch 848, loss = 1.62
I0630 19:09:21.983556 17469 caffe.cpp:312] Batch 849, accuracy/top1 = 0.58
I0630 19:09:21.983579 17469 caffe.cpp:312] Batch 849, accuracy/top5 = 0.82
I0630 19:09:21.983583 17469 caffe.cpp:312] Batch 849, loss = 1.24
I0630 19:09:22.036051 17469 caffe.cpp:312] Batch 850, accuracy/top1 = 0.62
I0630 19:09:22.036072 17469 caffe.cpp:312] Batch 850, accuracy/top5 = 0.82
I0630 19:09:22.036077 17469 caffe.cpp:312] Batch 850, loss = 1.34
I0630 19:09:22.084249 17469 caffe.cpp:312] Batch 851, accuracy/top1 = 0.5
I0630 19:09:22.084272 17469 caffe.cpp:312] Batch 851, accuracy/top5 = 0.82
I0630 19:09:22.084275 17469 caffe.cpp:312] Batch 851, loss = 1.66
I0630 19:09:22.137054 17469 caffe.cpp:312] Batch 852, accuracy/top1 = 0.56
I0630 19:09:22.137076 17469 caffe.cpp:312] Batch 852, accuracy/top5 = 0.76
I0630 19:09:22.137079 17469 caffe.cpp:312] Batch 852, loss = 1.52
I0630 19:09:22.186348 17469 caffe.cpp:312] Batch 853, accuracy/top1 = 0.68
I0630 19:09:22.186372 17469 caffe.cpp:312] Batch 853, accuracy/top5 = 0.86
I0630 19:09:22.186375 17469 caffe.cpp:312] Batch 853, loss = 1.22
I0630 19:09:22.238250 17469 caffe.cpp:312] Batch 854, accuracy/top1 = 0.6
I0630 19:09:22.238273 17469 caffe.cpp:312] Batch 854, accuracy/top5 = 0.78
I0630 19:09:22.238286 17469 caffe.cpp:312] Batch 854, loss = 1.32
I0630 19:09:22.286501 17469 caffe.cpp:312] Batch 855, accuracy/top1 = 0.52
I0630 19:09:22.286525 17469 caffe.cpp:312] Batch 855, accuracy/top5 = 0.8
I0630 19:09:22.286528 17469 caffe.cpp:312] Batch 855, loss = 1.72
I0630 19:09:22.339256 17469 caffe.cpp:312] Batch 856, accuracy/top1 = 0.36
I0630 19:09:22.339278 17469 caffe.cpp:312] Batch 856, accuracy/top5 = 0.7
I0630 19:09:22.339282 17469 caffe.cpp:312] Batch 856, loss = 2.04
I0630 19:09:22.387593 17469 caffe.cpp:312] Batch 857, accuracy/top1 = 0.62
I0630 19:09:22.387615 17469 caffe.cpp:312] Batch 857, accuracy/top5 = 0.82
I0630 19:09:22.387619 17469 caffe.cpp:312] Batch 857, loss = 1.58
I0630 19:09:22.439075 17469 caffe.cpp:312] Batch 858, accuracy/top1 = 0.62
I0630 19:09:22.439098 17469 caffe.cpp:312] Batch 858, accuracy/top5 = 0.84
I0630 19:09:22.439101 17469 caffe.cpp:312] Batch 858, loss = 1.2
I0630 19:09:22.487664 17469 caffe.cpp:312] Batch 859, accuracy/top1 = 0.52
I0630 19:09:22.487689 17469 caffe.cpp:312] Batch 859, accuracy/top5 = 0.82
I0630 19:09:22.487691 17469 caffe.cpp:312] Batch 859, loss = 1.62
I0630 19:09:22.540884 17469 caffe.cpp:312] Batch 860, accuracy/top1 = 0.66
I0630 19:09:22.540904 17469 caffe.cpp:312] Batch 860, accuracy/top5 = 0.82
I0630 19:09:22.540908 17469 caffe.cpp:312] Batch 860, loss = 1.4
I0630 19:09:22.588790 17469 caffe.cpp:312] Batch 861, accuracy/top1 = 0.64
I0630 19:09:22.588814 17469 caffe.cpp:312] Batch 861, accuracy/top5 = 0.82
I0630 19:09:22.588816 17469 caffe.cpp:312] Batch 861, loss = 1.12
I0630 19:09:22.641188 17469 caffe.cpp:312] Batch 862, accuracy/top1 = 0.52
I0630 19:09:22.641209 17469 caffe.cpp:312] Batch 862, accuracy/top5 = 0.78
I0630 19:09:22.641212 17469 caffe.cpp:312] Batch 862, loss = 1.5
I0630 19:09:22.689339 17469 caffe.cpp:312] Batch 863, accuracy/top1 = 0.56
I0630 19:09:22.689363 17469 caffe.cpp:312] Batch 863, accuracy/top5 = 0.8
I0630 19:09:22.689365 17469 caffe.cpp:312] Batch 863, loss = 1.54
I0630 19:09:22.741636 17469 caffe.cpp:312] Batch 864, accuracy/top1 = 0.76
I0630 19:09:22.741658 17469 caffe.cpp:312] Batch 864, accuracy/top5 = 0.84
I0630 19:09:22.741662 17469 caffe.cpp:312] Batch 864, loss = 1.34
I0630 19:09:22.790603 17469 caffe.cpp:312] Batch 865, accuracy/top1 = 0.56
I0630 19:09:22.790627 17469 caffe.cpp:312] Batch 865, accuracy/top5 = 0.74
I0630 19:09:22.790630 17469 caffe.cpp:312] Batch 865, loss = 1.72
I0630 19:09:22.842653 17469 caffe.cpp:312] Batch 866, accuracy/top1 = 0.58
I0630 19:09:22.842675 17469 caffe.cpp:312] Batch 866, accuracy/top5 = 0.84
I0630 19:09:22.842679 17469 caffe.cpp:312] Batch 866, loss = 1.24
I0630 19:09:22.894533 17469 caffe.cpp:312] Batch 867, accuracy/top1 = 0.54
I0630 19:09:22.894553 17469 caffe.cpp:312] Batch 867, accuracy/top5 = 0.8
I0630 19:09:22.894557 17469 caffe.cpp:312] Batch 867, loss = 1.78
I0630 19:09:22.943418 17469 caffe.cpp:312] Batch 868, accuracy/top1 = 0.52
I0630 19:09:22.943440 17469 caffe.cpp:312] Batch 868, accuracy/top5 = 0.82
I0630 19:09:22.943444 17469 caffe.cpp:312] Batch 868, loss = 1.6
I0630 19:09:22.991197 17469 caffe.cpp:312] Batch 869, accuracy/top1 = 0.42
I0630 19:09:22.991219 17469 caffe.cpp:312] Batch 869, accuracy/top5 = 0.82
I0630 19:09:22.991222 17469 caffe.cpp:312] Batch 869, loss = 1.82
I0630 19:09:23.039762 17469 caffe.cpp:312] Batch 870, accuracy/top1 = 0.44
I0630 19:09:23.039785 17469 caffe.cpp:312] Batch 870, accuracy/top5 = 0.74
I0630 19:09:23.039788 17469 caffe.cpp:312] Batch 870, loss = 1.88
I0630 19:09:23.088636 17469 caffe.cpp:312] Batch 871, accuracy/top1 = 0.6
I0630 19:09:23.088657 17469 caffe.cpp:312] Batch 871, accuracy/top5 = 0.88
I0630 19:09:23.088660 17469 caffe.cpp:312] Batch 871, loss = 1.34
I0630 19:09:23.136502 17469 caffe.cpp:312] Batch 872, accuracy/top1 = 0.52
I0630 19:09:23.136526 17469 caffe.cpp:312] Batch 872, accuracy/top5 = 0.78
I0630 19:09:23.136529 17469 caffe.cpp:312] Batch 872, loss = 1.56
I0630 19:09:23.185039 17469 caffe.cpp:312] Batch 873, accuracy/top1 = 0.58
I0630 19:09:23.185060 17469 caffe.cpp:312] Batch 873, accuracy/top5 = 0.78
I0630 19:09:23.185078 17469 caffe.cpp:312] Batch 873, loss = 1.46
I0630 19:09:23.233103 17469 caffe.cpp:312] Batch 874, accuracy/top1 = 0.62
I0630 19:09:23.233126 17469 caffe.cpp:312] Batch 874, accuracy/top5 = 0.82
I0630 19:09:23.233129 17469 caffe.cpp:312] Batch 874, loss = 1.36
I0630 19:09:23.281566 17469 caffe.cpp:312] Batch 875, accuracy/top1 = 0.76
I0630 19:09:23.281589 17469 caffe.cpp:312] Batch 875, accuracy/top5 = 0.84
I0630 19:09:23.281591 17469 caffe.cpp:312] Batch 875, loss = 1.04
I0630 19:09:23.329857 17469 caffe.cpp:312] Batch 876, accuracy/top1 = 0.5
I0630 19:09:23.329879 17469 caffe.cpp:312] Batch 876, accuracy/top5 = 0.8
I0630 19:09:23.329882 17469 caffe.cpp:312] Batch 876, loss = 1.46
I0630 19:09:23.378527 17469 caffe.cpp:312] Batch 877, accuracy/top1 = 0.62
I0630 19:09:23.378549 17469 caffe.cpp:312] Batch 877, accuracy/top5 = 0.8
I0630 19:09:23.378553 17469 caffe.cpp:312] Batch 877, loss = 1.32
I0630 19:09:23.426518 17469 caffe.cpp:312] Batch 878, accuracy/top1 = 0.54
I0630 19:09:23.426542 17469 caffe.cpp:312] Batch 878, accuracy/top5 = 0.7
I0630 19:09:23.426545 17469 caffe.cpp:312] Batch 878, loss = 1.78
I0630 19:09:23.475275 17469 caffe.cpp:312] Batch 879, accuracy/top1 = 0.58
I0630 19:09:23.475296 17469 caffe.cpp:312] Batch 879, accuracy/top5 = 0.88
I0630 19:09:23.475298 17469 caffe.cpp:312] Batch 879, loss = 1.16
I0630 19:09:23.522624 17469 caffe.cpp:312] Batch 880, accuracy/top1 = 0.66
I0630 19:09:23.522647 17469 caffe.cpp:312] Batch 880, accuracy/top5 = 0.84
I0630 19:09:23.522650 17469 caffe.cpp:312] Batch 880, loss = 1.36
I0630 19:09:23.570740 17469 caffe.cpp:312] Batch 881, accuracy/top1 = 0.54
I0630 19:09:23.570761 17469 caffe.cpp:312] Batch 881, accuracy/top5 = 0.78
I0630 19:09:23.570765 17469 caffe.cpp:312] Batch 881, loss = 1.6
I0630 19:09:23.618703 17469 caffe.cpp:312] Batch 882, accuracy/top1 = 0.5
I0630 19:09:23.618726 17469 caffe.cpp:312] Batch 882, accuracy/top5 = 0.8
I0630 19:09:23.618729 17469 caffe.cpp:312] Batch 882, loss = 1.42
I0630 19:09:23.666723 17469 caffe.cpp:312] Batch 883, accuracy/top1 = 0.56
I0630 19:09:23.666745 17469 caffe.cpp:312] Batch 883, accuracy/top5 = 0.78
I0630 19:09:23.666749 17469 caffe.cpp:312] Batch 883, loss = 1.46
I0630 19:09:23.715065 17469 caffe.cpp:312] Batch 884, accuracy/top1 = 0.46
I0630 19:09:23.715086 17469 caffe.cpp:312] Batch 884, accuracy/top5 = 0.74
I0630 19:09:23.715090 17469 caffe.cpp:312] Batch 884, loss = 1.74
I0630 19:09:23.763622 17469 caffe.cpp:312] Batch 885, accuracy/top1 = 0.54
I0630 19:09:23.763643 17469 caffe.cpp:312] Batch 885, accuracy/top5 = 0.82
I0630 19:09:23.763645 17469 caffe.cpp:312] Batch 885, loss = 1.42
I0630 19:09:23.810809 17469 caffe.cpp:312] Batch 886, accuracy/top1 = 0.6
I0630 19:09:23.810832 17469 caffe.cpp:312] Batch 886, accuracy/top5 = 0.8
I0630 19:09:23.810837 17469 caffe.cpp:312] Batch 886, loss = 1.62
I0630 19:09:23.858831 17469 caffe.cpp:312] Batch 887, accuracy/top1 = 0.5
I0630 19:09:23.858875 17469 caffe.cpp:312] Batch 887, accuracy/top5 = 0.76
I0630 19:09:23.858877 17469 caffe.cpp:312] Batch 887, loss = 1.68
I0630 19:09:23.907904 17469 caffe.cpp:312] Batch 888, accuracy/top1 = 0.52
I0630 19:09:23.907927 17469 caffe.cpp:312] Batch 888, accuracy/top5 = 0.76
I0630 19:09:23.907929 17469 caffe.cpp:312] Batch 888, loss = 1.62
I0630 19:09:23.956212 17469 caffe.cpp:312] Batch 889, accuracy/top1 = 0.46
I0630 19:09:23.956233 17469 caffe.cpp:312] Batch 889, accuracy/top5 = 0.82
I0630 19:09:23.956236 17469 caffe.cpp:312] Batch 889, loss = 1.72
I0630 19:09:24.004551 17469 caffe.cpp:312] Batch 890, accuracy/top1 = 0.56
I0630 19:09:24.004576 17469 caffe.cpp:312] Batch 890, accuracy/top5 = 0.74
I0630 19:09:24.004580 17469 caffe.cpp:312] Batch 890, loss = 1.74
I0630 19:09:24.053915 17469 caffe.cpp:312] Batch 891, accuracy/top1 = 0.62
I0630 19:09:24.053937 17469 caffe.cpp:312] Batch 891, accuracy/top5 = 0.82
I0630 19:09:24.053941 17469 caffe.cpp:312] Batch 891, loss = 1.4
I0630 19:09:24.102110 17469 caffe.cpp:312] Batch 892, accuracy/top1 = 0.5
I0630 19:09:24.102133 17469 caffe.cpp:312] Batch 892, accuracy/top5 = 0.78
I0630 19:09:24.102150 17469 caffe.cpp:312] Batch 892, loss = 1.68
I0630 19:09:24.151324 17469 caffe.cpp:312] Batch 893, accuracy/top1 = 0.5
I0630 19:09:24.151345 17469 caffe.cpp:312] Batch 893, accuracy/top5 = 0.84
I0630 19:09:24.151350 17469 caffe.cpp:312] Batch 893, loss = 1.54
I0630 19:09:24.200057 17469 caffe.cpp:312] Batch 894, accuracy/top1 = 0.5
I0630 19:09:24.200079 17469 caffe.cpp:312] Batch 894, accuracy/top5 = 0.74
I0630 19:09:24.200083 17469 caffe.cpp:312] Batch 894, loss = 1.8
I0630 19:09:24.249436 17469 caffe.cpp:312] Batch 895, accuracy/top1 = 0.72
I0630 19:09:24.249459 17469 caffe.cpp:312] Batch 895, accuracy/top5 = 0.88
I0630 19:09:24.249461 17469 caffe.cpp:312] Batch 895, loss = 0.78
I0630 19:09:24.298069 17469 caffe.cpp:312] Batch 896, accuracy/top1 = 0.52
I0630 19:09:24.298090 17469 caffe.cpp:312] Batch 896, accuracy/top5 = 0.74
I0630 19:09:24.298094 17469 caffe.cpp:312] Batch 896, loss = 1.68
I0630 19:09:24.346015 17469 caffe.cpp:312] Batch 897, accuracy/top1 = 0.54
I0630 19:09:24.346037 17469 caffe.cpp:312] Batch 897, accuracy/top5 = 0.82
I0630 19:09:24.346040 17469 caffe.cpp:312] Batch 897, loss = 1.4
I0630 19:09:24.394234 17469 caffe.cpp:312] Batch 898, accuracy/top1 = 0.58
I0630 19:09:24.394258 17469 caffe.cpp:312] Batch 898, accuracy/top5 = 0.88
I0630 19:09:24.394260 17469 caffe.cpp:312] Batch 898, loss = 1.12
I0630 19:09:24.442203 17469 caffe.cpp:312] Batch 899, accuracy/top1 = 0.62
I0630 19:09:24.442224 17469 caffe.cpp:312] Batch 899, accuracy/top5 = 0.84
I0630 19:09:24.442229 17469 caffe.cpp:312] Batch 899, loss = 1.36
I0630 19:09:24.490722 17469 caffe.cpp:312] Batch 900, accuracy/top1 = 0.7
I0630 19:09:24.490746 17469 caffe.cpp:312] Batch 900, accuracy/top5 = 0.9
I0630 19:09:24.490749 17469 caffe.cpp:312] Batch 900, loss = 0.88
I0630 19:09:24.539387 17469 caffe.cpp:312] Batch 901, accuracy/top1 = 0.54
I0630 19:09:24.539408 17469 caffe.cpp:312] Batch 901, accuracy/top5 = 0.72
I0630 19:09:24.539412 17469 caffe.cpp:312] Batch 901, loss = 1.48
I0630 19:09:24.587425 17469 caffe.cpp:312] Batch 902, accuracy/top1 = 0.56
I0630 19:09:24.587448 17469 caffe.cpp:312] Batch 902, accuracy/top5 = 0.78
I0630 19:09:24.587452 17469 caffe.cpp:312] Batch 902, loss = 1.44
I0630 19:09:24.635921 17469 caffe.cpp:312] Batch 903, accuracy/top1 = 0.56
I0630 19:09:24.635941 17469 caffe.cpp:312] Batch 903, accuracy/top5 = 0.82
I0630 19:09:24.635946 17469 caffe.cpp:312] Batch 903, loss = 1.3
I0630 19:09:24.684525 17469 caffe.cpp:312] Batch 904, accuracy/top1 = 0.66
I0630 19:09:24.684548 17469 caffe.cpp:312] Batch 904, accuracy/top5 = 0.86
I0630 19:09:24.684551 17469 caffe.cpp:312] Batch 904, loss = 1.4
I0630 19:09:24.733055 17469 caffe.cpp:312] Batch 905, accuracy/top1 = 0.6
I0630 19:09:24.733079 17469 caffe.cpp:312] Batch 905, accuracy/top5 = 0.82
I0630 19:09:24.733083 17469 caffe.cpp:312] Batch 905, loss = 1.42
I0630 19:09:24.780874 17469 caffe.cpp:312] Batch 906, accuracy/top1 = 0.7
I0630 19:09:24.780897 17469 caffe.cpp:312] Batch 906, accuracy/top5 = 0.82
I0630 19:09:24.780900 17469 caffe.cpp:312] Batch 906, loss = 1.18
I0630 19:09:24.828672 17469 caffe.cpp:312] Batch 907, accuracy/top1 = 0.66
I0630 19:09:24.828694 17469 caffe.cpp:312] Batch 907, accuracy/top5 = 0.8
I0630 19:09:24.828697 17469 caffe.cpp:312] Batch 907, loss = 1.46
I0630 19:09:24.877840 17469 caffe.cpp:312] Batch 908, accuracy/top1 = 0.54
I0630 19:09:24.877861 17469 caffe.cpp:312] Batch 908, accuracy/top5 = 0.86
I0630 19:09:24.877863 17469 caffe.cpp:312] Batch 908, loss = 1.4
I0630 19:09:24.927364 17469 caffe.cpp:312] Batch 909, accuracy/top1 = 0.64
I0630 19:09:24.927386 17469 caffe.cpp:312] Batch 909, accuracy/top5 = 0.88
I0630 19:09:24.927389 17469 caffe.cpp:312] Batch 909, loss = 1.04
I0630 19:09:24.974592 17469 caffe.cpp:312] Batch 910, accuracy/top1 = 0.48
I0630 19:09:24.974616 17469 caffe.cpp:312] Batch 910, accuracy/top5 = 0.72
I0630 19:09:24.974618 17469 caffe.cpp:312] Batch 910, loss = 1.78
I0630 19:09:25.022989 17469 caffe.cpp:312] Batch 911, accuracy/top1 = 0.58
I0630 19:09:25.023010 17469 caffe.cpp:312] Batch 911, accuracy/top5 = 0.78
I0630 19:09:25.023025 17469 caffe.cpp:312] Batch 911, loss = 1.54
I0630 19:09:25.071239 17469 caffe.cpp:312] Batch 912, accuracy/top1 = 0.54
I0630 19:09:25.071261 17469 caffe.cpp:312] Batch 912, accuracy/top5 = 0.8
I0630 19:09:25.071265 17469 caffe.cpp:312] Batch 912, loss = 1.5
I0630 19:09:25.119657 17469 caffe.cpp:312] Batch 913, accuracy/top1 = 0.54
I0630 19:09:25.119676 17469 caffe.cpp:312] Batch 913, accuracy/top5 = 0.86
I0630 19:09:25.119679 17469 caffe.cpp:312] Batch 913, loss = 1.58
I0630 19:09:25.168754 17469 caffe.cpp:312] Batch 914, accuracy/top1 = 0.54
I0630 19:09:25.168777 17469 caffe.cpp:312] Batch 914, accuracy/top5 = 0.74
I0630 19:09:25.168781 17469 caffe.cpp:312] Batch 914, loss = 2
I0630 19:09:25.216279 17469 caffe.cpp:312] Batch 915, accuracy/top1 = 0.64
I0630 19:09:25.216300 17469 caffe.cpp:312] Batch 915, accuracy/top5 = 0.8
I0630 19:09:25.216303 17469 caffe.cpp:312] Batch 915, loss = 1.56
I0630 19:09:25.264511 17469 caffe.cpp:312] Batch 916, accuracy/top1 = 0.7
I0630 19:09:25.264534 17469 caffe.cpp:312] Batch 916, accuracy/top5 = 0.86
I0630 19:09:25.264538 17469 caffe.cpp:312] Batch 916, loss = 1.2
I0630 19:09:25.313561 17469 caffe.cpp:312] Batch 917, accuracy/top1 = 0.56
I0630 19:09:25.313583 17469 caffe.cpp:312] Batch 917, accuracy/top5 = 0.84
I0630 19:09:25.313586 17469 caffe.cpp:312] Batch 917, loss = 1.44
I0630 19:09:25.362658 17469 caffe.cpp:312] Batch 918, accuracy/top1 = 0.58
I0630 19:09:25.362680 17469 caffe.cpp:312] Batch 918, accuracy/top5 = 0.8
I0630 19:09:25.362684 17469 caffe.cpp:312] Batch 918, loss = 1.6
I0630 19:09:25.411267 17469 caffe.cpp:312] Batch 919, accuracy/top1 = 0.58
I0630 19:09:25.411288 17469 caffe.cpp:312] Batch 919, accuracy/top5 = 0.74
I0630 19:09:25.411293 17469 caffe.cpp:312] Batch 919, loss = 1.74
I0630 19:09:25.459945 17469 caffe.cpp:312] Batch 920, accuracy/top1 = 0.56
I0630 19:09:25.459967 17469 caffe.cpp:312] Batch 920, accuracy/top5 = 0.76
I0630 19:09:25.459971 17469 caffe.cpp:312] Batch 920, loss = 1.5
I0630 19:09:25.509526 17469 caffe.cpp:312] Batch 921, accuracy/top1 = 0.48
I0630 19:09:25.509548 17469 caffe.cpp:312] Batch 921, accuracy/top5 = 0.78
I0630 19:09:25.509552 17469 caffe.cpp:312] Batch 921, loss = 1.62
I0630 19:09:25.558166 17469 caffe.cpp:312] Batch 922, accuracy/top1 = 0.6
I0630 19:09:25.558187 17469 caffe.cpp:312] Batch 922, accuracy/top5 = 0.9
I0630 19:09:25.558190 17469 caffe.cpp:312] Batch 922, loss = 0.94
I0630 19:09:25.606988 17469 caffe.cpp:312] Batch 923, accuracy/top1 = 0.62
I0630 19:09:25.607009 17469 caffe.cpp:312] Batch 923, accuracy/top5 = 0.8
I0630 19:09:25.607012 17469 caffe.cpp:312] Batch 923, loss = 1.62
I0630 19:09:25.656700 17469 caffe.cpp:312] Batch 924, accuracy/top1 = 0.58
I0630 19:09:25.656723 17469 caffe.cpp:312] Batch 924, accuracy/top5 = 0.86
I0630 19:09:25.656725 17469 caffe.cpp:312] Batch 924, loss = 1.18
I0630 19:09:25.706418 17469 caffe.cpp:312] Batch 925, accuracy/top1 = 0.56
I0630 19:09:25.706439 17469 caffe.cpp:312] Batch 925, accuracy/top5 = 0.78
I0630 19:09:25.706442 17469 caffe.cpp:312] Batch 925, loss = 1.6
I0630 19:09:25.755336 17469 caffe.cpp:312] Batch 926, accuracy/top1 = 0.6
I0630 19:09:25.755359 17469 caffe.cpp:312] Batch 926, accuracy/top5 = 0.8
I0630 19:09:25.755362 17469 caffe.cpp:312] Batch 926, loss = 1.68
I0630 19:09:25.803437 17469 caffe.cpp:312] Batch 927, accuracy/top1 = 0.54
I0630 19:09:25.803458 17469 caffe.cpp:312] Batch 927, accuracy/top5 = 0.72
I0630 19:09:25.803462 17469 caffe.cpp:312] Batch 927, loss = 1.84
I0630 19:09:25.851442 17469 caffe.cpp:312] Batch 928, accuracy/top1 = 0.6
I0630 19:09:25.851466 17469 caffe.cpp:312] Batch 928, accuracy/top5 = 0.78
I0630 19:09:25.851469 17469 caffe.cpp:312] Batch 928, loss = 1.22
I0630 19:09:25.900367 17469 caffe.cpp:312] Batch 929, accuracy/top1 = 0.52
I0630 19:09:25.900387 17469 caffe.cpp:312] Batch 929, accuracy/top5 = 0.82
I0630 19:09:25.900389 17469 caffe.cpp:312] Batch 929, loss = 1.36
I0630 19:09:25.949460 17469 caffe.cpp:312] Batch 930, accuracy/top1 = 0.6
I0630 19:09:25.949482 17469 caffe.cpp:312] Batch 930, accuracy/top5 = 0.86
I0630 19:09:25.949499 17469 caffe.cpp:312] Batch 930, loss = 1.18
I0630 19:09:25.998908 17469 caffe.cpp:312] Batch 931, accuracy/top1 = 0.52
I0630 19:09:25.998930 17469 caffe.cpp:312] Batch 931, accuracy/top5 = 0.78
I0630 19:09:25.998934 17469 caffe.cpp:312] Batch 931, loss = 1.66
I0630 19:09:26.048076 17469 caffe.cpp:312] Batch 932, accuracy/top1 = 0.64
I0630 19:09:26.048100 17469 caffe.cpp:312] Batch 932, accuracy/top5 = 0.84
I0630 19:09:26.048104 17469 caffe.cpp:312] Batch 932, loss = 1.2
I0630 19:09:26.097411 17469 caffe.cpp:312] Batch 933, accuracy/top1 = 0.56
I0630 19:09:26.097434 17469 caffe.cpp:312] Batch 933, accuracy/top5 = 0.82
I0630 19:09:26.097439 17469 caffe.cpp:312] Batch 933, loss = 1.82
I0630 19:09:26.146210 17469 caffe.cpp:312] Batch 934, accuracy/top1 = 0.62
I0630 19:09:26.146234 17469 caffe.cpp:312] Batch 934, accuracy/top5 = 0.78
I0630 19:09:26.146237 17469 caffe.cpp:312] Batch 934, loss = 1.56
I0630 19:09:26.193581 17469 caffe.cpp:312] Batch 935, accuracy/top1 = 0.54
I0630 19:09:26.193603 17469 caffe.cpp:312] Batch 935, accuracy/top5 = 0.74
I0630 19:09:26.193608 17469 caffe.cpp:312] Batch 935, loss = 2.08
I0630 19:09:26.240990 17469 caffe.cpp:312] Batch 936, accuracy/top1 = 0.68
I0630 19:09:26.241014 17469 caffe.cpp:312] Batch 936, accuracy/top5 = 0.82
I0630 19:09:26.241016 17469 caffe.cpp:312] Batch 936, loss = 1.22
I0630 19:09:26.289741 17469 caffe.cpp:312] Batch 937, accuracy/top1 = 0.58
I0630 19:09:26.289763 17469 caffe.cpp:312] Batch 937, accuracy/top5 = 0.8
I0630 19:09:26.289767 17469 caffe.cpp:312] Batch 937, loss = 1.74
I0630 19:09:26.337347 17469 caffe.cpp:312] Batch 938, accuracy/top1 = 0.54
I0630 19:09:26.337370 17469 caffe.cpp:312] Batch 938, accuracy/top5 = 0.82
I0630 19:09:26.337373 17469 caffe.cpp:312] Batch 938, loss = 1.56
I0630 19:09:26.385005 17469 caffe.cpp:312] Batch 939, accuracy/top1 = 0.56
I0630 19:09:26.385031 17469 caffe.cpp:312] Batch 939, accuracy/top5 = 0.8
I0630 19:09:26.385037 17469 caffe.cpp:312] Batch 939, loss = 1.62
I0630 19:09:26.432577 17469 caffe.cpp:312] Batch 940, accuracy/top1 = 0.48
I0630 19:09:26.432598 17469 caffe.cpp:312] Batch 940, accuracy/top5 = 0.78
I0630 19:09:26.432601 17469 caffe.cpp:312] Batch 940, loss = 1.6
I0630 19:09:26.480957 17469 caffe.cpp:312] Batch 941, accuracy/top1 = 0.64
I0630 19:09:26.480980 17469 caffe.cpp:312] Batch 941, accuracy/top5 = 0.78
I0630 19:09:26.480983 17469 caffe.cpp:312] Batch 941, loss = 1.72
I0630 19:09:26.529559 17469 caffe.cpp:312] Batch 942, accuracy/top1 = 0.54
I0630 19:09:26.529580 17469 caffe.cpp:312] Batch 942, accuracy/top5 = 0.76
I0630 19:09:26.529584 17469 caffe.cpp:312] Batch 942, loss = 1.98
I0630 19:09:26.579664 17469 caffe.cpp:312] Batch 943, accuracy/top1 = 0.64
I0630 19:09:26.579686 17469 caffe.cpp:312] Batch 943, accuracy/top5 = 0.8
I0630 19:09:26.579689 17469 caffe.cpp:312] Batch 943, loss = 1.28
I0630 19:09:26.627975 17469 caffe.cpp:312] Batch 944, accuracy/top1 = 0.6
I0630 19:09:26.627997 17469 caffe.cpp:312] Batch 944, accuracy/top5 = 0.8
I0630 19:09:26.628001 17469 caffe.cpp:312] Batch 944, loss = 1.34
I0630 19:09:26.680651 17469 caffe.cpp:312] Batch 945, accuracy/top1 = 0.54
I0630 19:09:26.680673 17469 caffe.cpp:312] Batch 945, accuracy/top5 = 0.76
I0630 19:09:26.680676 17469 caffe.cpp:312] Batch 945, loss = 1.92
I0630 19:09:26.729257 17469 caffe.cpp:312] Batch 946, accuracy/top1 = 0.56
I0630 19:09:26.729280 17469 caffe.cpp:312] Batch 946, accuracy/top5 = 0.72
I0630 19:09:26.729284 17469 caffe.cpp:312] Batch 946, loss = 1.92
I0630 19:09:26.782762 17469 caffe.cpp:312] Batch 947, accuracy/top1 = 0.74
I0630 19:09:26.782783 17469 caffe.cpp:312] Batch 947, accuracy/top5 = 0.9
I0630 19:09:26.782786 17469 caffe.cpp:312] Batch 947, loss = 0.78
I0630 19:09:26.831799 17469 caffe.cpp:312] Batch 948, accuracy/top1 = 0.56
I0630 19:09:26.831822 17469 caffe.cpp:312] Batch 948, accuracy/top5 = 0.78
I0630 19:09:26.831826 17469 caffe.cpp:312] Batch 948, loss = 1.84
I0630 19:09:26.883391 17469 caffe.cpp:312] Batch 949, accuracy/top1 = 0.68
I0630 19:09:26.883412 17469 caffe.cpp:312] Batch 949, accuracy/top5 = 0.88
I0630 19:09:26.883427 17469 caffe.cpp:312] Batch 949, loss = 0.92
I0630 19:09:26.933533 17469 caffe.cpp:312] Batch 950, accuracy/top1 = 0.58
I0630 19:09:26.933554 17469 caffe.cpp:312] Batch 950, accuracy/top5 = 0.76
I0630 19:09:26.933558 17469 caffe.cpp:312] Batch 950, loss = 1.84
I0630 19:09:26.983248 17469 caffe.cpp:312] Batch 951, accuracy/top1 = 0.68
I0630 19:09:26.983270 17469 caffe.cpp:312] Batch 951, accuracy/top5 = 0.86
I0630 19:09:26.983273 17469 caffe.cpp:312] Batch 951, loss = 0.98
I0630 19:09:27.034809 17469 caffe.cpp:312] Batch 952, accuracy/top1 = 0.58
I0630 19:09:27.034832 17469 caffe.cpp:312] Batch 952, accuracy/top5 = 0.8
I0630 19:09:27.034837 17469 caffe.cpp:312] Batch 952, loss = 1.8
I0630 19:09:27.083487 17469 caffe.cpp:312] Batch 953, accuracy/top1 = 0.54
I0630 19:09:27.083508 17469 caffe.cpp:312] Batch 953, accuracy/top5 = 0.82
I0630 19:09:27.083510 17469 caffe.cpp:312] Batch 953, loss = 1.58
I0630 19:09:27.135969 17469 caffe.cpp:312] Batch 954, accuracy/top1 = 0.48
I0630 19:09:27.135989 17469 caffe.cpp:312] Batch 954, accuracy/top5 = 0.7
I0630 19:09:27.135993 17469 caffe.cpp:312] Batch 954, loss = 2.08
I0630 19:09:27.185012 17469 caffe.cpp:312] Batch 955, accuracy/top1 = 0.54
I0630 19:09:27.185035 17469 caffe.cpp:312] Batch 955, accuracy/top5 = 0.72
I0630 19:09:27.185039 17469 caffe.cpp:312] Batch 955, loss = 1.7
I0630 19:09:27.237321 17469 caffe.cpp:312] Batch 956, accuracy/top1 = 0.56
I0630 19:09:27.237344 17469 caffe.cpp:312] Batch 956, accuracy/top5 = 0.88
I0630 19:09:27.237346 17469 caffe.cpp:312] Batch 956, loss = 1.48
I0630 19:09:27.286478 17469 caffe.cpp:312] Batch 957, accuracy/top1 = 0.5
I0630 19:09:27.286500 17469 caffe.cpp:312] Batch 957, accuracy/top5 = 0.74
I0630 19:09:27.286504 17469 caffe.cpp:312] Batch 957, loss = 1.66
I0630 19:09:27.338814 17469 caffe.cpp:312] Batch 958, accuracy/top1 = 0.58
I0630 19:09:27.338837 17469 caffe.cpp:312] Batch 958, accuracy/top5 = 0.86
I0630 19:09:27.338840 17469 caffe.cpp:312] Batch 958, loss = 1.56
I0630 19:09:27.387229 17469 caffe.cpp:312] Batch 959, accuracy/top1 = 0.46
I0630 19:09:27.387251 17469 caffe.cpp:312] Batch 959, accuracy/top5 = 0.72
I0630 19:09:27.387254 17469 caffe.cpp:312] Batch 959, loss = 2
I0630 19:09:27.438661 17469 caffe.cpp:312] Batch 960, accuracy/top1 = 0.66
I0630 19:09:27.438683 17469 caffe.cpp:312] Batch 960, accuracy/top5 = 0.82
I0630 19:09:27.438685 17469 caffe.cpp:312] Batch 960, loss = 1.28
I0630 19:09:27.486609 17469 caffe.cpp:312] Batch 961, accuracy/top1 = 0.5
I0630 19:09:27.486632 17469 caffe.cpp:312] Batch 961, accuracy/top5 = 0.74
I0630 19:09:27.486636 17469 caffe.cpp:312] Batch 961, loss = 1.98
I0630 19:09:27.539211 17469 caffe.cpp:312] Batch 962, accuracy/top1 = 0.56
I0630 19:09:27.539232 17469 caffe.cpp:312] Batch 962, accuracy/top5 = 0.8
I0630 19:09:27.539235 17469 caffe.cpp:312] Batch 962, loss = 1.62
I0630 19:09:27.587553 17469 caffe.cpp:312] Batch 963, accuracy/top1 = 0.46
I0630 19:09:27.587576 17469 caffe.cpp:312] Batch 963, accuracy/top5 = 0.74
I0630 19:09:27.587579 17469 caffe.cpp:312] Batch 963, loss = 1.82
I0630 19:09:27.639566 17469 caffe.cpp:312] Batch 964, accuracy/top1 = 0.6
I0630 19:09:27.639588 17469 caffe.cpp:312] Batch 964, accuracy/top5 = 0.8
I0630 19:09:27.639591 17469 caffe.cpp:312] Batch 964, loss = 1.36
I0630 19:09:27.687772 17469 caffe.cpp:312] Batch 965, accuracy/top1 = 0.52
I0630 19:09:27.687795 17469 caffe.cpp:312] Batch 965, accuracy/top5 = 0.78
I0630 19:09:27.687799 17469 caffe.cpp:312] Batch 965, loss = 1.66
I0630 19:09:27.740067 17469 caffe.cpp:312] Batch 966, accuracy/top1 = 0.62
I0630 19:09:27.740087 17469 caffe.cpp:312] Batch 966, accuracy/top5 = 0.84
I0630 19:09:27.740092 17469 caffe.cpp:312] Batch 966, loss = 1.62
I0630 19:09:27.789387 17469 caffe.cpp:312] Batch 967, accuracy/top1 = 0.4
I0630 19:09:27.789409 17469 caffe.cpp:312] Batch 967, accuracy/top5 = 0.7
I0630 19:09:27.789412 17469 caffe.cpp:312] Batch 967, loss = 1.82
I0630 19:09:27.841840 17469 caffe.cpp:312] Batch 968, accuracy/top1 = 0.46
I0630 19:09:27.841861 17469 caffe.cpp:312] Batch 968, accuracy/top5 = 0.78
I0630 19:09:27.841863 17469 caffe.cpp:312] Batch 968, loss = 1.8
I0630 19:09:27.891096 17469 caffe.cpp:312] Batch 969, accuracy/top1 = 0.46
I0630 19:09:27.891118 17469 caffe.cpp:312] Batch 969, accuracy/top5 = 0.82
I0630 19:09:27.891120 17469 caffe.cpp:312] Batch 969, loss = 1.54
I0630 19:09:27.943310 17469 caffe.cpp:312] Batch 970, accuracy/top1 = 0.58
I0630 19:09:27.943332 17469 caffe.cpp:312] Batch 970, accuracy/top5 = 0.84
I0630 19:09:27.943336 17469 caffe.cpp:312] Batch 970, loss = 1.36
I0630 19:09:27.991722 17469 caffe.cpp:312] Batch 971, accuracy/top1 = 0.56
I0630 19:09:27.991746 17469 caffe.cpp:312] Batch 971, accuracy/top5 = 0.8
I0630 19:09:27.991750 17469 caffe.cpp:312] Batch 971, loss = 1.38
I0630 19:09:28.044513 17469 caffe.cpp:312] Batch 972, accuracy/top1 = 0.66
I0630 19:09:28.044534 17469 caffe.cpp:312] Batch 972, accuracy/top5 = 0.84
I0630 19:09:28.044538 17469 caffe.cpp:312] Batch 972, loss = 1.34
I0630 19:09:28.094259 17469 caffe.cpp:312] Batch 973, accuracy/top1 = 0.46
I0630 19:09:28.094282 17469 caffe.cpp:312] Batch 973, accuracy/top5 = 0.68
I0630 19:09:28.094285 17469 caffe.cpp:312] Batch 973, loss = 2.46
I0630 19:09:28.145520 17469 caffe.cpp:312] Batch 974, accuracy/top1 = 0.46
I0630 19:09:28.145541 17469 caffe.cpp:312] Batch 974, accuracy/top5 = 0.78
I0630 19:09:28.145545 17469 caffe.cpp:312] Batch 974, loss = 1.44
I0630 19:09:28.193802 17469 caffe.cpp:312] Batch 975, accuracy/top1 = 0.62
I0630 19:09:28.193825 17469 caffe.cpp:312] Batch 975, accuracy/top5 = 0.76
I0630 19:09:28.193830 17469 caffe.cpp:312] Batch 975, loss = 1.5
I0630 19:09:28.245883 17469 caffe.cpp:312] Batch 976, accuracy/top1 = 0.58
I0630 19:09:28.245906 17469 caffe.cpp:312] Batch 976, accuracy/top5 = 0.82
I0630 19:09:28.245909 17469 caffe.cpp:312] Batch 976, loss = 1.4
I0630 19:09:28.294421 17469 caffe.cpp:312] Batch 977, accuracy/top1 = 0.48
I0630 19:09:28.294443 17469 caffe.cpp:312] Batch 977, accuracy/top5 = 0.82
I0630 19:09:28.294447 17469 caffe.cpp:312] Batch 977, loss = 1.6
I0630 19:09:28.345654 17469 caffe.cpp:312] Batch 978, accuracy/top1 = 0.54
I0630 19:09:28.345675 17469 caffe.cpp:312] Batch 978, accuracy/top5 = 0.78
I0630 19:09:28.345679 17469 caffe.cpp:312] Batch 978, loss = 1.66
I0630 19:09:28.394500 17469 caffe.cpp:312] Batch 979, accuracy/top1 = 0.6
I0630 19:09:28.394525 17469 caffe.cpp:312] Batch 979, accuracy/top5 = 0.86
I0630 19:09:28.394527 17469 caffe.cpp:312] Batch 979, loss = 1.48
I0630 19:09:28.445619 17469 caffe.cpp:312] Batch 980, accuracy/top1 = 0.44
I0630 19:09:28.445641 17469 caffe.cpp:312] Batch 980, accuracy/top5 = 0.7
I0630 19:09:28.445644 17469 caffe.cpp:312] Batch 980, loss = 1.86
I0630 19:09:28.494112 17469 caffe.cpp:312] Batch 981, accuracy/top1 = 0.54
I0630 19:09:28.494134 17469 caffe.cpp:312] Batch 981, accuracy/top5 = 0.8
I0630 19:09:28.494138 17469 caffe.cpp:312] Batch 981, loss = 1.66
I0630 19:09:28.546560 17469 caffe.cpp:312] Batch 982, accuracy/top1 = 0.46
I0630 19:09:28.546582 17469 caffe.cpp:312] Batch 982, accuracy/top5 = 0.74
I0630 19:09:28.546586 17469 caffe.cpp:312] Batch 982, loss = 2.42
I0630 19:09:28.594633 17469 caffe.cpp:312] Batch 983, accuracy/top1 = 0.66
I0630 19:09:28.594657 17469 caffe.cpp:312] Batch 983, accuracy/top5 = 0.82
I0630 19:09:28.594661 17469 caffe.cpp:312] Batch 983, loss = 1.9
I0630 19:09:28.643190 17469 caffe.cpp:312] Batch 984, accuracy/top1 = 0.62
I0630 19:09:28.643210 17469 caffe.cpp:312] Batch 984, accuracy/top5 = 0.78
I0630 19:09:28.643213 17469 caffe.cpp:312] Batch 984, loss = 1.28
I0630 19:09:28.690929 17469 caffe.cpp:312] Batch 985, accuracy/top1 = 0.44
I0630 19:09:28.690951 17469 caffe.cpp:312] Batch 985, accuracy/top5 = 0.8
I0630 19:09:28.690955 17469 caffe.cpp:312] Batch 985, loss = 1.74
I0630 19:09:28.739519 17469 caffe.cpp:312] Batch 986, accuracy/top1 = 0.56
I0630 19:09:28.739542 17469 caffe.cpp:312] Batch 986, accuracy/top5 = 0.76
I0630 19:09:28.739544 17469 caffe.cpp:312] Batch 986, loss = 1.76
I0630 19:09:28.787828 17469 caffe.cpp:312] Batch 987, accuracy/top1 = 0.48
I0630 19:09:28.787852 17469 caffe.cpp:312] Batch 987, accuracy/top5 = 0.74
I0630 19:09:28.787854 17469 caffe.cpp:312] Batch 987, loss = 1.68
I0630 19:09:28.835995 17469 caffe.cpp:312] Batch 988, accuracy/top1 = 0.6
I0630 19:09:28.836016 17469 caffe.cpp:312] Batch 988, accuracy/top5 = 0.74
I0630 19:09:28.836019 17469 caffe.cpp:312] Batch 988, loss = 1.46
I0630 19:09:28.885941 17469 caffe.cpp:312] Batch 989, accuracy/top1 = 0.44
I0630 19:09:28.885959 17469 caffe.cpp:312] Batch 989, accuracy/top5 = 0.72
I0630 19:09:28.885962 17469 caffe.cpp:312] Batch 989, loss = 2.1
I0630 19:09:28.935091 17469 caffe.cpp:312] Batch 990, accuracy/top1 = 0.62
I0630 19:09:28.935112 17469 caffe.cpp:312] Batch 990, accuracy/top5 = 0.84
I0630 19:09:28.935117 17469 caffe.cpp:312] Batch 990, loss = 1.1
I0630 19:09:28.983956 17469 caffe.cpp:312] Batch 991, accuracy/top1 = 0.64
I0630 19:09:28.983979 17469 caffe.cpp:312] Batch 991, accuracy/top5 = 0.86
I0630 19:09:28.983983 17469 caffe.cpp:312] Batch 991, loss = 1.4
I0630 19:09:29.034328 17469 caffe.cpp:312] Batch 992, accuracy/top1 = 0.66
I0630 19:09:29.034351 17469 caffe.cpp:312] Batch 992, accuracy/top5 = 0.84
I0630 19:09:29.034354 17469 caffe.cpp:312] Batch 992, loss = 1.18
I0630 19:09:29.083211 17469 caffe.cpp:312] Batch 993, accuracy/top1 = 0.64
I0630 19:09:29.083235 17469 caffe.cpp:312] Batch 993, accuracy/top5 = 0.86
I0630 19:09:29.083237 17469 caffe.cpp:312] Batch 993, loss = 1.18
I0630 19:09:29.131474 17469 caffe.cpp:312] Batch 994, accuracy/top1 = 0.68
I0630 19:09:29.131496 17469 caffe.cpp:312] Batch 994, accuracy/top5 = 0.84
I0630 19:09:29.131500 17469 caffe.cpp:312] Batch 994, loss = 1.32
I0630 19:09:29.179965 17469 caffe.cpp:312] Batch 995, accuracy/top1 = 0.52
I0630 19:09:29.179986 17469 caffe.cpp:312] Batch 995, accuracy/top5 = 0.78
I0630 19:09:29.179991 17469 caffe.cpp:312] Batch 995, loss = 1.52
I0630 19:09:29.228454 17469 caffe.cpp:312] Batch 996, accuracy/top1 = 0.54
I0630 19:09:29.228474 17469 caffe.cpp:312] Batch 996, accuracy/top5 = 0.82
I0630 19:09:29.228478 17469 caffe.cpp:312] Batch 996, loss = 1.4
I0630 19:09:29.276906 17469 caffe.cpp:312] Batch 997, accuracy/top1 = 0.56
I0630 19:09:29.276927 17469 caffe.cpp:312] Batch 997, accuracy/top5 = 0.84
I0630 19:09:29.276931 17469 caffe.cpp:312] Batch 997, loss = 1.44
I0630 19:09:29.324440 17469 caffe.cpp:312] Batch 998, accuracy/top1 = 0.44
I0630 19:09:29.324462 17469 caffe.cpp:312] Batch 998, accuracy/top5 = 0.82
I0630 19:09:29.324465 17469 caffe.cpp:312] Batch 998, loss = 1.62
I0630 19:09:29.372684 17469 caffe.cpp:312] Batch 999, accuracy/top1 = 0.74
I0630 19:09:29.372706 17469 caffe.cpp:312] Batch 999, accuracy/top5 = 0.86
I0630 19:09:29.372709 17469 caffe.cpp:312] Batch 999, loss = 1.16
I0630 19:09:29.372711 17469 caffe.cpp:317] Loss: 1.54764
I0630 19:09:29.372717 17469 caffe.cpp:329] accuracy/top1 = 0.562939
I0630 19:09:29.372721 17469 caffe.cpp:329] accuracy/top5 = 0.796562
I0630 19:09:29.372725 17469 caffe.cpp:329] loss = 1.54764 (* 1 = 1.54764 loss)
