I0629 14:05:54.989639 15179 caffe.cpp:608] This is NVCaffe 0.16.2 started at Thu Jun 29 14:05:54 2017
I0629 14:05:54.989766 15179 caffe.cpp:611] CuDNN version: 6.0.21
I0629 14:05:54.989770 15179 caffe.cpp:612] CuBLAS version: 8000
I0629 14:05:54.989773 15179 caffe.cpp:613] CUDA version: 8000
I0629 14:05:54.989774 15179 caffe.cpp:614] CUDA driver version: 8000
I0629 14:05:54.989779 15179 caffe.cpp:263] Not using GPU #1 for single-GPU function
I0629 14:05:54.990298 15179 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0629 14:05:54.990787 15179 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8277393408, dev_info[0]: total=8506769408 free=8277393408
I0629 14:05:54.990792 15179 caffe.cpp:275] Use GPU with device ID 0
I0629 14:05:54.991070 15179 caffe.cpp:279] GPU device name: GeForce GTX 1080
I0629 14:05:54.992336 15179 net.cpp:77] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0629 14:05:54.992453 15179 net.cpp:108] Using FLOAT as default forward math type
I0629 14:05:54.992460 15179 net.cpp:114] Using FLOAT as default backward math type
I0629 14:05:54.992465 15179 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0629 14:05:54.992468 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:54.992517 15179 net.cpp:183] Created Layer data (0)
I0629 14:05:54.992524 15179 net.cpp:529] data -> data
I0629 14:05:54.992535 15179 net.cpp:529] data -> label
I0629 14:05:54.992557 15179 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 50
I0629 14:05:54.992854 15179 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 14:05:55.038408 15193 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0629 14:05:55.042590 15179 data_layer.cpp:188] ReshapePrefetch 50, 3, 32, 32
I0629 14:05:55.042628 15179 data_layer.cpp:206] Output data size: 50, 3, 32, 32
I0629 14:05:55.042632 15179 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 14:05:55.042656 15179 net.cpp:244] Setting up data
I0629 14:05:55.042667 15179 net.cpp:251] TEST Top shape for layer 0 'data' 50 3 32 32 (153600)
I0629 14:05:55.042673 15179 net.cpp:251] TEST Top shape for layer 0 'data' 50 (50)
I0629 14:05:55.042680 15179 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0629 14:05:55.042683 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.042695 15179 net.cpp:183] Created Layer label_data_1_split (1)
I0629 14:05:55.042699 15179 net.cpp:560] label_data_1_split <- label
I0629 14:05:55.042706 15179 net.cpp:529] label_data_1_split -> label_data_1_split_0
I0629 14:05:55.042711 15179 net.cpp:529] label_data_1_split -> label_data_1_split_1
I0629 14:05:55.042714 15179 net.cpp:529] label_data_1_split -> label_data_1_split_2
I0629 14:05:55.042747 15179 net.cpp:244] Setting up label_data_1_split
I0629 14:05:55.042762 15179 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0629 14:05:55.042765 15179 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0629 14:05:55.042768 15179 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0629 14:05:55.042771 15179 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0629 14:05:55.042774 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.042784 15179 net.cpp:183] Created Layer data/bias (2)
I0629 14:05:55.042788 15179 net.cpp:560] data/bias <- data
I0629 14:05:55.042790 15179 net.cpp:529] data/bias -> data/bias
I0629 14:05:55.043846 15194 data_layer.cpp:188] ReshapePrefetch 50, 3, 32, 32
I0629 14:05:55.043858 15194 data_layer.cpp:206] Output data size: 50, 3, 32, 32
I0629 14:05:55.044900 15194 data_layer.cpp:110] [0] Parser threads: 1
I0629 14:05:55.044909 15194 data_layer.cpp:112] [0] Transformer threads: 1
I0629 14:05:55.044911 15179 net.cpp:244] Setting up data/bias
I0629 14:05:55.044921 15179 net.cpp:251] TEST Top shape for layer 2 'data/bias' 50 3 32 32 (153600)
I0629 14:05:55.044931 15179 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0629 14:05:55.044934 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.044950 15179 net.cpp:183] Created Layer conv1a (3)
I0629 14:05:55.044953 15179 net.cpp:560] conv1a <- data/bias
I0629 14:05:55.044956 15179 net.cpp:529] conv1a -> conv1a
I0629 14:05:55.338217 15179 net.cpp:244] Setting up conv1a
I0629 14:05:55.338238 15179 net.cpp:251] TEST Top shape for layer 3 'conv1a' 50 32 32 32 (1638400)
I0629 14:05:55.338249 15179 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0629 14:05:55.338254 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.338265 15179 net.cpp:183] Created Layer conv1a/bn (4)
I0629 14:05:55.338269 15179 net.cpp:560] conv1a/bn <- conv1a
I0629 14:05:55.338273 15179 net.cpp:512] conv1a/bn -> conv1a (in-place)
I0629 14:05:55.339105 15179 net.cpp:244] Setting up conv1a/bn
I0629 14:05:55.339114 15179 net.cpp:251] TEST Top shape for layer 4 'conv1a/bn' 50 32 32 32 (1638400)
I0629 14:05:55.339121 15179 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0629 14:05:55.339124 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.339128 15179 net.cpp:183] Created Layer conv1a/relu (5)
I0629 14:05:55.339130 15179 net.cpp:560] conv1a/relu <- conv1a
I0629 14:05:55.339133 15179 net.cpp:512] conv1a/relu -> conv1a (in-place)
I0629 14:05:55.339144 15179 net.cpp:244] Setting up conv1a/relu
I0629 14:05:55.339148 15179 net.cpp:251] TEST Top shape for layer 5 'conv1a/relu' 50 32 32 32 (1638400)
I0629 14:05:55.339150 15179 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0629 14:05:55.339154 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.339161 15179 net.cpp:183] Created Layer conv1b (6)
I0629 14:05:55.339164 15179 net.cpp:560] conv1b <- conv1a
I0629 14:05:55.339165 15179 net.cpp:529] conv1b -> conv1b
I0629 14:05:55.342173 15179 net.cpp:244] Setting up conv1b
I0629 14:05:55.342182 15179 net.cpp:251] TEST Top shape for layer 6 'conv1b' 50 32 32 32 (1638400)
I0629 14:05:55.342188 15179 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0629 14:05:55.342191 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.342198 15179 net.cpp:183] Created Layer conv1b/bn (7)
I0629 14:05:55.342201 15179 net.cpp:560] conv1b/bn <- conv1b
I0629 14:05:55.342203 15179 net.cpp:512] conv1b/bn -> conv1b (in-place)
I0629 14:05:55.342988 15179 net.cpp:244] Setting up conv1b/bn
I0629 14:05:55.342996 15179 net.cpp:251] TEST Top shape for layer 7 'conv1b/bn' 50 32 32 32 (1638400)
I0629 14:05:55.343003 15179 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0629 14:05:55.343014 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.343019 15179 net.cpp:183] Created Layer conv1b/relu (8)
I0629 14:05:55.343020 15179 net.cpp:560] conv1b/relu <- conv1b
I0629 14:05:55.343024 15179 net.cpp:512] conv1b/relu -> conv1b (in-place)
I0629 14:05:55.343027 15179 net.cpp:244] Setting up conv1b/relu
I0629 14:05:55.343030 15179 net.cpp:251] TEST Top shape for layer 8 'conv1b/relu' 50 32 32 32 (1638400)
I0629 14:05:55.343032 15179 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0629 14:05:55.343034 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.343040 15179 net.cpp:183] Created Layer pool1 (9)
I0629 14:05:55.343044 15179 net.cpp:560] pool1 <- conv1b
I0629 14:05:55.343045 15179 net.cpp:529] pool1 -> pool1
I0629 14:05:55.343087 15179 net.cpp:244] Setting up pool1
I0629 14:05:55.343091 15179 net.cpp:251] TEST Top shape for layer 9 'pool1' 50 32 32 32 (1638400)
I0629 14:05:55.343093 15179 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0629 14:05:55.343096 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.343101 15179 net.cpp:183] Created Layer res2a_branch2a (10)
I0629 14:05:55.343103 15179 net.cpp:560] res2a_branch2a <- pool1
I0629 14:05:55.343106 15179 net.cpp:529] res2a_branch2a -> res2a_branch2a
I0629 14:05:55.348008 15179 net.cpp:244] Setting up res2a_branch2a
I0629 14:05:55.348019 15179 net.cpp:251] TEST Top shape for layer 10 'res2a_branch2a' 50 64 32 32 (3276800)
I0629 14:05:55.348026 15179 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0629 14:05:55.348028 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.348033 15179 net.cpp:183] Created Layer res2a_branch2a/bn (11)
I0629 14:05:55.348036 15179 net.cpp:560] res2a_branch2a/bn <- res2a_branch2a
I0629 14:05:55.348038 15179 net.cpp:512] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0629 14:05:55.348865 15179 net.cpp:244] Setting up res2a_branch2a/bn
I0629 14:05:55.348875 15179 net.cpp:251] TEST Top shape for layer 11 'res2a_branch2a/bn' 50 64 32 32 (3276800)
I0629 14:05:55.348881 15179 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0629 14:05:55.348882 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.348886 15179 net.cpp:183] Created Layer res2a_branch2a/relu (12)
I0629 14:05:55.348887 15179 net.cpp:560] res2a_branch2a/relu <- res2a_branch2a
I0629 14:05:55.348891 15179 net.cpp:512] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0629 14:05:55.348893 15179 net.cpp:244] Setting up res2a_branch2a/relu
I0629 14:05:55.348896 15179 net.cpp:251] TEST Top shape for layer 12 'res2a_branch2a/relu' 50 64 32 32 (3276800)
I0629 14:05:55.348898 15179 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0629 14:05:55.348901 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.348906 15179 net.cpp:183] Created Layer res2a_branch2b (13)
I0629 14:05:55.348907 15179 net.cpp:560] res2a_branch2b <- res2a_branch2a
I0629 14:05:55.348912 15179 net.cpp:529] res2a_branch2b -> res2a_branch2b
I0629 14:05:55.351924 15179 net.cpp:244] Setting up res2a_branch2b
I0629 14:05:55.351935 15179 net.cpp:251] TEST Top shape for layer 13 'res2a_branch2b' 50 64 32 32 (3276800)
I0629 14:05:55.351940 15179 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0629 14:05:55.351943 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.351948 15179 net.cpp:183] Created Layer res2a_branch2b/bn (14)
I0629 14:05:55.351950 15179 net.cpp:560] res2a_branch2b/bn <- res2a_branch2b
I0629 14:05:55.351953 15179 net.cpp:512] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0629 14:05:55.352797 15179 net.cpp:244] Setting up res2a_branch2b/bn
I0629 14:05:55.352807 15179 net.cpp:251] TEST Top shape for layer 14 'res2a_branch2b/bn' 50 64 32 32 (3276800)
I0629 14:05:55.352813 15179 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0629 14:05:55.352815 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.352818 15179 net.cpp:183] Created Layer res2a_branch2b/relu (15)
I0629 14:05:55.352821 15179 net.cpp:560] res2a_branch2b/relu <- res2a_branch2b
I0629 14:05:55.352823 15179 net.cpp:512] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0629 14:05:55.352828 15179 net.cpp:244] Setting up res2a_branch2b/relu
I0629 14:05:55.352829 15179 net.cpp:251] TEST Top shape for layer 15 'res2a_branch2b/relu' 50 64 32 32 (3276800)
I0629 14:05:55.352831 15179 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0629 14:05:55.352833 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.352838 15179 net.cpp:183] Created Layer pool2 (16)
I0629 14:05:55.352840 15179 net.cpp:560] pool2 <- res2a_branch2b
I0629 14:05:55.352843 15179 net.cpp:529] pool2 -> pool2
I0629 14:05:55.352876 15179 net.cpp:244] Setting up pool2
I0629 14:05:55.352880 15179 net.cpp:251] TEST Top shape for layer 16 'pool2' 50 64 16 16 (819200)
I0629 14:05:55.352882 15179 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0629 14:05:55.352885 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.352890 15179 net.cpp:183] Created Layer res3a_branch2a (17)
I0629 14:05:55.352892 15179 net.cpp:560] res3a_branch2a <- pool2
I0629 14:05:55.352895 15179 net.cpp:529] res3a_branch2a -> res3a_branch2a
I0629 14:05:55.358002 15179 net.cpp:244] Setting up res3a_branch2a
I0629 14:05:55.358012 15179 net.cpp:251] TEST Top shape for layer 17 'res3a_branch2a' 50 128 16 16 (1638400)
I0629 14:05:55.358017 15179 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0629 14:05:55.358019 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.358024 15179 net.cpp:183] Created Layer res3a_branch2a/bn (18)
I0629 14:05:55.358026 15179 net.cpp:560] res3a_branch2a/bn <- res3a_branch2a
I0629 14:05:55.358029 15179 net.cpp:512] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0629 14:05:55.358814 15179 net.cpp:244] Setting up res3a_branch2a/bn
I0629 14:05:55.358821 15179 net.cpp:251] TEST Top shape for layer 18 'res3a_branch2a/bn' 50 128 16 16 (1638400)
I0629 14:05:55.358829 15179 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0629 14:05:55.358831 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.358835 15179 net.cpp:183] Created Layer res3a_branch2a/relu (19)
I0629 14:05:55.358837 15179 net.cpp:560] res3a_branch2a/relu <- res3a_branch2a
I0629 14:05:55.358839 15179 net.cpp:512] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0629 14:05:55.358844 15179 net.cpp:244] Setting up res3a_branch2a/relu
I0629 14:05:55.358846 15179 net.cpp:251] TEST Top shape for layer 19 'res3a_branch2a/relu' 50 128 16 16 (1638400)
I0629 14:05:55.358849 15179 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0629 14:05:55.358850 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.358857 15179 net.cpp:183] Created Layer res3a_branch2b (20)
I0629 14:05:55.358860 15179 net.cpp:560] res3a_branch2b <- res3a_branch2a
I0629 14:05:55.358862 15179 net.cpp:529] res3a_branch2b -> res3a_branch2b
I0629 14:05:55.361106 15179 net.cpp:244] Setting up res3a_branch2b
I0629 14:05:55.361115 15179 net.cpp:251] TEST Top shape for layer 20 'res3a_branch2b' 50 128 16 16 (1638400)
I0629 14:05:55.361120 15179 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0629 14:05:55.361124 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.361135 15179 net.cpp:183] Created Layer res3a_branch2b/bn (21)
I0629 14:05:55.361138 15179 net.cpp:560] res3a_branch2b/bn <- res3a_branch2b
I0629 14:05:55.361141 15179 net.cpp:512] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0629 14:05:55.361925 15179 net.cpp:244] Setting up res3a_branch2b/bn
I0629 14:05:55.361934 15179 net.cpp:251] TEST Top shape for layer 21 'res3a_branch2b/bn' 50 128 16 16 (1638400)
I0629 14:05:55.361940 15179 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0629 14:05:55.361943 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.361945 15179 net.cpp:183] Created Layer res3a_branch2b/relu (22)
I0629 14:05:55.361948 15179 net.cpp:560] res3a_branch2b/relu <- res3a_branch2b
I0629 14:05:55.361950 15179 net.cpp:512] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0629 14:05:55.361954 15179 net.cpp:244] Setting up res3a_branch2b/relu
I0629 14:05:55.361956 15179 net.cpp:251] TEST Top shape for layer 22 'res3a_branch2b/relu' 50 128 16 16 (1638400)
I0629 14:05:55.361958 15179 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0629 14:05:55.361960 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.361963 15179 net.cpp:183] Created Layer pool3 (23)
I0629 14:05:55.361965 15179 net.cpp:560] pool3 <- res3a_branch2b
I0629 14:05:55.361968 15179 net.cpp:529] pool3 -> pool3
I0629 14:05:55.362002 15179 net.cpp:244] Setting up pool3
I0629 14:05:55.362006 15179 net.cpp:251] TEST Top shape for layer 23 'pool3' 50 128 16 16 (1638400)
I0629 14:05:55.362009 15179 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0629 14:05:55.362010 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.362016 15179 net.cpp:183] Created Layer res4a_branch2a (24)
I0629 14:05:55.362018 15179 net.cpp:560] res4a_branch2a <- pool3
I0629 14:05:55.362021 15179 net.cpp:529] res4a_branch2a -> res4a_branch2a
I0629 14:05:55.375603 15179 net.cpp:244] Setting up res4a_branch2a
I0629 14:05:55.375623 15179 net.cpp:251] TEST Top shape for layer 24 'res4a_branch2a' 50 256 16 16 (3276800)
I0629 14:05:55.375630 15179 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0629 14:05:55.375634 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.375641 15179 net.cpp:183] Created Layer res4a_branch2a/bn (25)
I0629 14:05:55.375644 15179 net.cpp:560] res4a_branch2a/bn <- res4a_branch2a
I0629 14:05:55.375648 15179 net.cpp:512] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0629 14:05:55.376619 15179 net.cpp:244] Setting up res4a_branch2a/bn
I0629 14:05:55.376629 15179 net.cpp:251] TEST Top shape for layer 25 'res4a_branch2a/bn' 50 256 16 16 (3276800)
I0629 14:05:55.376636 15179 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0629 14:05:55.376638 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.376641 15179 net.cpp:183] Created Layer res4a_branch2a/relu (26)
I0629 14:05:55.376643 15179 net.cpp:560] res4a_branch2a/relu <- res4a_branch2a
I0629 14:05:55.376646 15179 net.cpp:512] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0629 14:05:55.376651 15179 net.cpp:244] Setting up res4a_branch2a/relu
I0629 14:05:55.376652 15179 net.cpp:251] TEST Top shape for layer 26 'res4a_branch2a/relu' 50 256 16 16 (3276800)
I0629 14:05:55.376654 15179 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0629 14:05:55.376657 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.376663 15179 net.cpp:183] Created Layer res4a_branch2b (27)
I0629 14:05:55.376667 15179 net.cpp:560] res4a_branch2b <- res4a_branch2a
I0629 14:05:55.376668 15179 net.cpp:529] res4a_branch2b -> res4a_branch2b
I0629 14:05:55.382719 15179 net.cpp:244] Setting up res4a_branch2b
I0629 14:05:55.382740 15179 net.cpp:251] TEST Top shape for layer 27 'res4a_branch2b' 50 256 16 16 (3276800)
I0629 14:05:55.382745 15179 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0629 14:05:55.382748 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.382752 15179 net.cpp:183] Created Layer res4a_branch2b/bn (28)
I0629 14:05:55.382755 15179 net.cpp:560] res4a_branch2b/bn <- res4a_branch2b
I0629 14:05:55.382757 15179 net.cpp:512] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0629 14:05:55.383599 15179 net.cpp:244] Setting up res4a_branch2b/bn
I0629 14:05:55.383608 15179 net.cpp:251] TEST Top shape for layer 28 'res4a_branch2b/bn' 50 256 16 16 (3276800)
I0629 14:05:55.383615 15179 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0629 14:05:55.383617 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.383620 15179 net.cpp:183] Created Layer res4a_branch2b/relu (29)
I0629 14:05:55.383622 15179 net.cpp:560] res4a_branch2b/relu <- res4a_branch2b
I0629 14:05:55.383625 15179 net.cpp:512] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0629 14:05:55.383630 15179 net.cpp:244] Setting up res4a_branch2b/relu
I0629 14:05:55.383631 15179 net.cpp:251] TEST Top shape for layer 29 'res4a_branch2b/relu' 50 256 16 16 (3276800)
I0629 14:05:55.383633 15179 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0629 14:05:55.383635 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.383640 15179 net.cpp:183] Created Layer pool4 (30)
I0629 14:05:55.383641 15179 net.cpp:560] pool4 <- res4a_branch2b
I0629 14:05:55.383644 15179 net.cpp:529] pool4 -> pool4
I0629 14:05:55.383678 15179 net.cpp:244] Setting up pool4
I0629 14:05:55.383682 15179 net.cpp:251] TEST Top shape for layer 30 'pool4' 50 256 8 8 (819200)
I0629 14:05:55.383684 15179 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0629 14:05:55.383687 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.383693 15179 net.cpp:183] Created Layer res5a_branch2a (31)
I0629 14:05:55.383697 15179 net.cpp:560] res5a_branch2a <- pool4
I0629 14:05:55.383698 15179 net.cpp:529] res5a_branch2a -> res5a_branch2a
I0629 14:05:55.420037 15179 net.cpp:244] Setting up res5a_branch2a
I0629 14:05:55.420058 15179 net.cpp:251] TEST Top shape for layer 31 'res5a_branch2a' 50 512 8 8 (1638400)
I0629 14:05:55.420063 15179 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0629 14:05:55.420068 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.420075 15179 net.cpp:183] Created Layer res5a_branch2a/bn (32)
I0629 14:05:55.420079 15179 net.cpp:560] res5a_branch2a/bn <- res5a_branch2a
I0629 14:05:55.420083 15179 net.cpp:512] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0629 14:05:55.420928 15179 net.cpp:244] Setting up res5a_branch2a/bn
I0629 14:05:55.420936 15179 net.cpp:251] TEST Top shape for layer 32 'res5a_branch2a/bn' 50 512 8 8 (1638400)
I0629 14:05:55.420943 15179 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0629 14:05:55.420945 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.420948 15179 net.cpp:183] Created Layer res5a_branch2a/relu (33)
I0629 14:05:55.420951 15179 net.cpp:560] res5a_branch2a/relu <- res5a_branch2a
I0629 14:05:55.420953 15179 net.cpp:512] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0629 14:05:55.420958 15179 net.cpp:244] Setting up res5a_branch2a/relu
I0629 14:05:55.420960 15179 net.cpp:251] TEST Top shape for layer 33 'res5a_branch2a/relu' 50 512 8 8 (1638400)
I0629 14:05:55.420963 15179 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0629 14:05:55.420964 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.420986 15179 net.cpp:183] Created Layer res5a_branch2b (34)
I0629 14:05:55.420989 15179 net.cpp:560] res5a_branch2b <- res5a_branch2a
I0629 14:05:55.420992 15179 net.cpp:529] res5a_branch2b -> res5a_branch2b
I0629 14:05:55.438122 15179 net.cpp:244] Setting up res5a_branch2b
I0629 14:05:55.438144 15179 net.cpp:251] TEST Top shape for layer 34 'res5a_branch2b' 50 512 8 8 (1638400)
I0629 14:05:55.438158 15179 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0629 14:05:55.438163 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.438172 15179 net.cpp:183] Created Layer res5a_branch2b/bn (35)
I0629 14:05:55.438177 15179 net.cpp:560] res5a_branch2b/bn <- res5a_branch2b
I0629 14:05:55.438182 15179 net.cpp:512] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0629 14:05:55.439018 15179 net.cpp:244] Setting up res5a_branch2b/bn
I0629 14:05:55.439025 15179 net.cpp:251] TEST Top shape for layer 35 'res5a_branch2b/bn' 50 512 8 8 (1638400)
I0629 14:05:55.439031 15179 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0629 14:05:55.439034 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.439038 15179 net.cpp:183] Created Layer res5a_branch2b/relu (36)
I0629 14:05:55.439040 15179 net.cpp:560] res5a_branch2b/relu <- res5a_branch2b
I0629 14:05:55.439043 15179 net.cpp:512] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0629 14:05:55.439046 15179 net.cpp:244] Setting up res5a_branch2b/relu
I0629 14:05:55.439049 15179 net.cpp:251] TEST Top shape for layer 36 'res5a_branch2b/relu' 50 512 8 8 (1638400)
I0629 14:05:55.439051 15179 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0629 14:05:55.439054 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.439057 15179 net.cpp:183] Created Layer pool5 (37)
I0629 14:05:55.439060 15179 net.cpp:560] pool5 <- res5a_branch2b
I0629 14:05:55.439064 15179 net.cpp:529] pool5 -> pool5
I0629 14:05:55.439082 15179 net.cpp:244] Setting up pool5
I0629 14:05:55.439086 15179 net.cpp:251] TEST Top shape for layer 37 'pool5' 50 512 1 1 (25600)
I0629 14:05:55.439088 15179 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0629 14:05:55.439090 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.439096 15179 net.cpp:183] Created Layer fc10 (38)
I0629 14:05:55.439098 15179 net.cpp:560] fc10 <- pool5
I0629 14:05:55.439100 15179 net.cpp:529] fc10 -> fc10
I0629 14:05:55.439301 15179 net.cpp:244] Setting up fc10
I0629 14:05:55.439307 15179 net.cpp:251] TEST Top shape for layer 38 'fc10' 50 10 (500)
I0629 14:05:55.439311 15179 layer_factory.hpp:136] Creating layer 'fc10_fc10_0_split' of type 'Split'
I0629 14:05:55.439313 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.439316 15179 net.cpp:183] Created Layer fc10_fc10_0_split (39)
I0629 14:05:55.439318 15179 net.cpp:560] fc10_fc10_0_split <- fc10
I0629 14:05:55.439321 15179 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0629 14:05:55.439323 15179 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0629 14:05:55.439327 15179 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0629 14:05:55.439354 15179 net.cpp:244] Setting up fc10_fc10_0_split
I0629 14:05:55.439358 15179 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0629 14:05:55.439360 15179 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0629 14:05:55.439363 15179 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0629 14:05:55.439364 15179 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0629 14:05:55.439366 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.439374 15179 net.cpp:183] Created Layer loss (40)
I0629 14:05:55.439376 15179 net.cpp:560] loss <- fc10_fc10_0_split_0
I0629 14:05:55.439391 15179 net.cpp:560] loss <- label_data_1_split_0
I0629 14:05:55.439394 15179 net.cpp:529] loss -> loss
I0629 14:05:55.439499 15179 net.cpp:244] Setting up loss
I0629 14:05:55.439505 15179 net.cpp:251] TEST Top shape for layer 40 'loss' (1)
I0629 14:05:55.439507 15179 net.cpp:255]     with loss weight 1
I0629 14:05:55.439518 15179 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0629 14:05:55.439522 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.439529 15179 net.cpp:183] Created Layer accuracy/top1 (41)
I0629 14:05:55.439532 15179 net.cpp:560] accuracy/top1 <- fc10_fc10_0_split_1
I0629 14:05:55.439535 15179 net.cpp:560] accuracy/top1 <- label_data_1_split_1
I0629 14:05:55.439538 15179 net.cpp:529] accuracy/top1 -> accuracy/top1
I0629 14:05:55.439546 15179 net.cpp:244] Setting up accuracy/top1
I0629 14:05:55.439550 15179 net.cpp:251] TEST Top shape for layer 41 'accuracy/top1' (1)
I0629 14:05:55.439553 15179 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0629 14:05:55.439554 15179 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 14:05:55.439560 15179 net.cpp:183] Created Layer accuracy/top5 (42)
I0629 14:05:55.439563 15179 net.cpp:560] accuracy/top5 <- fc10_fc10_0_split_2
I0629 14:05:55.439565 15179 net.cpp:560] accuracy/top5 <- label_data_1_split_2
I0629 14:05:55.439568 15179 net.cpp:529] accuracy/top5 -> accuracy/top5
I0629 14:05:55.439571 15179 net.cpp:244] Setting up accuracy/top5
I0629 14:05:55.439574 15179 net.cpp:251] TEST Top shape for layer 42 'accuracy/top5' (1)
I0629 14:05:55.439576 15179 net.cpp:324] accuracy/top5 does not need backward computation.
I0629 14:05:55.439579 15179 net.cpp:324] accuracy/top1 does not need backward computation.
I0629 14:05:55.439581 15179 net.cpp:322] loss needs backward computation.
I0629 14:05:55.439584 15179 net.cpp:322] fc10_fc10_0_split needs backward computation.
I0629 14:05:55.439585 15179 net.cpp:322] fc10 needs backward computation.
I0629 14:05:55.439586 15179 net.cpp:322] pool5 needs backward computation.
I0629 14:05:55.439589 15179 net.cpp:322] res5a_branch2b/relu needs backward computation.
I0629 14:05:55.439590 15179 net.cpp:322] res5a_branch2b/bn needs backward computation.
I0629 14:05:55.439592 15179 net.cpp:322] res5a_branch2b needs backward computation.
I0629 14:05:55.439595 15179 net.cpp:322] res5a_branch2a/relu needs backward computation.
I0629 14:05:55.439599 15179 net.cpp:322] res5a_branch2a/bn needs backward computation.
I0629 14:05:55.439602 15179 net.cpp:322] res5a_branch2a needs backward computation.
I0629 14:05:55.439605 15179 net.cpp:322] pool4 needs backward computation.
I0629 14:05:55.439609 15179 net.cpp:322] res4a_branch2b/relu needs backward computation.
I0629 14:05:55.439611 15179 net.cpp:322] res4a_branch2b/bn needs backward computation.
I0629 14:05:55.439615 15179 net.cpp:322] res4a_branch2b needs backward computation.
I0629 14:05:55.439620 15179 net.cpp:322] res4a_branch2a/relu needs backward computation.
I0629 14:05:55.439621 15179 net.cpp:322] res4a_branch2a/bn needs backward computation.
I0629 14:05:55.439625 15179 net.cpp:322] res4a_branch2a needs backward computation.
I0629 14:05:55.439627 15179 net.cpp:322] pool3 needs backward computation.
I0629 14:05:55.439631 15179 net.cpp:322] res3a_branch2b/relu needs backward computation.
I0629 14:05:55.439633 15179 net.cpp:322] res3a_branch2b/bn needs backward computation.
I0629 14:05:55.439635 15179 net.cpp:322] res3a_branch2b needs backward computation.
I0629 14:05:55.439637 15179 net.cpp:322] res3a_branch2a/relu needs backward computation.
I0629 14:05:55.439640 15179 net.cpp:322] res3a_branch2a/bn needs backward computation.
I0629 14:05:55.439643 15179 net.cpp:322] res3a_branch2a needs backward computation.
I0629 14:05:55.439646 15179 net.cpp:322] pool2 needs backward computation.
I0629 14:05:55.439649 15179 net.cpp:322] res2a_branch2b/relu needs backward computation.
I0629 14:05:55.439652 15179 net.cpp:322] res2a_branch2b/bn needs backward computation.
I0629 14:05:55.439661 15179 net.cpp:322] res2a_branch2b needs backward computation.
I0629 14:05:55.439663 15179 net.cpp:322] res2a_branch2a/relu needs backward computation.
I0629 14:05:55.439667 15179 net.cpp:322] res2a_branch2a/bn needs backward computation.
I0629 14:05:55.439669 15179 net.cpp:322] res2a_branch2a needs backward computation.
I0629 14:05:55.439673 15179 net.cpp:322] pool1 needs backward computation.
I0629 14:05:55.439677 15179 net.cpp:322] conv1b/relu needs backward computation.
I0629 14:05:55.439680 15179 net.cpp:322] conv1b/bn needs backward computation.
I0629 14:05:55.439682 15179 net.cpp:322] conv1b needs backward computation.
I0629 14:05:55.439687 15179 net.cpp:322] conv1a/relu needs backward computation.
I0629 14:05:55.439688 15179 net.cpp:322] conv1a/bn needs backward computation.
I0629 14:05:55.439692 15179 net.cpp:322] conv1a needs backward computation.
I0629 14:05:55.439695 15179 net.cpp:324] data/bias does not need backward computation.
I0629 14:05:55.439699 15179 net.cpp:324] label_data_1_split does not need backward computation.
I0629 14:05:55.439704 15179 net.cpp:324] data does not need backward computation.
I0629 14:05:55.439707 15179 net.cpp:366] This network produces output accuracy/top1
I0629 14:05:55.439710 15179 net.cpp:366] This network produces output accuracy/top5
I0629 14:05:55.439713 15179 net.cpp:366] This network produces output loss
I0629 14:05:55.439744 15179 net.cpp:388] Top memory (TEST) required for data: 275251200 diff: 183500808
I0629 14:05:55.439748 15179 net.cpp:391] Bottom memory (TEST) required for data: 275251200 diff: 275251200
I0629 14:05:55.439749 15179 net.cpp:394] Shared (in-place) memory (TEST) by data: 183500800 diff: 183500800
I0629 14:05:55.439751 15179 net.cpp:397] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0629 14:05:55.439754 15179 net.cpp:400] Parameters shared memory (TEST) by data: 0 diff: 0
I0629 14:05:55.439757 15179 net.cpp:406] Network initialization done.
I0629 14:05:55.512706 15179 net.cpp:1087] Copying source layer data Type:Data #blobs=0
I0629 14:05:55.512760 15179 net.cpp:1087] Copying source layer data/bias Type:Bias #blobs=1
I0629 14:05:55.512840 15179 net.cpp:1087] Copying source layer conv1a Type:Convolution #blobs=2
I0629 14:05:55.512902 15179 net.cpp:1087] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0629 14:05:55.513427 15179 net.cpp:1087] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0629 14:05:55.513453 15179 net.cpp:1087] Copying source layer conv1b Type:Convolution #blobs=2
I0629 14:05:55.513489 15179 net.cpp:1087] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0629 14:05:55.513931 15179 net.cpp:1087] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0629 14:05:55.513957 15179 net.cpp:1087] Copying source layer pool1 Type:Pooling #blobs=0
I0629 14:05:55.513967 15179 net.cpp:1087] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0629 14:05:55.514017 15179 net.cpp:1087] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0629 14:05:55.514402 15179 net.cpp:1087] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0629 14:05:55.514425 15179 net.cpp:1087] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0629 14:05:55.514467 15179 net.cpp:1087] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0629 14:05:55.514830 15179 net.cpp:1087] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0629 14:05:55.514853 15179 net.cpp:1087] Copying source layer pool2 Type:Pooling #blobs=0
I0629 14:05:55.514863 15179 net.cpp:1087] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0629 14:05:55.514960 15179 net.cpp:1087] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0629 14:05:55.515283 15179 net.cpp:1087] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0629 14:05:55.515303 15179 net.cpp:1087] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0629 14:05:55.515368 15179 net.cpp:1087] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0629 14:05:55.515745 15179 net.cpp:1087] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0629 14:05:55.515769 15179 net.cpp:1087] Copying source layer pool3 Type:Pooling #blobs=0
I0629 14:05:55.515780 15179 net.cpp:1087] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0629 14:05:55.516077 15179 net.cpp:1087] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0629 14:05:55.516429 15179 net.cpp:1087] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0629 14:05:55.516451 15179 net.cpp:1087] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0629 14:05:55.516618 15179 net.cpp:1087] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0629 14:05:55.516950 15179 net.cpp:1087] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0629 14:05:55.516973 15179 net.cpp:1087] Copying source layer pool4 Type:Pooling #blobs=0
I0629 14:05:55.516983 15179 net.cpp:1087] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0629 14:05:55.518025 15179 net.cpp:1087] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0629 14:05:55.518213 15179 net.cpp:1087] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0629 14:05:55.518216 15179 net.cpp:1087] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0629 14:05:55.518380 15179 net.cpp:1087] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0629 14:05:55.518461 15179 net.cpp:1087] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0629 14:05:55.518465 15179 net.cpp:1087] Copying source layer pool5 Type:Pooling #blobs=0
I0629 14:05:55.518466 15179 net.cpp:1087] Copying source layer fc10 Type:InnerProduct #blobs=2
I0629 14:05:55.518476 15179 net.cpp:1087] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0629 14:05:55.518517 15179 caffe.cpp:290] Running for 200 iterations.
I0629 14:05:55.568866 15179 caffe.cpp:313] Batch 0, accuracy/top1 = 0.94
I0629 14:05:55.568887 15179 caffe.cpp:313] Batch 0, accuracy/top5 = 1
I0629 14:05:55.568892 15179 caffe.cpp:313] Batch 0, loss = 0.153593
I0629 14:05:55.610738 15179 caffe.cpp:313] Batch 1, accuracy/top1 = 0.94
I0629 14:05:55.610757 15179 caffe.cpp:313] Batch 1, accuracy/top5 = 1
I0629 14:05:55.610760 15179 caffe.cpp:313] Batch 1, loss = 0.174427
I0629 14:05:55.620101 15179 caffe.cpp:313] Batch 2, accuracy/top1 = 0.96
I0629 14:05:55.620110 15179 caffe.cpp:313] Batch 2, accuracy/top5 = 1
I0629 14:05:55.620112 15179 caffe.cpp:313] Batch 2, loss = 0.186237
I0629 14:05:55.629480 15179 caffe.cpp:313] Batch 3, accuracy/top1 = 0.92
I0629 14:05:55.629489 15179 caffe.cpp:313] Batch 3, accuracy/top5 = 1
I0629 14:05:55.629492 15179 caffe.cpp:313] Batch 3, loss = 0.335301
I0629 14:05:55.638900 15179 caffe.cpp:313] Batch 4, accuracy/top1 = 0.94
I0629 14:05:55.638914 15179 caffe.cpp:313] Batch 4, accuracy/top5 = 1
I0629 14:05:55.638918 15179 caffe.cpp:313] Batch 4, loss = 0.219656
I0629 14:05:55.648586 15179 caffe.cpp:313] Batch 5, accuracy/top1 = 0.86
I0629 14:05:55.648603 15179 caffe.cpp:313] Batch 5, accuracy/top5 = 1
I0629 14:05:55.648609 15179 caffe.cpp:313] Batch 5, loss = 0.337282
I0629 14:05:55.657981 15179 caffe.cpp:313] Batch 6, accuracy/top1 = 0.9
I0629 14:05:55.657990 15179 caffe.cpp:313] Batch 6, accuracy/top5 = 1
I0629 14:05:55.657994 15179 caffe.cpp:313] Batch 6, loss = 0.18802
I0629 14:05:55.667291 15179 caffe.cpp:313] Batch 7, accuracy/top1 = 0.88
I0629 14:05:55.667299 15179 caffe.cpp:313] Batch 7, accuracy/top5 = 0.98
I0629 14:05:55.667301 15179 caffe.cpp:313] Batch 7, loss = 0.513992
I0629 14:05:55.676977 15179 caffe.cpp:313] Batch 8, accuracy/top1 = 0.9
I0629 14:05:55.676998 15179 caffe.cpp:313] Batch 8, accuracy/top5 = 1
I0629 14:05:55.677002 15179 caffe.cpp:313] Batch 8, loss = 0.217647
I0629 14:05:55.686403 15179 caffe.cpp:313] Batch 9, accuracy/top1 = 0.92
I0629 14:05:55.686414 15179 caffe.cpp:313] Batch 9, accuracy/top5 = 1
I0629 14:05:55.686417 15179 caffe.cpp:313] Batch 9, loss = 0.352723
I0629 14:05:55.695765 15179 caffe.cpp:313] Batch 10, accuracy/top1 = 0.96
I0629 14:05:55.695782 15179 caffe.cpp:313] Batch 10, accuracy/top5 = 1
I0629 14:05:55.695785 15179 caffe.cpp:313] Batch 10, loss = 0.155797
I0629 14:05:55.696086 15194 blocking_queue.cpp:40] Waiting for datum
I0629 14:05:55.705329 15179 caffe.cpp:313] Batch 11, accuracy/top1 = 0.96
I0629 14:05:55.705346 15179 caffe.cpp:313] Batch 11, accuracy/top5 = 1
I0629 14:05:55.705351 15179 caffe.cpp:313] Batch 11, loss = 0.170298
I0629 14:05:55.714689 15179 caffe.cpp:313] Batch 12, accuracy/top1 = 0.98
I0629 14:05:55.714699 15179 caffe.cpp:313] Batch 12, accuracy/top5 = 1
I0629 14:05:55.714702 15179 caffe.cpp:313] Batch 12, loss = 0.133915
I0629 14:05:55.723978 15179 caffe.cpp:313] Batch 13, accuracy/top1 = 0.9
I0629 14:05:55.723986 15179 caffe.cpp:313] Batch 13, accuracy/top5 = 1
I0629 14:05:55.723989 15179 caffe.cpp:313] Batch 13, loss = 0.407794
I0629 14:05:55.733103 15179 caffe.cpp:313] Batch 14, accuracy/top1 = 0.88
I0629 14:05:55.733119 15179 caffe.cpp:313] Batch 14, accuracy/top5 = 1
I0629 14:05:55.733124 15179 caffe.cpp:313] Batch 14, loss = 0.397547
I0629 14:05:55.742380 15179 caffe.cpp:313] Batch 15, accuracy/top1 = 0.9
I0629 14:05:55.742391 15179 caffe.cpp:313] Batch 15, accuracy/top5 = 1
I0629 14:05:55.742394 15179 caffe.cpp:313] Batch 15, loss = 0.377622
I0629 14:05:55.751356 15179 caffe.cpp:313] Batch 16, accuracy/top1 = 0.92
I0629 14:05:55.751363 15179 caffe.cpp:313] Batch 16, accuracy/top5 = 1
I0629 14:05:55.751366 15179 caffe.cpp:313] Batch 16, loss = 0.385913
I0629 14:05:55.760404 15179 caffe.cpp:313] Batch 17, accuracy/top1 = 0.88
I0629 14:05:55.760418 15179 caffe.cpp:313] Batch 17, accuracy/top5 = 1
I0629 14:05:55.760424 15179 caffe.cpp:313] Batch 17, loss = 0.332212
I0629 14:05:55.769727 15179 caffe.cpp:313] Batch 18, accuracy/top1 = 0.9
I0629 14:05:55.769742 15179 caffe.cpp:313] Batch 18, accuracy/top5 = 1
I0629 14:05:55.769744 15179 caffe.cpp:313] Batch 18, loss = 0.29718
I0629 14:05:55.778771 15179 caffe.cpp:313] Batch 19, accuracy/top1 = 0.92
I0629 14:05:55.778780 15179 caffe.cpp:313] Batch 19, accuracy/top5 = 1
I0629 14:05:55.778784 15179 caffe.cpp:313] Batch 19, loss = 0.258859
I0629 14:05:55.787740 15179 caffe.cpp:313] Batch 20, accuracy/top1 = 0.86
I0629 14:05:55.787748 15179 caffe.cpp:313] Batch 20, accuracy/top5 = 1
I0629 14:05:55.787751 15179 caffe.cpp:313] Batch 20, loss = 0.319892
I0629 14:05:55.796998 15179 caffe.cpp:313] Batch 21, accuracy/top1 = 0.96
I0629 14:05:55.797015 15179 caffe.cpp:313] Batch 21, accuracy/top5 = 1
I0629 14:05:55.797020 15179 caffe.cpp:313] Batch 21, loss = 0.261385
I0629 14:05:55.806157 15179 caffe.cpp:313] Batch 22, accuracy/top1 = 0.88
I0629 14:05:55.806169 15179 caffe.cpp:313] Batch 22, accuracy/top5 = 1
I0629 14:05:55.806171 15179 caffe.cpp:313] Batch 22, loss = 0.515505
I0629 14:05:55.815160 15179 caffe.cpp:313] Batch 23, accuracy/top1 = 0.9
I0629 14:05:55.815170 15179 caffe.cpp:313] Batch 23, accuracy/top5 = 1
I0629 14:05:55.815171 15179 caffe.cpp:313] Batch 23, loss = 0.343264
I0629 14:05:55.824321 15179 caffe.cpp:313] Batch 24, accuracy/top1 = 0.88
I0629 14:05:55.824338 15179 caffe.cpp:313] Batch 24, accuracy/top5 = 0.98
I0629 14:05:55.824342 15179 caffe.cpp:313] Batch 24, loss = 0.301905
I0629 14:05:55.833505 15179 caffe.cpp:313] Batch 25, accuracy/top1 = 0.92
I0629 14:05:55.833516 15179 caffe.cpp:313] Batch 25, accuracy/top5 = 1
I0629 14:05:55.833519 15179 caffe.cpp:313] Batch 25, loss = 0.22202
I0629 14:05:55.842497 15179 caffe.cpp:313] Batch 26, accuracy/top1 = 0.9
I0629 14:05:55.842505 15179 caffe.cpp:313] Batch 26, accuracy/top5 = 1
I0629 14:05:55.842509 15179 caffe.cpp:313] Batch 26, loss = 0.442399
I0629 14:05:55.851617 15179 caffe.cpp:313] Batch 27, accuracy/top1 = 0.94
I0629 14:05:55.851644 15179 caffe.cpp:313] Batch 27, accuracy/top5 = 1
I0629 14:05:55.851650 15179 caffe.cpp:313] Batch 27, loss = 0.199298
I0629 14:05:55.861085 15179 caffe.cpp:313] Batch 28, accuracy/top1 = 0.88
I0629 14:05:55.861104 15179 caffe.cpp:313] Batch 28, accuracy/top5 = 1
I0629 14:05:55.861107 15179 caffe.cpp:313] Batch 28, loss = 0.399529
I0629 14:05:55.870127 15179 caffe.cpp:313] Batch 29, accuracy/top1 = 0.9
I0629 14:05:55.870153 15179 caffe.cpp:313] Batch 29, accuracy/top5 = 1
I0629 14:05:55.870157 15179 caffe.cpp:313] Batch 29, loss = 0.456926
I0629 14:05:55.879230 15179 caffe.cpp:313] Batch 30, accuracy/top1 = 0.9
I0629 14:05:55.879246 15179 caffe.cpp:313] Batch 30, accuracy/top5 = 1
I0629 14:05:55.879248 15179 caffe.cpp:313] Batch 30, loss = 0.465783
I0629 14:05:55.888532 15179 caffe.cpp:313] Batch 31, accuracy/top1 = 0.88
I0629 14:05:55.888551 15179 caffe.cpp:313] Batch 31, accuracy/top5 = 1
I0629 14:05:55.888553 15179 caffe.cpp:313] Batch 31, loss = 0.249379
I0629 14:05:55.897526 15179 caffe.cpp:313] Batch 32, accuracy/top1 = 0.9
I0629 14:05:55.897541 15179 caffe.cpp:313] Batch 32, accuracy/top5 = 1
I0629 14:05:55.897544 15179 caffe.cpp:313] Batch 32, loss = 0.252434
I0629 14:05:55.906442 15179 caffe.cpp:313] Batch 33, accuracy/top1 = 0.96
I0629 14:05:55.906461 15179 caffe.cpp:313] Batch 33, accuracy/top5 = 0.98
I0629 14:05:55.906463 15179 caffe.cpp:313] Batch 33, loss = 0.195247
I0629 14:05:55.915365 15179 caffe.cpp:313] Batch 34, accuracy/top1 = 0.88
I0629 14:05:55.915382 15179 caffe.cpp:313] Batch 34, accuracy/top5 = 1
I0629 14:05:55.915385 15179 caffe.cpp:313] Batch 34, loss = 0.454933
I0629 14:05:55.924346 15179 caffe.cpp:313] Batch 35, accuracy/top1 = 0.94
I0629 14:05:55.924363 15179 caffe.cpp:313] Batch 35, accuracy/top5 = 1
I0629 14:05:55.924365 15179 caffe.cpp:313] Batch 35, loss = 0.20312
I0629 14:05:55.933226 15179 caffe.cpp:313] Batch 36, accuracy/top1 = 0.86
I0629 14:05:55.933240 15179 caffe.cpp:313] Batch 36, accuracy/top5 = 0.98
I0629 14:05:55.933243 15179 caffe.cpp:313] Batch 36, loss = 0.368316
I0629 14:05:55.942157 15179 caffe.cpp:313] Batch 37, accuracy/top1 = 0.84
I0629 14:05:55.942173 15179 caffe.cpp:313] Batch 37, accuracy/top5 = 0.98
I0629 14:05:55.942175 15179 caffe.cpp:313] Batch 37, loss = 0.668027
I0629 14:05:55.951097 15179 caffe.cpp:313] Batch 38, accuracy/top1 = 0.86
I0629 14:05:55.951115 15179 caffe.cpp:313] Batch 38, accuracy/top5 = 0.96
I0629 14:05:55.951118 15179 caffe.cpp:313] Batch 38, loss = 0.678779
I0629 14:05:55.959897 15179 caffe.cpp:313] Batch 39, accuracy/top1 = 0.9
I0629 14:05:55.959908 15179 caffe.cpp:313] Batch 39, accuracy/top5 = 0.96
I0629 14:05:55.959911 15179 caffe.cpp:313] Batch 39, loss = 0.37776
I0629 14:05:55.968797 15179 caffe.cpp:313] Batch 40, accuracy/top1 = 0.88
I0629 14:05:55.968816 15179 caffe.cpp:313] Batch 40, accuracy/top5 = 1
I0629 14:05:55.968817 15179 caffe.cpp:313] Batch 40, loss = 0.630621
I0629 14:05:55.977754 15179 caffe.cpp:313] Batch 41, accuracy/top1 = 0.94
I0629 14:05:55.977771 15179 caffe.cpp:313] Batch 41, accuracy/top5 = 1
I0629 14:05:55.977773 15179 caffe.cpp:313] Batch 41, loss = 0.229581
I0629 14:05:55.986656 15179 caffe.cpp:313] Batch 42, accuracy/top1 = 0.92
I0629 14:05:55.986672 15179 caffe.cpp:313] Batch 42, accuracy/top5 = 0.98
I0629 14:05:55.986675 15179 caffe.cpp:313] Batch 42, loss = 0.26178
I0629 14:05:55.995764 15179 caffe.cpp:313] Batch 43, accuracy/top1 = 0.88
I0629 14:05:55.995779 15179 caffe.cpp:313] Batch 43, accuracy/top5 = 1
I0629 14:05:55.995782 15179 caffe.cpp:313] Batch 43, loss = 0.289845
I0629 14:05:56.004725 15179 caffe.cpp:313] Batch 44, accuracy/top1 = 0.9
I0629 14:05:56.004750 15179 caffe.cpp:313] Batch 44, accuracy/top5 = 1
I0629 14:05:56.004752 15179 caffe.cpp:313] Batch 44, loss = 0.41315
I0629 14:05:56.013706 15179 caffe.cpp:313] Batch 45, accuracy/top1 = 0.9
I0629 14:05:56.013721 15179 caffe.cpp:313] Batch 45, accuracy/top5 = 1
I0629 14:05:56.013723 15179 caffe.cpp:313] Batch 45, loss = 0.364181
I0629 14:05:56.022577 15179 caffe.cpp:313] Batch 46, accuracy/top1 = 0.98
I0629 14:05:56.022586 15179 caffe.cpp:313] Batch 46, accuracy/top5 = 1
I0629 14:05:56.022588 15179 caffe.cpp:313] Batch 46, loss = 0.0612557
I0629 14:05:56.031394 15179 caffe.cpp:313] Batch 47, accuracy/top1 = 0.82
I0629 14:05:56.031404 15179 caffe.cpp:313] Batch 47, accuracy/top5 = 1
I0629 14:05:56.031406 15179 caffe.cpp:313] Batch 47, loss = 0.501736
I0629 14:05:56.040259 15179 caffe.cpp:313] Batch 48, accuracy/top1 = 0.9
I0629 14:05:56.040288 15179 caffe.cpp:313] Batch 48, accuracy/top5 = 0.98
I0629 14:05:56.040292 15179 caffe.cpp:313] Batch 48, loss = 0.703735
I0629 14:05:56.049124 15179 caffe.cpp:313] Batch 49, accuracy/top1 = 0.92
I0629 14:05:56.049134 15179 caffe.cpp:313] Batch 49, accuracy/top5 = 1
I0629 14:05:56.049136 15179 caffe.cpp:313] Batch 49, loss = 0.43355
I0629 14:05:56.057945 15179 caffe.cpp:313] Batch 50, accuracy/top1 = 0.8
I0629 14:05:56.057953 15179 caffe.cpp:313] Batch 50, accuracy/top5 = 0.98
I0629 14:05:56.057956 15179 caffe.cpp:313] Batch 50, loss = 0.751408
I0629 14:05:56.066776 15179 caffe.cpp:313] Batch 51, accuracy/top1 = 0.88
I0629 14:05:56.066787 15179 caffe.cpp:313] Batch 51, accuracy/top5 = 1
I0629 14:05:56.066790 15179 caffe.cpp:313] Batch 51, loss = 0.350897
I0629 14:05:56.075639 15179 caffe.cpp:313] Batch 52, accuracy/top1 = 0.88
I0629 14:05:56.075649 15179 caffe.cpp:313] Batch 52, accuracy/top5 = 1
I0629 14:05:56.075650 15179 caffe.cpp:313] Batch 52, loss = 0.247005
I0629 14:05:56.084450 15179 caffe.cpp:313] Batch 53, accuracy/top1 = 0.96
I0629 14:05:56.084458 15179 caffe.cpp:313] Batch 53, accuracy/top5 = 1
I0629 14:05:56.084461 15179 caffe.cpp:313] Batch 53, loss = 0.230147
I0629 14:05:56.093271 15179 caffe.cpp:313] Batch 54, accuracy/top1 = 0.92
I0629 14:05:56.093279 15179 caffe.cpp:313] Batch 54, accuracy/top5 = 1
I0629 14:05:56.093281 15179 caffe.cpp:313] Batch 54, loss = 0.270494
I0629 14:05:56.102157 15179 caffe.cpp:313] Batch 55, accuracy/top1 = 0.86
I0629 14:05:56.102174 15179 caffe.cpp:313] Batch 55, accuracy/top5 = 1
I0629 14:05:56.102177 15179 caffe.cpp:313] Batch 55, loss = 0.429864
I0629 14:05:56.111063 15179 caffe.cpp:313] Batch 56, accuracy/top1 = 0.84
I0629 14:05:56.111071 15179 caffe.cpp:313] Batch 56, accuracy/top5 = 0.96
I0629 14:05:56.111074 15179 caffe.cpp:313] Batch 56, loss = 0.646045
I0629 14:05:56.119858 15179 caffe.cpp:313] Batch 57, accuracy/top1 = 0.92
I0629 14:05:56.119865 15179 caffe.cpp:313] Batch 57, accuracy/top5 = 1
I0629 14:05:56.119868 15179 caffe.cpp:313] Batch 57, loss = 0.264563
I0629 14:05:56.128718 15179 caffe.cpp:313] Batch 58, accuracy/top1 = 0.9
I0629 14:05:56.128731 15179 caffe.cpp:313] Batch 58, accuracy/top5 = 0.98
I0629 14:05:56.128732 15179 caffe.cpp:313] Batch 58, loss = 0.357769
I0629 14:05:56.137574 15179 caffe.cpp:313] Batch 59, accuracy/top1 = 0.88
I0629 14:05:56.137583 15179 caffe.cpp:313] Batch 59, accuracy/top5 = 1
I0629 14:05:56.137585 15179 caffe.cpp:313] Batch 59, loss = 0.414792
I0629 14:05:56.146410 15179 caffe.cpp:313] Batch 60, accuracy/top1 = 0.92
I0629 14:05:56.146419 15179 caffe.cpp:313] Batch 60, accuracy/top5 = 0.98
I0629 14:05:56.146420 15179 caffe.cpp:313] Batch 60, loss = 0.203291
I0629 14:05:56.155230 15179 caffe.cpp:313] Batch 61, accuracy/top1 = 0.84
I0629 14:05:56.155238 15179 caffe.cpp:313] Batch 61, accuracy/top5 = 0.98
I0629 14:05:56.155241 15179 caffe.cpp:313] Batch 61, loss = 0.444668
I0629 14:05:56.164078 15179 caffe.cpp:313] Batch 62, accuracy/top1 = 0.98
I0629 14:05:56.164096 15179 caffe.cpp:313] Batch 62, accuracy/top5 = 1
I0629 14:05:56.164098 15179 caffe.cpp:313] Batch 62, loss = 0.074199
I0629 14:05:56.172924 15179 caffe.cpp:313] Batch 63, accuracy/top1 = 0.82
I0629 14:05:56.172932 15179 caffe.cpp:313] Batch 63, accuracy/top5 = 1
I0629 14:05:56.172935 15179 caffe.cpp:313] Batch 63, loss = 0.428295
I0629 14:05:56.181732 15179 caffe.cpp:313] Batch 64, accuracy/top1 = 0.92
I0629 14:05:56.181740 15179 caffe.cpp:313] Batch 64, accuracy/top5 = 0.98
I0629 14:05:56.181742 15179 caffe.cpp:313] Batch 64, loss = 0.317228
I0629 14:05:56.190567 15179 caffe.cpp:313] Batch 65, accuracy/top1 = 0.94
I0629 14:05:56.190580 15179 caffe.cpp:313] Batch 65, accuracy/top5 = 1
I0629 14:05:56.190582 15179 caffe.cpp:313] Batch 65, loss = 0.199978
I0629 14:05:56.199424 15179 caffe.cpp:313] Batch 66, accuracy/top1 = 0.9
I0629 14:05:56.199434 15179 caffe.cpp:313] Batch 66, accuracy/top5 = 1
I0629 14:05:56.199435 15179 caffe.cpp:313] Batch 66, loss = 0.274872
I0629 14:05:56.208261 15179 caffe.cpp:313] Batch 67, accuracy/top1 = 0.96
I0629 14:05:56.208278 15179 caffe.cpp:313] Batch 67, accuracy/top5 = 1
I0629 14:05:56.208281 15179 caffe.cpp:313] Batch 67, loss = 0.181648
I0629 14:05:56.217080 15179 caffe.cpp:313] Batch 68, accuracy/top1 = 0.88
I0629 14:05:56.217087 15179 caffe.cpp:313] Batch 68, accuracy/top5 = 1
I0629 14:05:56.217090 15179 caffe.cpp:313] Batch 68, loss = 0.357861
I0629 14:05:56.225951 15179 caffe.cpp:313] Batch 69, accuracy/top1 = 0.92
I0629 14:05:56.225968 15179 caffe.cpp:313] Batch 69, accuracy/top5 = 0.98
I0629 14:05:56.225970 15179 caffe.cpp:313] Batch 69, loss = 0.314714
I0629 14:05:56.234789 15179 caffe.cpp:313] Batch 70, accuracy/top1 = 0.92
I0629 14:05:56.234797 15179 caffe.cpp:313] Batch 70, accuracy/top5 = 0.98
I0629 14:05:56.234799 15179 caffe.cpp:313] Batch 70, loss = 0.444917
I0629 14:05:56.243644 15179 caffe.cpp:313] Batch 71, accuracy/top1 = 0.92
I0629 14:05:56.243652 15179 caffe.cpp:313] Batch 71, accuracy/top5 = 1
I0629 14:05:56.243655 15179 caffe.cpp:313] Batch 71, loss = 0.345279
I0629 14:05:56.252363 15179 caffe.cpp:313] Batch 72, accuracy/top1 = 0.88
I0629 14:05:56.252377 15179 caffe.cpp:313] Batch 72, accuracy/top5 = 0.96
I0629 14:05:56.252379 15179 caffe.cpp:313] Batch 72, loss = 0.579998
I0629 14:05:56.261251 15179 caffe.cpp:313] Batch 73, accuracy/top1 = 0.94
I0629 14:05:56.261260 15179 caffe.cpp:313] Batch 73, accuracy/top5 = 1
I0629 14:05:56.261263 15179 caffe.cpp:313] Batch 73, loss = 0.308165
I0629 14:05:56.270061 15179 caffe.cpp:313] Batch 74, accuracy/top1 = 0.96
I0629 14:05:56.270069 15179 caffe.cpp:313] Batch 74, accuracy/top5 = 1
I0629 14:05:56.270071 15179 caffe.cpp:313] Batch 74, loss = 0.133947
I0629 14:05:56.278892 15179 caffe.cpp:313] Batch 75, accuracy/top1 = 0.86
I0629 14:05:56.278899 15179 caffe.cpp:313] Batch 75, accuracy/top5 = 1
I0629 14:05:56.278903 15179 caffe.cpp:313] Batch 75, loss = 0.563858
I0629 14:05:56.287755 15179 caffe.cpp:313] Batch 76, accuracy/top1 = 0.94
I0629 14:05:56.287770 15179 caffe.cpp:313] Batch 76, accuracy/top5 = 1
I0629 14:05:56.287771 15179 caffe.cpp:313] Batch 76, loss = 0.183117
I0629 14:05:56.296620 15179 caffe.cpp:313] Batch 77, accuracy/top1 = 0.94
I0629 14:05:56.296628 15179 caffe.cpp:313] Batch 77, accuracy/top5 = 1
I0629 14:05:56.296630 15179 caffe.cpp:313] Batch 77, loss = 0.318662
I0629 14:05:56.305423 15179 caffe.cpp:313] Batch 78, accuracy/top1 = 0.96
I0629 14:05:56.305430 15179 caffe.cpp:313] Batch 78, accuracy/top5 = 1
I0629 14:05:56.305433 15179 caffe.cpp:313] Batch 78, loss = 0.104554
I0629 14:05:56.314255 15179 caffe.cpp:313] Batch 79, accuracy/top1 = 0.96
I0629 14:05:56.314270 15179 caffe.cpp:313] Batch 79, accuracy/top5 = 0.98
I0629 14:05:56.314272 15179 caffe.cpp:313] Batch 79, loss = 0.155901
I0629 14:05:56.323112 15179 caffe.cpp:313] Batch 80, accuracy/top1 = 0.88
I0629 14:05:56.323120 15179 caffe.cpp:313] Batch 80, accuracy/top5 = 1
I0629 14:05:56.323123 15179 caffe.cpp:313] Batch 80, loss = 0.404304
I0629 14:05:56.331929 15179 caffe.cpp:313] Batch 81, accuracy/top1 = 0.9
I0629 14:05:56.331938 15179 caffe.cpp:313] Batch 81, accuracy/top5 = 1
I0629 14:05:56.331939 15179 caffe.cpp:313] Batch 81, loss = 0.25662
I0629 14:05:56.340785 15179 caffe.cpp:313] Batch 82, accuracy/top1 = 0.86
I0629 14:05:56.340796 15179 caffe.cpp:313] Batch 82, accuracy/top5 = 1
I0629 14:05:56.340800 15179 caffe.cpp:313] Batch 82, loss = 0.386467
I0629 14:05:56.349637 15179 caffe.cpp:313] Batch 83, accuracy/top1 = 0.9
I0629 14:05:56.349647 15179 caffe.cpp:313] Batch 83, accuracy/top5 = 1
I0629 14:05:56.349649 15179 caffe.cpp:313] Batch 83, loss = 0.237622
I0629 14:05:56.358515 15179 caffe.cpp:313] Batch 84, accuracy/top1 = 0.94
I0629 14:05:56.358523 15179 caffe.cpp:313] Batch 84, accuracy/top5 = 0.98
I0629 14:05:56.358526 15179 caffe.cpp:313] Batch 84, loss = 0.227262
I0629 14:05:56.367317 15179 caffe.cpp:313] Batch 85, accuracy/top1 = 0.98
I0629 14:05:56.367326 15179 caffe.cpp:313] Batch 85, accuracy/top5 = 1
I0629 14:05:56.367328 15179 caffe.cpp:313] Batch 85, loss = 0.0551999
I0629 14:05:56.376188 15179 caffe.cpp:313] Batch 86, accuracy/top1 = 0.92
I0629 14:05:56.376205 15179 caffe.cpp:313] Batch 86, accuracy/top5 = 1
I0629 14:05:56.376220 15179 caffe.cpp:313] Batch 86, loss = 0.495665
I0629 14:05:56.385042 15179 caffe.cpp:313] Batch 87, accuracy/top1 = 0.98
I0629 14:05:56.385051 15179 caffe.cpp:313] Batch 87, accuracy/top5 = 1
I0629 14:05:56.385052 15179 caffe.cpp:313] Batch 87, loss = 0.0877222
I0629 14:05:56.393877 15179 caffe.cpp:313] Batch 88, accuracy/top1 = 0.92
I0629 14:05:56.393884 15179 caffe.cpp:313] Batch 88, accuracy/top5 = 1
I0629 14:05:56.393887 15179 caffe.cpp:313] Batch 88, loss = 0.305231
I0629 14:05:56.402709 15179 caffe.cpp:313] Batch 89, accuracy/top1 = 0.9
I0629 14:05:56.402721 15179 caffe.cpp:313] Batch 89, accuracy/top5 = 0.98
I0629 14:05:56.402724 15179 caffe.cpp:313] Batch 89, loss = 0.454218
I0629 14:05:56.411590 15179 caffe.cpp:313] Batch 90, accuracy/top1 = 0.9
I0629 14:05:56.411599 15179 caffe.cpp:313] Batch 90, accuracy/top5 = 1
I0629 14:05:56.411602 15179 caffe.cpp:313] Batch 90, loss = 0.334359
I0629 14:05:56.420403 15179 caffe.cpp:313] Batch 91, accuracy/top1 = 0.86
I0629 14:05:56.420411 15179 caffe.cpp:313] Batch 91, accuracy/top5 = 0.98
I0629 14:05:56.420414 15179 caffe.cpp:313] Batch 91, loss = 0.413313
I0629 14:05:56.429241 15179 caffe.cpp:313] Batch 92, accuracy/top1 = 0.9
I0629 14:05:56.429250 15179 caffe.cpp:313] Batch 92, accuracy/top5 = 1
I0629 14:05:56.429251 15179 caffe.cpp:313] Batch 92, loss = 0.361656
I0629 14:05:56.438122 15179 caffe.cpp:313] Batch 93, accuracy/top1 = 0.96
I0629 14:05:56.438138 15179 caffe.cpp:313] Batch 93, accuracy/top5 = 1
I0629 14:05:56.438140 15179 caffe.cpp:313] Batch 93, loss = 0.13497
I0629 14:05:56.446997 15179 caffe.cpp:313] Batch 94, accuracy/top1 = 0.86
I0629 14:05:56.447005 15179 caffe.cpp:313] Batch 94, accuracy/top5 = 0.98
I0629 14:05:56.447007 15179 caffe.cpp:313] Batch 94, loss = 0.634508
I0629 14:05:56.455813 15179 caffe.cpp:313] Batch 95, accuracy/top1 = 0.84
I0629 14:05:56.455821 15179 caffe.cpp:313] Batch 95, accuracy/top5 = 0.98
I0629 14:05:56.455823 15179 caffe.cpp:313] Batch 95, loss = 0.640646
I0629 14:05:56.464648 15179 caffe.cpp:313] Batch 96, accuracy/top1 = 1
I0629 14:05:56.464660 15179 caffe.cpp:313] Batch 96, accuracy/top5 = 1
I0629 14:05:56.464663 15179 caffe.cpp:313] Batch 96, loss = 0.046008
I0629 14:05:56.473503 15179 caffe.cpp:313] Batch 97, accuracy/top1 = 0.94
I0629 14:05:56.473512 15179 caffe.cpp:313] Batch 97, accuracy/top5 = 1
I0629 14:05:56.473515 15179 caffe.cpp:313] Batch 97, loss = 0.116041
I0629 14:05:56.482313 15179 caffe.cpp:313] Batch 98, accuracy/top1 = 0.9
I0629 14:05:56.482321 15179 caffe.cpp:313] Batch 98, accuracy/top5 = 1
I0629 14:05:56.482324 15179 caffe.cpp:313] Batch 98, loss = 0.398619
I0629 14:05:56.491158 15179 caffe.cpp:313] Batch 99, accuracy/top1 = 0.82
I0629 14:05:56.491165 15179 caffe.cpp:313] Batch 99, accuracy/top5 = 0.96
I0629 14:05:56.491168 15179 caffe.cpp:313] Batch 99, loss = 0.88456
I0629 14:05:56.500017 15179 caffe.cpp:313] Batch 100, accuracy/top1 = 0.94
I0629 14:05:56.500035 15179 caffe.cpp:313] Batch 100, accuracy/top5 = 1
I0629 14:05:56.500037 15179 caffe.cpp:313] Batch 100, loss = 0.166234
I0629 14:05:56.508896 15179 caffe.cpp:313] Batch 101, accuracy/top1 = 0.94
I0629 14:05:56.508904 15179 caffe.cpp:313] Batch 101, accuracy/top5 = 1
I0629 14:05:56.508908 15179 caffe.cpp:313] Batch 101, loss = 0.179935
I0629 14:05:56.517683 15179 caffe.cpp:313] Batch 102, accuracy/top1 = 0.94
I0629 14:05:56.517689 15179 caffe.cpp:313] Batch 102, accuracy/top5 = 1
I0629 14:05:56.517693 15179 caffe.cpp:313] Batch 102, loss = 0.140146
I0629 14:05:56.526553 15179 caffe.cpp:313] Batch 103, accuracy/top1 = 0.88
I0629 14:05:56.526566 15179 caffe.cpp:313] Batch 103, accuracy/top5 = 1
I0629 14:05:56.526569 15179 caffe.cpp:313] Batch 103, loss = 0.604123
I0629 14:05:56.535399 15179 caffe.cpp:313] Batch 104, accuracy/top1 = 0.88
I0629 14:05:56.535408 15179 caffe.cpp:313] Batch 104, accuracy/top5 = 0.98
I0629 14:05:56.535411 15179 caffe.cpp:313] Batch 104, loss = 0.535996
I0629 14:05:56.544265 15179 caffe.cpp:313] Batch 105, accuracy/top1 = 0.92
I0629 14:05:56.544271 15179 caffe.cpp:313] Batch 105, accuracy/top5 = 1
I0629 14:05:56.544282 15179 caffe.cpp:313] Batch 105, loss = 0.271488
I0629 14:05:56.553074 15179 caffe.cpp:313] Batch 106, accuracy/top1 = 0.92
I0629 14:05:56.553082 15179 caffe.cpp:313] Batch 106, accuracy/top5 = 1
I0629 14:05:56.553086 15179 caffe.cpp:313] Batch 106, loss = 0.249427
I0629 14:05:56.561856 15179 caffe.cpp:313] Batch 107, accuracy/top1 = 0.88
I0629 14:05:56.561872 15179 caffe.cpp:313] Batch 107, accuracy/top5 = 0.98
I0629 14:05:56.561874 15179 caffe.cpp:313] Batch 107, loss = 0.474796
I0629 14:05:56.570580 15179 caffe.cpp:313] Batch 108, accuracy/top1 = 0.9
I0629 14:05:56.570588 15179 caffe.cpp:313] Batch 108, accuracy/top5 = 1
I0629 14:05:56.570590 15179 caffe.cpp:313] Batch 108, loss = 0.335896
I0629 14:05:56.579437 15179 caffe.cpp:313] Batch 109, accuracy/top1 = 0.88
I0629 14:05:56.579445 15179 caffe.cpp:313] Batch 109, accuracy/top5 = 1
I0629 14:05:56.579447 15179 caffe.cpp:313] Batch 109, loss = 0.394468
I0629 14:05:56.588287 15179 caffe.cpp:313] Batch 110, accuracy/top1 = 0.88
I0629 14:05:56.588300 15179 caffe.cpp:313] Batch 110, accuracy/top5 = 1
I0629 14:05:56.588302 15179 caffe.cpp:313] Batch 110, loss = 0.689324
I0629 14:05:56.597148 15179 caffe.cpp:313] Batch 111, accuracy/top1 = 0.96
I0629 14:05:56.597157 15179 caffe.cpp:313] Batch 111, accuracy/top5 = 1
I0629 14:05:56.597159 15179 caffe.cpp:313] Batch 111, loss = 0.333748
I0629 14:05:56.605996 15179 caffe.cpp:313] Batch 112, accuracy/top1 = 0.86
I0629 14:05:56.606004 15179 caffe.cpp:313] Batch 112, accuracy/top5 = 1
I0629 14:05:56.606007 15179 caffe.cpp:313] Batch 112, loss = 0.567695
I0629 14:05:56.614827 15179 caffe.cpp:313] Batch 113, accuracy/top1 = 0.92
I0629 14:05:56.614840 15179 caffe.cpp:313] Batch 113, accuracy/top5 = 1
I0629 14:05:56.614842 15179 caffe.cpp:313] Batch 113, loss = 0.135976
I0629 14:05:56.623684 15179 caffe.cpp:313] Batch 114, accuracy/top1 = 0.9
I0629 14:05:56.623694 15179 caffe.cpp:313] Batch 114, accuracy/top5 = 1
I0629 14:05:56.623697 15179 caffe.cpp:313] Batch 114, loss = 0.389026
I0629 14:05:56.632510 15179 caffe.cpp:313] Batch 115, accuracy/top1 = 0.94
I0629 14:05:56.632519 15179 caffe.cpp:313] Batch 115, accuracy/top5 = 0.98
I0629 14:05:56.632521 15179 caffe.cpp:313] Batch 115, loss = 0.147252
I0629 14:05:56.641330 15179 caffe.cpp:313] Batch 116, accuracy/top1 = 0.9
I0629 14:05:56.641338 15179 caffe.cpp:313] Batch 116, accuracy/top5 = 1
I0629 14:05:56.641340 15179 caffe.cpp:313] Batch 116, loss = 0.550396
I0629 14:05:56.650183 15179 caffe.cpp:313] Batch 117, accuracy/top1 = 0.82
I0629 14:05:56.650200 15179 caffe.cpp:313] Batch 117, accuracy/top5 = 0.98
I0629 14:05:56.650202 15179 caffe.cpp:313] Batch 117, loss = 0.695791
I0629 14:05:56.659056 15179 caffe.cpp:313] Batch 118, accuracy/top1 = 0.9
I0629 14:05:56.659065 15179 caffe.cpp:313] Batch 118, accuracy/top5 = 1
I0629 14:05:56.659067 15179 caffe.cpp:313] Batch 118, loss = 0.321466
I0629 14:05:56.667868 15179 caffe.cpp:313] Batch 119, accuracy/top1 = 0.94
I0629 14:05:56.667876 15179 caffe.cpp:313] Batch 119, accuracy/top5 = 1
I0629 14:05:56.667879 15179 caffe.cpp:313] Batch 119, loss = 0.130195
I0629 14:05:56.676717 15179 caffe.cpp:313] Batch 120, accuracy/top1 = 0.88
I0629 14:05:56.676729 15179 caffe.cpp:313] Batch 120, accuracy/top5 = 1
I0629 14:05:56.676733 15179 caffe.cpp:313] Batch 120, loss = 0.316664
I0629 14:05:56.685561 15179 caffe.cpp:313] Batch 121, accuracy/top1 = 0.88
I0629 14:05:56.685570 15179 caffe.cpp:313] Batch 121, accuracy/top5 = 1
I0629 14:05:56.685572 15179 caffe.cpp:313] Batch 121, loss = 0.496821
I0629 14:05:56.694437 15179 caffe.cpp:313] Batch 122, accuracy/top1 = 0.86
I0629 14:05:56.694444 15179 caffe.cpp:313] Batch 122, accuracy/top5 = 1
I0629 14:05:56.694447 15179 caffe.cpp:313] Batch 122, loss = 0.294566
I0629 14:05:56.703233 15179 caffe.cpp:313] Batch 123, accuracy/top1 = 0.88
I0629 14:05:56.703240 15179 caffe.cpp:313] Batch 123, accuracy/top5 = 1
I0629 14:05:56.703243 15179 caffe.cpp:313] Batch 123, loss = 0.307435
I0629 14:05:56.712266 15179 caffe.cpp:313] Batch 124, accuracy/top1 = 0.88
I0629 14:05:56.712309 15179 caffe.cpp:313] Batch 124, accuracy/top5 = 0.98
I0629 14:05:56.712314 15179 caffe.cpp:313] Batch 124, loss = 0.517442
I0629 14:05:56.721184 15179 caffe.cpp:313] Batch 125, accuracy/top1 = 0.94
I0629 14:05:56.721194 15179 caffe.cpp:313] Batch 125, accuracy/top5 = 1
I0629 14:05:56.721196 15179 caffe.cpp:313] Batch 125, loss = 0.338362
I0629 14:05:56.730046 15179 caffe.cpp:313] Batch 126, accuracy/top1 = 1
I0629 14:05:56.730056 15179 caffe.cpp:313] Batch 126, accuracy/top5 = 1
I0629 14:05:56.730058 15179 caffe.cpp:313] Batch 126, loss = 0.0210553
I0629 14:05:56.738909 15179 caffe.cpp:313] Batch 127, accuracy/top1 = 0.96
I0629 14:05:56.738922 15179 caffe.cpp:313] Batch 127, accuracy/top5 = 1
I0629 14:05:56.738925 15179 caffe.cpp:313] Batch 127, loss = 0.114014
I0629 14:05:56.747781 15179 caffe.cpp:313] Batch 128, accuracy/top1 = 0.8
I0629 14:05:56.747790 15179 caffe.cpp:313] Batch 128, accuracy/top5 = 1
I0629 14:05:56.747793 15179 caffe.cpp:313] Batch 128, loss = 0.452436
I0629 14:05:56.756606 15179 caffe.cpp:313] Batch 129, accuracy/top1 = 0.88
I0629 14:05:56.756614 15179 caffe.cpp:313] Batch 129, accuracy/top5 = 1
I0629 14:05:56.756618 15179 caffe.cpp:313] Batch 129, loss = 0.378014
I0629 14:05:56.765419 15179 caffe.cpp:313] Batch 130, accuracy/top1 = 0.88
I0629 14:05:56.765426 15179 caffe.cpp:313] Batch 130, accuracy/top5 = 1
I0629 14:05:56.765429 15179 caffe.cpp:313] Batch 130, loss = 0.465846
I0629 14:05:56.774338 15179 caffe.cpp:313] Batch 131, accuracy/top1 = 0.92
I0629 14:05:56.774355 15179 caffe.cpp:313] Batch 131, accuracy/top5 = 1
I0629 14:05:56.774358 15179 caffe.cpp:313] Batch 131, loss = 0.223119
I0629 14:05:56.783166 15179 caffe.cpp:313] Batch 132, accuracy/top1 = 0.98
I0629 14:05:56.783174 15179 caffe.cpp:313] Batch 132, accuracy/top5 = 1
I0629 14:05:56.783177 15179 caffe.cpp:313] Batch 132, loss = 0.124597
I0629 14:05:56.792019 15179 caffe.cpp:313] Batch 133, accuracy/top1 = 0.92
I0629 14:05:56.792027 15179 caffe.cpp:313] Batch 133, accuracy/top5 = 1
I0629 14:05:56.792029 15179 caffe.cpp:313] Batch 133, loss = 0.282957
I0629 14:05:56.800850 15179 caffe.cpp:313] Batch 134, accuracy/top1 = 0.96
I0629 14:05:56.800865 15179 caffe.cpp:313] Batch 134, accuracy/top5 = 1
I0629 14:05:56.800868 15179 caffe.cpp:313] Batch 134, loss = 0.109786
I0629 14:05:56.809734 15179 caffe.cpp:313] Batch 135, accuracy/top1 = 0.88
I0629 14:05:56.809744 15179 caffe.cpp:313] Batch 135, accuracy/top5 = 0.98
I0629 14:05:56.809746 15179 caffe.cpp:313] Batch 135, loss = 0.409425
I0629 14:05:56.818558 15179 caffe.cpp:313] Batch 136, accuracy/top1 = 0.96
I0629 14:05:56.818567 15179 caffe.cpp:313] Batch 136, accuracy/top5 = 1
I0629 14:05:56.818569 15179 caffe.cpp:313] Batch 136, loss = 0.0595348
I0629 14:05:56.827410 15179 caffe.cpp:313] Batch 137, accuracy/top1 = 0.82
I0629 14:05:56.827419 15179 caffe.cpp:313] Batch 137, accuracy/top5 = 1
I0629 14:05:56.827420 15179 caffe.cpp:313] Batch 137, loss = 0.462451
I0629 14:05:56.836273 15179 caffe.cpp:313] Batch 138, accuracy/top1 = 0.94
I0629 14:05:56.836288 15179 caffe.cpp:313] Batch 138, accuracy/top5 = 0.98
I0629 14:05:56.836292 15179 caffe.cpp:313] Batch 138, loss = 0.237493
I0629 14:05:56.845158 15179 caffe.cpp:313] Batch 139, accuracy/top1 = 0.86
I0629 14:05:56.845166 15179 caffe.cpp:313] Batch 139, accuracy/top5 = 1
I0629 14:05:56.845170 15179 caffe.cpp:313] Batch 139, loss = 0.548756
I0629 14:05:56.853984 15179 caffe.cpp:313] Batch 140, accuracy/top1 = 0.9
I0629 14:05:56.853992 15179 caffe.cpp:313] Batch 140, accuracy/top5 = 1
I0629 14:05:56.853994 15179 caffe.cpp:313] Batch 140, loss = 0.354012
I0629 14:05:56.862854 15179 caffe.cpp:313] Batch 141, accuracy/top1 = 0.92
I0629 14:05:56.862867 15179 caffe.cpp:313] Batch 141, accuracy/top5 = 1
I0629 14:05:56.862870 15179 caffe.cpp:313] Batch 141, loss = 0.365016
I0629 14:05:56.871726 15179 caffe.cpp:313] Batch 142, accuracy/top1 = 0.9
I0629 14:05:56.871734 15179 caffe.cpp:313] Batch 142, accuracy/top5 = 1
I0629 14:05:56.871737 15179 caffe.cpp:313] Batch 142, loss = 0.277866
I0629 14:05:56.880550 15179 caffe.cpp:313] Batch 143, accuracy/top1 = 0.94
I0629 14:05:56.880566 15179 caffe.cpp:313] Batch 143, accuracy/top5 = 1
I0629 14:05:56.880569 15179 caffe.cpp:313] Batch 143, loss = 0.224683
I0629 14:05:56.889425 15179 caffe.cpp:313] Batch 144, accuracy/top1 = 0.94
I0629 14:05:56.889436 15179 caffe.cpp:313] Batch 144, accuracy/top5 = 1
I0629 14:05:56.889439 15179 caffe.cpp:313] Batch 144, loss = 0.224579
I0629 14:05:56.898288 15179 caffe.cpp:313] Batch 145, accuracy/top1 = 0.96
I0629 14:05:56.898298 15179 caffe.cpp:313] Batch 145, accuracy/top5 = 1
I0629 14:05:56.898300 15179 caffe.cpp:313] Batch 145, loss = 0.221321
I0629 14:05:56.907104 15179 caffe.cpp:313] Batch 146, accuracy/top1 = 0.92
I0629 14:05:56.907111 15179 caffe.cpp:313] Batch 146, accuracy/top5 = 1
I0629 14:05:56.907114 15179 caffe.cpp:313] Batch 146, loss = 0.172173
I0629 14:05:56.915905 15179 caffe.cpp:313] Batch 147, accuracy/top1 = 0.92
I0629 14:05:56.915913 15179 caffe.cpp:313] Batch 147, accuracy/top5 = 1
I0629 14:05:56.915916 15179 caffe.cpp:313] Batch 147, loss = 0.234113
I0629 14:05:56.924788 15179 caffe.cpp:313] Batch 148, accuracy/top1 = 0.9
I0629 14:05:56.924808 15179 caffe.cpp:313] Batch 148, accuracy/top5 = 1
I0629 14:05:56.924811 15179 caffe.cpp:313] Batch 148, loss = 0.363068
I0629 14:05:56.933666 15179 caffe.cpp:313] Batch 149, accuracy/top1 = 0.88
I0629 14:05:56.933676 15179 caffe.cpp:313] Batch 149, accuracy/top5 = 1
I0629 14:05:56.933677 15179 caffe.cpp:313] Batch 149, loss = 0.207936
I0629 14:05:56.942476 15179 caffe.cpp:313] Batch 150, accuracy/top1 = 0.9
I0629 14:05:56.942483 15179 caffe.cpp:313] Batch 150, accuracy/top5 = 0.98
I0629 14:05:56.942487 15179 caffe.cpp:313] Batch 150, loss = 0.269476
I0629 14:05:56.951285 15179 caffe.cpp:313] Batch 151, accuracy/top1 = 0.94
I0629 14:05:56.951297 15179 caffe.cpp:313] Batch 151, accuracy/top5 = 1
I0629 14:05:56.951301 15179 caffe.cpp:313] Batch 151, loss = 0.259803
I0629 14:05:56.960180 15179 caffe.cpp:313] Batch 152, accuracy/top1 = 0.86
I0629 14:05:56.960189 15179 caffe.cpp:313] Batch 152, accuracy/top5 = 1
I0629 14:05:56.960191 15179 caffe.cpp:313] Batch 152, loss = 0.414384
I0629 14:05:56.968989 15179 caffe.cpp:313] Batch 153, accuracy/top1 = 0.94
I0629 14:05:56.968997 15179 caffe.cpp:313] Batch 153, accuracy/top5 = 0.98
I0629 14:05:56.968999 15179 caffe.cpp:313] Batch 153, loss = 0.364942
I0629 14:05:56.977820 15179 caffe.cpp:313] Batch 154, accuracy/top1 = 0.96
I0629 14:05:56.977828 15179 caffe.cpp:313] Batch 154, accuracy/top5 = 1
I0629 14:05:56.977831 15179 caffe.cpp:313] Batch 154, loss = 0.129967
I0629 14:05:56.986671 15179 caffe.cpp:313] Batch 155, accuracy/top1 = 0.88
I0629 14:05:56.986690 15179 caffe.cpp:313] Batch 155, accuracy/top5 = 1
I0629 14:05:56.986692 15179 caffe.cpp:313] Batch 155, loss = 0.477463
I0629 14:05:56.995561 15179 caffe.cpp:313] Batch 156, accuracy/top1 = 0.88
I0629 14:05:56.995569 15179 caffe.cpp:313] Batch 156, accuracy/top5 = 1
I0629 14:05:56.995571 15179 caffe.cpp:313] Batch 156, loss = 0.475964
I0629 14:05:57.004359 15179 caffe.cpp:313] Batch 157, accuracy/top1 = 0.92
I0629 14:05:57.004367 15179 caffe.cpp:313] Batch 157, accuracy/top5 = 1
I0629 14:05:57.004370 15179 caffe.cpp:313] Batch 157, loss = 0.337381
I0629 14:05:57.013211 15179 caffe.cpp:313] Batch 158, accuracy/top1 = 0.86
I0629 14:05:57.013223 15179 caffe.cpp:313] Batch 158, accuracy/top5 = 1
I0629 14:05:57.013226 15179 caffe.cpp:313] Batch 158, loss = 0.324694
I0629 14:05:57.022076 15179 caffe.cpp:313] Batch 159, accuracy/top1 = 0.86
I0629 14:05:57.022086 15179 caffe.cpp:313] Batch 159, accuracy/top5 = 1
I0629 14:05:57.022089 15179 caffe.cpp:313] Batch 159, loss = 0.269764
I0629 14:05:57.030902 15179 caffe.cpp:313] Batch 160, accuracy/top1 = 0.94
I0629 14:05:57.030911 15179 caffe.cpp:313] Batch 160, accuracy/top5 = 1
I0629 14:05:57.030913 15179 caffe.cpp:313] Batch 160, loss = 0.197345
I0629 14:05:57.039718 15179 caffe.cpp:313] Batch 161, accuracy/top1 = 0.96
I0629 14:05:57.039726 15179 caffe.cpp:313] Batch 161, accuracy/top5 = 1
I0629 14:05:57.039729 15179 caffe.cpp:313] Batch 161, loss = 0.14668
I0629 14:05:57.048593 15179 caffe.cpp:313] Batch 162, accuracy/top1 = 0.94
I0629 14:05:57.048609 15179 caffe.cpp:313] Batch 162, accuracy/top5 = 1
I0629 14:05:57.048612 15179 caffe.cpp:313] Batch 162, loss = 0.28568
I0629 14:05:57.057473 15179 caffe.cpp:313] Batch 163, accuracy/top1 = 0.96
I0629 14:05:57.057482 15179 caffe.cpp:313] Batch 163, accuracy/top5 = 0.98
I0629 14:05:57.057484 15179 caffe.cpp:313] Batch 163, loss = 0.218996
I0629 14:05:57.066274 15179 caffe.cpp:313] Batch 164, accuracy/top1 = 0.86
I0629 14:05:57.066283 15179 caffe.cpp:313] Batch 164, accuracy/top5 = 1
I0629 14:05:57.066287 15179 caffe.cpp:313] Batch 164, loss = 0.517037
I0629 14:05:57.075152 15179 caffe.cpp:313] Batch 165, accuracy/top1 = 0.92
I0629 14:05:57.075166 15179 caffe.cpp:313] Batch 165, accuracy/top5 = 1
I0629 14:05:57.075170 15179 caffe.cpp:313] Batch 165, loss = 0.452105
I0629 14:05:57.083991 15179 caffe.cpp:313] Batch 166, accuracy/top1 = 0.96
I0629 14:05:57.083999 15179 caffe.cpp:313] Batch 166, accuracy/top5 = 1
I0629 14:05:57.084002 15179 caffe.cpp:313] Batch 166, loss = 0.126141
I0629 14:05:57.092818 15179 caffe.cpp:313] Batch 167, accuracy/top1 = 0.94
I0629 14:05:57.092825 15179 caffe.cpp:313] Batch 167, accuracy/top5 = 1
I0629 14:05:57.092828 15179 caffe.cpp:313] Batch 167, loss = 0.190324
I0629 14:05:57.101606 15179 caffe.cpp:313] Batch 168, accuracy/top1 = 0.88
I0629 14:05:57.101614 15179 caffe.cpp:313] Batch 168, accuracy/top5 = 1
I0629 14:05:57.101617 15179 caffe.cpp:313] Batch 168, loss = 0.430708
I0629 14:05:57.110488 15179 caffe.cpp:313] Batch 169, accuracy/top1 = 0.8
I0629 14:05:57.110507 15179 caffe.cpp:313] Batch 169, accuracy/top5 = 1
I0629 14:05:57.110508 15179 caffe.cpp:313] Batch 169, loss = 0.576801
I0629 14:05:57.119339 15179 caffe.cpp:313] Batch 170, accuracy/top1 = 0.88
I0629 14:05:57.119348 15179 caffe.cpp:313] Batch 170, accuracy/top5 = 1
I0629 14:05:57.119350 15179 caffe.cpp:313] Batch 170, loss = 0.613094
I0629 14:05:57.128175 15179 caffe.cpp:313] Batch 171, accuracy/top1 = 0.86
I0629 14:05:57.128183 15179 caffe.cpp:313] Batch 171, accuracy/top5 = 1
I0629 14:05:57.128185 15179 caffe.cpp:313] Batch 171, loss = 0.557645
I0629 14:05:57.136888 15179 caffe.cpp:313] Batch 172, accuracy/top1 = 0.94
I0629 14:05:57.136900 15179 caffe.cpp:313] Batch 172, accuracy/top5 = 1
I0629 14:05:57.136904 15179 caffe.cpp:313] Batch 172, loss = 0.355065
I0629 14:05:57.145788 15179 caffe.cpp:313] Batch 173, accuracy/top1 = 0.98
I0629 14:05:57.145797 15179 caffe.cpp:313] Batch 173, accuracy/top5 = 1
I0629 14:05:57.145800 15179 caffe.cpp:313] Batch 173, loss = 0.0744457
I0629 14:05:57.154580 15179 caffe.cpp:313] Batch 174, accuracy/top1 = 0.84
I0629 14:05:57.154588 15179 caffe.cpp:313] Batch 174, accuracy/top5 = 1
I0629 14:05:57.154590 15179 caffe.cpp:313] Batch 174, loss = 0.640624
I0629 14:05:57.163444 15179 caffe.cpp:313] Batch 175, accuracy/top1 = 0.94
I0629 14:05:57.163451 15179 caffe.cpp:313] Batch 175, accuracy/top5 = 1
I0629 14:05:57.163453 15179 caffe.cpp:313] Batch 175, loss = 0.264908
I0629 14:05:57.172302 15179 caffe.cpp:313] Batch 176, accuracy/top1 = 0.9
I0629 14:05:57.172315 15179 caffe.cpp:313] Batch 176, accuracy/top5 = 1
I0629 14:05:57.172318 15179 caffe.cpp:313] Batch 176, loss = 0.199097
I0629 14:05:57.181020 15179 caffe.cpp:313] Batch 177, accuracy/top1 = 0.98
I0629 14:05:57.181027 15179 caffe.cpp:313] Batch 177, accuracy/top5 = 1
I0629 14:05:57.181030 15179 caffe.cpp:313] Batch 177, loss = 0.0928101
I0629 14:05:57.189822 15179 caffe.cpp:313] Batch 178, accuracy/top1 = 0.86
I0629 14:05:57.189831 15179 caffe.cpp:313] Batch 178, accuracy/top5 = 1
I0629 14:05:57.189832 15179 caffe.cpp:313] Batch 178, loss = 0.438751
I0629 14:05:57.198642 15179 caffe.cpp:313] Batch 179, accuracy/top1 = 0.92
I0629 14:05:57.198655 15179 caffe.cpp:313] Batch 179, accuracy/top5 = 1
I0629 14:05:57.198657 15179 caffe.cpp:313] Batch 179, loss = 0.379267
I0629 14:05:57.207556 15179 caffe.cpp:313] Batch 180, accuracy/top1 = 0.9
I0629 14:05:57.207566 15179 caffe.cpp:313] Batch 180, accuracy/top5 = 1
I0629 14:05:57.207568 15179 caffe.cpp:313] Batch 180, loss = 0.420223
I0629 14:05:57.216375 15179 caffe.cpp:313] Batch 181, accuracy/top1 = 0.92
I0629 14:05:57.216383 15179 caffe.cpp:313] Batch 181, accuracy/top5 = 1
I0629 14:05:57.216385 15179 caffe.cpp:313] Batch 181, loss = 0.219923
I0629 14:05:57.225240 15179 caffe.cpp:313] Batch 182, accuracy/top1 = 0.9
I0629 14:05:57.225252 15179 caffe.cpp:313] Batch 182, accuracy/top5 = 0.98
I0629 14:05:57.225255 15179 caffe.cpp:313] Batch 182, loss = 0.42175
I0629 14:05:57.234107 15179 caffe.cpp:313] Batch 183, accuracy/top1 = 0.98
I0629 14:05:57.234117 15179 caffe.cpp:313] Batch 183, accuracy/top5 = 1
I0629 14:05:57.234120 15179 caffe.cpp:313] Batch 183, loss = 0.164773
I0629 14:05:57.242983 15179 caffe.cpp:313] Batch 184, accuracy/top1 = 0.88
I0629 14:05:57.242990 15179 caffe.cpp:313] Batch 184, accuracy/top5 = 1
I0629 14:05:57.242993 15179 caffe.cpp:313] Batch 184, loss = 0.862406
I0629 14:05:57.251791 15179 caffe.cpp:313] Batch 185, accuracy/top1 = 0.88
I0629 14:05:57.251798 15179 caffe.cpp:313] Batch 185, accuracy/top5 = 1
I0629 14:05:57.251801 15179 caffe.cpp:313] Batch 185, loss = 0.612708
I0629 14:05:57.260659 15179 caffe.cpp:313] Batch 186, accuracy/top1 = 0.9
I0629 14:05:57.260677 15179 caffe.cpp:313] Batch 186, accuracy/top5 = 1
I0629 14:05:57.260680 15179 caffe.cpp:313] Batch 186, loss = 0.263868
I0629 14:05:57.269518 15179 caffe.cpp:313] Batch 187, accuracy/top1 = 0.84
I0629 14:05:57.269526 15179 caffe.cpp:313] Batch 187, accuracy/top5 = 1
I0629 14:05:57.269529 15179 caffe.cpp:313] Batch 187, loss = 0.628898
I0629 14:05:57.278364 15179 caffe.cpp:313] Batch 188, accuracy/top1 = 0.92
I0629 14:05:57.278372 15179 caffe.cpp:313] Batch 188, accuracy/top5 = 0.98
I0629 14:05:57.278374 15179 caffe.cpp:313] Batch 188, loss = 0.326559
I0629 14:05:57.287170 15179 caffe.cpp:313] Batch 189, accuracy/top1 = 0.9
I0629 14:05:57.287181 15179 caffe.cpp:313] Batch 189, accuracy/top5 = 0.98
I0629 14:05:57.287184 15179 caffe.cpp:313] Batch 189, loss = 0.354482
I0629 14:05:57.296066 15179 caffe.cpp:313] Batch 190, accuracy/top1 = 0.9
I0629 14:05:57.296075 15179 caffe.cpp:313] Batch 190, accuracy/top5 = 1
I0629 14:05:57.296077 15179 caffe.cpp:313] Batch 190, loss = 0.315572
I0629 14:05:57.304891 15179 caffe.cpp:313] Batch 191, accuracy/top1 = 0.9
I0629 14:05:57.304899 15179 caffe.cpp:313] Batch 191, accuracy/top5 = 1
I0629 14:05:57.304901 15179 caffe.cpp:313] Batch 191, loss = 0.25199
I0629 14:05:57.313694 15179 caffe.cpp:313] Batch 192, accuracy/top1 = 0.9
I0629 14:05:57.313701 15179 caffe.cpp:313] Batch 192, accuracy/top5 = 0.98
I0629 14:05:57.313704 15179 caffe.cpp:313] Batch 192, loss = 0.431302
I0629 14:05:57.322582 15179 caffe.cpp:313] Batch 193, accuracy/top1 = 0.96
I0629 14:05:57.322600 15179 caffe.cpp:313] Batch 193, accuracy/top5 = 1
I0629 14:05:57.322603 15179 caffe.cpp:313] Batch 193, loss = 0.0733694
I0629 14:05:57.331430 15179 caffe.cpp:313] Batch 194, accuracy/top1 = 0.92
I0629 14:05:57.331437 15179 caffe.cpp:313] Batch 194, accuracy/top5 = 0.98
I0629 14:05:57.331440 15179 caffe.cpp:313] Batch 194, loss = 0.340824
I0629 14:05:57.340236 15179 caffe.cpp:313] Batch 195, accuracy/top1 = 0.92
I0629 14:05:57.340245 15179 caffe.cpp:313] Batch 195, accuracy/top5 = 1
I0629 14:05:57.340246 15179 caffe.cpp:313] Batch 195, loss = 0.27203
I0629 14:05:57.349057 15179 caffe.cpp:313] Batch 196, accuracy/top1 = 0.8
I0629 14:05:57.349071 15179 caffe.cpp:313] Batch 196, accuracy/top5 = 1
I0629 14:05:57.349073 15179 caffe.cpp:313] Batch 196, loss = 0.646502
I0629 14:05:57.349474 15193 data_reader.cpp:262] Starting prefetch of epoch 1
I0629 14:05:57.357945 15179 caffe.cpp:313] Batch 197, accuracy/top1 = 0.9
I0629 14:05:57.357954 15179 caffe.cpp:313] Batch 197, accuracy/top5 = 1
I0629 14:05:57.357956 15179 caffe.cpp:313] Batch 197, loss = 0.193363
I0629 14:05:57.366761 15179 caffe.cpp:313] Batch 198, accuracy/top1 = 0.92
I0629 14:05:57.366770 15179 caffe.cpp:313] Batch 198, accuracy/top5 = 1
I0629 14:05:57.366771 15179 caffe.cpp:313] Batch 198, loss = 0.222679
I0629 14:05:57.375579 15179 caffe.cpp:313] Batch 199, accuracy/top1 = 0.86
I0629 14:05:57.375596 15179 caffe.cpp:313] Batch 199, accuracy/top5 = 1
I0629 14:05:57.375597 15179 caffe.cpp:313] Batch 199, loss = 0.515246
I0629 14:05:57.375599 15179 caffe.cpp:318] Loss: 0.336866
I0629 14:05:57.375607 15179 caffe.cpp:330] accuracy/top1 = 0.9066
I0629 14:05:57.375612 15179 caffe.cpp:330] accuracy/top5 = 0.9955
I0629 14:05:57.375615 15179 caffe.cpp:330] loss = 0.336866 (* 1 = 0.336866 loss)
