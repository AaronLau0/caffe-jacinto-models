I0704 08:07:47.231745 27993 caffe.cpp:264] Not using GPU #2 for single-GPU function
I0704 08:07:47.231874 27993 caffe.cpp:264] Not using GPU #1 for single-GPU function
I0704 08:07:47.415587 27993 caffe.cpp:273] Use GPU with device ID 0
I0704 08:07:47.415944 27993 caffe.cpp:277] GPU device name: GeForce GTX 1080
I0704 08:07:47.808641 27993 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0704 08:07:47.808768 27993 layer_factory.hpp:77] Creating layer data
I0704 08:07:47.809132 27993 net.cpp:98] Creating Layer data
I0704 08:07:47.809140 27993 net.cpp:413] data -> data
I0704 08:07:47.809157 27993 net.cpp:413] data -> label
I0704 08:07:47.809940 28013 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0704 08:07:47.810580 27993 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0704 08:07:47.810611 27993 data_layer.cpp:83] output data size: 50,3,32,32
I0704 08:07:47.812201 27993 net.cpp:148] Setting up data
I0704 08:07:47.812211 27993 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0704 08:07:47.812216 27993 net.cpp:155] Top shape: 50 (50)
I0704 08:07:47.812217 27993 net.cpp:163] Memory required for data: 614600
I0704 08:07:47.812222 27993 layer_factory.hpp:77] Creating layer label_data_1_split
I0704 08:07:47.812232 27993 net.cpp:98] Creating Layer label_data_1_split
I0704 08:07:47.812234 27993 net.cpp:439] label_data_1_split <- label
I0704 08:07:47.812242 27993 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0704 08:07:47.812245 27993 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0704 08:07:47.812248 27993 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0704 08:07:47.812327 27993 net.cpp:148] Setting up label_data_1_split
I0704 08:07:47.812337 27993 net.cpp:155] Top shape: 50 (50)
I0704 08:07:47.812340 27993 net.cpp:155] Top shape: 50 (50)
I0704 08:07:47.812342 27993 net.cpp:155] Top shape: 50 (50)
I0704 08:07:47.812345 27993 net.cpp:163] Memory required for data: 615200
I0704 08:07:47.812348 27993 layer_factory.hpp:77] Creating layer data/bias
I0704 08:07:47.812356 27993 net.cpp:98] Creating Layer data/bias
I0704 08:07:47.812360 27993 net.cpp:439] data/bias <- data
I0704 08:07:47.812362 27993 net.cpp:413] data/bias -> data/bias
I0704 08:07:47.812966 27993 net.cpp:148] Setting up data/bias
I0704 08:07:47.812974 27993 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0704 08:07:47.812976 27993 net.cpp:163] Memory required for data: 1229600
I0704 08:07:47.812985 27993 layer_factory.hpp:77] Creating layer conv1a
I0704 08:07:47.812994 27993 net.cpp:98] Creating Layer conv1a
I0704 08:07:47.812996 27993 net.cpp:439] conv1a <- data/bias
I0704 08:07:47.812999 27993 net.cpp:413] conv1a -> conv1a
I0704 08:07:47.813902 27993 net.cpp:148] Setting up conv1a
I0704 08:07:47.813911 27993 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 08:07:47.813915 27993 net.cpp:163] Memory required for data: 7783200
I0704 08:07:47.813918 27993 layer_factory.hpp:77] Creating layer conv1a/bn
I0704 08:07:47.813925 27993 net.cpp:98] Creating Layer conv1a/bn
I0704 08:07:47.813926 27993 net.cpp:439] conv1a/bn <- conv1a
I0704 08:07:47.813930 27993 net.cpp:413] conv1a/bn -> conv1a/bn
I0704 08:07:47.814249 27993 net.cpp:148] Setting up conv1a/bn
I0704 08:07:47.814256 27993 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 08:07:47.814258 27993 net.cpp:163] Memory required for data: 14336800
I0704 08:07:47.814265 27993 layer_factory.hpp:77] Creating layer conv1a/relu
I0704 08:07:47.814267 27993 net.cpp:98] Creating Layer conv1a/relu
I0704 08:07:47.814270 27993 net.cpp:439] conv1a/relu <- conv1a/bn
I0704 08:07:47.814272 27993 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0704 08:07:47.814280 27993 net.cpp:148] Setting up conv1a/relu
I0704 08:07:47.814283 27993 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 08:07:47.814285 27993 net.cpp:163] Memory required for data: 20890400
I0704 08:07:47.814287 27993 layer_factory.hpp:77] Creating layer conv1b
I0704 08:07:47.814291 27993 net.cpp:98] Creating Layer conv1b
I0704 08:07:47.814307 27993 net.cpp:439] conv1b <- conv1a/bn
I0704 08:07:47.814311 27993 net.cpp:413] conv1b -> conv1b
I0704 08:07:47.814503 27993 net.cpp:148] Setting up conv1b
I0704 08:07:47.814509 27993 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 08:07:47.814512 27993 net.cpp:163] Memory required for data: 27444000
I0704 08:07:47.814515 27993 layer_factory.hpp:77] Creating layer conv1b/bn
I0704 08:07:47.814519 27993 net.cpp:98] Creating Layer conv1b/bn
I0704 08:07:47.814522 27993 net.cpp:439] conv1b/bn <- conv1b
I0704 08:07:47.814523 27993 net.cpp:413] conv1b/bn -> conv1b/bn
I0704 08:07:47.814816 27993 net.cpp:148] Setting up conv1b/bn
I0704 08:07:47.814821 27993 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 08:07:47.814823 27993 net.cpp:163] Memory required for data: 33997600
I0704 08:07:47.814828 27993 layer_factory.hpp:77] Creating layer conv1b/relu
I0704 08:07:47.814831 27993 net.cpp:98] Creating Layer conv1b/relu
I0704 08:07:47.814833 27993 net.cpp:439] conv1b/relu <- conv1b/bn
I0704 08:07:47.814836 27993 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0704 08:07:47.814839 27993 net.cpp:148] Setting up conv1b/relu
I0704 08:07:47.814841 27993 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 08:07:47.814843 27993 net.cpp:163] Memory required for data: 40551200
I0704 08:07:47.814844 27993 layer_factory.hpp:77] Creating layer pool1
I0704 08:07:47.814849 27993 net.cpp:98] Creating Layer pool1
I0704 08:07:47.814851 27993 net.cpp:439] pool1 <- conv1b/bn
I0704 08:07:47.814854 27993 net.cpp:413] pool1 -> pool1
I0704 08:07:47.814882 27993 net.cpp:148] Setting up pool1
I0704 08:07:47.814885 27993 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 08:07:47.814888 27993 net.cpp:163] Memory required for data: 47104800
I0704 08:07:47.814889 27993 layer_factory.hpp:77] Creating layer res2a_branch2a
I0704 08:07:47.814900 27993 net.cpp:98] Creating Layer res2a_branch2a
I0704 08:07:47.814903 27993 net.cpp:439] res2a_branch2a <- pool1
I0704 08:07:47.814905 27993 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0704 08:07:47.816004 27993 net.cpp:148] Setting up res2a_branch2a
I0704 08:07:47.816012 27993 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 08:07:47.816015 27993 net.cpp:163] Memory required for data: 60212000
I0704 08:07:47.816020 27993 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0704 08:07:47.816023 27993 net.cpp:98] Creating Layer res2a_branch2a/bn
I0704 08:07:47.816025 27993 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0704 08:07:47.816028 27993 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0704 08:07:47.816316 27993 net.cpp:148] Setting up res2a_branch2a/bn
I0704 08:07:47.816323 27993 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 08:07:47.816324 27993 net.cpp:163] Memory required for data: 73319200
I0704 08:07:47.816329 27993 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0704 08:07:47.816331 27993 net.cpp:98] Creating Layer res2a_branch2a/relu
I0704 08:07:47.816334 27993 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0704 08:07:47.816336 27993 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0704 08:07:47.816339 27993 net.cpp:148] Setting up res2a_branch2a/relu
I0704 08:07:47.816342 27993 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 08:07:47.816344 27993 net.cpp:163] Memory required for data: 86426400
I0704 08:07:47.816345 27993 layer_factory.hpp:77] Creating layer res2a_branch2b
I0704 08:07:47.816349 27993 net.cpp:98] Creating Layer res2a_branch2b
I0704 08:07:47.816351 27993 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0704 08:07:47.816354 27993 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0704 08:07:47.817205 27993 net.cpp:148] Setting up res2a_branch2b
I0704 08:07:47.817214 27993 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 08:07:47.817215 27993 net.cpp:163] Memory required for data: 99533600
I0704 08:07:47.817219 27993 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0704 08:07:47.817224 27993 net.cpp:98] Creating Layer res2a_branch2b/bn
I0704 08:07:47.817226 27993 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0704 08:07:47.817235 27993 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0704 08:07:47.817610 27993 net.cpp:148] Setting up res2a_branch2b/bn
I0704 08:07:47.817617 27993 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 08:07:47.817620 27993 net.cpp:163] Memory required for data: 112640800
I0704 08:07:47.817625 27993 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0704 08:07:47.817627 27993 net.cpp:98] Creating Layer res2a_branch2b/relu
I0704 08:07:47.817629 27993 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0704 08:07:47.817632 27993 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0704 08:07:47.817636 27993 net.cpp:148] Setting up res2a_branch2b/relu
I0704 08:07:47.817638 27993 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 08:07:47.817639 27993 net.cpp:163] Memory required for data: 125748000
I0704 08:07:47.817641 27993 layer_factory.hpp:77] Creating layer pool2
I0704 08:07:47.817646 27993 net.cpp:98] Creating Layer pool2
I0704 08:07:47.817647 27993 net.cpp:439] pool2 <- res2a_branch2b/bn
I0704 08:07:47.817651 27993 net.cpp:413] pool2 -> pool2
I0704 08:07:47.817667 27993 net.cpp:148] Setting up pool2
I0704 08:07:47.817672 27993 net.cpp:155] Top shape: 50 64 16 16 (819200)
I0704 08:07:47.817673 27993 net.cpp:163] Memory required for data: 129024800
I0704 08:07:47.817675 27993 layer_factory.hpp:77] Creating layer res3a_branch2a
I0704 08:07:47.817682 27993 net.cpp:98] Creating Layer res3a_branch2a
I0704 08:07:47.817684 27993 net.cpp:439] res3a_branch2a <- pool2
I0704 08:07:47.817687 27993 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0704 08:07:47.819794 27993 net.cpp:148] Setting up res3a_branch2a
I0704 08:07:47.819803 27993 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 08:07:47.819805 27993 net.cpp:163] Memory required for data: 135578400
I0704 08:07:47.819809 27993 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0704 08:07:47.819818 27993 net.cpp:98] Creating Layer res3a_branch2a/bn
I0704 08:07:47.819819 27993 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0704 08:07:47.819823 27993 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0704 08:07:47.820077 27993 net.cpp:148] Setting up res3a_branch2a/bn
I0704 08:07:47.820082 27993 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 08:07:47.820085 27993 net.cpp:163] Memory required for data: 142132000
I0704 08:07:47.820091 27993 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0704 08:07:47.820094 27993 net.cpp:98] Creating Layer res3a_branch2a/relu
I0704 08:07:47.820097 27993 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0704 08:07:47.820099 27993 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0704 08:07:47.820102 27993 net.cpp:148] Setting up res3a_branch2a/relu
I0704 08:07:47.820106 27993 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 08:07:47.820106 27993 net.cpp:163] Memory required for data: 148685600
I0704 08:07:47.820108 27993 layer_factory.hpp:77] Creating layer res3a_branch2b
I0704 08:07:47.820112 27993 net.cpp:98] Creating Layer res3a_branch2b
I0704 08:07:47.820116 27993 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0704 08:07:47.820118 27993 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0704 08:07:47.820969 27993 net.cpp:148] Setting up res3a_branch2b
I0704 08:07:47.820976 27993 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 08:07:47.820977 27993 net.cpp:163] Memory required for data: 155239200
I0704 08:07:47.820981 27993 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0704 08:07:47.820984 27993 net.cpp:98] Creating Layer res3a_branch2b/bn
I0704 08:07:47.820987 27993 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0704 08:07:47.820989 27993 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0704 08:07:47.821244 27993 net.cpp:148] Setting up res3a_branch2b/bn
I0704 08:07:47.821249 27993 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 08:07:47.821250 27993 net.cpp:163] Memory required for data: 161792800
I0704 08:07:47.821254 27993 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0704 08:07:47.821264 27993 net.cpp:98] Creating Layer res3a_branch2b/relu
I0704 08:07:47.821267 27993 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0704 08:07:47.821269 27993 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0704 08:07:47.821274 27993 net.cpp:148] Setting up res3a_branch2b/relu
I0704 08:07:47.821276 27993 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 08:07:47.821279 27993 net.cpp:163] Memory required for data: 168346400
I0704 08:07:47.821280 27993 layer_factory.hpp:77] Creating layer pool3
I0704 08:07:47.821285 27993 net.cpp:98] Creating Layer pool3
I0704 08:07:47.821286 27993 net.cpp:439] pool3 <- res3a_branch2b/bn
I0704 08:07:47.821288 27993 net.cpp:413] pool3 -> pool3
I0704 08:07:47.821310 27993 net.cpp:148] Setting up pool3
I0704 08:07:47.821315 27993 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 08:07:47.821316 27993 net.cpp:163] Memory required for data: 174900000
I0704 08:07:47.821318 27993 layer_factory.hpp:77] Creating layer res4a_branch2a
I0704 08:07:47.821322 27993 net.cpp:98] Creating Layer res4a_branch2a
I0704 08:07:47.821324 27993 net.cpp:439] res4a_branch2a <- pool3
I0704 08:07:47.821327 27993 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0704 08:07:47.827198 27993 net.cpp:148] Setting up res4a_branch2a
I0704 08:07:47.827204 27993 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 08:07:47.827206 27993 net.cpp:163] Memory required for data: 188007200
I0704 08:07:47.827209 27993 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0704 08:07:47.827214 27993 net.cpp:98] Creating Layer res4a_branch2a/bn
I0704 08:07:47.827216 27993 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0704 08:07:47.827219 27993 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0704 08:07:47.827472 27993 net.cpp:148] Setting up res4a_branch2a/bn
I0704 08:07:47.827477 27993 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 08:07:47.827479 27993 net.cpp:163] Memory required for data: 201114400
I0704 08:07:47.827486 27993 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0704 08:07:47.827488 27993 net.cpp:98] Creating Layer res4a_branch2a/relu
I0704 08:07:47.827491 27993 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0704 08:07:47.827494 27993 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0704 08:07:47.827498 27993 net.cpp:148] Setting up res4a_branch2a/relu
I0704 08:07:47.827502 27993 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 08:07:47.827503 27993 net.cpp:163] Memory required for data: 214221600
I0704 08:07:47.827505 27993 layer_factory.hpp:77] Creating layer res4a_branch2b
I0704 08:07:47.827510 27993 net.cpp:98] Creating Layer res4a_branch2b
I0704 08:07:47.827512 27993 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0704 08:07:47.827515 27993 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0704 08:07:47.830515 27993 net.cpp:148] Setting up res4a_branch2b
I0704 08:07:47.830520 27993 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 08:07:47.830523 27993 net.cpp:163] Memory required for data: 227328800
I0704 08:07:47.830526 27993 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0704 08:07:47.830530 27993 net.cpp:98] Creating Layer res4a_branch2b/bn
I0704 08:07:47.830533 27993 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0704 08:07:47.830538 27993 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0704 08:07:47.830788 27993 net.cpp:148] Setting up res4a_branch2b/bn
I0704 08:07:47.830792 27993 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 08:07:47.830796 27993 net.cpp:163] Memory required for data: 240436000
I0704 08:07:47.830801 27993 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0704 08:07:47.830803 27993 net.cpp:98] Creating Layer res4a_branch2b/relu
I0704 08:07:47.830806 27993 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0704 08:07:47.830809 27993 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0704 08:07:47.830813 27993 net.cpp:148] Setting up res4a_branch2b/relu
I0704 08:07:47.830816 27993 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 08:07:47.830823 27993 net.cpp:163] Memory required for data: 253543200
I0704 08:07:47.830826 27993 layer_factory.hpp:77] Creating layer pool4
I0704 08:07:47.830831 27993 net.cpp:98] Creating Layer pool4
I0704 08:07:47.830833 27993 net.cpp:439] pool4 <- res4a_branch2b/bn
I0704 08:07:47.830835 27993 net.cpp:413] pool4 -> pool4
I0704 08:07:47.830853 27993 net.cpp:148] Setting up pool4
I0704 08:07:47.830857 27993 net.cpp:155] Top shape: 50 256 8 8 (819200)
I0704 08:07:47.830859 27993 net.cpp:163] Memory required for data: 256820000
I0704 08:07:47.830862 27993 layer_factory.hpp:77] Creating layer res5a_branch2a
I0704 08:07:47.830865 27993 net.cpp:98] Creating Layer res5a_branch2a
I0704 08:07:47.830868 27993 net.cpp:439] res5a_branch2a <- pool4
I0704 08:07:47.830871 27993 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0704 08:07:47.854976 27993 net.cpp:148] Setting up res5a_branch2a
I0704 08:07:47.854991 27993 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 08:07:47.854995 27993 net.cpp:163] Memory required for data: 263373600
I0704 08:07:47.855000 27993 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0704 08:07:47.855006 27993 net.cpp:98] Creating Layer res5a_branch2a/bn
I0704 08:07:47.855010 27993 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0704 08:07:47.855013 27993 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0704 08:07:47.855293 27993 net.cpp:148] Setting up res5a_branch2a/bn
I0704 08:07:47.855298 27993 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 08:07:47.855300 27993 net.cpp:163] Memory required for data: 269927200
I0704 08:07:47.855305 27993 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0704 08:07:47.855309 27993 net.cpp:98] Creating Layer res5a_branch2a/relu
I0704 08:07:47.855310 27993 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0704 08:07:47.855314 27993 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0704 08:07:47.855316 27993 net.cpp:148] Setting up res5a_branch2a/relu
I0704 08:07:47.855319 27993 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 08:07:47.855320 27993 net.cpp:163] Memory required for data: 276480800
I0704 08:07:47.855322 27993 layer_factory.hpp:77] Creating layer res5a_branch2b
I0704 08:07:47.855326 27993 net.cpp:98] Creating Layer res5a_branch2b
I0704 08:07:47.855329 27993 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0704 08:07:47.855331 27993 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0704 08:07:47.867575 27993 net.cpp:148] Setting up res5a_branch2b
I0704 08:07:47.867584 27993 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 08:07:47.867588 27993 net.cpp:163] Memory required for data: 283034400
I0704 08:07:47.867594 27993 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0704 08:07:47.867599 27993 net.cpp:98] Creating Layer res5a_branch2b/bn
I0704 08:07:47.867602 27993 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0704 08:07:47.867605 27993 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0704 08:07:47.867892 27993 net.cpp:148] Setting up res5a_branch2b/bn
I0704 08:07:47.867897 27993 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 08:07:47.867899 27993 net.cpp:163] Memory required for data: 289588000
I0704 08:07:47.867904 27993 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0704 08:07:47.867908 27993 net.cpp:98] Creating Layer res5a_branch2b/relu
I0704 08:07:47.867910 27993 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0704 08:07:47.867913 27993 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0704 08:07:47.867916 27993 net.cpp:148] Setting up res5a_branch2b/relu
I0704 08:07:47.867918 27993 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 08:07:47.867920 27993 net.cpp:163] Memory required for data: 296141600
I0704 08:07:47.867921 27993 layer_factory.hpp:77] Creating layer pool5
I0704 08:07:47.867928 27993 net.cpp:98] Creating Layer pool5
I0704 08:07:47.867929 27993 net.cpp:439] pool5 <- res5a_branch2b/bn
I0704 08:07:47.867931 27993 net.cpp:413] pool5 -> pool5
I0704 08:07:47.867949 27993 net.cpp:148] Setting up pool5
I0704 08:07:47.867954 27993 net.cpp:155] Top shape: 50 512 1 1 (25600)
I0704 08:07:47.867962 27993 net.cpp:163] Memory required for data: 296244000
I0704 08:07:47.867964 27993 layer_factory.hpp:77] Creating layer fc10
I0704 08:07:47.867972 27993 net.cpp:98] Creating Layer fc10
I0704 08:07:47.867974 27993 net.cpp:439] fc10 <- pool5
I0704 08:07:47.867977 27993 net.cpp:413] fc10 -> fc10
I0704 08:07:47.868134 27993 net.cpp:148] Setting up fc10
I0704 08:07:47.868139 27993 net.cpp:155] Top shape: 50 10 (500)
I0704 08:07:47.868140 27993 net.cpp:163] Memory required for data: 296246000
I0704 08:07:47.868144 27993 layer_factory.hpp:77] Creating layer fc10_fc10_0_split
I0704 08:07:47.868147 27993 net.cpp:98] Creating Layer fc10_fc10_0_split
I0704 08:07:47.868149 27993 net.cpp:439] fc10_fc10_0_split <- fc10
I0704 08:07:47.868152 27993 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0704 08:07:47.868155 27993 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0704 08:07:47.868158 27993 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0704 08:07:47.868188 27993 net.cpp:148] Setting up fc10_fc10_0_split
I0704 08:07:47.868192 27993 net.cpp:155] Top shape: 50 10 (500)
I0704 08:07:47.868194 27993 net.cpp:155] Top shape: 50 10 (500)
I0704 08:07:47.868196 27993 net.cpp:155] Top shape: 50 10 (500)
I0704 08:07:47.868198 27993 net.cpp:163] Memory required for data: 296252000
I0704 08:07:47.868201 27993 layer_factory.hpp:77] Creating layer loss
I0704 08:07:47.868206 27993 net.cpp:98] Creating Layer loss
I0704 08:07:47.868207 27993 net.cpp:439] loss <- fc10_fc10_0_split_0
I0704 08:07:47.868211 27993 net.cpp:439] loss <- label_data_1_split_0
I0704 08:07:47.868212 27993 net.cpp:413] loss -> loss
I0704 08:07:47.868217 27993 layer_factory.hpp:77] Creating layer loss
I0704 08:07:47.868268 27993 net.cpp:148] Setting up loss
I0704 08:07:47.868271 27993 net.cpp:155] Top shape: (1)
I0704 08:07:47.868273 27993 net.cpp:158]     with loss weight 1
I0704 08:07:47.868285 27993 net.cpp:163] Memory required for data: 296252004
I0704 08:07:47.868288 27993 layer_factory.hpp:77] Creating layer accuracy/top1
I0704 08:07:47.868291 27993 net.cpp:98] Creating Layer accuracy/top1
I0704 08:07:47.868294 27993 net.cpp:439] accuracy/top1 <- fc10_fc10_0_split_1
I0704 08:07:47.868296 27993 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0704 08:07:47.868299 27993 net.cpp:413] accuracy/top1 -> accuracy/top1
I0704 08:07:47.868305 27993 net.cpp:148] Setting up accuracy/top1
I0704 08:07:47.868309 27993 net.cpp:155] Top shape: (1)
I0704 08:07:47.868310 27993 net.cpp:163] Memory required for data: 296252008
I0704 08:07:47.868311 27993 layer_factory.hpp:77] Creating layer accuracy/top5
I0704 08:07:47.868315 27993 net.cpp:98] Creating Layer accuracy/top5
I0704 08:07:47.868317 27993 net.cpp:439] accuracy/top5 <- fc10_fc10_0_split_2
I0704 08:07:47.868320 27993 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0704 08:07:47.868322 27993 net.cpp:413] accuracy/top5 -> accuracy/top5
I0704 08:07:47.868327 27993 net.cpp:148] Setting up accuracy/top5
I0704 08:07:47.868330 27993 net.cpp:155] Top shape: (1)
I0704 08:07:47.868332 27993 net.cpp:163] Memory required for data: 296252012
I0704 08:07:47.868335 27993 net.cpp:226] accuracy/top5 does not need backward computation.
I0704 08:07:47.868337 27993 net.cpp:226] accuracy/top1 does not need backward computation.
I0704 08:07:47.868340 27993 net.cpp:224] loss needs backward computation.
I0704 08:07:47.868342 27993 net.cpp:224] fc10_fc10_0_split needs backward computation.
I0704 08:07:47.868345 27993 net.cpp:224] fc10 needs backward computation.
I0704 08:07:47.868346 27993 net.cpp:224] pool5 needs backward computation.
I0704 08:07:47.868350 27993 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0704 08:07:47.868352 27993 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0704 08:07:47.868355 27993 net.cpp:224] res5a_branch2b needs backward computation.
I0704 08:07:47.868356 27993 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0704 08:07:47.868360 27993 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0704 08:07:47.868366 27993 net.cpp:224] res5a_branch2a needs backward computation.
I0704 08:07:47.868368 27993 net.cpp:224] pool4 needs backward computation.
I0704 08:07:47.868371 27993 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0704 08:07:47.868374 27993 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0704 08:07:47.868377 27993 net.cpp:224] res4a_branch2b needs backward computation.
I0704 08:07:47.868379 27993 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0704 08:07:47.868382 27993 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0704 08:07:47.868384 27993 net.cpp:224] res4a_branch2a needs backward computation.
I0704 08:07:47.868387 27993 net.cpp:224] pool3 needs backward computation.
I0704 08:07:47.868391 27993 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0704 08:07:47.868393 27993 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0704 08:07:47.868396 27993 net.cpp:224] res3a_branch2b needs backward computation.
I0704 08:07:47.868398 27993 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0704 08:07:47.868401 27993 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0704 08:07:47.868403 27993 net.cpp:224] res3a_branch2a needs backward computation.
I0704 08:07:47.868405 27993 net.cpp:224] pool2 needs backward computation.
I0704 08:07:47.868407 27993 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0704 08:07:47.868410 27993 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0704 08:07:47.868413 27993 net.cpp:224] res2a_branch2b needs backward computation.
I0704 08:07:47.868415 27993 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0704 08:07:47.868418 27993 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0704 08:07:47.868420 27993 net.cpp:224] res2a_branch2a needs backward computation.
I0704 08:07:47.868423 27993 net.cpp:224] pool1 needs backward computation.
I0704 08:07:47.868425 27993 net.cpp:224] conv1b/relu needs backward computation.
I0704 08:07:47.868427 27993 net.cpp:224] conv1b/bn needs backward computation.
I0704 08:07:47.868430 27993 net.cpp:224] conv1b needs backward computation.
I0704 08:07:47.868432 27993 net.cpp:224] conv1a/relu needs backward computation.
I0704 08:07:47.868435 27993 net.cpp:224] conv1a/bn needs backward computation.
I0704 08:07:47.868438 27993 net.cpp:224] conv1a needs backward computation.
I0704 08:07:47.868440 27993 net.cpp:226] data/bias does not need backward computation.
I0704 08:07:47.868443 27993 net.cpp:226] label_data_1_split does not need backward computation.
I0704 08:07:47.868446 27993 net.cpp:226] data does not need backward computation.
I0704 08:07:47.868448 27993 net.cpp:268] This network produces output accuracy/top1
I0704 08:07:47.868450 27993 net.cpp:268] This network produces output accuracy/top5
I0704 08:07:47.868453 27993 net.cpp:268] This network produces output loss
I0704 08:07:47.868472 27993 net.cpp:288] Network initialization done.
I0704 08:07:47.879020 27993 caffe.cpp:289] Running for 200 iterations.
I0704 08:07:47.901266 27993 caffe.cpp:312] Batch 0, accuracy/top1 = 0.96
I0704 08:07:47.901284 27993 caffe.cpp:312] Batch 0, accuracy/top5 = 1
I0704 08:07:47.901288 27993 caffe.cpp:312] Batch 0, loss = 0.08
I0704 08:07:47.909477 27993 caffe.cpp:312] Batch 1, accuracy/top1 = 0.92
I0704 08:07:47.909487 27993 caffe.cpp:312] Batch 1, accuracy/top5 = 1
I0704 08:07:47.909488 27993 caffe.cpp:312] Batch 1, loss = 0.14
I0704 08:07:47.917752 27993 caffe.cpp:312] Batch 2, accuracy/top1 = 0.9
I0704 08:07:47.917760 27993 caffe.cpp:312] Batch 2, accuracy/top5 = 1
I0704 08:07:47.917763 27993 caffe.cpp:312] Batch 2, loss = 0.18
I0704 08:07:47.925891 27993 caffe.cpp:312] Batch 3, accuracy/top1 = 0.9
I0704 08:07:47.925899 27993 caffe.cpp:312] Batch 3, accuracy/top5 = 1
I0704 08:07:47.925902 27993 caffe.cpp:312] Batch 3, loss = 0.1
I0704 08:07:47.934108 27993 caffe.cpp:312] Batch 4, accuracy/top1 = 0.88
I0704 08:07:47.934116 27993 caffe.cpp:312] Batch 4, accuracy/top5 = 1
I0704 08:07:47.934119 27993 caffe.cpp:312] Batch 4, loss = 0.16
I0704 08:07:47.942284 27993 caffe.cpp:312] Batch 5, accuracy/top1 = 0.92
I0704 08:07:47.942292 27993 caffe.cpp:312] Batch 5, accuracy/top5 = 1
I0704 08:07:47.942296 27993 caffe.cpp:312] Batch 5, loss = 0.16
I0704 08:07:47.950485 27993 caffe.cpp:312] Batch 6, accuracy/top1 = 0.9
I0704 08:07:47.950492 27993 caffe.cpp:312] Batch 6, accuracy/top5 = 1
I0704 08:07:47.950495 27993 caffe.cpp:312] Batch 6, loss = 0.18
I0704 08:07:47.958602 27993 caffe.cpp:312] Batch 7, accuracy/top1 = 0.9
I0704 08:07:47.958611 27993 caffe.cpp:312] Batch 7, accuracy/top5 = 1
I0704 08:07:47.958613 27993 caffe.cpp:312] Batch 7, loss = 0.26
I0704 08:07:47.966838 27993 caffe.cpp:312] Batch 8, accuracy/top1 = 0.88
I0704 08:07:47.966846 27993 caffe.cpp:312] Batch 8, accuracy/top5 = 1
I0704 08:07:47.966848 27993 caffe.cpp:312] Batch 8, loss = 0.14
I0704 08:07:47.975044 27993 caffe.cpp:312] Batch 9, accuracy/top1 = 0.86
I0704 08:07:47.975050 27993 caffe.cpp:312] Batch 9, accuracy/top5 = 1
I0704 08:07:47.975054 27993 caffe.cpp:312] Batch 9, loss = 0.16
I0704 08:07:47.983283 27993 caffe.cpp:312] Batch 10, accuracy/top1 = 0.94
I0704 08:07:47.983289 27993 caffe.cpp:312] Batch 10, accuracy/top5 = 1
I0704 08:07:47.983292 27993 caffe.cpp:312] Batch 10, loss = 0.12
I0704 08:07:47.991459 27993 caffe.cpp:312] Batch 11, accuracy/top1 = 0.98
I0704 08:07:47.991466 27993 caffe.cpp:312] Batch 11, accuracy/top5 = 1
I0704 08:07:47.991469 27993 caffe.cpp:312] Batch 11, loss = 0.08
I0704 08:07:47.999577 27993 caffe.cpp:312] Batch 12, accuracy/top1 = 0.96
I0704 08:07:47.999584 27993 caffe.cpp:312] Batch 12, accuracy/top5 = 1
I0704 08:07:47.999588 27993 caffe.cpp:312] Batch 12, loss = 0.04
I0704 08:07:48.007774 27993 caffe.cpp:312] Batch 13, accuracy/top1 = 0.88
I0704 08:07:48.007782 27993 caffe.cpp:312] Batch 13, accuracy/top5 = 1
I0704 08:07:48.007786 27993 caffe.cpp:312] Batch 13, loss = 0.26
I0704 08:07:48.015961 27993 caffe.cpp:312] Batch 14, accuracy/top1 = 0.94
I0704 08:07:48.015969 27993 caffe.cpp:312] Batch 14, accuracy/top5 = 1
I0704 08:07:48.015971 27993 caffe.cpp:312] Batch 14, loss = 0.18
I0704 08:07:48.024180 27993 caffe.cpp:312] Batch 15, accuracy/top1 = 0.86
I0704 08:07:48.024188 27993 caffe.cpp:312] Batch 15, accuracy/top5 = 1
I0704 08:07:48.024191 27993 caffe.cpp:312] Batch 15, loss = 0.3
I0704 08:07:48.032402 27993 caffe.cpp:312] Batch 16, accuracy/top1 = 0.9
I0704 08:07:48.032409 27993 caffe.cpp:312] Batch 16, accuracy/top5 = 0.98
I0704 08:07:48.032413 27993 caffe.cpp:312] Batch 16, loss = 0.34
I0704 08:07:48.040560 27993 caffe.cpp:312] Batch 17, accuracy/top1 = 0.9
I0704 08:07:48.040568 27993 caffe.cpp:312] Batch 17, accuracy/top5 = 1
I0704 08:07:48.040571 27993 caffe.cpp:312] Batch 17, loss = 0.26
I0704 08:07:48.048737 27993 caffe.cpp:312] Batch 18, accuracy/top1 = 0.94
I0704 08:07:48.048743 27993 caffe.cpp:312] Batch 18, accuracy/top5 = 1
I0704 08:07:48.048746 27993 caffe.cpp:312] Batch 18, loss = 0.14
I0704 08:07:48.056922 27993 caffe.cpp:312] Batch 19, accuracy/top1 = 0.92
I0704 08:07:48.056929 27993 caffe.cpp:312] Batch 19, accuracy/top5 = 1
I0704 08:07:48.056932 27993 caffe.cpp:312] Batch 19, loss = 0.1
I0704 08:07:48.065099 27993 caffe.cpp:312] Batch 20, accuracy/top1 = 0.94
I0704 08:07:48.065106 27993 caffe.cpp:312] Batch 20, accuracy/top5 = 1
I0704 08:07:48.065109 27993 caffe.cpp:312] Batch 20, loss = 0.14
I0704 08:07:48.073314 27993 caffe.cpp:312] Batch 21, accuracy/top1 = 0.88
I0704 08:07:48.073321 27993 caffe.cpp:312] Batch 21, accuracy/top5 = 1
I0704 08:07:48.073324 27993 caffe.cpp:312] Batch 21, loss = 0.16
I0704 08:07:48.081497 27993 caffe.cpp:312] Batch 22, accuracy/top1 = 0.9
I0704 08:07:48.081504 27993 caffe.cpp:312] Batch 22, accuracy/top5 = 1
I0704 08:07:48.081507 27993 caffe.cpp:312] Batch 22, loss = 0.26
I0704 08:07:48.089696 27993 caffe.cpp:312] Batch 23, accuracy/top1 = 0.9
I0704 08:07:48.089704 27993 caffe.cpp:312] Batch 23, accuracy/top5 = 1
I0704 08:07:48.089706 27993 caffe.cpp:312] Batch 23, loss = 0.2
I0704 08:07:48.097893 27993 caffe.cpp:312] Batch 24, accuracy/top1 = 0.88
I0704 08:07:48.097901 27993 caffe.cpp:312] Batch 24, accuracy/top5 = 1
I0704 08:07:48.097913 27993 caffe.cpp:312] Batch 24, loss = 0.24
I0704 08:07:48.106073 27993 caffe.cpp:312] Batch 25, accuracy/top1 = 0.98
I0704 08:07:48.106081 27993 caffe.cpp:312] Batch 25, accuracy/top5 = 1
I0704 08:07:48.106083 27993 caffe.cpp:312] Batch 25, loss = 0.02
I0704 08:07:48.114256 27993 caffe.cpp:312] Batch 26, accuracy/top1 = 0.9
I0704 08:07:48.114264 27993 caffe.cpp:312] Batch 26, accuracy/top5 = 1
I0704 08:07:48.114266 27993 caffe.cpp:312] Batch 26, loss = 0.32
I0704 08:07:48.122360 27993 caffe.cpp:312] Batch 27, accuracy/top1 = 0.92
I0704 08:07:48.122369 27993 caffe.cpp:312] Batch 27, accuracy/top5 = 1
I0704 08:07:48.122370 27993 caffe.cpp:312] Batch 27, loss = 0.16
I0704 08:07:48.130573 27993 caffe.cpp:312] Batch 28, accuracy/top1 = 0.94
I0704 08:07:48.130581 27993 caffe.cpp:312] Batch 28, accuracy/top5 = 1
I0704 08:07:48.130584 27993 caffe.cpp:312] Batch 28, loss = 0.06
I0704 08:07:48.138772 27993 caffe.cpp:312] Batch 29, accuracy/top1 = 0.88
I0704 08:07:48.138778 27993 caffe.cpp:312] Batch 29, accuracy/top5 = 1
I0704 08:07:48.138782 27993 caffe.cpp:312] Batch 29, loss = 0.26
I0704 08:07:48.146925 27993 caffe.cpp:312] Batch 30, accuracy/top1 = 0.9
I0704 08:07:48.146934 27993 caffe.cpp:312] Batch 30, accuracy/top5 = 1
I0704 08:07:48.146935 27993 caffe.cpp:312] Batch 30, loss = 0.28
I0704 08:07:48.155120 27993 caffe.cpp:312] Batch 31, accuracy/top1 = 0.92
I0704 08:07:48.155128 27993 caffe.cpp:312] Batch 31, accuracy/top5 = 1
I0704 08:07:48.155130 27993 caffe.cpp:312] Batch 31, loss = 0.18
I0704 08:07:48.163326 27993 caffe.cpp:312] Batch 32, accuracy/top1 = 0.94
I0704 08:07:48.163333 27993 caffe.cpp:312] Batch 32, accuracy/top5 = 1
I0704 08:07:48.163336 27993 caffe.cpp:312] Batch 32, loss = 0.1
I0704 08:07:48.171499 27993 caffe.cpp:312] Batch 33, accuracy/top1 = 0.9
I0704 08:07:48.171505 27993 caffe.cpp:312] Batch 33, accuracy/top5 = 1
I0704 08:07:48.171509 27993 caffe.cpp:312] Batch 33, loss = 0.24
I0704 08:07:48.179610 27993 caffe.cpp:312] Batch 34, accuracy/top1 = 0.92
I0704 08:07:48.179617 27993 caffe.cpp:312] Batch 34, accuracy/top5 = 1
I0704 08:07:48.179620 27993 caffe.cpp:312] Batch 34, loss = 0.1
I0704 08:07:48.187860 27993 caffe.cpp:312] Batch 35, accuracy/top1 = 0.94
I0704 08:07:48.187866 27993 caffe.cpp:312] Batch 35, accuracy/top5 = 1
I0704 08:07:48.187870 27993 caffe.cpp:312] Batch 35, loss = 0.08
I0704 08:07:48.196000 27993 caffe.cpp:312] Batch 36, accuracy/top1 = 0.88
I0704 08:07:48.196007 27993 caffe.cpp:312] Batch 36, accuracy/top5 = 1
I0704 08:07:48.196010 27993 caffe.cpp:312] Batch 36, loss = 0.18
I0704 08:07:48.204212 27993 caffe.cpp:312] Batch 37, accuracy/top1 = 0.88
I0704 08:07:48.204219 27993 caffe.cpp:312] Batch 37, accuracy/top5 = 1
I0704 08:07:48.204222 27993 caffe.cpp:312] Batch 37, loss = 0.22
I0704 08:07:48.212427 27993 caffe.cpp:312] Batch 38, accuracy/top1 = 0.92
I0704 08:07:48.212435 27993 caffe.cpp:312] Batch 38, accuracy/top5 = 0.96
I0704 08:07:48.212437 27993 caffe.cpp:312] Batch 38, loss = 0.38
I0704 08:07:48.220492 27993 caffe.cpp:312] Batch 39, accuracy/top1 = 0.94
I0704 08:07:48.220499 27993 caffe.cpp:312] Batch 39, accuracy/top5 = 0.98
I0704 08:07:48.220502 27993 caffe.cpp:312] Batch 39, loss = 0.2
I0704 08:07:48.228646 27993 caffe.cpp:312] Batch 40, accuracy/top1 = 0.92
I0704 08:07:48.228652 27993 caffe.cpp:312] Batch 40, accuracy/top5 = 1
I0704 08:07:48.228655 27993 caffe.cpp:312] Batch 40, loss = 0.2
I0704 08:07:48.236816 27993 caffe.cpp:312] Batch 41, accuracy/top1 = 0.94
I0704 08:07:48.236824 27993 caffe.cpp:312] Batch 41, accuracy/top5 = 1
I0704 08:07:48.236825 27993 caffe.cpp:312] Batch 41, loss = 0.14
I0704 08:07:48.244946 27993 caffe.cpp:312] Batch 42, accuracy/top1 = 0.92
I0704 08:07:48.244952 27993 caffe.cpp:312] Batch 42, accuracy/top5 = 1
I0704 08:07:48.244954 27993 caffe.cpp:312] Batch 42, loss = 0.16
I0704 08:07:48.253160 27993 caffe.cpp:312] Batch 43, accuracy/top1 = 0.86
I0704 08:07:48.253167 27993 caffe.cpp:312] Batch 43, accuracy/top5 = 1
I0704 08:07:48.253170 27993 caffe.cpp:312] Batch 43, loss = 0.3
I0704 08:07:48.261328 27993 caffe.cpp:312] Batch 44, accuracy/top1 = 0.86
I0704 08:07:48.261342 27993 caffe.cpp:312] Batch 44, accuracy/top5 = 0.98
I0704 08:07:48.261344 27993 caffe.cpp:312] Batch 44, loss = 0.26
I0704 08:07:48.269577 27993 caffe.cpp:312] Batch 45, accuracy/top1 = 0.86
I0704 08:07:48.269584 27993 caffe.cpp:312] Batch 45, accuracy/top5 = 1
I0704 08:07:48.269587 27993 caffe.cpp:312] Batch 45, loss = 0.3
I0704 08:07:48.277709 27993 caffe.cpp:312] Batch 46, accuracy/top1 = 0.94
I0704 08:07:48.277716 27993 caffe.cpp:312] Batch 46, accuracy/top5 = 1
I0704 08:07:48.277719 27993 caffe.cpp:312] Batch 46, loss = 0.04
I0704 08:07:48.285895 27993 caffe.cpp:312] Batch 47, accuracy/top1 = 0.88
I0704 08:07:48.285902 27993 caffe.cpp:312] Batch 47, accuracy/top5 = 1
I0704 08:07:48.285905 27993 caffe.cpp:312] Batch 47, loss = 0.22
I0704 08:07:48.293944 27993 caffe.cpp:312] Batch 48, accuracy/top1 = 0.96
I0704 08:07:48.293951 27993 caffe.cpp:312] Batch 48, accuracy/top5 = 0.98
I0704 08:07:48.293954 27993 caffe.cpp:312] Batch 48, loss = 0.26
I0704 08:07:48.302145 27993 caffe.cpp:312] Batch 49, accuracy/top1 = 0.88
I0704 08:07:48.302153 27993 caffe.cpp:312] Batch 49, accuracy/top5 = 1
I0704 08:07:48.302156 27993 caffe.cpp:312] Batch 49, loss = 0.2
I0704 08:07:48.310333 27993 caffe.cpp:312] Batch 50, accuracy/top1 = 0.88
I0704 08:07:48.310339 27993 caffe.cpp:312] Batch 50, accuracy/top5 = 0.96
I0704 08:07:48.310343 27993 caffe.cpp:312] Batch 50, loss = 0.32
I0704 08:07:48.318521 27993 caffe.cpp:312] Batch 51, accuracy/top1 = 0.92
I0704 08:07:48.318527 27993 caffe.cpp:312] Batch 51, accuracy/top5 = 0.98
I0704 08:07:48.318531 27993 caffe.cpp:312] Batch 51, loss = 0.24
I0704 08:07:48.326678 27993 caffe.cpp:312] Batch 52, accuracy/top1 = 0.94
I0704 08:07:48.326685 27993 caffe.cpp:312] Batch 52, accuracy/top5 = 1
I0704 08:07:48.326689 27993 caffe.cpp:312] Batch 52, loss = 0.06
I0704 08:07:48.334848 27993 caffe.cpp:312] Batch 53, accuracy/top1 = 0.9
I0704 08:07:48.334856 27993 caffe.cpp:312] Batch 53, accuracy/top5 = 0.98
I0704 08:07:48.334858 27993 caffe.cpp:312] Batch 53, loss = 0.1
I0704 08:07:48.343055 27993 caffe.cpp:312] Batch 54, accuracy/top1 = 0.94
I0704 08:07:48.343062 27993 caffe.cpp:312] Batch 54, accuracy/top5 = 1
I0704 08:07:48.343065 27993 caffe.cpp:312] Batch 54, loss = 0.18
I0704 08:07:48.351253 27993 caffe.cpp:312] Batch 55, accuracy/top1 = 0.9
I0704 08:07:48.351259 27993 caffe.cpp:312] Batch 55, accuracy/top5 = 1
I0704 08:07:48.351263 27993 caffe.cpp:312] Batch 55, loss = 0.26
I0704 08:07:48.359467 27993 caffe.cpp:312] Batch 56, accuracy/top1 = 0.88
I0704 08:07:48.359474 27993 caffe.cpp:312] Batch 56, accuracy/top5 = 0.98
I0704 08:07:48.359477 27993 caffe.cpp:312] Batch 56, loss = 0.34
I0704 08:07:48.367684 27993 caffe.cpp:312] Batch 57, accuracy/top1 = 0.98
I0704 08:07:48.367692 27993 caffe.cpp:312] Batch 57, accuracy/top5 = 1
I0704 08:07:48.367694 27993 caffe.cpp:312] Batch 57, loss = 0.06
I0704 08:07:48.375849 27993 caffe.cpp:312] Batch 58, accuracy/top1 = 0.94
I0704 08:07:48.375856 27993 caffe.cpp:312] Batch 58, accuracy/top5 = 1
I0704 08:07:48.375859 27993 caffe.cpp:312] Batch 58, loss = 0.12
I0704 08:07:48.384014 27993 caffe.cpp:312] Batch 59, accuracy/top1 = 0.92
I0704 08:07:48.384021 27993 caffe.cpp:312] Batch 59, accuracy/top5 = 1
I0704 08:07:48.384024 27993 caffe.cpp:312] Batch 59, loss = 0.12
I0704 08:07:48.392225 27993 caffe.cpp:312] Batch 60, accuracy/top1 = 0.92
I0704 08:07:48.392232 27993 caffe.cpp:312] Batch 60, accuracy/top5 = 0.98
I0704 08:07:48.392235 27993 caffe.cpp:312] Batch 60, loss = 0.22
I0704 08:07:48.400454 27993 caffe.cpp:312] Batch 61, accuracy/top1 = 0.86
I0704 08:07:48.400460 27993 caffe.cpp:312] Batch 61, accuracy/top5 = 0.98
I0704 08:07:48.400463 27993 caffe.cpp:312] Batch 61, loss = 0.34
I0704 08:07:48.408658 27993 caffe.cpp:312] Batch 62, accuracy/top1 = 0.98
I0704 08:07:48.408665 27993 caffe.cpp:312] Batch 62, accuracy/top5 = 1
I0704 08:07:48.408668 27993 caffe.cpp:312] Batch 62, loss = 0.02
I0704 08:07:48.416858 27993 caffe.cpp:312] Batch 63, accuracy/top1 = 0.9
I0704 08:07:48.416865 27993 caffe.cpp:312] Batch 63, accuracy/top5 = 1
I0704 08:07:48.416875 27993 caffe.cpp:312] Batch 63, loss = 0.18
I0704 08:07:48.425014 27993 caffe.cpp:312] Batch 64, accuracy/top1 = 0.84
I0704 08:07:48.425021 27993 caffe.cpp:312] Batch 64, accuracy/top5 = 1
I0704 08:07:48.425024 27993 caffe.cpp:312] Batch 64, loss = 0.16
I0704 08:07:48.433219 27993 caffe.cpp:312] Batch 65, accuracy/top1 = 0.94
I0704 08:07:48.433226 27993 caffe.cpp:312] Batch 65, accuracy/top5 = 0.98
I0704 08:07:48.433228 27993 caffe.cpp:312] Batch 65, loss = 0.12
I0704 08:07:48.441427 27993 caffe.cpp:312] Batch 66, accuracy/top1 = 0.88
I0704 08:07:48.441435 27993 caffe.cpp:312] Batch 66, accuracy/top5 = 1
I0704 08:07:48.441437 27993 caffe.cpp:312] Batch 66, loss = 0.14
I0704 08:07:48.449640 27993 caffe.cpp:312] Batch 67, accuracy/top1 = 0.88
I0704 08:07:48.449647 27993 caffe.cpp:312] Batch 67, accuracy/top5 = 0.98
I0704 08:07:48.449651 27993 caffe.cpp:312] Batch 67, loss = 0.2
I0704 08:07:48.457861 27993 caffe.cpp:312] Batch 68, accuracy/top1 = 0.9
I0704 08:07:48.457868 27993 caffe.cpp:312] Batch 68, accuracy/top5 = 1
I0704 08:07:48.457871 27993 caffe.cpp:312] Batch 68, loss = 0.26
I0704 08:07:48.465982 27993 caffe.cpp:312] Batch 69, accuracy/top1 = 0.92
I0704 08:07:48.465989 27993 caffe.cpp:312] Batch 69, accuracy/top5 = 1
I0704 08:07:48.465992 27993 caffe.cpp:312] Batch 69, loss = 0.08
I0704 08:07:48.474179 27993 caffe.cpp:312] Batch 70, accuracy/top1 = 0.94
I0704 08:07:48.474186 27993 caffe.cpp:312] Batch 70, accuracy/top5 = 0.98
I0704 08:07:48.474189 27993 caffe.cpp:312] Batch 70, loss = 0.22
I0704 08:07:48.482354 27993 caffe.cpp:312] Batch 71, accuracy/top1 = 0.86
I0704 08:07:48.482362 27993 caffe.cpp:312] Batch 71, accuracy/top5 = 1
I0704 08:07:48.482365 27993 caffe.cpp:312] Batch 71, loss = 0.26
I0704 08:07:48.490499 27993 caffe.cpp:312] Batch 72, accuracy/top1 = 0.86
I0704 08:07:48.490506 27993 caffe.cpp:312] Batch 72, accuracy/top5 = 0.98
I0704 08:07:48.490509 27993 caffe.cpp:312] Batch 72, loss = 0.34
I0704 08:07:48.498591 27993 caffe.cpp:312] Batch 73, accuracy/top1 = 0.94
I0704 08:07:48.498600 27993 caffe.cpp:312] Batch 73, accuracy/top5 = 1
I0704 08:07:48.498601 27993 caffe.cpp:312] Batch 73, loss = 0.2
I0704 08:07:48.506778 27993 caffe.cpp:312] Batch 74, accuracy/top1 = 0.94
I0704 08:07:48.506784 27993 caffe.cpp:312] Batch 74, accuracy/top5 = 1
I0704 08:07:48.506788 27993 caffe.cpp:312] Batch 74, loss = 0.08
I0704 08:07:48.514987 27993 caffe.cpp:312] Batch 75, accuracy/top1 = 0.92
I0704 08:07:48.514994 27993 caffe.cpp:312] Batch 75, accuracy/top5 = 1
I0704 08:07:48.514997 27993 caffe.cpp:312] Batch 75, loss = 0.16
I0704 08:07:48.523181 27993 caffe.cpp:312] Batch 76, accuracy/top1 = 0.92
I0704 08:07:48.523192 27993 caffe.cpp:312] Batch 76, accuracy/top5 = 1
I0704 08:07:48.523195 27993 caffe.cpp:312] Batch 76, loss = 0.14
I0704 08:07:48.531455 27993 caffe.cpp:312] Batch 77, accuracy/top1 = 0.92
I0704 08:07:48.531473 27993 caffe.cpp:312] Batch 77, accuracy/top5 = 1
I0704 08:07:48.531476 27993 caffe.cpp:312] Batch 77, loss = 0.08
I0704 08:07:48.539728 27993 caffe.cpp:312] Batch 78, accuracy/top1 = 0.94
I0704 08:07:48.539744 27993 caffe.cpp:312] Batch 78, accuracy/top5 = 1
I0704 08:07:48.539747 27993 caffe.cpp:312] Batch 78, loss = 0.02
I0704 08:07:48.547925 27993 caffe.cpp:312] Batch 79, accuracy/top1 = 0.94
I0704 08:07:48.547933 27993 caffe.cpp:312] Batch 79, accuracy/top5 = 1
I0704 08:07:48.547935 27993 caffe.cpp:312] Batch 79, loss = 0.16
I0704 08:07:48.556143 27993 caffe.cpp:312] Batch 80, accuracy/top1 = 0.94
I0704 08:07:48.556150 27993 caffe.cpp:312] Batch 80, accuracy/top5 = 0.98
I0704 08:07:48.556154 27993 caffe.cpp:312] Batch 80, loss = 0.14
I0704 08:07:48.564318 27993 caffe.cpp:312] Batch 81, accuracy/top1 = 0.88
I0704 08:07:48.564327 27993 caffe.cpp:312] Batch 81, accuracy/top5 = 1
I0704 08:07:48.564328 27993 caffe.cpp:312] Batch 81, loss = 0.16
I0704 08:07:48.572521 27993 caffe.cpp:312] Batch 82, accuracy/top1 = 0.92
I0704 08:07:48.572530 27993 caffe.cpp:312] Batch 82, accuracy/top5 = 0.98
I0704 08:07:48.572533 27993 caffe.cpp:312] Batch 82, loss = 0.32
I0704 08:07:48.580770 27993 caffe.cpp:312] Batch 83, accuracy/top1 = 0.94
I0704 08:07:48.580780 27993 caffe.cpp:312] Batch 83, accuracy/top5 = 1
I0704 08:07:48.580783 27993 caffe.cpp:312] Batch 83, loss = 0.1
I0704 08:07:48.588977 27993 caffe.cpp:312] Batch 84, accuracy/top1 = 0.98
I0704 08:07:48.588991 27993 caffe.cpp:312] Batch 84, accuracy/top5 = 1
I0704 08:07:48.588994 27993 caffe.cpp:312] Batch 84, loss = 0.08
I0704 08:07:48.597172 27993 caffe.cpp:312] Batch 85, accuracy/top1 = 0.92
I0704 08:07:48.597185 27993 caffe.cpp:312] Batch 85, accuracy/top5 = 1
I0704 08:07:48.597188 27993 caffe.cpp:312] Batch 85, loss = 0.14
I0704 08:07:48.605397 27993 caffe.cpp:312] Batch 86, accuracy/top1 = 0.9
I0704 08:07:48.605408 27993 caffe.cpp:312] Batch 86, accuracy/top5 = 1
I0704 08:07:48.605412 27993 caffe.cpp:312] Batch 86, loss = 0.18
I0704 08:07:48.613657 27993 caffe.cpp:312] Batch 87, accuracy/top1 = 0.96
I0704 08:07:48.613672 27993 caffe.cpp:312] Batch 87, accuracy/top5 = 1
I0704 08:07:48.613674 27993 caffe.cpp:312] Batch 87, loss = 0.1
I0704 08:07:48.621924 27993 caffe.cpp:312] Batch 88, accuracy/top1 = 0.94
I0704 08:07:48.621937 27993 caffe.cpp:312] Batch 88, accuracy/top5 = 1
I0704 08:07:48.621940 27993 caffe.cpp:312] Batch 88, loss = 0.14
I0704 08:07:48.630183 27993 caffe.cpp:312] Batch 89, accuracy/top1 = 0.88
I0704 08:07:48.630201 27993 caffe.cpp:312] Batch 89, accuracy/top5 = 1
I0704 08:07:48.630204 27993 caffe.cpp:312] Batch 89, loss = 0.2
I0704 08:07:48.638300 27993 caffe.cpp:312] Batch 90, accuracy/top1 = 0.84
I0704 08:07:48.638317 27993 caffe.cpp:312] Batch 90, accuracy/top5 = 1
I0704 08:07:48.638319 27993 caffe.cpp:312] Batch 90, loss = 0.32
I0704 08:07:48.646574 27993 caffe.cpp:312] Batch 91, accuracy/top1 = 0.92
I0704 08:07:48.646587 27993 caffe.cpp:312] Batch 91, accuracy/top5 = 1
I0704 08:07:48.646590 27993 caffe.cpp:312] Batch 91, loss = 0.16
I0704 08:07:48.654858 27993 caffe.cpp:312] Batch 92, accuracy/top1 = 0.82
I0704 08:07:48.654870 27993 caffe.cpp:312] Batch 92, accuracy/top5 = 1
I0704 08:07:48.654873 27993 caffe.cpp:312] Batch 92, loss = 0.22
I0704 08:07:48.663075 27993 caffe.cpp:312] Batch 93, accuracy/top1 = 0.96
I0704 08:07:48.663089 27993 caffe.cpp:312] Batch 93, accuracy/top5 = 1
I0704 08:07:48.663091 27993 caffe.cpp:312] Batch 93, loss = 0.04
I0704 08:07:48.671326 27993 caffe.cpp:312] Batch 94, accuracy/top1 = 0.94
I0704 08:07:48.671344 27993 caffe.cpp:312] Batch 94, accuracy/top5 = 1
I0704 08:07:48.671346 27993 caffe.cpp:312] Batch 94, loss = 0.18
I0704 08:07:48.679502 27993 caffe.cpp:312] Batch 95, accuracy/top1 = 0.86
I0704 08:07:48.679514 27993 caffe.cpp:312] Batch 95, accuracy/top5 = 0.98
I0704 08:07:48.679517 27993 caffe.cpp:312] Batch 95, loss = 0.32
I0704 08:07:48.687713 27993 caffe.cpp:312] Batch 96, accuracy/top1 = 0.98
I0704 08:07:48.687726 27993 caffe.cpp:312] Batch 96, accuracy/top5 = 1
I0704 08:07:48.687728 27993 caffe.cpp:312] Batch 96, loss = 0
I0704 08:07:48.695912 27993 caffe.cpp:312] Batch 97, accuracy/top1 = 0.96
I0704 08:07:48.695926 27993 caffe.cpp:312] Batch 97, accuracy/top5 = 1
I0704 08:07:48.695929 27993 caffe.cpp:312] Batch 97, loss = 0.04
I0704 08:07:48.704177 27993 caffe.cpp:312] Batch 98, accuracy/top1 = 0.92
I0704 08:07:48.704190 27993 caffe.cpp:312] Batch 98, accuracy/top5 = 0.98
I0704 08:07:48.704192 27993 caffe.cpp:312] Batch 98, loss = 0.16
I0704 08:07:48.712383 27993 caffe.cpp:312] Batch 99, accuracy/top1 = 0.9
I0704 08:07:48.712390 27993 caffe.cpp:312] Batch 99, accuracy/top5 = 1
I0704 08:07:48.712393 27993 caffe.cpp:312] Batch 99, loss = 0.38
I0704 08:07:48.720633 27993 caffe.cpp:312] Batch 100, accuracy/top1 = 0.96
I0704 08:07:48.720640 27993 caffe.cpp:312] Batch 100, accuracy/top5 = 1
I0704 08:07:48.720643 27993 caffe.cpp:312] Batch 100, loss = 0.04
I0704 08:07:48.728821 27993 caffe.cpp:312] Batch 101, accuracy/top1 = 0.96
I0704 08:07:48.728828 27993 caffe.cpp:312] Batch 101, accuracy/top5 = 1
I0704 08:07:48.728830 27993 caffe.cpp:312] Batch 101, loss = 0.02
I0704 08:07:48.737038 27993 caffe.cpp:312] Batch 102, accuracy/top1 = 0.9
I0704 08:07:48.737046 27993 caffe.cpp:312] Batch 102, accuracy/top5 = 1
I0704 08:07:48.737056 27993 caffe.cpp:312] Batch 102, loss = 0.16
I0704 08:07:48.745218 27993 caffe.cpp:312] Batch 103, accuracy/top1 = 0.92
I0704 08:07:48.745225 27993 caffe.cpp:312] Batch 103, accuracy/top5 = 1
I0704 08:07:48.745229 27993 caffe.cpp:312] Batch 103, loss = 0.14
I0704 08:07:48.753432 27993 caffe.cpp:312] Batch 104, accuracy/top1 = 0.88
I0704 08:07:48.753438 27993 caffe.cpp:312] Batch 104, accuracy/top5 = 1
I0704 08:07:48.753442 27993 caffe.cpp:312] Batch 104, loss = 0.14
I0704 08:07:48.761590 27993 caffe.cpp:312] Batch 105, accuracy/top1 = 0.94
I0704 08:07:48.761596 27993 caffe.cpp:312] Batch 105, accuracy/top5 = 1
I0704 08:07:48.761598 27993 caffe.cpp:312] Batch 105, loss = 0.12
I0704 08:07:48.769852 27993 caffe.cpp:312] Batch 106, accuracy/top1 = 0.96
I0704 08:07:48.769860 27993 caffe.cpp:312] Batch 106, accuracy/top5 = 1
I0704 08:07:48.769862 27993 caffe.cpp:312] Batch 106, loss = 0.1
I0704 08:07:48.778000 27993 caffe.cpp:312] Batch 107, accuracy/top1 = 0.9
I0704 08:07:48.778007 27993 caffe.cpp:312] Batch 107, accuracy/top5 = 1
I0704 08:07:48.778010 27993 caffe.cpp:312] Batch 107, loss = 0.18
I0704 08:07:48.786187 27993 caffe.cpp:312] Batch 108, accuracy/top1 = 0.92
I0704 08:07:48.786195 27993 caffe.cpp:312] Batch 108, accuracy/top5 = 1
I0704 08:07:48.786197 27993 caffe.cpp:312] Batch 108, loss = 0.1
I0704 08:07:48.794353 27993 caffe.cpp:312] Batch 109, accuracy/top1 = 0.94
I0704 08:07:48.794360 27993 caffe.cpp:312] Batch 109, accuracy/top5 = 1
I0704 08:07:48.794363 27993 caffe.cpp:312] Batch 109, loss = 0.08
I0704 08:07:48.802553 27993 caffe.cpp:312] Batch 110, accuracy/top1 = 0.9
I0704 08:07:48.802561 27993 caffe.cpp:312] Batch 110, accuracy/top5 = 1
I0704 08:07:48.802563 27993 caffe.cpp:312] Batch 110, loss = 0.3
I0704 08:07:48.810746 27993 caffe.cpp:312] Batch 111, accuracy/top1 = 0.94
I0704 08:07:48.810755 27993 caffe.cpp:312] Batch 111, accuracy/top5 = 1
I0704 08:07:48.810756 27993 caffe.cpp:312] Batch 111, loss = 0.06
I0704 08:07:48.818912 27993 caffe.cpp:312] Batch 112, accuracy/top1 = 0.92
I0704 08:07:48.818918 27993 caffe.cpp:312] Batch 112, accuracy/top5 = 1
I0704 08:07:48.818920 27993 caffe.cpp:312] Batch 112, loss = 0.22
I0704 08:07:48.827082 27993 caffe.cpp:312] Batch 113, accuracy/top1 = 0.94
I0704 08:07:48.827090 27993 caffe.cpp:312] Batch 113, accuracy/top5 = 1
I0704 08:07:48.827092 27993 caffe.cpp:312] Batch 113, loss = 0.12
I0704 08:07:48.835223 27993 caffe.cpp:312] Batch 114, accuracy/top1 = 0.9
I0704 08:07:48.835230 27993 caffe.cpp:312] Batch 114, accuracy/top5 = 0.98
I0704 08:07:48.835233 27993 caffe.cpp:312] Batch 114, loss = 0.22
I0704 08:07:48.843399 27993 caffe.cpp:312] Batch 115, accuracy/top1 = 0.98
I0704 08:07:48.843405 27993 caffe.cpp:312] Batch 115, accuracy/top5 = 1
I0704 08:07:48.843408 27993 caffe.cpp:312] Batch 115, loss = 0.02
I0704 08:07:48.851527 27993 caffe.cpp:312] Batch 116, accuracy/top1 = 0.84
I0704 08:07:48.851536 27993 caffe.cpp:312] Batch 116, accuracy/top5 = 1
I0704 08:07:48.851537 27993 caffe.cpp:312] Batch 116, loss = 0.16
I0704 08:07:48.859688 27993 caffe.cpp:312] Batch 117, accuracy/top1 = 0.86
I0704 08:07:48.859694 27993 caffe.cpp:312] Batch 117, accuracy/top5 = 1
I0704 08:07:48.859697 27993 caffe.cpp:312] Batch 117, loss = 0.22
I0704 08:07:48.867877 27993 caffe.cpp:312] Batch 118, accuracy/top1 = 0.9
I0704 08:07:48.867883 27993 caffe.cpp:312] Batch 118, accuracy/top5 = 1
I0704 08:07:48.867887 27993 caffe.cpp:312] Batch 118, loss = 0.08
I0704 08:07:48.875998 27993 caffe.cpp:312] Batch 119, accuracy/top1 = 0.88
I0704 08:07:48.876004 27993 caffe.cpp:312] Batch 119, accuracy/top5 = 1
I0704 08:07:48.876008 27993 caffe.cpp:312] Batch 119, loss = 0.28
I0704 08:07:48.884263 27993 caffe.cpp:312] Batch 120, accuracy/top1 = 0.92
I0704 08:07:48.884269 27993 caffe.cpp:312] Batch 120, accuracy/top5 = 1
I0704 08:07:48.884272 27993 caffe.cpp:312] Batch 120, loss = 0.14
I0704 08:07:48.892422 27993 caffe.cpp:312] Batch 121, accuracy/top1 = 0.92
I0704 08:07:48.892429 27993 caffe.cpp:312] Batch 121, accuracy/top5 = 1
I0704 08:07:48.892438 27993 caffe.cpp:312] Batch 121, loss = 0.2
I0704 08:07:48.900635 27993 caffe.cpp:312] Batch 122, accuracy/top1 = 0.9
I0704 08:07:48.900641 27993 caffe.cpp:312] Batch 122, accuracy/top5 = 1
I0704 08:07:48.900645 27993 caffe.cpp:312] Batch 122, loss = 0.1
I0704 08:07:48.908807 27993 caffe.cpp:312] Batch 123, accuracy/top1 = 0.9
I0704 08:07:48.908814 27993 caffe.cpp:312] Batch 123, accuracy/top5 = 1
I0704 08:07:48.908818 27993 caffe.cpp:312] Batch 123, loss = 0.2
I0704 08:07:48.916985 27993 caffe.cpp:312] Batch 124, accuracy/top1 = 0.92
I0704 08:07:48.916991 27993 caffe.cpp:312] Batch 124, accuracy/top5 = 1
I0704 08:07:48.916995 27993 caffe.cpp:312] Batch 124, loss = 0.14
I0704 08:07:48.925127 27993 caffe.cpp:312] Batch 125, accuracy/top1 = 0.94
I0704 08:07:48.925134 27993 caffe.cpp:312] Batch 125, accuracy/top5 = 1
I0704 08:07:48.925137 27993 caffe.cpp:312] Batch 125, loss = 0.14
I0704 08:07:48.933313 27993 caffe.cpp:312] Batch 126, accuracy/top1 = 0.96
I0704 08:07:48.933320 27993 caffe.cpp:312] Batch 126, accuracy/top5 = 1
I0704 08:07:48.933322 27993 caffe.cpp:312] Batch 126, loss = 0.04
I0704 08:07:48.941470 27993 caffe.cpp:312] Batch 127, accuracy/top1 = 0.96
I0704 08:07:48.941478 27993 caffe.cpp:312] Batch 127, accuracy/top5 = 1
I0704 08:07:48.941480 27993 caffe.cpp:312] Batch 127, loss = 0.02
I0704 08:07:48.949692 27993 caffe.cpp:312] Batch 128, accuracy/top1 = 0.86
I0704 08:07:48.949698 27993 caffe.cpp:312] Batch 128, accuracy/top5 = 1
I0704 08:07:48.949700 27993 caffe.cpp:312] Batch 128, loss = 0.18
I0704 08:07:48.957895 27993 caffe.cpp:312] Batch 129, accuracy/top1 = 0.96
I0704 08:07:48.957902 27993 caffe.cpp:312] Batch 129, accuracy/top5 = 1
I0704 08:07:48.957906 27993 caffe.cpp:312] Batch 129, loss = 0.08
I0704 08:07:48.966018 27993 caffe.cpp:312] Batch 130, accuracy/top1 = 0.9
I0704 08:07:48.966025 27993 caffe.cpp:312] Batch 130, accuracy/top5 = 1
I0704 08:07:48.966027 27993 caffe.cpp:312] Batch 130, loss = 0.18
I0704 08:07:48.974154 27993 caffe.cpp:312] Batch 131, accuracy/top1 = 0.9
I0704 08:07:48.974161 27993 caffe.cpp:312] Batch 131, accuracy/top5 = 1
I0704 08:07:48.974164 27993 caffe.cpp:312] Batch 131, loss = 0.16
I0704 08:07:48.982321 27993 caffe.cpp:312] Batch 132, accuracy/top1 = 0.96
I0704 08:07:48.982327 27993 caffe.cpp:312] Batch 132, accuracy/top5 = 1
I0704 08:07:48.982331 27993 caffe.cpp:312] Batch 132, loss = 0.06
I0704 08:07:48.990487 27993 caffe.cpp:312] Batch 133, accuracy/top1 = 0.94
I0704 08:07:48.990494 27993 caffe.cpp:312] Batch 133, accuracy/top5 = 1
I0704 08:07:48.990496 27993 caffe.cpp:312] Batch 133, loss = 0.06
I0704 08:07:48.998677 27993 caffe.cpp:312] Batch 134, accuracy/top1 = 0.92
I0704 08:07:48.998685 27993 caffe.cpp:312] Batch 134, accuracy/top5 = 1
I0704 08:07:48.998687 27993 caffe.cpp:312] Batch 134, loss = 0.16
I0704 08:07:49.006898 27993 caffe.cpp:312] Batch 135, accuracy/top1 = 0.86
I0704 08:07:49.006906 27993 caffe.cpp:312] Batch 135, accuracy/top5 = 0.98
I0704 08:07:49.006907 27993 caffe.cpp:312] Batch 135, loss = 0.44
I0704 08:07:49.015136 27993 caffe.cpp:312] Batch 136, accuracy/top1 = 0.96
I0704 08:07:49.015143 27993 caffe.cpp:312] Batch 136, accuracy/top5 = 1
I0704 08:07:49.015146 27993 caffe.cpp:312] Batch 136, loss = 0.08
I0704 08:07:49.023334 27993 caffe.cpp:312] Batch 137, accuracy/top1 = 0.9
I0704 08:07:49.023340 27993 caffe.cpp:312] Batch 137, accuracy/top5 = 1
I0704 08:07:49.023344 27993 caffe.cpp:312] Batch 137, loss = 0.2
I0704 08:07:49.031550 27993 caffe.cpp:312] Batch 138, accuracy/top1 = 0.92
I0704 08:07:49.031558 27993 caffe.cpp:312] Batch 138, accuracy/top5 = 0.98
I0704 08:07:49.031560 27993 caffe.cpp:312] Batch 138, loss = 0.14
I0704 08:07:49.039695 27993 caffe.cpp:312] Batch 139, accuracy/top1 = 0.88
I0704 08:07:49.039701 27993 caffe.cpp:312] Batch 139, accuracy/top5 = 1
I0704 08:07:49.039705 27993 caffe.cpp:312] Batch 139, loss = 0.24
I0704 08:07:49.047844 27993 caffe.cpp:312] Batch 140, accuracy/top1 = 0.9
I0704 08:07:49.047852 27993 caffe.cpp:312] Batch 140, accuracy/top5 = 1
I0704 08:07:49.047854 27993 caffe.cpp:312] Batch 140, loss = 0.3
I0704 08:07:49.056033 27993 caffe.cpp:312] Batch 141, accuracy/top1 = 0.92
I0704 08:07:49.056041 27993 caffe.cpp:312] Batch 141, accuracy/top5 = 1
I0704 08:07:49.056043 27993 caffe.cpp:312] Batch 141, loss = 0.18
I0704 08:07:49.064234 27993 caffe.cpp:312] Batch 142, accuracy/top1 = 0.94
I0704 08:07:49.064241 27993 caffe.cpp:312] Batch 142, accuracy/top5 = 1
I0704 08:07:49.064244 27993 caffe.cpp:312] Batch 142, loss = 0.12
I0704 08:07:49.072350 27993 caffe.cpp:312] Batch 143, accuracy/top1 = 0.94
I0704 08:07:49.072358 27993 caffe.cpp:312] Batch 143, accuracy/top5 = 1
I0704 08:07:49.072360 27993 caffe.cpp:312] Batch 143, loss = 0.12
I0704 08:07:49.080519 27993 caffe.cpp:312] Batch 144, accuracy/top1 = 0.92
I0704 08:07:49.080526 27993 caffe.cpp:312] Batch 144, accuracy/top5 = 1
I0704 08:07:49.080529 27993 caffe.cpp:312] Batch 144, loss = 0.2
I0704 08:07:49.088708 27993 caffe.cpp:312] Batch 145, accuracy/top1 = 0.94
I0704 08:07:49.088716 27993 caffe.cpp:312] Batch 145, accuracy/top5 = 1
I0704 08:07:49.088718 27993 caffe.cpp:312] Batch 145, loss = 0.1
I0704 08:07:49.096864 27993 caffe.cpp:312] Batch 146, accuracy/top1 = 0.94
I0704 08:07:49.096873 27993 caffe.cpp:312] Batch 146, accuracy/top5 = 1
I0704 08:07:49.096874 27993 caffe.cpp:312] Batch 146, loss = 0.14
I0704 08:07:49.105036 27993 caffe.cpp:312] Batch 147, accuracy/top1 = 0.92
I0704 08:07:49.105042 27993 caffe.cpp:312] Batch 147, accuracy/top5 = 1
I0704 08:07:49.105044 27993 caffe.cpp:312] Batch 147, loss = 0.18
I0704 08:07:49.113194 27993 caffe.cpp:312] Batch 148, accuracy/top1 = 0.92
I0704 08:07:49.113201 27993 caffe.cpp:312] Batch 148, accuracy/top5 = 1
I0704 08:07:49.113204 27993 caffe.cpp:312] Batch 148, loss = 0.08
I0704 08:07:49.121388 27993 caffe.cpp:312] Batch 149, accuracy/top1 = 0.92
I0704 08:07:49.121395 27993 caffe.cpp:312] Batch 149, accuracy/top5 = 1
I0704 08:07:49.121398 27993 caffe.cpp:312] Batch 149, loss = 0.08
I0704 08:07:49.129513 27993 caffe.cpp:312] Batch 150, accuracy/top1 = 0.92
I0704 08:07:49.129519 27993 caffe.cpp:312] Batch 150, accuracy/top5 = 0.98
I0704 08:07:49.129521 27993 caffe.cpp:312] Batch 150, loss = 0.16
I0704 08:07:49.137701 27993 caffe.cpp:312] Batch 151, accuracy/top1 = 0.9
I0704 08:07:49.137707 27993 caffe.cpp:312] Batch 151, accuracy/top5 = 1
I0704 08:07:49.137709 27993 caffe.cpp:312] Batch 151, loss = 0.16
I0704 08:07:49.145917 27993 caffe.cpp:312] Batch 152, accuracy/top1 = 0.9
I0704 08:07:49.145925 27993 caffe.cpp:312] Batch 152, accuracy/top5 = 1
I0704 08:07:49.145927 27993 caffe.cpp:312] Batch 152, loss = 0.14
I0704 08:07:49.154130 27993 caffe.cpp:312] Batch 153, accuracy/top1 = 0.92
I0704 08:07:49.154137 27993 caffe.cpp:312] Batch 153, accuracy/top5 = 0.98
I0704 08:07:49.154140 27993 caffe.cpp:312] Batch 153, loss = 0.34
I0704 08:07:49.162297 27993 caffe.cpp:312] Batch 154, accuracy/top1 = 0.94
I0704 08:07:49.162304 27993 caffe.cpp:312] Batch 154, accuracy/top5 = 1
I0704 08:07:49.162307 27993 caffe.cpp:312] Batch 154, loss = 0.08
I0704 08:07:49.170442 27993 caffe.cpp:312] Batch 155, accuracy/top1 = 0.86
I0704 08:07:49.170449 27993 caffe.cpp:312] Batch 155, accuracy/top5 = 1
I0704 08:07:49.170451 27993 caffe.cpp:312] Batch 155, loss = 0.36
I0704 08:07:49.178568 27993 caffe.cpp:312] Batch 156, accuracy/top1 = 0.88
I0704 08:07:49.178575 27993 caffe.cpp:312] Batch 156, accuracy/top5 = 1
I0704 08:07:49.178577 27993 caffe.cpp:312] Batch 156, loss = 0.22
I0704 08:07:49.186784 27993 caffe.cpp:312] Batch 157, accuracy/top1 = 0.92
I0704 08:07:49.186790 27993 caffe.cpp:312] Batch 157, accuracy/top5 = 0.98
I0704 08:07:49.186794 27993 caffe.cpp:312] Batch 157, loss = 0.22
I0704 08:07:49.194994 27993 caffe.cpp:312] Batch 158, accuracy/top1 = 0.96
I0704 08:07:49.195001 27993 caffe.cpp:312] Batch 158, accuracy/top5 = 1
I0704 08:07:49.195004 27993 caffe.cpp:312] Batch 158, loss = 0.1
I0704 08:07:49.203194 27993 caffe.cpp:312] Batch 159, accuracy/top1 = 0.94
I0704 08:07:49.203202 27993 caffe.cpp:312] Batch 159, accuracy/top5 = 1
I0704 08:07:49.203204 27993 caffe.cpp:312] Batch 159, loss = 0.08
I0704 08:07:49.211371 27993 caffe.cpp:312] Batch 160, accuracy/top1 = 0.92
I0704 08:07:49.211385 27993 caffe.cpp:312] Batch 160, accuracy/top5 = 1
I0704 08:07:49.211388 27993 caffe.cpp:312] Batch 160, loss = 0.1
I0704 08:07:49.219573 27993 caffe.cpp:312] Batch 161, accuracy/top1 = 0.94
I0704 08:07:49.219579 27993 caffe.cpp:312] Batch 161, accuracy/top5 = 1
I0704 08:07:49.219583 27993 caffe.cpp:312] Batch 161, loss = 0.1
I0704 08:07:49.227720 27993 caffe.cpp:312] Batch 162, accuracy/top1 = 0.94
I0704 08:07:49.227726 27993 caffe.cpp:312] Batch 162, accuracy/top5 = 1
I0704 08:07:49.227730 27993 caffe.cpp:312] Batch 162, loss = 0.14
I0704 08:07:49.235911 27993 caffe.cpp:312] Batch 163, accuracy/top1 = 0.94
I0704 08:07:49.235918 27993 caffe.cpp:312] Batch 163, accuracy/top5 = 1
I0704 08:07:49.235921 27993 caffe.cpp:312] Batch 163, loss = 0.12
I0704 08:07:49.243952 27993 caffe.cpp:312] Batch 164, accuracy/top1 = 0.92
I0704 08:07:49.243958 27993 caffe.cpp:312] Batch 164, accuracy/top5 = 1
I0704 08:07:49.243960 27993 caffe.cpp:312] Batch 164, loss = 0.1
I0704 08:07:49.252162 27993 caffe.cpp:312] Batch 165, accuracy/top1 = 0.86
I0704 08:07:49.252169 27993 caffe.cpp:312] Batch 165, accuracy/top5 = 1
I0704 08:07:49.252172 27993 caffe.cpp:312] Batch 165, loss = 0.2
I0704 08:07:49.260349 27993 caffe.cpp:312] Batch 166, accuracy/top1 = 0.94
I0704 08:07:49.260355 27993 caffe.cpp:312] Batch 166, accuracy/top5 = 1
I0704 08:07:49.260357 27993 caffe.cpp:312] Batch 166, loss = 0.1
I0704 08:07:49.268559 27993 caffe.cpp:312] Batch 167, accuracy/top1 = 0.96
I0704 08:07:49.268566 27993 caffe.cpp:312] Batch 167, accuracy/top5 = 1
I0704 08:07:49.268569 27993 caffe.cpp:312] Batch 167, loss = 0.08
I0704 08:07:49.276758 27993 caffe.cpp:312] Batch 168, accuracy/top1 = 0.88
I0704 08:07:49.276765 27993 caffe.cpp:312] Batch 168, accuracy/top5 = 1
I0704 08:07:49.276768 27993 caffe.cpp:312] Batch 168, loss = 0.28
I0704 08:07:49.284924 27993 caffe.cpp:312] Batch 169, accuracy/top1 = 0.84
I0704 08:07:49.284931 27993 caffe.cpp:312] Batch 169, accuracy/top5 = 0.98
I0704 08:07:49.284934 27993 caffe.cpp:312] Batch 169, loss = 0.2
I0704 08:07:49.293118 27993 caffe.cpp:312] Batch 170, accuracy/top1 = 0.9
I0704 08:07:49.293125 27993 caffe.cpp:312] Batch 170, accuracy/top5 = 0.98
I0704 08:07:49.293128 27993 caffe.cpp:312] Batch 170, loss = 0.3
I0704 08:07:49.301249 27993 caffe.cpp:312] Batch 171, accuracy/top1 = 0.88
I0704 08:07:49.301255 27993 caffe.cpp:312] Batch 171, accuracy/top5 = 1
I0704 08:07:49.301259 27993 caffe.cpp:312] Batch 171, loss = 0.32
I0704 08:07:49.309437 27993 caffe.cpp:312] Batch 172, accuracy/top1 = 0.88
I0704 08:07:49.309444 27993 caffe.cpp:312] Batch 172, accuracy/top5 = 1
I0704 08:07:49.309448 27993 caffe.cpp:312] Batch 172, loss = 0.12
I0704 08:07:49.317651 27993 caffe.cpp:312] Batch 173, accuracy/top1 = 0.96
I0704 08:07:49.317658 27993 caffe.cpp:312] Batch 173, accuracy/top5 = 1
I0704 08:07:49.317662 27993 caffe.cpp:312] Batch 173, loss = 0.04
I0704 08:07:49.325829 27993 caffe.cpp:312] Batch 174, accuracy/top1 = 0.86
I0704 08:07:49.325835 27993 caffe.cpp:312] Batch 174, accuracy/top5 = 1
I0704 08:07:49.325839 27993 caffe.cpp:312] Batch 174, loss = 0.46
I0704 08:07:49.334022 27993 caffe.cpp:312] Batch 175, accuracy/top1 = 0.98
I0704 08:07:49.334028 27993 caffe.cpp:312] Batch 175, accuracy/top5 = 1
I0704 08:07:49.334031 27993 caffe.cpp:312] Batch 175, loss = 0.02
I0704 08:07:49.342233 27993 caffe.cpp:312] Batch 176, accuracy/top1 = 0.9
I0704 08:07:49.342241 27993 caffe.cpp:312] Batch 176, accuracy/top5 = 0.98
I0704 08:07:49.342242 27993 caffe.cpp:312] Batch 176, loss = 0.32
I0704 08:07:49.350430 27993 caffe.cpp:312] Batch 177, accuracy/top1 = 0.94
I0704 08:07:49.350436 27993 caffe.cpp:312] Batch 177, accuracy/top5 = 1
I0704 08:07:49.350438 27993 caffe.cpp:312] Batch 177, loss = 0.08
I0704 08:07:49.358583 27993 caffe.cpp:312] Batch 178, accuracy/top1 = 0.9
I0704 08:07:49.358590 27993 caffe.cpp:312] Batch 178, accuracy/top5 = 1
I0704 08:07:49.358593 27993 caffe.cpp:312] Batch 178, loss = 0.28
I0704 08:07:49.366760 27993 caffe.cpp:312] Batch 179, accuracy/top1 = 0.96
I0704 08:07:49.366773 27993 caffe.cpp:312] Batch 179, accuracy/top5 = 1
I0704 08:07:49.366776 27993 caffe.cpp:312] Batch 179, loss = 0.14
I0704 08:07:49.374948 27993 caffe.cpp:312] Batch 180, accuracy/top1 = 0.92
I0704 08:07:49.374955 27993 caffe.cpp:312] Batch 180, accuracy/top5 = 1
I0704 08:07:49.374958 27993 caffe.cpp:312] Batch 180, loss = 0.06
I0704 08:07:49.383204 27993 caffe.cpp:312] Batch 181, accuracy/top1 = 0.96
I0704 08:07:49.383211 27993 caffe.cpp:312] Batch 181, accuracy/top5 = 1
I0704 08:07:49.383214 27993 caffe.cpp:312] Batch 181, loss = 0.06
I0704 08:07:49.391399 27993 caffe.cpp:312] Batch 182, accuracy/top1 = 0.92
I0704 08:07:49.391407 27993 caffe.cpp:312] Batch 182, accuracy/top5 = 0.98
I0704 08:07:49.391409 27993 caffe.cpp:312] Batch 182, loss = 0.14
I0704 08:07:49.399559 27993 caffe.cpp:312] Batch 183, accuracy/top1 = 0.96
I0704 08:07:49.399566 27993 caffe.cpp:312] Batch 183, accuracy/top5 = 1
I0704 08:07:49.399569 27993 caffe.cpp:312] Batch 183, loss = 0.1
I0704 08:07:49.407708 27993 caffe.cpp:312] Batch 184, accuracy/top1 = 0.88
I0704 08:07:49.407716 27993 caffe.cpp:312] Batch 184, accuracy/top5 = 1
I0704 08:07:49.407718 27993 caffe.cpp:312] Batch 184, loss = 0.26
I0704 08:07:49.415904 27993 caffe.cpp:312] Batch 185, accuracy/top1 = 0.92
I0704 08:07:49.415910 27993 caffe.cpp:312] Batch 185, accuracy/top5 = 1
I0704 08:07:49.415912 27993 caffe.cpp:312] Batch 185, loss = 0.16
I0704 08:07:49.424067 27993 caffe.cpp:312] Batch 186, accuracy/top1 = 0.92
I0704 08:07:49.424073 27993 caffe.cpp:312] Batch 186, accuracy/top5 = 1
I0704 08:07:49.424077 27993 caffe.cpp:312] Batch 186, loss = 0.08
I0704 08:07:49.432200 27993 caffe.cpp:312] Batch 187, accuracy/top1 = 0.86
I0704 08:07:49.432207 27993 caffe.cpp:312] Batch 187, accuracy/top5 = 0.98
I0704 08:07:49.432210 27993 caffe.cpp:312] Batch 187, loss = 0.34
I0704 08:07:49.440409 27993 caffe.cpp:312] Batch 188, accuracy/top1 = 0.88
I0704 08:07:49.440417 27993 caffe.cpp:312] Batch 188, accuracy/top5 = 1
I0704 08:07:49.440419 27993 caffe.cpp:312] Batch 188, loss = 0.08
I0704 08:07:49.448606 27993 caffe.cpp:312] Batch 189, accuracy/top1 = 0.98
I0704 08:07:49.448612 27993 caffe.cpp:312] Batch 189, accuracy/top5 = 1
I0704 08:07:49.448616 27993 caffe.cpp:312] Batch 189, loss = 0.1
I0704 08:07:49.456779 27993 caffe.cpp:312] Batch 190, accuracy/top1 = 0.94
I0704 08:07:49.456786 27993 caffe.cpp:312] Batch 190, accuracy/top5 = 1
I0704 08:07:49.456789 27993 caffe.cpp:312] Batch 190, loss = 0.08
I0704 08:07:49.465013 27993 caffe.cpp:312] Batch 191, accuracy/top1 = 0.94
I0704 08:07:49.465019 27993 caffe.cpp:312] Batch 191, accuracy/top5 = 1
I0704 08:07:49.465023 27993 caffe.cpp:312] Batch 191, loss = 0.08
I0704 08:07:49.473233 27993 caffe.cpp:312] Batch 192, accuracy/top1 = 0.9
I0704 08:07:49.473240 27993 caffe.cpp:312] Batch 192, accuracy/top5 = 1
I0704 08:07:49.473243 27993 caffe.cpp:312] Batch 192, loss = 0.14
I0704 08:07:49.481413 27993 caffe.cpp:312] Batch 193, accuracy/top1 = 0.98
I0704 08:07:49.481420 27993 caffe.cpp:312] Batch 193, accuracy/top5 = 1
I0704 08:07:49.481423 27993 caffe.cpp:312] Batch 193, loss = 0.02
I0704 08:07:49.489542 27993 caffe.cpp:312] Batch 194, accuracy/top1 = 0.94
I0704 08:07:49.489549 27993 caffe.cpp:312] Batch 194, accuracy/top5 = 0.98
I0704 08:07:49.489552 27993 caffe.cpp:312] Batch 194, loss = 0.2
I0704 08:07:49.497681 27993 caffe.cpp:312] Batch 195, accuracy/top1 = 0.9
I0704 08:07:49.497689 27993 caffe.cpp:312] Batch 195, accuracy/top5 = 1
I0704 08:07:49.497691 27993 caffe.cpp:312] Batch 195, loss = 0.24
I0704 08:07:49.505903 27993 caffe.cpp:312] Batch 196, accuracy/top1 = 0.84
I0704 08:07:49.505910 27993 caffe.cpp:312] Batch 196, accuracy/top5 = 1
I0704 08:07:49.505913 27993 caffe.cpp:312] Batch 196, loss = 0.3
I0704 08:07:49.514127 27993 caffe.cpp:312] Batch 197, accuracy/top1 = 0.9
I0704 08:07:49.514133 27993 caffe.cpp:312] Batch 197, accuracy/top5 = 1
I0704 08:07:49.514137 27993 caffe.cpp:312] Batch 197, loss = 0.16
I0704 08:07:49.522322 27993 caffe.cpp:312] Batch 198, accuracy/top1 = 0.92
I0704 08:07:49.522330 27993 caffe.cpp:312] Batch 198, accuracy/top5 = 1
I0704 08:07:49.522341 27993 caffe.cpp:312] Batch 198, loss = 0.1
I0704 08:07:49.530760 27993 caffe.cpp:312] Batch 199, accuracy/top1 = 0.94
I0704 08:07:49.530782 27993 caffe.cpp:312] Batch 199, accuracy/top5 = 1
I0704 08:07:49.530786 27993 caffe.cpp:312] Batch 199, loss = 0.12
I0704 08:07:49.530788 27993 caffe.cpp:317] Loss: 0.166
I0704 08:07:49.530797 27993 caffe.cpp:329] accuracy/top1 = 0.9155
I0704 08:07:49.530802 27993 caffe.cpp:329] accuracy/top5 = 0.9967
I0704 08:07:49.530807 27993 caffe.cpp:329] loss = 0.166 (* 1 = 0.166 loss)
