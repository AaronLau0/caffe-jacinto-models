Logging output to training/cifar10_jacintonet11v2_2017-08-01_13-11-28/train-log_2017-08-01_13-11-28.txt
training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg
training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse
training/cifar10_jacintonet11v2_2017-08-01_13-11-28/test
I0801 13:11:30.970116 12832 caffe.cpp:608] This is NVCaffe 0.16.3 started at Tue Aug  1 13:11:30 2017
I0801 13:11:30.970247 12832 caffe.cpp:611] CuDNN version: 6021
I0801 13:11:30.970252 12832 caffe.cpp:612] CuBLAS version: 8000
I0801 13:11:30.970252 12832 caffe.cpp:613] CUDA version: 8000
I0801 13:11:30.970254 12832 caffe.cpp:614] CUDA driver version: 8000
I0801 13:11:31.235533 12832 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0801 13:11:31.236104 12832 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0801 13:11:31.236624 12832 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0801 13:11:31.237141 12832 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0801 13:11:31.237150 12832 caffe.cpp:208] Using GPUs 0, 1, 2
I0801 13:11:31.237471 12832 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0801 13:11:31.237793 12832 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0801 13:11:31.238114 12832 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0801 13:11:31.238155 12832 solver.cpp:42] Solver data type: FLOAT
I0801 13:11:31.238185 12832 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.1
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: true
iter_size: 1
type: "SGD"
I0801 13:11:31.245029 12832 solver.cpp:77] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/train.prototxt
I0801 13:11:31.245455 12832 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0801 13:11:31.245462 12832 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0801 13:11:31.245486 12832 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0801 13:11:31.245671 12832 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 22
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0801 13:11:31.245775 12832 net.cpp:104] Using FLOAT as default forward math type
I0801 13:11:31.245782 12832 net.cpp:110] Using FLOAT as default backward math type
I0801 13:11:31.245786 12832 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0801 13:11:31.245791 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.245833 12832 net.cpp:184] Created Layer data (0)
I0801 13:11:31.245839 12832 net.cpp:530] data -> data
I0801 13:11:31.245851 12832 net.cpp:530] data -> label
I0801 13:11:31.245874 12832 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0801 13:11:31.245890 12832 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:11:31.247082 12870 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0801 13:11:31.248091 12832 data_layer.cpp:184] [0] ReshapePrefetch 22, 3, 32, 32
I0801 13:11:31.248160 12832 data_layer.cpp:208] [0] Output data size: 22, 3, 32, 32
I0801 13:11:31.248167 12832 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:11:31.248188 12832 net.cpp:245] Setting up data
I0801 13:11:31.248198 12832 net.cpp:252] TRAIN Top shape for layer 0 'data' 22 3 32 32 (67584)
I0801 13:11:31.248208 12832 net.cpp:252] TRAIN Top shape for layer 0 'data' 22 (22)
I0801 13:11:31.248215 12832 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0801 13:11:31.248220 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.248232 12832 net.cpp:184] Created Layer data/bias (1)
I0801 13:11:31.248237 12832 net.cpp:561] data/bias <- data
I0801 13:11:31.248246 12832 net.cpp:530] data/bias -> data/bias
I0801 13:11:31.250210 12832 net.cpp:245] Setting up data/bias
I0801 13:11:31.250221 12832 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 22 3 32 32 (67584)
I0801 13:11:31.250231 12832 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0801 13:11:31.250236 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.250249 12832 net.cpp:184] Created Layer conv1a (2)
I0801 13:11:31.250254 12832 net.cpp:561] conv1a <- data/bias
I0801 13:11:31.250259 12832 net.cpp:530] conv1a -> conv1a
I0801 13:11:31.548847 12832 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 0  (limit 8.15G, req 0G)
I0801 13:11:31.548869 12832 net.cpp:245] Setting up conv1a
I0801 13:11:31.548877 12832 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 22 32 32 32 (720896)
I0801 13:11:31.548888 12832 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0801 13:11:31.548894 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.548907 12832 net.cpp:184] Created Layer conv1a/bn (3)
I0801 13:11:31.548912 12832 net.cpp:561] conv1a/bn <- conv1a
I0801 13:11:31.548918 12832 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0801 13:11:31.549579 12832 net.cpp:245] Setting up conv1a/bn
I0801 13:11:31.549587 12832 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 22 32 32 32 (720896)
I0801 13:11:31.549598 12832 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0801 13:11:31.549602 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.549610 12832 net.cpp:184] Created Layer conv1a/relu (4)
I0801 13:11:31.549614 12832 net.cpp:561] conv1a/relu <- conv1a
I0801 13:11:31.549618 12832 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0801 13:11:31.549633 12832 net.cpp:245] Setting up conv1a/relu
I0801 13:11:31.549638 12832 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 22 32 32 32 (720896)
I0801 13:11:31.549643 12832 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0801 13:11:31.549646 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.549656 12832 net.cpp:184] Created Layer conv1b (5)
I0801 13:11:31.549660 12832 net.cpp:561] conv1b <- conv1a
I0801 13:11:31.549665 12832 net.cpp:530] conv1b -> conv1b
I0801 13:11:31.556358 12832 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.13G, req 0G)
I0801 13:11:31.556370 12832 net.cpp:245] Setting up conv1b
I0801 13:11:31.556378 12832 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 22 32 32 32 (720896)
I0801 13:11:31.556386 12832 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0801 13:11:31.556391 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.556399 12832 net.cpp:184] Created Layer conv1b/bn (6)
I0801 13:11:31.556403 12832 net.cpp:561] conv1b/bn <- conv1b
I0801 13:11:31.556416 12832 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0801 13:11:31.557029 12832 net.cpp:245] Setting up conv1b/bn
I0801 13:11:31.557037 12832 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 22 32 32 32 (720896)
I0801 13:11:31.557046 12832 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0801 13:11:31.557050 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.557056 12832 net.cpp:184] Created Layer conv1b/relu (7)
I0801 13:11:31.557060 12832 net.cpp:561] conv1b/relu <- conv1b
I0801 13:11:31.557065 12832 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0801 13:11:31.557071 12832 net.cpp:245] Setting up conv1b/relu
I0801 13:11:31.557075 12832 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 22 32 32 32 (720896)
I0801 13:11:31.557080 12832 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0801 13:11:31.557083 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.557091 12832 net.cpp:184] Created Layer pool1 (8)
I0801 13:11:31.557096 12832 net.cpp:561] pool1 <- conv1b
I0801 13:11:31.557099 12832 net.cpp:530] pool1 -> pool1
I0801 13:11:31.557173 12832 net.cpp:245] Setting up pool1
I0801 13:11:31.557178 12832 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 22 32 32 32 (720896)
I0801 13:11:31.557183 12832 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0801 13:11:31.557188 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.557199 12832 net.cpp:184] Created Layer res2a_branch2a (9)
I0801 13:11:31.557204 12832 net.cpp:561] res2a_branch2a <- pool1
I0801 13:11:31.557209 12832 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0801 13:11:31.568933 12832 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.11G, req 0G)
I0801 13:11:31.568948 12832 net.cpp:245] Setting up res2a_branch2a
I0801 13:11:31.568954 12832 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 22 64 32 32 (1441792)
I0801 13:11:31.568964 12832 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0801 13:11:31.568969 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.568976 12832 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0801 13:11:31.568979 12832 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0801 13:11:31.568984 12832 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0801 13:11:31.569608 12832 net.cpp:245] Setting up res2a_branch2a/bn
I0801 13:11:31.569617 12832 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 22 64 32 32 (1441792)
I0801 13:11:31.569627 12832 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0801 13:11:31.569630 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.569638 12832 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0801 13:11:31.569641 12832 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0801 13:11:31.569646 12832 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0801 13:11:31.569653 12832 net.cpp:245] Setting up res2a_branch2a/relu
I0801 13:11:31.569656 12832 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 22 64 32 32 (1441792)
I0801 13:11:31.569661 12832 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0801 13:11:31.569666 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.569675 12832 net.cpp:184] Created Layer res2a_branch2b (12)
I0801 13:11:31.569679 12832 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0801 13:11:31.569684 12832 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0801 13:11:31.576776 12832 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.1G, req 0G)
I0801 13:11:31.576794 12832 net.cpp:245] Setting up res2a_branch2b
I0801 13:11:31.576829 12832 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 22 64 32 32 (1441792)
I0801 13:11:31.576840 12832 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0801 13:11:31.576854 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.576869 12832 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0801 13:11:31.576874 12832 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0801 13:11:31.576881 12832 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0801 13:11:31.577540 12832 net.cpp:245] Setting up res2a_branch2b/bn
I0801 13:11:31.577548 12832 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 22 64 32 32 (1441792)
I0801 13:11:31.577558 12832 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0801 13:11:31.577561 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.577566 12832 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0801 13:11:31.577570 12832 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0801 13:11:31.577574 12832 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0801 13:11:31.577580 12832 net.cpp:245] Setting up res2a_branch2b/relu
I0801 13:11:31.577586 12832 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 22 64 32 32 (1441792)
I0801 13:11:31.577590 12832 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0801 13:11:31.577594 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.577666 12832 net.cpp:184] Created Layer pool2 (15)
I0801 13:11:31.577672 12832 net.cpp:561] pool2 <- res2a_branch2b
I0801 13:11:31.577675 12832 net.cpp:530] pool2 -> pool2
I0801 13:11:31.577744 12832 net.cpp:245] Setting up pool2
I0801 13:11:31.577750 12832 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 22 64 16 16 (360448)
I0801 13:11:31.577754 12832 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0801 13:11:31.577759 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.577769 12832 net.cpp:184] Created Layer res3a_branch2a (16)
I0801 13:11:31.577774 12832 net.cpp:561] res3a_branch2a <- pool2
I0801 13:11:31.577778 12832 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0801 13:11:31.590695 12832 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.09G, req 0.01G)
I0801 13:11:31.590711 12832 net.cpp:245] Setting up res3a_branch2a
I0801 13:11:31.590718 12832 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 22 128 16 16 (720896)
I0801 13:11:31.590728 12832 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0801 13:11:31.590732 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.590741 12832 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0801 13:11:31.590745 12832 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0801 13:11:31.590750 12832 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0801 13:11:31.591387 12832 net.cpp:245] Setting up res3a_branch2a/bn
I0801 13:11:31.591394 12832 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 22 128 16 16 (720896)
I0801 13:11:31.591404 12832 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0801 13:11:31.591409 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.591414 12832 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0801 13:11:31.591419 12832 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0801 13:11:31.591423 12832 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0801 13:11:31.591429 12832 net.cpp:245] Setting up res3a_branch2a/relu
I0801 13:11:31.591434 12832 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 22 128 16 16 (720896)
I0801 13:11:31.591439 12832 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0801 13:11:31.591451 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.591461 12832 net.cpp:184] Created Layer res3a_branch2b (19)
I0801 13:11:31.591465 12832 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0801 13:11:31.591470 12832 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0801 13:11:31.596166 12832 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.08G, req 0.01G)
I0801 13:11:31.596177 12832 net.cpp:245] Setting up res3a_branch2b
I0801 13:11:31.596184 12832 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 22 128 16 16 (720896)
I0801 13:11:31.596192 12832 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0801 13:11:31.596196 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.596204 12832 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0801 13:11:31.596209 12832 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0801 13:11:31.596212 12832 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0801 13:11:31.596845 12832 net.cpp:245] Setting up res3a_branch2b/bn
I0801 13:11:31.596854 12832 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 22 128 16 16 (720896)
I0801 13:11:31.596863 12832 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0801 13:11:31.596866 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.596871 12832 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0801 13:11:31.596875 12832 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0801 13:11:31.596879 12832 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0801 13:11:31.596886 12832 net.cpp:245] Setting up res3a_branch2b/relu
I0801 13:11:31.596891 12832 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 22 128 16 16 (720896)
I0801 13:11:31.596895 12832 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0801 13:11:31.596900 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.596906 12832 net.cpp:184] Created Layer pool3 (22)
I0801 13:11:31.596910 12832 net.cpp:561] pool3 <- res3a_branch2b
I0801 13:11:31.596915 12832 net.cpp:530] pool3 -> pool3
I0801 13:11:31.596981 12832 net.cpp:245] Setting up pool3
I0801 13:11:31.596987 12832 net.cpp:252] TRAIN Top shape for layer 22 'pool3' 22 128 16 16 (720896)
I0801 13:11:31.596990 12832 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0801 13:11:31.596995 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.597003 12832 net.cpp:184] Created Layer res4a_branch2a (23)
I0801 13:11:31.597007 12832 net.cpp:561] res4a_branch2a <- pool3
I0801 13:11:31.597012 12832 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0801 13:11:31.617228 12832 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.05G, req 0.01G)
I0801 13:11:31.617249 12832 net.cpp:245] Setting up res4a_branch2a
I0801 13:11:31.617256 12832 net.cpp:252] TRAIN Top shape for layer 23 'res4a_branch2a' 22 256 16 16 (1441792)
I0801 13:11:31.617266 12832 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0801 13:11:31.617271 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.617288 12832 net.cpp:184] Created Layer res4a_branch2a/bn (24)
I0801 13:11:31.617293 12832 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0801 13:11:31.617300 12832 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0801 13:11:31.618002 12832 net.cpp:245] Setting up res4a_branch2a/bn
I0801 13:11:31.618011 12832 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a/bn' 22 256 16 16 (1441792)
I0801 13:11:31.618021 12832 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0801 13:11:31.618032 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.618039 12832 net.cpp:184] Created Layer res4a_branch2a/relu (25)
I0801 13:11:31.618043 12832 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0801 13:11:31.618047 12832 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0801 13:11:31.618054 12832 net.cpp:245] Setting up res4a_branch2a/relu
I0801 13:11:31.618060 12832 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/relu' 22 256 16 16 (1441792)
I0801 13:11:31.618064 12832 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0801 13:11:31.618069 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.618079 12832 net.cpp:184] Created Layer res4a_branch2b (26)
I0801 13:11:31.618083 12832 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0801 13:11:31.618088 12832 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0801 13:11:31.626850 12832 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.04G, req 0.01G)
I0801 13:11:31.626864 12832 net.cpp:245] Setting up res4a_branch2b
I0801 13:11:31.626871 12832 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2b' 22 256 16 16 (1441792)
I0801 13:11:31.626879 12832 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0801 13:11:31.626883 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.626890 12832 net.cpp:184] Created Layer res4a_branch2b/bn (27)
I0801 13:11:31.626895 12832 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0801 13:11:31.626900 12832 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0801 13:11:31.627543 12832 net.cpp:245] Setting up res4a_branch2b/bn
I0801 13:11:31.627552 12832 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b/bn' 22 256 16 16 (1441792)
I0801 13:11:31.627559 12832 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0801 13:11:31.627563 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.627569 12832 net.cpp:184] Created Layer res4a_branch2b/relu (28)
I0801 13:11:31.627574 12832 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0801 13:11:31.627578 12832 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0801 13:11:31.627585 12832 net.cpp:245] Setting up res4a_branch2b/relu
I0801 13:11:31.627590 12832 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/relu' 22 256 16 16 (1441792)
I0801 13:11:31.627594 12832 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0801 13:11:31.627599 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.627605 12832 net.cpp:184] Created Layer pool4 (29)
I0801 13:11:31.627609 12832 net.cpp:561] pool4 <- res4a_branch2b
I0801 13:11:31.627614 12832 net.cpp:530] pool4 -> pool4
I0801 13:11:31.627682 12832 net.cpp:245] Setting up pool4
I0801 13:11:31.627688 12832 net.cpp:252] TRAIN Top shape for layer 29 'pool4' 22 256 8 8 (360448)
I0801 13:11:31.627693 12832 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0801 13:11:31.627697 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.627710 12832 net.cpp:184] Created Layer res5a_branch2a (30)
I0801 13:11:31.627714 12832 net.cpp:561] res5a_branch2a <- pool4
I0801 13:11:31.627718 12832 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0801 13:11:31.670059 12832 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.02G, req 0.01G)
I0801 13:11:31.670094 12832 net.cpp:245] Setting up res5a_branch2a
I0801 13:11:31.670101 12832 net.cpp:252] TRAIN Top shape for layer 30 'res5a_branch2a' 22 512 8 8 (720896)
I0801 13:11:31.670114 12832 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0801 13:11:31.670120 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.670147 12832 net.cpp:184] Created Layer res5a_branch2a/bn (31)
I0801 13:11:31.670152 12832 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0801 13:11:31.670157 12832 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0801 13:11:31.670994 12832 net.cpp:245] Setting up res5a_branch2a/bn
I0801 13:11:31.671007 12832 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a/bn' 22 512 8 8 (720896)
I0801 13:11:31.671015 12832 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0801 13:11:31.671018 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.671025 12832 net.cpp:184] Created Layer res5a_branch2a/relu (32)
I0801 13:11:31.671030 12832 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0801 13:11:31.671032 12832 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0801 13:11:31.671036 12832 net.cpp:245] Setting up res5a_branch2a/relu
I0801 13:11:31.671041 12832 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/relu' 22 512 8 8 (720896)
I0801 13:11:31.671042 12832 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0801 13:11:31.671046 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.671058 12832 net.cpp:184] Created Layer res5a_branch2b (33)
I0801 13:11:31.671061 12832 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0801 13:11:31.671064 12832 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0801 13:11:31.691817 12832 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8G, req 0.01G)
I0801 13:11:31.691834 12832 net.cpp:245] Setting up res5a_branch2b
I0801 13:11:31.691840 12832 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2b' 22 512 8 8 (720896)
I0801 13:11:31.691851 12832 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0801 13:11:31.691855 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.691869 12832 net.cpp:184] Created Layer res5a_branch2b/bn (34)
I0801 13:11:31.691872 12832 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0801 13:11:31.691875 12832 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0801 13:11:31.692595 12832 net.cpp:245] Setting up res5a_branch2b/bn
I0801 13:11:31.692605 12832 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b/bn' 22 512 8 8 (720896)
I0801 13:11:31.692610 12832 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0801 13:11:31.692615 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.692620 12832 net.cpp:184] Created Layer res5a_branch2b/relu (35)
I0801 13:11:31.692622 12832 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0801 13:11:31.692625 12832 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0801 13:11:31.692631 12832 net.cpp:245] Setting up res5a_branch2b/relu
I0801 13:11:31.692633 12832 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/relu' 22 512 8 8 (720896)
I0801 13:11:31.692636 12832 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0801 13:11:31.692639 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.692646 12832 net.cpp:184] Created Layer pool5 (36)
I0801 13:11:31.692651 12832 net.cpp:561] pool5 <- res5a_branch2b
I0801 13:11:31.692654 12832 net.cpp:530] pool5 -> pool5
I0801 13:11:31.692684 12832 net.cpp:245] Setting up pool5
I0801 13:11:31.692690 12832 net.cpp:252] TRAIN Top shape for layer 36 'pool5' 22 512 1 1 (11264)
I0801 13:11:31.692694 12832 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0801 13:11:31.692699 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.692708 12832 net.cpp:184] Created Layer fc10 (37)
I0801 13:11:31.692711 12832 net.cpp:561] fc10 <- pool5
I0801 13:11:31.692715 12832 net.cpp:530] fc10 -> fc10
I0801 13:11:31.693017 12832 net.cpp:245] Setting up fc10
I0801 13:11:31.693027 12832 net.cpp:252] TRAIN Top shape for layer 37 'fc10' 22 10 (220)
I0801 13:11:31.693033 12832 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0801 13:11:31.693037 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.693049 12832 net.cpp:184] Created Layer loss (38)
I0801 13:11:31.693053 12832 net.cpp:561] loss <- fc10
I0801 13:11:31.693058 12832 net.cpp:561] loss <- label
I0801 13:11:31.693064 12832 net.cpp:530] loss -> loss
I0801 13:11:31.693236 12832 net.cpp:245] Setting up loss
I0801 13:11:31.693244 12832 net.cpp:252] TRAIN Top shape for layer 38 'loss' (1)
I0801 13:11:31.693248 12832 net.cpp:256]     with loss weight 1
I0801 13:11:31.693255 12832 net.cpp:323] loss needs backward computation.
I0801 13:11:31.693260 12832 net.cpp:323] fc10 needs backward computation.
I0801 13:11:31.693264 12832 net.cpp:323] pool5 needs backward computation.
I0801 13:11:31.693267 12832 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0801 13:11:31.693271 12832 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0801 13:11:31.693275 12832 net.cpp:323] res5a_branch2b needs backward computation.
I0801 13:11:31.693280 12832 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0801 13:11:31.693284 12832 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0801 13:11:31.693287 12832 net.cpp:323] res5a_branch2a needs backward computation.
I0801 13:11:31.693292 12832 net.cpp:323] pool4 needs backward computation.
I0801 13:11:31.693296 12832 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0801 13:11:31.693300 12832 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0801 13:11:31.693305 12832 net.cpp:323] res4a_branch2b needs backward computation.
I0801 13:11:31.693310 12832 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0801 13:11:31.693313 12832 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0801 13:11:31.693317 12832 net.cpp:323] res4a_branch2a needs backward computation.
I0801 13:11:31.693320 12832 net.cpp:323] pool3 needs backward computation.
I0801 13:11:31.693325 12832 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0801 13:11:31.693328 12832 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0801 13:11:31.693331 12832 net.cpp:323] res3a_branch2b needs backward computation.
I0801 13:11:31.693336 12832 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0801 13:11:31.693338 12832 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0801 13:11:31.693342 12832 net.cpp:323] res3a_branch2a needs backward computation.
I0801 13:11:31.693346 12832 net.cpp:323] pool2 needs backward computation.
I0801 13:11:31.693352 12832 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0801 13:11:31.693356 12832 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0801 13:11:31.693361 12832 net.cpp:323] res2a_branch2b needs backward computation.
I0801 13:11:31.693364 12832 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0801 13:11:31.693368 12832 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0801 13:11:31.693372 12832 net.cpp:323] res2a_branch2a needs backward computation.
I0801 13:11:31.693377 12832 net.cpp:323] pool1 needs backward computation.
I0801 13:11:31.693382 12832 net.cpp:323] conv1b/relu needs backward computation.
I0801 13:11:31.693385 12832 net.cpp:323] conv1b/bn needs backward computation.
I0801 13:11:31.693389 12832 net.cpp:323] conv1b needs backward computation.
I0801 13:11:31.693393 12832 net.cpp:323] conv1a/relu needs backward computation.
I0801 13:11:31.693397 12832 net.cpp:323] conv1a/bn needs backward computation.
I0801 13:11:31.693401 12832 net.cpp:323] conv1a needs backward computation.
I0801 13:11:31.693405 12832 net.cpp:325] data/bias does not need backward computation.
I0801 13:11:31.693410 12832 net.cpp:325] data does not need backward computation.
I0801 13:11:31.693414 12832 net.cpp:367] This network produces output loss
I0801 13:11:31.693452 12832 net.cpp:389] Top memory (TRAIN) required for data: 121110528 diff: 121110536
I0801 13:11:31.693456 12832 net.cpp:392] Bottom memory (TRAIN) required for data: 121110528 diff: 121110528
I0801 13:11:31.693460 12832 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 80740352 diff: 80740352
I0801 13:11:31.693464 12832 net.cpp:398] Parameters memory (TRAIN) required for data: 9450960 diff: 9450960
I0801 13:11:31.693469 12832 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0801 13:11:31.693472 12832 net.cpp:407] Network initialization done.
I0801 13:11:31.693838 12832 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/test.prototxt
W0801 13:11:31.693897 12832 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0801 13:11:31.694025 12832 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 17
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0801 13:11:31.694119 12832 net.cpp:104] Using FLOAT as default forward math type
I0801 13:11:31.694124 12832 net.cpp:110] Using FLOAT as default backward math type
I0801 13:11:31.694128 12832 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0801 13:11:31.694133 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.694145 12832 net.cpp:184] Created Layer data (0)
I0801 13:11:31.694149 12832 net.cpp:530] data -> data
I0801 13:11:31.694154 12832 net.cpp:530] data -> label
I0801 13:11:31.694164 12832 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0801 13:11:31.694170 12832 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:11:31.697546 12893 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0801 13:11:31.697643 12832 data_layer.cpp:184] (0) ReshapePrefetch 17, 3, 32, 32
I0801 13:11:31.697721 12832 data_layer.cpp:208] (0) Output data size: 17, 3, 32, 32
I0801 13:11:31.697726 12832 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:11:31.697741 12832 net.cpp:245] Setting up data
I0801 13:11:31.697746 12832 net.cpp:252] TEST Top shape for layer 0 'data' 17 3 32 32 (52224)
I0801 13:11:31.697749 12832 net.cpp:252] TEST Top shape for layer 0 'data' 17 (17)
I0801 13:11:31.697752 12832 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0801 13:11:31.697755 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.697760 12832 net.cpp:184] Created Layer label_data_1_split (1)
I0801 13:11:31.697762 12832 net.cpp:561] label_data_1_split <- label
I0801 13:11:31.697765 12832 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0801 13:11:31.697775 12832 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0801 13:11:31.697778 12832 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0801 13:11:31.697846 12832 net.cpp:245] Setting up label_data_1_split
I0801 13:11:31.697851 12832 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0801 13:11:31.697854 12832 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0801 13:11:31.697857 12832 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0801 13:11:31.697860 12832 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0801 13:11:31.697863 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.697868 12832 net.cpp:184] Created Layer data/bias (2)
I0801 13:11:31.697871 12832 net.cpp:561] data/bias <- data
I0801 13:11:31.697875 12832 net.cpp:530] data/bias -> data/bias
I0801 13:11:31.698026 12832 net.cpp:245] Setting up data/bias
I0801 13:11:31.698034 12832 net.cpp:252] TEST Top shape for layer 2 'data/bias' 17 3 32 32 (52224)
I0801 13:11:31.698040 12832 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0801 13:11:31.698042 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.698051 12832 net.cpp:184] Created Layer conv1a (3)
I0801 13:11:31.698055 12832 net.cpp:561] conv1a <- data/bias
I0801 13:11:31.698057 12832 net.cpp:530] conv1a -> conv1a
I0801 13:11:31.698526 12894 data_layer.cpp:97] (0) Parser threads: 1
I0801 13:11:31.698535 12894 data_layer.cpp:99] (0) Transformer threads: 1
I0801 13:11:31.701913 12832 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8G, req 0.01G)
I0801 13:11:31.701933 12832 net.cpp:245] Setting up conv1a
I0801 13:11:31.701941 12832 net.cpp:252] TEST Top shape for layer 3 'conv1a' 17 32 32 32 (557056)
I0801 13:11:31.701951 12832 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0801 13:11:31.701957 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.701968 12832 net.cpp:184] Created Layer conv1a/bn (4)
I0801 13:11:31.701973 12832 net.cpp:561] conv1a/bn <- conv1a
I0801 13:11:31.701977 12832 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0801 13:11:31.702977 12832 net.cpp:245] Setting up conv1a/bn
I0801 13:11:31.702991 12832 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 17 32 32 32 (557056)
I0801 13:11:31.703004 12832 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0801 13:11:31.703011 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.703017 12832 net.cpp:184] Created Layer conv1a/relu (5)
I0801 13:11:31.703023 12832 net.cpp:561] conv1a/relu <- conv1a
I0801 13:11:31.703028 12832 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0801 13:11:31.703035 12832 net.cpp:245] Setting up conv1a/relu
I0801 13:11:31.703042 12832 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 17 32 32 32 (557056)
I0801 13:11:31.703045 12832 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0801 13:11:31.703049 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.703066 12832 net.cpp:184] Created Layer conv1b (6)
I0801 13:11:31.703073 12832 net.cpp:561] conv1b <- conv1a
I0801 13:11:31.703076 12832 net.cpp:530] conv1b -> conv1b
I0801 13:11:31.706704 12832 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8G, req 0.01G)
I0801 13:11:31.706717 12832 net.cpp:245] Setting up conv1b
I0801 13:11:31.706722 12832 net.cpp:252] TEST Top shape for layer 6 'conv1b' 17 32 32 32 (557056)
I0801 13:11:31.706729 12832 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0801 13:11:31.706733 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.706740 12832 net.cpp:184] Created Layer conv1b/bn (7)
I0801 13:11:31.706753 12832 net.cpp:561] conv1b/bn <- conv1b
I0801 13:11:31.706759 12832 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0801 13:11:31.707458 12832 net.cpp:245] Setting up conv1b/bn
I0801 13:11:31.707465 12832 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 17 32 32 32 (557056)
I0801 13:11:31.707471 12832 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0801 13:11:31.707475 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.707480 12832 net.cpp:184] Created Layer conv1b/relu (8)
I0801 13:11:31.707484 12832 net.cpp:561] conv1b/relu <- conv1b
I0801 13:11:31.707486 12832 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0801 13:11:31.707490 12832 net.cpp:245] Setting up conv1b/relu
I0801 13:11:31.707494 12832 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 17 32 32 32 (557056)
I0801 13:11:31.707496 12832 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0801 13:11:31.707501 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.707509 12832 net.cpp:184] Created Layer pool1 (9)
I0801 13:11:31.707512 12832 net.cpp:561] pool1 <- conv1b
I0801 13:11:31.707515 12832 net.cpp:530] pool1 -> pool1
I0801 13:11:31.707586 12832 net.cpp:245] Setting up pool1
I0801 13:11:31.707590 12832 net.cpp:252] TEST Top shape for layer 9 'pool1' 17 32 32 32 (557056)
I0801 13:11:31.707593 12832 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0801 13:11:31.707598 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.707612 12832 net.cpp:184] Created Layer res2a_branch2a (10)
I0801 13:11:31.707615 12832 net.cpp:561] res2a_branch2a <- pool1
I0801 13:11:31.707618 12832 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0801 13:11:31.711448 12832 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.99G, req 0.01G)
I0801 13:11:31.711458 12832 net.cpp:245] Setting up res2a_branch2a
I0801 13:11:31.711462 12832 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 17 64 32 32 (1114112)
I0801 13:11:31.711468 12832 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0801 13:11:31.711472 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.711477 12832 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0801 13:11:31.711479 12832 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0801 13:11:31.711482 12832 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0801 13:11:31.712173 12832 net.cpp:245] Setting up res2a_branch2a/bn
I0801 13:11:31.712182 12832 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 17 64 32 32 (1114112)
I0801 13:11:31.712186 12832 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0801 13:11:31.712190 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.712194 12832 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0801 13:11:31.712198 12832 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0801 13:11:31.712199 12832 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0801 13:11:31.712205 12832 net.cpp:245] Setting up res2a_branch2a/relu
I0801 13:11:31.712209 12832 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 17 64 32 32 (1114112)
I0801 13:11:31.712213 12832 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0801 13:11:31.712215 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.712225 12832 net.cpp:184] Created Layer res2a_branch2b (13)
I0801 13:11:31.712229 12832 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0801 13:11:31.712232 12832 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0801 13:11:31.715647 12832 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.98G, req 0.01G)
I0801 13:11:31.715661 12832 net.cpp:245] Setting up res2a_branch2b
I0801 13:11:31.715675 12832 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 17 64 32 32 (1114112)
I0801 13:11:31.715682 12832 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0801 13:11:31.715684 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.715690 12832 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0801 13:11:31.715693 12832 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0801 13:11:31.715698 12832 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0801 13:11:31.716403 12832 net.cpp:245] Setting up res2a_branch2b/bn
I0801 13:11:31.716411 12832 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 17 64 32 32 (1114112)
I0801 13:11:31.716418 12832 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0801 13:11:31.716419 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.716423 12832 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0801 13:11:31.716425 12832 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0801 13:11:31.716428 12832 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0801 13:11:31.716433 12832 net.cpp:245] Setting up res2a_branch2b/relu
I0801 13:11:31.716434 12832 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 17 64 32 32 (1114112)
I0801 13:11:31.716437 12832 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0801 13:11:31.716439 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.716444 12832 net.cpp:184] Created Layer pool2 (16)
I0801 13:11:31.716446 12832 net.cpp:561] pool2 <- res2a_branch2b
I0801 13:11:31.716449 12832 net.cpp:530] pool2 -> pool2
I0801 13:11:31.716511 12832 net.cpp:245] Setting up pool2
I0801 13:11:31.716516 12832 net.cpp:252] TEST Top shape for layer 16 'pool2' 17 64 16 16 (278528)
I0801 13:11:31.716518 12832 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0801 13:11:31.716521 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.716528 12832 net.cpp:184] Created Layer res3a_branch2a (17)
I0801 13:11:31.716531 12832 net.cpp:561] res3a_branch2a <- pool2
I0801 13:11:31.716533 12832 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0801 13:11:31.722843 12832 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.97G, req 0.01G)
I0801 13:11:31.722862 12832 net.cpp:245] Setting up res3a_branch2a
I0801 13:11:31.722867 12832 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 17 128 16 16 (557056)
I0801 13:11:31.722874 12832 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0801 13:11:31.722878 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.722885 12832 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0801 13:11:31.722888 12832 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0801 13:11:31.722892 12832 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0801 13:11:31.723605 12832 net.cpp:245] Setting up res3a_branch2a/bn
I0801 13:11:31.723613 12832 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 17 128 16 16 (557056)
I0801 13:11:31.723620 12832 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0801 13:11:31.723623 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.723628 12832 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0801 13:11:31.723630 12832 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0801 13:11:31.723633 12832 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0801 13:11:31.723636 12832 net.cpp:245] Setting up res3a_branch2a/relu
I0801 13:11:31.723639 12832 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 17 128 16 16 (557056)
I0801 13:11:31.723640 12832 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0801 13:11:31.723652 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.723661 12832 net.cpp:184] Created Layer res3a_branch2b (20)
I0801 13:11:31.723664 12832 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0801 13:11:31.723666 12832 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0801 13:11:31.727133 12832 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.97G, req 0.01G)
I0801 13:11:31.727144 12832 net.cpp:245] Setting up res3a_branch2b
I0801 13:11:31.727149 12832 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 17 128 16 16 (557056)
I0801 13:11:31.727154 12832 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0801 13:11:31.727157 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.727166 12832 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0801 13:11:31.727169 12832 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0801 13:11:31.727172 12832 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0801 13:11:31.727954 12832 net.cpp:245] Setting up res3a_branch2b/bn
I0801 13:11:31.727963 12832 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 17 128 16 16 (557056)
I0801 13:11:31.727969 12832 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0801 13:11:31.727972 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.727975 12832 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0801 13:11:31.727978 12832 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0801 13:11:31.727980 12832 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0801 13:11:31.727983 12832 net.cpp:245] Setting up res3a_branch2b/relu
I0801 13:11:31.727987 12832 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 17 128 16 16 (557056)
I0801 13:11:31.727989 12832 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0801 13:11:31.727991 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.727995 12832 net.cpp:184] Created Layer pool3 (23)
I0801 13:11:31.727998 12832 net.cpp:561] pool3 <- res3a_branch2b
I0801 13:11:31.728000 12832 net.cpp:530] pool3 -> pool3
I0801 13:11:31.728065 12832 net.cpp:245] Setting up pool3
I0801 13:11:31.728070 12832 net.cpp:252] TEST Top shape for layer 23 'pool3' 17 128 16 16 (557056)
I0801 13:11:31.728072 12832 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0801 13:11:31.728075 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.728091 12832 net.cpp:184] Created Layer res4a_branch2a (24)
I0801 13:11:31.728096 12832 net.cpp:561] res4a_branch2a <- pool3
I0801 13:11:31.728097 12832 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0801 13:11:31.739334 12832 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0.01G)
I0801 13:11:31.739351 12832 net.cpp:245] Setting up res4a_branch2a
I0801 13:11:31.739357 12832 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 17 256 16 16 (1114112)
I0801 13:11:31.739365 12832 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0801 13:11:31.739368 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.739377 12832 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0801 13:11:31.739380 12832 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0801 13:11:31.739384 12832 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0801 13:11:31.740125 12832 net.cpp:245] Setting up res4a_branch2a/bn
I0801 13:11:31.740134 12832 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 17 256 16 16 (1114112)
I0801 13:11:31.740139 12832 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0801 13:11:31.740142 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.740154 12832 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0801 13:11:31.740157 12832 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0801 13:11:31.740160 12832 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0801 13:11:31.740164 12832 net.cpp:245] Setting up res4a_branch2a/relu
I0801 13:11:31.740167 12832 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 17 256 16 16 (1114112)
I0801 13:11:31.740170 12832 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0801 13:11:31.740173 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.740180 12832 net.cpp:184] Created Layer res4a_branch2b (27)
I0801 13:11:31.740183 12832 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0801 13:11:31.740186 12832 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0801 13:11:31.745883 12832 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.95G, req 0.01G)
I0801 13:11:31.745898 12832 net.cpp:245] Setting up res4a_branch2b
I0801 13:11:31.745903 12832 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 17 256 16 16 (1114112)
I0801 13:11:31.745908 12832 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0801 13:11:31.745913 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.745919 12832 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0801 13:11:31.745923 12832 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0801 13:11:31.745925 12832 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0801 13:11:31.746639 12832 net.cpp:245] Setting up res4a_branch2b/bn
I0801 13:11:31.746646 12832 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 17 256 16 16 (1114112)
I0801 13:11:31.746652 12832 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0801 13:11:31.746655 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.746659 12832 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0801 13:11:31.746661 12832 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0801 13:11:31.746664 12832 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0801 13:11:31.746667 12832 net.cpp:245] Setting up res4a_branch2b/relu
I0801 13:11:31.746670 12832 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 17 256 16 16 (1114112)
I0801 13:11:31.746671 12832 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0801 13:11:31.746675 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.746677 12832 net.cpp:184] Created Layer pool4 (30)
I0801 13:11:31.746681 12832 net.cpp:561] pool4 <- res4a_branch2b
I0801 13:11:31.746685 12832 net.cpp:530] pool4 -> pool4
I0801 13:11:31.746752 12832 net.cpp:245] Setting up pool4
I0801 13:11:31.746757 12832 net.cpp:252] TEST Top shape for layer 30 'pool4' 17 256 8 8 (278528)
I0801 13:11:31.746759 12832 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0801 13:11:31.746762 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.746768 12832 net.cpp:184] Created Layer res5a_branch2a (31)
I0801 13:11:31.746772 12832 net.cpp:561] res5a_branch2a <- pool4
I0801 13:11:31.746773 12832 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0801 13:11:31.778327 12832 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.94G, req 0.01G)
I0801 13:11:31.778345 12832 net.cpp:245] Setting up res5a_branch2a
I0801 13:11:31.778352 12832 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 17 512 8 8 (557056)
I0801 13:11:31.778358 12832 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0801 13:11:31.778362 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.778385 12832 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0801 13:11:31.778388 12832 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0801 13:11:31.778393 12832 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0801 13:11:31.779140 12832 net.cpp:245] Setting up res5a_branch2a/bn
I0801 13:11:31.779148 12832 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 17 512 8 8 (557056)
I0801 13:11:31.779155 12832 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0801 13:11:31.779158 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.779162 12832 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0801 13:11:31.779165 12832 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0801 13:11:31.779166 12832 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0801 13:11:31.779170 12832 net.cpp:245] Setting up res5a_branch2a/relu
I0801 13:11:31.779173 12832 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 17 512 8 8 (557056)
I0801 13:11:31.779175 12832 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0801 13:11:31.779177 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.779184 12832 net.cpp:184] Created Layer res5a_branch2b (34)
I0801 13:11:31.779187 12832 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0801 13:11:31.779189 12832 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0801 13:11:31.796063 12832 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.93G, req 0.01G)
I0801 13:11:31.796080 12832 net.cpp:245] Setting up res5a_branch2b
I0801 13:11:31.796087 12832 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 17 512 8 8 (557056)
I0801 13:11:31.796097 12832 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0801 13:11:31.796100 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.796108 12832 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0801 13:11:31.796111 12832 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0801 13:11:31.796114 12832 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0801 13:11:31.796846 12832 net.cpp:245] Setting up res5a_branch2b/bn
I0801 13:11:31.796854 12832 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 17 512 8 8 (557056)
I0801 13:11:31.796860 12832 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0801 13:11:31.796864 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.796932 12832 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0801 13:11:31.796937 12832 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0801 13:11:31.796941 12832 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0801 13:11:31.796944 12832 net.cpp:245] Setting up res5a_branch2b/relu
I0801 13:11:31.796947 12832 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 17 512 8 8 (557056)
I0801 13:11:31.796950 12832 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0801 13:11:31.796953 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.796962 12832 net.cpp:184] Created Layer pool5 (37)
I0801 13:11:31.796965 12832 net.cpp:561] pool5 <- res5a_branch2b
I0801 13:11:31.796968 12832 net.cpp:530] pool5 -> pool5
I0801 13:11:31.796999 12832 net.cpp:245] Setting up pool5
I0801 13:11:31.797004 12832 net.cpp:252] TEST Top shape for layer 37 'pool5' 17 512 1 1 (8704)
I0801 13:11:31.797008 12832 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0801 13:11:31.797010 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.797014 12832 net.cpp:184] Created Layer fc10 (38)
I0801 13:11:31.797018 12832 net.cpp:561] fc10 <- pool5
I0801 13:11:31.797020 12832 net.cpp:530] fc10 -> fc10
I0801 13:11:31.797319 12832 net.cpp:245] Setting up fc10
I0801 13:11:31.797338 12832 net.cpp:252] TEST Top shape for layer 38 'fc10' 17 10 (170)
I0801 13:11:31.797343 12832 layer_factory.hpp:136] Creating layer 'fc10_fc10_0_split' of type 'Split'
I0801 13:11:31.797346 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.797350 12832 net.cpp:184] Created Layer fc10_fc10_0_split (39)
I0801 13:11:31.797353 12832 net.cpp:561] fc10_fc10_0_split <- fc10
I0801 13:11:31.797356 12832 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0801 13:11:31.797359 12832 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0801 13:11:31.797363 12832 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0801 13:11:31.797441 12832 net.cpp:245] Setting up fc10_fc10_0_split
I0801 13:11:31.797446 12832 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0801 13:11:31.797449 12832 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0801 13:11:31.797452 12832 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0801 13:11:31.797454 12832 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0801 13:11:31.797457 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.797466 12832 net.cpp:184] Created Layer loss (40)
I0801 13:11:31.797469 12832 net.cpp:561] loss <- fc10_fc10_0_split_0
I0801 13:11:31.797472 12832 net.cpp:561] loss <- label_data_1_split_0
I0801 13:11:31.797475 12832 net.cpp:530] loss -> loss
I0801 13:11:31.797632 12832 net.cpp:245] Setting up loss
I0801 13:11:31.797639 12832 net.cpp:252] TEST Top shape for layer 40 'loss' (1)
I0801 13:11:31.797641 12832 net.cpp:256]     with loss weight 1
I0801 13:11:31.797646 12832 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0801 13:11:31.797649 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.797657 12832 net.cpp:184] Created Layer accuracy/top1 (41)
I0801 13:11:31.797660 12832 net.cpp:561] accuracy/top1 <- fc10_fc10_0_split_1
I0801 13:11:31.797663 12832 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0801 13:11:31.797667 12832 net.cpp:530] accuracy/top1 -> accuracy/top1
I0801 13:11:31.797672 12832 net.cpp:245] Setting up accuracy/top1
I0801 13:11:31.797675 12832 net.cpp:252] TEST Top shape for layer 41 'accuracy/top1' (1)
I0801 13:11:31.797678 12832 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0801 13:11:31.797680 12832 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:11:31.797684 12832 net.cpp:184] Created Layer accuracy/top5 (42)
I0801 13:11:31.797686 12832 net.cpp:561] accuracy/top5 <- fc10_fc10_0_split_2
I0801 13:11:31.797689 12832 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0801 13:11:31.797693 12832 net.cpp:530] accuracy/top5 -> accuracy/top5
I0801 13:11:31.797695 12832 net.cpp:245] Setting up accuracy/top5
I0801 13:11:31.797699 12832 net.cpp:252] TEST Top shape for layer 42 'accuracy/top5' (1)
I0801 13:11:31.797703 12832 net.cpp:325] accuracy/top5 does not need backward computation.
I0801 13:11:31.797704 12832 net.cpp:325] accuracy/top1 does not need backward computation.
I0801 13:11:31.797708 12832 net.cpp:323] loss needs backward computation.
I0801 13:11:31.797709 12832 net.cpp:323] fc10_fc10_0_split needs backward computation.
I0801 13:11:31.797713 12832 net.cpp:323] fc10 needs backward computation.
I0801 13:11:31.797714 12832 net.cpp:323] pool5 needs backward computation.
I0801 13:11:31.797716 12832 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0801 13:11:31.797719 12832 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0801 13:11:31.797720 12832 net.cpp:323] res5a_branch2b needs backward computation.
I0801 13:11:31.797722 12832 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0801 13:11:31.797725 12832 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0801 13:11:31.797727 12832 net.cpp:323] res5a_branch2a needs backward computation.
I0801 13:11:31.797736 12832 net.cpp:323] pool4 needs backward computation.
I0801 13:11:31.797739 12832 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0801 13:11:31.797741 12832 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0801 13:11:31.797744 12832 net.cpp:323] res4a_branch2b needs backward computation.
I0801 13:11:31.797746 12832 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0801 13:11:31.797749 12832 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0801 13:11:31.797751 12832 net.cpp:323] res4a_branch2a needs backward computation.
I0801 13:11:31.797755 12832 net.cpp:323] pool3 needs backward computation.
I0801 13:11:31.797756 12832 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0801 13:11:31.797758 12832 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0801 13:11:31.797761 12832 net.cpp:323] res3a_branch2b needs backward computation.
I0801 13:11:31.797763 12832 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0801 13:11:31.797765 12832 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0801 13:11:31.797767 12832 net.cpp:323] res3a_branch2a needs backward computation.
I0801 13:11:31.797770 12832 net.cpp:323] pool2 needs backward computation.
I0801 13:11:31.797772 12832 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0801 13:11:31.797775 12832 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0801 13:11:31.797777 12832 net.cpp:323] res2a_branch2b needs backward computation.
I0801 13:11:31.797780 12832 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0801 13:11:31.797782 12832 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0801 13:11:31.797785 12832 net.cpp:323] res2a_branch2a needs backward computation.
I0801 13:11:31.797786 12832 net.cpp:323] pool1 needs backward computation.
I0801 13:11:31.797790 12832 net.cpp:323] conv1b/relu needs backward computation.
I0801 13:11:31.797791 12832 net.cpp:323] conv1b/bn needs backward computation.
I0801 13:11:31.797794 12832 net.cpp:323] conv1b needs backward computation.
I0801 13:11:31.797796 12832 net.cpp:323] conv1a/relu needs backward computation.
I0801 13:11:31.797799 12832 net.cpp:323] conv1a/bn needs backward computation.
I0801 13:11:31.797801 12832 net.cpp:323] conv1a needs backward computation.
I0801 13:11:31.797803 12832 net.cpp:325] data/bias does not need backward computation.
I0801 13:11:31.797807 12832 net.cpp:325] label_data_1_split does not need backward computation.
I0801 13:11:31.797809 12832 net.cpp:325] data does not need backward computation.
I0801 13:11:31.797812 12832 net.cpp:367] This network produces output accuracy/top1
I0801 13:11:31.797814 12832 net.cpp:367] This network produces output accuracy/top5
I0801 13:11:31.797816 12832 net.cpp:367] This network produces output loss
I0801 13:11:31.797850 12832 net.cpp:389] Top memory (TEST) required for data: 93585408 diff: 8
I0801 13:11:31.797853 12832 net.cpp:392] Bottom memory (TEST) required for data: 93585408 diff: 93585408
I0801 13:11:31.797855 12832 net.cpp:395] Shared (in-place) memory (TEST) by data: 62390272 diff: 62390272
I0801 13:11:31.797857 12832 net.cpp:398] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0801 13:11:31.797860 12832 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0801 13:11:31.797863 12832 net.cpp:407] Network initialization done.
I0801 13:11:31.797917 12832 solver.cpp:56] Solver scaffolding done.
I0801 13:11:31.802109 12832 parallel.cpp:108] [0 - 0] P2pSync adding callback
I0801 13:11:31.802119 12832 parallel.cpp:108] [1 - 1] P2pSync adding callback
I0801 13:11:31.802121 12832 parallel.cpp:108] [2 - 2] P2pSync adding callback
I0801 13:11:31.802124 12832 parallel.cpp:61] Starting Optimization
I0801 13:11:31.802125 12832 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:11:31.802146 12832 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:11:31.802261 12832 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:11:31.802956 12903 device_alternate.hpp:116] NVML initialized on thread 140416049637120
I0801 13:11:31.815770 12903 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0801 13:11:31.815811 12904 device_alternate.hpp:116] NVML initialized on thread 140416041244416
I0801 13:11:31.816948 12904 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0801 13:11:31.816990 12905 device_alternate.hpp:116] NVML initialized on thread 140416032851712
I0801 13:11:31.817873 12905 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0801 13:11:31.822919 12904 solver.cpp:42] Solver data type: FLOAT
W0801 13:11:31.823340 12904 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0801 13:11:31.823459 12904 net.cpp:104] Using FLOAT as default forward math type
I0801 13:11:31.823467 12904 net.cpp:110] Using FLOAT as default backward math type
I0801 13:11:31.823494 12904 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0801 13:11:31.823501 12904 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:11:31.829360 12905 solver.cpp:42] Solver data type: FLOAT
W0801 13:11:31.830039 12905 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0801 13:11:31.830066 12906 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0801 13:11:31.830154 12905 net.cpp:104] Using FLOAT as default forward math type
I0801 13:11:31.830162 12905 net.cpp:110] Using FLOAT as default backward math type
I0801 13:11:31.830202 12905 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0801 13:11:31.830216 12905 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:11:31.831046 12907 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0801 13:11:31.831145 12904 data_layer.cpp:184] [1] ReshapePrefetch 22, 3, 32, 32
I0801 13:11:31.832110 12905 data_layer.cpp:184] [2] ReshapePrefetch 22, 3, 32, 32
I0801 13:11:31.832128 12904 data_layer.cpp:208] [1] Output data size: 22, 3, 32, 32
I0801 13:11:31.832135 12904 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:11:31.832247 12905 data_layer.cpp:208] [2] Output data size: 22, 3, 32, 32
I0801 13:11:31.832257 12905 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:11:32.255859 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.25G, req 0G)
I0801 13:11:32.292644 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.25G, req 0G)
I0801 13:11:32.295126 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.23G, req 0G)
I0801 13:11:32.301831 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.23G, req 0G)
I0801 13:11:32.308153 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.21G, req 0G)
I0801 13:11:32.313410 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.21G, req 0G)
I0801 13:11:32.317062 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.19G, req 0G)
I0801 13:11:32.323158 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.19G, req 0G)
I0801 13:11:32.331441 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.18G, req 0.01G)
I0801 13:11:32.336865 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.18G, req 0.01G)
I0801 13:11:32.338165 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0.01G)
I0801 13:11:32.342612 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0.01G)
I0801 13:11:32.361412 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.15G, req 0.01G)
I0801 13:11:32.364269 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 3  (limit 8.15G, req 0.01G)
I0801 13:11:32.371371 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.14G, req 0.01G)
I0801 13:11:32.374678 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.14G, req 0.01G)
I0801 13:11:32.417062 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0.01G)
I0801 13:11:32.418705 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0.01G)
I0801 13:11:32.437849 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8.1G, req 0.01G)
I0801 13:11:32.439458 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8.1G, req 0.01G)
I0801 13:11:32.439640 12904 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/test.prototxt
W0801 13:11:32.439741 12904 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0801 13:11:32.439837 12904 net.cpp:104] Using FLOAT as default forward math type
I0801 13:11:32.439842 12904 net.cpp:110] Using FLOAT as default backward math type
I0801 13:11:32.439860 12904 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0801 13:11:32.439869 12904 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:11:32.441218 12905 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/test.prototxt
W0801 13:11:32.441288 12905 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0801 13:11:32.441395 12905 net.cpp:104] Using FLOAT as default forward math type
I0801 13:11:32.441401 12905 net.cpp:110] Using FLOAT as default backward math type
I0801 13:11:32.441416 12905 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0801 13:11:32.441423 12905 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:11:32.441540 12910 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0801 13:11:32.441623 12904 data_layer.cpp:184] (1) ReshapePrefetch 17, 3, 32, 32
I0801 13:11:32.441725 12904 data_layer.cpp:208] (1) Output data size: 17, 3, 32, 32
I0801 13:11:32.441730 12904 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:11:32.442180 12911 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0801 13:11:32.442291 12905 data_layer.cpp:184] (2) ReshapePrefetch 17, 3, 32, 32
I0801 13:11:32.442405 12905 data_layer.cpp:208] (2) Output data size: 17, 3, 32, 32
I0801 13:11:32.442410 12905 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:11:32.442888 12912 data_layer.cpp:97] (1) Parser threads: 1
I0801 13:11:32.442896 12912 data_layer.cpp:99] (1) Transformer threads: 1
I0801 13:11:32.443898 12913 data_layer.cpp:97] (2) Parser threads: 1
I0801 13:11:32.443907 12913 data_layer.cpp:99] (2) Transformer threads: 1
I0801 13:11:32.446748 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8.1G, req 0.01G)
I0801 13:11:32.447299 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8.1G, req 0.01G)
I0801 13:11:32.451643 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.09G, req 0.01G)
I0801 13:11:32.452172 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.09G, req 0.01G)
I0801 13:11:32.457224 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0.01G)
I0801 13:11:32.458299 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0.01G)
I0801 13:11:32.461889 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.08G, req 0.01G)
I0801 13:11:32.463157 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.08G, req 0.01G)
I0801 13:11:32.470088 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.07G, req 0.01G)
I0801 13:11:32.471611 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.07G, req 0.01G)
I0801 13:11:32.474647 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.06G, req 0.01G)
I0801 13:11:32.476845 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.06G, req 0.01G)
I0801 13:11:32.487634 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0.01G)
I0801 13:11:32.489132 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0.01G)
I0801 13:11:32.494931 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.05G, req 0.01G)
I0801 13:11:32.495694 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.05G, req 0.01G)
I0801 13:11:32.529136 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.03G, req 0.01G)
I0801 13:11:32.530444 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.03G, req 0.01G)
I0801 13:11:32.547308 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8.02G, req 0.01G)
I0801 13:11:32.548954 12904 solver.cpp:56] Solver scaffolding done.
I0801 13:11:32.548979 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8.02G, req 0.01G)
I0801 13:11:32.551641 12905 solver.cpp:56] Solver scaffolding done.
I0801 13:11:32.596570 12905 parallel.cpp:164] [2 - 2] P2pSync adding callback
I0801 13:11:32.596570 12904 parallel.cpp:164] [1 - 1] P2pSync adding callback
I0801 13:11:32.596570 12903 parallel.cpp:164] [0 - 0] P2pSync adding callback
I0801 13:11:32.812036 12903 solver.cpp:479] Solving jacintonet11v2_train
I0801 13:11:32.812055 12903 solver.cpp:480] Learning Rate Policy: poly
I0801 13:11:32.812062 12905 solver.cpp:479] Solving jacintonet11v2_train
I0801 13:11:32.812070 12905 solver.cpp:480] Learning Rate Policy: poly
I0801 13:11:32.812093 12904 solver.cpp:479] Solving jacintonet11v2_train
I0801 13:11:32.812103 12904 solver.cpp:480] Learning Rate Policy: poly
I0801 13:11:32.818377 12904 solver.cpp:268] Starting Optimization on GPU 1
I0801 13:11:32.818380 12905 solver.cpp:268] Starting Optimization on GPU 2
I0801 13:11:32.818384 12903 solver.cpp:268] Starting Optimization on GPU 0
I0801 13:11:32.818671 12903 solver.cpp:550] Iteration 0, Testing net (#0)
I0801 13:11:32.818687 12931 device_alternate.hpp:116] NVML initialized on thread 140415277430528
I0801 13:11:32.818707 12931 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0801 13:11:32.818719 12930 device_alternate.hpp:116] NVML initialized on thread 140415269037824
I0801 13:11:32.818735 12930 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0801 13:11:32.818750 12932 device_alternate.hpp:116] NVML initialized on thread 140415260645120
I0801 13:11:32.818761 12932 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0801 13:11:32.830557 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 0  (limit 7.99G, req 0.01G)
I0801 13:11:32.830838 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.99G, req 0.01G)
I0801 13:11:32.832025 12903 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 7.92G, req 0G)
I0801 13:11:32.836846 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.98G, req 0.01G)
I0801 13:11:32.837093 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.98G, req 0.01G)
I0801 13:11:32.839046 12903 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.9G, req 0G)
I0801 13:11:32.845527 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.97G, req 0.01G)
I0801 13:11:32.846027 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.97G, req 0.01G)
I0801 13:11:32.847280 12903 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.89G, req 0G)
I0801 13:11:32.852174 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.96G, req 0.01G)
I0801 13:11:32.853627 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.96G, req 0.01G)
I0801 13:11:32.854104 12903 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0G)
I0801 13:11:32.859119 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.94G, req 0.01G)
I0801 13:11:32.860810 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.94G, req 0.01G)
I0801 13:11:32.861845 12903 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.86G, req 0G)
I0801 13:11:32.864555 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.94G, req 0.01G)
I0801 13:11:32.867116 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.94G, req 0.01G)
I0801 13:11:32.868065 12903 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.85G, req 0G)
I0801 13:11:32.873355 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.92G, req 0.01G)
I0801 13:11:32.876505 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.92G, req 0.01G)
I0801 13:11:32.877053 12903 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.84G, req 0G)
I0801 13:11:32.878301 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.91G, req 0.01G)
I0801 13:11:32.883409 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.91G, req 0.01G)
I0801 13:11:32.883749 12903 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.83G, req 0G)
I0801 13:11:32.887611 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.9G, req 0.01G)
I0801 13:11:32.892279 12904 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.89G, req 0.01G)
I0801 13:11:32.893220 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.9G, req 0.01G)
I0801 13:11:32.893733 12903 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.81G, req 0G)
I0801 13:11:32.898883 12903 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.8G, req 0G)
I0801 13:11:32.899408 12905 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.89G, req 0.01G)
I0801 13:11:32.902508 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.176471
I0801 13:11:32.902521 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.470588
I0801 13:11:32.902529 12903 solver.cpp:635]     Test net output #2: loss = 71.9242 (* 1 = 71.9242 loss)
I0801 13:11:32.902536 12903 solver.cpp:295] [MultiGPU] Initial Test completed
I0801 13:11:32.902551 12905 blocking_queue.cpp:40] Data layer prefetch queue empty
I0801 13:11:32.911949 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.88G, req 0.01G)
I0801 13:11:32.912586 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.88G, req 0.01G)
I0801 13:11:32.913465 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.8G, req 0G)
I0801 13:11:32.921797 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.87G, req 0.01G)
I0801 13:11:32.922413 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 1 3  (limit 7.79G, req 0G)
I0801 13:11:32.922798 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.87G, req 0.01G)
I0801 13:11:32.932942 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.86G, req 0.01G)
I0801 13:11:32.934037 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.86G, req 0.01G)
I0801 13:11:32.934751 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.77G, req 0G)
I0801 13:11:32.941846 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.85G, req 0.01G)
I0801 13:11:32.943269 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.85G, req 0.01G)
I0801 13:11:32.943527 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.76G, req 0G)
I0801 13:11:32.953407 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.83G, req 0.01G)
I0801 13:11:32.954684 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.83G, req 0.01G)
I0801 13:11:32.955873 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.75G, req 0.01G)
I0801 13:11:32.960441 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.82G, req 0.01G)
I0801 13:11:32.961796 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.82G, req 0.01G)
I0801 13:11:32.962955 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.74G, req 0.01G)
I0801 13:11:32.977159 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.81G, req 0.01G)
I0801 13:11:32.977588 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 3  (limit 7.81G, req 0.01G)
I0801 13:11:32.979619 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 3  (limit 7.72G, req 0.01G)
I0801 13:11:32.986232 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.8G, req 0.01G)
I0801 13:11:32.986631 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.8G, req 0.01G)
I0801 13:11:32.987221 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.71G, req 0.01G)
I0801 13:11:33.006700 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 3  (limit 7.78G, req 0.01G)
I0801 13:11:33.007408 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.78G, req 0.01G)
I0801 13:11:33.008667 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.69G, req 0.01G)
I0801 13:11:33.015650 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.77G, req 0.01G)
I0801 13:11:33.017277 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.77G, req 0.01G)
I0801 13:11:33.017868 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.68G, req 0.01G)
I0801 13:11:33.051841 12908 data_layer.cpp:97] [1] Parser threads: 1
I0801 13:11:33.051856 12908 data_layer.cpp:99] [1] Transformer threads: 1
I0801 13:11:33.051954 12871 data_layer.cpp:97] [0] Parser threads: 1
I0801 13:11:33.051966 12871 data_layer.cpp:99] [0] Transformer threads: 1
I0801 13:11:33.053287 12909 data_layer.cpp:97] [2] Parser threads: 1
I0801 13:11:33.053298 12909 data_layer.cpp:99] [2] Transformer threads: 1
I0801 13:11:33.054494 12903 solver.cpp:358] Iteration 0 (0.151878 s), loss = 2.32328
I0801 13:11:33.054513 12903 solver.cpp:375]     Train net output #0: loss = 2.32328 (* 1 = 2.32328 loss)
I0801 13:11:33.054519 12903 sgd_solver.cpp:136] Iteration 0, lr = 0.1, m = 0.9
I0801 13:11:33.081914 12903 solver.cpp:358] Iteration 1 (0.0274135 s), loss = 2.19252
I0801 13:11:33.081960 12903 solver.cpp:375]     Train net output #0: loss = 2.19252 (* 1 = 2.19252 loss)
I0801 13:11:33.093075 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 7.07G, req 0.01G)
I0801 13:11:33.093354 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 6.98G, req 0.01G)
I0801 13:11:33.093552 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 7.07G, req 0.01G)
I0801 13:11:33.105315 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.43G, req 0.01G)
I0801 13:11:33.108330 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.43G, req 0.01G)
I0801 13:11:33.108530 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.34G, req 0.01G)
I0801 13:11:33.121667 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.43G, req 0.01G)
I0801 13:11:33.124449 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.43G, req 0.01G)
I0801 13:11:33.124646 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.34G, req 0.01G)
I0801 13:11:33.128487 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.01G)
I0801 13:11:33.132149 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.01G)
I0801 13:11:33.132315 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.34G, req 0.01G)
I0801 13:11:33.138290 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.43G, req 0.01G)
I0801 13:11:33.142653 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.43G, req 0.01G)
I0801 13:11:33.143148 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.34G, req 0.01G)
I0801 13:11:33.143623 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.01G)
I0801 13:11:33.148010 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 0  (limit 6.43G, req 0.01G)
I0801 13:11:33.148514 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 0  (limit 6.34G, req 0.01G)
I0801 13:11:33.168022 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.43G, req 0.02G)
I0801 13:11:33.173910 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.43G, req 0.02G)
I0801 13:11:33.174095 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.34G, req 0.02G)
I0801 13:11:33.176501 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.02G)
I0801 13:11:33.182030 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.02G)
I0801 13:11:33.182242 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.34G, req 0.02G)
I0801 13:11:33.212615 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.43G, req 0.03G)
I0801 13:11:33.219210 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.43G, req 0.03G)
I0801 13:11:33.219375 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.34G, req 0.03G)
I0801 13:11:33.222070 12904 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.43G, req 0.03G)
I0801 13:11:33.228199 12905 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.43G, req 0.03G)
I0801 13:11:33.228502 12903 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.34G, req 0.03G)
I0801 13:11:33.238867 12903 solver.cpp:358] Iteration 2 (0.156942 s), loss = 2.16932
I0801 13:11:33.238889 12903 solver.cpp:375]     Train net output #0: loss = 2.16932 (* 1 = 2.16932 loss)
I0801 13:11:33.238900 12905 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0801 13:11:33.238903 12904 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0801 13:11:33.238922 12903 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0801 13:11:34.723676 12903 solver.cpp:353] Iteration 100 (66.004 iter/s, 1.48476s/98 iter), loss = 2.16189
I0801 13:11:34.723701 12903 solver.cpp:375]     Train net output #0: loss = 2.16189 (* 1 = 2.16189 loss)
I0801 13:11:34.723706 12903 sgd_solver.cpp:136] Iteration 100, lr = 0.0998438, m = 0.9
I0801 13:11:36.243721 12903 solver.cpp:353] Iteration 200 (65.7897 iter/s, 1.51999s/100 iter), loss = 1.422
I0801 13:11:36.243769 12903 solver.cpp:375]     Train net output #0: loss = 1.422 (* 1 = 1.422 loss)
I0801 13:11:36.243782 12903 sgd_solver.cpp:136] Iteration 200, lr = 0.0996875, m = 0.9
I0801 13:11:37.761785 12903 solver.cpp:353] Iteration 300 (65.8755 iter/s, 1.51801s/100 iter), loss = 1.27943
I0801 13:11:37.761847 12903 solver.cpp:375]     Train net output #0: loss = 1.27943 (* 1 = 1.27943 loss)
I0801 13:11:37.761865 12903 sgd_solver.cpp:136] Iteration 300, lr = 0.0995313, m = 0.9
I0801 13:11:39.284245 12903 solver.cpp:353] Iteration 400 (65.6853 iter/s, 1.52241s/100 iter), loss = 1.08396
I0801 13:11:39.284272 12903 solver.cpp:375]     Train net output #0: loss = 1.08396 (* 1 = 1.08396 loss)
I0801 13:11:39.284278 12903 sgd_solver.cpp:136] Iteration 400, lr = 0.099375, m = 0.9
I0801 13:11:40.824879 12903 solver.cpp:353] Iteration 500 (64.9104 iter/s, 1.54058s/100 iter), loss = 1.62044
I0801 13:11:40.824906 12903 solver.cpp:375]     Train net output #0: loss = 1.62044 (* 1 = 1.62044 loss)
I0801 13:11:40.824913 12903 sgd_solver.cpp:136] Iteration 500, lr = 0.0992187, m = 0.9
I0801 13:11:42.348969 12903 solver.cpp:353] Iteration 600 (65.6151 iter/s, 1.52404s/100 iter), loss = 1.05527
I0801 13:11:42.349118 12903 solver.cpp:375]     Train net output #0: loss = 1.05527 (* 1 = 1.05527 loss)
I0801 13:11:42.349138 12903 sgd_solver.cpp:136] Iteration 600, lr = 0.0990625, m = 0.9
I0801 13:11:43.881842 12903 solver.cpp:353] Iteration 700 (65.239 iter/s, 1.53282s/100 iter), loss = 0.796999
I0801 13:11:43.881867 12903 solver.cpp:375]     Train net output #0: loss = 0.796999 (* 1 = 0.796999 loss)
I0801 13:11:43.881892 12903 sgd_solver.cpp:136] Iteration 700, lr = 0.0989062, m = 0.9
I0801 13:11:44.708917 12870 data_reader.cpp:264] Starting prefetch of epoch 1
I0801 13:11:45.410946 12903 solver.cpp:353] Iteration 800 (65.3999 iter/s, 1.52905s/100 iter), loss = 0.583395
I0801 13:11:45.410974 12903 solver.cpp:375]     Train net output #0: loss = 0.583395 (* 1 = 0.583395 loss)
I0801 13:11:45.410980 12903 sgd_solver.cpp:136] Iteration 800, lr = 0.09875, m = 0.9
I0801 13:11:46.985213 12903 solver.cpp:353] Iteration 900 (63.5237 iter/s, 1.57422s/100 iter), loss = 0.516644
I0801 13:11:46.985236 12903 solver.cpp:375]     Train net output #0: loss = 0.516644 (* 1 = 0.516644 loss)
I0801 13:11:46.985241 12903 sgd_solver.cpp:136] Iteration 900, lr = 0.0985937, m = 0.9
I0801 13:11:48.487159 12903 solver.cpp:550] Iteration 1000, Testing net (#0)
I0801 13:11:49.271790 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.559412
I0801 13:11:49.271808 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.925883
I0801 13:11:49.271813 12903 solver.cpp:635]     Test net output #2: loss = 1.30952 (* 1 = 1.30952 loss)
I0801 13:11:49.271829 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.784647s
I0801 13:11:49.286805 12903 solver.cpp:353] Iteration 1000 (43.4495 iter/s, 2.30152s/100 iter), loss = 1.1014
I0801 13:11:49.286820 12903 solver.cpp:375]     Train net output #0: loss = 1.1014 (* 1 = 1.1014 loss)
I0801 13:11:49.286825 12903 sgd_solver.cpp:136] Iteration 1000, lr = 0.0984375, m = 0.9
I0801 13:11:50.789301 12903 solver.cpp:353] Iteration 1100 (66.5582 iter/s, 1.50244s/100 iter), loss = 0.806615
I0801 13:11:50.789350 12903 solver.cpp:375]     Train net output #0: loss = 0.806615 (* 1 = 0.806615 loss)
I0801 13:11:50.789362 12903 sgd_solver.cpp:136] Iteration 1100, lr = 0.0982813, m = 0.9
I0801 13:11:52.305724 12903 solver.cpp:353] Iteration 1200 (65.9468 iter/s, 1.51637s/100 iter), loss = 0.708315
I0801 13:11:52.305750 12903 solver.cpp:375]     Train net output #0: loss = 0.708315 (* 1 = 0.708315 loss)
I0801 13:11:52.305757 12903 sgd_solver.cpp:136] Iteration 1200, lr = 0.098125, m = 0.9
I0801 13:11:53.854111 12903 solver.cpp:353] Iteration 1300 (64.5854 iter/s, 1.54834s/100 iter), loss = 0.76829
I0801 13:11:53.854136 12903 solver.cpp:375]     Train net output #0: loss = 0.768291 (* 1 = 0.768291 loss)
I0801 13:11:53.854142 12903 sgd_solver.cpp:136] Iteration 1300, lr = 0.0979687, m = 0.9
I0801 13:11:55.372375 12903 solver.cpp:353] Iteration 1400 (65.8669 iter/s, 1.51821s/100 iter), loss = 0.699026
I0801 13:11:55.372444 12903 solver.cpp:375]     Train net output #0: loss = 0.699026 (* 1 = 0.699026 loss)
I0801 13:11:55.372464 12903 sgd_solver.cpp:136] Iteration 1400, lr = 0.0978125, m = 0.9
I0801 13:11:56.886628 12903 solver.cpp:353] Iteration 1500 (66.0413 iter/s, 1.5142s/100 iter), loss = 0.421276
I0801 13:11:56.886653 12903 solver.cpp:375]     Train net output #0: loss = 0.421277 (* 1 = 0.421277 loss)
I0801 13:11:56.886659 12903 sgd_solver.cpp:136] Iteration 1500, lr = 0.0976562, m = 0.9
I0801 13:11:58.446661 12903 solver.cpp:353] Iteration 1600 (64.1033 iter/s, 1.55998s/100 iter), loss = 0.719585
I0801 13:11:58.446684 12903 solver.cpp:375]     Train net output #0: loss = 0.719585 (* 1 = 0.719585 loss)
I0801 13:11:58.446689 12903 sgd_solver.cpp:136] Iteration 1600, lr = 0.0975, m = 0.9
I0801 13:11:59.973229 12903 solver.cpp:353] Iteration 1700 (65.5086 iter/s, 1.52652s/100 iter), loss = 0.59711
I0801 13:11:59.973253 12903 solver.cpp:375]     Train net output #0: loss = 0.597111 (* 1 = 0.597111 loss)
I0801 13:11:59.973258 12903 sgd_solver.cpp:136] Iteration 1700, lr = 0.0973438, m = 0.9
I0801 13:12:01.494354 12903 solver.cpp:353] Iteration 1800 (65.7429 iter/s, 1.52108s/100 iter), loss = 0.369106
I0801 13:12:01.494470 12903 solver.cpp:375]     Train net output #0: loss = 0.369106 (* 1 = 0.369106 loss)
I0801 13:12:01.494478 12903 sgd_solver.cpp:136] Iteration 1800, lr = 0.0971875, m = 0.9
I0801 13:12:03.027909 12903 solver.cpp:353] Iteration 1900 (65.2101 iter/s, 1.53351s/100 iter), loss = 0.806105
I0801 13:12:03.027962 12903 solver.cpp:375]     Train net output #0: loss = 0.806105 (* 1 = 0.806105 loss)
I0801 13:12:03.027977 12903 sgd_solver.cpp:136] Iteration 1900, lr = 0.0970313, m = 0.9
I0801 13:12:04.552696 12903 solver.cpp:550] Iteration 2000, Testing net (#0)
I0801 13:12:05.358249 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.643823
I0801 13:12:05.358268 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.961471
I0801 13:12:05.358275 12903 solver.cpp:635]     Test net output #2: loss = 1.07889 (* 1 = 1.07889 loss)
I0801 13:12:05.358294 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.805575s
I0801 13:12:05.373716 12903 solver.cpp:353] Iteration 2000 (42.6305 iter/s, 2.34574s/100 iter), loss = 0.831556
I0801 13:12:05.373745 12903 solver.cpp:375]     Train net output #0: loss = 0.831557 (* 1 = 0.831557 loss)
I0801 13:12:05.373757 12903 sgd_solver.cpp:136] Iteration 2000, lr = 0.096875, m = 0.9
I0801 13:12:06.911116 12903 solver.cpp:353] Iteration 2100 (65.0471 iter/s, 1.53735s/100 iter), loss = 0.675357
I0801 13:12:06.911140 12903 solver.cpp:375]     Train net output #0: loss = 0.675357 (* 1 = 0.675357 loss)
I0801 13:12:06.911146 12903 sgd_solver.cpp:136] Iteration 2100, lr = 0.0967188, m = 0.9
I0801 13:12:08.451097 12903 solver.cpp:353] Iteration 2200 (64.9379 iter/s, 1.53993s/100 iter), loss = 0.580449
I0801 13:12:08.451122 12903 solver.cpp:375]     Train net output #0: loss = 0.58045 (* 1 = 0.58045 loss)
I0801 13:12:08.451128 12903 sgd_solver.cpp:136] Iteration 2200, lr = 0.0965625, m = 0.9
I0801 13:12:09.973106 12903 solver.cpp:353] Iteration 2300 (65.7048 iter/s, 1.52196s/100 iter), loss = 0.312723
I0801 13:12:09.973129 12903 solver.cpp:375]     Train net output #0: loss = 0.312723 (* 1 = 0.312723 loss)
I0801 13:12:09.973134 12903 sgd_solver.cpp:136] Iteration 2300, lr = 0.0964063, m = 0.9
I0801 13:12:11.509886 12903 solver.cpp:353] Iteration 2400 (65.0733 iter/s, 1.53673s/100 iter), loss = 0.566582
I0801 13:12:11.509923 12903 solver.cpp:375]     Train net output #0: loss = 0.566583 (* 1 = 0.566583 loss)
I0801 13:12:11.509930 12903 sgd_solver.cpp:136] Iteration 2400, lr = 0.09625, m = 0.9
I0801 13:12:13.044085 12903 solver.cpp:353] Iteration 2500 (65.1826 iter/s, 1.53415s/100 iter), loss = 0.245808
I0801 13:12:13.044111 12903 solver.cpp:375]     Train net output #0: loss = 0.245808 (* 1 = 0.245808 loss)
I0801 13:12:13.044117 12903 sgd_solver.cpp:136] Iteration 2500, lr = 0.0960938, m = 0.9
I0801 13:12:14.576988 12903 solver.cpp:353] Iteration 2600 (65.2378 iter/s, 1.53285s/100 iter), loss = 0.364048
I0801 13:12:14.577013 12903 solver.cpp:375]     Train net output #0: loss = 0.364049 (* 1 = 0.364049 loss)
I0801 13:12:14.577018 12903 sgd_solver.cpp:136] Iteration 2600, lr = 0.0959375, m = 0.9
I0801 13:12:16.104897 12903 solver.cpp:353] Iteration 2700 (65.4511 iter/s, 1.52786s/100 iter), loss = 0.285343
I0801 13:12:16.104921 12903 solver.cpp:375]     Train net output #0: loss = 0.285343 (* 1 = 0.285343 loss)
I0801 13:12:16.104928 12903 sgd_solver.cpp:136] Iteration 2700, lr = 0.0957813, m = 0.9
I0801 13:12:17.650336 12903 solver.cpp:353] Iteration 2800 (64.7087 iter/s, 1.54539s/100 iter), loss = 0.456162
I0801 13:12:17.650362 12903 solver.cpp:375]     Train net output #0: loss = 0.456162 (* 1 = 0.456162 loss)
I0801 13:12:17.650367 12903 sgd_solver.cpp:136] Iteration 2800, lr = 0.095625, m = 0.9
I0801 13:12:19.178120 12903 solver.cpp:353] Iteration 2900 (65.4563 iter/s, 1.52774s/100 iter), loss = 0.212026
I0801 13:12:19.178148 12903 solver.cpp:375]     Train net output #0: loss = 0.212026 (* 1 = 0.212026 loss)
I0801 13:12:19.178155 12903 sgd_solver.cpp:136] Iteration 2900, lr = 0.0954688, m = 0.9
I0801 13:12:20.700646 12903 solver.cpp:550] Iteration 3000, Testing net (#0)
I0801 13:12:21.508605 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.672353
I0801 13:12:21.508637 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.976177
I0801 13:12:21.508646 12903 solver.cpp:635]     Test net output #2: loss = 0.973545 (* 1 = 0.973545 loss)
I0801 13:12:21.508666 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.807995s
I0801 13:12:21.523877 12903 solver.cpp:353] Iteration 3000 (42.6314 iter/s, 2.34569s/100 iter), loss = 0.529866
I0801 13:12:21.523892 12903 solver.cpp:375]     Train net output #0: loss = 0.529866 (* 1 = 0.529866 loss)
I0801 13:12:21.523898 12903 sgd_solver.cpp:136] Iteration 3000, lr = 0.0953125, m = 0.9
I0801 13:12:23.055491 12903 solver.cpp:353] Iteration 3100 (65.2929 iter/s, 1.53156s/100 iter), loss = 0.196981
I0801 13:12:23.055519 12903 solver.cpp:375]     Train net output #0: loss = 0.196981 (* 1 = 0.196981 loss)
I0801 13:12:23.055526 12903 sgd_solver.cpp:136] Iteration 3100, lr = 0.0951563, m = 0.9
I0801 13:12:24.608433 12903 solver.cpp:353] Iteration 3200 (64.3961 iter/s, 1.55289s/100 iter), loss = 0.650997
I0801 13:12:24.608485 12903 solver.cpp:375]     Train net output #0: loss = 0.650997 (* 1 = 0.650997 loss)
I0801 13:12:24.608501 12903 sgd_solver.cpp:136] Iteration 3200, lr = 0.095, m = 0.9
I0801 13:12:26.163813 12903 solver.cpp:353] Iteration 3300 (64.295 iter/s, 1.55533s/100 iter), loss = 0.101506
I0801 13:12:26.163859 12903 solver.cpp:375]     Train net output #0: loss = 0.101506 (* 1 = 0.101506 loss)
I0801 13:12:26.163867 12903 sgd_solver.cpp:136] Iteration 3300, lr = 0.0948438, m = 0.9
I0801 13:12:27.731096 12903 solver.cpp:353] Iteration 3400 (63.8069 iter/s, 1.56723s/100 iter), loss = 0.525725
I0801 13:12:27.731154 12903 solver.cpp:375]     Train net output #0: loss = 0.525725 (* 1 = 0.525725 loss)
I0801 13:12:27.731169 12903 sgd_solver.cpp:136] Iteration 3400, lr = 0.0946875, m = 0.9
I0801 13:12:29.293987 12903 solver.cpp:353] Iteration 3500 (63.9862 iter/s, 1.56284s/100 iter), loss = 0.531755
I0801 13:12:29.294013 12903 solver.cpp:375]     Train net output #0: loss = 0.531756 (* 1 = 0.531756 loss)
I0801 13:12:29.294018 12903 sgd_solver.cpp:136] Iteration 3500, lr = 0.0945313, m = 0.9
I0801 13:12:30.882663 12903 solver.cpp:353] Iteration 3600 (62.9475 iter/s, 1.58863s/100 iter), loss = 0.241733
I0801 13:12:30.882711 12903 solver.cpp:375]     Train net output #0: loss = 0.241733 (* 1 = 0.241733 loss)
I0801 13:12:30.882725 12903 sgd_solver.cpp:136] Iteration 3600, lr = 0.094375, m = 0.9
I0801 13:12:32.468650 12903 solver.cpp:353] Iteration 3700 (63.0543 iter/s, 1.58594s/100 iter), loss = 0.214614
I0801 13:12:32.468736 12903 solver.cpp:375]     Train net output #0: loss = 0.214614 (* 1 = 0.214614 loss)
I0801 13:12:32.468744 12903 sgd_solver.cpp:136] Iteration 3700, lr = 0.0942188, m = 0.9
I0801 13:12:34.065635 12903 solver.cpp:353] Iteration 3800 (62.6199 iter/s, 1.59694s/100 iter), loss = 0.831711
I0801 13:12:34.065659 12903 solver.cpp:375]     Train net output #0: loss = 0.831711 (* 1 = 0.831711 loss)
I0801 13:12:34.065665 12903 sgd_solver.cpp:136] Iteration 3800, lr = 0.0940625, m = 0.9
I0801 13:12:35.622169 12903 solver.cpp:353] Iteration 3900 (64.2474 iter/s, 1.55648s/100 iter), loss = 0.799882
I0801 13:12:35.622193 12903 solver.cpp:375]     Train net output #0: loss = 0.799882 (* 1 = 0.799882 loss)
I0801 13:12:35.622200 12903 sgd_solver.cpp:136] Iteration 3900, lr = 0.0939062, m = 0.9
I0801 13:12:37.187930 12903 solver.cpp:550] Iteration 4000, Testing net (#0)
I0801 13:12:38.021989 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.716471
I0801 13:12:38.022012 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.984412
I0801 13:12:38.022017 12903 solver.cpp:635]     Test net output #2: loss = 0.848979 (* 1 = 0.848979 loss)
I0801 13:12:38.022044 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.834091s
I0801 13:12:38.037724 12903 solver.cpp:353] Iteration 4000 (41.3996 iter/s, 2.41548s/100 iter), loss = 0.279819
I0801 13:12:38.037740 12903 solver.cpp:375]     Train net output #0: loss = 0.279819 (* 1 = 0.279819 loss)
I0801 13:12:38.037744 12903 sgd_solver.cpp:136] Iteration 4000, lr = 0.09375, m = 0.9
I0801 13:12:39.602623 12903 solver.cpp:353] Iteration 4100 (63.9041 iter/s, 1.56485s/100 iter), loss = 0.198207
I0801 13:12:39.602648 12903 solver.cpp:375]     Train net output #0: loss = 0.198207 (* 1 = 0.198207 loss)
I0801 13:12:39.602654 12903 sgd_solver.cpp:136] Iteration 4100, lr = 0.0935938, m = 0.9
I0801 13:12:41.196776 12903 solver.cpp:353] Iteration 4200 (62.7312 iter/s, 1.5941s/100 iter), loss = 0.582615
I0801 13:12:41.196799 12903 solver.cpp:375]     Train net output #0: loss = 0.582615 (* 1 = 0.582615 loss)
I0801 13:12:41.196804 12903 sgd_solver.cpp:136] Iteration 4200, lr = 0.0934375, m = 0.9
I0801 13:12:42.781155 12903 solver.cpp:353] Iteration 4300 (63.1183 iter/s, 1.58433s/100 iter), loss = 0.316222
I0801 13:12:42.781183 12903 solver.cpp:375]     Train net output #0: loss = 0.316222 (* 1 = 0.316222 loss)
I0801 13:12:42.781188 12903 sgd_solver.cpp:136] Iteration 4300, lr = 0.0932813, m = 0.9
I0801 13:12:44.379304 12903 solver.cpp:353] Iteration 4400 (62.5744 iter/s, 1.5981s/100 iter), loss = 0.478641
I0801 13:12:44.379351 12903 solver.cpp:375]     Train net output #0: loss = 0.478641 (* 1 = 0.478641 loss)
I0801 13:12:44.379359 12903 sgd_solver.cpp:136] Iteration 4400, lr = 0.093125, m = 0.9
I0801 13:12:45.973162 12903 solver.cpp:353] Iteration 4500 (62.7429 iter/s, 1.59381s/100 iter), loss = 0.267014
I0801 13:12:45.973214 12903 solver.cpp:375]     Train net output #0: loss = 0.267014 (* 1 = 0.267014 loss)
I0801 13:12:45.973229 12903 sgd_solver.cpp:136] Iteration 4500, lr = 0.0929688, m = 0.9
I0801 13:12:47.551133 12903 solver.cpp:353] Iteration 4600 (63.3746 iter/s, 1.57792s/100 iter), loss = 0.155118
I0801 13:12:47.551184 12903 solver.cpp:375]     Train net output #0: loss = 0.155118 (* 1 = 0.155118 loss)
I0801 13:12:47.551198 12903 sgd_solver.cpp:136] Iteration 4600, lr = 0.0928125, m = 0.9
I0801 13:12:49.117053 12903 solver.cpp:353] Iteration 4700 (63.8623 iter/s, 1.56587s/100 iter), loss = 0.653835
I0801 13:12:49.117105 12903 solver.cpp:375]     Train net output #0: loss = 0.653835 (* 1 = 0.653835 loss)
I0801 13:12:49.117120 12903 sgd_solver.cpp:136] Iteration 4700, lr = 0.0926562, m = 0.9
I0801 13:12:50.679749 12903 solver.cpp:353] Iteration 4800 (63.994 iter/s, 1.56265s/100 iter), loss = 0.494451
I0801 13:12:50.679775 12903 solver.cpp:375]     Train net output #0: loss = 0.494451 (* 1 = 0.494451 loss)
I0801 13:12:50.679781 12903 sgd_solver.cpp:136] Iteration 4800, lr = 0.0925, m = 0.9
I0801 13:12:52.249183 12903 solver.cpp:353] Iteration 4900 (63.7193 iter/s, 1.56938s/100 iter), loss = 0.157635
I0801 13:12:52.249224 12903 solver.cpp:375]     Train net output #0: loss = 0.157635 (* 1 = 0.157635 loss)
I0801 13:12:52.249230 12903 sgd_solver.cpp:136] Iteration 4900, lr = 0.0923437, m = 0.9
I0801 13:12:53.821192 12903 solver.cpp:550] Iteration 5000, Testing net (#0)
I0801 13:12:54.488847 12893 data_reader.cpp:264] Starting prefetch of epoch 1
I0801 13:12:54.634626 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.826766
I0801 13:12:54.634645 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.991471
I0801 13:12:54.634650 12903 solver.cpp:635]     Test net output #2: loss = 0.533382 (* 1 = 0.533382 loss)
I0801 13:12:54.634670 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.813453s
I0801 13:12:54.650584 12903 solver.cpp:353] Iteration 5000 (41.6436 iter/s, 2.40133s/100 iter), loss = 0.576708
I0801 13:12:54.650605 12903 solver.cpp:375]     Train net output #0: loss = 0.576708 (* 1 = 0.576708 loss)
I0801 13:12:54.650611 12903 sgd_solver.cpp:136] Iteration 5000, lr = 0.0921875, m = 0.9
I0801 13:12:56.219452 12903 solver.cpp:353] Iteration 5100 (63.7424 iter/s, 1.56881s/100 iter), loss = 0.188347
I0801 13:12:56.219478 12903 solver.cpp:375]     Train net output #0: loss = 0.188347 (* 1 = 0.188347 loss)
I0801 13:12:56.219485 12903 sgd_solver.cpp:136] Iteration 5100, lr = 0.0920313, m = 0.9
I0801 13:12:57.802814 12903 solver.cpp:353] Iteration 5200 (63.1587 iter/s, 1.58331s/100 iter), loss = 0.285026
I0801 13:12:57.802839 12903 solver.cpp:375]     Train net output #0: loss = 0.285026 (* 1 = 0.285026 loss)
I0801 13:12:57.802845 12903 sgd_solver.cpp:136] Iteration 5200, lr = 0.091875, m = 0.9
I0801 13:12:59.355649 12903 solver.cpp:353] Iteration 5300 (64.4004 iter/s, 1.55279s/100 iter), loss = 0.522394
I0801 13:12:59.355712 12903 solver.cpp:375]     Train net output #0: loss = 0.522394 (* 1 = 0.522394 loss)
I0801 13:12:59.355729 12903 sgd_solver.cpp:136] Iteration 5300, lr = 0.0917188, m = 0.9
I0801 13:13:00.932822 12903 solver.cpp:353] Iteration 5400 (63.4068 iter/s, 1.57712s/100 iter), loss = 0.489632
I0801 13:13:00.932848 12903 solver.cpp:375]     Train net output #0: loss = 0.489632 (* 1 = 0.489632 loss)
I0801 13:13:00.932854 12903 sgd_solver.cpp:136] Iteration 5400, lr = 0.0915625, m = 0.9
I0801 13:13:02.495333 12903 solver.cpp:353] Iteration 5500 (64.0016 iter/s, 1.56246s/100 iter), loss = 0.400948
I0801 13:13:02.495405 12903 solver.cpp:375]     Train net output #0: loss = 0.400948 (* 1 = 0.400948 loss)
I0801 13:13:02.495411 12903 sgd_solver.cpp:136] Iteration 5500, lr = 0.0914062, m = 0.9
I0801 13:13:04.066716 12903 solver.cpp:353] Iteration 5600 (63.6404 iter/s, 1.57133s/100 iter), loss = 0.176532
I0801 13:13:04.066741 12903 solver.cpp:375]     Train net output #0: loss = 0.176532 (* 1 = 0.176532 loss)
I0801 13:13:04.066747 12903 sgd_solver.cpp:136] Iteration 5600, lr = 0.09125, m = 0.9
I0801 13:13:05.640213 12903 solver.cpp:353] Iteration 5700 (63.5547 iter/s, 1.57345s/100 iter), loss = 0.092255
I0801 13:13:05.640266 12903 solver.cpp:375]     Train net output #0: loss = 0.0922551 (* 1 = 0.0922551 loss)
I0801 13:13:05.640280 12903 sgd_solver.cpp:136] Iteration 5700, lr = 0.0910937, m = 0.9
I0801 13:13:07.230319 12903 solver.cpp:353] Iteration 5800 (62.8908 iter/s, 1.59006s/100 iter), loss = 0.487442
I0801 13:13:07.230388 12903 solver.cpp:375]     Train net output #0: loss = 0.487442 (* 1 = 0.487442 loss)
I0801 13:13:07.230408 12903 sgd_solver.cpp:136] Iteration 5800, lr = 0.0909375, m = 0.9
I0801 13:13:08.817369 12903 solver.cpp:353] Iteration 5900 (63.012 iter/s, 1.587s/100 iter), loss = 0.38352
I0801 13:13:08.817394 12903 solver.cpp:375]     Train net output #0: loss = 0.38352 (* 1 = 0.38352 loss)
I0801 13:13:08.817399 12903 sgd_solver.cpp:136] Iteration 5900, lr = 0.0907812, m = 0.9
I0801 13:13:10.380487 12903 solver.cpp:550] Iteration 6000, Testing net (#0)
I0801 13:13:11.218156 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.733236
I0801 13:13:11.218176 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.977059
I0801 13:13:11.218181 12903 solver.cpp:635]     Test net output #2: loss = 0.923823 (* 1 = 0.923823 loss)
I0801 13:13:11.218197 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.837686s
I0801 13:13:11.234141 12903 solver.cpp:353] Iteration 6000 (41.3787 iter/s, 2.4167s/100 iter), loss = 0.21119
I0801 13:13:11.234158 12903 solver.cpp:375]     Train net output #0: loss = 0.21119 (* 1 = 0.21119 loss)
I0801 13:13:11.234163 12903 sgd_solver.cpp:136] Iteration 6000, lr = 0.090625, m = 0.9
I0801 13:13:12.813745 12903 solver.cpp:353] Iteration 6100 (63.3091 iter/s, 1.57955s/100 iter), loss = 0.308741
I0801 13:13:12.813771 12903 solver.cpp:375]     Train net output #0: loss = 0.308741 (* 1 = 0.308741 loss)
I0801 13:13:12.813777 12903 sgd_solver.cpp:136] Iteration 6100, lr = 0.0904688, m = 0.9
I0801 13:13:14.393987 12903 solver.cpp:353] Iteration 6200 (63.2835 iter/s, 1.58019s/100 iter), loss = 0.460715
I0801 13:13:14.394011 12903 solver.cpp:375]     Train net output #0: loss = 0.460715 (* 1 = 0.460715 loss)
I0801 13:13:14.394014 12903 sgd_solver.cpp:136] Iteration 6200, lr = 0.0903125, m = 0.9
I0801 13:13:15.959728 12903 solver.cpp:353] Iteration 6300 (63.8696 iter/s, 1.56569s/100 iter), loss = 0.478396
I0801 13:13:15.959754 12903 solver.cpp:375]     Train net output #0: loss = 0.478396 (* 1 = 0.478396 loss)
I0801 13:13:15.959760 12903 sgd_solver.cpp:136] Iteration 6300, lr = 0.0901562, m = 0.9
I0801 13:13:17.544641 12903 solver.cpp:353] Iteration 6400 (63.097 iter/s, 1.58486s/100 iter), loss = 0.414294
I0801 13:13:17.544669 12903 solver.cpp:375]     Train net output #0: loss = 0.414294 (* 1 = 0.414294 loss)
I0801 13:13:17.544677 12903 sgd_solver.cpp:136] Iteration 6400, lr = 0.09, m = 0.9
I0801 13:13:19.143203 12903 solver.cpp:353] Iteration 6500 (62.5582 iter/s, 1.59851s/100 iter), loss = 0.260976
I0801 13:13:19.143229 12903 solver.cpp:375]     Train net output #0: loss = 0.260976 (* 1 = 0.260976 loss)
I0801 13:13:19.143235 12903 sgd_solver.cpp:136] Iteration 6500, lr = 0.0898438, m = 0.9
I0801 13:13:20.712553 12903 solver.cpp:353] Iteration 6600 (63.7227 iter/s, 1.5693s/100 iter), loss = 0.255901
I0801 13:13:20.712577 12903 solver.cpp:375]     Train net output #0: loss = 0.255901 (* 1 = 0.255901 loss)
I0801 13:13:20.712581 12903 sgd_solver.cpp:136] Iteration 6600, lr = 0.0896875, m = 0.9
I0801 13:13:22.282143 12903 solver.cpp:353] Iteration 6700 (63.7129 iter/s, 1.56954s/100 iter), loss = 0.0730741
I0801 13:13:22.282194 12903 solver.cpp:375]     Train net output #0: loss = 0.0730744 (* 1 = 0.0730744 loss)
I0801 13:13:22.282202 12903 sgd_solver.cpp:136] Iteration 6700, lr = 0.0895313, m = 0.9
I0801 13:13:23.873383 12903 solver.cpp:353] Iteration 6800 (62.8462 iter/s, 1.59119s/100 iter), loss = 0.253255
I0801 13:13:23.873411 12903 solver.cpp:375]     Train net output #0: loss = 0.253256 (* 1 = 0.253256 loss)
I0801 13:13:23.873420 12903 sgd_solver.cpp:136] Iteration 6800, lr = 0.089375, m = 0.9
I0801 13:13:25.441468 12903 solver.cpp:353] Iteration 6900 (63.7741 iter/s, 1.56804s/100 iter), loss = 0.429764
I0801 13:13:25.441520 12903 solver.cpp:375]     Train net output #0: loss = 0.429765 (* 1 = 0.429765 loss)
I0801 13:13:25.441535 12903 sgd_solver.cpp:136] Iteration 6900, lr = 0.0892188, m = 0.9
I0801 13:13:27.006330 12903 solver.cpp:550] Iteration 7000, Testing net (#0)
I0801 13:13:27.828063 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.76853
I0801 13:13:27.828081 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.984412
I0801 13:13:27.828086 12903 solver.cpp:635]     Test net output #2: loss = 0.829131 (* 1 = 0.829131 loss)
I0801 13:13:27.828110 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.821756s
I0801 13:13:27.844494 12903 solver.cpp:353] Iteration 7000 (41.6155 iter/s, 2.40295s/100 iter), loss = 0.266784
I0801 13:13:27.844521 12903 solver.cpp:375]     Train net output #0: loss = 0.266785 (* 1 = 0.266785 loss)
I0801 13:13:27.844527 12903 sgd_solver.cpp:136] Iteration 7000, lr = 0.0890625, m = 0.9
I0801 13:13:29.412071 12903 solver.cpp:353] Iteration 7100 (63.7947 iter/s, 1.56753s/100 iter), loss = 0.0672804
I0801 13:13:29.412099 12903 solver.cpp:375]     Train net output #0: loss = 0.0672807 (* 1 = 0.0672807 loss)
I0801 13:13:29.412106 12903 sgd_solver.cpp:136] Iteration 7100, lr = 0.0889063, m = 0.9
I0801 13:13:31.010226 12903 solver.cpp:353] Iteration 7200 (62.5742 iter/s, 1.5981s/100 iter), loss = 0.732841
I0801 13:13:31.010251 12903 solver.cpp:375]     Train net output #0: loss = 0.732841 (* 1 = 0.732841 loss)
I0801 13:13:31.010255 12903 sgd_solver.cpp:136] Iteration 7200, lr = 0.08875, m = 0.9
I0801 13:13:32.597729 12903 solver.cpp:353] Iteration 7300 (62.994 iter/s, 1.58745s/100 iter), loss = 0.267667
I0801 13:13:32.597839 12903 solver.cpp:375]     Train net output #0: loss = 0.267668 (* 1 = 0.267668 loss)
I0801 13:13:32.597852 12903 sgd_solver.cpp:136] Iteration 7300, lr = 0.0885938, m = 0.9
I0801 13:13:34.188274 12903 solver.cpp:353] Iteration 7400 (62.8735 iter/s, 1.5905s/100 iter), loss = 0.299707
I0801 13:13:34.188298 12903 solver.cpp:375]     Train net output #0: loss = 0.299708 (* 1 = 0.299708 loss)
I0801 13:13:34.188302 12903 sgd_solver.cpp:136] Iteration 7400, lr = 0.0884375, m = 0.9
I0801 13:13:35.749122 12903 solver.cpp:353] Iteration 7500 (64.0699 iter/s, 1.5608s/100 iter), loss = 0.179877
I0801 13:13:35.749147 12903 solver.cpp:375]     Train net output #0: loss = 0.179878 (* 1 = 0.179878 loss)
I0801 13:13:35.749152 12903 sgd_solver.cpp:136] Iteration 7500, lr = 0.0882813, m = 0.9
I0801 13:13:37.327405 12903 solver.cpp:353] Iteration 7600 (63.3619 iter/s, 1.57823s/100 iter), loss = 0.41949
I0801 13:13:37.327433 12903 solver.cpp:375]     Train net output #0: loss = 0.41949 (* 1 = 0.41949 loss)
I0801 13:13:37.327440 12903 sgd_solver.cpp:136] Iteration 7600, lr = 0.088125, m = 0.9
I0801 13:13:38.903018 12903 solver.cpp:353] Iteration 7700 (63.4694 iter/s, 1.57556s/100 iter), loss = 0.0513038
I0801 13:13:38.903108 12903 solver.cpp:375]     Train net output #0: loss = 0.051304 (* 1 = 0.051304 loss)
I0801 13:13:38.903131 12903 sgd_solver.cpp:136] Iteration 7700, lr = 0.0879688, m = 0.9
I0801 13:13:40.471758 12903 solver.cpp:353] Iteration 7800 (63.7474 iter/s, 1.56869s/100 iter), loss = 0.584675
I0801 13:13:40.471807 12903 solver.cpp:375]     Train net output #0: loss = 0.584675 (* 1 = 0.584675 loss)
I0801 13:13:40.471820 12903 sgd_solver.cpp:136] Iteration 7800, lr = 0.0878125, m = 0.9
I0801 13:13:42.037654 12903 solver.cpp:353] Iteration 7900 (63.8633 iter/s, 1.56584s/100 iter), loss = 0.188815
I0801 13:13:42.037740 12903 solver.cpp:375]     Train net output #0: loss = 0.188815 (* 1 = 0.188815 loss)
I0801 13:13:42.037760 12903 sgd_solver.cpp:136] Iteration 7900, lr = 0.0876563, m = 0.9
I0801 13:13:43.609072 12903 solver.cpp:550] Iteration 8000, Testing net (#0)
I0801 13:13:44.425168 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.773825
I0801 13:13:44.425189 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.979706
I0801 13:13:44.425194 12903 solver.cpp:635]     Test net output #2: loss = 0.785101 (* 1 = 0.785101 loss)
I0801 13:13:44.425209 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.816113s
I0801 13:13:44.440757 12903 solver.cpp:353] Iteration 8000 (41.6141 iter/s, 2.40303s/100 iter), loss = 0.201074
I0801 13:13:44.440774 12903 solver.cpp:375]     Train net output #0: loss = 0.201074 (* 1 = 0.201074 loss)
I0801 13:13:44.440778 12903 sgd_solver.cpp:136] Iteration 8000, lr = 0.0875, m = 0.9
I0801 13:13:46.016144 12903 solver.cpp:353] Iteration 8100 (63.4786 iter/s, 1.57533s/100 iter), loss = 0.211855
I0801 13:13:46.016170 12903 solver.cpp:375]     Train net output #0: loss = 0.211855 (* 1 = 0.211855 loss)
I0801 13:13:46.016175 12903 sgd_solver.cpp:136] Iteration 8100, lr = 0.0873438, m = 0.9
I0801 13:13:47.592468 12903 solver.cpp:353] Iteration 8200 (63.4409 iter/s, 1.57627s/100 iter), loss = 0.175261
I0801 13:13:47.592497 12903 solver.cpp:375]     Train net output #0: loss = 0.175261 (* 1 = 0.175261 loss)
I0801 13:13:47.592504 12903 sgd_solver.cpp:136] Iteration 8200, lr = 0.0871875, m = 0.9
I0801 13:13:49.151078 12903 solver.cpp:353] Iteration 8300 (64.1617 iter/s, 1.55856s/100 iter), loss = 0.386741
I0801 13:13:49.151104 12903 solver.cpp:375]     Train net output #0: loss = 0.386742 (* 1 = 0.386742 loss)
I0801 13:13:49.151110 12903 sgd_solver.cpp:136] Iteration 8300, lr = 0.0870313, m = 0.9
I0801 13:13:50.721679 12903 solver.cpp:353] Iteration 8400 (63.672 iter/s, 1.57055s/100 iter), loss = 0.081487
I0801 13:13:50.721729 12903 solver.cpp:375]     Train net output #0: loss = 0.081487 (* 1 = 0.081487 loss)
I0801 13:13:50.721741 12903 sgd_solver.cpp:136] Iteration 8400, lr = 0.086875, m = 0.9
I0801 13:13:52.280230 12903 solver.cpp:353] Iteration 8500 (64.1642 iter/s, 1.5585s/100 iter), loss = 0.476171
I0801 13:13:52.280302 12903 solver.cpp:375]     Train net output #0: loss = 0.476171 (* 1 = 0.476171 loss)
I0801 13:13:52.280315 12903 sgd_solver.cpp:136] Iteration 8500, lr = 0.0867188, m = 0.9
I0801 13:13:53.859277 12903 solver.cpp:353] Iteration 8600 (63.3313 iter/s, 1.579s/100 iter), loss = 0.0597984
I0801 13:13:53.859302 12903 solver.cpp:375]     Train net output #0: loss = 0.0597985 (* 1 = 0.0597985 loss)
I0801 13:13:53.859308 12903 sgd_solver.cpp:136] Iteration 8600, lr = 0.0865625, m = 0.9
I0801 13:13:55.414697 12903 solver.cpp:353] Iteration 8700 (64.2934 iter/s, 1.55537s/100 iter), loss = 0.166269
I0801 13:13:55.414750 12903 solver.cpp:375]     Train net output #0: loss = 0.166269 (* 1 = 0.166269 loss)
I0801 13:13:55.414763 12903 sgd_solver.cpp:136] Iteration 8700, lr = 0.0864063, m = 0.9
I0801 13:13:56.980902 12903 solver.cpp:353] Iteration 8800 (63.8508 iter/s, 1.56615s/100 iter), loss = 0.281878
I0801 13:13:56.980931 12903 solver.cpp:375]     Train net output #0: loss = 0.281879 (* 1 = 0.281879 loss)
I0801 13:13:56.980938 12903 sgd_solver.cpp:136] Iteration 8800, lr = 0.08625, m = 0.9
I0801 13:13:58.572885 12903 solver.cpp:353] Iteration 8900 (62.8168 iter/s, 1.59193s/100 iter), loss = 0.216198
I0801 13:13:58.572912 12903 solver.cpp:375]     Train net output #0: loss = 0.216198 (* 1 = 0.216198 loss)
I0801 13:13:58.572918 12903 sgd_solver.cpp:136] Iteration 8900, lr = 0.0860937, m = 0.9
I0801 13:14:00.119240 12903 solver.cpp:550] Iteration 9000, Testing net (#0)
I0801 13:14:00.967067 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.757354
I0801 13:14:00.967087 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.981471
I0801 13:14:00.967092 12903 solver.cpp:635]     Test net output #2: loss = 0.84657 (* 1 = 0.84657 loss)
I0801 13:14:00.967108 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.847844s
I0801 13:14:00.985702 12903 solver.cpp:353] Iteration 9000 (41.4466 iter/s, 2.41275s/100 iter), loss = 0.129191
I0801 13:14:00.985720 12903 solver.cpp:375]     Train net output #0: loss = 0.129191 (* 1 = 0.129191 loss)
I0801 13:14:00.985724 12903 sgd_solver.cpp:136] Iteration 9000, lr = 0.0859375, m = 0.9
I0801 13:14:02.348058 12870 data_reader.cpp:264] Starting prefetch of epoch 2
I0801 13:14:02.549327 12903 solver.cpp:353] Iteration 9100 (63.9561 iter/s, 1.56357s/100 iter), loss = 0.324097
I0801 13:14:02.549355 12903 solver.cpp:375]     Train net output #0: loss = 0.324097 (* 1 = 0.324097 loss)
I0801 13:14:02.549360 12903 sgd_solver.cpp:136] Iteration 9100, lr = 0.0857813, m = 0.9
I0801 13:14:04.111938 12903 solver.cpp:353] Iteration 9200 (63.9975 iter/s, 1.56256s/100 iter), loss = 0.387215
I0801 13:14:04.112035 12903 solver.cpp:375]     Train net output #0: loss = 0.387215 (* 1 = 0.387215 loss)
I0801 13:14:04.112043 12903 sgd_solver.cpp:136] Iteration 9200, lr = 0.085625, m = 0.9
I0801 13:14:05.696894 12903 solver.cpp:353] Iteration 9300 (63.0953 iter/s, 1.5849s/100 iter), loss = 0.318413
I0801 13:14:05.696918 12903 solver.cpp:375]     Train net output #0: loss = 0.318413 (* 1 = 0.318413 loss)
I0801 13:14:05.696921 12903 sgd_solver.cpp:136] Iteration 9300, lr = 0.0854688, m = 0.9
I0801 13:14:07.270874 12903 solver.cpp:353] Iteration 9400 (63.5351 iter/s, 1.57393s/100 iter), loss = 0.33538
I0801 13:14:07.270900 12903 solver.cpp:375]     Train net output #0: loss = 0.335381 (* 1 = 0.335381 loss)
I0801 13:14:07.270905 12903 sgd_solver.cpp:136] Iteration 9400, lr = 0.0853125, m = 0.9
I0801 13:14:08.844954 12903 solver.cpp:353] Iteration 9500 (63.5314 iter/s, 1.57403s/100 iter), loss = 0.0883561
I0801 13:14:08.844980 12903 solver.cpp:375]     Train net output #0: loss = 0.0883565 (* 1 = 0.0883565 loss)
I0801 13:14:08.844987 12903 sgd_solver.cpp:136] Iteration 9500, lr = 0.0851563, m = 0.9
I0801 13:14:10.411011 12903 solver.cpp:353] Iteration 9600 (63.8567 iter/s, 1.56601s/100 iter), loss = 0.333449
I0801 13:14:10.411039 12903 solver.cpp:375]     Train net output #0: loss = 0.33345 (* 1 = 0.33345 loss)
I0801 13:14:10.411046 12903 sgd_solver.cpp:136] Iteration 9600, lr = 0.085, m = 0.9
I0801 13:14:11.975211 12903 solver.cpp:353] Iteration 9700 (63.9326 iter/s, 1.56415s/100 iter), loss = 0.155642
I0801 13:14:11.975235 12903 solver.cpp:375]     Train net output #0: loss = 0.155643 (* 1 = 0.155643 loss)
I0801 13:14:11.975240 12903 sgd_solver.cpp:136] Iteration 9700, lr = 0.0848437, m = 0.9
I0801 13:14:13.547852 12903 solver.cpp:353] Iteration 9800 (63.5893 iter/s, 1.57259s/100 iter), loss = 0.114806
I0801 13:14:13.547878 12903 solver.cpp:375]     Train net output #0: loss = 0.114806 (* 1 = 0.114806 loss)
I0801 13:14:13.547883 12903 sgd_solver.cpp:136] Iteration 9800, lr = 0.0846875, m = 0.9
I0801 13:14:15.117142 12903 solver.cpp:353] Iteration 9900 (63.7252 iter/s, 1.56924s/100 iter), loss = 0.215342
I0801 13:14:15.117172 12903 solver.cpp:375]     Train net output #0: loss = 0.215342 (* 1 = 0.215342 loss)
I0801 13:14:15.117177 12903 sgd_solver.cpp:136] Iteration 9900, lr = 0.0845312, m = 0.9
I0801 13:14:16.665267 12903 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/cifar10_jacintonet11v2_iter_10000.caffemodel
I0801 13:14:16.682497 12903 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/cifar10_jacintonet11v2_iter_10000.solverstate
I0801 13:14:16.686014 12903 solver.cpp:550] Iteration 10000, Testing net (#0)
I0801 13:14:17.490361 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.795001
I0801 13:14:17.490381 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.989706
I0801 13:14:17.490388 12903 solver.cpp:635]     Test net output #2: loss = 0.652113 (* 1 = 0.652113 loss)
I0801 13:14:17.490407 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.804367s
I0801 13:14:17.505964 12903 solver.cpp:353] Iteration 10000 (41.8629 iter/s, 2.38875s/100 iter), loss = 0.0968604
I0801 13:14:17.505982 12903 solver.cpp:375]     Train net output #0: loss = 0.0968611 (* 1 = 0.0968611 loss)
I0801 13:14:17.505987 12903 sgd_solver.cpp:136] Iteration 10000, lr = 0.084375, m = 0.9
I0801 13:14:19.076315 12903 solver.cpp:353] Iteration 10100 (63.6821 iter/s, 1.5703s/100 iter), loss = 0.247012
I0801 13:14:19.076341 12903 solver.cpp:375]     Train net output #0: loss = 0.247013 (* 1 = 0.247013 loss)
I0801 13:14:19.076347 12903 sgd_solver.cpp:136] Iteration 10100, lr = 0.0842188, m = 0.9
I0801 13:14:20.655696 12903 solver.cpp:353] Iteration 10200 (63.3179 iter/s, 1.57933s/100 iter), loss = 0.183296
I0801 13:14:20.655748 12903 solver.cpp:375]     Train net output #0: loss = 0.183297 (* 1 = 0.183297 loss)
I0801 13:14:20.655762 12903 sgd_solver.cpp:136] Iteration 10200, lr = 0.0840625, m = 0.9
I0801 13:14:22.256867 12903 solver.cpp:353] Iteration 10300 (62.4563 iter/s, 1.60112s/100 iter), loss = 0.22598
I0801 13:14:22.256894 12903 solver.cpp:375]     Train net output #0: loss = 0.225981 (* 1 = 0.225981 loss)
I0801 13:14:22.256901 12903 sgd_solver.cpp:136] Iteration 10300, lr = 0.0839063, m = 0.9
I0801 13:14:23.842947 12903 solver.cpp:353] Iteration 10400 (63.0506 iter/s, 1.58603s/100 iter), loss = 0.140279
I0801 13:14:23.842973 12903 solver.cpp:375]     Train net output #0: loss = 0.14028 (* 1 = 0.14028 loss)
I0801 13:14:23.842979 12903 sgd_solver.cpp:136] Iteration 10400, lr = 0.08375, m = 0.9
I0801 13:14:25.404311 12903 solver.cpp:353] Iteration 10500 (64.0486 iter/s, 1.56131s/100 iter), loss = 0.313812
I0801 13:14:25.404337 12903 solver.cpp:375]     Train net output #0: loss = 0.313812 (* 1 = 0.313812 loss)
I0801 13:14:25.404343 12903 sgd_solver.cpp:136] Iteration 10500, lr = 0.0835937, m = 0.9
I0801 13:14:26.987125 12903 solver.cpp:353] Iteration 10600 (63.1807 iter/s, 1.58276s/100 iter), loss = 0.27377
I0801 13:14:26.987150 12903 solver.cpp:375]     Train net output #0: loss = 0.273771 (* 1 = 0.273771 loss)
I0801 13:14:26.987154 12903 sgd_solver.cpp:136] Iteration 10600, lr = 0.0834375, m = 0.9
I0801 13:14:28.557603 12903 solver.cpp:353] Iteration 10700 (63.677 iter/s, 1.57043s/100 iter), loss = 0.184891
I0801 13:14:28.557631 12903 solver.cpp:375]     Train net output #0: loss = 0.184892 (* 1 = 0.184892 loss)
I0801 13:14:28.557636 12903 sgd_solver.cpp:136] Iteration 10700, lr = 0.0832812, m = 0.9
I0801 13:14:30.137354 12903 solver.cpp:353] Iteration 10800 (63.303 iter/s, 1.5797s/100 iter), loss = 0.201943
I0801 13:14:30.137504 12903 solver.cpp:375]     Train net output #0: loss = 0.201944 (* 1 = 0.201944 loss)
I0801 13:14:30.137522 12903 sgd_solver.cpp:136] Iteration 10800, lr = 0.083125, m = 0.9
I0801 13:14:31.701612 12903 solver.cpp:353] Iteration 10900 (63.9302 iter/s, 1.56421s/100 iter), loss = 0.157755
I0801 13:14:31.701664 12903 solver.cpp:375]     Train net output #0: loss = 0.157756 (* 1 = 0.157756 loss)
I0801 13:14:31.701678 12903 sgd_solver.cpp:136] Iteration 10900, lr = 0.0829687, m = 0.9
I0801 13:14:33.261041 12903 solver.cpp:550] Iteration 11000, Testing net (#0)
I0801 13:14:34.084204 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.666176
I0801 13:14:34.084224 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.953236
I0801 13:14:34.084228 12903 solver.cpp:635]     Test net output #2: loss = 1.38065 (* 1 = 1.38065 loss)
I0801 13:14:34.084245 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.82318s
I0801 13:14:34.099776 12903 solver.cpp:353] Iteration 11000 (41.6998 iter/s, 2.39809s/100 iter), loss = 0.0286317
I0801 13:14:34.099794 12903 solver.cpp:375]     Train net output #0: loss = 0.0286323 (* 1 = 0.0286323 loss)
I0801 13:14:34.099800 12903 sgd_solver.cpp:136] Iteration 11000, lr = 0.0828125, m = 0.9
I0801 13:14:35.670325 12903 solver.cpp:353] Iteration 11100 (63.6741 iter/s, 1.5705s/100 iter), loss = 0.114052
I0801 13:14:35.670405 12903 solver.cpp:375]     Train net output #0: loss = 0.114053 (* 1 = 0.114053 loss)
I0801 13:14:35.670413 12903 sgd_solver.cpp:136] Iteration 11100, lr = 0.0826563, m = 0.9
I0801 13:14:37.247364 12903 solver.cpp:353] Iteration 11200 (63.412 iter/s, 1.57699s/100 iter), loss = 0.0629293
I0801 13:14:37.247453 12903 solver.cpp:375]     Train net output #0: loss = 0.06293 (* 1 = 0.06293 loss)
I0801 13:14:37.247462 12903 sgd_solver.cpp:136] Iteration 11200, lr = 0.0825, m = 0.9
I0801 13:14:38.828712 12903 solver.cpp:353] Iteration 11300 (63.2392 iter/s, 1.5813s/100 iter), loss = 0.140887
I0801 13:14:38.828738 12903 solver.cpp:375]     Train net output #0: loss = 0.140887 (* 1 = 0.140887 loss)
I0801 13:14:38.828744 12903 sgd_solver.cpp:136] Iteration 11300, lr = 0.0823437, m = 0.9
I0801 13:14:40.411739 12903 solver.cpp:353] Iteration 11400 (63.1721 iter/s, 1.58298s/100 iter), loss = 0.18077
I0801 13:14:40.411768 12903 solver.cpp:375]     Train net output #0: loss = 0.180771 (* 1 = 0.180771 loss)
I0801 13:14:40.411775 12903 sgd_solver.cpp:136] Iteration 11400, lr = 0.0821875, m = 0.9
I0801 13:14:41.991463 12903 solver.cpp:353] Iteration 11500 (63.3042 iter/s, 1.57967s/100 iter), loss = 0.215835
I0801 13:14:41.991488 12903 solver.cpp:375]     Train net output #0: loss = 0.215835 (* 1 = 0.215835 loss)
I0801 13:14:41.991494 12903 sgd_solver.cpp:136] Iteration 11500, lr = 0.0820312, m = 0.9
I0801 13:14:43.566066 12903 solver.cpp:353] Iteration 11600 (63.5102 iter/s, 1.57455s/100 iter), loss = 0.39012
I0801 13:14:43.566093 12903 solver.cpp:375]     Train net output #0: loss = 0.39012 (* 1 = 0.39012 loss)
I0801 13:14:43.566099 12903 sgd_solver.cpp:136] Iteration 11600, lr = 0.081875, m = 0.9
I0801 13:14:45.122215 12903 solver.cpp:353] Iteration 11700 (64.2633 iter/s, 1.5561s/100 iter), loss = 0.0656421
I0801 13:14:45.122241 12903 solver.cpp:375]     Train net output #0: loss = 0.0656427 (* 1 = 0.0656427 loss)
I0801 13:14:45.122246 12903 sgd_solver.cpp:136] Iteration 11700, lr = 0.0817188, m = 0.9
I0801 13:14:46.689723 12903 solver.cpp:353] Iteration 11800 (63.7975 iter/s, 1.56746s/100 iter), loss = 0.0495864
I0801 13:14:46.689761 12903 solver.cpp:375]     Train net output #0: loss = 0.049587 (* 1 = 0.049587 loss)
I0801 13:14:46.689772 12903 sgd_solver.cpp:136] Iteration 11800, lr = 0.0815625, m = 0.9
I0801 13:14:48.275430 12903 solver.cpp:353] Iteration 11900 (63.0653 iter/s, 1.58566s/100 iter), loss = 0.307851
I0801 13:14:48.275456 12903 solver.cpp:375]     Train net output #0: loss = 0.307851 (* 1 = 0.307851 loss)
I0801 13:14:48.275463 12903 sgd_solver.cpp:136] Iteration 11900, lr = 0.0814063, m = 0.9
I0801 13:14:49.819017 12903 solver.cpp:550] Iteration 12000, Testing net (#0)
I0801 13:14:50.635424 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.786471
I0801 13:14:50.635444 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.987647
I0801 13:14:50.635452 12903 solver.cpp:635]     Test net output #2: loss = 0.7812 (* 1 = 0.7812 loss)
I0801 13:14:50.635469 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.816429s
I0801 13:14:50.651510 12903 solver.cpp:353] Iteration 12000 (42.0874 iter/s, 2.37601s/100 iter), loss = 0.031052
I0801 13:14:50.651528 12903 solver.cpp:375]     Train net output #0: loss = 0.0310526 (* 1 = 0.0310526 loss)
I0801 13:14:50.651535 12903 sgd_solver.cpp:136] Iteration 12000, lr = 0.08125, m = 0.9
I0801 13:14:52.239353 12903 solver.cpp:353] Iteration 12100 (62.9806 iter/s, 1.58779s/100 iter), loss = 0.0276272
I0801 13:14:52.239408 12903 solver.cpp:375]     Train net output #0: loss = 0.0276278 (* 1 = 0.0276278 loss)
I0801 13:14:52.239420 12903 sgd_solver.cpp:136] Iteration 12100, lr = 0.0810938, m = 0.9
I0801 13:14:53.814644 12903 solver.cpp:353] Iteration 12200 (63.4824 iter/s, 1.57524s/100 iter), loss = 0.0802128
I0801 13:14:53.814669 12903 solver.cpp:375]     Train net output #0: loss = 0.0802134 (* 1 = 0.0802134 loss)
I0801 13:14:53.814674 12903 sgd_solver.cpp:136] Iteration 12200, lr = 0.0809375, m = 0.9
I0801 13:14:55.385556 12903 solver.cpp:353] Iteration 12300 (63.6593 iter/s, 1.57086s/100 iter), loss = 0.0586162
I0801 13:14:55.385596 12903 solver.cpp:375]     Train net output #0: loss = 0.0586169 (* 1 = 0.0586169 loss)
I0801 13:14:55.385602 12903 sgd_solver.cpp:136] Iteration 12300, lr = 0.0807813, m = 0.9
I0801 13:14:56.991448 12903 solver.cpp:353] Iteration 12400 (62.2727 iter/s, 1.60584s/100 iter), loss = 0.109059
I0801 13:14:56.991472 12903 solver.cpp:375]     Train net output #0: loss = 0.10906 (* 1 = 0.10906 loss)
I0801 13:14:56.991478 12903 sgd_solver.cpp:136] Iteration 12400, lr = 0.080625, m = 0.9
I0801 13:14:58.561775 12903 solver.cpp:353] Iteration 12500 (63.6831 iter/s, 1.57028s/100 iter), loss = 0.210456
I0801 13:14:58.561826 12903 solver.cpp:375]     Train net output #0: loss = 0.210456 (* 1 = 0.210456 loss)
I0801 13:14:58.561841 12903 sgd_solver.cpp:136] Iteration 12500, lr = 0.0804688, m = 0.9
I0801 13:15:00.150984 12903 solver.cpp:353] Iteration 12600 (62.9266 iter/s, 1.58915s/100 iter), loss = 0.502962
I0801 13:15:00.151013 12903 solver.cpp:375]     Train net output #0: loss = 0.502963 (* 1 = 0.502963 loss)
I0801 13:15:00.151021 12903 sgd_solver.cpp:136] Iteration 12600, lr = 0.0803125, m = 0.9
I0801 13:15:01.724153 12903 solver.cpp:353] Iteration 12700 (63.5679 iter/s, 1.57312s/100 iter), loss = 0.212604
I0801 13:15:01.724242 12903 solver.cpp:375]     Train net output #0: loss = 0.212605 (* 1 = 0.212605 loss)
I0801 13:15:01.724264 12903 sgd_solver.cpp:136] Iteration 12700, lr = 0.0801563, m = 0.9
I0801 13:15:03.324414 12903 solver.cpp:353] Iteration 12800 (62.4918 iter/s, 1.60021s/100 iter), loss = 0.308461
I0801 13:15:03.324465 12903 solver.cpp:375]     Train net output #0: loss = 0.308462 (* 1 = 0.308462 loss)
I0801 13:15:03.324479 12903 sgd_solver.cpp:136] Iteration 12800, lr = 0.08, m = 0.9
I0801 13:15:04.910869 12903 solver.cpp:353] Iteration 12900 (63.0356 iter/s, 1.58641s/100 iter), loss = 0.0632347
I0801 13:15:04.910895 12903 solver.cpp:375]     Train net output #0: loss = 0.0632353 (* 1 = 0.0632353 loss)
I0801 13:15:04.910902 12903 sgd_solver.cpp:136] Iteration 12900, lr = 0.0798438, m = 0.9
I0801 13:15:06.463152 12903 solver.cpp:550] Iteration 13000, Testing net (#0)
I0801 13:15:07.283565 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.828825
I0801 13:15:07.283586 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.987353
I0801 13:15:07.283591 12903 solver.cpp:635]     Test net output #2: loss = 0.543 (* 1 = 0.543 loss)
I0801 13:15:07.283610 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.820436s
I0801 13:15:07.301355 12903 solver.cpp:353] Iteration 13000 (41.8338 iter/s, 2.39041s/100 iter), loss = 0.0616847
I0801 13:15:07.301373 12903 solver.cpp:375]     Train net output #0: loss = 0.0616853 (* 1 = 0.0616853 loss)
I0801 13:15:07.301378 12903 sgd_solver.cpp:136] Iteration 13000, lr = 0.0796875, m = 0.9
I0801 13:15:08.872885 12903 solver.cpp:353] Iteration 13100 (63.6345 iter/s, 1.57147s/100 iter), loss = 0.191889
I0801 13:15:08.872939 12903 solver.cpp:375]     Train net output #0: loss = 0.19189 (* 1 = 0.19189 loss)
I0801 13:15:08.872953 12903 sgd_solver.cpp:136] Iteration 13100, lr = 0.0795313, m = 0.9
I0801 13:15:10.476745 12903 solver.cpp:353] Iteration 13200 (62.3515 iter/s, 1.60381s/100 iter), loss = 0.26734
I0801 13:15:10.476775 12903 solver.cpp:375]     Train net output #0: loss = 0.26734 (* 1 = 0.26734 loss)
I0801 13:15:10.476783 12903 sgd_solver.cpp:136] Iteration 13200, lr = 0.079375, m = 0.9
I0801 13:15:12.040865 12903 solver.cpp:353] Iteration 13300 (63.9357 iter/s, 1.56407s/100 iter), loss = 0.050729
I0801 13:15:12.040891 12903 solver.cpp:375]     Train net output #0: loss = 0.0507296 (* 1 = 0.0507296 loss)
I0801 13:15:12.040897 12903 sgd_solver.cpp:136] Iteration 13300, lr = 0.0792188, m = 0.9
I0801 13:15:13.612516 12903 solver.cpp:353] Iteration 13400 (63.6294 iter/s, 1.5716s/100 iter), loss = 0.0947752
I0801 13:15:13.612542 12903 solver.cpp:375]     Train net output #0: loss = 0.0947758 (* 1 = 0.0947758 loss)
I0801 13:15:13.612548 12903 sgd_solver.cpp:136] Iteration 13400, lr = 0.0790625, m = 0.9
I0801 13:15:15.199750 12903 solver.cpp:353] Iteration 13500 (63.0047 iter/s, 1.58718s/100 iter), loss = 0.318299
I0801 13:15:15.199774 12903 solver.cpp:375]     Train net output #0: loss = 0.3183 (* 1 = 0.3183 loss)
I0801 13:15:15.199780 12903 sgd_solver.cpp:136] Iteration 13500, lr = 0.0789063, m = 0.9
I0801 13:15:16.778798 12903 solver.cpp:353] Iteration 13600 (63.3314 iter/s, 1.57899s/100 iter), loss = 0.0254486
I0801 13:15:16.778869 12903 solver.cpp:375]     Train net output #0: loss = 0.0254492 (* 1 = 0.0254492 loss)
I0801 13:15:16.778890 12903 sgd_solver.cpp:136] Iteration 13600, lr = 0.07875, m = 0.9
I0801 13:15:17.302585 12870 data_reader.cpp:264] Starting prefetch of epoch 3
I0801 13:15:18.363423 12903 solver.cpp:353] Iteration 13700 (63.1083 iter/s, 1.58458s/100 iter), loss = 0.0738615
I0801 13:15:18.363447 12903 solver.cpp:375]     Train net output #0: loss = 0.0738621 (* 1 = 0.0738621 loss)
I0801 13:15:18.363454 12903 sgd_solver.cpp:136] Iteration 13700, lr = 0.0785938, m = 0.9
I0801 13:15:19.934172 12903 solver.cpp:353] Iteration 13800 (63.666 iter/s, 1.5707s/100 iter), loss = 0.176015
I0801 13:15:19.934197 12903 solver.cpp:375]     Train net output #0: loss = 0.176016 (* 1 = 0.176016 loss)
I0801 13:15:19.934203 12903 sgd_solver.cpp:136] Iteration 13800, lr = 0.0784375, m = 0.9
I0801 13:15:21.513840 12903 solver.cpp:353] Iteration 13900 (63.3064 iter/s, 1.57962s/100 iter), loss = 0.0669967
I0801 13:15:21.513865 12903 solver.cpp:375]     Train net output #0: loss = 0.0669973 (* 1 = 0.0669973 loss)
I0801 13:15:21.513870 12903 sgd_solver.cpp:136] Iteration 13900, lr = 0.0782812, m = 0.9
I0801 13:15:23.086127 12903 solver.cpp:550] Iteration 14000, Testing net (#0)
I0801 13:15:23.936877 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.803825
I0801 13:15:23.936894 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.976765
I0801 13:15:23.936899 12903 solver.cpp:635]     Test net output #2: loss = 0.778601 (* 1 = 0.778601 loss)
I0801 13:15:23.936918 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.850765s
I0801 13:15:23.952594 12903 solver.cpp:353] Iteration 14000 (41.0058 iter/s, 2.43868s/100 iter), loss = 0.0218137
I0801 13:15:23.952623 12903 solver.cpp:375]     Train net output #0: loss = 0.0218143 (* 1 = 0.0218143 loss)
I0801 13:15:23.952627 12903 sgd_solver.cpp:136] Iteration 14000, lr = 0.078125, m = 0.9
I0801 13:15:25.531596 12903 solver.cpp:353] Iteration 14100 (63.3334 iter/s, 1.57895s/100 iter), loss = 0.179836
I0801 13:15:25.531625 12903 solver.cpp:375]     Train net output #0: loss = 0.179837 (* 1 = 0.179837 loss)
I0801 13:15:25.531632 12903 sgd_solver.cpp:136] Iteration 14100, lr = 0.0779688, m = 0.9
I0801 13:15:27.156195 12903 solver.cpp:353] Iteration 14200 (61.5555 iter/s, 1.62455s/100 iter), loss = 0.118573
I0801 13:15:27.156268 12903 solver.cpp:375]     Train net output #0: loss = 0.118574 (* 1 = 0.118574 loss)
I0801 13:15:27.156289 12903 sgd_solver.cpp:136] Iteration 14200, lr = 0.0778125, m = 0.9
I0801 13:15:28.731712 12903 solver.cpp:353] Iteration 14300 (63.4733 iter/s, 1.57547s/100 iter), loss = 0.10363
I0801 13:15:28.731739 12903 solver.cpp:375]     Train net output #0: loss = 0.103631 (* 1 = 0.103631 loss)
I0801 13:15:28.731745 12903 sgd_solver.cpp:136] Iteration 14300, lr = 0.0776563, m = 0.9
I0801 13:15:30.320605 12903 solver.cpp:353] Iteration 14400 (62.9389 iter/s, 1.58884s/100 iter), loss = 0.0805124
I0801 13:15:30.320633 12903 solver.cpp:375]     Train net output #0: loss = 0.080513 (* 1 = 0.080513 loss)
I0801 13:15:30.320639 12903 sgd_solver.cpp:136] Iteration 14400, lr = 0.0775, m = 0.9
I0801 13:15:31.881691 12903 solver.cpp:353] Iteration 14500 (64.06 iter/s, 1.56104s/100 iter), loss = 0.0920467
I0801 13:15:31.881716 12903 solver.cpp:375]     Train net output #0: loss = 0.0920474 (* 1 = 0.0920474 loss)
I0801 13:15:31.881721 12903 sgd_solver.cpp:136] Iteration 14500, lr = 0.0773438, m = 0.9
I0801 13:15:33.450800 12903 solver.cpp:353] Iteration 14600 (63.7324 iter/s, 1.56906s/100 iter), loss = 0.153982
I0801 13:15:33.450826 12903 solver.cpp:375]     Train net output #0: loss = 0.153983 (* 1 = 0.153983 loss)
I0801 13:15:33.450832 12903 sgd_solver.cpp:136] Iteration 14600, lr = 0.0771875, m = 0.9
I0801 13:15:35.018544 12903 solver.cpp:353] Iteration 14700 (63.788 iter/s, 1.56769s/100 iter), loss = 0.0648973
I0801 13:15:35.018647 12903 solver.cpp:375]     Train net output #0: loss = 0.0648978 (* 1 = 0.0648978 loss)
I0801 13:15:35.018663 12903 sgd_solver.cpp:136] Iteration 14700, lr = 0.0770312, m = 0.9
I0801 13:15:36.605067 12903 solver.cpp:353] Iteration 14800 (63.0329 iter/s, 1.58647s/100 iter), loss = 0.230067
I0801 13:15:36.605139 12903 solver.cpp:375]     Train net output #0: loss = 0.230068 (* 1 = 0.230068 loss)
I0801 13:15:36.605147 12903 sgd_solver.cpp:136] Iteration 14800, lr = 0.076875, m = 0.9
I0801 13:15:38.162236 12903 solver.cpp:353] Iteration 14900 (64.2212 iter/s, 1.55712s/100 iter), loss = 0.2231
I0801 13:15:38.162261 12903 solver.cpp:375]     Train net output #0: loss = 0.2231 (* 1 = 0.2231 loss)
I0801 13:15:38.162266 12903 sgd_solver.cpp:136] Iteration 14900, lr = 0.0767187, m = 0.9
I0801 13:15:39.729168 12903 solver.cpp:550] Iteration 15000, Testing net (#0)
I0801 13:15:40.544236 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.79353
I0801 13:15:40.544256 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.982941
I0801 13:15:40.544263 12903 solver.cpp:635]     Test net output #2: loss = 0.730555 (* 1 = 0.730555 loss)
I0801 13:15:40.544281 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.815089s
I0801 13:15:40.560045 12903 solver.cpp:353] Iteration 15000 (41.706 iter/s, 2.39774s/100 iter), loss = 0.147933
I0801 13:15:40.560065 12903 solver.cpp:375]     Train net output #0: loss = 0.147933 (* 1 = 0.147933 loss)
I0801 13:15:40.560071 12903 sgd_solver.cpp:136] Iteration 15000, lr = 0.0765625, m = 0.9
I0801 13:15:42.127401 12903 solver.cpp:353] Iteration 15100 (63.8038 iter/s, 1.5673s/100 iter), loss = 0.11041
I0801 13:15:42.127452 12903 solver.cpp:375]     Train net output #0: loss = 0.11041 (* 1 = 0.11041 loss)
I0801 13:15:42.127465 12903 sgd_solver.cpp:136] Iteration 15100, lr = 0.0764063, m = 0.9
I0801 13:15:43.701954 12903 solver.cpp:353] Iteration 15200 (63.5122 iter/s, 1.5745s/100 iter), loss = 0.0395188
I0801 13:15:43.702003 12903 solver.cpp:375]     Train net output #0: loss = 0.0395194 (* 1 = 0.0395194 loss)
I0801 13:15:43.702016 12903 sgd_solver.cpp:136] Iteration 15200, lr = 0.07625, m = 0.9
I0801 13:15:45.278079 12903 solver.cpp:353] Iteration 15300 (63.4487 iter/s, 1.57608s/100 iter), loss = 0.107459
I0801 13:15:45.278126 12903 solver.cpp:375]     Train net output #0: loss = 0.10746 (* 1 = 0.10746 loss)
I0801 13:15:45.278138 12903 sgd_solver.cpp:136] Iteration 15300, lr = 0.0760938, m = 0.9
I0801 13:15:46.834820 12903 solver.cpp:353] Iteration 15400 (64.239 iter/s, 1.55669s/100 iter), loss = 0.132986
I0801 13:15:46.834846 12903 solver.cpp:375]     Train net output #0: loss = 0.132987 (* 1 = 0.132987 loss)
I0801 13:15:46.834853 12903 sgd_solver.cpp:136] Iteration 15400, lr = 0.0759375, m = 0.9
I0801 13:15:48.405814 12903 solver.cpp:353] Iteration 15500 (63.6559 iter/s, 1.57095s/100 iter), loss = 0.0621492
I0801 13:15:48.405838 12903 solver.cpp:375]     Train net output #0: loss = 0.0621499 (* 1 = 0.0621499 loss)
I0801 13:15:48.405844 12903 sgd_solver.cpp:136] Iteration 15500, lr = 0.0757812, m = 0.9
I0801 13:15:49.988392 12903 solver.cpp:353] Iteration 15600 (63.19 iter/s, 1.58253s/100 iter), loss = 0.0472353
I0801 13:15:49.988417 12903 solver.cpp:375]     Train net output #0: loss = 0.047236 (* 1 = 0.047236 loss)
I0801 13:15:49.988421 12903 sgd_solver.cpp:136] Iteration 15600, lr = 0.075625, m = 0.9
I0801 13:15:51.547634 12903 solver.cpp:353] Iteration 15700 (64.1358 iter/s, 1.55919s/100 iter), loss = 0.153151
I0801 13:15:51.547660 12903 solver.cpp:375]     Train net output #0: loss = 0.153152 (* 1 = 0.153152 loss)
I0801 13:15:51.547667 12903 sgd_solver.cpp:136] Iteration 15700, lr = 0.0754687, m = 0.9
I0801 13:15:53.126308 12903 solver.cpp:353] Iteration 15800 (63.3463 iter/s, 1.57862s/100 iter), loss = 0.156411
I0801 13:15:53.126333 12903 solver.cpp:375]     Train net output #0: loss = 0.156412 (* 1 = 0.156412 loss)
I0801 13:15:53.126339 12903 sgd_solver.cpp:136] Iteration 15800, lr = 0.0753125, m = 0.9
I0801 13:15:54.695632 12903 solver.cpp:353] Iteration 15900 (63.7237 iter/s, 1.56927s/100 iter), loss = 0.295374
I0801 13:15:54.695659 12903 solver.cpp:375]     Train net output #0: loss = 0.295374 (* 1 = 0.295374 loss)
I0801 13:15:54.695667 12903 sgd_solver.cpp:136] Iteration 15900, lr = 0.0751562, m = 0.9
I0801 13:15:56.270628 12903 solver.cpp:550] Iteration 16000, Testing net (#0)
I0801 13:15:56.946890 12913 blocking_queue.cpp:40] Waiting for datum
I0801 13:15:57.088625 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.815001
I0801 13:15:57.088647 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.97853
I0801 13:15:57.088655 12903 solver.cpp:635]     Test net output #2: loss = 0.798367 (* 1 = 0.798367 loss)
I0801 13:15:57.088673 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.818021s
I0801 13:15:57.104346 12903 solver.cpp:353] Iteration 16000 (41.5172 iter/s, 2.40864s/100 iter), loss = 0.158883
I0801 13:15:57.104362 12903 solver.cpp:375]     Train net output #0: loss = 0.158883 (* 1 = 0.158883 loss)
I0801 13:15:57.104368 12903 sgd_solver.cpp:136] Iteration 16000, lr = 0.075, m = 0.9
I0801 13:15:58.674341 12903 solver.cpp:353] Iteration 16100 (63.6966 iter/s, 1.56994s/100 iter), loss = 0.0435842
I0801 13:15:58.674370 12903 solver.cpp:375]     Train net output #0: loss = 0.043585 (* 1 = 0.043585 loss)
I0801 13:15:58.674376 12903 sgd_solver.cpp:136] Iteration 16100, lr = 0.0748438, m = 0.9
I0801 13:16:00.233573 12903 solver.cpp:353] Iteration 16200 (64.1363 iter/s, 1.55918s/100 iter), loss = 0.203749
I0801 13:16:00.233599 12903 solver.cpp:375]     Train net output #0: loss = 0.203749 (* 1 = 0.203749 loss)
I0801 13:16:00.233605 12903 sgd_solver.cpp:136] Iteration 16200, lr = 0.0746875, m = 0.9
I0801 13:16:01.787672 12903 solver.cpp:353] Iteration 16300 (64.348 iter/s, 1.55405s/100 iter), loss = 0.0404613
I0801 13:16:01.787698 12903 solver.cpp:375]     Train net output #0: loss = 0.040462 (* 1 = 0.040462 loss)
I0801 13:16:01.787703 12903 sgd_solver.cpp:136] Iteration 16300, lr = 0.0745312, m = 0.9
I0801 13:16:03.363639 12903 solver.cpp:353] Iteration 16400 (63.4551 iter/s, 1.57592s/100 iter), loss = 0.068397
I0801 13:16:03.363668 12903 solver.cpp:375]     Train net output #0: loss = 0.0683977 (* 1 = 0.0683977 loss)
I0801 13:16:03.363677 12903 sgd_solver.cpp:136] Iteration 16400, lr = 0.074375, m = 0.9
I0801 13:16:04.927234 12903 solver.cpp:353] Iteration 16500 (63.9572 iter/s, 1.56355s/100 iter), loss = 0.0644417
I0801 13:16:04.927258 12903 solver.cpp:375]     Train net output #0: loss = 0.0644423 (* 1 = 0.0644423 loss)
I0801 13:16:04.927264 12903 sgd_solver.cpp:136] Iteration 16500, lr = 0.0742188, m = 0.9
I0801 13:16:06.483500 12903 solver.cpp:353] Iteration 16600 (64.2584 iter/s, 1.55622s/100 iter), loss = 0.0465417
I0801 13:16:06.483528 12903 solver.cpp:375]     Train net output #0: loss = 0.0465424 (* 1 = 0.0465424 loss)
I0801 13:16:06.483534 12903 sgd_solver.cpp:136] Iteration 16600, lr = 0.0740625, m = 0.9
I0801 13:16:08.043642 12903 solver.cpp:353] Iteration 16700 (64.0988 iter/s, 1.56009s/100 iter), loss = 0.0663972
I0801 13:16:08.043726 12903 solver.cpp:375]     Train net output #0: loss = 0.0663977 (* 1 = 0.0663977 loss)
I0801 13:16:08.043733 12903 sgd_solver.cpp:136] Iteration 16700, lr = 0.0739063, m = 0.9
I0801 13:16:09.614581 12903 solver.cpp:353] Iteration 16800 (63.6581 iter/s, 1.57089s/100 iter), loss = 0.00790468
I0801 13:16:09.614608 12903 solver.cpp:375]     Train net output #0: loss = 0.00790514 (* 1 = 0.00790514 loss)
I0801 13:16:09.614614 12903 sgd_solver.cpp:136] Iteration 16800, lr = 0.07375, m = 0.9
I0801 13:16:11.184695 12903 solver.cpp:353] Iteration 16900 (63.6917 iter/s, 1.57006s/100 iter), loss = 0.00988932
I0801 13:16:11.184743 12903 solver.cpp:375]     Train net output #0: loss = 0.00988976 (* 1 = 0.00988976 loss)
I0801 13:16:11.184756 12903 sgd_solver.cpp:136] Iteration 16900, lr = 0.0735938, m = 0.9
I0801 13:16:12.740751 12903 solver.cpp:550] Iteration 17000, Testing net (#0)
I0801 13:16:13.559648 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.776178
I0801 13:16:13.559669 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.976765
I0801 13:16:13.559675 12903 solver.cpp:635]     Test net output #2: loss = 0.986495 (* 1 = 0.986495 loss)
I0801 13:16:13.559695 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.81892s
I0801 13:16:13.575240 12903 solver.cpp:353] Iteration 17000 (41.8327 iter/s, 2.39048s/100 iter), loss = 0.429367
I0801 13:16:13.575256 12903 solver.cpp:375]     Train net output #0: loss = 0.429367 (* 1 = 0.429367 loss)
I0801 13:16:13.575263 12903 sgd_solver.cpp:136] Iteration 17000, lr = 0.0734375, m = 0.9
I0801 13:16:15.141163 12903 solver.cpp:353] Iteration 17100 (63.8623 iter/s, 1.56587s/100 iter), loss = 0.0869716
I0801 13:16:15.141216 12903 solver.cpp:375]     Train net output #0: loss = 0.086972 (* 1 = 0.086972 loss)
I0801 13:16:15.141232 12903 sgd_solver.cpp:136] Iteration 17100, lr = 0.0732813, m = 0.9
I0801 13:16:16.699821 12903 solver.cpp:353] Iteration 17200 (64.1599 iter/s, 1.55861s/100 iter), loss = 0.038889
I0801 13:16:16.699846 12903 solver.cpp:375]     Train net output #0: loss = 0.0388895 (* 1 = 0.0388895 loss)
I0801 13:16:16.699852 12903 sgd_solver.cpp:136] Iteration 17200, lr = 0.073125, m = 0.9
I0801 13:16:18.273562 12903 solver.cpp:353] Iteration 17300 (63.5448 iter/s, 1.57369s/100 iter), loss = 0.0269961
I0801 13:16:18.273588 12903 solver.cpp:375]     Train net output #0: loss = 0.0269965 (* 1 = 0.0269965 loss)
I0801 13:16:18.273594 12903 sgd_solver.cpp:136] Iteration 17300, lr = 0.0729688, m = 0.9
I0801 13:16:19.849478 12903 solver.cpp:353] Iteration 17400 (63.4572 iter/s, 1.57587s/100 iter), loss = 0.123826
I0801 13:16:19.849501 12903 solver.cpp:375]     Train net output #0: loss = 0.123826 (* 1 = 0.123826 loss)
I0801 13:16:19.849506 12903 sgd_solver.cpp:136] Iteration 17400, lr = 0.0728125, m = 0.9
I0801 13:16:21.411478 12903 solver.cpp:353] Iteration 17500 (64.0225 iter/s, 1.56195s/100 iter), loss = 0.190857
I0801 13:16:21.411500 12903 solver.cpp:375]     Train net output #0: loss = 0.190858 (* 1 = 0.190858 loss)
I0801 13:16:21.411505 12903 sgd_solver.cpp:136] Iteration 17500, lr = 0.0726563, m = 0.9
I0801 13:16:22.977277 12903 solver.cpp:353] Iteration 17600 (63.8673 iter/s, 1.56575s/100 iter), loss = 0.0515884
I0801 13:16:22.977336 12903 solver.cpp:375]     Train net output #0: loss = 0.0515889 (* 1 = 0.0515889 loss)
I0801 13:16:22.977352 12903 sgd_solver.cpp:136] Iteration 17600, lr = 0.0725, m = 0.9
I0801 13:16:24.538825 12903 solver.cpp:353] Iteration 17700 (64.041 iter/s, 1.5615s/100 iter), loss = 0.173723
I0801 13:16:24.538856 12903 solver.cpp:375]     Train net output #0: loss = 0.173724 (* 1 = 0.173724 loss)
I0801 13:16:24.538862 12903 sgd_solver.cpp:136] Iteration 17700, lr = 0.0723438, m = 0.9
I0801 13:16:26.109226 12903 solver.cpp:353] Iteration 17800 (63.6799 iter/s, 1.57035s/100 iter), loss = 0.0634107
I0801 13:16:26.109277 12903 solver.cpp:375]     Train net output #0: loss = 0.0634112 (* 1 = 0.0634112 loss)
I0801 13:16:26.109284 12903 sgd_solver.cpp:136] Iteration 17800, lr = 0.0721875, m = 0.9
I0801 13:16:27.663343 12903 solver.cpp:353] Iteration 17900 (64.3474 iter/s, 1.55406s/100 iter), loss = 0.342297
I0801 13:16:27.663393 12903 solver.cpp:375]     Train net output #0: loss = 0.342297 (* 1 = 0.342297 loss)
I0801 13:16:27.663400 12903 sgd_solver.cpp:136] Iteration 17900, lr = 0.0720313, m = 0.9
I0801 13:16:29.239886 12903 solver.cpp:550] Iteration 18000, Testing net (#0)
I0801 13:16:29.716502 12893 data_reader.cpp:264] Starting prefetch of epoch 2
I0801 13:16:30.064829 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.736177
I0801 13:16:30.064849 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.985294
I0801 13:16:30.064854 12903 solver.cpp:635]     Test net output #2: loss = 1.34523 (* 1 = 1.34523 loss)
I0801 13:16:30.064872 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.824963s
I0801 13:16:30.080520 12903 solver.cpp:353] Iteration 18000 (41.3718 iter/s, 2.41711s/100 iter), loss = 0.0494345
I0801 13:16:30.080551 12903 solver.cpp:375]     Train net output #0: loss = 0.049435 (* 1 = 0.049435 loss)
I0801 13:16:30.080564 12903 sgd_solver.cpp:136] Iteration 18000, lr = 0.071875, m = 0.9
I0801 13:16:31.652369 12903 solver.cpp:353] Iteration 18100 (63.6214 iter/s, 1.5718s/100 iter), loss = 0.00731387
I0801 13:16:31.652416 12903 solver.cpp:375]     Train net output #0: loss = 0.0073145 (* 1 = 0.0073145 loss)
I0801 13:16:31.652429 12903 sgd_solver.cpp:136] Iteration 18100, lr = 0.0717188, m = 0.9
I0801 13:16:33.210959 12903 solver.cpp:353] Iteration 18200 (64.1626 iter/s, 1.55854s/100 iter), loss = 0.0181449
I0801 13:16:33.210984 12903 solver.cpp:375]     Train net output #0: loss = 0.0181455 (* 1 = 0.0181455 loss)
I0801 13:16:33.210990 12903 sgd_solver.cpp:136] Iteration 18200, lr = 0.0715625, m = 0.9
I0801 13:16:34.775666 12903 solver.cpp:353] Iteration 18300 (63.9117 iter/s, 1.56466s/100 iter), loss = 0.0653655
I0801 13:16:34.775715 12903 solver.cpp:375]     Train net output #0: loss = 0.0653662 (* 1 = 0.0653662 loss)
I0801 13:16:34.775729 12903 sgd_solver.cpp:136] Iteration 18300, lr = 0.0714063, m = 0.9
I0801 13:16:36.358712 12903 solver.cpp:353] Iteration 18400 (63.1714 iter/s, 1.583s/100 iter), loss = 0.0643747
I0801 13:16:36.358741 12903 solver.cpp:375]     Train net output #0: loss = 0.0643754 (* 1 = 0.0643754 loss)
I0801 13:16:36.358745 12903 sgd_solver.cpp:136] Iteration 18400, lr = 0.07125, m = 0.9
I0801 13:16:37.941381 12903 solver.cpp:353] Iteration 18500 (63.1864 iter/s, 1.58262s/100 iter), loss = 0.084998
I0801 13:16:37.941417 12903 solver.cpp:375]     Train net output #0: loss = 0.0849987 (* 1 = 0.0849987 loss)
I0801 13:16:37.941427 12903 sgd_solver.cpp:136] Iteration 18500, lr = 0.0710938, m = 0.9
I0801 13:16:39.517455 12903 solver.cpp:353] Iteration 18600 (63.4509 iter/s, 1.57602s/100 iter), loss = 0.246818
I0801 13:16:39.517518 12903 solver.cpp:375]     Train net output #0: loss = 0.246818 (* 1 = 0.246818 loss)
I0801 13:16:39.517524 12903 sgd_solver.cpp:136] Iteration 18600, lr = 0.0709375, m = 0.9
I0801 13:16:41.098078 12903 solver.cpp:353] Iteration 18700 (63.2682 iter/s, 1.58057s/100 iter), loss = 0.179071
I0801 13:16:41.098103 12903 solver.cpp:375]     Train net output #0: loss = 0.179072 (* 1 = 0.179072 loss)
I0801 13:16:41.098106 12903 sgd_solver.cpp:136] Iteration 18700, lr = 0.0707813, m = 0.9
I0801 13:16:42.654813 12903 solver.cpp:353] Iteration 18800 (64.2392 iter/s, 1.55668s/100 iter), loss = 0.0873211
I0801 13:16:42.654839 12903 solver.cpp:375]     Train net output #0: loss = 0.0873217 (* 1 = 0.0873217 loss)
I0801 13:16:42.654844 12903 sgd_solver.cpp:136] Iteration 18800, lr = 0.070625, m = 0.9
I0801 13:16:44.235460 12903 solver.cpp:353] Iteration 18900 (63.2673 iter/s, 1.5806s/100 iter), loss = 0.0394266
I0801 13:16:44.235550 12903 solver.cpp:375]     Train net output #0: loss = 0.0394273 (* 1 = 0.0394273 loss)
I0801 13:16:44.235572 12903 sgd_solver.cpp:136] Iteration 18900, lr = 0.0704687, m = 0.9
I0801 13:16:45.793783 12903 solver.cpp:550] Iteration 19000, Testing net (#0)
I0801 13:16:46.610960 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.844119
I0801 13:16:46.610977 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.991765
I0801 13:16:46.610985 12903 solver.cpp:635]     Test net output #2: loss = 0.5505 (* 1 = 0.5505 loss)
I0801 13:16:46.611002 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.817196s
I0801 13:16:46.626462 12903 solver.cpp:353] Iteration 19000 (41.8246 iter/s, 2.39094s/100 iter), loss = 0.206606
I0801 13:16:46.626478 12903 solver.cpp:375]     Train net output #0: loss = 0.206607 (* 1 = 0.206607 loss)
I0801 13:16:46.626484 12903 sgd_solver.cpp:136] Iteration 19000, lr = 0.0703125, m = 0.9
I0801 13:16:48.202502 12903 solver.cpp:353] Iteration 19100 (63.4522 iter/s, 1.57599s/100 iter), loss = 0.133242
I0801 13:16:48.202527 12903 solver.cpp:375]     Train net output #0: loss = 0.133243 (* 1 = 0.133243 loss)
I0801 13:16:48.202533 12903 sgd_solver.cpp:136] Iteration 19100, lr = 0.0701563, m = 0.9
I0801 13:16:49.774096 12903 solver.cpp:353] Iteration 19200 (63.6317 iter/s, 1.57154s/100 iter), loss = 0.139233
I0801 13:16:49.774123 12903 solver.cpp:375]     Train net output #0: loss = 0.139234 (* 1 = 0.139234 loss)
I0801 13:16:49.774129 12903 sgd_solver.cpp:136] Iteration 19200, lr = 0.07, m = 0.9
I0801 13:16:51.329727 12903 solver.cpp:353] Iteration 19300 (64.2847 iter/s, 1.55558s/100 iter), loss = 0.0800915
I0801 13:16:51.329754 12903 solver.cpp:375]     Train net output #0: loss = 0.0800922 (* 1 = 0.0800922 loss)
I0801 13:16:51.329761 12903 sgd_solver.cpp:136] Iteration 19300, lr = 0.0698438, m = 0.9
I0801 13:16:52.899533 12903 solver.cpp:353] Iteration 19400 (63.7042 iter/s, 1.56976s/100 iter), loss = 0.0929699
I0801 13:16:52.899559 12903 solver.cpp:375]     Train net output #0: loss = 0.0929706 (* 1 = 0.0929706 loss)
I0801 13:16:52.899564 12903 sgd_solver.cpp:136] Iteration 19400, lr = 0.0696875, m = 0.9
I0801 13:16:54.468325 12903 solver.cpp:353] Iteration 19500 (63.7454 iter/s, 1.56874s/100 iter), loss = 0.145609
I0801 13:16:54.468349 12903 solver.cpp:375]     Train net output #0: loss = 0.14561 (* 1 = 0.14561 loss)
I0801 13:16:54.468355 12903 sgd_solver.cpp:136] Iteration 19500, lr = 0.0695313, m = 0.9
I0801 13:16:56.035140 12903 solver.cpp:353] Iteration 19600 (63.8257 iter/s, 1.56677s/100 iter), loss = 0.0838101
I0801 13:16:56.035171 12903 solver.cpp:375]     Train net output #0: loss = 0.0838108 (* 1 = 0.0838108 loss)
I0801 13:16:56.035177 12903 sgd_solver.cpp:136] Iteration 19600, lr = 0.069375, m = 0.9
I0801 13:16:57.604518 12903 solver.cpp:353] Iteration 19700 (63.7215 iter/s, 1.56933s/100 iter), loss = 0.0582358
I0801 13:16:57.604564 12903 solver.cpp:375]     Train net output #0: loss = 0.0582365 (* 1 = 0.0582365 loss)
I0801 13:16:57.604578 12903 sgd_solver.cpp:136] Iteration 19700, lr = 0.0692187, m = 0.9
I0801 13:16:59.183856 12903 solver.cpp:353] Iteration 19800 (63.3197 iter/s, 1.57929s/100 iter), loss = 0.206744
I0801 13:16:59.183895 12903 solver.cpp:375]     Train net output #0: loss = 0.206745 (* 1 = 0.206745 loss)
I0801 13:16:59.183902 12903 sgd_solver.cpp:136] Iteration 19800, lr = 0.0690625, m = 0.9
I0801 13:17:00.748004 12903 solver.cpp:353] Iteration 19900 (63.9346 iter/s, 1.5641s/100 iter), loss = 0.308058
I0801 13:17:00.748091 12903 solver.cpp:375]     Train net output #0: loss = 0.308059 (* 1 = 0.308059 loss)
I0801 13:17:00.748111 12903 sgd_solver.cpp:136] Iteration 19900, lr = 0.0689062, m = 0.9
I0801 13:17:02.299149 12903 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/cifar10_jacintonet11v2_iter_20000.caffemodel
I0801 13:17:02.307430 12903 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/cifar10_jacintonet11v2_iter_20000.solverstate
I0801 13:17:02.311074 12903 solver.cpp:550] Iteration 20000, Testing net (#0)
I0801 13:17:03.123563 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.801766
I0801 13:17:03.123584 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.989706
I0801 13:17:03.123589 12903 solver.cpp:635]     Test net output #2: loss = 0.735514 (* 1 = 0.735514 loss)
I0801 13:17:03.123610 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.812512s
I0801 13:17:03.141013 12903 solver.cpp:353] Iteration 20000 (41.7896 iter/s, 2.39294s/100 iter), loss = 0.0798239
I0801 13:17:03.141031 12903 solver.cpp:375]     Train net output #0: loss = 0.0798245 (* 1 = 0.0798245 loss)
I0801 13:17:03.141037 12903 sgd_solver.cpp:136] Iteration 20000, lr = 0.06875, m = 0.9
I0801 13:17:04.735405 12903 solver.cpp:353] Iteration 20100 (62.7218 iter/s, 1.59434s/100 iter), loss = 0.0351999
I0801 13:17:04.735430 12903 solver.cpp:375]     Train net output #0: loss = 0.0352005 (* 1 = 0.0352005 loss)
I0801 13:17:04.735436 12903 sgd_solver.cpp:136] Iteration 20100, lr = 0.0685938, m = 0.9
I0801 13:17:06.315487 12903 solver.cpp:353] Iteration 20200 (63.2898 iter/s, 1.58003s/100 iter), loss = 0.282975
I0801 13:17:06.315534 12903 solver.cpp:375]     Train net output #0: loss = 0.282975 (* 1 = 0.282975 loss)
I0801 13:17:06.315549 12903 sgd_solver.cpp:136] Iteration 20200, lr = 0.0684375, m = 0.9
I0801 13:17:07.881752 12903 solver.cpp:353] Iteration 20300 (63.8483 iter/s, 1.56621s/100 iter), loss = 0.0405654
I0801 13:17:07.881778 12903 solver.cpp:375]     Train net output #0: loss = 0.040566 (* 1 = 0.040566 loss)
I0801 13:17:07.881784 12903 sgd_solver.cpp:136] Iteration 20300, lr = 0.0682813, m = 0.9
I0801 13:17:09.455365 12903 solver.cpp:353] Iteration 20400 (63.5501 iter/s, 1.57356s/100 iter), loss = 0.249243
I0801 13:17:09.455433 12903 solver.cpp:375]     Train net output #0: loss = 0.249243 (* 1 = 0.249243 loss)
I0801 13:17:09.455451 12903 sgd_solver.cpp:136] Iteration 20400, lr = 0.068125, m = 0.9
I0801 13:17:11.031255 12903 solver.cpp:353] Iteration 20500 (63.4583 iter/s, 1.57584s/100 iter), loss = 0.0385901
I0801 13:17:11.031358 12903 solver.cpp:375]     Train net output #0: loss = 0.0385907 (* 1 = 0.0385907 loss)
I0801 13:17:11.031375 12903 sgd_solver.cpp:136] Iteration 20500, lr = 0.0679687, m = 0.9
I0801 13:17:12.589074 12903 solver.cpp:353] Iteration 20600 (64.1944 iter/s, 1.55777s/100 iter), loss = 0.0532564
I0801 13:17:12.589100 12903 solver.cpp:375]     Train net output #0: loss = 0.0532571 (* 1 = 0.0532571 loss)
I0801 13:17:12.589107 12903 sgd_solver.cpp:136] Iteration 20600, lr = 0.0678125, m = 0.9
I0801 13:17:14.163791 12903 solver.cpp:353] Iteration 20700 (63.5055 iter/s, 1.57467s/100 iter), loss = 0.105265
I0801 13:17:14.163842 12903 solver.cpp:375]     Train net output #0: loss = 0.105266 (* 1 = 0.105266 loss)
I0801 13:17:14.163856 12903 sgd_solver.cpp:136] Iteration 20700, lr = 0.0676562, m = 0.9
I0801 13:17:15.754593 12903 solver.cpp:353] Iteration 20800 (62.8634 iter/s, 1.59075s/100 iter), loss = 0.0502444
I0801 13:17:15.754617 12903 solver.cpp:375]     Train net output #0: loss = 0.050245 (* 1 = 0.050245 loss)
I0801 13:17:15.754621 12903 sgd_solver.cpp:136] Iteration 20800, lr = 0.0675, m = 0.9
I0801 13:17:17.349426 12903 solver.cpp:353] Iteration 20900 (62.7045 iter/s, 1.59478s/100 iter), loss = 0.115353
I0801 13:17:17.349457 12903 solver.cpp:375]     Train net output #0: loss = 0.115354 (* 1 = 0.115354 loss)
I0801 13:17:17.349463 12903 sgd_solver.cpp:136] Iteration 20900, lr = 0.0673437, m = 0.9
I0801 13:17:18.934700 12903 solver.cpp:550] Iteration 21000, Testing net (#0)
I0801 13:17:19.756031 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.798236
I0801 13:17:19.756052 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.988824
I0801 13:17:19.756057 12903 solver.cpp:635]     Test net output #2: loss = 0.733127 (* 1 = 0.733127 loss)
I0801 13:17:19.756075 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.821353s
I0801 13:17:19.771512 12903 solver.cpp:353] Iteration 21000 (41.288 iter/s, 2.42201s/100 iter), loss = 0.190796
I0801 13:17:19.771530 12903 solver.cpp:375]     Train net output #0: loss = 0.190797 (* 1 = 0.190797 loss)
I0801 13:17:19.771536 12903 sgd_solver.cpp:136] Iteration 21000, lr = 0.0671875, m = 0.9
I0801 13:17:21.344230 12903 solver.cpp:353] Iteration 21100 (63.5862 iter/s, 1.57267s/100 iter), loss = 0.185934
I0801 13:17:21.344254 12903 solver.cpp:375]     Train net output #0: loss = 0.185935 (* 1 = 0.185935 loss)
I0801 13:17:21.344260 12903 sgd_solver.cpp:136] Iteration 21100, lr = 0.0670313, m = 0.9
I0801 13:17:22.943061 12903 solver.cpp:353] Iteration 21200 (62.5476 iter/s, 1.59878s/100 iter), loss = 0.12885
I0801 13:17:22.943089 12903 solver.cpp:375]     Train net output #0: loss = 0.128851 (* 1 = 0.128851 loss)
I0801 13:17:22.943094 12903 sgd_solver.cpp:136] Iteration 21200, lr = 0.066875, m = 0.9
I0801 13:17:24.556046 12903 solver.cpp:353] Iteration 21300 (61.9989 iter/s, 1.61293s/100 iter), loss = 0.107908
I0801 13:17:24.556073 12903 solver.cpp:375]     Train net output #0: loss = 0.107908 (* 1 = 0.107908 loss)
I0801 13:17:24.556079 12903 sgd_solver.cpp:136] Iteration 21300, lr = 0.0667187, m = 0.9
I0801 13:17:26.162856 12903 solver.cpp:353] Iteration 21400 (62.237 iter/s, 1.60676s/100 iter), loss = 0.0233504
I0801 13:17:26.162883 12903 solver.cpp:375]     Train net output #0: loss = 0.0233512 (* 1 = 0.0233512 loss)
I0801 13:17:26.162889 12903 sgd_solver.cpp:136] Iteration 21400, lr = 0.0665625, m = 0.9
I0801 13:17:27.721737 12903 solver.cpp:353] Iteration 21500 (64.1505 iter/s, 1.55883s/100 iter), loss = 0.0514108
I0801 13:17:27.721786 12903 solver.cpp:375]     Train net output #0: loss = 0.0514114 (* 1 = 0.0514114 loss)
I0801 13:17:27.721801 12903 sgd_solver.cpp:136] Iteration 21500, lr = 0.0664062, m = 0.9
I0801 13:17:29.315318 12903 solver.cpp:353] Iteration 21600 (62.7537 iter/s, 1.59353s/100 iter), loss = 0.0338371
I0801 13:17:29.315369 12903 solver.cpp:375]     Train net output #0: loss = 0.0338378 (* 1 = 0.0338378 loss)
I0801 13:17:29.315377 12903 sgd_solver.cpp:136] Iteration 21600, lr = 0.06625, m = 0.9
I0801 13:17:30.901733 12903 solver.cpp:353] Iteration 21700 (63.0372 iter/s, 1.58637s/100 iter), loss = 0.20073
I0801 13:17:30.901774 12903 solver.cpp:375]     Train net output #0: loss = 0.200731 (* 1 = 0.200731 loss)
I0801 13:17:30.901782 12903 sgd_solver.cpp:136] Iteration 21700, lr = 0.0660938, m = 0.9
I0801 13:17:32.491482 12903 solver.cpp:353] Iteration 21800 (62.9051 iter/s, 1.5897s/100 iter), loss = 0.047012
I0801 13:17:32.491544 12903 solver.cpp:375]     Train net output #0: loss = 0.0470126 (* 1 = 0.0470126 loss)
I0801 13:17:32.491564 12903 sgd_solver.cpp:136] Iteration 21800, lr = 0.0659375, m = 0.9
I0801 13:17:34.062002 12903 solver.cpp:353] Iteration 21900 (63.6752 iter/s, 1.57047s/100 iter), loss = 0.157895
I0801 13:17:34.062029 12903 solver.cpp:375]     Train net output #0: loss = 0.157896 (* 1 = 0.157896 loss)
I0801 13:17:34.062036 12903 sgd_solver.cpp:136] Iteration 21900, lr = 0.0657813, m = 0.9
I0801 13:17:35.625849 12903 solver.cpp:550] Iteration 22000, Testing net (#0)
I0801 13:17:36.060626 12893 data_reader.cpp:264] Starting prefetch of epoch 3
I0801 13:17:36.454396 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.789707
I0801 13:17:36.454416 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.987647
I0801 13:17:36.454421 12903 solver.cpp:635]     Test net output #2: loss = 0.752263 (* 1 = 0.752263 loss)
I0801 13:17:36.454435 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.828565s
I0801 13:17:36.470975 12903 solver.cpp:353] Iteration 22000 (41.5127 iter/s, 2.4089s/100 iter), loss = 0.0319324
I0801 13:17:36.470993 12903 solver.cpp:375]     Train net output #0: loss = 0.031933 (* 1 = 0.031933 loss)
I0801 13:17:36.470999 12903 sgd_solver.cpp:136] Iteration 22000, lr = 0.065625, m = 0.9
I0801 13:17:38.058241 12903 solver.cpp:353] Iteration 22100 (63.0036 iter/s, 1.58721s/100 iter), loss = 0.0547343
I0801 13:17:38.058267 12903 solver.cpp:375]     Train net output #0: loss = 0.0547349 (* 1 = 0.0547349 loss)
I0801 13:17:38.058274 12903 sgd_solver.cpp:136] Iteration 22100, lr = 0.0654688, m = 0.9
I0801 13:17:39.618600 12903 solver.cpp:353] Iteration 22200 (64.0898 iter/s, 1.56031s/100 iter), loss = 0.0524233
I0801 13:17:39.618651 12903 solver.cpp:375]     Train net output #0: loss = 0.0524239 (* 1 = 0.0524239 loss)
I0801 13:17:39.618666 12903 sgd_solver.cpp:136] Iteration 22200, lr = 0.0653125, m = 0.9
I0801 13:17:41.230015 12903 solver.cpp:353] Iteration 22300 (62.0592 iter/s, 1.61136s/100 iter), loss = 0.0111228
I0801 13:17:41.230135 12903 solver.cpp:375]     Train net output #0: loss = 0.0111234 (* 1 = 0.0111234 loss)
I0801 13:17:41.230145 12903 sgd_solver.cpp:136] Iteration 22300, lr = 0.0651563, m = 0.9
I0801 13:17:42.795311 12903 solver.cpp:353] Iteration 22400 (63.8875 iter/s, 1.56525s/100 iter), loss = 0.0443752
I0801 13:17:42.795339 12903 solver.cpp:375]     Train net output #0: loss = 0.0443758 (* 1 = 0.0443758 loss)
I0801 13:17:42.795346 12903 sgd_solver.cpp:136] Iteration 22400, lr = 0.065, m = 0.9
I0801 13:17:44.398124 12903 solver.cpp:353] Iteration 22500 (62.3923 iter/s, 1.60276s/100 iter), loss = 0.131248
I0801 13:17:44.398177 12903 solver.cpp:375]     Train net output #0: loss = 0.131249 (* 1 = 0.131249 loss)
I0801 13:17:44.398190 12903 sgd_solver.cpp:136] Iteration 22500, lr = 0.0648438, m = 0.9
I0801 13:17:46.019071 12903 solver.cpp:353] Iteration 22600 (61.6943 iter/s, 1.62089s/100 iter), loss = 0.0305089
I0801 13:17:46.019098 12903 solver.cpp:375]     Train net output #0: loss = 0.0305095 (* 1 = 0.0305095 loss)
I0801 13:17:46.019101 12903 sgd_solver.cpp:136] Iteration 22600, lr = 0.0646875, m = 0.9
I0801 13:17:47.614580 12903 solver.cpp:353] Iteration 22700 (62.6779 iter/s, 1.59546s/100 iter), loss = 0.0776237
I0801 13:17:47.614634 12903 solver.cpp:375]     Train net output #0: loss = 0.0776243 (* 1 = 0.0776243 loss)
I0801 13:17:47.614650 12903 sgd_solver.cpp:136] Iteration 22700, lr = 0.0645313, m = 0.9
I0801 13:17:49.179945 12903 solver.cpp:353] Iteration 22800 (63.8848 iter/s, 1.56532s/100 iter), loss = 0.142916
I0801 13:17:49.179973 12903 solver.cpp:375]     Train net output #0: loss = 0.142916 (* 1 = 0.142916 loss)
I0801 13:17:49.179980 12903 sgd_solver.cpp:136] Iteration 22800, lr = 0.064375, m = 0.9
I0801 13:17:50.752549 12903 solver.cpp:353] Iteration 22900 (63.5908 iter/s, 1.57255s/100 iter), loss = 0.0195689
I0801 13:17:50.752611 12903 solver.cpp:375]     Train net output #0: loss = 0.0195696 (* 1 = 0.0195696 loss)
I0801 13:17:50.752619 12903 sgd_solver.cpp:136] Iteration 22900, lr = 0.0642188, m = 0.9
I0801 13:17:52.310751 12903 solver.cpp:550] Iteration 23000, Testing net (#0)
I0801 13:17:53.129485 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.83853
I0801 13:17:53.129503 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.990294
I0801 13:17:53.129508 12903 solver.cpp:635]     Test net output #2: loss = 0.55757 (* 1 = 0.55757 loss)
I0801 13:17:53.129525 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.818751s
I0801 13:17:53.145289 12903 solver.cpp:353] Iteration 23000 (41.7943 iter/s, 2.39267s/100 iter), loss = 0.15341
I0801 13:17:53.145308 12903 solver.cpp:375]     Train net output #0: loss = 0.15341 (* 1 = 0.15341 loss)
I0801 13:17:53.145313 12903 sgd_solver.cpp:136] Iteration 23000, lr = 0.0640625, m = 0.9
I0801 13:17:54.706693 12903 solver.cpp:353] Iteration 23100 (64.0469 iter/s, 1.56136s/100 iter), loss = 0.0606729
I0801 13:17:54.706722 12903 solver.cpp:375]     Train net output #0: loss = 0.0606735 (* 1 = 0.0606735 loss)
I0801 13:17:54.706727 12903 sgd_solver.cpp:136] Iteration 23100, lr = 0.0639063, m = 0.9
I0801 13:17:56.283850 12903 solver.cpp:353] Iteration 23200 (63.4074 iter/s, 1.5771s/100 iter), loss = 0.0618001
I0801 13:17:56.283875 12903 solver.cpp:375]     Train net output #0: loss = 0.0618008 (* 1 = 0.0618008 loss)
I0801 13:17:56.283879 12903 sgd_solver.cpp:136] Iteration 23200, lr = 0.06375, m = 0.9
I0801 13:17:57.845881 12903 solver.cpp:353] Iteration 23300 (64.0211 iter/s, 1.56198s/100 iter), loss = 0.0129523
I0801 13:17:57.845911 12903 solver.cpp:375]     Train net output #0: loss = 0.012953 (* 1 = 0.012953 loss)
I0801 13:17:57.845916 12903 sgd_solver.cpp:136] Iteration 23300, lr = 0.0635938, m = 0.9
I0801 13:17:59.420377 12903 solver.cpp:353] Iteration 23400 (63.5144 iter/s, 1.57445s/100 iter), loss = 0.0354972
I0801 13:17:59.420426 12903 solver.cpp:375]     Train net output #0: loss = 0.035498 (* 1 = 0.035498 loss)
I0801 13:17:59.420440 12903 sgd_solver.cpp:136] Iteration 23400, lr = 0.0634375, m = 0.9
I0801 13:18:00.981323 12903 solver.cpp:353] Iteration 23500 (64.0658 iter/s, 1.56089s/100 iter), loss = 0.0336877
I0801 13:18:00.981387 12903 solver.cpp:375]     Train net output #0: loss = 0.0336885 (* 1 = 0.0336885 loss)
I0801 13:18:00.981400 12903 sgd_solver.cpp:136] Iteration 23500, lr = 0.0632813, m = 0.9
I0801 13:18:02.558248 12903 solver.cpp:353] Iteration 23600 (63.4165 iter/s, 1.57688s/100 iter), loss = 0.0725311
I0801 13:18:02.558296 12903 solver.cpp:375]     Train net output #0: loss = 0.0725319 (* 1 = 0.0725319 loss)
I0801 13:18:02.558310 12903 sgd_solver.cpp:136] Iteration 23600, lr = 0.063125, m = 0.9
I0801 13:18:04.136199 12903 solver.cpp:353] Iteration 23700 (63.3754 iter/s, 1.5779s/100 iter), loss = 0.0202771
I0801 13:18:04.136225 12903 solver.cpp:375]     Train net output #0: loss = 0.0202779 (* 1 = 0.0202779 loss)
I0801 13:18:04.136231 12903 sgd_solver.cpp:136] Iteration 23700, lr = 0.0629688, m = 0.9
I0801 13:18:05.715359 12903 solver.cpp:353] Iteration 23800 (63.3269 iter/s, 1.57911s/100 iter), loss = 0.0552958
I0801 13:18:05.715410 12903 solver.cpp:375]     Train net output #0: loss = 0.0552966 (* 1 = 0.0552966 loss)
I0801 13:18:05.715425 12903 sgd_solver.cpp:136] Iteration 23800, lr = 0.0628125, m = 0.9
I0801 13:18:07.298463 12903 solver.cpp:353] Iteration 23900 (63.169 iter/s, 1.58305s/100 iter), loss = 0.0468401
I0801 13:18:07.298493 12903 solver.cpp:375]     Train net output #0: loss = 0.0468409 (* 1 = 0.0468409 loss)
I0801 13:18:07.298501 12903 sgd_solver.cpp:136] Iteration 23900, lr = 0.0626562, m = 0.9
I0801 13:18:08.866011 12903 solver.cpp:550] Iteration 24000, Testing net (#0)
I0801 13:18:09.689333 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.764707
I0801 13:18:09.689353 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.986765
I0801 13:18:09.689358 12903 solver.cpp:635]     Test net output #2: loss = 0.981463 (* 1 = 0.981463 loss)
I0801 13:18:09.689374 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.823342s
I0801 13:18:09.704948 12903 solver.cpp:353] Iteration 24000 (41.5556 iter/s, 2.40642s/100 iter), loss = 0.282579
I0801 13:18:09.704969 12903 solver.cpp:375]     Train net output #0: loss = 0.282579 (* 1 = 0.282579 loss)
I0801 13:18:09.704974 12903 sgd_solver.cpp:136] Iteration 24000, lr = 0.0625, m = 0.9
I0801 13:18:11.261765 12903 solver.cpp:353] Iteration 24100 (64.2357 iter/s, 1.55677s/100 iter), loss = 0.234483
I0801 13:18:11.261828 12903 solver.cpp:375]     Train net output #0: loss = 0.234484 (* 1 = 0.234484 loss)
I0801 13:18:11.261837 12903 sgd_solver.cpp:136] Iteration 24100, lr = 0.0623438, m = 0.9
I0801 13:18:12.835752 12903 solver.cpp:353] Iteration 24200 (63.535 iter/s, 1.57394s/100 iter), loss = 0.0764246
I0801 13:18:12.835817 12903 solver.cpp:375]     Train net output #0: loss = 0.0764253 (* 1 = 0.0764253 loss)
I0801 13:18:12.835837 12903 sgd_solver.cpp:136] Iteration 24200, lr = 0.0621875, m = 0.9
I0801 13:18:14.423643 12903 solver.cpp:353] Iteration 24300 (62.9785 iter/s, 1.58784s/100 iter), loss = 0.0421338
I0801 13:18:14.423667 12903 solver.cpp:375]     Train net output #0: loss = 0.0421344 (* 1 = 0.0421344 loss)
I0801 13:18:14.423671 12903 sgd_solver.cpp:136] Iteration 24300, lr = 0.0620313, m = 0.9
I0801 13:18:16.012585 12903 solver.cpp:353] Iteration 24400 (62.937 iter/s, 1.58889s/100 iter), loss = 0.0200215
I0801 13:18:16.012610 12903 solver.cpp:375]     Train net output #0: loss = 0.0200222 (* 1 = 0.0200222 loss)
I0801 13:18:16.012616 12903 sgd_solver.cpp:136] Iteration 24400, lr = 0.061875, m = 0.9
I0801 13:18:17.579010 12903 solver.cpp:353] Iteration 24500 (63.8417 iter/s, 1.56637s/100 iter), loss = 0.0234707
I0801 13:18:17.579061 12903 solver.cpp:375]     Train net output #0: loss = 0.0234714 (* 1 = 0.0234714 loss)
I0801 13:18:17.579068 12903 sgd_solver.cpp:136] Iteration 24500, lr = 0.0617188, m = 0.9
I0801 13:18:19.146867 12903 solver.cpp:353] Iteration 24600 (63.7834 iter/s, 1.56781s/100 iter), loss = 0.349178
I0801 13:18:19.146891 12903 solver.cpp:375]     Train net output #0: loss = 0.349179 (* 1 = 0.349179 loss)
I0801 13:18:19.146896 12903 sgd_solver.cpp:136] Iteration 24600, lr = 0.0615625, m = 0.9
I0801 13:18:20.727782 12903 solver.cpp:353] Iteration 24700 (63.2564 iter/s, 1.58087s/100 iter), loss = 0.0277971
I0801 13:18:20.727833 12903 solver.cpp:375]     Train net output #0: loss = 0.0277978 (* 1 = 0.0277978 loss)
I0801 13:18:20.727846 12903 sgd_solver.cpp:136] Iteration 24700, lr = 0.0614063, m = 0.9
I0801 13:18:22.298023 12903 solver.cpp:353] Iteration 24800 (63.6865 iter/s, 1.57019s/100 iter), loss = 0.0229158
I0801 13:18:22.298049 12903 solver.cpp:375]     Train net output #0: loss = 0.0229165 (* 1 = 0.0229165 loss)
I0801 13:18:22.298055 12903 sgd_solver.cpp:136] Iteration 24800, lr = 0.06125, m = 0.9
I0801 13:18:23.871305 12903 solver.cpp:353] Iteration 24900 (63.5635 iter/s, 1.57323s/100 iter), loss = 0.209355
I0801 13:18:23.871332 12903 solver.cpp:375]     Train net output #0: loss = 0.209356 (* 1 = 0.209356 loss)
I0801 13:18:23.871338 12903 sgd_solver.cpp:136] Iteration 24900, lr = 0.0610937, m = 0.9
I0801 13:18:25.446949 12903 solver.cpp:550] Iteration 25000, Testing net (#0)
I0801 13:18:26.284291 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.802354
I0801 13:18:26.284310 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.987353
I0801 13:18:26.284315 12903 solver.cpp:635]     Test net output #2: loss = 0.76946 (* 1 = 0.76946 loss)
I0801 13:18:26.284330 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.837359s
I0801 13:18:26.299943 12903 solver.cpp:353] Iteration 25000 (41.1765 iter/s, 2.42857s/100 iter), loss = 0.0128152
I0801 13:18:26.299960 12903 solver.cpp:375]     Train net output #0: loss = 0.0128161 (* 1 = 0.0128161 loss)
I0801 13:18:26.299963 12903 sgd_solver.cpp:136] Iteration 25000, lr = 0.0609375, m = 0.9
I0801 13:18:27.879655 12903 solver.cpp:353] Iteration 25100 (63.3049 iter/s, 1.57966s/100 iter), loss = 0.0378628
I0801 13:18:27.879684 12903 solver.cpp:375]     Train net output #0: loss = 0.0378637 (* 1 = 0.0378637 loss)
I0801 13:18:27.879691 12903 sgd_solver.cpp:136] Iteration 25100, lr = 0.0607813, m = 0.9
I0801 13:18:29.481611 12903 solver.cpp:353] Iteration 25200 (62.4257 iter/s, 1.6019s/100 iter), loss = 0.145939
I0801 13:18:29.481637 12903 solver.cpp:375]     Train net output #0: loss = 0.14594 (* 1 = 0.14594 loss)
I0801 13:18:29.481642 12903 sgd_solver.cpp:136] Iteration 25200, lr = 0.060625, m = 0.9
I0801 13:18:31.055794 12903 solver.cpp:353] Iteration 25300 (63.5269 iter/s, 1.57414s/100 iter), loss = 0.0300126
I0801 13:18:31.055863 12903 solver.cpp:375]     Train net output #0: loss = 0.0300135 (* 1 = 0.0300135 loss)
I0801 13:18:31.055876 12903 sgd_solver.cpp:136] Iteration 25300, lr = 0.0604688, m = 0.9
I0801 13:18:32.636147 12903 solver.cpp:353] Iteration 25400 (63.279 iter/s, 1.5803s/100 iter), loss = 0.0341789
I0801 13:18:32.636174 12903 solver.cpp:375]     Train net output #0: loss = 0.0341798 (* 1 = 0.0341798 loss)
I0801 13:18:32.636180 12903 sgd_solver.cpp:136] Iteration 25400, lr = 0.0603125, m = 0.9
I0801 13:18:34.195500 12903 solver.cpp:353] Iteration 25500 (64.1312 iter/s, 1.5593s/100 iter), loss = 0.0432964
I0801 13:18:34.195525 12903 solver.cpp:375]     Train net output #0: loss = 0.0432974 (* 1 = 0.0432974 loss)
I0801 13:18:34.195531 12903 sgd_solver.cpp:136] Iteration 25500, lr = 0.0601563, m = 0.9
I0801 13:18:35.766695 12903 solver.cpp:353] Iteration 25600 (63.6478 iter/s, 1.57115s/100 iter), loss = 0.196588
I0801 13:18:35.766719 12903 solver.cpp:375]     Train net output #0: loss = 0.196589 (* 1 = 0.196589 loss)
I0801 13:18:35.766723 12903 sgd_solver.cpp:136] Iteration 25600, lr = 0.06, m = 0.9
I0801 13:18:37.341743 12903 solver.cpp:353] Iteration 25700 (63.4921 iter/s, 1.575s/100 iter), loss = 0.010989
I0801 13:18:37.341770 12903 solver.cpp:375]     Train net output #0: loss = 0.0109898 (* 1 = 0.0109898 loss)
I0801 13:18:37.341776 12903 sgd_solver.cpp:136] Iteration 25700, lr = 0.0598437, m = 0.9
I0801 13:18:38.929780 12903 solver.cpp:353] Iteration 25800 (62.9729 iter/s, 1.58798s/100 iter), loss = 0.147114
I0801 13:18:38.929802 12903 solver.cpp:375]     Train net output #0: loss = 0.147115 (* 1 = 0.147115 loss)
I0801 13:18:38.929807 12903 sgd_solver.cpp:136] Iteration 25800, lr = 0.0596875, m = 0.9
I0801 13:18:40.496073 12903 solver.cpp:353] Iteration 25900 (63.8469 iter/s, 1.56625s/100 iter), loss = 0.0229598
I0801 13:18:40.496122 12903 solver.cpp:375]     Train net output #0: loss = 0.0229606 (* 1 = 0.0229606 loss)
I0801 13:18:40.496134 12903 sgd_solver.cpp:136] Iteration 25900, lr = 0.0595312, m = 0.9
I0801 13:18:42.041857 12903 solver.cpp:550] Iteration 26000, Testing net (#0)
I0801 13:18:42.863293 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.841472
I0801 13:18:42.863313 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.990588
I0801 13:18:42.863318 12903 solver.cpp:635]     Test net output #2: loss = 0.65314 (* 1 = 0.65314 loss)
I0801 13:18:42.863335 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.821455s
I0801 13:18:42.879252 12903 solver.cpp:353] Iteration 26000 (41.962 iter/s, 2.38311s/100 iter), loss = 0.182856
I0801 13:18:42.879271 12903 solver.cpp:375]     Train net output #0: loss = 0.182857 (* 1 = 0.182857 loss)
I0801 13:18:42.879277 12903 sgd_solver.cpp:136] Iteration 26000, lr = 0.059375, m = 0.9
I0801 13:18:44.462620 12903 solver.cpp:353] Iteration 26100 (63.1585 iter/s, 1.58332s/100 iter), loss = 0.0362426
I0801 13:18:44.462646 12903 solver.cpp:375]     Train net output #0: loss = 0.0362434 (* 1 = 0.0362434 loss)
I0801 13:18:44.462651 12903 sgd_solver.cpp:136] Iteration 26100, lr = 0.0592188, m = 0.9
I0801 13:18:46.031821 12903 solver.cpp:353] Iteration 26200 (63.7289 iter/s, 1.56915s/100 iter), loss = 0.0030601
I0801 13:18:46.031847 12903 solver.cpp:375]     Train net output #0: loss = 0.00306086 (* 1 = 0.00306086 loss)
I0801 13:18:46.031852 12903 sgd_solver.cpp:136] Iteration 26200, lr = 0.0590625, m = 0.9
I0801 13:18:47.604722 12903 solver.cpp:353] Iteration 26300 (63.5787 iter/s, 1.57285s/100 iter), loss = 0.0444908
I0801 13:18:47.604746 12903 solver.cpp:375]     Train net output #0: loss = 0.0444916 (* 1 = 0.0444916 loss)
I0801 13:18:47.604753 12903 sgd_solver.cpp:136] Iteration 26300, lr = 0.0589063, m = 0.9
I0801 13:18:49.168068 12903 solver.cpp:353] Iteration 26400 (63.9673 iter/s, 1.5633s/100 iter), loss = 0.0316287
I0801 13:18:49.168093 12903 solver.cpp:375]     Train net output #0: loss = 0.0316294 (* 1 = 0.0316294 loss)
I0801 13:18:49.168099 12903 sgd_solver.cpp:136] Iteration 26400, lr = 0.05875, m = 0.9
I0801 13:18:50.738174 12903 solver.cpp:353] Iteration 26500 (63.692 iter/s, 1.57006s/100 iter), loss = 0.0137714
I0801 13:18:50.738200 12903 solver.cpp:375]     Train net output #0: loss = 0.0137721 (* 1 = 0.0137721 loss)
I0801 13:18:50.738205 12903 sgd_solver.cpp:136] Iteration 26500, lr = 0.0585938, m = 0.9
I0801 13:18:50.926573 12870 data_reader.cpp:264] Starting prefetch of epoch 4
I0801 13:18:52.308274 12903 solver.cpp:353] Iteration 26600 (63.6922 iter/s, 1.57005s/100 iter), loss = 0.138731
I0801 13:18:52.308298 12903 solver.cpp:375]     Train net output #0: loss = 0.138732 (* 1 = 0.138732 loss)
I0801 13:18:52.308305 12903 sgd_solver.cpp:136] Iteration 26600, lr = 0.0584375, m = 0.9
I0801 13:18:53.866943 12903 solver.cpp:353] Iteration 26700 (64.1594 iter/s, 1.55862s/100 iter), loss = 0.124244
I0801 13:18:53.866968 12903 solver.cpp:375]     Train net output #0: loss = 0.124245 (* 1 = 0.124245 loss)
I0801 13:18:53.866976 12903 sgd_solver.cpp:136] Iteration 26700, lr = 0.0582813, m = 0.9
I0801 13:18:55.444183 12903 solver.cpp:353] Iteration 26800 (63.4038 iter/s, 1.57719s/100 iter), loss = 0.0136097
I0801 13:18:55.444211 12903 solver.cpp:375]     Train net output #0: loss = 0.0136105 (* 1 = 0.0136105 loss)
I0801 13:18:55.444216 12903 sgd_solver.cpp:136] Iteration 26800, lr = 0.058125, m = 0.9
I0801 13:18:57.002624 12903 solver.cpp:353] Iteration 26900 (64.1688 iter/s, 1.55839s/100 iter), loss = 0.134228
I0801 13:18:57.002648 12903 solver.cpp:375]     Train net output #0: loss = 0.134229 (* 1 = 0.134229 loss)
I0801 13:18:57.002655 12903 sgd_solver.cpp:136] Iteration 26900, lr = 0.0579687, m = 0.9
I0801 13:18:58.559458 12903 solver.cpp:550] Iteration 27000, Testing net (#0)
I0801 13:18:59.374589 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.761177
I0801 13:18:59.374613 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.98
I0801 13:18:59.374619 12903 solver.cpp:635]     Test net output #2: loss = 1.04647 (* 1 = 1.04647 loss)
I0801 13:18:59.374639 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.815155s
I0801 13:18:59.390285 12903 solver.cpp:353] Iteration 27000 (41.8832 iter/s, 2.38759s/100 iter), loss = 0.155631
I0801 13:18:59.390326 12903 solver.cpp:375]     Train net output #0: loss = 0.155632 (* 1 = 0.155632 loss)
I0801 13:18:59.390333 12903 sgd_solver.cpp:136] Iteration 27000, lr = 0.0578125, m = 0.9
I0801 13:19:00.951516 12903 solver.cpp:353] Iteration 27100 (64.0541 iter/s, 1.56118s/100 iter), loss = 0.0136827
I0801 13:19:00.951539 12903 solver.cpp:375]     Train net output #0: loss = 0.0136835 (* 1 = 0.0136835 loss)
I0801 13:19:00.951544 12903 sgd_solver.cpp:136] Iteration 27100, lr = 0.0576563, m = 0.9
I0801 13:19:02.522156 12903 solver.cpp:353] Iteration 27200 (63.6703 iter/s, 1.57059s/100 iter), loss = 0.0298563
I0801 13:19:02.522209 12903 solver.cpp:375]     Train net output #0: loss = 0.0298572 (* 1 = 0.0298572 loss)
I0801 13:19:02.522218 12903 sgd_solver.cpp:136] Iteration 27200, lr = 0.0575, m = 0.9
I0801 13:19:04.096778 12903 solver.cpp:353] Iteration 27300 (63.5093 iter/s, 1.57457s/100 iter), loss = 0.0440706
I0801 13:19:04.096803 12903 solver.cpp:375]     Train net output #0: loss = 0.0440715 (* 1 = 0.0440715 loss)
I0801 13:19:04.096807 12903 sgd_solver.cpp:136] Iteration 27300, lr = 0.0573438, m = 0.9
I0801 13:19:05.652714 12903 solver.cpp:353] Iteration 27400 (64.272 iter/s, 1.55589s/100 iter), loss = 0.0122278
I0801 13:19:05.652737 12903 solver.cpp:375]     Train net output #0: loss = 0.0122286 (* 1 = 0.0122286 loss)
I0801 13:19:05.652741 12903 sgd_solver.cpp:136] Iteration 27400, lr = 0.0571875, m = 0.9
I0801 13:19:07.231357 12903 solver.cpp:353] Iteration 27500 (63.3475 iter/s, 1.57859s/100 iter), loss = 0.00341758
I0801 13:19:07.231384 12903 solver.cpp:375]     Train net output #0: loss = 0.00341844 (* 1 = 0.00341844 loss)
I0801 13:19:07.231389 12903 sgd_solver.cpp:136] Iteration 27500, lr = 0.0570313, m = 0.9
I0801 13:19:08.784945 12903 solver.cpp:353] Iteration 27600 (64.3692 iter/s, 1.55354s/100 iter), loss = 0.0835366
I0801 13:19:08.784970 12903 solver.cpp:375]     Train net output #0: loss = 0.0835375 (* 1 = 0.0835375 loss)
I0801 13:19:08.784976 12903 sgd_solver.cpp:136] Iteration 27600, lr = 0.056875, m = 0.9
I0801 13:19:10.342945 12903 solver.cpp:353] Iteration 27700 (64.1867 iter/s, 1.55796s/100 iter), loss = 0.0201948
I0801 13:19:10.342970 12903 solver.cpp:375]     Train net output #0: loss = 0.0201957 (* 1 = 0.0201957 loss)
I0801 13:19:10.342975 12903 sgd_solver.cpp:136] Iteration 27700, lr = 0.0567187, m = 0.9
I0801 13:19:11.909867 12903 solver.cpp:353] Iteration 27800 (63.8215 iter/s, 1.56687s/100 iter), loss = 0.110205
I0801 13:19:11.909917 12903 solver.cpp:375]     Train net output #0: loss = 0.110206 (* 1 = 0.110206 loss)
I0801 13:19:11.909931 12903 sgd_solver.cpp:136] Iteration 27800, lr = 0.0565625, m = 0.9
I0801 13:19:13.484905 12903 solver.cpp:353] Iteration 27900 (63.4925 iter/s, 1.57499s/100 iter), loss = 0.0792626
I0801 13:19:13.484977 12903 solver.cpp:375]     Train net output #0: loss = 0.0792634 (* 1 = 0.0792634 loss)
I0801 13:19:13.484984 12903 sgd_solver.cpp:136] Iteration 27900, lr = 0.0564062, m = 0.9
I0801 13:19:15.039685 12903 solver.cpp:550] Iteration 28000, Testing net (#0)
I0801 13:19:15.865063 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.818237
I0801 13:19:15.865083 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.985882
I0801 13:19:15.865088 12903 solver.cpp:635]     Test net output #2: loss = 0.720152 (* 1 = 0.720152 loss)
I0801 13:19:15.865103 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.825396s
I0801 13:19:15.880712 12903 solver.cpp:353] Iteration 28000 (41.7408 iter/s, 2.39574s/100 iter), loss = 0.0103387
I0801 13:19:15.880730 12903 solver.cpp:375]     Train net output #0: loss = 0.0103395 (* 1 = 0.0103395 loss)
I0801 13:19:15.880735 12903 sgd_solver.cpp:136] Iteration 28000, lr = 0.05625, m = 0.9
I0801 13:19:17.458732 12903 solver.cpp:353] Iteration 28100 (63.3727 iter/s, 1.57797s/100 iter), loss = 0.00308307
I0801 13:19:17.458758 12903 solver.cpp:375]     Train net output #0: loss = 0.00308388 (* 1 = 0.00308388 loss)
I0801 13:19:17.458765 12903 sgd_solver.cpp:136] Iteration 28100, lr = 0.0560938, m = 0.9
I0801 13:19:19.025322 12903 solver.cpp:353] Iteration 28200 (63.8348 iter/s, 1.56654s/100 iter), loss = 0.0154131
I0801 13:19:19.025374 12903 solver.cpp:375]     Train net output #0: loss = 0.0154139 (* 1 = 0.0154139 loss)
I0801 13:19:19.025389 12903 sgd_solver.cpp:136] Iteration 28200, lr = 0.0559375, m = 0.9
I0801 13:19:20.597263 12903 solver.cpp:353] Iteration 28300 (63.6176 iter/s, 1.57189s/100 iter), loss = 0.00487105
I0801 13:19:20.597332 12903 solver.cpp:375]     Train net output #0: loss = 0.00487193 (* 1 = 0.00487193 loss)
I0801 13:19:20.597352 12903 sgd_solver.cpp:136] Iteration 28300, lr = 0.0557813, m = 0.9
I0801 13:19:22.177201 12903 solver.cpp:353] Iteration 28400 (63.2958 iter/s, 1.57988s/100 iter), loss = 0.00554207
I0801 13:19:22.177222 12903 solver.cpp:375]     Train net output #0: loss = 0.00554294 (* 1 = 0.00554294 loss)
I0801 13:19:22.177227 12903 sgd_solver.cpp:136] Iteration 28400, lr = 0.055625, m = 0.9
I0801 13:19:23.737725 12903 solver.cpp:353] Iteration 28500 (64.0829 iter/s, 1.56048s/100 iter), loss = 0.0227279
I0801 13:19:23.737751 12903 solver.cpp:375]     Train net output #0: loss = 0.0227288 (* 1 = 0.0227288 loss)
I0801 13:19:23.737757 12903 sgd_solver.cpp:136] Iteration 28500, lr = 0.0554687, m = 0.9
I0801 13:19:25.313868 12903 solver.cpp:353] Iteration 28600 (63.448 iter/s, 1.57609s/100 iter), loss = 0.0582862
I0801 13:19:25.313930 12903 solver.cpp:375]     Train net output #0: loss = 0.0582871 (* 1 = 0.0582871 loss)
I0801 13:19:25.313946 12903 sgd_solver.cpp:136] Iteration 28600, lr = 0.0553125, m = 0.9
I0801 13:19:26.871470 12903 solver.cpp:353] Iteration 28700 (64.2033 iter/s, 1.55755s/100 iter), loss = 0.00931215
I0801 13:19:26.871495 12903 solver.cpp:375]     Train net output #0: loss = 0.00931306 (* 1 = 0.00931306 loss)
I0801 13:19:26.871501 12903 sgd_solver.cpp:136] Iteration 28700, lr = 0.0551562, m = 0.9
I0801 13:19:28.437305 12903 solver.cpp:353] Iteration 28800 (63.8657 iter/s, 1.56579s/100 iter), loss = 0.204172
I0801 13:19:28.437330 12903 solver.cpp:375]     Train net output #0: loss = 0.204173 (* 1 = 0.204173 loss)
I0801 13:19:28.437335 12903 sgd_solver.cpp:136] Iteration 28800, lr = 0.055, m = 0.9
I0801 13:19:30.002331 12903 solver.cpp:353] Iteration 28900 (63.8986 iter/s, 1.56498s/100 iter), loss = 0.384051
I0801 13:19:30.002377 12903 solver.cpp:375]     Train net output #0: loss = 0.384052 (* 1 = 0.384052 loss)
I0801 13:19:30.002390 12903 sgd_solver.cpp:136] Iteration 28900, lr = 0.0548437, m = 0.9
I0801 13:19:31.560981 12903 solver.cpp:550] Iteration 29000, Testing net (#0)
I0801 13:19:32.377212 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.808237
I0801 13:19:32.377233 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.989706
I0801 13:19:32.377239 12903 solver.cpp:635]     Test net output #2: loss = 0.727913 (* 1 = 0.727913 loss)
I0801 13:19:32.377271 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.816268s
I0801 13:19:32.392943 12903 solver.cpp:353] Iteration 29000 (41.8315 iter/s, 2.39054s/100 iter), loss = 0.0255828
I0801 13:19:32.392961 12903 solver.cpp:375]     Train net output #0: loss = 0.0255838 (* 1 = 0.0255838 loss)
I0801 13:19:32.392966 12903 sgd_solver.cpp:136] Iteration 29000, lr = 0.0546875, m = 0.9
I0801 13:19:33.978689 12903 solver.cpp:353] Iteration 29100 (63.0639 iter/s, 1.58569s/100 iter), loss = 0.166557
I0801 13:19:33.978715 12903 solver.cpp:375]     Train net output #0: loss = 0.166558 (* 1 = 0.166558 loss)
I0801 13:19:33.978720 12903 sgd_solver.cpp:136] Iteration 29100, lr = 0.0545313, m = 0.9
I0801 13:19:35.544332 12903 solver.cpp:353] Iteration 29200 (63.8734 iter/s, 1.5656s/100 iter), loss = 0.0122728
I0801 13:19:35.544358 12903 solver.cpp:375]     Train net output #0: loss = 0.0122737 (* 1 = 0.0122737 loss)
I0801 13:19:35.544361 12903 sgd_solver.cpp:136] Iteration 29200, lr = 0.054375, m = 0.9
I0801 13:19:37.114794 12903 solver.cpp:353] Iteration 29300 (63.6776 iter/s, 1.57041s/100 iter), loss = 0.0565891
I0801 13:19:37.114819 12903 solver.cpp:375]     Train net output #0: loss = 0.05659 (* 1 = 0.05659 loss)
I0801 13:19:37.114825 12903 sgd_solver.cpp:136] Iteration 29300, lr = 0.0542188, m = 0.9
I0801 13:19:38.693460 12903 solver.cpp:353] Iteration 29400 (63.3466 iter/s, 1.57862s/100 iter), loss = 0.0995631
I0801 13:19:38.693488 12903 solver.cpp:375]     Train net output #0: loss = 0.099564 (* 1 = 0.099564 loss)
I0801 13:19:38.693495 12903 sgd_solver.cpp:136] Iteration 29400, lr = 0.0540625, m = 0.9
I0801 13:19:40.271533 12903 solver.cpp:353] Iteration 29500 (63.3705 iter/s, 1.57802s/100 iter), loss = 0.108334
I0801 13:19:40.271558 12903 solver.cpp:375]     Train net output #0: loss = 0.108335 (* 1 = 0.108335 loss)
I0801 13:19:40.271562 12903 sgd_solver.cpp:136] Iteration 29500, lr = 0.0539063, m = 0.9
I0801 13:19:41.827592 12903 solver.cpp:353] Iteration 29600 (64.2669 iter/s, 1.55601s/100 iter), loss = 0.0224814
I0801 13:19:41.827620 12903 solver.cpp:375]     Train net output #0: loss = 0.0224822 (* 1 = 0.0224822 loss)
I0801 13:19:41.827626 12903 sgd_solver.cpp:136] Iteration 29600, lr = 0.05375, m = 0.9
I0801 13:19:43.404446 12903 solver.cpp:353] Iteration 29700 (63.4194 iter/s, 1.57681s/100 iter), loss = 0.0276138
I0801 13:19:43.404470 12903 solver.cpp:375]     Train net output #0: loss = 0.0276147 (* 1 = 0.0276147 loss)
I0801 13:19:43.404475 12903 sgd_solver.cpp:136] Iteration 29700, lr = 0.0535938, m = 0.9
I0801 13:19:44.973008 12903 solver.cpp:353] Iteration 29800 (63.7547 iter/s, 1.56851s/100 iter), loss = 0.2419
I0801 13:19:44.973090 12903 solver.cpp:375]     Train net output #0: loss = 0.241901 (* 1 = 0.241901 loss)
I0801 13:19:44.973098 12903 sgd_solver.cpp:136] Iteration 29800, lr = 0.0534375, m = 0.9
I0801 13:19:46.556089 12903 solver.cpp:353] Iteration 29900 (63.17 iter/s, 1.58303s/100 iter), loss = 0.0645409
I0801 13:19:46.556155 12903 solver.cpp:375]     Train net output #0: loss = 0.0645418 (* 1 = 0.0645418 loss)
I0801 13:19:46.556174 12903 sgd_solver.cpp:136] Iteration 29900, lr = 0.0532812, m = 0.9
I0801 13:19:48.107110 12903 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/cifar10_jacintonet11v2_iter_30000.caffemodel
I0801 13:19:48.115345 12903 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/cifar10_jacintonet11v2_iter_30000.solverstate
I0801 13:19:48.119196 12903 solver.cpp:550] Iteration 30000, Testing net (#0)
I0801 13:19:48.924273 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.816766
I0801 13:19:48.924293 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.977647
I0801 13:19:48.924299 12903 solver.cpp:635]     Test net output #2: loss = 0.774208 (* 1 = 0.774208 loss)
I0801 13:19:48.924316 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.805095s
I0801 13:19:48.939811 12903 solver.cpp:353] Iteration 30000 (41.9524 iter/s, 2.38365s/100 iter), loss = 0.00282293
I0801 13:19:48.939831 12903 solver.cpp:375]     Train net output #0: loss = 0.00282387 (* 1 = 0.00282387 loss)
I0801 13:19:48.939836 12903 sgd_solver.cpp:136] Iteration 30000, lr = 0.053125, m = 0.9
I0801 13:19:50.522711 12903 solver.cpp:353] Iteration 30100 (63.1772 iter/s, 1.58285s/100 iter), loss = 0.0173817
I0801 13:19:50.522737 12903 solver.cpp:375]     Train net output #0: loss = 0.0173826 (* 1 = 0.0173826 loss)
I0801 13:19:50.522742 12903 sgd_solver.cpp:136] Iteration 30100, lr = 0.0529688, m = 0.9
I0801 13:19:52.087143 12903 solver.cpp:353] Iteration 30200 (63.923 iter/s, 1.56438s/100 iter), loss = 0.0142286
I0801 13:19:52.087172 12903 solver.cpp:375]     Train net output #0: loss = 0.0142297 (* 1 = 0.0142297 loss)
I0801 13:19:52.087177 12903 sgd_solver.cpp:136] Iteration 30200, lr = 0.0528125, m = 0.9
I0801 13:19:53.657212 12903 solver.cpp:353] Iteration 30300 (63.6935 iter/s, 1.57002s/100 iter), loss = 0.0293468
I0801 13:19:53.657268 12903 solver.cpp:375]     Train net output #0: loss = 0.0293478 (* 1 = 0.0293478 loss)
I0801 13:19:53.657281 12903 sgd_solver.cpp:136] Iteration 30300, lr = 0.0526563, m = 0.9
I0801 13:19:55.222640 12903 solver.cpp:353] Iteration 30400 (63.8822 iter/s, 1.56538s/100 iter), loss = 0.124695
I0801 13:19:55.222664 12903 solver.cpp:375]     Train net output #0: loss = 0.124696 (* 1 = 0.124696 loss)
I0801 13:19:55.222671 12903 sgd_solver.cpp:136] Iteration 30400, lr = 0.0525, m = 0.9
I0801 13:19:56.792764 12903 solver.cpp:353] Iteration 30500 (63.6913 iter/s, 1.57007s/100 iter), loss = 0.0972575
I0801 13:19:56.792790 12903 solver.cpp:375]     Train net output #0: loss = 0.0972584 (* 1 = 0.0972584 loss)
I0801 13:19:56.792795 12903 sgd_solver.cpp:136] Iteration 30500, lr = 0.0523438, m = 0.9
I0801 13:19:58.362519 12903 solver.cpp:353] Iteration 30600 (63.7061 iter/s, 1.56971s/100 iter), loss = 0.0933293
I0801 13:19:58.362543 12903 solver.cpp:375]     Train net output #0: loss = 0.0933302 (* 1 = 0.0933302 loss)
I0801 13:19:58.362550 12903 sgd_solver.cpp:136] Iteration 30600, lr = 0.0521875, m = 0.9
I0801 13:19:59.947283 12903 solver.cpp:353] Iteration 30700 (63.1029 iter/s, 1.58471s/100 iter), loss = 0.0172982
I0801 13:19:59.947332 12903 solver.cpp:375]     Train net output #0: loss = 0.0172991 (* 1 = 0.0172991 loss)
I0801 13:19:59.947345 12903 sgd_solver.cpp:136] Iteration 30700, lr = 0.0520312, m = 0.9
I0801 13:20:01.516372 12903 solver.cpp:353] Iteration 30800 (63.7333 iter/s, 1.56904s/100 iter), loss = 0.0307401
I0801 13:20:01.516422 12903 solver.cpp:375]     Train net output #0: loss = 0.030741 (* 1 = 0.030741 loss)
I0801 13:20:01.516436 12903 sgd_solver.cpp:136] Iteration 30800, lr = 0.051875, m = 0.9
I0801 13:20:03.079638 12903 solver.cpp:353] Iteration 30900 (63.9706 iter/s, 1.56322s/100 iter), loss = 0.00696703
I0801 13:20:03.079664 12903 solver.cpp:375]     Train net output #0: loss = 0.00696792 (* 1 = 0.00696792 loss)
I0801 13:20:03.079668 12903 sgd_solver.cpp:136] Iteration 30900, lr = 0.0517187, m = 0.9
I0801 13:20:04.635092 12903 solver.cpp:550] Iteration 31000, Testing net (#0)
I0801 13:20:04.919972 12893 data_reader.cpp:264] Starting prefetch of epoch 4
I0801 13:20:05.459306 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.808825
I0801 13:20:05.459326 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.990882
I0801 13:20:05.459331 12903 solver.cpp:635]     Test net output #2: loss = 0.72431 (* 1 = 0.72431 loss)
I0801 13:20:05.459347 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.824233s
I0801 13:20:05.474874 12903 solver.cpp:353] Iteration 31000 (41.7508 iter/s, 2.39517s/100 iter), loss = 0.123113
I0801 13:20:05.474894 12903 solver.cpp:375]     Train net output #0: loss = 0.123114 (* 1 = 0.123114 loss)
I0801 13:20:05.474900 12903 sgd_solver.cpp:136] Iteration 31000, lr = 0.0515625, m = 0.9
I0801 13:20:07.033507 12903 solver.cpp:353] Iteration 31100 (64.1609 iter/s, 1.55858s/100 iter), loss = 0.0327819
I0801 13:20:07.033534 12903 solver.cpp:375]     Train net output #0: loss = 0.0327828 (* 1 = 0.0327828 loss)
I0801 13:20:07.033540 12903 sgd_solver.cpp:136] Iteration 31100, lr = 0.0514063, m = 0.9
I0801 13:20:08.597133 12903 solver.cpp:353] Iteration 31200 (63.9559 iter/s, 1.56358s/100 iter), loss = 0.0352371
I0801 13:20:08.597158 12903 solver.cpp:375]     Train net output #0: loss = 0.0352379 (* 1 = 0.0352379 loss)
I0801 13:20:08.597162 12903 sgd_solver.cpp:136] Iteration 31200, lr = 0.05125, m = 0.9
I0801 13:20:10.186554 12903 solver.cpp:353] Iteration 31300 (62.918 iter/s, 1.58937s/100 iter), loss = 0.0120523
I0801 13:20:10.186581 12903 solver.cpp:375]     Train net output #0: loss = 0.0120531 (* 1 = 0.0120531 loss)
I0801 13:20:10.186588 12903 sgd_solver.cpp:136] Iteration 31300, lr = 0.0510938, m = 0.9
I0801 13:20:11.760176 12903 solver.cpp:353] Iteration 31400 (63.5495 iter/s, 1.57358s/100 iter), loss = 0.122995
I0801 13:20:11.760202 12903 solver.cpp:375]     Train net output #0: loss = 0.122996 (* 1 = 0.122996 loss)
I0801 13:20:11.760208 12903 sgd_solver.cpp:136] Iteration 31400, lr = 0.0509375, m = 0.9
I0801 13:20:13.330746 12903 solver.cpp:353] Iteration 31500 (63.6731 iter/s, 1.57052s/100 iter), loss = 0.0744548
I0801 13:20:13.330772 12903 solver.cpp:375]     Train net output #0: loss = 0.0744555 (* 1 = 0.0744555 loss)
I0801 13:20:13.330778 12903 sgd_solver.cpp:136] Iteration 31500, lr = 0.0507812, m = 0.9
I0801 13:20:14.903846 12903 solver.cpp:353] Iteration 31600 (63.5708 iter/s, 1.57305s/100 iter), loss = 0.144658
I0801 13:20:14.903872 12903 solver.cpp:375]     Train net output #0: loss = 0.144658 (* 1 = 0.144658 loss)
I0801 13:20:14.903878 12903 sgd_solver.cpp:136] Iteration 31600, lr = 0.050625, m = 0.9
I0801 13:20:16.484014 12903 solver.cpp:353] Iteration 31700 (63.2864 iter/s, 1.58012s/100 iter), loss = 0.0161382
I0801 13:20:16.484099 12903 solver.cpp:375]     Train net output #0: loss = 0.0161389 (* 1 = 0.0161389 loss)
I0801 13:20:16.484107 12903 sgd_solver.cpp:136] Iteration 31700, lr = 0.0504688, m = 0.9
I0801 13:20:18.054582 12903 solver.cpp:353] Iteration 31800 (63.673 iter/s, 1.57052s/100 iter), loss = 0.0319641
I0801 13:20:18.054630 12903 solver.cpp:375]     Train net output #0: loss = 0.0319648 (* 1 = 0.0319648 loss)
I0801 13:20:18.054643 12903 sgd_solver.cpp:136] Iteration 31800, lr = 0.0503125, m = 0.9
I0801 13:20:19.630633 12903 solver.cpp:353] Iteration 31900 (63.4518 iter/s, 1.576s/100 iter), loss = 0.160699
I0801 13:20:19.630659 12903 solver.cpp:375]     Train net output #0: loss = 0.160699 (* 1 = 0.160699 loss)
I0801 13:20:19.630666 12903 sgd_solver.cpp:136] Iteration 31900, lr = 0.0501562, m = 0.9
I0801 13:20:21.188724 12903 solver.cpp:550] Iteration 32000, Testing net (#0)
I0801 13:20:22.004017 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.842648
I0801 13:20:22.004041 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.987941
I0801 13:20:22.004047 12903 solver.cpp:635]     Test net output #2: loss = 0.627861 (* 1 = 0.627861 loss)
I0801 13:20:22.004062 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.815318s
I0801 13:20:22.021642 12903 solver.cpp:353] Iteration 32000 (41.8246 iter/s, 2.39094s/100 iter), loss = 0.141767
I0801 13:20:22.021663 12903 solver.cpp:375]     Train net output #0: loss = 0.141767 (* 1 = 0.141767 loss)
I0801 13:20:22.021667 12903 sgd_solver.cpp:136] Iteration 32000, lr = 0.05, m = 0.9
I0801 13:20:23.589875 12903 solver.cpp:353] Iteration 32100 (63.7681 iter/s, 1.56818s/100 iter), loss = 0.0670104
I0801 13:20:23.589901 12903 solver.cpp:375]     Train net output #0: loss = 0.0670111 (* 1 = 0.0670111 loss)
I0801 13:20:23.589907 12903 sgd_solver.cpp:136] Iteration 32100, lr = 0.0498438, m = 0.9
I0801 13:20:25.155197 12903 solver.cpp:353] Iteration 32200 (63.8866 iter/s, 1.56527s/100 iter), loss = 0.103009
I0801 13:20:25.155264 12903 solver.cpp:375]     Train net output #0: loss = 0.10301 (* 1 = 0.10301 loss)
I0801 13:20:25.155282 12903 sgd_solver.cpp:136] Iteration 32200, lr = 0.0496875, m = 0.9
I0801 13:20:26.748344 12903 solver.cpp:353] Iteration 32300 (62.7708 iter/s, 1.5931s/100 iter), loss = 0.0432137
I0801 13:20:26.748370 12903 solver.cpp:375]     Train net output #0: loss = 0.0432144 (* 1 = 0.0432144 loss)
I0801 13:20:26.748376 12903 sgd_solver.cpp:136] Iteration 32300, lr = 0.0495313, m = 0.9
I0801 13:20:28.314968 12903 solver.cpp:353] Iteration 32400 (63.8335 iter/s, 1.56658s/100 iter), loss = 0.0215235
I0801 13:20:28.315028 12903 solver.cpp:375]     Train net output #0: loss = 0.0215242 (* 1 = 0.0215242 loss)
I0801 13:20:28.315035 12903 sgd_solver.cpp:136] Iteration 32400, lr = 0.049375, m = 0.9
I0801 13:20:29.881680 12903 solver.cpp:353] Iteration 32500 (63.8299 iter/s, 1.56666s/100 iter), loss = 0.0548085
I0801 13:20:29.881705 12903 solver.cpp:375]     Train net output #0: loss = 0.0548092 (* 1 = 0.0548092 loss)
I0801 13:20:29.881709 12903 sgd_solver.cpp:136] Iteration 32500, lr = 0.0492188, m = 0.9
I0801 13:20:31.458968 12903 solver.cpp:353] Iteration 32600 (63.402 iter/s, 1.57724s/100 iter), loss = 0.141973
I0801 13:20:31.458993 12903 solver.cpp:375]     Train net output #0: loss = 0.141974 (* 1 = 0.141974 loss)
I0801 13:20:31.458999 12903 sgd_solver.cpp:136] Iteration 32600, lr = 0.0490625, m = 0.9
I0801 13:20:33.024302 12903 solver.cpp:353] Iteration 32700 (63.8862 iter/s, 1.56528s/100 iter), loss = 0.00710496
I0801 13:20:33.024332 12903 solver.cpp:375]     Train net output #0: loss = 0.00710569 (* 1 = 0.00710569 loss)
I0801 13:20:33.024338 12903 sgd_solver.cpp:136] Iteration 32700, lr = 0.0489062, m = 0.9
I0801 13:20:34.599494 12903 solver.cpp:353] Iteration 32800 (63.4863 iter/s, 1.57514s/100 iter), loss = 0.2191
I0801 13:20:34.599522 12903 solver.cpp:375]     Train net output #0: loss = 0.219101 (* 1 = 0.219101 loss)
I0801 13:20:34.599529 12903 sgd_solver.cpp:136] Iteration 32800, lr = 0.04875, m = 0.9
I0801 13:20:36.169252 12903 solver.cpp:353] Iteration 32900 (63.706 iter/s, 1.56971s/100 iter), loss = 0.131595
I0801 13:20:36.169296 12903 solver.cpp:375]     Train net output #0: loss = 0.131596 (* 1 = 0.131596 loss)
I0801 13:20:36.169303 12903 sgd_solver.cpp:136] Iteration 32900, lr = 0.0485937, m = 0.9
I0801 13:20:37.733317 12903 solver.cpp:550] Iteration 33000, Testing net (#0)
I0801 13:20:38.563258 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.845589
I0801 13:20:38.563279 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.991177
I0801 13:20:38.563284 12903 solver.cpp:635]     Test net output #2: loss = 0.648838 (* 1 = 0.648838 loss)
I0801 13:20:38.563300 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.82996s
I0801 13:20:38.578841 12903 solver.cpp:353] Iteration 33000 (41.502 iter/s, 2.40952s/100 iter), loss = 0.0207509
I0801 13:20:38.578876 12903 solver.cpp:375]     Train net output #0: loss = 0.0207516 (* 1 = 0.0207516 loss)
I0801 13:20:38.578891 12903 sgd_solver.cpp:136] Iteration 33000, lr = 0.0484375, m = 0.9
I0801 13:20:40.173532 12903 solver.cpp:353] Iteration 33100 (62.7101 iter/s, 1.59464s/100 iter), loss = 0.0663823
I0801 13:20:40.173583 12903 solver.cpp:375]     Train net output #0: loss = 0.066383 (* 1 = 0.066383 loss)
I0801 13:20:40.173595 12903 sgd_solver.cpp:136] Iteration 33100, lr = 0.0482813, m = 0.9
I0801 13:20:41.750718 12903 solver.cpp:353] Iteration 33200 (63.4061 iter/s, 1.57714s/100 iter), loss = 0.00263963
I0801 13:20:41.750746 12903 solver.cpp:375]     Train net output #0: loss = 0.00264033 (* 1 = 0.00264033 loss)
I0801 13:20:41.750752 12903 sgd_solver.cpp:136] Iteration 33200, lr = 0.048125, m = 0.9
I0801 13:20:43.322434 12903 solver.cpp:353] Iteration 33300 (63.6266 iter/s, 1.57167s/100 iter), loss = 0.0533619
I0801 13:20:43.322484 12903 solver.cpp:375]     Train net output #0: loss = 0.0533626 (* 1 = 0.0533626 loss)
I0801 13:20:43.322497 12903 sgd_solver.cpp:136] Iteration 33300, lr = 0.0479688, m = 0.9
I0801 13:20:44.906105 12903 solver.cpp:353] Iteration 33400 (63.1464 iter/s, 1.58362s/100 iter), loss = 0.0122192
I0801 13:20:44.906157 12903 solver.cpp:375]     Train net output #0: loss = 0.0122198 (* 1 = 0.0122198 loss)
I0801 13:20:44.906170 12903 sgd_solver.cpp:136] Iteration 33400, lr = 0.0478125, m = 0.9
I0801 13:20:46.475555 12903 solver.cpp:353] Iteration 33500 (63.7187 iter/s, 1.5694s/100 iter), loss = 0.0520932
I0801 13:20:46.475580 12903 solver.cpp:375]     Train net output #0: loss = 0.0520939 (* 1 = 0.0520939 loss)
I0801 13:20:46.475587 12903 sgd_solver.cpp:136] Iteration 33500, lr = 0.0476562, m = 0.9
I0801 13:20:48.040910 12903 solver.cpp:353] Iteration 33600 (63.8852 iter/s, 1.56531s/100 iter), loss = 0.114354
I0801 13:20:48.040997 12903 solver.cpp:375]     Train net output #0: loss = 0.114355 (* 1 = 0.114355 loss)
I0801 13:20:48.041005 12903 sgd_solver.cpp:136] Iteration 33600, lr = 0.0475, m = 0.9
I0801 13:20:49.619446 12903 solver.cpp:353] Iteration 33700 (63.3518 iter/s, 1.57849s/100 iter), loss = 0.0320136
I0801 13:20:49.619472 12903 solver.cpp:375]     Train net output #0: loss = 0.0320143 (* 1 = 0.0320143 loss)
I0801 13:20:49.619477 12903 sgd_solver.cpp:136] Iteration 33700, lr = 0.0473437, m = 0.9
I0801 13:20:51.187954 12903 solver.cpp:353] Iteration 33800 (63.7568 iter/s, 1.56846s/100 iter), loss = 0.00604984
I0801 13:20:51.187978 12903 solver.cpp:375]     Train net output #0: loss = 0.00605062 (* 1 = 0.00605062 loss)
I0801 13:20:51.187983 12903 sgd_solver.cpp:136] Iteration 33800, lr = 0.0471875, m = 0.9
I0801 13:20:52.757736 12903 solver.cpp:353] Iteration 33900 (63.7052 iter/s, 1.56973s/100 iter), loss = 0.0420881
I0801 13:20:52.757761 12903 solver.cpp:375]     Train net output #0: loss = 0.0420888 (* 1 = 0.0420888 loss)
I0801 13:20:52.757767 12903 sgd_solver.cpp:136] Iteration 33900, lr = 0.0470312, m = 0.9
I0801 13:20:54.307013 12903 solver.cpp:550] Iteration 34000, Testing net (#0)
I0801 13:20:55.124490 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.864413
I0801 13:20:55.124511 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.993529
I0801 13:20:55.124516 12903 solver.cpp:635]     Test net output #2: loss = 0.50605 (* 1 = 0.50605 loss)
I0801 13:20:55.124531 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.817495s
I0801 13:20:55.140169 12903 solver.cpp:353] Iteration 34000 (41.9751 iter/s, 2.38236s/100 iter), loss = 0.0788738
I0801 13:20:55.140188 12903 solver.cpp:375]     Train net output #0: loss = 0.0788746 (* 1 = 0.0788746 loss)
I0801 13:20:55.140194 12903 sgd_solver.cpp:136] Iteration 34000, lr = 0.046875, m = 0.9
I0801 13:20:56.720142 12903 solver.cpp:353] Iteration 34100 (63.2943 iter/s, 1.57992s/100 iter), loss = 0.0111962
I0801 13:20:56.720167 12903 solver.cpp:375]     Train net output #0: loss = 0.011197 (* 1 = 0.011197 loss)
I0801 13:20:56.720172 12903 sgd_solver.cpp:136] Iteration 34100, lr = 0.0467188, m = 0.9
I0801 13:20:58.288722 12903 solver.cpp:353] Iteration 34200 (63.7539 iter/s, 1.56853s/100 iter), loss = 0.0603026
I0801 13:20:58.288748 12903 solver.cpp:375]     Train net output #0: loss = 0.0603034 (* 1 = 0.0603034 loss)
I0801 13:20:58.288754 12903 sgd_solver.cpp:136] Iteration 34200, lr = 0.0465625, m = 0.9
I0801 13:20:59.862128 12903 solver.cpp:353] Iteration 34300 (63.5584 iter/s, 1.57336s/100 iter), loss = 0.00766906
I0801 13:20:59.862155 12903 solver.cpp:375]     Train net output #0: loss = 0.00766982 (* 1 = 0.00766982 loss)
I0801 13:20:59.862161 12903 sgd_solver.cpp:136] Iteration 34300, lr = 0.0464063, m = 0.9
I0801 13:21:01.422943 12903 solver.cpp:353] Iteration 34400 (64.0712 iter/s, 1.56076s/100 iter), loss = 0.0704771
I0801 13:21:01.422971 12903 solver.cpp:375]     Train net output #0: loss = 0.0704779 (* 1 = 0.0704779 loss)
I0801 13:21:01.422977 12903 sgd_solver.cpp:136] Iteration 34400, lr = 0.04625, m = 0.9
I0801 13:21:02.978734 12903 solver.cpp:353] Iteration 34500 (64.278 iter/s, 1.55574s/100 iter), loss = 0.000861189
I0801 13:21:02.978759 12903 solver.cpp:375]     Train net output #0: loss = 0.000861965 (* 1 = 0.000861965 loss)
I0801 13:21:02.978765 12903 sgd_solver.cpp:136] Iteration 34500, lr = 0.0460938, m = 0.9
I0801 13:21:04.536851 12903 solver.cpp:353] Iteration 34600 (64.182 iter/s, 1.55807s/100 iter), loss = 0.0459457
I0801 13:21:04.536876 12903 solver.cpp:375]     Train net output #0: loss = 0.0459465 (* 1 = 0.0459465 loss)
I0801 13:21:04.536882 12903 sgd_solver.cpp:136] Iteration 34600, lr = 0.0459375, m = 0.9
I0801 13:21:06.111032 12903 solver.cpp:353] Iteration 34700 (63.5271 iter/s, 1.57413s/100 iter), loss = 0.0359318
I0801 13:21:06.111086 12903 solver.cpp:375]     Train net output #0: loss = 0.0359325 (* 1 = 0.0359325 loss)
I0801 13:21:06.111100 12903 sgd_solver.cpp:136] Iteration 34700, lr = 0.0457813, m = 0.9
I0801 13:21:07.684150 12903 solver.cpp:353] Iteration 34800 (63.5701 iter/s, 1.57307s/100 iter), loss = 0.00243393
I0801 13:21:07.684216 12903 solver.cpp:375]     Train net output #0: loss = 0.00243463 (* 1 = 0.00243463 loss)
I0801 13:21:07.684229 12903 sgd_solver.cpp:136] Iteration 34800, lr = 0.045625, m = 0.9
I0801 13:21:09.245198 12903 solver.cpp:353] Iteration 34900 (64.0616 iter/s, 1.561s/100 iter), loss = 0.100616
I0801 13:21:09.245223 12903 solver.cpp:375]     Train net output #0: loss = 0.100617 (* 1 = 0.100617 loss)
I0801 13:21:09.245229 12903 sgd_solver.cpp:136] Iteration 34900, lr = 0.0454687, m = 0.9
I0801 13:21:10.787868 12903 solver.cpp:550] Iteration 35000, Testing net (#0)
I0801 13:21:11.010586 12893 data_reader.cpp:264] Starting prefetch of epoch 5
I0801 13:21:11.604420 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.824413
I0801 13:21:11.604444 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.986471
I0801 13:21:11.604454 12903 solver.cpp:635]     Test net output #2: loss = 0.700598 (* 1 = 0.700598 loss)
I0801 13:21:11.604478 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.816586s
I0801 13:21:11.624210 12903 solver.cpp:353] Iteration 35000 (42.0355 iter/s, 2.37894s/100 iter), loss = 0.084809
I0801 13:21:11.624236 12903 solver.cpp:375]     Train net output #0: loss = 0.0848097 (* 1 = 0.0848097 loss)
I0801 13:21:11.624243 12903 sgd_solver.cpp:136] Iteration 35000, lr = 0.0453125, m = 0.9
I0801 13:21:13.184963 12903 solver.cpp:353] Iteration 35100 (64.0739 iter/s, 1.5607s/100 iter), loss = 0.0110816
I0801 13:21:13.185019 12903 solver.cpp:375]     Train net output #0: loss = 0.0110823 (* 1 = 0.0110823 loss)
I0801 13:21:13.185034 12903 sgd_solver.cpp:136] Iteration 35100, lr = 0.0451563, m = 0.9
I0801 13:21:14.768916 12903 solver.cpp:353] Iteration 35200 (63.1352 iter/s, 1.5839s/100 iter), loss = 0.145628
I0801 13:21:14.768968 12903 solver.cpp:375]     Train net output #0: loss = 0.145629 (* 1 = 0.145629 loss)
I0801 13:21:14.768982 12903 sgd_solver.cpp:136] Iteration 35200, lr = 0.045, m = 0.9
I0801 13:21:16.345621 12903 solver.cpp:353] Iteration 35300 (63.4253 iter/s, 1.57666s/100 iter), loss = 0.0221993
I0801 13:21:16.345671 12903 solver.cpp:375]     Train net output #0: loss = 0.0222 (* 1 = 0.0222 loss)
I0801 13:21:16.345679 12903 sgd_solver.cpp:136] Iteration 35300, lr = 0.0448438, m = 0.9
I0801 13:21:17.919375 12903 solver.cpp:353] Iteration 35400 (63.5444 iter/s, 1.5737s/100 iter), loss = 0.0069335
I0801 13:21:17.919401 12903 solver.cpp:375]     Train net output #0: loss = 0.0069342 (* 1 = 0.0069342 loss)
I0801 13:21:17.919406 12903 sgd_solver.cpp:136] Iteration 35400, lr = 0.0446875, m = 0.9
I0801 13:21:19.490217 12903 solver.cpp:353] Iteration 35500 (63.6621 iter/s, 1.57079s/100 iter), loss = 0.0534759
I0801 13:21:19.490308 12903 solver.cpp:375]     Train net output #0: loss = 0.0534765 (* 1 = 0.0534765 loss)
I0801 13:21:19.490315 12903 sgd_solver.cpp:136] Iteration 35500, lr = 0.0445313, m = 0.9
I0801 13:21:21.066292 12903 solver.cpp:353] Iteration 35600 (63.4508 iter/s, 1.57602s/100 iter), loss = 0.167116
I0801 13:21:21.066318 12903 solver.cpp:375]     Train net output #0: loss = 0.167117 (* 1 = 0.167117 loss)
I0801 13:21:21.066323 12903 sgd_solver.cpp:136] Iteration 35600, lr = 0.044375, m = 0.9
I0801 13:21:22.637814 12903 solver.cpp:353] Iteration 35700 (63.6346 iter/s, 1.57147s/100 iter), loss = 0.0044398
I0801 13:21:22.637841 12903 solver.cpp:375]     Train net output #0: loss = 0.00444054 (* 1 = 0.00444054 loss)
I0801 13:21:22.637847 12903 sgd_solver.cpp:136] Iteration 35700, lr = 0.0442187, m = 0.9
I0801 13:21:24.193387 12903 solver.cpp:353] Iteration 35800 (64.287 iter/s, 1.55552s/100 iter), loss = 0.183123
I0801 13:21:24.193413 12903 solver.cpp:375]     Train net output #0: loss = 0.183124 (* 1 = 0.183124 loss)
I0801 13:21:24.193418 12903 sgd_solver.cpp:136] Iteration 35800, lr = 0.0440625, m = 0.9
I0801 13:21:25.755043 12903 solver.cpp:353] Iteration 35900 (64.0367 iter/s, 1.56161s/100 iter), loss = 0.00455636
I0801 13:21:25.755069 12903 solver.cpp:375]     Train net output #0: loss = 0.00455704 (* 1 = 0.00455704 loss)
I0801 13:21:25.755075 12903 sgd_solver.cpp:136] Iteration 35900, lr = 0.0439062, m = 0.9
I0801 13:21:27.327338 12903 solver.cpp:550] Iteration 36000, Testing net (#0)
I0801 13:21:28.143462 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.850001
I0801 13:21:28.143481 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.990588
I0801 13:21:28.143486 12903 solver.cpp:635]     Test net output #2: loss = 0.556262 (* 1 = 0.556262 loss)
I0801 13:21:28.143501 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.81614s
I0801 13:21:28.159039 12903 solver.cpp:353] Iteration 36000 (41.5987 iter/s, 2.40392s/100 iter), loss = 0.00880365
I0801 13:21:28.159073 12903 solver.cpp:375]     Train net output #0: loss = 0.00880438 (* 1 = 0.00880438 loss)
I0801 13:21:28.159086 12903 sgd_solver.cpp:136] Iteration 36000, lr = 0.04375, m = 0.9
I0801 13:21:29.736711 12903 solver.cpp:353] Iteration 36100 (63.3866 iter/s, 1.57762s/100 iter), loss = 0.0686506
I0801 13:21:29.736763 12903 solver.cpp:375]     Train net output #0: loss = 0.0686513 (* 1 = 0.0686513 loss)
I0801 13:21:29.736775 12903 sgd_solver.cpp:136] Iteration 36100, lr = 0.0435938, m = 0.9
I0801 13:21:31.309363 12903 solver.cpp:353] Iteration 36200 (63.589 iter/s, 1.5726s/100 iter), loss = 0.010064
I0801 13:21:31.309392 12903 solver.cpp:375]     Train net output #0: loss = 0.0100648 (* 1 = 0.0100648 loss)
I0801 13:21:31.309399 12903 sgd_solver.cpp:136] Iteration 36200, lr = 0.0434375, m = 0.9
I0801 13:21:32.883000 12903 solver.cpp:353] Iteration 36300 (63.549 iter/s, 1.57359s/100 iter), loss = 0.0260525
I0801 13:21:32.883026 12903 solver.cpp:375]     Train net output #0: loss = 0.0260533 (* 1 = 0.0260533 loss)
I0801 13:21:32.883030 12903 sgd_solver.cpp:136] Iteration 36300, lr = 0.0432813, m = 0.9
I0801 13:21:34.456220 12903 solver.cpp:353] Iteration 36400 (63.5659 iter/s, 1.57317s/100 iter), loss = 0.00627342
I0801 13:21:34.456244 12903 solver.cpp:375]     Train net output #0: loss = 0.00627415 (* 1 = 0.00627415 loss)
I0801 13:21:34.456248 12903 sgd_solver.cpp:136] Iteration 36400, lr = 0.043125, m = 0.9
I0801 13:21:36.021198 12903 solver.cpp:353] Iteration 36500 (63.9007 iter/s, 1.56493s/100 iter), loss = 0.0158319
I0801 13:21:36.021224 12903 solver.cpp:375]     Train net output #0: loss = 0.0158326 (* 1 = 0.0158326 loss)
I0801 13:21:36.021229 12903 sgd_solver.cpp:136] Iteration 36500, lr = 0.0429688, m = 0.9
I0801 13:21:37.585971 12903 solver.cpp:353] Iteration 36600 (63.909 iter/s, 1.56472s/100 iter), loss = 0.0196456
I0801 13:21:37.585997 12903 solver.cpp:375]     Train net output #0: loss = 0.0196463 (* 1 = 0.0196463 loss)
I0801 13:21:37.586004 12903 sgd_solver.cpp:136] Iteration 36600, lr = 0.0428125, m = 0.9
I0801 13:21:39.153141 12903 solver.cpp:353] Iteration 36700 (63.8114 iter/s, 1.56712s/100 iter), loss = 0.0356679
I0801 13:21:39.153185 12903 solver.cpp:375]     Train net output #0: loss = 0.0356686 (* 1 = 0.0356686 loss)
I0801 13:21:39.153192 12903 sgd_solver.cpp:136] Iteration 36700, lr = 0.0426563, m = 0.9
I0801 13:21:40.729609 12903 solver.cpp:353] Iteration 36800 (63.4349 iter/s, 1.57642s/100 iter), loss = 0.0022766
I0801 13:21:40.729670 12903 solver.cpp:375]     Train net output #0: loss = 0.00227732 (* 1 = 0.00227732 loss)
I0801 13:21:40.729688 12903 sgd_solver.cpp:136] Iteration 36800, lr = 0.0425, m = 0.9
I0801 13:21:42.294559 12903 solver.cpp:353] Iteration 36900 (63.9019 iter/s, 1.5649s/100 iter), loss = 0.0258758
I0801 13:21:42.294589 12903 solver.cpp:375]     Train net output #0: loss = 0.0258765 (* 1 = 0.0258765 loss)
I0801 13:21:42.294595 12903 sgd_solver.cpp:136] Iteration 36900, lr = 0.0423437, m = 0.9
I0801 13:21:43.840965 12903 solver.cpp:550] Iteration 37000, Testing net (#0)
I0801 13:21:44.658710 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.810001
I0801 13:21:44.658727 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.991471
I0801 13:21:44.658732 12903 solver.cpp:635]     Test net output #2: loss = 0.832782 (* 1 = 0.832782 loss)
I0801 13:21:44.658748 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.817761s
I0801 13:21:44.674500 12903 solver.cpp:353] Iteration 37000 (42.0191 iter/s, 2.37987s/100 iter), loss = 0.00558834
I0801 13:21:44.674520 12903 solver.cpp:375]     Train net output #0: loss = 0.00558905 (* 1 = 0.00558905 loss)
I0801 13:21:44.674525 12903 sgd_solver.cpp:136] Iteration 37000, lr = 0.0421875, m = 0.9
I0801 13:21:46.266409 12903 solver.cpp:353] Iteration 37100 (62.8198 iter/s, 1.59186s/100 iter), loss = 0.00339739
I0801 13:21:46.266463 12903 solver.cpp:375]     Train net output #0: loss = 0.00339808 (* 1 = 0.00339808 loss)
I0801 13:21:46.266471 12903 sgd_solver.cpp:136] Iteration 37100, lr = 0.0420313, m = 0.9
I0801 13:21:47.839165 12903 solver.cpp:353] Iteration 37200 (63.5845 iter/s, 1.57271s/100 iter), loss = 0.012429
I0801 13:21:47.839190 12903 solver.cpp:375]     Train net output #0: loss = 0.0124297 (* 1 = 0.0124297 loss)
I0801 13:21:47.839196 12903 sgd_solver.cpp:136] Iteration 37200, lr = 0.041875, m = 0.9
I0801 13:21:49.413099 12903 solver.cpp:353] Iteration 37300 (63.5372 iter/s, 1.57388s/100 iter), loss = 0.0185539
I0801 13:21:49.413122 12903 solver.cpp:375]     Train net output #0: loss = 0.0185545 (* 1 = 0.0185545 loss)
I0801 13:21:49.413126 12903 sgd_solver.cpp:136] Iteration 37300, lr = 0.0417188, m = 0.9
I0801 13:21:50.979746 12903 solver.cpp:353] Iteration 37400 (63.8325 iter/s, 1.5666s/100 iter), loss = 0.0139212
I0801 13:21:50.979845 12903 solver.cpp:375]     Train net output #0: loss = 0.0139218 (* 1 = 0.0139218 loss)
I0801 13:21:50.979857 12903 sgd_solver.cpp:136] Iteration 37400, lr = 0.0415625, m = 0.9
I0801 13:21:52.541896 12903 solver.cpp:353] Iteration 37500 (64.0164 iter/s, 1.5621s/100 iter), loss = 0.0437395
I0801 13:21:52.541923 12903 solver.cpp:375]     Train net output #0: loss = 0.0437402 (* 1 = 0.0437402 loss)
I0801 13:21:52.541929 12903 sgd_solver.cpp:136] Iteration 37500, lr = 0.0414063, m = 0.9
I0801 13:21:54.121470 12903 solver.cpp:353] Iteration 37600 (63.3103 iter/s, 1.57952s/100 iter), loss = 0.0486163
I0801 13:21:54.121496 12903 solver.cpp:375]     Train net output #0: loss = 0.048617 (* 1 = 0.048617 loss)
I0801 13:21:54.121502 12903 sgd_solver.cpp:136] Iteration 37600, lr = 0.04125, m = 0.9
I0801 13:21:55.690500 12903 solver.cpp:353] Iteration 37700 (63.7356 iter/s, 1.56898s/100 iter), loss = 0.0429001
I0801 13:21:55.690546 12903 solver.cpp:375]     Train net output #0: loss = 0.0429007 (* 1 = 0.0429007 loss)
I0801 13:21:55.690559 12903 sgd_solver.cpp:136] Iteration 37700, lr = 0.0410937, m = 0.9
I0801 13:21:57.274880 12903 solver.cpp:353] Iteration 37800 (63.1182 iter/s, 1.58433s/100 iter), loss = 0.0267743
I0801 13:21:57.274943 12903 solver.cpp:375]     Train net output #0: loss = 0.0267749 (* 1 = 0.0267749 loss)
I0801 13:21:57.274961 12903 sgd_solver.cpp:136] Iteration 37800, lr = 0.0409375, m = 0.9
I0801 13:21:58.852851 12903 solver.cpp:353] Iteration 37900 (63.3745 iter/s, 1.57792s/100 iter), loss = 0.00860051
I0801 13:21:58.852896 12903 solver.cpp:375]     Train net output #0: loss = 0.00860116 (* 1 = 0.00860116 loss)
I0801 13:21:58.852910 12903 sgd_solver.cpp:136] Iteration 37900, lr = 0.0407812, m = 0.9
I0801 13:22:00.400579 12903 solver.cpp:550] Iteration 38000, Testing net (#0)
I0801 13:22:01.229962 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.853237
I0801 13:22:01.229984 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.994706
I0801 13:22:01.229990 12903 solver.cpp:635]     Test net output #2: loss = 0.560239 (* 1 = 0.560239 loss)
I0801 13:22:01.230007 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.829403s
I0801 13:22:01.245518 12903 solver.cpp:353] Iteration 38000 (41.7956 iter/s, 2.3926s/100 iter), loss = 0.02575
I0801 13:22:01.245551 12903 solver.cpp:375]     Train net output #0: loss = 0.0257506 (* 1 = 0.0257506 loss)
I0801 13:22:01.245565 12903 sgd_solver.cpp:136] Iteration 38000, lr = 0.040625, m = 0.9
I0801 13:22:02.805614 12903 solver.cpp:353] Iteration 38100 (64.1007 iter/s, 1.56005s/100 iter), loss = 0.00618284
I0801 13:22:02.805640 12903 solver.cpp:375]     Train net output #0: loss = 0.00618346 (* 1 = 0.00618346 loss)
I0801 13:22:02.805646 12903 sgd_solver.cpp:136] Iteration 38100, lr = 0.0404688, m = 0.9
I0801 13:22:04.388962 12903 solver.cpp:353] Iteration 38200 (63.1594 iter/s, 1.58329s/100 iter), loss = 0.00480429
I0801 13:22:04.389015 12903 solver.cpp:375]     Train net output #0: loss = 0.00480495 (* 1 = 0.00480495 loss)
I0801 13:22:04.389024 12903 sgd_solver.cpp:136] Iteration 38200, lr = 0.0403125, m = 0.9
I0801 13:22:05.972122 12903 solver.cpp:353] Iteration 38300 (63.1668 iter/s, 1.58311s/100 iter), loss = 0.00375757
I0801 13:22:05.972149 12903 solver.cpp:375]     Train net output #0: loss = 0.00375821 (* 1 = 0.00375821 loss)
I0801 13:22:05.972156 12903 sgd_solver.cpp:136] Iteration 38300, lr = 0.0401563, m = 0.9
I0801 13:22:07.556260 12903 solver.cpp:353] Iteration 38400 (63.1279 iter/s, 1.58409s/100 iter), loss = 0.0140965
I0801 13:22:07.556289 12903 solver.cpp:375]     Train net output #0: loss = 0.0140971 (* 1 = 0.0140971 loss)
I0801 13:22:07.556298 12903 sgd_solver.cpp:136] Iteration 38400, lr = 0.04, m = 0.9
I0801 13:22:09.123580 12903 solver.cpp:353] Iteration 38500 (63.8051 iter/s, 1.56727s/100 iter), loss = 0.044271
I0801 13:22:09.123607 12903 solver.cpp:375]     Train net output #0: loss = 0.0442716 (* 1 = 0.0442716 loss)
I0801 13:22:09.123613 12903 sgd_solver.cpp:136] Iteration 38500, lr = 0.0398437, m = 0.9
I0801 13:22:10.715175 12903 solver.cpp:353] Iteration 38600 (62.832 iter/s, 1.59154s/100 iter), loss = 0.00170557
I0801 13:22:10.715224 12903 solver.cpp:375]     Train net output #0: loss = 0.00170623 (* 1 = 0.00170623 loss)
I0801 13:22:10.715230 12903 sgd_solver.cpp:136] Iteration 38600, lr = 0.0396875, m = 0.9
I0801 13:22:12.289366 12903 solver.cpp:353] Iteration 38700 (63.5267 iter/s, 1.57414s/100 iter), loss = 0.0383477
I0801 13:22:12.289392 12903 solver.cpp:375]     Train net output #0: loss = 0.0383483 (* 1 = 0.0383483 loss)
I0801 13:22:12.289398 12903 sgd_solver.cpp:136] Iteration 38700, lr = 0.0395312, m = 0.9
I0801 13:22:13.851032 12903 solver.cpp:353] Iteration 38800 (64.0363 iter/s, 1.56162s/100 iter), loss = 0.0363005
I0801 13:22:13.851058 12903 solver.cpp:375]     Train net output #0: loss = 0.0363011 (* 1 = 0.0363011 loss)
I0801 13:22:13.851063 12903 sgd_solver.cpp:136] Iteration 38800, lr = 0.039375, m = 0.9
I0801 13:22:15.424202 12903 solver.cpp:353] Iteration 38900 (63.5679 iter/s, 1.57312s/100 iter), loss = 0.0199156
I0801 13:22:15.424227 12903 solver.cpp:375]     Train net output #0: loss = 0.0199162 (* 1 = 0.0199162 loss)
I0801 13:22:15.424233 12903 sgd_solver.cpp:136] Iteration 38900, lr = 0.0392187, m = 0.9
I0801 13:22:16.977782 12903 solver.cpp:550] Iteration 39000, Testing net (#0)
I0801 13:22:17.794718 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.866472
I0801 13:22:17.794737 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.991471
I0801 13:22:17.794742 12903 solver.cpp:635]     Test net output #2: loss = 0.540518 (* 1 = 0.540518 loss)
I0801 13:22:17.794756 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.816951s
I0801 13:22:17.810317 12903 solver.cpp:353] Iteration 39000 (41.9104 iter/s, 2.38605s/100 iter), loss = 0.00339754
I0801 13:22:17.810333 12903 solver.cpp:375]     Train net output #0: loss = 0.0033982 (* 1 = 0.0033982 loss)
I0801 13:22:17.810338 12903 sgd_solver.cpp:136] Iteration 39000, lr = 0.0390625, m = 0.9
I0801 13:22:19.380826 12903 solver.cpp:353] Iteration 39100 (63.6758 iter/s, 1.57046s/100 iter), loss = 0.00743481
I0801 13:22:19.380854 12903 solver.cpp:375]     Train net output #0: loss = 0.00743545 (* 1 = 0.00743545 loss)
I0801 13:22:19.380862 12903 sgd_solver.cpp:136] Iteration 39100, lr = 0.0389063, m = 0.9
I0801 13:22:20.957968 12903 solver.cpp:353] Iteration 39200 (63.4078 iter/s, 1.57709s/100 iter), loss = 0.00468189
I0801 13:22:20.957993 12903 solver.cpp:375]     Train net output #0: loss = 0.00468255 (* 1 = 0.00468255 loss)
I0801 13:22:20.957998 12903 sgd_solver.cpp:136] Iteration 39200, lr = 0.03875, m = 0.9
I0801 13:22:22.529660 12903 solver.cpp:353] Iteration 39300 (63.6277 iter/s, 1.57164s/100 iter), loss = 0.0260674
I0801 13:22:22.529747 12903 solver.cpp:375]     Train net output #0: loss = 0.026068 (* 1 = 0.026068 loss)
I0801 13:22:22.529755 12903 sgd_solver.cpp:136] Iteration 39300, lr = 0.0385938, m = 0.9
I0801 13:22:23.943529 12870 data_reader.cpp:264] Starting prefetch of epoch 5
I0801 13:22:24.097896 12903 solver.cpp:353] Iteration 39400 (63.7679 iter/s, 1.56819s/100 iter), loss = 0.0311039
I0801 13:22:24.097921 12903 solver.cpp:375]     Train net output #0: loss = 0.0311046 (* 1 = 0.0311046 loss)
I0801 13:22:24.097928 12903 sgd_solver.cpp:136] Iteration 39400, lr = 0.0384375, m = 0.9
I0801 13:22:25.673393 12903 solver.cpp:353] Iteration 39500 (63.474 iter/s, 1.57545s/100 iter), loss = 0.00666544
I0801 13:22:25.673420 12903 solver.cpp:375]     Train net output #0: loss = 0.00666608 (* 1 = 0.00666608 loss)
I0801 13:22:25.673426 12903 sgd_solver.cpp:136] Iteration 39500, lr = 0.0382813, m = 0.9
I0801 13:22:27.244699 12903 solver.cpp:353] Iteration 39600 (63.6435 iter/s, 1.57125s/100 iter), loss = 0.0184722
I0801 13:22:27.244724 12903 solver.cpp:375]     Train net output #0: loss = 0.0184729 (* 1 = 0.0184729 loss)
I0801 13:22:27.244730 12903 sgd_solver.cpp:136] Iteration 39600, lr = 0.038125, m = 0.9
I0801 13:22:28.826341 12903 solver.cpp:353] Iteration 39700 (63.2273 iter/s, 1.58159s/100 iter), loss = 0.0589073
I0801 13:22:28.826370 12903 solver.cpp:375]     Train net output #0: loss = 0.0589079 (* 1 = 0.0589079 loss)
I0801 13:22:28.826377 12903 sgd_solver.cpp:136] Iteration 39700, lr = 0.0379688, m = 0.9
I0801 13:22:30.408896 12903 solver.cpp:353] Iteration 39800 (63.191 iter/s, 1.5825s/100 iter), loss = 0.0327832
I0801 13:22:30.408939 12903 solver.cpp:375]     Train net output #0: loss = 0.0327838 (* 1 = 0.0327838 loss)
I0801 13:22:30.408946 12903 sgd_solver.cpp:136] Iteration 39800, lr = 0.0378125, m = 0.9
I0801 13:22:31.965821 12903 solver.cpp:353] Iteration 39900 (64.2312 iter/s, 1.55688s/100 iter), loss = 0.00806561
I0801 13:22:31.965847 12903 solver.cpp:375]     Train net output #0: loss = 0.00806624 (* 1 = 0.00806624 loss)
I0801 13:22:31.965852 12903 sgd_solver.cpp:136] Iteration 39900, lr = 0.0376562, m = 0.9
I0801 13:22:33.530298 12903 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/cifar10_jacintonet11v2_iter_40000.caffemodel
I0801 13:22:33.540218 12903 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/cifar10_jacintonet11v2_iter_40000.solverstate
I0801 13:22:33.545266 12903 solver.cpp:550] Iteration 40000, Testing net (#0)
I0801 13:22:34.372448 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.851472
I0801 13:22:34.372469 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.992353
I0801 13:22:34.372475 12903 solver.cpp:635]     Test net output #2: loss = 0.613981 (* 1 = 0.613981 loss)
I0801 13:22:34.372501 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.82721s
I0801 13:22:34.388134 12903 solver.cpp:353] Iteration 40000 (41.284 iter/s, 2.42224s/100 iter), loss = 0.000913055
I0801 13:22:34.388150 12903 solver.cpp:375]     Train net output #0: loss = 0.000913658 (* 1 = 0.000913658 loss)
I0801 13:22:34.388157 12903 sgd_solver.cpp:136] Iteration 40000, lr = 0.0375, m = 0.9
I0801 13:22:35.952852 12903 solver.cpp:353] Iteration 40100 (63.9114 iter/s, 1.56467s/100 iter), loss = 0.0289101
I0801 13:22:35.952881 12903 solver.cpp:375]     Train net output #0: loss = 0.0289107 (* 1 = 0.0289107 loss)
I0801 13:22:35.952889 12903 sgd_solver.cpp:136] Iteration 40100, lr = 0.0373438, m = 0.9
I0801 13:22:37.518323 12903 solver.cpp:353] Iteration 40200 (63.8806 iter/s, 1.56542s/100 iter), loss = 0.00203802
I0801 13:22:37.518354 12903 solver.cpp:375]     Train net output #0: loss = 0.00203862 (* 1 = 0.00203862 loss)
I0801 13:22:37.518362 12903 sgd_solver.cpp:136] Iteration 40200, lr = 0.0371875, m = 0.9
I0801 13:22:39.099884 12903 solver.cpp:353] Iteration 40300 (63.2306 iter/s, 1.58151s/100 iter), loss = 0.07079
I0801 13:22:39.099911 12903 solver.cpp:375]     Train net output #0: loss = 0.0707906 (* 1 = 0.0707906 loss)
I0801 13:22:39.099947 12903 sgd_solver.cpp:136] Iteration 40300, lr = 0.0370313, m = 0.9
I0801 13:22:40.678938 12903 solver.cpp:353] Iteration 40400 (63.3311 iter/s, 1.579s/100 iter), loss = 0.0392231
I0801 13:22:40.678966 12903 solver.cpp:375]     Train net output #0: loss = 0.0392237 (* 1 = 0.0392237 loss)
I0801 13:22:40.678972 12903 sgd_solver.cpp:136] Iteration 40400, lr = 0.036875, m = 0.9
I0801 13:22:42.247707 12903 solver.cpp:353] Iteration 40500 (63.7462 iter/s, 1.56872s/100 iter), loss = 0.0323806
I0801 13:22:42.247737 12903 solver.cpp:375]     Train net output #0: loss = 0.0323812 (* 1 = 0.0323812 loss)
I0801 13:22:42.247745 12903 sgd_solver.cpp:136] Iteration 40500, lr = 0.0367188, m = 0.9
I0801 13:22:43.820482 12903 solver.cpp:353] Iteration 40600 (63.5839 iter/s, 1.57272s/100 iter), loss = 0.0357822
I0801 13:22:43.820508 12903 solver.cpp:375]     Train net output #0: loss = 0.0357828 (* 1 = 0.0357828 loss)
I0801 13:22:43.820513 12903 sgd_solver.cpp:136] Iteration 40600, lr = 0.0365625, m = 0.9
I0801 13:22:45.390982 12903 solver.cpp:353] Iteration 40700 (63.6759 iter/s, 1.57045s/100 iter), loss = 0.00581616
I0801 13:22:45.391008 12903 solver.cpp:375]     Train net output #0: loss = 0.00581676 (* 1 = 0.00581676 loss)
I0801 13:22:45.391016 12903 sgd_solver.cpp:136] Iteration 40700, lr = 0.0364062, m = 0.9
I0801 13:22:46.952445 12903 solver.cpp:353] Iteration 40800 (64.0447 iter/s, 1.56141s/100 iter), loss = 0.0219299
I0801 13:22:46.952498 12903 solver.cpp:375]     Train net output #0: loss = 0.0219305 (* 1 = 0.0219305 loss)
I0801 13:22:46.952512 12903 sgd_solver.cpp:136] Iteration 40800, lr = 0.03625, m = 0.9
I0801 13:22:48.525328 12903 solver.cpp:353] Iteration 40900 (63.5794 iter/s, 1.57284s/100 iter), loss = 0.0670992
I0801 13:22:48.525355 12903 solver.cpp:375]     Train net output #0: loss = 0.0670998 (* 1 = 0.0670998 loss)
I0801 13:22:48.525362 12903 sgd_solver.cpp:136] Iteration 40900, lr = 0.0360937, m = 0.9
I0801 13:22:50.075798 12903 solver.cpp:550] Iteration 41000, Testing net (#0)
I0801 13:22:50.900959 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.864707
I0801 13:22:50.900979 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.993235
I0801 13:22:50.900984 12903 solver.cpp:635]     Test net output #2: loss = 0.555799 (* 1 = 0.555799 loss)
I0801 13:22:50.901000 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.825179s
I0801 13:22:50.916640 12903 solver.cpp:353] Iteration 41000 (41.8193 iter/s, 2.39124s/100 iter), loss = 0.0284629
I0801 13:22:50.916658 12903 solver.cpp:375]     Train net output #0: loss = 0.0284635 (* 1 = 0.0284635 loss)
I0801 13:22:50.916664 12903 sgd_solver.cpp:136] Iteration 41000, lr = 0.0359375, m = 0.9
I0801 13:22:52.487674 12903 solver.cpp:353] Iteration 41100 (63.6545 iter/s, 1.57098s/100 iter), loss = 0.0324772
I0801 13:22:52.487726 12903 solver.cpp:375]     Train net output #0: loss = 0.0324778 (* 1 = 0.0324778 loss)
I0801 13:22:52.487741 12903 sgd_solver.cpp:136] Iteration 41100, lr = 0.0357813, m = 0.9
I0801 13:22:54.059928 12903 solver.cpp:353] Iteration 41200 (63.6048 iter/s, 1.57221s/100 iter), loss = 0.0432132
I0801 13:22:54.060039 12903 solver.cpp:375]     Train net output #0: loss = 0.0432138 (* 1 = 0.0432138 loss)
I0801 13:22:54.060055 12903 sgd_solver.cpp:136] Iteration 41200, lr = 0.035625, m = 0.9
I0801 13:22:55.631335 12903 solver.cpp:353] Iteration 41300 (63.6393 iter/s, 1.57136s/100 iter), loss = 0.0174801
I0801 13:22:55.631361 12903 solver.cpp:375]     Train net output #0: loss = 0.0174807 (* 1 = 0.0174807 loss)
I0801 13:22:55.631367 12903 sgd_solver.cpp:136] Iteration 41300, lr = 0.0354688, m = 0.9
I0801 13:22:57.213966 12903 solver.cpp:353] Iteration 41400 (63.1879 iter/s, 1.58258s/100 iter), loss = 0.0106245
I0801 13:22:57.213994 12903 solver.cpp:375]     Train net output #0: loss = 0.0106251 (* 1 = 0.0106251 loss)
I0801 13:22:57.213999 12903 sgd_solver.cpp:136] Iteration 41400, lr = 0.0353125, m = 0.9
I0801 13:22:58.781814 12903 solver.cpp:353] Iteration 41500 (63.7838 iter/s, 1.5678s/100 iter), loss = 0.0101751
I0801 13:22:58.781841 12903 solver.cpp:375]     Train net output #0: loss = 0.0101757 (* 1 = 0.0101757 loss)
I0801 13:22:58.781847 12903 sgd_solver.cpp:136] Iteration 41500, lr = 0.0351562, m = 0.9
I0801 13:23:00.344100 12903 solver.cpp:353] Iteration 41600 (64.0108 iter/s, 1.56224s/100 iter), loss = 0.0621812
I0801 13:23:00.344130 12903 solver.cpp:375]     Train net output #0: loss = 0.0621818 (* 1 = 0.0621818 loss)
I0801 13:23:00.344137 12903 sgd_solver.cpp:136] Iteration 41600, lr = 0.035, m = 0.9
I0801 13:23:01.918496 12903 solver.cpp:353] Iteration 41700 (63.5185 iter/s, 1.57435s/100 iter), loss = 0.00498844
I0801 13:23:01.918521 12903 solver.cpp:375]     Train net output #0: loss = 0.00498905 (* 1 = 0.00498905 loss)
I0801 13:23:01.918527 12903 sgd_solver.cpp:136] Iteration 41700, lr = 0.0348438, m = 0.9
I0801 13:23:03.496642 12903 solver.cpp:353] Iteration 41800 (63.3674 iter/s, 1.5781s/100 iter), loss = 0.013951
I0801 13:23:03.496666 12903 solver.cpp:375]     Train net output #0: loss = 0.0139516 (* 1 = 0.0139516 loss)
I0801 13:23:03.496673 12903 sgd_solver.cpp:136] Iteration 41800, lr = 0.0346875, m = 0.9
I0801 13:23:05.061095 12903 solver.cpp:353] Iteration 41900 (63.9221 iter/s, 1.5644s/100 iter), loss = 0.00662456
I0801 13:23:05.061121 12903 solver.cpp:375]     Train net output #0: loss = 0.00662518 (* 1 = 0.00662518 loss)
I0801 13:23:05.061127 12903 sgd_solver.cpp:136] Iteration 41900, lr = 0.0345312, m = 0.9
I0801 13:23:06.615298 12903 solver.cpp:550] Iteration 42000, Testing net (#0)
I0801 13:23:07.431303 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.875001
I0801 13:23:07.431321 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 13:23:07.431327 12903 solver.cpp:635]     Test net output #2: loss = 0.459553 (* 1 = 0.459553 loss)
I0801 13:23:07.431349 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.816028s
I0801 13:23:07.446903 12903 solver.cpp:353] Iteration 42000 (41.9158 iter/s, 2.38574s/100 iter), loss = 0.00358973
I0801 13:23:07.446920 12903 solver.cpp:375]     Train net output #0: loss = 0.00359034 (* 1 = 0.00359034 loss)
I0801 13:23:07.446926 12903 sgd_solver.cpp:136] Iteration 42000, lr = 0.034375, m = 0.9
I0801 13:23:09.014989 12903 solver.cpp:353] Iteration 42100 (63.7741 iter/s, 1.56803s/100 iter), loss = 0.00491471
I0801 13:23:09.015038 12903 solver.cpp:375]     Train net output #0: loss = 0.00491534 (* 1 = 0.00491534 loss)
I0801 13:23:09.015049 12903 sgd_solver.cpp:136] Iteration 42100, lr = 0.0342188, m = 0.9
I0801 13:23:10.599735 12903 solver.cpp:353] Iteration 42200 (63.1035 iter/s, 1.5847s/100 iter), loss = 0.00190843
I0801 13:23:10.599761 12903 solver.cpp:375]     Train net output #0: loss = 0.00190906 (* 1 = 0.00190906 loss)
I0801 13:23:10.599766 12903 sgd_solver.cpp:136] Iteration 42200, lr = 0.0340625, m = 0.9
I0801 13:23:12.179766 12903 solver.cpp:353] Iteration 42300 (63.292 iter/s, 1.57998s/100 iter), loss = 0.0228381
I0801 13:23:12.179795 12903 solver.cpp:375]     Train net output #0: loss = 0.0228387 (* 1 = 0.0228387 loss)
I0801 13:23:12.179802 12903 sgd_solver.cpp:136] Iteration 42300, lr = 0.0339063, m = 0.9
I0801 13:23:13.747807 12903 solver.cpp:353] Iteration 42400 (63.7759 iter/s, 1.56799s/100 iter), loss = 0.0104314
I0801 13:23:13.747848 12903 solver.cpp:375]     Train net output #0: loss = 0.0104321 (* 1 = 0.0104321 loss)
I0801 13:23:13.747853 12903 sgd_solver.cpp:136] Iteration 42400, lr = 0.03375, m = 0.9
I0801 13:23:15.320464 12903 solver.cpp:353] Iteration 42500 (63.5886 iter/s, 1.57261s/100 iter), loss = 0.0415968
I0801 13:23:15.320492 12903 solver.cpp:375]     Train net output #0: loss = 0.0415975 (* 1 = 0.0415975 loss)
I0801 13:23:15.320498 12903 sgd_solver.cpp:136] Iteration 42500, lr = 0.0335938, m = 0.9
I0801 13:23:16.892467 12903 solver.cpp:353] Iteration 42600 (63.6151 iter/s, 1.57195s/100 iter), loss = 0.00319946
I0801 13:23:16.892537 12903 solver.cpp:375]     Train net output #0: loss = 0.0032001 (* 1 = 0.0032001 loss)
I0801 13:23:16.892555 12903 sgd_solver.cpp:136] Iteration 42600, lr = 0.0334375, m = 0.9
I0801 13:23:18.472226 12903 solver.cpp:353] Iteration 42700 (63.3029 iter/s, 1.57971s/100 iter), loss = 0.00331861
I0801 13:23:18.472254 12903 solver.cpp:375]     Train net output #0: loss = 0.00331926 (* 1 = 0.00331926 loss)
I0801 13:23:18.472261 12903 sgd_solver.cpp:136] Iteration 42700, lr = 0.0332812, m = 0.9
I0801 13:23:20.042201 12903 solver.cpp:353] Iteration 42800 (63.6973 iter/s, 1.56993s/100 iter), loss = 0.000590882
I0801 13:23:20.042254 12903 solver.cpp:375]     Train net output #0: loss = 0.000591527 (* 1 = 0.000591527 loss)
I0801 13:23:20.042268 12903 sgd_solver.cpp:136] Iteration 42800, lr = 0.033125, m = 0.9
I0801 13:23:21.603500 12903 solver.cpp:353] Iteration 42900 (64.0512 iter/s, 1.56125s/100 iter), loss = 0.018764
I0801 13:23:21.603524 12903 solver.cpp:375]     Train net output #0: loss = 0.0187646 (* 1 = 0.0187646 loss)
I0801 13:23:21.603529 12903 sgd_solver.cpp:136] Iteration 42900, lr = 0.0329687, m = 0.9
I0801 13:23:23.164796 12903 solver.cpp:550] Iteration 43000, Testing net (#0)
I0801 13:23:23.990846 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.870295
I0801 13:23:23.990865 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.994412
I0801 13:23:23.990870 12903 solver.cpp:635]     Test net output #2: loss = 0.49932 (* 1 = 0.49932 loss)
I0801 13:23:23.990885 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.826067s
I0801 13:23:24.008705 12903 solver.cpp:353] Iteration 43000 (41.5778 iter/s, 2.40513s/100 iter), loss = 0.00163354
I0801 13:23:24.008724 12903 solver.cpp:375]     Train net output #0: loss = 0.00163418 (* 1 = 0.00163418 loss)
I0801 13:23:24.008730 12903 sgd_solver.cpp:136] Iteration 43000, lr = 0.0328125, m = 0.9
I0801 13:23:25.595888 12903 solver.cpp:353] Iteration 43100 (63.0067 iter/s, 1.58713s/100 iter), loss = 0.00049281
I0801 13:23:25.595991 12903 solver.cpp:375]     Train net output #0: loss = 0.000493442 (* 1 = 0.000493442 loss)
I0801 13:23:25.595999 12903 sgd_solver.cpp:136] Iteration 43100, lr = 0.0326563, m = 0.9
I0801 13:23:27.160331 12903 solver.cpp:353] Iteration 43200 (63.9227 iter/s, 1.56439s/100 iter), loss = 0.00382919
I0801 13:23:27.160382 12903 solver.cpp:375]     Train net output #0: loss = 0.00382983 (* 1 = 0.00382983 loss)
I0801 13:23:27.160398 12903 sgd_solver.cpp:136] Iteration 43200, lr = 0.0325, m = 0.9
I0801 13:23:28.720990 12903 solver.cpp:353] Iteration 43300 (64.0775 iter/s, 1.56061s/100 iter), loss = 0.00966514
I0801 13:23:28.721016 12903 solver.cpp:375]     Train net output #0: loss = 0.00966578 (* 1 = 0.00966578 loss)
I0801 13:23:28.721022 12903 sgd_solver.cpp:136] Iteration 43300, lr = 0.0323438, m = 0.9
I0801 13:23:30.284855 12903 solver.cpp:353] Iteration 43400 (63.946 iter/s, 1.56382s/100 iter), loss = 0.00849421
I0801 13:23:30.284883 12903 solver.cpp:375]     Train net output #0: loss = 0.00849485 (* 1 = 0.00849485 loss)
I0801 13:23:30.284889 12903 sgd_solver.cpp:136] Iteration 43400, lr = 0.0321875, m = 0.9
I0801 13:23:31.859689 12903 solver.cpp:353] Iteration 43500 (63.5009 iter/s, 1.57478s/100 iter), loss = 0.00529374
I0801 13:23:31.859714 12903 solver.cpp:375]     Train net output #0: loss = 0.00529439 (* 1 = 0.00529439 loss)
I0801 13:23:31.859719 12903 sgd_solver.cpp:136] Iteration 43500, lr = 0.0320312, m = 0.9
I0801 13:23:33.444329 12903 solver.cpp:353] Iteration 43600 (63.1078 iter/s, 1.58459s/100 iter), loss = 0.00571609
I0801 13:23:33.444403 12903 solver.cpp:375]     Train net output #0: loss = 0.00571675 (* 1 = 0.00571675 loss)
I0801 13:23:33.444425 12903 sgd_solver.cpp:136] Iteration 43600, lr = 0.031875, m = 0.9
I0801 13:23:35.008502 12903 solver.cpp:353] Iteration 43700 (63.9335 iter/s, 1.56413s/100 iter), loss = 0.0170403
I0801 13:23:35.008530 12903 solver.cpp:375]     Train net output #0: loss = 0.017041 (* 1 = 0.017041 loss)
I0801 13:23:35.008536 12903 sgd_solver.cpp:136] Iteration 43700, lr = 0.0317187, m = 0.9
I0801 13:23:36.602890 12903 solver.cpp:353] Iteration 43800 (62.722 iter/s, 1.59434s/100 iter), loss = 0.000179543
I0801 13:23:36.602947 12903 solver.cpp:375]     Train net output #0: loss = 0.000180218 (* 1 = 0.000180218 loss)
I0801 13:23:36.602963 12903 sgd_solver.cpp:136] Iteration 43800, lr = 0.0315625, m = 0.9
I0801 13:23:38.171965 12903 solver.cpp:353] Iteration 43900 (63.7339 iter/s, 1.56902s/100 iter), loss = 0.00262171
I0801 13:23:38.171989 12903 solver.cpp:375]     Train net output #0: loss = 0.00262239 (* 1 = 0.00262239 loss)
I0801 13:23:38.171994 12903 sgd_solver.cpp:136] Iteration 43900, lr = 0.0314062, m = 0.9
I0801 13:23:38.738921 12870 data_reader.cpp:264] Starting prefetch of epoch 6
I0801 13:23:39.725353 12903 solver.cpp:550] Iteration 44000, Testing net (#0)
I0801 13:23:40.546962 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.889707
I0801 13:23:40.546983 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.994706
I0801 13:23:40.546988 12903 solver.cpp:635]     Test net output #2: loss = 0.466261 (* 1 = 0.466261 loss)
I0801 13:23:40.547001 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.821626s
I0801 13:23:40.570926 12903 solver.cpp:353] Iteration 44000 (41.6859 iter/s, 2.39889s/100 iter), loss = 0.024151
I0801 13:23:40.570962 12903 solver.cpp:375]     Train net output #0: loss = 0.0241516 (* 1 = 0.0241516 loss)
I0801 13:23:40.570973 12903 sgd_solver.cpp:136] Iteration 44000, lr = 0.03125, m = 0.9
I0801 13:23:42.140739 12903 solver.cpp:353] Iteration 44100 (63.7039 iter/s, 1.56976s/100 iter), loss = 0.007219
I0801 13:23:42.140768 12903 solver.cpp:375]     Train net output #0: loss = 0.00721968 (* 1 = 0.00721968 loss)
I0801 13:23:42.140774 12903 sgd_solver.cpp:136] Iteration 44100, lr = 0.0310938, m = 0.9
I0801 13:23:43.725574 12903 solver.cpp:353] Iteration 44200 (63.1001 iter/s, 1.58478s/100 iter), loss = 0.0373023
I0801 13:23:43.725603 12903 solver.cpp:375]     Train net output #0: loss = 0.0373029 (* 1 = 0.0373029 loss)
I0801 13:23:43.725611 12903 sgd_solver.cpp:136] Iteration 44200, lr = 0.0309375, m = 0.9
I0801 13:23:45.320130 12903 solver.cpp:353] Iteration 44300 (62.7154 iter/s, 1.5945s/100 iter), loss = 0.0334322
I0801 13:23:45.320157 12903 solver.cpp:375]     Train net output #0: loss = 0.0334328 (* 1 = 0.0334328 loss)
I0801 13:23:45.320163 12903 sgd_solver.cpp:136] Iteration 44300, lr = 0.0307813, m = 0.9
I0801 13:23:46.888900 12903 solver.cpp:353] Iteration 44400 (63.7463 iter/s, 1.56872s/100 iter), loss = 0.00275525
I0801 13:23:46.888948 12903 solver.cpp:375]     Train net output #0: loss = 0.00275593 (* 1 = 0.00275593 loss)
I0801 13:23:46.888962 12903 sgd_solver.cpp:136] Iteration 44400, lr = 0.030625, m = 0.9
I0801 13:23:48.465529 12903 solver.cpp:353] Iteration 44500 (63.4284 iter/s, 1.57658s/100 iter), loss = 0.00144589
I0801 13:23:48.465713 12903 solver.cpp:375]     Train net output #0: loss = 0.00144657 (* 1 = 0.00144657 loss)
I0801 13:23:48.465801 12903 sgd_solver.cpp:136] Iteration 44500, lr = 0.0304688, m = 0.9
I0801 13:23:50.035730 12903 solver.cpp:353] Iteration 44600 (63.6881 iter/s, 1.57015s/100 iter), loss = 0.000496966
I0801 13:23:50.035758 12903 solver.cpp:375]     Train net output #0: loss = 0.000497654 (* 1 = 0.000497654 loss)
I0801 13:23:50.035763 12903 sgd_solver.cpp:136] Iteration 44600, lr = 0.0303125, m = 0.9
I0801 13:23:51.630412 12903 solver.cpp:353] Iteration 44700 (62.7105 iter/s, 1.59463s/100 iter), loss = 0.00047225
I0801 13:23:51.630475 12903 solver.cpp:375]     Train net output #0: loss = 0.000472938 (* 1 = 0.000472938 loss)
I0801 13:23:51.630491 12903 sgd_solver.cpp:136] Iteration 44700, lr = 0.0301562, m = 0.9
I0801 13:23:53.204741 12903 solver.cpp:353] Iteration 44800 (63.5211 iter/s, 1.57428s/100 iter), loss = 0.000775094
I0801 13:23:53.204766 12903 solver.cpp:375]     Train net output #0: loss = 0.000775782 (* 1 = 0.000775782 loss)
I0801 13:23:53.204772 12903 sgd_solver.cpp:136] Iteration 44800, lr = 0.03, m = 0.9
I0801 13:23:54.762850 12903 solver.cpp:353] Iteration 44900 (64.1824 iter/s, 1.55806s/100 iter), loss = 0.0126118
I0801 13:23:54.762876 12903 solver.cpp:375]     Train net output #0: loss = 0.0126125 (* 1 = 0.0126125 loss)
I0801 13:23:54.762882 12903 sgd_solver.cpp:136] Iteration 44900, lr = 0.0298437, m = 0.9
I0801 13:23:56.331804 12903 solver.cpp:550] Iteration 45000, Testing net (#0)
I0801 13:23:57.158357 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.902354
I0801 13:23:57.158380 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.994412
I0801 13:23:57.158385 12903 solver.cpp:635]     Test net output #2: loss = 0.38598 (* 1 = 0.38598 loss)
I0801 13:23:57.158443 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.826616s
I0801 13:23:57.173914 12903 solver.cpp:353] Iteration 45000 (41.4767 iter/s, 2.41099s/100 iter), loss = 0.00244864
I0801 13:23:57.173933 12903 solver.cpp:375]     Train net output #0: loss = 0.00244932 (* 1 = 0.00244932 loss)
I0801 13:23:57.173938 12903 sgd_solver.cpp:136] Iteration 45000, lr = 0.0296875, m = 0.9
I0801 13:23:58.740428 12903 solver.cpp:353] Iteration 45100 (63.8381 iter/s, 1.56646s/100 iter), loss = 0.000717065
I0801 13:23:58.740455 12903 solver.cpp:375]     Train net output #0: loss = 0.000717748 (* 1 = 0.000717748 loss)
I0801 13:23:58.740461 12903 sgd_solver.cpp:136] Iteration 45100, lr = 0.0295313, m = 0.9
I0801 13:24:00.339850 12903 solver.cpp:353] Iteration 45200 (62.5245 iter/s, 1.59937s/100 iter), loss = 0.00151424
I0801 13:24:00.339879 12903 solver.cpp:375]     Train net output #0: loss = 0.00151492 (* 1 = 0.00151492 loss)
I0801 13:24:00.339885 12903 sgd_solver.cpp:136] Iteration 45200, lr = 0.029375, m = 0.9
I0801 13:24:01.924839 12903 solver.cpp:353] Iteration 45300 (63.094 iter/s, 1.58494s/100 iter), loss = 0.000231652
I0801 13:24:01.924863 12903 solver.cpp:375]     Train net output #0: loss = 0.000232331 (* 1 = 0.000232331 loss)
I0801 13:24:01.924868 12903 sgd_solver.cpp:136] Iteration 45300, lr = 0.0292188, m = 0.9
I0801 13:24:03.499505 12903 solver.cpp:353] Iteration 45400 (63.5074 iter/s, 1.57462s/100 iter), loss = 0.00135304
I0801 13:24:03.499534 12903 solver.cpp:375]     Train net output #0: loss = 0.00135372 (* 1 = 0.00135372 loss)
I0801 13:24:03.499541 12903 sgd_solver.cpp:136] Iteration 45400, lr = 0.0290625, m = 0.9
I0801 13:24:05.059900 12903 solver.cpp:353] Iteration 45500 (64.0884 iter/s, 1.56035s/100 iter), loss = 0.0066793
I0801 13:24:05.059924 12903 solver.cpp:375]     Train net output #0: loss = 0.00667997 (* 1 = 0.00667997 loss)
I0801 13:24:05.059929 12903 sgd_solver.cpp:136] Iteration 45500, lr = 0.0289063, m = 0.9
I0801 13:24:06.633359 12903 solver.cpp:353] Iteration 45600 (63.5562 iter/s, 1.57341s/100 iter), loss = 0.00493855
I0801 13:24:06.633383 12903 solver.cpp:375]     Train net output #0: loss = 0.00493923 (* 1 = 0.00493923 loss)
I0801 13:24:06.633389 12903 sgd_solver.cpp:136] Iteration 45600, lr = 0.02875, m = 0.9
I0801 13:24:08.220883 12903 solver.cpp:353] Iteration 45700 (62.9931 iter/s, 1.58747s/100 iter), loss = 0.0044013
I0801 13:24:08.220947 12903 solver.cpp:375]     Train net output #0: loss = 0.00440198 (* 1 = 0.00440198 loss)
I0801 13:24:08.220966 12903 sgd_solver.cpp:136] Iteration 45700, lr = 0.0285937, m = 0.9
I0801 13:24:09.790107 12903 solver.cpp:353] Iteration 45800 (63.7279 iter/s, 1.56917s/100 iter), loss = 0.00065469
I0801 13:24:09.790156 12903 solver.cpp:375]     Train net output #0: loss = 0.000655367 (* 1 = 0.000655367 loss)
I0801 13:24:09.790169 12903 sgd_solver.cpp:136] Iteration 45800, lr = 0.0284375, m = 0.9
I0801 13:24:11.368975 12903 solver.cpp:353] Iteration 45900 (63.3386 iter/s, 1.57882s/100 iter), loss = 0.00223704
I0801 13:24:11.369000 12903 solver.cpp:375]     Train net output #0: loss = 0.00223772 (* 1 = 0.00223772 loss)
I0801 13:24:11.369006 12903 sgd_solver.cpp:136] Iteration 45900, lr = 0.0282812, m = 0.9
I0801 13:24:12.920959 12903 solver.cpp:550] Iteration 46000, Testing net (#0)
I0801 13:24:13.736898 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.907648
I0801 13:24:13.736918 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 13:24:13.736923 12903 solver.cpp:635]     Test net output #2: loss = 0.34087 (* 1 = 0.34087 loss)
I0801 13:24:13.736937 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.815956s
I0801 13:24:13.752568 12903 solver.cpp:353] Iteration 46000 (41.9547 iter/s, 2.38352s/100 iter), loss = 0.00445603
I0801 13:24:13.752588 12903 solver.cpp:375]     Train net output #0: loss = 0.00445671 (* 1 = 0.00445671 loss)
I0801 13:24:13.752614 12903 sgd_solver.cpp:136] Iteration 46000, lr = 0.028125, m = 0.9
I0801 13:24:15.307039 12903 solver.cpp:353] Iteration 46100 (64.3327 iter/s, 1.55442s/100 iter), loss = 0.00200996
I0801 13:24:15.307065 12903 solver.cpp:375]     Train net output #0: loss = 0.00201065 (* 1 = 0.00201065 loss)
I0801 13:24:15.307070 12903 sgd_solver.cpp:136] Iteration 46100, lr = 0.0279688, m = 0.9
I0801 13:24:16.885483 12903 solver.cpp:353] Iteration 46200 (63.3556 iter/s, 1.57839s/100 iter), loss = 0.000643884
I0801 13:24:16.885509 12903 solver.cpp:375]     Train net output #0: loss = 0.000644577 (* 1 = 0.000644577 loss)
I0801 13:24:16.885514 12903 sgd_solver.cpp:136] Iteration 46200, lr = 0.0278125, m = 0.9
I0801 13:24:18.489982 12903 solver.cpp:353] Iteration 46300 (62.3266 iter/s, 1.60445s/100 iter), loss = 0.000431973
I0801 13:24:18.490006 12903 solver.cpp:375]     Train net output #0: loss = 0.000432667 (* 1 = 0.000432667 loss)
I0801 13:24:18.490010 12903 sgd_solver.cpp:136] Iteration 46300, lr = 0.0276563, m = 0.9
I0801 13:24:20.052724 12903 solver.cpp:353] Iteration 46400 (63.9922 iter/s, 1.56269s/100 iter), loss = 0.000461719
I0801 13:24:20.052774 12903 solver.cpp:375]     Train net output #0: loss = 0.000462417 (* 1 = 0.000462417 loss)
I0801 13:24:20.052788 12903 sgd_solver.cpp:136] Iteration 46400, lr = 0.0275, m = 0.9
I0801 13:24:21.617964 12903 solver.cpp:353] Iteration 46500 (63.89 iter/s, 1.56519s/100 iter), loss = 0.00184761
I0801 13:24:21.618015 12903 solver.cpp:375]     Train net output #0: loss = 0.00184832 (* 1 = 0.00184832 loss)
I0801 13:24:21.618027 12903 sgd_solver.cpp:136] Iteration 46500, lr = 0.0273438, m = 0.9
I0801 13:24:23.187100 12903 solver.cpp:353] Iteration 46600 (63.7313 iter/s, 1.56909s/100 iter), loss = 0.00628541
I0801 13:24:23.187126 12903 solver.cpp:375]     Train net output #0: loss = 0.00628611 (* 1 = 0.00628611 loss)
I0801 13:24:23.187132 12903 sgd_solver.cpp:136] Iteration 46600, lr = 0.0271875, m = 0.9
I0801 13:24:24.764751 12903 solver.cpp:353] Iteration 46700 (63.3875 iter/s, 1.5776s/100 iter), loss = 0.00393961
I0801 13:24:24.764780 12903 solver.cpp:375]     Train net output #0: loss = 0.00394032 (* 1 = 0.00394032 loss)
I0801 13:24:24.764784 12903 sgd_solver.cpp:136] Iteration 46700, lr = 0.0270312, m = 0.9
I0801 13:24:26.337888 12903 solver.cpp:353] Iteration 46800 (63.5693 iter/s, 1.57309s/100 iter), loss = 0.00156902
I0801 13:24:26.337977 12903 solver.cpp:375]     Train net output #0: loss = 0.00156972 (* 1 = 0.00156972 loss)
I0801 13:24:26.337985 12903 sgd_solver.cpp:136] Iteration 46800, lr = 0.026875, m = 0.9
I0801 13:24:27.906286 12903 solver.cpp:353] Iteration 46900 (63.7613 iter/s, 1.56835s/100 iter), loss = 0.000759462
I0801 13:24:27.906313 12903 solver.cpp:375]     Train net output #0: loss = 0.000760166 (* 1 = 0.000760166 loss)
I0801 13:24:27.906319 12903 sgd_solver.cpp:136] Iteration 46900, lr = 0.0267187, m = 0.9
I0801 13:24:29.453059 12903 solver.cpp:550] Iteration 47000, Testing net (#0)
I0801 13:24:30.270009 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.905295
I0801 13:24:30.270028 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 13:24:30.270033 12903 solver.cpp:635]     Test net output #2: loss = 0.361522 (* 1 = 0.361522 loss)
I0801 13:24:30.270047 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.816965s
I0801 13:24:30.285619 12903 solver.cpp:353] Iteration 47000 (42.0298 iter/s, 2.37926s/100 iter), loss = 0.00262795
I0801 13:24:30.285639 12903 solver.cpp:375]     Train net output #0: loss = 0.00262866 (* 1 = 0.00262866 loss)
I0801 13:24:30.285642 12903 sgd_solver.cpp:136] Iteration 47000, lr = 0.0265625, m = 0.9
I0801 13:24:31.855190 12903 solver.cpp:353] Iteration 47100 (63.7137 iter/s, 1.56952s/100 iter), loss = 0.0232653
I0801 13:24:31.855218 12903 solver.cpp:375]     Train net output #0: loss = 0.023266 (* 1 = 0.023266 loss)
I0801 13:24:31.855224 12903 sgd_solver.cpp:136] Iteration 47100, lr = 0.0264063, m = 0.9
I0801 13:24:33.419955 12903 solver.cpp:353] Iteration 47200 (63.9095 iter/s, 1.56471s/100 iter), loss = 0.00329942
I0801 13:24:33.419981 12903 solver.cpp:375]     Train net output #0: loss = 0.00330012 (* 1 = 0.00330012 loss)
I0801 13:24:33.419987 12903 sgd_solver.cpp:136] Iteration 47200, lr = 0.02625, m = 0.9
I0801 13:24:34.994333 12903 solver.cpp:353] Iteration 47300 (63.5191 iter/s, 1.57433s/100 iter), loss = 0.000508554
I0801 13:24:34.994380 12903 solver.cpp:375]     Train net output #0: loss = 0.000509257 (* 1 = 0.000509257 loss)
I0801 13:24:34.994393 12903 sgd_solver.cpp:136] Iteration 47300, lr = 0.0260938, m = 0.9
I0801 13:24:36.579315 12903 solver.cpp:353] Iteration 47400 (63.0943 iter/s, 1.58493s/100 iter), loss = 0.00672366
I0801 13:24:36.579371 12903 solver.cpp:375]     Train net output #0: loss = 0.00672437 (* 1 = 0.00672437 loss)
I0801 13:24:36.579383 12903 sgd_solver.cpp:136] Iteration 47400, lr = 0.0259375, m = 0.9
I0801 13:24:38.144776 12903 solver.cpp:353] Iteration 47500 (63.881 iter/s, 1.56541s/100 iter), loss = 0.00104128
I0801 13:24:38.144804 12903 solver.cpp:375]     Train net output #0: loss = 0.00104198 (* 1 = 0.00104198 loss)
I0801 13:24:38.144809 12903 sgd_solver.cpp:136] Iteration 47500, lr = 0.0257812, m = 0.9
I0801 13:24:39.704267 12903 solver.cpp:353] Iteration 47600 (64.1256 iter/s, 1.55944s/100 iter), loss = 0.000792594
I0801 13:24:39.704293 12903 solver.cpp:375]     Train net output #0: loss = 0.000793299 (* 1 = 0.000793299 loss)
I0801 13:24:39.704300 12903 sgd_solver.cpp:136] Iteration 47600, lr = 0.025625, m = 0.9
I0801 13:24:41.276806 12903 solver.cpp:353] Iteration 47700 (63.5934 iter/s, 1.57249s/100 iter), loss = 0.00168473
I0801 13:24:41.276836 12903 solver.cpp:375]     Train net output #0: loss = 0.00168544 (* 1 = 0.00168544 loss)
I0801 13:24:41.276842 12903 sgd_solver.cpp:136] Iteration 47700, lr = 0.0254687, m = 0.9
I0801 13:24:42.859408 12903 solver.cpp:353] Iteration 47800 (63.1891 iter/s, 1.58255s/100 iter), loss = 0.0012303
I0801 13:24:42.859433 12903 solver.cpp:375]     Train net output #0: loss = 0.001231 (* 1 = 0.001231 loss)
I0801 13:24:42.859437 12903 sgd_solver.cpp:136] Iteration 47800, lr = 0.0253125, m = 0.9
I0801 13:24:44.432739 12903 solver.cpp:353] Iteration 47900 (63.5615 iter/s, 1.57328s/100 iter), loss = 7.00162e-05
I0801 13:24:44.432763 12903 solver.cpp:375]     Train net output #0: loss = 7.07198e-05 (* 1 = 7.07198e-05 loss)
I0801 13:24:44.432767 12903 sgd_solver.cpp:136] Iteration 47900, lr = 0.0251562, m = 0.9
I0801 13:24:45.976703 12903 solver.cpp:550] Iteration 48000, Testing net (#0)
I0801 13:24:46.007869 12893 data_reader.cpp:264] Starting prefetch of epoch 6
I0801 13:24:46.801564 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.910295
I0801 13:24:46.801584 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997353
I0801 13:24:46.801589 12903 solver.cpp:635]     Test net output #2: loss = 0.3393 (* 1 = 0.3393 loss)
I0801 13:24:46.801604 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.824879s
I0801 13:24:46.817209 12903 solver.cpp:353] Iteration 48000 (41.9392 iter/s, 2.3844s/100 iter), loss = 0.00096214
I0801 13:24:46.817239 12903 solver.cpp:375]     Train net output #0: loss = 0.000962844 (* 1 = 0.000962844 loss)
I0801 13:24:46.817250 12903 sgd_solver.cpp:136] Iteration 48000, lr = 0.025, m = 0.9
I0801 13:24:48.391712 12903 solver.cpp:353] Iteration 48100 (63.5142 iter/s, 1.57445s/100 iter), loss = 0.00148356
I0801 13:24:48.391774 12903 solver.cpp:375]     Train net output #0: loss = 0.00148426 (* 1 = 0.00148426 loss)
I0801 13:24:48.391791 12903 sgd_solver.cpp:136] Iteration 48100, lr = 0.0248438, m = 0.9
I0801 13:24:49.980670 12903 solver.cpp:353] Iteration 48200 (62.9363 iter/s, 1.58891s/100 iter), loss = 0.00194852
I0801 13:24:49.980695 12903 solver.cpp:375]     Train net output #0: loss = 0.00194922 (* 1 = 0.00194922 loss)
I0801 13:24:49.980701 12903 sgd_solver.cpp:136] Iteration 48200, lr = 0.0246875, m = 0.9
I0801 13:24:51.558743 12903 solver.cpp:353] Iteration 48300 (63.3705 iter/s, 1.57802s/100 iter), loss = 0.000907289
I0801 13:24:51.558770 12903 solver.cpp:375]     Train net output #0: loss = 0.000907992 (* 1 = 0.000907992 loss)
I0801 13:24:51.558776 12903 sgd_solver.cpp:136] Iteration 48300, lr = 0.0245313, m = 0.9
I0801 13:24:53.142416 12903 solver.cpp:353] Iteration 48400 (63.1463 iter/s, 1.58362s/100 iter), loss = 0.000900091
I0801 13:24:53.142441 12903 solver.cpp:375]     Train net output #0: loss = 0.000900794 (* 1 = 0.000900794 loss)
I0801 13:24:53.142446 12903 sgd_solver.cpp:136] Iteration 48400, lr = 0.024375, m = 0.9
I0801 13:24:54.708547 12903 solver.cpp:353] Iteration 48500 (63.8535 iter/s, 1.56608s/100 iter), loss = 0.000506197
I0801 13:24:54.708575 12903 solver.cpp:375]     Train net output #0: loss = 0.000506901 (* 1 = 0.000506901 loss)
I0801 13:24:54.708580 12903 sgd_solver.cpp:136] Iteration 48500, lr = 0.0242188, m = 0.9
I0801 13:24:56.290611 12903 solver.cpp:353] Iteration 48600 (63.2106 iter/s, 1.58201s/100 iter), loss = 0.00143697
I0801 13:24:56.290637 12903 solver.cpp:375]     Train net output #0: loss = 0.00143767 (* 1 = 0.00143767 loss)
I0801 13:24:56.290642 12903 sgd_solver.cpp:136] Iteration 48600, lr = 0.0240625, m = 0.9
I0801 13:24:57.869447 12903 solver.cpp:353] Iteration 48700 (63.3397 iter/s, 1.57879s/100 iter), loss = 0.000564478
I0801 13:24:57.869539 12903 solver.cpp:375]     Train net output #0: loss = 0.000565183 (* 1 = 0.000565183 loss)
I0801 13:24:57.869546 12903 sgd_solver.cpp:136] Iteration 48700, lr = 0.0239062, m = 0.9
I0801 13:24:59.455576 12903 solver.cpp:353] Iteration 48800 (63.0486 iter/s, 1.58608s/100 iter), loss = 0.000833325
I0801 13:24:59.455605 12903 solver.cpp:375]     Train net output #0: loss = 0.000834032 (* 1 = 0.000834032 loss)
I0801 13:24:59.455612 12903 sgd_solver.cpp:136] Iteration 48800, lr = 0.02375, m = 0.9
I0801 13:25:01.014787 12903 solver.cpp:353] Iteration 48900 (64.137 iter/s, 1.55916s/100 iter), loss = 0.000657971
I0801 13:25:01.014813 12903 solver.cpp:375]     Train net output #0: loss = 0.000658678 (* 1 = 0.000658678 loss)
I0801 13:25:01.014819 12903 sgd_solver.cpp:136] Iteration 48900, lr = 0.0235937, m = 0.9
I0801 13:25:02.568639 12903 solver.cpp:550] Iteration 49000, Testing net (#0)
I0801 13:25:03.382109 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.919119
I0801 13:25:03.382129 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 13:25:03.382134 12903 solver.cpp:635]     Test net output #2: loss = 0.313162 (* 1 = 0.313162 loss)
I0801 13:25:03.382148 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.813486s
I0801 13:25:03.397828 12903 solver.cpp:353] Iteration 49000 (41.9644 iter/s, 2.38297s/100 iter), loss = 0.0013943
I0801 13:25:03.397847 12903 solver.cpp:375]     Train net output #0: loss = 0.00139501 (* 1 = 0.00139501 loss)
I0801 13:25:03.397853 12903 sgd_solver.cpp:136] Iteration 49000, lr = 0.0234375, m = 0.9
I0801 13:25:04.990711 12903 solver.cpp:353] Iteration 49100 (62.7813 iter/s, 1.59283s/100 iter), loss = 0.000739293
I0801 13:25:04.990761 12903 solver.cpp:375]     Train net output #0: loss = 0.000739998 (* 1 = 0.000739998 loss)
I0801 13:25:04.990775 12903 sgd_solver.cpp:136] Iteration 49100, lr = 0.0232813, m = 0.9
I0801 13:25:06.582110 12903 solver.cpp:353] Iteration 49200 (62.8398 iter/s, 1.59135s/100 iter), loss = 0.000679991
I0801 13:25:06.582134 12903 solver.cpp:375]     Train net output #0: loss = 0.000680696 (* 1 = 0.000680696 loss)
I0801 13:25:06.582139 12903 sgd_solver.cpp:136] Iteration 49200, lr = 0.023125, m = 0.9
I0801 13:25:08.149071 12903 solver.cpp:353] Iteration 49300 (63.8198 iter/s, 1.56691s/100 iter), loss = 0.00296403
I0801 13:25:08.149096 12903 solver.cpp:375]     Train net output #0: loss = 0.00296474 (* 1 = 0.00296474 loss)
I0801 13:25:08.149102 12903 sgd_solver.cpp:136] Iteration 49300, lr = 0.0229688, m = 0.9
I0801 13:25:09.735374 12903 solver.cpp:353] Iteration 49400 (63.0417 iter/s, 1.58625s/100 iter), loss = 0.00084682
I0801 13:25:09.735400 12903 solver.cpp:375]     Train net output #0: loss = 0.000847525 (* 1 = 0.000847525 loss)
I0801 13:25:09.735405 12903 sgd_solver.cpp:136] Iteration 49400, lr = 0.0228125, m = 0.9
I0801 13:25:11.319339 12903 solver.cpp:353] Iteration 49500 (63.1347 iter/s, 1.58391s/100 iter), loss = 0.000480535
I0801 13:25:11.319366 12903 solver.cpp:375]     Train net output #0: loss = 0.00048124 (* 1 = 0.00048124 loss)
I0801 13:25:11.319371 12903 sgd_solver.cpp:136] Iteration 49500, lr = 0.0226563, m = 0.9
I0801 13:25:12.882639 12903 solver.cpp:353] Iteration 49600 (63.9692 iter/s, 1.56325s/100 iter), loss = 0.00132814
I0801 13:25:12.882668 12903 solver.cpp:375]     Train net output #0: loss = 0.00132885 (* 1 = 0.00132885 loss)
I0801 13:25:12.882673 12903 sgd_solver.cpp:136] Iteration 49600, lr = 0.0225, m = 0.9
I0801 13:25:14.451462 12903 solver.cpp:353] Iteration 49700 (63.7441 iter/s, 1.56877s/100 iter), loss = 0.000381254
I0801 13:25:14.451520 12903 solver.cpp:375]     Train net output #0: loss = 0.000381959 (* 1 = 0.000381959 loss)
I0801 13:25:14.451539 12903 sgd_solver.cpp:136] Iteration 49700, lr = 0.0223437, m = 0.9
I0801 13:25:16.011600 12903 solver.cpp:353] Iteration 49800 (64.099 iter/s, 1.56009s/100 iter), loss = 0.000385292
I0801 13:25:16.011652 12903 solver.cpp:375]     Train net output #0: loss = 0.000385997 (* 1 = 0.000385997 loss)
I0801 13:25:16.011667 12903 sgd_solver.cpp:136] Iteration 49800, lr = 0.0221875, m = 0.9
I0801 13:25:17.605468 12903 solver.cpp:353] Iteration 49900 (62.7424 iter/s, 1.59382s/100 iter), loss = 0.000165863
I0801 13:25:17.605520 12903 solver.cpp:375]     Train net output #0: loss = 0.000166567 (* 1 = 0.000166567 loss)
I0801 13:25:17.605533 12903 sgd_solver.cpp:136] Iteration 49900, lr = 0.0220312, m = 0.9
I0801 13:25:19.151702 12903 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/cifar10_jacintonet11v2_iter_50000.caffemodel
I0801 13:25:19.159831 12903 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/cifar10_jacintonet11v2_iter_50000.solverstate
I0801 13:25:19.163585 12903 solver.cpp:550] Iteration 50000, Testing net (#0)
I0801 13:25:19.969437 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.915001
I0801 13:25:19.969458 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995588
I0801 13:25:19.969465 12903 solver.cpp:635]     Test net output #2: loss = 0.328909 (* 1 = 0.328909 loss)
I0801 13:25:19.969481 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.805871s
I0801 13:25:19.985030 12903 solver.cpp:353] Iteration 50000 (42.0258 iter/s, 2.37949s/100 iter), loss = 0.000590556
I0801 13:25:19.985047 12903 solver.cpp:375]     Train net output #0: loss = 0.000591259 (* 1 = 0.000591259 loss)
I0801 13:25:19.985054 12903 sgd_solver.cpp:136] Iteration 50000, lr = 0.021875, m = 0.9
I0801 13:25:21.565487 12903 solver.cpp:353] Iteration 50100 (63.2749 iter/s, 1.5804s/100 iter), loss = 0.000640826
I0801 13:25:21.565512 12903 solver.cpp:375]     Train net output #0: loss = 0.00064153 (* 1 = 0.00064153 loss)
I0801 13:25:21.565518 12903 sgd_solver.cpp:136] Iteration 50100, lr = 0.0217188, m = 0.9
I0801 13:25:23.149327 12903 solver.cpp:353] Iteration 50200 (63.1398 iter/s, 1.58379s/100 iter), loss = 0.00143141
I0801 13:25:23.149353 12903 solver.cpp:375]     Train net output #0: loss = 0.00143211 (* 1 = 0.00143211 loss)
I0801 13:25:23.149356 12903 sgd_solver.cpp:136] Iteration 50200, lr = 0.0215625, m = 0.9
I0801 13:25:24.711545 12903 solver.cpp:353] Iteration 50300 (64.0136 iter/s, 1.56217s/100 iter), loss = 0.00119181
I0801 13:25:24.711575 12903 solver.cpp:375]     Train net output #0: loss = 0.00119251 (* 1 = 0.00119251 loss)
I0801 13:25:24.711582 12903 sgd_solver.cpp:136] Iteration 50300, lr = 0.0214063, m = 0.9
I0801 13:25:26.276260 12903 solver.cpp:353] Iteration 50400 (63.9114 iter/s, 1.56467s/100 iter), loss = 0.000815547
I0801 13:25:26.276288 12903 solver.cpp:375]     Train net output #0: loss = 0.000816249 (* 1 = 0.000816249 loss)
I0801 13:25:26.276293 12903 sgd_solver.cpp:136] Iteration 50400, lr = 0.02125, m = 0.9
I0801 13:25:27.840178 12903 solver.cpp:353] Iteration 50500 (63.944 iter/s, 1.56387s/100 iter), loss = 0.00130608
I0801 13:25:27.840204 12903 solver.cpp:375]     Train net output #0: loss = 0.00130678 (* 1 = 0.00130678 loss)
I0801 13:25:27.840209 12903 sgd_solver.cpp:136] Iteration 50500, lr = 0.0210938, m = 0.9
I0801 13:25:29.413121 12903 solver.cpp:353] Iteration 50600 (63.5771 iter/s, 1.57289s/100 iter), loss = 0.00107607
I0801 13:25:29.413215 12903 solver.cpp:375]     Train net output #0: loss = 0.00107678 (* 1 = 0.00107678 loss)
I0801 13:25:29.413223 12903 sgd_solver.cpp:136] Iteration 50600, lr = 0.0209375, m = 0.9
I0801 13:25:30.983394 12903 solver.cpp:353] Iteration 50700 (63.6852 iter/s, 1.57022s/100 iter), loss = 0.00135973
I0801 13:25:30.983443 12903 solver.cpp:375]     Train net output #0: loss = 0.00136043 (* 1 = 0.00136043 loss)
I0801 13:25:30.983454 12903 sgd_solver.cpp:136] Iteration 50700, lr = 0.0207812, m = 0.9
I0801 13:25:32.568013 12903 solver.cpp:353] Iteration 50800 (63.1087 iter/s, 1.58457s/100 iter), loss = 0.000312682
I0801 13:25:32.568039 12903 solver.cpp:375]     Train net output #0: loss = 0.000313386 (* 1 = 0.000313386 loss)
I0801 13:25:32.568045 12903 sgd_solver.cpp:136] Iteration 50800, lr = 0.020625, m = 0.9
I0801 13:25:34.143808 12903 solver.cpp:353] Iteration 50900 (63.462 iter/s, 1.57575s/100 iter), loss = 0.000513935
I0801 13:25:34.143838 12903 solver.cpp:375]     Train net output #0: loss = 0.000514639 (* 1 = 0.000514639 loss)
I0801 13:25:34.143846 12903 sgd_solver.cpp:136] Iteration 50900, lr = 0.0204687, m = 0.9
I0801 13:25:35.703783 12903 solver.cpp:550] Iteration 51000, Testing net (#0)
I0801 13:25:36.521924 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.914119
I0801 13:25:36.521944 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 13:25:36.521950 12903 solver.cpp:635]     Test net output #2: loss = 0.325929 (* 1 = 0.325929 loss)
I0801 13:25:36.521965 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.818159s
I0801 13:25:36.538552 12903 solver.cpp:353] Iteration 51000 (41.7594 iter/s, 2.39467s/100 iter), loss = 0.00179885
I0801 13:25:36.538586 12903 solver.cpp:375]     Train net output #0: loss = 0.00179956 (* 1 = 0.00179956 loss)
I0801 13:25:36.538594 12903 sgd_solver.cpp:136] Iteration 51000, lr = 0.0203125, m = 0.9
I0801 13:25:38.116425 12903 solver.cpp:353] Iteration 51100 (63.3784 iter/s, 1.57782s/100 iter), loss = 0.000391277
I0801 13:25:38.116452 12903 solver.cpp:375]     Train net output #0: loss = 0.000391981 (* 1 = 0.000391981 loss)
I0801 13:25:38.116458 12903 sgd_solver.cpp:136] Iteration 51100, lr = 0.0201563, m = 0.9
I0801 13:25:39.682034 12903 solver.cpp:353] Iteration 51200 (63.875 iter/s, 1.56556s/100 iter), loss = 0.000696904
I0801 13:25:39.682061 12903 solver.cpp:375]     Train net output #0: loss = 0.000697608 (* 1 = 0.000697608 loss)
I0801 13:25:39.682066 12903 sgd_solver.cpp:136] Iteration 51200, lr = 0.02, m = 0.9
I0801 13:25:41.262037 12903 solver.cpp:353] Iteration 51300 (63.293 iter/s, 1.57995s/100 iter), loss = 0.000892735
I0801 13:25:41.262257 12903 solver.cpp:375]     Train net output #0: loss = 0.000893439 (* 1 = 0.000893439 loss)
I0801 13:25:41.262267 12903 sgd_solver.cpp:136] Iteration 51300, lr = 0.0198438, m = 0.9
I0801 13:25:42.835777 12903 solver.cpp:353] Iteration 51400 (63.5449 iter/s, 1.57369s/100 iter), loss = 0.00351579
I0801 13:25:42.835808 12903 solver.cpp:375]     Train net output #0: loss = 0.0035165 (* 1 = 0.0035165 loss)
I0801 13:25:42.835814 12903 sgd_solver.cpp:136] Iteration 51400, lr = 0.0196875, m = 0.9
I0801 13:25:44.400761 12903 solver.cpp:353] Iteration 51500 (63.9005 iter/s, 1.56493s/100 iter), loss = 0.000573569
I0801 13:25:44.400791 12903 solver.cpp:375]     Train net output #0: loss = 0.000574273 (* 1 = 0.000574273 loss)
I0801 13:25:44.400799 12903 sgd_solver.cpp:136] Iteration 51500, lr = 0.0195312, m = 0.9
I0801 13:25:45.980165 12903 solver.cpp:353] Iteration 51600 (63.3171 iter/s, 1.57935s/100 iter), loss = 0.000836615
I0801 13:25:45.980216 12903 solver.cpp:375]     Train net output #0: loss = 0.000837321 (* 1 = 0.000837321 loss)
I0801 13:25:45.980229 12903 sgd_solver.cpp:136] Iteration 51600, lr = 0.019375, m = 0.9
I0801 13:25:47.591292 12903 solver.cpp:353] Iteration 51700 (62.0703 iter/s, 1.61108s/100 iter), loss = 0.000292465
I0801 13:25:47.591320 12903 solver.cpp:375]     Train net output #0: loss = 0.00029317 (* 1 = 0.00029317 loss)
I0801 13:25:47.591327 12903 sgd_solver.cpp:136] Iteration 51700, lr = 0.0192187, m = 0.9
I0801 13:25:49.159061 12903 solver.cpp:353] Iteration 51800 (63.7869 iter/s, 1.56772s/100 iter), loss = 0.00181618
I0801 13:25:49.159086 12903 solver.cpp:375]     Train net output #0: loss = 0.00181688 (* 1 = 0.00181688 loss)
I0801 13:25:49.159090 12903 sgd_solver.cpp:136] Iteration 51800, lr = 0.0190625, m = 0.9
I0801 13:25:50.739166 12903 solver.cpp:353] Iteration 51900 (63.289 iter/s, 1.58005s/100 iter), loss = 0.000260359
I0801 13:25:50.739192 12903 solver.cpp:375]     Train net output #0: loss = 0.000261065 (* 1 = 0.000261065 loss)
I0801 13:25:50.739198 12903 sgd_solver.cpp:136] Iteration 51900, lr = 0.0189062, m = 0.9
I0801 13:25:52.285542 12903 solver.cpp:550] Iteration 52000, Testing net (#0)
I0801 13:25:53.017277 12893 data_reader.cpp:264] Starting prefetch of epoch 7
I0801 13:25:53.109104 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.915883
I0801 13:25:53.109122 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 13:25:53.109127 12903 solver.cpp:635]     Test net output #2: loss = 0.316027 (* 1 = 0.316027 loss)
I0801 13:25:53.109144 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.823582s
I0801 13:25:53.124887 12903 solver.cpp:353] Iteration 52000 (41.9173 iter/s, 2.38565s/100 iter), loss = 0.000716578
I0801 13:25:53.124904 12903 solver.cpp:375]     Train net output #0: loss = 0.000717283 (* 1 = 0.000717283 loss)
I0801 13:25:53.124909 12903 sgd_solver.cpp:136] Iteration 52000, lr = 0.01875, m = 0.9
I0801 13:25:54.681800 12903 solver.cpp:353] Iteration 52100 (64.2317 iter/s, 1.55686s/100 iter), loss = 0.000778962
I0801 13:25:54.681849 12903 solver.cpp:375]     Train net output #0: loss = 0.000779667 (* 1 = 0.000779667 loss)
I0801 13:25:54.681857 12903 sgd_solver.cpp:136] Iteration 52100, lr = 0.0185938, m = 0.9
I0801 13:25:56.248119 12903 solver.cpp:353] Iteration 52200 (63.8459 iter/s, 1.56627s/100 iter), loss = 0.000693935
I0801 13:25:56.248147 12903 solver.cpp:375]     Train net output #0: loss = 0.000694641 (* 1 = 0.000694641 loss)
I0801 13:25:56.248153 12903 sgd_solver.cpp:136] Iteration 52200, lr = 0.0184375, m = 0.9
I0801 13:25:57.836972 12903 solver.cpp:353] Iteration 52300 (62.9404 iter/s, 1.5888s/100 iter), loss = 0.000633808
I0801 13:25:57.837023 12903 solver.cpp:375]     Train net output #0: loss = 0.000634514 (* 1 = 0.000634514 loss)
I0801 13:25:57.837036 12903 sgd_solver.cpp:136] Iteration 52300, lr = 0.0182813, m = 0.9
I0801 13:25:59.421001 12903 solver.cpp:353] Iteration 52400 (63.1322 iter/s, 1.58398s/100 iter), loss = 0.00088626
I0801 13:25:59.421056 12903 solver.cpp:375]     Train net output #0: loss = 0.000886966 (* 1 = 0.000886966 loss)
I0801 13:25:59.421063 12903 sgd_solver.cpp:136] Iteration 52400, lr = 0.018125, m = 0.9
I0801 13:26:01.000838 12903 solver.cpp:353] Iteration 52500 (63.2997 iter/s, 1.57979s/100 iter), loss = 0.000941273
I0801 13:26:01.000903 12903 solver.cpp:375]     Train net output #0: loss = 0.000941979 (* 1 = 0.000941979 loss)
I0801 13:26:01.000924 12903 sgd_solver.cpp:136] Iteration 52500, lr = 0.0179687, m = 0.9
I0801 13:26:02.577596 12903 solver.cpp:353] Iteration 52600 (63.4234 iter/s, 1.57671s/100 iter), loss = 0.00150245
I0801 13:26:02.577622 12903 solver.cpp:375]     Train net output #0: loss = 0.00150315 (* 1 = 0.00150315 loss)
I0801 13:26:02.577628 12903 sgd_solver.cpp:136] Iteration 52600, lr = 0.0178125, m = 0.9
I0801 13:26:04.146711 12903 solver.cpp:353] Iteration 52700 (63.7321 iter/s, 1.56907s/100 iter), loss = 0.000527074
I0801 13:26:04.146736 12903 solver.cpp:375]     Train net output #0: loss = 0.00052778 (* 1 = 0.00052778 loss)
I0801 13:26:04.146742 12903 sgd_solver.cpp:136] Iteration 52700, lr = 0.0176562, m = 0.9
I0801 13:26:05.728227 12903 solver.cpp:353] Iteration 52800 (63.2325 iter/s, 1.58146s/100 iter), loss = 0.000359977
I0801 13:26:05.728291 12903 solver.cpp:375]     Train net output #0: loss = 0.000360684 (* 1 = 0.000360684 loss)
I0801 13:26:05.728301 12903 sgd_solver.cpp:136] Iteration 52800, lr = 0.0175, m = 0.9
I0801 13:26:07.300171 12903 solver.cpp:353] Iteration 52900 (63.6175 iter/s, 1.57189s/100 iter), loss = 0.000546978
I0801 13:26:07.300192 12903 solver.cpp:375]     Train net output #0: loss = 0.000547685 (* 1 = 0.000547685 loss)
I0801 13:26:07.300199 12903 sgd_solver.cpp:136] Iteration 52900, lr = 0.0173437, m = 0.9
I0801 13:26:08.853878 12903 solver.cpp:550] Iteration 53000, Testing net (#0)
I0801 13:26:09.672026 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.92206
I0801 13:26:09.672049 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 13:26:09.672053 12903 solver.cpp:635]     Test net output #2: loss = 0.299387 (* 1 = 0.299387 loss)
I0801 13:26:09.672068 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.818168s
I0801 13:26:09.688053 12903 solver.cpp:353] Iteration 53000 (41.8793 iter/s, 2.38781s/100 iter), loss = 0.00106192
I0801 13:26:09.688071 12903 solver.cpp:375]     Train net output #0: loss = 0.00106262 (* 1 = 0.00106262 loss)
I0801 13:26:09.688074 12903 sgd_solver.cpp:136] Iteration 53000, lr = 0.0171875, m = 0.9
I0801 13:26:11.253165 12903 solver.cpp:353] Iteration 53100 (63.8953 iter/s, 1.56506s/100 iter), loss = 0.000354649
I0801 13:26:11.253196 12903 solver.cpp:375]     Train net output #0: loss = 0.000355355 (* 1 = 0.000355355 loss)
I0801 13:26:11.253202 12903 sgd_solver.cpp:136] Iteration 53100, lr = 0.0170313, m = 0.9
I0801 13:26:12.826190 12903 solver.cpp:353] Iteration 53200 (63.5738 iter/s, 1.57297s/100 iter), loss = 0.000400784
I0801 13:26:12.826239 12903 solver.cpp:375]     Train net output #0: loss = 0.00040149 (* 1 = 0.00040149 loss)
I0801 13:26:12.826251 12903 sgd_solver.cpp:136] Iteration 53200, lr = 0.016875, m = 0.9
I0801 13:26:14.381875 12903 solver.cpp:353] Iteration 53300 (64.2824 iter/s, 1.55564s/100 iter), loss = 0.000891638
I0801 13:26:14.381899 12903 solver.cpp:375]     Train net output #0: loss = 0.000892345 (* 1 = 0.000892345 loss)
I0801 13:26:14.381904 12903 sgd_solver.cpp:136] Iteration 53300, lr = 0.0167188, m = 0.9
I0801 13:26:15.961071 12903 solver.cpp:353] Iteration 53400 (63.3254 iter/s, 1.57915s/100 iter), loss = 0.000831972
I0801 13:26:15.961097 12903 solver.cpp:375]     Train net output #0: loss = 0.000832679 (* 1 = 0.000832679 loss)
I0801 13:26:15.961102 12903 sgd_solver.cpp:136] Iteration 53400, lr = 0.0165625, m = 0.9
I0801 13:26:17.532518 12903 solver.cpp:353] Iteration 53500 (63.6376 iter/s, 1.5714s/100 iter), loss = 0.00101401
I0801 13:26:17.532542 12903 solver.cpp:375]     Train net output #0: loss = 0.00101471 (* 1 = 0.00101471 loss)
I0801 13:26:17.532548 12903 sgd_solver.cpp:136] Iteration 53500, lr = 0.0164063, m = 0.9
I0801 13:26:19.096906 12903 solver.cpp:353] Iteration 53600 (63.9248 iter/s, 1.56434s/100 iter), loss = 0.000911714
I0801 13:26:19.096932 12903 solver.cpp:375]     Train net output #0: loss = 0.000912421 (* 1 = 0.000912421 loss)
I0801 13:26:19.096938 12903 sgd_solver.cpp:136] Iteration 53600, lr = 0.01625, m = 0.9
I0801 13:26:20.662706 12903 solver.cpp:353] Iteration 53700 (63.8672 iter/s, 1.56575s/100 iter), loss = 0.000385599
I0801 13:26:20.662755 12903 solver.cpp:375]     Train net output #0: loss = 0.000386305 (* 1 = 0.000386305 loss)
I0801 13:26:20.662766 12903 sgd_solver.cpp:136] Iteration 53700, lr = 0.0160937, m = 0.9
I0801 13:26:22.235769 12903 solver.cpp:353] Iteration 53800 (63.5723 iter/s, 1.57301s/100 iter), loss = 0.000702459
I0801 13:26:22.235795 12903 solver.cpp:375]     Train net output #0: loss = 0.000703165 (* 1 = 0.000703165 loss)
I0801 13:26:22.235800 12903 sgd_solver.cpp:136] Iteration 53800, lr = 0.0159375, m = 0.9
I0801 13:26:23.813295 12903 solver.cpp:353] Iteration 53900 (63.3923 iter/s, 1.57748s/100 iter), loss = 0.00320757
I0801 13:26:23.813344 12903 solver.cpp:375]     Train net output #0: loss = 0.00320828 (* 1 = 0.00320828 loss)
I0801 13:26:23.813355 12903 sgd_solver.cpp:136] Iteration 53900, lr = 0.0157812, m = 0.9
I0801 13:26:25.365409 12903 solver.cpp:550] Iteration 54000, Testing net (#0)
I0801 13:26:26.186767 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.92853
I0801 13:26:26.186790 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 13:26:26.186796 12903 solver.cpp:635]     Test net output #2: loss = 0.270657 (* 1 = 0.270657 loss)
I0801 13:26:26.186838 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.821407s
I0801 13:26:26.202659 12903 solver.cpp:353] Iteration 54000 (41.8534 iter/s, 2.38929s/100 iter), loss = 0.00339948
I0801 13:26:26.202677 12903 solver.cpp:375]     Train net output #0: loss = 0.00340019 (* 1 = 0.00340019 loss)
I0801 13:26:26.202680 12903 sgd_solver.cpp:136] Iteration 54000, lr = 0.015625, m = 0.9
I0801 13:26:27.801885 12903 solver.cpp:353] Iteration 54100 (62.5323 iter/s, 1.59917s/100 iter), loss = 0.000838586
I0801 13:26:27.801911 12903 solver.cpp:375]     Train net output #0: loss = 0.000839292 (* 1 = 0.000839292 loss)
I0801 13:26:27.801918 12903 sgd_solver.cpp:136] Iteration 54100, lr = 0.0154688, m = 0.9
I0801 13:26:29.375375 12903 solver.cpp:353] Iteration 54200 (63.555 iter/s, 1.57344s/100 iter), loss = 0.00139794
I0801 13:26:29.375403 12903 solver.cpp:375]     Train net output #0: loss = 0.00139865 (* 1 = 0.00139865 loss)
I0801 13:26:29.375411 12903 sgd_solver.cpp:136] Iteration 54200, lr = 0.0153125, m = 0.9
I0801 13:26:30.957545 12903 solver.cpp:353] Iteration 54300 (63.2063 iter/s, 1.58212s/100 iter), loss = 0.00110081
I0801 13:26:30.957630 12903 solver.cpp:375]     Train net output #0: loss = 0.00110152 (* 1 = 0.00110152 loss)
I0801 13:26:30.957638 12903 sgd_solver.cpp:136] Iteration 54300, lr = 0.0151563, m = 0.9
I0801 13:26:32.529386 12903 solver.cpp:353] Iteration 54400 (63.6217 iter/s, 1.57179s/100 iter), loss = 0.00134927
I0801 13:26:32.529417 12903 solver.cpp:375]     Train net output #0: loss = 0.00134997 (* 1 = 0.00134997 loss)
I0801 13:26:32.529423 12903 sgd_solver.cpp:136] Iteration 54400, lr = 0.015, m = 0.9
I0801 13:26:34.115867 12903 solver.cpp:353] Iteration 54500 (63.0344 iter/s, 1.58643s/100 iter), loss = 0.00102437
I0801 13:26:34.115895 12903 solver.cpp:375]     Train net output #0: loss = 0.00102507 (* 1 = 0.00102507 loss)
I0801 13:26:34.115901 12903 sgd_solver.cpp:136] Iteration 54500, lr = 0.0148437, m = 0.9
I0801 13:26:35.687711 12903 solver.cpp:353] Iteration 54600 (63.6216 iter/s, 1.57179s/100 iter), loss = 0.000828277
I0801 13:26:35.687757 12903 solver.cpp:375]     Train net output #0: loss = 0.000828984 (* 1 = 0.000828984 loss)
I0801 13:26:35.687768 12903 sgd_solver.cpp:136] Iteration 54600, lr = 0.0146875, m = 0.9
I0801 13:26:37.270185 12903 solver.cpp:353] Iteration 54700 (63.1941 iter/s, 1.58243s/100 iter), loss = 0.00237774
I0801 13:26:37.270210 12903 solver.cpp:375]     Train net output #0: loss = 0.00237845 (* 1 = 0.00237845 loss)
I0801 13:26:37.270215 12903 sgd_solver.cpp:136] Iteration 54700, lr = 0.0145312, m = 0.9
I0801 13:26:38.843350 12903 solver.cpp:353] Iteration 54800 (63.5682 iter/s, 1.57311s/100 iter), loss = 0.00100538
I0801 13:26:38.843377 12903 solver.cpp:375]     Train net output #0: loss = 0.00100609 (* 1 = 0.00100609 loss)
I0801 13:26:38.843384 12903 sgd_solver.cpp:136] Iteration 54800, lr = 0.014375, m = 0.9
I0801 13:26:40.430138 12903 solver.cpp:353] Iteration 54900 (63.0224 iter/s, 1.58674s/100 iter), loss = 0.000933578
I0801 13:26:40.430164 12903 solver.cpp:375]     Train net output #0: loss = 0.000934284 (* 1 = 0.000934284 loss)
I0801 13:26:40.430171 12903 sgd_solver.cpp:136] Iteration 54900, lr = 0.0142187, m = 0.9
I0801 13:26:41.978225 12903 solver.cpp:550] Iteration 55000, Testing net (#0)
I0801 13:26:42.799010 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.927942
I0801 13:26:42.799029 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 13:26:42.799034 12903 solver.cpp:635]     Test net output #2: loss = 0.254924 (* 1 = 0.254924 loss)
I0801 13:26:42.799048 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.820801s
I0801 13:26:42.814617 12903 solver.cpp:353] Iteration 55000 (41.9391 iter/s, 2.38441s/100 iter), loss = 0.000735547
I0801 13:26:42.814635 12903 solver.cpp:375]     Train net output #0: loss = 0.000736253 (* 1 = 0.000736253 loss)
I0801 13:26:42.814640 12903 sgd_solver.cpp:136] Iteration 55000, lr = 0.0140625, m = 0.9
I0801 13:26:44.381736 12903 solver.cpp:353] Iteration 55100 (63.8134 iter/s, 1.56707s/100 iter), loss = 0.00125402
I0801 13:26:44.381788 12903 solver.cpp:375]     Train net output #0: loss = 0.00125473 (* 1 = 0.00125473 loss)
I0801 13:26:44.381796 12903 sgd_solver.cpp:136] Iteration 55100, lr = 0.0139063, m = 0.9
I0801 13:26:45.950969 12903 solver.cpp:353] Iteration 55200 (63.7274 iter/s, 1.56918s/100 iter), loss = 0.00140519
I0801 13:26:45.950996 12903 solver.cpp:375]     Train net output #0: loss = 0.00140589 (* 1 = 0.00140589 loss)
I0801 13:26:45.951004 12903 sgd_solver.cpp:136] Iteration 55200, lr = 0.01375, m = 0.9
I0801 13:26:47.527004 12903 solver.cpp:353] Iteration 55300 (63.4523 iter/s, 1.57599s/100 iter), loss = 0.00249049
I0801 13:26:47.527031 12903 solver.cpp:375]     Train net output #0: loss = 0.0024912 (* 1 = 0.0024912 loss)
I0801 13:26:47.527037 12903 sgd_solver.cpp:136] Iteration 55300, lr = 0.0135938, m = 0.9
I0801 13:26:49.114117 12903 solver.cpp:353] Iteration 55400 (63.0095 iter/s, 1.58706s/100 iter), loss = 0.00128893
I0801 13:26:49.114142 12903 solver.cpp:375]     Train net output #0: loss = 0.00128964 (* 1 = 0.00128964 loss)
I0801 13:26:49.114147 12903 sgd_solver.cpp:136] Iteration 55400, lr = 0.0134375, m = 0.9
I0801 13:26:50.679811 12903 solver.cpp:353] Iteration 55500 (63.8714 iter/s, 1.56565s/100 iter), loss = 0.000798003
I0801 13:26:50.679836 12903 solver.cpp:375]     Train net output #0: loss = 0.000798709 (* 1 = 0.000798709 loss)
I0801 13:26:50.679842 12903 sgd_solver.cpp:136] Iteration 55500, lr = 0.0132813, m = 0.9
I0801 13:26:52.248179 12903 solver.cpp:353] Iteration 55600 (63.7627 iter/s, 1.56832s/100 iter), loss = 0.000996815
I0801 13:26:52.248208 12903 solver.cpp:375]     Train net output #0: loss = 0.000997521 (* 1 = 0.000997521 loss)
I0801 13:26:52.248215 12903 sgd_solver.cpp:136] Iteration 55600, lr = 0.013125, m = 0.9
I0801 13:26:53.832119 12903 solver.cpp:353] Iteration 55700 (63.1356 iter/s, 1.58389s/100 iter), loss = 0.000572216
I0801 13:26:53.832146 12903 solver.cpp:375]     Train net output #0: loss = 0.000572923 (* 1 = 0.000572923 loss)
I0801 13:26:53.832152 12903 sgd_solver.cpp:136] Iteration 55700, lr = 0.0129687, m = 0.9
I0801 13:26:55.413475 12903 solver.cpp:353] Iteration 55800 (63.2388 iter/s, 1.58131s/100 iter), loss = 0.000747486
I0801 13:26:55.413501 12903 solver.cpp:375]     Train net output #0: loss = 0.000748193 (* 1 = 0.000748193 loss)
I0801 13:26:55.413506 12903 sgd_solver.cpp:136] Iteration 55800, lr = 0.0128125, m = 0.9
I0801 13:26:56.991545 12903 solver.cpp:353] Iteration 55900 (63.3705 iter/s, 1.57802s/100 iter), loss = 0.0019735
I0801 13:26:56.991606 12903 solver.cpp:375]     Train net output #0: loss = 0.0019742 (* 1 = 0.0019742 loss)
I0801 13:26:56.991626 12903 sgd_solver.cpp:136] Iteration 55900, lr = 0.0126562, m = 0.9
I0801 13:26:58.550799 12903 solver.cpp:550] Iteration 56000, Testing net (#0)
I0801 13:26:59.364428 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.927354
I0801 13:26:59.364447 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 13:26:59.364454 12903 solver.cpp:635]     Test net output #2: loss = 0.258522 (* 1 = 0.258522 loss)
I0801 13:26:59.364471 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.813649s
I0801 13:26:59.379948 12903 solver.cpp:353] Iteration 56000 (41.8702 iter/s, 2.38833s/100 iter), loss = 0.000184903
I0801 13:26:59.379966 12903 solver.cpp:375]     Train net output #0: loss = 0.000185611 (* 1 = 0.000185611 loss)
I0801 13:26:59.379971 12903 sgd_solver.cpp:136] Iteration 56000, lr = 0.0125, m = 0.9
I0801 13:27:00.291280 12870 data_reader.cpp:264] Starting prefetch of epoch 7
I0801 13:27:00.964884 12903 solver.cpp:353] Iteration 56100 (63.0962 iter/s, 1.58488s/100 iter), loss = 0.00215221
I0801 13:27:00.964980 12903 solver.cpp:375]     Train net output #0: loss = 0.00215292 (* 1 = 0.00215292 loss)
I0801 13:27:00.964988 12903 sgd_solver.cpp:136] Iteration 56100, lr = 0.0123438, m = 0.9
I0801 13:27:02.539563 12903 solver.cpp:353] Iteration 56200 (63.5069 iter/s, 1.57463s/100 iter), loss = 0.00107203
I0801 13:27:02.539590 12903 solver.cpp:375]     Train net output #0: loss = 0.00107274 (* 1 = 0.00107274 loss)
I0801 13:27:02.539597 12903 sgd_solver.cpp:136] Iteration 56200, lr = 0.0121875, m = 0.9
I0801 13:27:04.119421 12903 solver.cpp:353] Iteration 56300 (63.2988 iter/s, 1.57981s/100 iter), loss = 0.000766076
I0801 13:27:04.119469 12903 solver.cpp:375]     Train net output #0: loss = 0.000766782 (* 1 = 0.000766782 loss)
I0801 13:27:04.119483 12903 sgd_solver.cpp:136] Iteration 56300, lr = 0.0120313, m = 0.9
I0801 13:27:05.693104 12903 solver.cpp:353] Iteration 56400 (63.5472 iter/s, 1.57363s/100 iter), loss = 0.00143096
I0801 13:27:05.693131 12903 solver.cpp:375]     Train net output #0: loss = 0.00143167 (* 1 = 0.00143167 loss)
I0801 13:27:05.693136 12903 sgd_solver.cpp:136] Iteration 56400, lr = 0.011875, m = 0.9
I0801 13:27:07.270184 12903 solver.cpp:353] Iteration 56500 (63.4104 iter/s, 1.57703s/100 iter), loss = 0.00115065
I0801 13:27:07.270212 12903 solver.cpp:375]     Train net output #0: loss = 0.00115136 (* 1 = 0.00115136 loss)
I0801 13:27:07.270218 12903 sgd_solver.cpp:136] Iteration 56500, lr = 0.0117188, m = 0.9
I0801 13:27:08.853374 12903 solver.cpp:353] Iteration 56600 (63.1657 iter/s, 1.58314s/100 iter), loss = 0.00097385
I0801 13:27:08.853427 12903 solver.cpp:375]     Train net output #0: loss = 0.000974556 (* 1 = 0.000974556 loss)
I0801 13:27:08.853443 12903 sgd_solver.cpp:136] Iteration 56600, lr = 0.0115625, m = 0.9
I0801 13:27:10.453546 12903 solver.cpp:353] Iteration 56700 (62.4951 iter/s, 1.60012s/100 iter), loss = 0.000849945
I0801 13:27:10.453572 12903 solver.cpp:375]     Train net output #0: loss = 0.000850651 (* 1 = 0.000850651 loss)
I0801 13:27:10.453578 12903 sgd_solver.cpp:136] Iteration 56700, lr = 0.0114062, m = 0.9
I0801 13:27:12.032438 12903 solver.cpp:353] Iteration 56800 (63.3375 iter/s, 1.57884s/100 iter), loss = 0.000787578
I0801 13:27:12.032464 12903 solver.cpp:375]     Train net output #0: loss = 0.000788284 (* 1 = 0.000788284 loss)
I0801 13:27:12.032470 12903 sgd_solver.cpp:136] Iteration 56800, lr = 0.01125, m = 0.9
I0801 13:27:13.604084 12903 solver.cpp:353] Iteration 56900 (63.6297 iter/s, 1.57159s/100 iter), loss = 0.000532971
I0801 13:27:13.604112 12903 solver.cpp:375]     Train net output #0: loss = 0.000533676 (* 1 = 0.000533676 loss)
I0801 13:27:13.604118 12903 sgd_solver.cpp:136] Iteration 56900, lr = 0.0110937, m = 0.9
I0801 13:27:15.184379 12903 solver.cpp:550] Iteration 57000, Testing net (#0)
I0801 13:27:15.999169 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.924707
I0801 13:27:15.999189 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 13:27:15.999197 12903 solver.cpp:635]     Test net output #2: loss = 0.267246 (* 1 = 0.267246 loss)
I0801 13:27:15.999213 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.814812s
I0801 13:27:16.014741 12903 solver.cpp:353] Iteration 57000 (41.4837 iter/s, 2.41059s/100 iter), loss = 0.00408351
I0801 13:27:16.014758 12903 solver.cpp:375]     Train net output #0: loss = 0.00408422 (* 1 = 0.00408422 loss)
I0801 13:27:16.014765 12903 sgd_solver.cpp:136] Iteration 57000, lr = 0.0109375, m = 0.9
I0801 13:27:17.605005 12903 solver.cpp:353] Iteration 57100 (62.8847 iter/s, 1.59021s/100 iter), loss = 0.00104905
I0801 13:27:17.605034 12903 solver.cpp:375]     Train net output #0: loss = 0.00104976 (* 1 = 0.00104976 loss)
I0801 13:27:17.605041 12903 sgd_solver.cpp:136] Iteration 57100, lr = 0.0107813, m = 0.9
I0801 13:27:19.204416 12903 solver.cpp:353] Iteration 57200 (62.5251 iter/s, 1.59936s/100 iter), loss = 0.00138985
I0801 13:27:19.204468 12903 solver.cpp:375]     Train net output #0: loss = 0.00139056 (* 1 = 0.00139056 loss)
I0801 13:27:19.204494 12903 sgd_solver.cpp:136] Iteration 57200, lr = 0.010625, m = 0.9
I0801 13:27:20.780978 12903 solver.cpp:353] Iteration 57300 (63.4311 iter/s, 1.57651s/100 iter), loss = 0.00123318
I0801 13:27:20.781006 12903 solver.cpp:375]     Train net output #0: loss = 0.00123388 (* 1 = 0.00123388 loss)
I0801 13:27:20.781011 12903 sgd_solver.cpp:136] Iteration 57300, lr = 0.0104688, m = 0.9
I0801 13:27:22.380795 12903 solver.cpp:353] Iteration 57400 (62.5091 iter/s, 1.59977s/100 iter), loss = 0.00179599
I0801 13:27:22.380827 12903 solver.cpp:375]     Train net output #0: loss = 0.00179669 (* 1 = 0.00179669 loss)
I0801 13:27:22.380834 12903 sgd_solver.cpp:136] Iteration 57400, lr = 0.0103125, m = 0.9
I0801 13:27:23.950064 12903 solver.cpp:353] Iteration 57500 (63.726 iter/s, 1.56922s/100 iter), loss = 0.00157718
I0801 13:27:23.950114 12903 solver.cpp:375]     Train net output #0: loss = 0.00157789 (* 1 = 0.00157789 loss)
I0801 13:27:23.950126 12903 sgd_solver.cpp:136] Iteration 57500, lr = 0.0101563, m = 0.9
I0801 13:27:25.526654 12903 solver.cpp:353] Iteration 57600 (63.4299 iter/s, 1.57654s/100 iter), loss = 0.00236302
I0801 13:27:25.526684 12903 solver.cpp:375]     Train net output #0: loss = 0.00236373 (* 1 = 0.00236373 loss)
I0801 13:27:25.526690 12903 sgd_solver.cpp:136] Iteration 57600, lr = 0.01, m = 0.9
I0801 13:27:27.094729 12903 solver.cpp:353] Iteration 57700 (63.7746 iter/s, 1.56802s/100 iter), loss = 0.000587495
I0801 13:27:27.094753 12903 solver.cpp:375]     Train net output #0: loss = 0.000588201 (* 1 = 0.000588201 loss)
I0801 13:27:27.094758 12903 sgd_solver.cpp:136] Iteration 57700, lr = 0.00984375, m = 0.9
I0801 13:27:28.662787 12903 solver.cpp:353] Iteration 57800 (63.7752 iter/s, 1.56801s/100 iter), loss = 0.0011759
I0801 13:27:28.662817 12903 solver.cpp:375]     Train net output #0: loss = 0.00117661 (* 1 = 0.00117661 loss)
I0801 13:27:28.662824 12903 sgd_solver.cpp:136] Iteration 57800, lr = 0.0096875, m = 0.9
I0801 13:27:30.227545 12903 solver.cpp:353] Iteration 57900 (63.9097 iter/s, 1.56471s/100 iter), loss = 0.00141676
I0801 13:27:30.227568 12903 solver.cpp:375]     Train net output #0: loss = 0.00141747 (* 1 = 0.00141747 loss)
I0801 13:27:30.227572 12903 sgd_solver.cpp:136] Iteration 57900, lr = 0.00953125, m = 0.9
I0801 13:27:31.784024 12903 solver.cpp:550] Iteration 58000, Testing net (#0)
I0801 13:27:32.603066 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.919118
I0801 13:27:32.603086 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 13:27:32.603091 12903 solver.cpp:635]     Test net output #2: loss = 0.296162 (* 1 = 0.296162 loss)
I0801 13:27:32.603107 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.819062s
I0801 13:27:32.618710 12903 solver.cpp:353] Iteration 58000 (41.8218 iter/s, 2.3911s/100 iter), loss = 0.00214606
I0801 13:27:32.618728 12903 solver.cpp:375]     Train net output #0: loss = 0.00214677 (* 1 = 0.00214677 loss)
I0801 13:27:32.618734 12903 sgd_solver.cpp:136] Iteration 58000, lr = 0.009375, m = 0.9
I0801 13:27:34.202669 12903 solver.cpp:353] Iteration 58100 (63.135 iter/s, 1.58391s/100 iter), loss = 0.00272546
I0801 13:27:34.202697 12903 solver.cpp:375]     Train net output #0: loss = 0.00272616 (* 1 = 0.00272616 loss)
I0801 13:27:34.202702 12903 sgd_solver.cpp:136] Iteration 58100, lr = 0.00921875, m = 0.9
I0801 13:27:35.775634 12903 solver.cpp:353] Iteration 58200 (63.5762 iter/s, 1.57292s/100 iter), loss = 0.000810932
I0801 13:27:35.775660 12903 solver.cpp:375]     Train net output #0: loss = 0.000811638 (* 1 = 0.000811638 loss)
I0801 13:27:35.775665 12903 sgd_solver.cpp:136] Iteration 58200, lr = 0.0090625, m = 0.9
I0801 13:27:37.348601 12903 solver.cpp:353] Iteration 58300 (63.576 iter/s, 1.57292s/100 iter), loss = 0.00163075
I0801 13:27:37.348628 12903 solver.cpp:375]     Train net output #0: loss = 0.00163146 (* 1 = 0.00163146 loss)
I0801 13:27:37.348634 12903 sgd_solver.cpp:136] Iteration 58300, lr = 0.00890625, m = 0.9
I0801 13:27:38.920658 12903 solver.cpp:353] Iteration 58400 (63.613 iter/s, 1.57201s/100 iter), loss = 0.000631986
I0801 13:27:38.920682 12903 solver.cpp:375]     Train net output #0: loss = 0.000632692 (* 1 = 0.000632692 loss)
I0801 13:27:38.920686 12903 sgd_solver.cpp:136] Iteration 58400, lr = 0.00875, m = 0.9
I0801 13:27:40.490636 12903 solver.cpp:353] Iteration 58500 (63.6972 iter/s, 1.56993s/100 iter), loss = 0.00064692
I0801 13:27:40.490692 12903 solver.cpp:375]     Train net output #0: loss = 0.000647626 (* 1 = 0.000647626 loss)
I0801 13:27:40.490707 12903 sgd_solver.cpp:136] Iteration 58500, lr = 0.00859375, m = 0.9
I0801 13:27:42.051048 12903 solver.cpp:353] Iteration 58600 (64.0876 iter/s, 1.56036s/100 iter), loss = 0.00025714
I0801 13:27:42.051076 12903 solver.cpp:375]     Train net output #0: loss = 0.000257846 (* 1 = 0.000257846 loss)
I0801 13:27:42.051082 12903 sgd_solver.cpp:136] Iteration 58600, lr = 0.0084375, m = 0.9
I0801 13:27:43.628379 12903 solver.cpp:353] Iteration 58700 (63.4002 iter/s, 1.57728s/100 iter), loss = 0.00112536
I0801 13:27:43.628406 12903 solver.cpp:375]     Train net output #0: loss = 0.00112607 (* 1 = 0.00112607 loss)
I0801 13:27:43.628412 12903 sgd_solver.cpp:136] Iteration 58700, lr = 0.00828125, m = 0.9
I0801 13:27:45.188421 12903 solver.cpp:353] Iteration 58800 (64.1029 iter/s, 1.55999s/100 iter), loss = 0.000975752
I0801 13:27:45.188448 12903 solver.cpp:375]     Train net output #0: loss = 0.000976458 (* 1 = 0.000976458 loss)
I0801 13:27:45.188454 12903 sgd_solver.cpp:136] Iteration 58800, lr = 0.008125, m = 0.9
I0801 13:27:46.753273 12903 solver.cpp:353] Iteration 58900 (63.9059 iter/s, 1.5648s/100 iter), loss = 0.00208931
I0801 13:27:46.753298 12903 solver.cpp:375]     Train net output #0: loss = 0.00209001 (* 1 = 0.00209001 loss)
I0801 13:27:46.753304 12903 sgd_solver.cpp:136] Iteration 58900, lr = 0.00796875, m = 0.9
I0801 13:27:48.324371 12903 solver.cpp:550] Iteration 59000, Testing net (#0)
I0801 13:27:49.138923 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.91706
I0801 13:27:49.138942 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 13:27:49.138950 12903 solver.cpp:635]     Test net output #2: loss = 0.292 (* 1 = 0.292 loss)
I0801 13:27:49.138974 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.814578s
I0801 13:27:49.154676 12903 solver.cpp:353] Iteration 59000 (41.6436 iter/s, 2.40133s/100 iter), loss = 0.0013726
I0801 13:27:49.154700 12903 solver.cpp:375]     Train net output #0: loss = 0.00137331 (* 1 = 0.00137331 loss)
I0801 13:27:49.154724 12903 sgd_solver.cpp:136] Iteration 59000, lr = 0.0078125, m = 0.9
I0801 13:27:50.717939 12903 solver.cpp:353] Iteration 59100 (63.9708 iter/s, 1.56321s/100 iter), loss = 0.00104596
I0801 13:27:50.717962 12903 solver.cpp:375]     Train net output #0: loss = 0.00104667 (* 1 = 0.00104667 loss)
I0801 13:27:50.717967 12903 sgd_solver.cpp:136] Iteration 59100, lr = 0.00765625, m = 0.9
I0801 13:27:52.296286 12903 solver.cpp:353] Iteration 59200 (63.3595 iter/s, 1.5783s/100 iter), loss = 0.00116725
I0801 13:27:52.296335 12903 solver.cpp:375]     Train net output #0: loss = 0.00116796 (* 1 = 0.00116796 loss)
I0801 13:27:52.296349 12903 sgd_solver.cpp:136] Iteration 59200, lr = 0.0075, m = 0.9
I0801 13:27:53.867784 12903 solver.cpp:353] Iteration 59300 (63.6357 iter/s, 1.57145s/100 iter), loss = 0.00149698
I0801 13:27:53.867810 12903 solver.cpp:375]     Train net output #0: loss = 0.00149768 (* 1 = 0.00149768 loss)
I0801 13:27:53.867815 12903 sgd_solver.cpp:136] Iteration 59300, lr = 0.00734375, m = 0.9
I0801 13:27:55.424927 12903 solver.cpp:353] Iteration 59400 (64.2221 iter/s, 1.5571s/100 iter), loss = 0.0021473
I0801 13:27:55.424949 12903 solver.cpp:375]     Train net output #0: loss = 0.00214801 (* 1 = 0.00214801 loss)
I0801 13:27:55.424953 12903 sgd_solver.cpp:136] Iteration 59400, lr = 0.0071875, m = 0.9
I0801 13:27:57.005421 12903 solver.cpp:353] Iteration 59500 (63.2734 iter/s, 1.58044s/100 iter), loss = 0.000855981
I0801 13:27:57.005475 12903 solver.cpp:375]     Train net output #0: loss = 0.000856685 (* 1 = 0.000856685 loss)
I0801 13:27:57.005491 12903 sgd_solver.cpp:136] Iteration 59500, lr = 0.00703125, m = 0.9
I0801 13:27:58.578826 12903 solver.cpp:353] Iteration 59600 (63.5586 iter/s, 1.57335s/100 iter), loss = 0.00139616
I0801 13:27:58.578878 12903 solver.cpp:375]     Train net output #0: loss = 0.00139686 (* 1 = 0.00139686 loss)
I0801 13:27:58.578892 12903 sgd_solver.cpp:136] Iteration 59600, lr = 0.006875, m = 0.9
I0801 13:28:00.164309 12903 solver.cpp:353] Iteration 59700 (63.0742 iter/s, 1.58543s/100 iter), loss = 0.00177853
I0801 13:28:00.164338 12903 solver.cpp:375]     Train net output #0: loss = 0.00177924 (* 1 = 0.00177924 loss)
I0801 13:28:00.164345 12903 sgd_solver.cpp:136] Iteration 59700, lr = 0.00671875, m = 0.9
I0801 13:28:01.735613 12903 solver.cpp:353] Iteration 59800 (63.6434 iter/s, 1.57126s/100 iter), loss = 0.00134299
I0801 13:28:01.735636 12903 solver.cpp:375]     Train net output #0: loss = 0.00134369 (* 1 = 0.00134369 loss)
I0801 13:28:01.735641 12903 sgd_solver.cpp:136] Iteration 59800, lr = 0.0065625, m = 0.9
I0801 13:28:03.315631 12903 solver.cpp:353] Iteration 59900 (63.2923 iter/s, 1.57997s/100 iter), loss = 0.00115282
I0801 13:28:03.315722 12903 solver.cpp:375]     Train net output #0: loss = 0.00115353 (* 1 = 0.00115353 loss)
I0801 13:28:03.315729 12903 sgd_solver.cpp:136] Iteration 59900, lr = 0.00640625, m = 0.9
I0801 13:28:04.871706 12903 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/cifar10_jacintonet11v2_iter_60000.caffemodel
I0801 13:28:04.879818 12903 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/cifar10_jacintonet11v2_iter_60000.solverstate
I0801 13:28:04.883352 12903 solver.cpp:550] Iteration 60000, Testing net (#0)
I0801 13:28:05.688143 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.917942
I0801 13:28:05.688168 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 13:28:05.688174 12903 solver.cpp:635]     Test net output #2: loss = 0.308536 (* 1 = 0.308536 loss)
I0801 13:28:05.688199 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.80482s
I0801 13:28:05.705426 12903 solver.cpp:353] Iteration 60000 (41.8459 iter/s, 2.38972s/100 iter), loss = 0.00178693
I0801 13:28:05.705451 12903 solver.cpp:375]     Train net output #0: loss = 0.00178763 (* 1 = 0.00178763 loss)
I0801 13:28:05.705456 12903 sgd_solver.cpp:136] Iteration 60000, lr = 0.00625, m = 0.9
I0801 13:28:07.269307 12903 solver.cpp:353] Iteration 60100 (63.9454 iter/s, 1.56383s/100 iter), loss = 0.00143045
I0801 13:28:07.269335 12903 solver.cpp:375]     Train net output #0: loss = 0.00143115 (* 1 = 0.00143115 loss)
I0801 13:28:07.269340 12903 sgd_solver.cpp:136] Iteration 60100, lr = 0.00609375, m = 0.9
I0801 13:28:08.840478 12903 solver.cpp:353] Iteration 60200 (63.6487 iter/s, 1.57112s/100 iter), loss = 0.00160128
I0801 13:28:08.840503 12903 solver.cpp:375]     Train net output #0: loss = 0.00160198 (* 1 = 0.00160198 loss)
I0801 13:28:08.840509 12903 sgd_solver.cpp:136] Iteration 60200, lr = 0.0059375, m = 0.9
I0801 13:28:10.395427 12903 solver.cpp:353] Iteration 60300 (64.3128 iter/s, 1.5549s/100 iter), loss = 0.00156852
I0801 13:28:10.395452 12903 solver.cpp:375]     Train net output #0: loss = 0.00156922 (* 1 = 0.00156922 loss)
I0801 13:28:10.395458 12903 sgd_solver.cpp:136] Iteration 60300, lr = 0.00578125, m = 0.9
I0801 13:28:11.962107 12903 solver.cpp:353] Iteration 60400 (63.8313 iter/s, 1.56663s/100 iter), loss = 0.000430844
I0801 13:28:11.962137 12903 solver.cpp:375]     Train net output #0: loss = 0.000431549 (* 1 = 0.000431549 loss)
I0801 13:28:11.962146 12903 sgd_solver.cpp:136] Iteration 60400, lr = 0.005625, m = 0.9
I0801 13:28:13.542696 12903 solver.cpp:353] Iteration 60500 (63.2695 iter/s, 1.58054s/100 iter), loss = 0.0015867
I0801 13:28:13.542747 12903 solver.cpp:375]     Train net output #0: loss = 0.0015874 (* 1 = 0.0015874 loss)
I0801 13:28:13.542762 12903 sgd_solver.cpp:136] Iteration 60500, lr = 0.00546875, m = 0.9
I0801 13:28:15.116317 12903 solver.cpp:353] Iteration 60600 (63.5498 iter/s, 1.57357s/100 iter), loss = 0.00137623
I0801 13:28:15.116345 12903 solver.cpp:375]     Train net output #0: loss = 0.00137693 (* 1 = 0.00137693 loss)
I0801 13:28:15.116351 12903 sgd_solver.cpp:136] Iteration 60600, lr = 0.0053125, m = 0.9
I0801 13:28:15.166486 12870 data_reader.cpp:264] Starting prefetch of epoch 8
I0801 13:28:16.690309 12903 solver.cpp:353] Iteration 60700 (63.5349 iter/s, 1.57394s/100 iter), loss = 0.00136159
I0801 13:28:16.690337 12903 solver.cpp:375]     Train net output #0: loss = 0.00136229 (* 1 = 0.00136229 loss)
I0801 13:28:16.690343 12903 sgd_solver.cpp:136] Iteration 60700, lr = 0.00515625, m = 0.9
I0801 13:28:18.266511 12903 solver.cpp:353] Iteration 60800 (63.4456 iter/s, 1.57615s/100 iter), loss = 0.00147759
I0801 13:28:18.266540 12903 solver.cpp:375]     Train net output #0: loss = 0.00147829 (* 1 = 0.00147829 loss)
I0801 13:28:18.266546 12903 sgd_solver.cpp:136] Iteration 60800, lr = 0.005, m = 0.9
I0801 13:28:19.850352 12903 solver.cpp:353] Iteration 60900 (63.1396 iter/s, 1.58379s/100 iter), loss = 0.000706712
I0801 13:28:19.850378 12903 solver.cpp:375]     Train net output #0: loss = 0.000707416 (* 1 = 0.000707416 loss)
I0801 13:28:19.850412 12903 sgd_solver.cpp:136] Iteration 60900, lr = 0.00484375, m = 0.9
I0801 13:28:21.396302 12903 solver.cpp:550] Iteration 61000, Testing net (#0)
I0801 13:28:22.213696 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.915883
I0801 13:28:22.213717 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 13:28:22.213722 12903 solver.cpp:635]     Test net output #2: loss = 0.298915 (* 1 = 0.298915 loss)
I0801 13:28:22.213737 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.817413s
I0801 13:28:22.229301 12903 solver.cpp:353] Iteration 61000 (42.0366 iter/s, 2.37888s/100 iter), loss = 0.0011437
I0801 13:28:22.229318 12903 solver.cpp:375]     Train net output #0: loss = 0.0011444 (* 1 = 0.0011444 loss)
I0801 13:28:22.229322 12903 sgd_solver.cpp:136] Iteration 61000, lr = 0.0046875, m = 0.9
I0801 13:28:23.802240 12903 solver.cpp:353] Iteration 61100 (63.5773 iter/s, 1.57289s/100 iter), loss = 0.000591563
I0801 13:28:23.802266 12903 solver.cpp:375]     Train net output #0: loss = 0.000592267 (* 1 = 0.000592267 loss)
I0801 13:28:23.802273 12903 sgd_solver.cpp:136] Iteration 61100, lr = 0.00453125, m = 0.9
I0801 13:28:25.358064 12903 solver.cpp:353] Iteration 61200 (64.2766 iter/s, 1.55578s/100 iter), loss = 0.000864033
I0801 13:28:25.358089 12903 solver.cpp:375]     Train net output #0: loss = 0.000864738 (* 1 = 0.000864738 loss)
I0801 13:28:25.358095 12903 sgd_solver.cpp:136] Iteration 61200, lr = 0.004375, m = 0.9
I0801 13:28:26.943308 12903 solver.cpp:353] Iteration 61300 (63.0836 iter/s, 1.5852s/100 iter), loss = 0.00127265
I0801 13:28:26.943334 12903 solver.cpp:375]     Train net output #0: loss = 0.00127336 (* 1 = 0.00127336 loss)
I0801 13:28:26.943341 12903 sgd_solver.cpp:136] Iteration 61300, lr = 0.00421875, m = 0.9
I0801 13:28:28.515003 12903 solver.cpp:353] Iteration 61400 (63.6277 iter/s, 1.57164s/100 iter), loss = 0.00112791
I0801 13:28:28.515030 12903 solver.cpp:375]     Train net output #0: loss = 0.00112861 (* 1 = 0.00112861 loss)
I0801 13:28:28.515036 12903 sgd_solver.cpp:136] Iteration 61400, lr = 0.0040625, m = 0.9
I0801 13:28:30.115522 12903 solver.cpp:353] Iteration 61500 (62.4816 iter/s, 1.60047s/100 iter), loss = 0.00241912
I0801 13:28:30.115571 12903 solver.cpp:375]     Train net output #0: loss = 0.00241982 (* 1 = 0.00241982 loss)
I0801 13:28:30.115586 12903 sgd_solver.cpp:136] Iteration 61500, lr = 0.00390625, m = 0.9
I0801 13:28:31.675556 12903 solver.cpp:353] Iteration 61600 (64.1033 iter/s, 1.55998s/100 iter), loss = 0.00259818
I0801 13:28:31.675586 12903 solver.cpp:375]     Train net output #0: loss = 0.00259889 (* 1 = 0.00259889 loss)
I0801 13:28:31.675593 12903 sgd_solver.cpp:136] Iteration 61600, lr = 0.00375, m = 0.9
I0801 13:28:33.312621 12903 solver.cpp:353] Iteration 61700 (61.0868 iter/s, 1.63702s/100 iter), loss = 0.000434242
I0801 13:28:33.312649 12903 solver.cpp:375]     Train net output #0: loss = 0.000434947 (* 1 = 0.000434947 loss)
I0801 13:28:33.312656 12903 sgd_solver.cpp:136] Iteration 61700, lr = 0.00359375, m = 0.9
I0801 13:28:34.973466 12903 solver.cpp:353] Iteration 61800 (60.2122 iter/s, 1.66079s/100 iter), loss = 0.000707011
I0801 13:28:34.973562 12903 solver.cpp:375]     Train net output #0: loss = 0.000707716 (* 1 = 0.000707716 loss)
I0801 13:28:34.973569 12903 sgd_solver.cpp:136] Iteration 61800, lr = 0.0034375, m = 0.9
I0801 13:28:36.579821 12903 solver.cpp:353] Iteration 61900 (62.2547 iter/s, 1.6063s/100 iter), loss = 0.00105429
I0801 13:28:36.579849 12903 solver.cpp:375]     Train net output #0: loss = 0.00105499 (* 1 = 0.00105499 loss)
I0801 13:28:36.579855 12903 sgd_solver.cpp:136] Iteration 61900, lr = 0.00328125, m = 0.9
I0801 13:28:38.166905 12903 solver.cpp:550] Iteration 62000, Testing net (#0)
I0801 13:28:39.005867 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.915001
I0801 13:28:39.005887 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995588
I0801 13:28:39.005892 12903 solver.cpp:635]     Test net output #2: loss = 0.315541 (* 1 = 0.315541 loss)
I0801 13:28:39.005906 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.838978s
I0801 13:28:39.021929 12903 solver.cpp:353] Iteration 62000 (40.9495 iter/s, 2.44203s/100 iter), loss = 0.000610828
I0801 13:28:39.021950 12903 solver.cpp:375]     Train net output #0: loss = 0.000611534 (* 1 = 0.000611534 loss)
I0801 13:28:39.021955 12903 sgd_solver.cpp:136] Iteration 62000, lr = 0.003125, m = 0.9
I0801 13:28:40.608515 12903 solver.cpp:353] Iteration 62100 (63.0305 iter/s, 1.58653s/100 iter), loss = 0.000730326
I0801 13:28:40.608541 12903 solver.cpp:375]     Train net output #0: loss = 0.000731031 (* 1 = 0.000731031 loss)
I0801 13:28:40.608547 12903 sgd_solver.cpp:136] Iteration 62100, lr = 0.00296875, m = 0.9
I0801 13:28:42.208557 12903 solver.cpp:353] Iteration 62200 (62.5004 iter/s, 1.59999s/100 iter), loss = 0.00114449
I0801 13:28:42.208611 12903 solver.cpp:375]     Train net output #0: loss = 0.0011452 (* 1 = 0.0011452 loss)
I0801 13:28:42.208626 12903 sgd_solver.cpp:136] Iteration 62200, lr = 0.0028125, m = 0.9
I0801 13:28:43.791429 12903 solver.cpp:353] Iteration 62300 (63.1784 iter/s, 1.58282s/100 iter), loss = 0.00143707
I0801 13:28:43.791462 12903 solver.cpp:375]     Train net output #0: loss = 0.00143778 (* 1 = 0.00143778 loss)
I0801 13:28:43.791468 12903 sgd_solver.cpp:136] Iteration 62300, lr = 0.00265625, m = 0.9
I0801 13:28:45.453290 12903 solver.cpp:353] Iteration 62400 (60.1753 iter/s, 1.66181s/100 iter), loss = 0.00109732
I0801 13:28:45.453316 12903 solver.cpp:375]     Train net output #0: loss = 0.00109803 (* 1 = 0.00109803 loss)
I0801 13:28:45.453322 12903 sgd_solver.cpp:136] Iteration 62400, lr = 0.0025, m = 0.9
I0801 13:28:47.033403 12903 solver.cpp:353] Iteration 62500 (63.2887 iter/s, 1.58006s/100 iter), loss = 0.000554101
I0801 13:28:47.033433 12903 solver.cpp:375]     Train net output #0: loss = 0.000554806 (* 1 = 0.000554806 loss)
I0801 13:28:47.033439 12903 sgd_solver.cpp:136] Iteration 62500, lr = 0.00234375, m = 0.9
I0801 13:28:48.603997 12903 solver.cpp:353] Iteration 62600 (63.6721 iter/s, 1.57055s/100 iter), loss = 0.00202539
I0801 13:28:48.604048 12903 solver.cpp:375]     Train net output #0: loss = 0.00202609 (* 1 = 0.00202609 loss)
I0801 13:28:48.604061 12903 sgd_solver.cpp:136] Iteration 62600, lr = 0.0021875, m = 0.9
I0801 13:28:50.233196 12903 solver.cpp:353] Iteration 62700 (61.3819 iter/s, 1.62914s/100 iter), loss = 0.00161189
I0801 13:28:50.233248 12903 solver.cpp:375]     Train net output #0: loss = 0.0016126 (* 1 = 0.0016126 loss)
I0801 13:28:50.233264 12903 sgd_solver.cpp:136] Iteration 62700, lr = 0.00203125, m = 0.9
I0801 13:28:51.841365 12903 solver.cpp:353] Iteration 62800 (62.1844 iter/s, 1.60812s/100 iter), loss = 0.00134809
I0801 13:28:51.841392 12903 solver.cpp:375]     Train net output #0: loss = 0.00134879 (* 1 = 0.00134879 loss)
I0801 13:28:51.841399 12903 sgd_solver.cpp:136] Iteration 62800, lr = 0.001875, m = 0.9
I0801 13:28:53.471943 12903 solver.cpp:353] Iteration 62900 (61.33 iter/s, 1.63052s/100 iter), loss = 0.00068901
I0801 13:28:53.471971 12903 solver.cpp:375]     Train net output #0: loss = 0.000689715 (* 1 = 0.000689715 loss)
I0801 13:28:53.471976 12903 sgd_solver.cpp:136] Iteration 62900, lr = 0.00171875, m = 0.9
I0801 13:28:55.066890 12903 solver.cpp:550] Iteration 63000, Testing net (#0)
I0801 13:28:55.899088 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.914413
I0801 13:28:55.899114 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995588
I0801 13:28:55.899121 12903 solver.cpp:635]     Test net output #2: loss = 0.317738 (* 1 = 0.317738 loss)
I0801 13:28:55.899147 12903 solver.cpp:305] [MultiGPU] Tests completed in 0.83223s
I0801 13:28:55.918171 12903 solver.cpp:353] Iteration 63000 (40.8805 iter/s, 2.44615s/100 iter), loss = 0.000878555
I0801 13:28:55.918483 12903 solver.cpp:375]     Train net output #0: loss = 0.000879261 (* 1 = 0.000879261 loss)
I0801 13:28:55.918505 12903 sgd_solver.cpp:136] Iteration 63000, lr = 0.0015625, m = 0.9
I0801 13:28:57.508882 12903 solver.cpp:353] Iteration 63100 (62.8668 iter/s, 1.59066s/100 iter), loss = 0.00103529
I0801 13:28:57.508937 12903 solver.cpp:375]     Train net output #0: loss = 0.001036 (* 1 = 0.001036 loss)
I0801 13:28:57.508952 12903 sgd_solver.cpp:136] Iteration 63100, lr = 0.00140625, m = 0.9
I0801 13:28:59.107507 12903 solver.cpp:353] Iteration 63200 (62.5557 iter/s, 1.59857s/100 iter), loss = 0.00124583
I0801 13:28:59.107532 12903 solver.cpp:375]     Train net output #0: loss = 0.00124654 (* 1 = 0.00124654 loss)
I0801 13:28:59.107538 12903 sgd_solver.cpp:136] Iteration 63200, lr = 0.00125, m = 0.9
I0801 13:29:00.724050 12903 solver.cpp:353] Iteration 63300 (61.8624 iter/s, 1.61649s/100 iter), loss = 0.0018219
I0801 13:29:00.724076 12903 solver.cpp:375]     Train net output #0: loss = 0.00182261 (* 1 = 0.00182261 loss)
I0801 13:29:00.724082 12903 sgd_solver.cpp:136] Iteration 63300, lr = 0.00109375, m = 0.9
I0801 13:29:02.369475 12903 solver.cpp:353] Iteration 63400 (60.7765 iter/s, 1.64537s/100 iter), loss = 0.000529927
I0801 13:29:02.369500 12903 solver.cpp:375]     Train net output #0: loss = 0.000530634 (* 1 = 0.000530634 loss)
I0801 13:29:02.369504 12903 sgd_solver.cpp:136] Iteration 63400, lr = 0.000937498, m = 0.9
I0801 13:29:03.969825 12903 solver.cpp:353] Iteration 63500 (62.4882 iter/s, 1.6003s/100 iter), loss = 0.00106112
I0801 13:29:03.969851 12903 solver.cpp:375]     Train net output #0: loss = 0.00106182 (* 1 = 0.00106182 loss)
I0801 13:29:03.969856 12903 sgd_solver.cpp:136] Iteration 63500, lr = 0.00078125, m = 0.9
I0801 13:29:05.552489 12903 solver.cpp:353] Iteration 63600 (63.1867 iter/s, 1.58261s/100 iter), loss = 0.000353943
I0801 13:29:05.552564 12903 solver.cpp:375]     Train net output #0: loss = 0.000354649 (* 1 = 0.000354649 loss)
I0801 13:29:05.552572 12903 sgd_solver.cpp:136] Iteration 63600, lr = 0.000625002, m = 0.9
I0801 13:29:07.173454 12903 solver.cpp:353] Iteration 63700 (61.6936 iter/s, 1.62091s/100 iter), loss = 0.000846336
I0801 13:29:07.173481 12903 solver.cpp:375]     Train net output #0: loss = 0.000847042 (* 1 = 0.000847042 loss)
I0801 13:29:07.173487 12903 sgd_solver.cpp:136] Iteration 63700, lr = 0.000468749, m = 0.9
I0801 13:29:08.748536 12903 solver.cpp:353] Iteration 63800 (63.4907 iter/s, 1.57503s/100 iter), loss = 0.00156873
I0801 13:29:08.748569 12903 solver.cpp:375]     Train net output #0: loss = 0.00156943 (* 1 = 0.00156943 loss)
I0801 13:29:08.748576 12903 sgd_solver.cpp:136] Iteration 63800, lr = 0.000312501, m = 0.9
I0801 13:29:10.339066 12903 solver.cpp:353] Iteration 63900 (62.8741 iter/s, 1.59048s/100 iter), loss = 0.00114089
I0801 13:29:10.339118 12903 solver.cpp:375]     Train net output #0: loss = 0.00114159 (* 1 = 0.00114159 loss)
I0801 13:29:10.339133 12903 sgd_solver.cpp:136] Iteration 63900, lr = 0.000156248, m = 0.9
I0801 13:29:11.957785 12903 solver.cpp:353] Iteration 63999 (61.1615 iter/s, 1.61867s/99 iter), loss = 0.000412225
I0801 13:29:11.957821 12903 solver.cpp:375]     Train net output #0: loss = 0.00041293 (* 1 = 0.00041293 loss)
I0801 13:29:11.957875 12903 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/cifar10_jacintonet11v2_iter_64000.caffemodel
I0801 13:29:11.969826 12903 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/cifar10_jacintonet11v2_iter_64000.solverstate
I0801 13:29:11.980089 12903 solver.cpp:527] Iteration 64000, loss = 0.000956977
I0801 13:29:11.980119 12903 solver.cpp:550] Iteration 64000, Testing net (#0)
I0801 13:29:12.787591 12903 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.916177
I0801 13:29:12.787618 12903 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995294
I0801 13:29:12.787624 12903 solver.cpp:635]     Test net output #2: loss = 0.307259 (* 1 = 0.307259 loss)
I0801 13:29:12.790948 12832 parallel.cpp:73] Root Solver performance on device 0: 60.45 * 22 = 1330 img/sec (64000 itr in 1059 sec)
I0801 13:29:12.790963 12832 parallel.cpp:78]      Solver performance on device 1: 60.45 * 22 = 1330 img/sec (64000 itr in 1059 sec)
I0801 13:29:12.790967 12832 parallel.cpp:78]      Solver performance on device 2: 60.45 * 22 = 1330 img/sec (64000 itr in 1059 sec)
I0801 13:29:12.790969 12832 parallel.cpp:81] Overall multi-GPU performance: 3989.68 img/sec
I0801 13:29:12.890754 12832 caffe.cpp:247] Optimization Done in 17m 42s
I0801 13:29:13.756988  3122 caffe.cpp:608] This is NVCaffe 0.16.3 started at Tue Aug  1 13:29:13 2017
I0801 13:29:13.757109  3122 caffe.cpp:611] CuDNN version: 6021
I0801 13:29:13.757114  3122 caffe.cpp:612] CuBLAS version: 8000
I0801 13:29:13.757117  3122 caffe.cpp:613] CUDA version: 8000
I0801 13:29:13.757120  3122 caffe.cpp:614] CUDA driver version: 8000
I0801 13:29:14.010447  3122 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0801 13:29:14.011023  3122 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0801 13:29:14.011545  3122 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0801 13:29:14.012064  3122 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0801 13:29:14.012074  3122 caffe.cpp:208] Using GPUs 0, 1, 2
I0801 13:29:14.012399  3122 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0801 13:29:14.012737  3122 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0801 13:29:14.013077  3122 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0801 13:29:14.013119  3122 solver.cpp:42] Solver data type: FLOAT
I0801 13:29:14.013149  3122 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: true
iter_size: 1
type: "SGD"
I0801 13:29:14.019997  3122 solver.cpp:77] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/train.prototxt
I0801 13:29:14.020421  3122 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0801 13:29:14.020426  3122 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0801 13:29:14.020450  3122 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0801 13:29:14.020635  3122 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 22
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0801 13:29:14.020740  3122 net.cpp:104] Using FLOAT as default forward math type
I0801 13:29:14.020743  3122 net.cpp:110] Using FLOAT as default backward math type
I0801 13:29:14.020748  3122 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0801 13:29:14.020751  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.020797  3122 net.cpp:184] Created Layer data (0)
I0801 13:29:14.020802  3122 net.cpp:530] data -> data
I0801 13:29:14.020812  3122 net.cpp:530] data -> label
I0801 13:29:14.020841  3122 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0801 13:29:14.020861  3122 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:29:14.022356  3155 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0801 13:29:14.023387  3122 data_layer.cpp:184] [0] ReshapePrefetch 22, 3, 32, 32
I0801 13:29:14.023450  3122 data_layer.cpp:208] [0] Output data size: 22, 3, 32, 32
I0801 13:29:14.023455  3122 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:29:14.023473  3122 net.cpp:245] Setting up data
I0801 13:29:14.023479  3122 net.cpp:252] TRAIN Top shape for layer 0 'data' 22 3 32 32 (67584)
I0801 13:29:14.023484  3122 net.cpp:252] TRAIN Top shape for layer 0 'data' 22 (22)
I0801 13:29:14.023489  3122 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0801 13:29:14.023494  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.023505  3122 net.cpp:184] Created Layer data/bias (1)
I0801 13:29:14.023510  3122 net.cpp:561] data/bias <- data
I0801 13:29:14.023516  3122 net.cpp:530] data/bias -> data/bias
I0801 13:29:14.025501  3122 net.cpp:245] Setting up data/bias
I0801 13:29:14.025511  3122 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 22 3 32 32 (67584)
I0801 13:29:14.025521  3122 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0801 13:29:14.025527  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.025540  3122 net.cpp:184] Created Layer conv1a (2)
I0801 13:29:14.025544  3122 net.cpp:561] conv1a <- data/bias
I0801 13:29:14.025549  3122 net.cpp:530] conv1a -> conv1a
I0801 13:29:14.343989  3122 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.15G, req 0G)
I0801 13:29:14.344012  3122 net.cpp:245] Setting up conv1a
I0801 13:29:14.344019  3122 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 22 32 32 32 (720896)
I0801 13:29:14.344033  3122 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0801 13:29:14.344038  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.344051  3122 net.cpp:184] Created Layer conv1a/bn (3)
I0801 13:29:14.344055  3122 net.cpp:561] conv1a/bn <- conv1a
I0801 13:29:14.344061  3122 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0801 13:29:14.344754  3122 net.cpp:245] Setting up conv1a/bn
I0801 13:29:14.344763  3122 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 22 32 32 32 (720896)
I0801 13:29:14.344779  3122 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0801 13:29:14.344782  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.344790  3122 net.cpp:184] Created Layer conv1a/relu (4)
I0801 13:29:14.344794  3122 net.cpp:561] conv1a/relu <- conv1a
I0801 13:29:14.344799  3122 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0801 13:29:14.344813  3122 net.cpp:245] Setting up conv1a/relu
I0801 13:29:14.344830  3122 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 22 32 32 32 (720896)
I0801 13:29:14.344832  3122 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0801 13:29:14.344835  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.344842  3122 net.cpp:184] Created Layer conv1b (5)
I0801 13:29:14.344844  3122 net.cpp:561] conv1b <- conv1a
I0801 13:29:14.344846  3122 net.cpp:530] conv1b -> conv1b
I0801 13:29:14.352490  3122 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.13G, req 0G)
I0801 13:29:14.352502  3122 net.cpp:245] Setting up conv1b
I0801 13:29:14.352506  3122 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 22 32 32 32 (720896)
I0801 13:29:14.352511  3122 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0801 13:29:14.352514  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.352519  3122 net.cpp:184] Created Layer conv1b/bn (6)
I0801 13:29:14.352522  3122 net.cpp:561] conv1b/bn <- conv1b
I0801 13:29:14.352535  3122 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0801 13:29:14.353173  3122 net.cpp:245] Setting up conv1b/bn
I0801 13:29:14.353181  3122 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 22 32 32 32 (720896)
I0801 13:29:14.353188  3122 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0801 13:29:14.353189  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.353193  3122 net.cpp:184] Created Layer conv1b/relu (7)
I0801 13:29:14.353195  3122 net.cpp:561] conv1b/relu <- conv1b
I0801 13:29:14.353199  3122 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0801 13:29:14.353201  3122 net.cpp:245] Setting up conv1b/relu
I0801 13:29:14.353204  3122 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 22 32 32 32 (720896)
I0801 13:29:14.353205  3122 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0801 13:29:14.353207  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.353212  3122 net.cpp:184] Created Layer pool1 (8)
I0801 13:29:14.353214  3122 net.cpp:561] pool1 <- conv1b
I0801 13:29:14.353216  3122 net.cpp:530] pool1 -> pool1
I0801 13:29:14.353298  3122 net.cpp:245] Setting up pool1
I0801 13:29:14.353305  3122 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 22 32 32 32 (720896)
I0801 13:29:14.353310  3122 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0801 13:29:14.353312  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.353322  3122 net.cpp:184] Created Layer res2a_branch2a (9)
I0801 13:29:14.353325  3122 net.cpp:561] res2a_branch2a <- pool1
I0801 13:29:14.353327  3122 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0801 13:29:14.364796  3122 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0G)
I0801 13:29:14.364809  3122 net.cpp:245] Setting up res2a_branch2a
I0801 13:29:14.364820  3122 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 22 64 32 32 (1441792)
I0801 13:29:14.364830  3122 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0801 13:29:14.364833  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.364837  3122 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0801 13:29:14.364840  3122 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0801 13:29:14.364842  3122 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0801 13:29:14.365476  3122 net.cpp:245] Setting up res2a_branch2a/bn
I0801 13:29:14.365484  3122 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 22 64 32 32 (1441792)
I0801 13:29:14.365490  3122 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0801 13:29:14.365492  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.365496  3122 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0801 13:29:14.365499  3122 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0801 13:29:14.365501  3122 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0801 13:29:14.365504  3122 net.cpp:245] Setting up res2a_branch2a/relu
I0801 13:29:14.365506  3122 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 22 64 32 32 (1441792)
I0801 13:29:14.365509  3122 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0801 13:29:14.365511  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.365517  3122 net.cpp:184] Created Layer res2a_branch2b (12)
I0801 13:29:14.365520  3122 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0801 13:29:14.365523  3122 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0801 13:29:14.373082  3122 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.1G, req 0G)
I0801 13:29:14.373102  3122 net.cpp:245] Setting up res2a_branch2b
I0801 13:29:14.373126  3122 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 22 64 32 32 (1441792)
I0801 13:29:14.373136  3122 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0801 13:29:14.373143  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.373157  3122 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0801 13:29:14.373162  3122 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0801 13:29:14.373165  3122 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0801 13:29:14.373837  3122 net.cpp:245] Setting up res2a_branch2b/bn
I0801 13:29:14.373845  3122 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 22 64 32 32 (1441792)
I0801 13:29:14.373855  3122 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0801 13:29:14.373862  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.373867  3122 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0801 13:29:14.373872  3122 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0801 13:29:14.373878  3122 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0801 13:29:14.373884  3122 net.cpp:245] Setting up res2a_branch2b/relu
I0801 13:29:14.373889  3122 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 22 64 32 32 (1441792)
I0801 13:29:14.373894  3122 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0801 13:29:14.373898  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.373908  3122 net.cpp:184] Created Layer pool2 (15)
I0801 13:29:14.373913  3122 net.cpp:561] pool2 <- res2a_branch2b
I0801 13:29:14.373916  3122 net.cpp:530] pool2 -> pool2
I0801 13:29:14.373983  3122 net.cpp:245] Setting up pool2
I0801 13:29:14.373989  3122 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 22 64 16 16 (360448)
I0801 13:29:14.373993  3122 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0801 13:29:14.373998  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.374011  3122 net.cpp:184] Created Layer res3a_branch2a (16)
I0801 13:29:14.374014  3122 net.cpp:561] res3a_branch2a <- pool2
I0801 13:29:14.374017  3122 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0801 13:29:14.385411  3122 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 8.09G, req 0G)
I0801 13:29:14.385426  3122 net.cpp:245] Setting up res3a_branch2a
I0801 13:29:14.385432  3122 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 22 128 16 16 (720896)
I0801 13:29:14.385442  3122 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0801 13:29:14.385447  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.385457  3122 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0801 13:29:14.385462  3122 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0801 13:29:14.385464  3122 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0801 13:29:14.386086  3122 net.cpp:245] Setting up res3a_branch2a/bn
I0801 13:29:14.386095  3122 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 22 128 16 16 (720896)
I0801 13:29:14.386106  3122 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0801 13:29:14.386111  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.386117  3122 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0801 13:29:14.386121  3122 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0801 13:29:14.386127  3122 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0801 13:29:14.386133  3122 net.cpp:245] Setting up res3a_branch2a/relu
I0801 13:29:14.386139  3122 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 22 128 16 16 (720896)
I0801 13:29:14.386143  3122 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0801 13:29:14.386157  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.386168  3122 net.cpp:184] Created Layer res3a_branch2b (19)
I0801 13:29:14.386173  3122 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0801 13:29:14.386175  3122 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0801 13:29:14.390890  3122 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.08G, req 0G)
I0801 13:29:14.390902  3122 net.cpp:245] Setting up res3a_branch2b
I0801 13:29:14.390908  3122 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 22 128 16 16 (720896)
I0801 13:29:14.390915  3122 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0801 13:29:14.390921  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.390928  3122 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0801 13:29:14.390933  3122 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0801 13:29:14.390938  3122 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0801 13:29:14.391541  3122 net.cpp:245] Setting up res3a_branch2b/bn
I0801 13:29:14.391548  3122 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 22 128 16 16 (720896)
I0801 13:29:14.391557  3122 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0801 13:29:14.391562  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.391567  3122 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0801 13:29:14.391571  3122 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0801 13:29:14.391577  3122 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0801 13:29:14.391583  3122 net.cpp:245] Setting up res3a_branch2b/relu
I0801 13:29:14.391588  3122 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 22 128 16 16 (720896)
I0801 13:29:14.391593  3122 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0801 13:29:14.391597  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.391604  3122 net.cpp:184] Created Layer pool3 (22)
I0801 13:29:14.391609  3122 net.cpp:561] pool3 <- res3a_branch2b
I0801 13:29:14.391614  3122 net.cpp:530] pool3 -> pool3
I0801 13:29:14.391680  3122 net.cpp:245] Setting up pool3
I0801 13:29:14.391685  3122 net.cpp:252] TRAIN Top shape for layer 22 'pool3' 22 128 16 16 (720896)
I0801 13:29:14.391690  3122 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0801 13:29:14.391695  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.391703  3122 net.cpp:184] Created Layer res4a_branch2a (23)
I0801 13:29:14.391707  3122 net.cpp:561] res4a_branch2a <- pool3
I0801 13:29:14.391711  3122 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0801 13:29:14.410507  3122 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.05G, req 0G)
I0801 13:29:14.410531  3122 net.cpp:245] Setting up res4a_branch2a
I0801 13:29:14.410538  3122 net.cpp:252] TRAIN Top shape for layer 23 'res4a_branch2a' 22 256 16 16 (1441792)
I0801 13:29:14.410547  3122 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0801 13:29:14.410552  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.410568  3122 net.cpp:184] Created Layer res4a_branch2a/bn (24)
I0801 13:29:14.410573  3122 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0801 13:29:14.410581  3122 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0801 13:29:14.411303  3122 net.cpp:245] Setting up res4a_branch2a/bn
I0801 13:29:14.411311  3122 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a/bn' 22 256 16 16 (1441792)
I0801 13:29:14.411317  3122 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0801 13:29:14.411331  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.411335  3122 net.cpp:184] Created Layer res4a_branch2a/relu (25)
I0801 13:29:14.411339  3122 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0801 13:29:14.411341  3122 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0801 13:29:14.411346  3122 net.cpp:245] Setting up res4a_branch2a/relu
I0801 13:29:14.411352  3122 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/relu' 22 256 16 16 (1441792)
I0801 13:29:14.411357  3122 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0801 13:29:14.411360  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.411370  3122 net.cpp:184] Created Layer res4a_branch2b (26)
I0801 13:29:14.411373  3122 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0801 13:29:14.411375  3122 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0801 13:29:14.420203  3122 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.04G, req 0G)
I0801 13:29:14.420214  3122 net.cpp:245] Setting up res4a_branch2b
I0801 13:29:14.420219  3122 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2b' 22 256 16 16 (1441792)
I0801 13:29:14.420223  3122 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0801 13:29:14.420227  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.420230  3122 net.cpp:184] Created Layer res4a_branch2b/bn (27)
I0801 13:29:14.420233  3122 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0801 13:29:14.420235  3122 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0801 13:29:14.420864  3122 net.cpp:245] Setting up res4a_branch2b/bn
I0801 13:29:14.420872  3122 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b/bn' 22 256 16 16 (1441792)
I0801 13:29:14.420878  3122 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0801 13:29:14.420881  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.420884  3122 net.cpp:184] Created Layer res4a_branch2b/relu (28)
I0801 13:29:14.420886  3122 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0801 13:29:14.420888  3122 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0801 13:29:14.420892  3122 net.cpp:245] Setting up res4a_branch2b/relu
I0801 13:29:14.420894  3122 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/relu' 22 256 16 16 (1441792)
I0801 13:29:14.420897  3122 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0801 13:29:14.420898  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.420905  3122 net.cpp:184] Created Layer pool4 (29)
I0801 13:29:14.420909  3122 net.cpp:561] pool4 <- res4a_branch2b
I0801 13:29:14.420913  3122 net.cpp:530] pool4 -> pool4
I0801 13:29:14.420981  3122 net.cpp:245] Setting up pool4
I0801 13:29:14.420986  3122 net.cpp:252] TRAIN Top shape for layer 29 'pool4' 22 256 8 8 (360448)
I0801 13:29:14.420989  3122 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0801 13:29:14.420991  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.420997  3122 net.cpp:184] Created Layer res5a_branch2a (30)
I0801 13:29:14.421000  3122 net.cpp:561] res5a_branch2a <- pool4
I0801 13:29:14.421002  3122 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0801 13:29:14.464953  3122 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.02G, req 0.01G)
I0801 13:29:14.464972  3122 net.cpp:245] Setting up res5a_branch2a
I0801 13:29:14.464977  3122 net.cpp:252] TRAIN Top shape for layer 30 'res5a_branch2a' 22 512 8 8 (720896)
I0801 13:29:14.464983  3122 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0801 13:29:14.464987  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.465013  3122 net.cpp:184] Created Layer res5a_branch2a/bn (31)
I0801 13:29:14.465018  3122 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0801 13:29:14.465023  3122 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0801 13:29:14.465677  3122 net.cpp:245] Setting up res5a_branch2a/bn
I0801 13:29:14.465684  3122 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a/bn' 22 512 8 8 (720896)
I0801 13:29:14.465689  3122 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0801 13:29:14.465692  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.465697  3122 net.cpp:184] Created Layer res5a_branch2a/relu (32)
I0801 13:29:14.465698  3122 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0801 13:29:14.465701  3122 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0801 13:29:14.465704  3122 net.cpp:245] Setting up res5a_branch2a/relu
I0801 13:29:14.465708  3122 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/relu' 22 512 8 8 (720896)
I0801 13:29:14.465709  3122 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0801 13:29:14.465711  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.465721  3122 net.cpp:184] Created Layer res5a_branch2b (33)
I0801 13:29:14.465726  3122 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0801 13:29:14.465730  3122 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0801 13:29:14.485797  3122 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 7 4 3  (limit 8G, req 0.01G)
I0801 13:29:14.485812  3122 net.cpp:245] Setting up res5a_branch2b
I0801 13:29:14.485821  3122 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2b' 22 512 8 8 (720896)
I0801 13:29:14.485831  3122 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0801 13:29:14.485837  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.485846  3122 net.cpp:184] Created Layer res5a_branch2b/bn (34)
I0801 13:29:14.485848  3122 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0801 13:29:14.485851  3122 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0801 13:29:14.486500  3122 net.cpp:245] Setting up res5a_branch2b/bn
I0801 13:29:14.486508  3122 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b/bn' 22 512 8 8 (720896)
I0801 13:29:14.486517  3122 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0801 13:29:14.486523  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.486529  3122 net.cpp:184] Created Layer res5a_branch2b/relu (35)
I0801 13:29:14.486534  3122 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0801 13:29:14.486539  3122 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0801 13:29:14.486546  3122 net.cpp:245] Setting up res5a_branch2b/relu
I0801 13:29:14.486551  3122 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/relu' 22 512 8 8 (720896)
I0801 13:29:14.486555  3122 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0801 13:29:14.486560  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.486567  3122 net.cpp:184] Created Layer pool5 (36)
I0801 13:29:14.486572  3122 net.cpp:561] pool5 <- res5a_branch2b
I0801 13:29:14.486577  3122 net.cpp:530] pool5 -> pool5
I0801 13:29:14.486608  3122 net.cpp:245] Setting up pool5
I0801 13:29:14.486613  3122 net.cpp:252] TRAIN Top shape for layer 36 'pool5' 22 512 1 1 (11264)
I0801 13:29:14.486616  3122 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0801 13:29:14.486621  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.486629  3122 net.cpp:184] Created Layer fc10 (37)
I0801 13:29:14.486634  3122 net.cpp:561] fc10 <- pool5
I0801 13:29:14.486645  3122 net.cpp:530] fc10 -> fc10
I0801 13:29:14.486909  3122 net.cpp:245] Setting up fc10
I0801 13:29:14.486917  3122 net.cpp:252] TRAIN Top shape for layer 37 'fc10' 22 10 (220)
I0801 13:29:14.486923  3122 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0801 13:29:14.486928  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.486944  3122 net.cpp:184] Created Layer loss (38)
I0801 13:29:14.486948  3122 net.cpp:561] loss <- fc10
I0801 13:29:14.486951  3122 net.cpp:561] loss <- label
I0801 13:29:14.486958  3122 net.cpp:530] loss -> loss
I0801 13:29:14.487110  3122 net.cpp:245] Setting up loss
I0801 13:29:14.487118  3122 net.cpp:252] TRAIN Top shape for layer 38 'loss' (1)
I0801 13:29:14.487121  3122 net.cpp:256]     with loss weight 1
I0801 13:29:14.487128  3122 net.cpp:323] loss needs backward computation.
I0801 13:29:14.487133  3122 net.cpp:323] fc10 needs backward computation.
I0801 13:29:14.487136  3122 net.cpp:323] pool5 needs backward computation.
I0801 13:29:14.487140  3122 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0801 13:29:14.487144  3122 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0801 13:29:14.487149  3122 net.cpp:323] res5a_branch2b needs backward computation.
I0801 13:29:14.487154  3122 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0801 13:29:14.487157  3122 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0801 13:29:14.487161  3122 net.cpp:323] res5a_branch2a needs backward computation.
I0801 13:29:14.487165  3122 net.cpp:323] pool4 needs backward computation.
I0801 13:29:14.487170  3122 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0801 13:29:14.487174  3122 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0801 13:29:14.487179  3122 net.cpp:323] res4a_branch2b needs backward computation.
I0801 13:29:14.487182  3122 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0801 13:29:14.487186  3122 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0801 13:29:14.487190  3122 net.cpp:323] res4a_branch2a needs backward computation.
I0801 13:29:14.487195  3122 net.cpp:323] pool3 needs backward computation.
I0801 13:29:14.487200  3122 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0801 13:29:14.487203  3122 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0801 13:29:14.487207  3122 net.cpp:323] res3a_branch2b needs backward computation.
I0801 13:29:14.487211  3122 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0801 13:29:14.487215  3122 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0801 13:29:14.487220  3122 net.cpp:323] res3a_branch2a needs backward computation.
I0801 13:29:14.487223  3122 net.cpp:323] pool2 needs backward computation.
I0801 13:29:14.487228  3122 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0801 13:29:14.487232  3122 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0801 13:29:14.487236  3122 net.cpp:323] res2a_branch2b needs backward computation.
I0801 13:29:14.487241  3122 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0801 13:29:14.487246  3122 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0801 13:29:14.487249  3122 net.cpp:323] res2a_branch2a needs backward computation.
I0801 13:29:14.487253  3122 net.cpp:323] pool1 needs backward computation.
I0801 13:29:14.487258  3122 net.cpp:323] conv1b/relu needs backward computation.
I0801 13:29:14.487262  3122 net.cpp:323] conv1b/bn needs backward computation.
I0801 13:29:14.487267  3122 net.cpp:323] conv1b needs backward computation.
I0801 13:29:14.487270  3122 net.cpp:323] conv1a/relu needs backward computation.
I0801 13:29:14.487274  3122 net.cpp:323] conv1a/bn needs backward computation.
I0801 13:29:14.487279  3122 net.cpp:323] conv1a needs backward computation.
I0801 13:29:14.487283  3122 net.cpp:325] data/bias does not need backward computation.
I0801 13:29:14.487288  3122 net.cpp:325] data does not need backward computation.
I0801 13:29:14.487293  3122 net.cpp:367] This network produces output loss
I0801 13:29:14.487326  3122 net.cpp:389] Top memory (TRAIN) required for data: 121110528 diff: 121110536
I0801 13:29:14.487330  3122 net.cpp:392] Bottom memory (TRAIN) required for data: 121110528 diff: 121110528
I0801 13:29:14.487334  3122 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 80740352 diff: 80740352
I0801 13:29:14.487337  3122 net.cpp:398] Parameters memory (TRAIN) required for data: 9450960 diff: 9450960
I0801 13:29:14.487341  3122 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0801 13:29:14.487345  3122 net.cpp:407] Network initialization done.
I0801 13:29:14.487692  3122 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/test.prototxt
W0801 13:29:14.487741  3122 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0801 13:29:14.487864  3122 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 17
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0801 13:29:14.487973  3122 net.cpp:104] Using FLOAT as default forward math type
I0801 13:29:14.487979  3122 net.cpp:110] Using FLOAT as default backward math type
I0801 13:29:14.487982  3122 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0801 13:29:14.487987  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.487998  3122 net.cpp:184] Created Layer data (0)
I0801 13:29:14.488003  3122 net.cpp:530] data -> data
I0801 13:29:14.488008  3122 net.cpp:530] data -> label
I0801 13:29:14.488016  3122 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0801 13:29:14.488024  3122 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:29:14.488734  3160 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0801 13:29:14.488790  3122 data_layer.cpp:184] (0) ReshapePrefetch 17, 3, 32, 32
I0801 13:29:14.488869  3122 data_layer.cpp:208] (0) Output data size: 17, 3, 32, 32
I0801 13:29:14.488874  3122 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:29:14.488890  3122 net.cpp:245] Setting up data
I0801 13:29:14.488895  3122 net.cpp:252] TEST Top shape for layer 0 'data' 17 3 32 32 (52224)
I0801 13:29:14.488901  3122 net.cpp:252] TEST Top shape for layer 0 'data' 17 (17)
I0801 13:29:14.488906  3122 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0801 13:29:14.488910  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.488917  3122 net.cpp:184] Created Layer label_data_1_split (1)
I0801 13:29:14.488922  3122 net.cpp:561] label_data_1_split <- label
I0801 13:29:14.488926  3122 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0801 13:29:14.488939  3122 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0801 13:29:14.488945  3122 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0801 13:29:14.489011  3122 net.cpp:245] Setting up label_data_1_split
I0801 13:29:14.489017  3122 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0801 13:29:14.489022  3122 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0801 13:29:14.489027  3122 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0801 13:29:14.489032  3122 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0801 13:29:14.489035  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.489042  3122 net.cpp:184] Created Layer data/bias (2)
I0801 13:29:14.489048  3122 net.cpp:561] data/bias <- data
I0801 13:29:14.489051  3122 net.cpp:530] data/bias -> data/bias
I0801 13:29:14.489181  3122 net.cpp:245] Setting up data/bias
I0801 13:29:14.489187  3122 net.cpp:252] TEST Top shape for layer 2 'data/bias' 17 3 32 32 (52224)
I0801 13:29:14.489194  3122 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0801 13:29:14.489199  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.489209  3122 net.cpp:184] Created Layer conv1a (3)
I0801 13:29:14.489213  3122 net.cpp:561] conv1a <- data/bias
I0801 13:29:14.489217  3122 net.cpp:530] conv1a -> conv1a
I0801 13:29:14.489534  3161 data_layer.cpp:97] (0) Parser threads: 1
I0801 13:29:14.489540  3161 data_layer.cpp:99] (0) Transformer threads: 1
I0801 13:29:14.492254  3122 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8G, req 0.01G)
I0801 13:29:14.492262  3122 net.cpp:245] Setting up conv1a
I0801 13:29:14.492269  3122 net.cpp:252] TEST Top shape for layer 3 'conv1a' 17 32 32 32 (557056)
I0801 13:29:14.492276  3122 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0801 13:29:14.492278  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.492283  3122 net.cpp:184] Created Layer conv1a/bn (4)
I0801 13:29:14.492286  3122 net.cpp:561] conv1a/bn <- conv1a
I0801 13:29:14.492288  3122 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0801 13:29:14.492924  3122 net.cpp:245] Setting up conv1a/bn
I0801 13:29:14.492933  3122 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 17 32 32 32 (557056)
I0801 13:29:14.492941  3122 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0801 13:29:14.492947  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.492954  3122 net.cpp:184] Created Layer conv1a/relu (5)
I0801 13:29:14.492966  3122 net.cpp:561] conv1a/relu <- conv1a
I0801 13:29:14.492971  3122 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0801 13:29:14.492979  3122 net.cpp:245] Setting up conv1a/relu
I0801 13:29:14.492985  3122 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 17 32 32 32 (557056)
I0801 13:29:14.492988  3122 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0801 13:29:14.492993  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.493008  3122 net.cpp:184] Created Layer conv1b (6)
I0801 13:29:14.493012  3122 net.cpp:561] conv1b <- conv1a
I0801 13:29:14.493016  3122 net.cpp:530] conv1b -> conv1b
I0801 13:29:14.496242  3122 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8G, req 0.01G)
I0801 13:29:14.496253  3122 net.cpp:245] Setting up conv1b
I0801 13:29:14.496258  3122 net.cpp:252] TEST Top shape for layer 6 'conv1b' 17 32 32 32 (557056)
I0801 13:29:14.496268  3122 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0801 13:29:14.496273  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.496281  3122 net.cpp:184] Created Layer conv1b/bn (7)
I0801 13:29:14.496294  3122 net.cpp:561] conv1b/bn <- conv1b
I0801 13:29:14.496297  3122 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0801 13:29:14.496953  3122 net.cpp:245] Setting up conv1b/bn
I0801 13:29:14.496961  3122 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 17 32 32 32 (557056)
I0801 13:29:14.496968  3122 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0801 13:29:14.496969  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.496973  3122 net.cpp:184] Created Layer conv1b/relu (8)
I0801 13:29:14.496975  3122 net.cpp:561] conv1b/relu <- conv1b
I0801 13:29:14.496978  3122 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0801 13:29:14.496981  3122 net.cpp:245] Setting up conv1b/relu
I0801 13:29:14.496984  3122 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 17 32 32 32 (557056)
I0801 13:29:14.496985  3122 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0801 13:29:14.496987  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.496991  3122 net.cpp:184] Created Layer pool1 (9)
I0801 13:29:14.496994  3122 net.cpp:561] pool1 <- conv1b
I0801 13:29:14.496997  3122 net.cpp:530] pool1 -> pool1
I0801 13:29:14.497063  3122 net.cpp:245] Setting up pool1
I0801 13:29:14.497068  3122 net.cpp:252] TEST Top shape for layer 9 'pool1' 17 32 32 32 (557056)
I0801 13:29:14.497071  3122 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0801 13:29:14.497073  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.497081  3122 net.cpp:184] Created Layer res2a_branch2a (10)
I0801 13:29:14.497084  3122 net.cpp:561] res2a_branch2a <- pool1
I0801 13:29:14.497087  3122 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0801 13:29:14.501042  3122 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.99G, req 0.01G)
I0801 13:29:14.501054  3122 net.cpp:245] Setting up res2a_branch2a
I0801 13:29:14.501058  3122 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 17 64 32 32 (1114112)
I0801 13:29:14.501067  3122 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0801 13:29:14.501075  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.501081  3122 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0801 13:29:14.501086  3122 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0801 13:29:14.501106  3122 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0801 13:29:14.501754  3122 net.cpp:245] Setting up res2a_branch2a/bn
I0801 13:29:14.501761  3122 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 17 64 32 32 (1114112)
I0801 13:29:14.501770  3122 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0801 13:29:14.501775  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.501781  3122 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0801 13:29:14.501785  3122 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0801 13:29:14.501791  3122 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0801 13:29:14.501799  3122 net.cpp:245] Setting up res2a_branch2a/relu
I0801 13:29:14.501806  3122 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 17 64 32 32 (1114112)
I0801 13:29:14.501809  3122 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0801 13:29:14.501814  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.501826  3122 net.cpp:184] Created Layer res2a_branch2b (13)
I0801 13:29:14.501829  3122 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0801 13:29:14.501832  3122 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0801 13:29:14.504802  3122 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.98G, req 0.01G)
I0801 13:29:14.504813  3122 net.cpp:245] Setting up res2a_branch2b
I0801 13:29:14.504835  3122 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 17 64 32 32 (1114112)
I0801 13:29:14.504843  3122 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0801 13:29:14.504848  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.504855  3122 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0801 13:29:14.504860  3122 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0801 13:29:14.504865  3122 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0801 13:29:14.505497  3122 net.cpp:245] Setting up res2a_branch2b/bn
I0801 13:29:14.505506  3122 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 17 64 32 32 (1114112)
I0801 13:29:14.505514  3122 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0801 13:29:14.505519  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.505525  3122 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0801 13:29:14.505530  3122 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0801 13:29:14.505535  3122 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0801 13:29:14.505542  3122 net.cpp:245] Setting up res2a_branch2b/relu
I0801 13:29:14.505547  3122 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 17 64 32 32 (1114112)
I0801 13:29:14.505551  3122 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0801 13:29:14.505555  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.505563  3122 net.cpp:184] Created Layer pool2 (16)
I0801 13:29:14.505568  3122 net.cpp:561] pool2 <- res2a_branch2b
I0801 13:29:14.505571  3122 net.cpp:530] pool2 -> pool2
I0801 13:29:14.505635  3122 net.cpp:245] Setting up pool2
I0801 13:29:14.505640  3122 net.cpp:252] TEST Top shape for layer 16 'pool2' 17 64 16 16 (278528)
I0801 13:29:14.505645  3122 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0801 13:29:14.505648  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.505657  3122 net.cpp:184] Created Layer res3a_branch2a (17)
I0801 13:29:14.505661  3122 net.cpp:561] res3a_branch2a <- pool2
I0801 13:29:14.505666  3122 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0801 13:29:14.511615  3122 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.97G, req 0.01G)
I0801 13:29:14.511626  3122 net.cpp:245] Setting up res3a_branch2a
I0801 13:29:14.511629  3122 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 17 128 16 16 (557056)
I0801 13:29:14.511634  3122 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0801 13:29:14.511637  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.511641  3122 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0801 13:29:14.511643  3122 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0801 13:29:14.511646  3122 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0801 13:29:14.512364  3122 net.cpp:245] Setting up res3a_branch2a/bn
I0801 13:29:14.512372  3122 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 17 128 16 16 (557056)
I0801 13:29:14.512380  3122 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0801 13:29:14.512383  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.512387  3122 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0801 13:29:14.512388  3122 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0801 13:29:14.512392  3122 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0801 13:29:14.512394  3122 net.cpp:245] Setting up res3a_branch2a/relu
I0801 13:29:14.512398  3122 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 17 128 16 16 (557056)
I0801 13:29:14.512401  3122 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0801 13:29:14.512419  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.512431  3122 net.cpp:184] Created Layer res3a_branch2b (20)
I0801 13:29:14.512435  3122 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0801 13:29:14.512439  3122 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0801 13:29:14.515883  3122 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.97G, req 0.01G)
I0801 13:29:14.515892  3122 net.cpp:245] Setting up res3a_branch2b
I0801 13:29:14.515897  3122 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 17 128 16 16 (557056)
I0801 13:29:14.515904  3122 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0801 13:29:14.515910  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.515923  3122 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0801 13:29:14.515926  3122 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0801 13:29:14.515931  3122 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0801 13:29:14.516579  3122 net.cpp:245] Setting up res3a_branch2b/bn
I0801 13:29:14.516587  3122 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 17 128 16 16 (557056)
I0801 13:29:14.516597  3122 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0801 13:29:14.516602  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.516607  3122 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0801 13:29:14.516611  3122 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0801 13:29:14.516616  3122 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0801 13:29:14.516623  3122 net.cpp:245] Setting up res3a_branch2b/relu
I0801 13:29:14.516628  3122 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 17 128 16 16 (557056)
I0801 13:29:14.516633  3122 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0801 13:29:14.516638  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.516644  3122 net.cpp:184] Created Layer pool3 (23)
I0801 13:29:14.516649  3122 net.cpp:561] pool3 <- res3a_branch2b
I0801 13:29:14.516654  3122 net.cpp:530] pool3 -> pool3
I0801 13:29:14.516718  3122 net.cpp:245] Setting up pool3
I0801 13:29:14.516724  3122 net.cpp:252] TEST Top shape for layer 23 'pool3' 17 128 16 16 (557056)
I0801 13:29:14.516728  3122 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0801 13:29:14.516733  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.516754  3122 net.cpp:184] Created Layer res4a_branch2a (24)
I0801 13:29:14.516758  3122 net.cpp:561] res4a_branch2a <- pool3
I0801 13:29:14.516762  3122 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0801 13:29:14.527681  3122 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0.01G)
I0801 13:29:14.527698  3122 net.cpp:245] Setting up res4a_branch2a
I0801 13:29:14.527704  3122 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 17 256 16 16 (1114112)
I0801 13:29:14.527711  3122 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0801 13:29:14.527714  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.527724  3122 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0801 13:29:14.527726  3122 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0801 13:29:14.527729  3122 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0801 13:29:14.528458  3122 net.cpp:245] Setting up res4a_branch2a/bn
I0801 13:29:14.528466  3122 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 17 256 16 16 (1114112)
I0801 13:29:14.528472  3122 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0801 13:29:14.528475  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.528491  3122 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0801 13:29:14.528494  3122 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0801 13:29:14.528496  3122 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0801 13:29:14.528501  3122 net.cpp:245] Setting up res4a_branch2a/relu
I0801 13:29:14.528504  3122 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 17 256 16 16 (1114112)
I0801 13:29:14.528507  3122 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0801 13:29:14.528511  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.528517  3122 net.cpp:184] Created Layer res4a_branch2b (27)
I0801 13:29:14.528522  3122 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0801 13:29:14.528523  3122 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0801 13:29:14.534788  3122 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.95G, req 0.01G)
I0801 13:29:14.534802  3122 net.cpp:245] Setting up res4a_branch2b
I0801 13:29:14.534807  3122 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 17 256 16 16 (1114112)
I0801 13:29:14.534812  3122 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0801 13:29:14.534816  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.534821  3122 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0801 13:29:14.534824  3122 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0801 13:29:14.534827  3122 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0801 13:29:14.535502  3122 net.cpp:245] Setting up res4a_branch2b/bn
I0801 13:29:14.535511  3122 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 17 256 16 16 (1114112)
I0801 13:29:14.535516  3122 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0801 13:29:14.535521  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.535523  3122 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0801 13:29:14.535526  3122 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0801 13:29:14.535528  3122 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0801 13:29:14.535532  3122 net.cpp:245] Setting up res4a_branch2b/relu
I0801 13:29:14.535536  3122 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 17 256 16 16 (1114112)
I0801 13:29:14.535538  3122 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0801 13:29:14.535540  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.535545  3122 net.cpp:184] Created Layer pool4 (30)
I0801 13:29:14.535547  3122 net.cpp:561] pool4 <- res4a_branch2b
I0801 13:29:14.535550  3122 net.cpp:530] pool4 -> pool4
I0801 13:29:14.535616  3122 net.cpp:245] Setting up pool4
I0801 13:29:14.535621  3122 net.cpp:252] TEST Top shape for layer 30 'pool4' 17 256 8 8 (278528)
I0801 13:29:14.535624  3122 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0801 13:29:14.535627  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.535634  3122 net.cpp:184] Created Layer res5a_branch2a (31)
I0801 13:29:14.535637  3122 net.cpp:561] res5a_branch2a <- pool4
I0801 13:29:14.535640  3122 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0801 13:29:14.567392  3122 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.94G, req 0.01G)
I0801 13:29:14.567409  3122 net.cpp:245] Setting up res5a_branch2a
I0801 13:29:14.567415  3122 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 17 512 8 8 (557056)
I0801 13:29:14.567421  3122 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0801 13:29:14.567425  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.567447  3122 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0801 13:29:14.567451  3122 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0801 13:29:14.567456  3122 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0801 13:29:14.568148  3122 net.cpp:245] Setting up res5a_branch2a/bn
I0801 13:29:14.568156  3122 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 17 512 8 8 (557056)
I0801 13:29:14.568162  3122 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0801 13:29:14.568166  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.568168  3122 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0801 13:29:14.568171  3122 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0801 13:29:14.568173  3122 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0801 13:29:14.568177  3122 net.cpp:245] Setting up res5a_branch2a/relu
I0801 13:29:14.568179  3122 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 17 512 8 8 (557056)
I0801 13:29:14.568181  3122 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0801 13:29:14.568183  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.568190  3122 net.cpp:184] Created Layer res5a_branch2b (34)
I0801 13:29:14.568193  3122 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0801 13:29:14.568195  3122 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0801 13:29:14.585315  3122 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.93G, req 0.01G)
I0801 13:29:14.585327  3122 net.cpp:245] Setting up res5a_branch2b
I0801 13:29:14.585331  3122 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 17 512 8 8 (557056)
I0801 13:29:14.585340  3122 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0801 13:29:14.585342  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.585348  3122 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0801 13:29:14.585351  3122 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0801 13:29:14.585353  3122 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0801 13:29:14.586086  3122 net.cpp:245] Setting up res5a_branch2b/bn
I0801 13:29:14.586094  3122 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 17 512 8 8 (557056)
I0801 13:29:14.586100  3122 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0801 13:29:14.586104  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.586114  3122 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0801 13:29:14.586117  3122 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0801 13:29:14.586119  3122 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0801 13:29:14.586124  3122 net.cpp:245] Setting up res5a_branch2b/relu
I0801 13:29:14.586127  3122 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 17 512 8 8 (557056)
I0801 13:29:14.586128  3122 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0801 13:29:14.586132  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.586143  3122 net.cpp:184] Created Layer pool5 (37)
I0801 13:29:14.586145  3122 net.cpp:561] pool5 <- res5a_branch2b
I0801 13:29:14.586148  3122 net.cpp:530] pool5 -> pool5
I0801 13:29:14.586179  3122 net.cpp:245] Setting up pool5
I0801 13:29:14.586182  3122 net.cpp:252] TEST Top shape for layer 37 'pool5' 17 512 1 1 (8704)
I0801 13:29:14.586185  3122 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0801 13:29:14.586187  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.586192  3122 net.cpp:184] Created Layer fc10 (38)
I0801 13:29:14.586194  3122 net.cpp:561] fc10 <- pool5
I0801 13:29:14.586197  3122 net.cpp:530] fc10 -> fc10
I0801 13:29:14.586484  3122 net.cpp:245] Setting up fc10
I0801 13:29:14.586501  3122 net.cpp:252] TEST Top shape for layer 38 'fc10' 17 10 (170)
I0801 13:29:14.586506  3122 layer_factory.hpp:136] Creating layer 'fc10_fc10_0_split' of type 'Split'
I0801 13:29:14.586509  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.586514  3122 net.cpp:184] Created Layer fc10_fc10_0_split (39)
I0801 13:29:14.586518  3122 net.cpp:561] fc10_fc10_0_split <- fc10
I0801 13:29:14.586520  3122 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0801 13:29:14.586524  3122 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0801 13:29:14.586526  3122 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0801 13:29:14.586596  3122 net.cpp:245] Setting up fc10_fc10_0_split
I0801 13:29:14.586599  3122 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0801 13:29:14.586602  3122 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0801 13:29:14.586604  3122 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0801 13:29:14.586607  3122 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0801 13:29:14.586609  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.586617  3122 net.cpp:184] Created Layer loss (40)
I0801 13:29:14.586621  3122 net.cpp:561] loss <- fc10_fc10_0_split_0
I0801 13:29:14.586623  3122 net.cpp:561] loss <- label_data_1_split_0
I0801 13:29:14.586627  3122 net.cpp:530] loss -> loss
I0801 13:29:14.586772  3122 net.cpp:245] Setting up loss
I0801 13:29:14.586778  3122 net.cpp:252] TEST Top shape for layer 40 'loss' (1)
I0801 13:29:14.586781  3122 net.cpp:256]     with loss weight 1
I0801 13:29:14.586786  3122 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0801 13:29:14.586788  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.586796  3122 net.cpp:184] Created Layer accuracy/top1 (41)
I0801 13:29:14.586798  3122 net.cpp:561] accuracy/top1 <- fc10_fc10_0_split_1
I0801 13:29:14.586800  3122 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0801 13:29:14.586803  3122 net.cpp:530] accuracy/top1 -> accuracy/top1
I0801 13:29:14.586808  3122 net.cpp:245] Setting up accuracy/top1
I0801 13:29:14.586812  3122 net.cpp:252] TEST Top shape for layer 41 'accuracy/top1' (1)
I0801 13:29:14.586813  3122 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0801 13:29:14.586815  3122 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:29:14.586819  3122 net.cpp:184] Created Layer accuracy/top5 (42)
I0801 13:29:14.586822  3122 net.cpp:561] accuracy/top5 <- fc10_fc10_0_split_2
I0801 13:29:14.586824  3122 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0801 13:29:14.586827  3122 net.cpp:530] accuracy/top5 -> accuracy/top5
I0801 13:29:14.586830  3122 net.cpp:245] Setting up accuracy/top5
I0801 13:29:14.586833  3122 net.cpp:252] TEST Top shape for layer 42 'accuracy/top5' (1)
I0801 13:29:14.586836  3122 net.cpp:325] accuracy/top5 does not need backward computation.
I0801 13:29:14.586838  3122 net.cpp:325] accuracy/top1 does not need backward computation.
I0801 13:29:14.586840  3122 net.cpp:323] loss needs backward computation.
I0801 13:29:14.586843  3122 net.cpp:323] fc10_fc10_0_split needs backward computation.
I0801 13:29:14.586845  3122 net.cpp:323] fc10 needs backward computation.
I0801 13:29:14.586848  3122 net.cpp:323] pool5 needs backward computation.
I0801 13:29:14.586850  3122 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0801 13:29:14.586851  3122 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0801 13:29:14.586854  3122 net.cpp:323] res5a_branch2b needs backward computation.
I0801 13:29:14.586856  3122 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0801 13:29:14.586858  3122 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0801 13:29:14.586861  3122 net.cpp:323] res5a_branch2a needs backward computation.
I0801 13:29:14.586869  3122 net.cpp:323] pool4 needs backward computation.
I0801 13:29:14.586871  3122 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0801 13:29:14.586874  3122 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0801 13:29:14.586875  3122 net.cpp:323] res4a_branch2b needs backward computation.
I0801 13:29:14.586879  3122 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0801 13:29:14.586880  3122 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0801 13:29:14.586882  3122 net.cpp:323] res4a_branch2a needs backward computation.
I0801 13:29:14.586884  3122 net.cpp:323] pool3 needs backward computation.
I0801 13:29:14.586887  3122 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0801 13:29:14.586889  3122 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0801 13:29:14.586891  3122 net.cpp:323] res3a_branch2b needs backward computation.
I0801 13:29:14.586894  3122 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0801 13:29:14.586895  3122 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0801 13:29:14.586897  3122 net.cpp:323] res3a_branch2a needs backward computation.
I0801 13:29:14.586899  3122 net.cpp:323] pool2 needs backward computation.
I0801 13:29:14.586902  3122 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0801 13:29:14.586905  3122 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0801 13:29:14.586905  3122 net.cpp:323] res2a_branch2b needs backward computation.
I0801 13:29:14.586907  3122 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0801 13:29:14.586910  3122 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0801 13:29:14.586912  3122 net.cpp:323] res2a_branch2a needs backward computation.
I0801 13:29:14.586915  3122 net.cpp:323] pool1 needs backward computation.
I0801 13:29:14.586916  3122 net.cpp:323] conv1b/relu needs backward computation.
I0801 13:29:14.586918  3122 net.cpp:323] conv1b/bn needs backward computation.
I0801 13:29:14.586920  3122 net.cpp:323] conv1b needs backward computation.
I0801 13:29:14.586922  3122 net.cpp:323] conv1a/relu needs backward computation.
I0801 13:29:14.586925  3122 net.cpp:323] conv1a/bn needs backward computation.
I0801 13:29:14.586927  3122 net.cpp:323] conv1a needs backward computation.
I0801 13:29:14.586930  3122 net.cpp:325] data/bias does not need backward computation.
I0801 13:29:14.586932  3122 net.cpp:325] label_data_1_split does not need backward computation.
I0801 13:29:14.586935  3122 net.cpp:325] data does not need backward computation.
I0801 13:29:14.586938  3122 net.cpp:367] This network produces output accuracy/top1
I0801 13:29:14.586941  3122 net.cpp:367] This network produces output accuracy/top5
I0801 13:29:14.586942  3122 net.cpp:367] This network produces output loss
I0801 13:29:14.586969  3122 net.cpp:389] Top memory (TEST) required for data: 93585408 diff: 8
I0801 13:29:14.586972  3122 net.cpp:392] Bottom memory (TEST) required for data: 93585408 diff: 93585408
I0801 13:29:14.586974  3122 net.cpp:395] Shared (in-place) memory (TEST) by data: 62390272 diff: 62390272
I0801 13:29:14.586977  3122 net.cpp:398] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0801 13:29:14.586979  3122 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0801 13:29:14.586982  3122 net.cpp:407] Network initialization done.
I0801 13:29:14.587034  3122 solver.cpp:56] Solver scaffolding done.
I0801 13:29:14.591033  3122 caffe.cpp:137] Finetuning from training/cifar10_jacintonet11v2_2017-08-01_13-11-28/initial/cifar10_jacintonet11v2_iter_64000.caffemodel
I0801 13:29:14.595549  3122 net.cpp:1089] Copying source layer data Type:Data #blobs=0
I0801 13:29:14.595571  3122 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0801 13:29:14.595602  3122 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0801 13:29:14.595613  3122 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0801 13:29:14.595877  3122 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0801 13:29:14.595892  3122 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0801 13:29:14.595902  3122 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0801 13:29:14.596053  3122 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0801 13:29:14.596057  3122 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0801 13:29:14.596060  3122 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0801 13:29:14.596077  3122 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:29:14.596233  3122 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0801 13:29:14.596237  3122 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0801 13:29:14.596251  3122 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:29:14.596392  3122 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0801 13:29:14.596396  3122 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0801 13:29:14.596398  3122 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0801 13:29:14.596438  3122 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:29:14.596565  3122 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0801 13:29:14.596570  3122 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0801 13:29:14.596591  3122 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:29:14.596712  3122 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0801 13:29:14.596716  3122 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0801 13:29:14.596719  3122 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0801 13:29:14.596839  3122 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:29:14.596967  3122 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0801 13:29:14.596971  3122 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0801 13:29:14.597030  3122 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:29:14.597147  3122 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0801 13:29:14.597151  3122 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0801 13:29:14.597154  3122 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0801 13:29:14.597513  3122 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:29:14.597645  3122 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0801 13:29:14.597651  3122 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0801 13:29:14.597810  3122 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:29:14.597935  3122 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0801 13:29:14.597940  3122 net.cpp:1089] Copying source layer pool5 Type:Pooling #blobs=0
I0801 13:29:14.597942  3122 net.cpp:1089] Copying source layer fc10 Type:InnerProduct #blobs=2
I0801 13:29:14.597952  3122 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0801 13:29:14.600687  3122 net.cpp:1089] Copying source layer data Type:Data #blobs=0
I0801 13:29:14.600704  3122 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0801 13:29:14.600725  3122 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0801 13:29:14.600736  3122 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0801 13:29:14.600980  3122 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0801 13:29:14.600986  3122 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0801 13:29:14.600996  3122 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0801 13:29:14.601150  3122 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0801 13:29:14.601155  3122 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0801 13:29:14.601157  3122 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0801 13:29:14.601172  3122 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:29:14.601327  3122 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0801 13:29:14.601331  3122 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0801 13:29:14.601343  3122 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:29:14.601488  3122 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0801 13:29:14.601492  3122 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0801 13:29:14.601495  3122 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0801 13:29:14.601532  3122 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:29:14.601662  3122 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0801 13:29:14.601666  3122 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0801 13:29:14.601687  3122 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:29:14.601799  3122 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0801 13:29:14.601804  3122 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0801 13:29:14.601805  3122 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0801 13:29:14.601917  3122 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:29:14.602041  3122 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0801 13:29:14.602044  3122 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0801 13:29:14.602115  3122 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:29:14.602237  3122 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0801 13:29:14.602242  3122 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0801 13:29:14.602244  3122 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0801 13:29:14.602617  3122 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:29:14.602751  3122 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0801 13:29:14.602756  3122 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0801 13:29:14.602917  3122 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:29:14.603041  3122 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0801 13:29:14.603046  3122 net.cpp:1089] Copying source layer pool5 Type:Pooling #blobs=0
I0801 13:29:14.603049  3122 net.cpp:1089] Copying source layer fc10 Type:InnerProduct #blobs=2
I0801 13:29:14.603057  3122 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0801 13:29:14.603119  3122 parallel.cpp:108] [0 - 0] P2pSync adding callback
I0801 13:29:14.603127  3122 parallel.cpp:108] [1 - 1] P2pSync adding callback
I0801 13:29:14.603129  3122 parallel.cpp:108] [2 - 2] P2pSync adding callback
I0801 13:29:14.603132  3122 parallel.cpp:61] Starting Optimization
I0801 13:29:14.603133  3122 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:29:14.603158  3122 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:29:14.603173  3122 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:29:14.603864  3162 device_alternate.hpp:116] NVML initialized on thread 139819137378048
I0801 13:29:14.616526  3162 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0801 13:29:14.616583  3164 device_alternate.hpp:116] NVML initialized on thread 139819120592640
I0801 13:29:14.617558  3164 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0801 13:29:14.617575  3163 device_alternate.hpp:116] NVML initialized on thread 139819128985344
I0801 13:29:14.618023  3163 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0801 13:29:14.621947  3164 solver.cpp:42] Solver data type: FLOAT
W0801 13:29:14.622326  3164 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0801 13:29:14.622409  3164 net.cpp:104] Using FLOAT as default forward math type
I0801 13:29:14.622414  3164 net.cpp:110] Using FLOAT as default backward math type
I0801 13:29:14.622439  3164 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0801 13:29:14.622450  3164 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:29:14.626257  3163 solver.cpp:42] Solver data type: FLOAT
W0801 13:29:14.626652  3163 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0801 13:29:14.626725  3163 net.cpp:104] Using FLOAT as default forward math type
I0801 13:29:14.626731  3163 net.cpp:110] Using FLOAT as default backward math type
I0801 13:29:14.626770  3163 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0801 13:29:14.626780  3163 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:29:14.626943  3165 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0801 13:29:14.627977  3164 data_layer.cpp:184] [2] ReshapePrefetch 22, 3, 32, 32
I0801 13:29:14.628002  3166 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0801 13:29:14.628077  3164 data_layer.cpp:208] [2] Output data size: 22, 3, 32, 32
I0801 13:29:14.628083  3164 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:29:14.629179  3163 data_layer.cpp:184] [1] ReshapePrefetch 22, 3, 32, 32
I0801 13:29:14.629335  3163 data_layer.cpp:208] [1] Output data size: 22, 3, 32, 32
I0801 13:29:14.629343  3163 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:29:15.062960  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.25G, req 0G)
I0801 13:29:15.087224  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.25G, req 0G)
I0801 13:29:15.089684  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.23G, req 0G)
I0801 13:29:15.096436  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.23G, req 0G)
I0801 13:29:15.102864  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.21G, req 0G)
I0801 13:29:15.109472  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.21G, req 0G)
I0801 13:29:15.111850  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.19G, req 0G)
I0801 13:29:15.118031  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.19G, req 0G)
I0801 13:29:15.125424  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.18G, req 0.01G)
I0801 13:29:15.131359  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.18G, req 0.01G)
I0801 13:29:15.131760  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0.01G)
I0801 13:29:15.137384  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0.01G)
I0801 13:29:15.153551  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.15G, req 0.01G)
I0801 13:29:15.159842  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.15G, req 0.01G)
I0801 13:29:15.163216  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.14G, req 0.01G)
I0801 13:29:15.170840  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.14G, req 0.01G)
I0801 13:29:15.208693  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0.01G)
I0801 13:29:15.216064  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0.01G)
I0801 13:29:15.229213  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8.1G, req 0.01G)
I0801 13:29:15.231143  3164 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/test.prototxt
W0801 13:29:15.231196  3164 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0801 13:29:15.231300  3164 net.cpp:104] Using FLOAT as default forward math type
I0801 13:29:15.231307  3164 net.cpp:110] Using FLOAT as default backward math type
I0801 13:29:15.231323  3164 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0801 13:29:15.231333  3164 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:29:15.232579  3199 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0801 13:29:15.232650  3164 data_layer.cpp:184] (2) ReshapePrefetch 17, 3, 32, 32
I0801 13:29:15.232729  3164 data_layer.cpp:208] (2) Output data size: 17, 3, 32, 32
I0801 13:29:15.232733  3164 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:29:15.233520  3200 data_layer.cpp:97] (2) Parser threads: 1
I0801 13:29:15.233527  3200 data_layer.cpp:99] (2) Transformer threads: 1
I0801 13:29:15.236889  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8.1G, req 0.01G)
I0801 13:29:15.237282  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8.1G, req 0.01G)
I0801 13:29:15.239449  3163 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/test.prototxt
W0801 13:29:15.239502  3163 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0801 13:29:15.239593  3163 net.cpp:104] Using FLOAT as default forward math type
I0801 13:29:15.239598  3163 net.cpp:110] Using FLOAT as default backward math type
I0801 13:29:15.239617  3163 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0801 13:29:15.239626  3163 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:29:15.240434  3201 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0801 13:29:15.240499  3163 data_layer.cpp:184] (1) ReshapePrefetch 17, 3, 32, 32
I0801 13:29:15.240581  3163 data_layer.cpp:208] (1) Output data size: 17, 3, 32, 32
I0801 13:29:15.240586  3163 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:29:15.241341  3202 data_layer.cpp:97] (1) Parser threads: 1
I0801 13:29:15.241349  3202 data_layer.cpp:99] (1) Transformer threads: 1
I0801 13:29:15.241582  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.09G, req 0.01G)
I0801 13:29:15.244722  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8.1G, req 0.01G)
I0801 13:29:15.248422  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0.01G)
I0801 13:29:15.250787  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.09G, req 0.01G)
I0801 13:29:15.253612  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.08G, req 0.01G)
I0801 13:29:15.256777  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0.01G)
I0801 13:29:15.261252  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.08G, req 0.01G)
I0801 13:29:15.262959  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.07G, req 0.01G)
I0801 13:29:15.268293  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.06G, req 0.01G)
I0801 13:29:15.269264  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.07G, req 0.01G)
I0801 13:29:15.273605  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.06G, req 0.01G)
I0801 13:29:15.280712  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0.01G)
I0801 13:29:15.286226  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0.01G)
I0801 13:29:15.288936  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.05G, req 0.01G)
I0801 13:29:15.294080  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.05G, req 0.01G)
I0801 13:29:15.322609  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.03G, req 0.01G)
I0801 13:29:15.327172  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.03G, req 0.01G)
I0801 13:29:15.341019  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8.02G, req 0.01G)
I0801 13:29:15.343916  3164 solver.cpp:56] Solver scaffolding done.
I0801 13:29:15.345284  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8.02G, req 0.01G)
I0801 13:29:15.347918  3163 solver.cpp:56] Solver scaffolding done.
I0801 13:29:15.392069  3163 parallel.cpp:164] [1 - 1] P2pSync adding callback
I0801 13:29:15.392069  3164 parallel.cpp:164] [2 - 2] P2pSync adding callback
I0801 13:29:15.392069  3162 parallel.cpp:164] [0 - 0] P2pSync adding callback
I0801 13:29:15.598502  3162 solver.cpp:479] Solving jacintonet11v2_train
I0801 13:29:15.598521  3162 solver.cpp:480] Learning Rate Policy: poly
I0801 13:29:15.598629  3163 solver.cpp:479] Solving jacintonet11v2_train
I0801 13:29:15.598639  3163 solver.cpp:480] Learning Rate Policy: poly
I0801 13:29:15.598654  3164 solver.cpp:479] Solving jacintonet11v2_train
I0801 13:29:15.598662  3164 solver.cpp:480] Learning Rate Policy: poly
I0801 13:29:15.605558  3163 solver.cpp:268] Starting Optimization on GPU 1
I0801 13:29:15.605561  3162 solver.cpp:268] Starting Optimization on GPU 0
I0801 13:29:15.605559  3164 solver.cpp:268] Starting Optimization on GPU 2
I0801 13:29:15.605805  3162 solver.cpp:550] Iteration 0, Testing net (#0)
I0801 13:29:15.605845  3204 device_alternate.hpp:116] NVML initialized on thread 139818360960768
I0801 13:29:15.605867  3204 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0801 13:29:15.605883  3203 device_alternate.hpp:116] NVML initialized on thread 139818344175360
I0801 13:29:15.605900  3203 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0801 13:29:15.605907  3205 device_alternate.hpp:116] NVML initialized on thread 139818352568064
I0801 13:29:15.605913  3205 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0801 13:29:15.613801  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.99G, req 0.01G)
I0801 13:29:15.614630  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.99G, req 0.01G)
I0801 13:29:15.619359  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.98G, req 0.01G)
I0801 13:29:15.619904  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 0  (limit 7.98G, req 0.01G)
I0801 13:29:15.626740  3162 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 7.92G, req 0G)
I0801 13:29:15.627140  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.97G, req 0.01G)
I0801 13:29:15.628572  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.97G, req 0.01G)
I0801 13:29:15.633829  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.96G, req 0.01G)
I0801 13:29:15.634358  3162 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.9G, req 0G)
I0801 13:29:15.635562  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.96G, req 0.01G)
I0801 13:29:15.640805  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.94G, req 0.01G)
I0801 13:29:15.642652  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.94G, req 0.01G)
I0801 13:29:15.644711  3162 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.89G, req 0G)
I0801 13:29:15.647100  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.94G, req 0.01G)
I0801 13:29:15.648241  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.94G, req 0.01G)
I0801 13:29:15.651068  3162 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0G)
I0801 13:29:15.655203  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.92G, req 0.01G)
I0801 13:29:15.656754  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.92G, req 0.01G)
I0801 13:29:15.657341  3162 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.86G, req 0G)
I0801 13:29:15.661970  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.91G, req 0.01G)
I0801 13:29:15.662678  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.91G, req 0.01G)
I0801 13:29:15.663537  3162 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.85G, req 0G)
I0801 13:29:15.673192  3162 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.84G, req 0G)
I0801 13:29:15.675915  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.9G, req 0.01G)
I0801 13:29:15.680377  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 6  (limit 7.9G, req 0.01G)
I0801 13:29:15.697088  3162 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.83G, req 0G)
I0801 13:29:15.702097  3163 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.89G, req 0.01G)
I0801 13:29:15.704711  3164 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 7  (limit 7.89G, req 0.01G)
I0801 13:29:15.716457  3162 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.81G, req 0G)
I0801 13:29:15.723232  3162 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.8G, req 0G)
I0801 13:29:15.727095  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 1
I0801 13:29:15.727116  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0801 13:29:15.727128  3162 solver.cpp:635]     Test net output #2: loss = 0.0237409 (* 1 = 0.0237409 loss)
I0801 13:29:15.727138  3162 solver.cpp:295] [MultiGPU] Initial Test completed
I0801 13:29:15.727192  3163 blocking_queue.cpp:40] Data layer prefetch queue empty
I0801 13:29:15.739651  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.88G, req 0.01G)
I0801 13:29:15.740289  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.88G, req 0.01G)
I0801 13:29:15.741392  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.8G, req 0G)
I0801 13:29:15.750335  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.79G, req 0G)
I0801 13:29:15.752027  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.87G, req 0.01G)
I0801 13:29:15.752079  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.87G, req 0.01G)
I0801 13:29:15.763388  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.77G, req 0G)
I0801 13:29:15.765116  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.86G, req 0.01G)
I0801 13:29:15.765527  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.86G, req 0.01G)
I0801 13:29:15.771883  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.76G, req 0G)
I0801 13:29:15.774112  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.85G, req 0.01G)
I0801 13:29:15.774379  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.85G, req 0.01G)
I0801 13:29:15.783619  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.75G, req 0.01G)
I0801 13:29:15.785270  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.83G, req 0.01G)
I0801 13:29:15.786154  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.83G, req 0.01G)
I0801 13:29:15.791059  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.74G, req 0.01G)
I0801 13:29:15.792491  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.82G, req 0.01G)
I0801 13:29:15.793196  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.82G, req 0.01G)
I0801 13:29:15.809177  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.81G, req 0.01G)
I0801 13:29:15.811017  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.72G, req 0.01G)
I0801 13:29:15.811231  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.81G, req 0.01G)
I0801 13:29:15.817015  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.8G, req 0.01G)
I0801 13:29:15.818725  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.71G, req 0.01G)
I0801 13:29:15.819743  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.8G, req 0.01G)
I0801 13:29:15.838498  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.78G, req 0.01G)
I0801 13:29:15.841948  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.69G, req 0.01G)
I0801 13:29:15.842185  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.78G, req 0.01G)
I0801 13:29:15.847117  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.77G, req 0.01G)
I0801 13:29:15.850098  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.68G, req 0.01G)
I0801 13:29:15.850919  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.77G, req 0.01G)
I0801 13:29:15.881527  3168 data_layer.cpp:97] [1] Parser threads: 1
I0801 13:29:15.881543  3168 data_layer.cpp:99] [1] Transformer threads: 1
I0801 13:29:15.886729  3156 data_layer.cpp:97] [0] Parser threads: 1
I0801 13:29:15.886739  3156 data_layer.cpp:99] [0] Transformer threads: 1
I0801 13:29:15.889241  3167 data_layer.cpp:97] [2] Parser threads: 1
I0801 13:29:15.889251  3167 data_layer.cpp:99] [2] Transformer threads: 1
I0801 13:29:15.891764  3162 solver.cpp:358] Iteration 0 (0.163392 s), loss = 0.000644316
I0801 13:29:15.891783  3162 solver.cpp:375]     Train net output #0: loss = 0.000644316 (* 1 = 0.000644316 loss)
I0801 13:29:15.891788  3162 sgd_solver.cpp:136] Iteration 0, lr = 0.01, m = 0.9
I0801 13:29:15.919473  3162 solver.cpp:358] Iteration 1 (0.0277002 s), loss = 0.0023243
I0801 13:29:15.919497  3162 solver.cpp:375]     Train net output #0: loss = 0.0023243 (* 1 = 0.0023243 loss)
I0801 13:29:15.930356  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 6.98G, req 0.01G)
I0801 13:29:15.930719  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 7.07G, req 0.01G)
I0801 13:29:15.931094  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 7.07G, req 0.01G)
I0801 13:29:15.938777  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.34G, req 0.01G)
I0801 13:29:15.940126  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.43G, req 0.01G)
I0801 13:29:15.941627  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.43G, req 0.01G)
I0801 13:29:15.952899  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.34G, req 0.01G)
I0801 13:29:15.955171  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.43G, req 0.01G)
I0801 13:29:15.955700  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.43G, req 0.01G)
I0801 13:29:15.960572  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.34G, req 0.01G)
I0801 13:29:15.962363  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.01G)
I0801 13:29:15.962921  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.01G)
I0801 13:29:15.970629  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.34G, req 0.01G)
I0801 13:29:15.972206  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.43G, req 0.01G)
I0801 13:29:15.972919  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.43G, req 0.01G)
I0801 13:29:15.975869  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 0  (limit 6.34G, req 0.01G)
I0801 13:29:15.977636  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.01G)
I0801 13:29:15.978096  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 0  (limit 6.43G, req 0.01G)
I0801 13:29:16.001245  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.34G, req 0.02G)
I0801 13:29:16.003756  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.43G, req 0.02G)
I0801 13:29:16.004195  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.43G, req 0.02G)
I0801 13:29:16.009896  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.34G, req 0.02G)
I0801 13:29:16.011837  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.02G)
I0801 13:29:16.012176  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.43G, req 0.02G)
I0801 13:29:16.046934  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.34G, req 0.03G)
I0801 13:29:16.050330  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.43G, req 0.03G)
I0801 13:29:16.050546  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.43G, req 0.03G)
I0801 13:29:16.055833  3162 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.34G, req 0.03G)
I0801 13:29:16.059402  3164 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.43G, req 0.03G)
I0801 13:29:16.059744  3163 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.43G, req 0.03G)
I0801 13:29:16.077657  3162 solver.cpp:358] Iteration 2 (0.158173 s), loss = 0.000399625
I0801 13:29:16.077674  3162 solver.cpp:375]     Train net output #0: loss = 0.000399625 (* 1 = 0.000399625 loss)
I0801 13:29:16.077690  3163 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0801 13:29:16.077702  3162 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0801 13:29:16.077852  3164 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0801 13:29:17.624125  3162 solver.cpp:353] Iteration 100 (63.3722 iter/s, 1.54642s/98 iter), loss = 0.000982303
I0801 13:29:17.624174  3162 solver.cpp:375]     Train net output #0: loss = 0.000982303 (* 1 = 0.000982303 loss)
I0801 13:29:17.624188  3162 sgd_solver.cpp:136] Iteration 100, lr = 0.00998437, m = 0.9
I0801 13:29:19.204876  3162 solver.cpp:353] Iteration 200 (63.2631 iter/s, 1.5807s/100 iter), loss = 0.00218474
I0801 13:29:19.204944  3162 solver.cpp:375]     Train net output #0: loss = 0.00218474 (* 1 = 0.00218474 loss)
I0801 13:29:19.204962  3162 sgd_solver.cpp:136] Iteration 200, lr = 0.00996875, m = 0.9
I0801 13:29:20.789155  3162 solver.cpp:353] Iteration 300 (63.1223 iter/s, 1.58423s/100 iter), loss = 0.000952938
I0801 13:29:20.789180  3162 solver.cpp:375]     Train net output #0: loss = 0.000952939 (* 1 = 0.000952939 loss)
I0801 13:29:20.789186  3162 sgd_solver.cpp:136] Iteration 300, lr = 0.00995312, m = 0.9
I0801 13:29:22.380322  3162 solver.cpp:353] Iteration 400 (62.8489 iter/s, 1.59112s/100 iter), loss = 0.000873645
I0801 13:29:22.380347  3162 solver.cpp:375]     Train net output #0: loss = 0.000873646 (* 1 = 0.000873646 loss)
I0801 13:29:22.380352  3162 sgd_solver.cpp:136] Iteration 400, lr = 0.0099375, m = 0.9
I0801 13:29:23.970485  3162 solver.cpp:353] Iteration 500 (62.8887 iter/s, 1.59011s/100 iter), loss = 0.00223915
I0801 13:29:23.970546  3162 solver.cpp:375]     Train net output #0: loss = 0.00223915 (* 1 = 0.00223915 loss)
I0801 13:29:23.970562  3162 sgd_solver.cpp:136] Iteration 500, lr = 0.00992187, m = 0.9
I0801 13:29:25.554164  3162 solver.cpp:353] Iteration 600 (63.1461 iter/s, 1.58363s/100 iter), loss = 0.00193867
I0801 13:29:25.554217  3162 solver.cpp:375]     Train net output #0: loss = 0.00193867 (* 1 = 0.00193867 loss)
I0801 13:29:25.554235  3162 sgd_solver.cpp:136] Iteration 600, lr = 0.00990625, m = 0.9
I0801 13:29:27.123188  3162 solver.cpp:353] Iteration 700 (63.736 iter/s, 1.56897s/100 iter), loss = 0.00019915
I0801 13:29:27.123217  3162 solver.cpp:375]     Train net output #0: loss = 0.000199149 (* 1 = 0.000199149 loss)
I0801 13:29:27.123224  3162 sgd_solver.cpp:136] Iteration 700, lr = 0.00989062, m = 0.9
I0801 13:29:27.981598  3155 data_reader.cpp:264] Starting prefetch of epoch 1
I0801 13:29:28.704540  3162 solver.cpp:353] Iteration 800 (63.2391 iter/s, 1.5813s/100 iter), loss = 0.00237601
I0801 13:29:28.704565  3162 solver.cpp:375]     Train net output #0: loss = 0.00237601 (* 1 = 0.00237601 loss)
I0801 13:29:28.704571  3162 sgd_solver.cpp:136] Iteration 800, lr = 0.009875, m = 0.9
I0801 13:29:30.273176  3162 solver.cpp:353] Iteration 900 (63.7515 iter/s, 1.56859s/100 iter), loss = 0.000469317
I0801 13:29:30.273201  3162 solver.cpp:375]     Train net output #0: loss = 0.000469317 (* 1 = 0.000469317 loss)
I0801 13:29:30.273207  3162 sgd_solver.cpp:136] Iteration 900, lr = 0.00985937, m = 0.9
I0801 13:29:31.825103  3162 solver.cpp:550] Iteration 1000, Testing net (#0)
I0801 13:29:32.669004  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.914707
I0801 13:29:32.669023  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995588
I0801 13:29:32.669028  3162 solver.cpp:635]     Test net output #2: loss = 0.312955 (* 1 = 0.312955 loss)
I0801 13:29:32.669044  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.843918s
I0801 13:29:32.685055  3162 solver.cpp:353] Iteration 1000 (41.4627 iter/s, 2.41181s/100 iter), loss = 0.00061575
I0801 13:29:32.685073  3162 solver.cpp:375]     Train net output #0: loss = 0.00061575 (* 1 = 0.00061575 loss)
I0801 13:29:32.685079  3162 sgd_solver.cpp:136] Iteration 1000, lr = 0.00984375, m = 0.9
I0801 13:29:34.262768  3162 solver.cpp:353] Iteration 1100 (63.3849 iter/s, 1.57766s/100 iter), loss = 0.000395793
I0801 13:29:34.262815  3162 solver.cpp:375]     Train net output #0: loss = 0.000395793 (* 1 = 0.000395793 loss)
I0801 13:29:34.262830  3162 sgd_solver.cpp:136] Iteration 1100, lr = 0.00982813, m = 0.9
I0801 13:29:35.834139  3162 solver.cpp:353] Iteration 1200 (63.6406 iter/s, 1.57132s/100 iter), loss = 0.00133903
I0801 13:29:35.834162  3162 solver.cpp:375]     Train net output #0: loss = 0.00133903 (* 1 = 0.00133903 loss)
I0801 13:29:35.834167  3162 sgd_solver.cpp:136] Iteration 1200, lr = 0.0098125, m = 0.9
I0801 13:29:37.398190  3162 solver.cpp:353] Iteration 1300 (63.9386 iter/s, 1.564s/100 iter), loss = 0.000768105
I0801 13:29:37.398217  3162 solver.cpp:375]     Train net output #0: loss = 0.000768105 (* 1 = 0.000768105 loss)
I0801 13:29:37.398224  3162 sgd_solver.cpp:136] Iteration 1300, lr = 0.00979687, m = 0.9
I0801 13:29:38.973649  3162 solver.cpp:353] Iteration 1400 (63.4756 iter/s, 1.57541s/100 iter), loss = 0.00120894
I0801 13:29:38.973671  3162 solver.cpp:375]     Train net output #0: loss = 0.00120894 (* 1 = 0.00120894 loss)
I0801 13:29:38.973676  3162 sgd_solver.cpp:136] Iteration 1400, lr = 0.00978125, m = 0.9
I0801 13:29:40.540851  3162 solver.cpp:353] Iteration 1500 (63.8101 iter/s, 1.56715s/100 iter), loss = 0.000604387
I0801 13:29:40.540880  3162 solver.cpp:375]     Train net output #0: loss = 0.000604387 (* 1 = 0.000604387 loss)
I0801 13:29:40.540887  3162 sgd_solver.cpp:136] Iteration 1500, lr = 0.00976562, m = 0.9
I0801 13:29:42.120851  3162 solver.cpp:353] Iteration 1600 (63.2932 iter/s, 1.57995s/100 iter), loss = 0.000616028
I0801 13:29:42.120874  3162 solver.cpp:375]     Train net output #0: loss = 0.000616028 (* 1 = 0.000616028 loss)
I0801 13:29:42.120880  3162 sgd_solver.cpp:136] Iteration 1600, lr = 0.00975, m = 0.9
I0801 13:29:43.689839  3162 solver.cpp:353] Iteration 1700 (63.7373 iter/s, 1.56894s/100 iter), loss = 0.000947564
I0801 13:29:43.689863  3162 solver.cpp:375]     Train net output #0: loss = 0.000947564 (* 1 = 0.000947564 loss)
I0801 13:29:43.689869  3162 sgd_solver.cpp:136] Iteration 1700, lr = 0.00973437, m = 0.9
I0801 13:29:45.301298  3162 solver.cpp:353] Iteration 1800 (62.0575 iter/s, 1.61141s/100 iter), loss = 0.00104569
I0801 13:29:45.301399  3162 solver.cpp:375]     Train net output #0: loss = 0.00104569 (* 1 = 0.00104569 loss)
I0801 13:29:45.301406  3162 sgd_solver.cpp:136] Iteration 1800, lr = 0.00971875, m = 0.9
I0801 13:29:46.870904  3162 solver.cpp:353] Iteration 1900 (63.7122 iter/s, 1.56956s/100 iter), loss = 0.00066838
I0801 13:29:46.870931  3162 solver.cpp:375]     Train net output #0: loss = 0.000668379 (* 1 = 0.000668379 loss)
I0801 13:29:46.870936  3162 sgd_solver.cpp:136] Iteration 1900, lr = 0.00970312, m = 0.9
I0801 13:29:48.420970  3162 solver.cpp:550] Iteration 2000, Testing net (#0)
I0801 13:29:49.259698  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.913824
I0801 13:29:49.259716  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 13:29:49.259722  3162 solver.cpp:635]     Test net output #2: loss = 0.30619 (* 1 = 0.30619 loss)
I0801 13:29:49.259739  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.838746s
I0801 13:29:49.277870  3162 solver.cpp:353] Iteration 2000 (41.5473 iter/s, 2.40689s/100 iter), loss = 0.0005338
I0801 13:29:49.277899  3162 solver.cpp:375]     Train net output #0: loss = 0.0005338 (* 1 = 0.0005338 loss)
I0801 13:29:49.277904  3162 sgd_solver.cpp:136] Iteration 2000, lr = 0.0096875, m = 0.9
I0801 13:29:50.858700  3162 solver.cpp:353] Iteration 2100 (63.26 iter/s, 1.58078s/100 iter), loss = 0.000777376
I0801 13:29:50.858726  3162 solver.cpp:375]     Train net output #0: loss = 0.000777376 (* 1 = 0.000777376 loss)
I0801 13:29:50.858732  3162 sgd_solver.cpp:136] Iteration 2100, lr = 0.00967188, m = 0.9
I0801 13:29:52.425135  3162 solver.cpp:353] Iteration 2200 (63.8412 iter/s, 1.56639s/100 iter), loss = 0.00101096
I0801 13:29:52.425160  3162 solver.cpp:375]     Train net output #0: loss = 0.00101096 (* 1 = 0.00101096 loss)
I0801 13:29:52.425165  3162 sgd_solver.cpp:136] Iteration 2200, lr = 0.00965625, m = 0.9
I0801 13:29:53.997603  3162 solver.cpp:353] Iteration 2300 (63.5964 iter/s, 1.57242s/100 iter), loss = 0.000499038
I0801 13:29:53.997629  3162 solver.cpp:375]     Train net output #0: loss = 0.000499037 (* 1 = 0.000499037 loss)
I0801 13:29:53.997635  3162 sgd_solver.cpp:136] Iteration 2300, lr = 0.00964062, m = 0.9
I0801 13:29:55.571142  3162 solver.cpp:353] Iteration 2400 (63.5529 iter/s, 1.57349s/100 iter), loss = 0.000929002
I0801 13:29:55.571204  3162 solver.cpp:375]     Train net output #0: loss = 0.000929001 (* 1 = 0.000929001 loss)
I0801 13:29:55.571224  3162 sgd_solver.cpp:136] Iteration 2400, lr = 0.009625, m = 0.9
I0801 13:29:57.142524  3162 solver.cpp:353] Iteration 2500 (63.6403 iter/s, 1.57133s/100 iter), loss = 0.00106055
I0801 13:29:57.142551  3162 solver.cpp:375]     Train net output #0: loss = 0.00106055 (* 1 = 0.00106055 loss)
I0801 13:29:57.142556  3162 sgd_solver.cpp:136] Iteration 2500, lr = 0.00960938, m = 0.9
I0801 13:29:58.714123  3162 solver.cpp:353] Iteration 2600 (63.6315 iter/s, 1.57155s/100 iter), loss = 0.000959743
I0801 13:29:58.714146  3162 solver.cpp:375]     Train net output #0: loss = 0.000959743 (* 1 = 0.000959743 loss)
I0801 13:29:58.714150  3162 sgd_solver.cpp:136] Iteration 2600, lr = 0.00959375, m = 0.9
I0801 13:30:00.308389  3162 solver.cpp:353] Iteration 2700 (62.7266 iter/s, 1.59422s/100 iter), loss = 0.000694996
I0801 13:30:00.308413  3162 solver.cpp:375]     Train net output #0: loss = 0.000694996 (* 1 = 0.000694996 loss)
I0801 13:30:00.308420  3162 sgd_solver.cpp:136] Iteration 2700, lr = 0.00957812, m = 0.9
I0801 13:30:01.866765  3162 solver.cpp:353] Iteration 2800 (64.1714 iter/s, 1.55833s/100 iter), loss = 0.000510695
I0801 13:30:01.866791  3162 solver.cpp:375]     Train net output #0: loss = 0.000510694 (* 1 = 0.000510694 loss)
I0801 13:30:01.866796  3162 sgd_solver.cpp:136] Iteration 2800, lr = 0.0095625, m = 0.9
I0801 13:30:03.448011  3162 solver.cpp:353] Iteration 2900 (63.2433 iter/s, 1.58119s/100 iter), loss = 0.00111062
I0801 13:30:03.448037  3162 solver.cpp:375]     Train net output #0: loss = 0.00111062 (* 1 = 0.00111062 loss)
I0801 13:30:03.448042  3162 sgd_solver.cpp:136] Iteration 2900, lr = 0.00954687, m = 0.9
I0801 13:30:05.013067  3162 solver.cpp:550] Iteration 3000, Testing net (#0)
I0801 13:30:05.848307  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.920883
I0801 13:30:05.848325  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 13:30:05.848332  3162 solver.cpp:635]     Test net output #2: loss = 0.289071 (* 1 = 0.289071 loss)
I0801 13:30:05.848352  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.83526s
I0801 13:30:05.864193  3162 solver.cpp:353] Iteration 3000 (41.3889 iter/s, 2.41611s/100 iter), loss = 0.00152305
I0801 13:30:05.864223  3162 solver.cpp:375]     Train net output #0: loss = 0.00152305 (* 1 = 0.00152305 loss)
I0801 13:30:05.864228  3162 sgd_solver.cpp:136] Iteration 3000, lr = 0.00953125, m = 0.9
I0801 13:30:07.439182  3162 solver.cpp:353] Iteration 3100 (63.4945 iter/s, 1.57494s/100 iter), loss = 0.000504059
I0801 13:30:07.439211  3162 solver.cpp:375]     Train net output #0: loss = 0.000504058 (* 1 = 0.000504058 loss)
I0801 13:30:07.439218  3162 sgd_solver.cpp:136] Iteration 3100, lr = 0.00951563, m = 0.9
I0801 13:30:09.006346  3162 solver.cpp:353] Iteration 3200 (63.8116 iter/s, 1.56711s/100 iter), loss = 0.000869849
I0801 13:30:09.006373  3162 solver.cpp:375]     Train net output #0: loss = 0.000869849 (* 1 = 0.000869849 loss)
I0801 13:30:09.006381  3162 sgd_solver.cpp:136] Iteration 3200, lr = 0.0095, m = 0.9
I0801 13:30:10.567947  3162 solver.cpp:353] Iteration 3300 (64.0389 iter/s, 1.56155s/100 iter), loss = 0.00133201
I0801 13:30:10.567973  3162 solver.cpp:375]     Train net output #0: loss = 0.00133201 (* 1 = 0.00133201 loss)
I0801 13:30:10.567980  3162 sgd_solver.cpp:136] Iteration 3300, lr = 0.00948437, m = 0.9
I0801 13:30:12.153888  3162 solver.cpp:353] Iteration 3400 (63.056 iter/s, 1.58589s/100 iter), loss = 0.000910939
I0801 13:30:12.153916  3162 solver.cpp:375]     Train net output #0: loss = 0.000910938 (* 1 = 0.000910938 loss)
I0801 13:30:12.153923  3162 sgd_solver.cpp:136] Iteration 3400, lr = 0.00946875, m = 0.9
I0801 13:30:13.738107  3162 solver.cpp:353] Iteration 3500 (63.1246 iter/s, 1.58417s/100 iter), loss = 0.00133019
I0801 13:30:13.738154  3162 solver.cpp:375]     Train net output #0: loss = 0.00133019 (* 1 = 0.00133019 loss)
I0801 13:30:13.738168  3162 sgd_solver.cpp:136] Iteration 3500, lr = 0.00945312, m = 0.9
I0801 13:30:15.335289  3162 solver.cpp:353] Iteration 3600 (62.6123 iter/s, 1.59713s/100 iter), loss = 0.000561474
I0801 13:30:15.335400  3162 solver.cpp:375]     Train net output #0: loss = 0.000561474 (* 1 = 0.000561474 loss)
I0801 13:30:15.335409  3162 sgd_solver.cpp:136] Iteration 3600, lr = 0.0094375, m = 0.9
I0801 13:30:16.904639  3162 solver.cpp:353] Iteration 3700 (63.7227 iter/s, 1.5693s/100 iter), loss = 0.00135393
I0801 13:30:16.904666  3162 solver.cpp:375]     Train net output #0: loss = 0.00135393 (* 1 = 0.00135393 loss)
I0801 13:30:16.904673  3162 sgd_solver.cpp:136] Iteration 3700, lr = 0.00942187, m = 0.9
I0801 13:30:18.476472  3162 solver.cpp:353] Iteration 3800 (63.6221 iter/s, 1.57178s/100 iter), loss = 0.00134666
I0801 13:30:18.476498  3162 solver.cpp:375]     Train net output #0: loss = 0.00134667 (* 1 = 0.00134667 loss)
I0801 13:30:18.476505  3162 sgd_solver.cpp:136] Iteration 3800, lr = 0.00940625, m = 0.9
I0801 13:30:20.048913  3162 solver.cpp:353] Iteration 3900 (63.5974 iter/s, 1.57239s/100 iter), loss = 0.00127371
I0801 13:30:20.048939  3162 solver.cpp:375]     Train net output #0: loss = 0.00127371 (* 1 = 0.00127371 loss)
I0801 13:30:20.048945  3162 sgd_solver.cpp:136] Iteration 3900, lr = 0.00939062, m = 0.9
I0801 13:30:21.598333  3162 solver.cpp:550] Iteration 4000, Testing net (#0)
I0801 13:30:22.435119  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.923824
I0801 13:30:22.435140  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 13:30:22.435147  3162 solver.cpp:635]     Test net output #2: loss = 0.265277 (* 1 = 0.265277 loss)
I0801 13:30:22.435166  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.836807s
I0801 13:30:22.455036  3162 solver.cpp:353] Iteration 4000 (41.5618 iter/s, 2.40605s/100 iter), loss = 0.00138526
I0801 13:30:22.455054  3162 solver.cpp:375]     Train net output #0: loss = 0.00138526 (* 1 = 0.00138526 loss)
I0801 13:30:22.455060  3162 sgd_solver.cpp:136] Iteration 4000, lr = 0.009375, m = 0.9
I0801 13:30:24.024078  3162 solver.cpp:353] Iteration 4100 (63.7354 iter/s, 1.56899s/100 iter), loss = 0.000911294
I0801 13:30:24.024147  3162 solver.cpp:375]     Train net output #0: loss = 0.000911294 (* 1 = 0.000911294 loss)
I0801 13:30:24.024168  3162 sgd_solver.cpp:136] Iteration 4100, lr = 0.00935937, m = 0.9
I0801 13:30:25.591672  3162 solver.cpp:353] Iteration 4200 (63.794 iter/s, 1.56755s/100 iter), loss = 0.00114406
I0801 13:30:25.591697  3162 solver.cpp:375]     Train net output #0: loss = 0.00114406 (* 1 = 0.00114406 loss)
I0801 13:30:25.591703  3162 sgd_solver.cpp:136] Iteration 4200, lr = 0.00934375, m = 0.9
I0801 13:30:27.170917  3162 solver.cpp:353] Iteration 4300 (63.3233 iter/s, 1.5792s/100 iter), loss = 0.00143925
I0801 13:30:27.170967  3162 solver.cpp:375]     Train net output #0: loss = 0.00143925 (* 1 = 0.00143925 loss)
I0801 13:30:27.170979  3162 sgd_solver.cpp:136] Iteration 4300, lr = 0.00932813, m = 0.9
I0801 13:30:28.759378  3162 solver.cpp:353] Iteration 4400 (62.956 iter/s, 1.58841s/100 iter), loss = 0.00203717
I0801 13:30:28.759402  3162 solver.cpp:375]     Train net output #0: loss = 0.00203717 (* 1 = 0.00203717 loss)
I0801 13:30:28.759407  3162 sgd_solver.cpp:136] Iteration 4400, lr = 0.0093125, m = 0.9
I0801 13:30:30.325217  3162 solver.cpp:353] Iteration 4500 (63.8655 iter/s, 1.56579s/100 iter), loss = 0.00106383
I0801 13:30:30.325242  3162 solver.cpp:375]     Train net output #0: loss = 0.00106382 (* 1 = 0.00106382 loss)
I0801 13:30:30.325248  3162 sgd_solver.cpp:136] Iteration 4500, lr = 0.00929687, m = 0.9
I0801 13:30:31.888751  3162 solver.cpp:353] Iteration 4600 (63.9597 iter/s, 1.56349s/100 iter), loss = 0.0003711
I0801 13:30:31.888775  3162 solver.cpp:375]     Train net output #0: loss = 0.0003711 (* 1 = 0.0003711 loss)
I0801 13:30:31.888782  3162 sgd_solver.cpp:136] Iteration 4600, lr = 0.00928125, m = 0.9
I0801 13:30:33.469310  3162 solver.cpp:353] Iteration 4700 (63.2709 iter/s, 1.58051s/100 iter), loss = 0.0027463
I0801 13:30:33.469377  3162 solver.cpp:375]     Train net output #0: loss = 0.0027463 (* 1 = 0.0027463 loss)
I0801 13:30:33.469396  3162 sgd_solver.cpp:136] Iteration 4700, lr = 0.00926562, m = 0.9
I0801 13:30:35.045347  3162 solver.cpp:353] Iteration 4800 (63.4522 iter/s, 1.57599s/100 iter), loss = 0.000925855
I0801 13:30:35.045372  3162 solver.cpp:375]     Train net output #0: loss = 0.000925854 (* 1 = 0.000925854 loss)
I0801 13:30:35.045377  3162 sgd_solver.cpp:136] Iteration 4800, lr = 0.00925, m = 0.9
I0801 13:30:36.620357  3162 solver.cpp:353] Iteration 4900 (63.4937 iter/s, 1.57496s/100 iter), loss = 0.00101231
I0801 13:30:36.620406  3162 solver.cpp:375]     Train net output #0: loss = 0.0010123 (* 1 = 0.0010123 loss)
I0801 13:30:36.620415  3162 sgd_solver.cpp:136] Iteration 4900, lr = 0.00923437, m = 0.9
I0801 13:30:38.198106  3162 solver.cpp:550] Iteration 5000, Testing net (#0)
I0801 13:30:38.886670  3160 data_reader.cpp:264] Starting prefetch of epoch 1
I0801 13:30:39.039919  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.926177
I0801 13:30:39.039939  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 13:30:39.039947  3162 solver.cpp:635]     Test net output #2: loss = 0.25179 (* 1 = 0.25179 loss)
I0801 13:30:39.039964  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.841835s
I0801 13:30:39.055541  3162 solver.cpp:353] Iteration 5000 (41.0659 iter/s, 2.43511s/100 iter), loss = 0.0011585
I0801 13:30:39.055572  3162 solver.cpp:375]     Train net output #0: loss = 0.0011585 (* 1 = 0.0011585 loss)
I0801 13:30:39.055584  3162 sgd_solver.cpp:136] Iteration 5000, lr = 0.00921875, m = 0.9
I0801 13:30:40.632364  3162 solver.cpp:353] Iteration 5100 (63.4208 iter/s, 1.57677s/100 iter), loss = 0.00159239
I0801 13:30:40.632417  3162 solver.cpp:375]     Train net output #0: loss = 0.00159239 (* 1 = 0.00159239 loss)
I0801 13:30:40.632431  3162 sgd_solver.cpp:136] Iteration 5100, lr = 0.00920312, m = 0.9
I0801 13:30:42.203094  3162 solver.cpp:353] Iteration 5200 (63.6667 iter/s, 1.57068s/100 iter), loss = 0.00243349
I0801 13:30:42.203146  3162 solver.cpp:375]     Train net output #0: loss = 0.00243349 (* 1 = 0.00243349 loss)
I0801 13:30:42.203161  3162 sgd_solver.cpp:136] Iteration 5200, lr = 0.0091875, m = 0.9
I0801 13:30:43.778520  3162 solver.cpp:353] Iteration 5300 (63.4768 iter/s, 1.57538s/100 iter), loss = 0.00206227
I0801 13:30:43.778548  3162 solver.cpp:375]     Train net output #0: loss = 0.00206227 (* 1 = 0.00206227 loss)
I0801 13:30:43.778553  3162 sgd_solver.cpp:136] Iteration 5300, lr = 0.00917188, m = 0.9
I0801 13:30:45.374445  3162 solver.cpp:353] Iteration 5400 (62.6615 iter/s, 1.59588s/100 iter), loss = 0.000611016
I0801 13:30:45.374569  3162 solver.cpp:375]     Train net output #0: loss = 0.000611015 (* 1 = 0.000611015 loss)
I0801 13:30:45.374588  3162 sgd_solver.cpp:136] Iteration 5400, lr = 0.00915625, m = 0.9
I0801 13:30:46.934321  3162 solver.cpp:353] Iteration 5500 (64.1097 iter/s, 1.55983s/100 iter), loss = 0.000581691
I0801 13:30:46.934367  3162 solver.cpp:375]     Train net output #0: loss = 0.000581689 (* 1 = 0.000581689 loss)
I0801 13:30:46.934375  3162 sgd_solver.cpp:136] Iteration 5500, lr = 0.00914062, m = 0.9
I0801 13:30:48.504204  3162 solver.cpp:353] Iteration 5600 (63.7011 iter/s, 1.56983s/100 iter), loss = 0.00248146
I0801 13:30:48.504253  3162 solver.cpp:375]     Train net output #0: loss = 0.00248145 (* 1 = 0.00248145 loss)
I0801 13:30:48.504266  3162 sgd_solver.cpp:136] Iteration 5600, lr = 0.009125, m = 0.9
I0801 13:30:50.088028  3162 solver.cpp:353] Iteration 5700 (63.1403 iter/s, 1.58378s/100 iter), loss = 0.000499506
I0801 13:30:50.088093  3162 solver.cpp:375]     Train net output #0: loss = 0.000499505 (* 1 = 0.000499505 loss)
I0801 13:30:50.088114  3162 sgd_solver.cpp:136] Iteration 5700, lr = 0.00910938, m = 0.9
I0801 13:30:51.657032  3162 solver.cpp:353] Iteration 5800 (63.7368 iter/s, 1.56895s/100 iter), loss = 0.000713967
I0801 13:30:51.657096  3162 solver.cpp:375]     Train net output #0: loss = 0.000713966 (* 1 = 0.000713966 loss)
I0801 13:30:51.657115  3162 sgd_solver.cpp:136] Iteration 5800, lr = 0.00909375, m = 0.9
I0801 13:30:53.242010  3162 solver.cpp:353] Iteration 5900 (63.0944 iter/s, 1.58493s/100 iter), loss = 0.00209601
I0801 13:30:53.242034  3162 solver.cpp:375]     Train net output #0: loss = 0.00209601 (* 1 = 0.00209601 loss)
I0801 13:30:53.242038  3162 sgd_solver.cpp:136] Iteration 5900, lr = 0.00907812, m = 0.9
I0801 13:30:54.810497  3162 solver.cpp:550] Iteration 6000, Testing net (#0)
I0801 13:30:55.645879  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.925001
I0801 13:30:55.645901  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 13:30:55.645907  3162 solver.cpp:635]     Test net output #2: loss = 0.254283 (* 1 = 0.254283 loss)
I0801 13:30:55.646004  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.835483s
I0801 13:30:55.661891  3162 solver.cpp:353] Iteration 6000 (41.3255 iter/s, 2.41981s/100 iter), loss = 0.000416732
I0801 13:30:55.661911  3162 solver.cpp:375]     Train net output #0: loss = 0.000416731 (* 1 = 0.000416731 loss)
I0801 13:30:55.661916  3162 sgd_solver.cpp:136] Iteration 6000, lr = 0.0090625, m = 0.9
I0801 13:30:57.245215  3162 solver.cpp:353] Iteration 6100 (63.1604 iter/s, 1.58327s/100 iter), loss = 0.00303363
I0801 13:30:57.245240  3162 solver.cpp:375]     Train net output #0: loss = 0.00303363 (* 1 = 0.00303363 loss)
I0801 13:30:57.245246  3162 sgd_solver.cpp:136] Iteration 6100, lr = 0.00904687, m = 0.9
I0801 13:30:58.834581  3162 solver.cpp:353] Iteration 6200 (62.9201 iter/s, 1.58932s/100 iter), loss = 0.00108963
I0801 13:30:58.834609  3162 solver.cpp:375]     Train net output #0: loss = 0.00108962 (* 1 = 0.00108962 loss)
I0801 13:30:58.834614  3162 sgd_solver.cpp:136] Iteration 6200, lr = 0.00903125, m = 0.9
I0801 13:31:00.420506  3162 solver.cpp:353] Iteration 6300 (63.0566 iter/s, 1.58588s/100 iter), loss = 0.000840407
I0801 13:31:00.420531  3162 solver.cpp:375]     Train net output #0: loss = 0.000840406 (* 1 = 0.000840406 loss)
I0801 13:31:00.420537  3162 sgd_solver.cpp:136] Iteration 6300, lr = 0.00901563, m = 0.9
I0801 13:31:01.976672  3162 solver.cpp:353] Iteration 6400 (64.2626 iter/s, 1.55612s/100 iter), loss = 0.00129467
I0801 13:31:01.976723  3162 solver.cpp:375]     Train net output #0: loss = 0.00129467 (* 1 = 0.00129467 loss)
I0801 13:31:01.976738  3162 sgd_solver.cpp:136] Iteration 6400, lr = 0.009, m = 0.9
I0801 13:31:03.538628  3162 solver.cpp:353] Iteration 6500 (64.0243 iter/s, 1.56191s/100 iter), loss = 0.00221487
I0801 13:31:03.538693  3162 solver.cpp:375]     Train net output #0: loss = 0.00221487 (* 1 = 0.00221487 loss)
I0801 13:31:03.538712  3162 sgd_solver.cpp:136] Iteration 6500, lr = 0.00898437, m = 0.9
I0801 13:31:05.105468  3162 solver.cpp:353] Iteration 6600 (63.8247 iter/s, 1.56679s/100 iter), loss = 0.00145209
I0801 13:31:05.105492  3162 solver.cpp:375]     Train net output #0: loss = 0.00145209 (* 1 = 0.00145209 loss)
I0801 13:31:05.105499  3162 sgd_solver.cpp:136] Iteration 6600, lr = 0.00896875, m = 0.9
I0801 13:31:06.667665  3162 solver.cpp:353] Iteration 6700 (64.0145 iter/s, 1.56215s/100 iter), loss = 0.000583105
I0801 13:31:06.667691  3162 solver.cpp:375]     Train net output #0: loss = 0.000583104 (* 1 = 0.000583104 loss)
I0801 13:31:06.667695  3162 sgd_solver.cpp:136] Iteration 6700, lr = 0.00895312, m = 0.9
I0801 13:31:08.237725  3162 solver.cpp:353] Iteration 6800 (63.6937 iter/s, 1.57001s/100 iter), loss = 0.000802125
I0801 13:31:08.237751  3162 solver.cpp:375]     Train net output #0: loss = 0.000802125 (* 1 = 0.000802125 loss)
I0801 13:31:08.237756  3162 sgd_solver.cpp:136] Iteration 6800, lr = 0.0089375, m = 0.9
I0801 13:31:09.803293  3162 solver.cpp:353] Iteration 6900 (63.8766 iter/s, 1.56552s/100 iter), loss = 0.000980888
I0801 13:31:09.803323  3162 solver.cpp:375]     Train net output #0: loss = 0.000980888 (* 1 = 0.000980888 loss)
I0801 13:31:09.803329  3162 sgd_solver.cpp:136] Iteration 6900, lr = 0.00892187, m = 0.9
I0801 13:31:11.366915  3162 solver.cpp:550] Iteration 7000, Testing net (#0)
I0801 13:31:12.204002  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.925001
I0801 13:31:12.204021  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 13:31:12.204027  3162 solver.cpp:635]     Test net output #2: loss = 0.265721 (* 1 = 0.265721 loss)
I0801 13:31:12.204041  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.837104s
I0801 13:31:12.219604  3162 solver.cpp:353] Iteration 7000 (41.3866 iter/s, 2.41624s/100 iter), loss = 0.00175226
I0801 13:31:12.219621  3162 solver.cpp:375]     Train net output #0: loss = 0.00175226 (* 1 = 0.00175226 loss)
I0801 13:31:12.219625  3162 sgd_solver.cpp:136] Iteration 7000, lr = 0.00890625, m = 0.9
I0801 13:31:13.781908  3162 solver.cpp:353] Iteration 7100 (64.0101 iter/s, 1.56225s/100 iter), loss = 0.00102344
I0801 13:31:13.781971  3162 solver.cpp:375]     Train net output #0: loss = 0.00102344 (* 1 = 0.00102344 loss)
I0801 13:31:13.781991  3162 sgd_solver.cpp:136] Iteration 7100, lr = 0.00889063, m = 0.9
I0801 13:31:15.354163  3162 solver.cpp:353] Iteration 7200 (63.6049 iter/s, 1.57221s/100 iter), loss = 0.00190367
I0801 13:31:15.354188  3162 solver.cpp:375]     Train net output #0: loss = 0.00190367 (* 1 = 0.00190367 loss)
I0801 13:31:15.354194  3162 sgd_solver.cpp:136] Iteration 7200, lr = 0.008875, m = 0.9
I0801 13:31:16.917906  3162 solver.cpp:353] Iteration 7300 (63.9512 iter/s, 1.56369s/100 iter), loss = 0.0014461
I0801 13:31:16.917965  3162 solver.cpp:375]     Train net output #0: loss = 0.0014461 (* 1 = 0.0014461 loss)
I0801 13:31:16.917973  3162 sgd_solver.cpp:136] Iteration 7300, lr = 0.00885937, m = 0.9
I0801 13:31:18.500180  3162 solver.cpp:353] Iteration 7400 (63.2021 iter/s, 1.58223s/100 iter), loss = 0.00140969
I0801 13:31:18.500228  3162 solver.cpp:375]     Train net output #0: loss = 0.00140969 (* 1 = 0.00140969 loss)
I0801 13:31:18.500244  3162 sgd_solver.cpp:136] Iteration 7400, lr = 0.00884375, m = 0.9
I0801 13:31:20.082496  3162 solver.cpp:353] Iteration 7500 (63.2005 iter/s, 1.58227s/100 iter), loss = 0.000922829
I0801 13:31:20.082522  3162 solver.cpp:375]     Train net output #0: loss = 0.000922827 (* 1 = 0.000922827 loss)
I0801 13:31:20.082530  3162 sgd_solver.cpp:136] Iteration 7500, lr = 0.00882812, m = 0.9
I0801 13:31:21.657436  3162 solver.cpp:353] Iteration 7600 (63.4965 iter/s, 1.57489s/100 iter), loss = 0.00206065
I0801 13:31:21.657461  3162 solver.cpp:375]     Train net output #0: loss = 0.00206065 (* 1 = 0.00206065 loss)
I0801 13:31:21.657467  3162 sgd_solver.cpp:136] Iteration 7600, lr = 0.0088125, m = 0.9
I0801 13:31:23.219352  3162 solver.cpp:353] Iteration 7700 (64.0259 iter/s, 1.56187s/100 iter), loss = 0.000426825
I0801 13:31:23.219415  3162 solver.cpp:375]     Train net output #0: loss = 0.000426824 (* 1 = 0.000426824 loss)
I0801 13:31:23.219436  3162 sgd_solver.cpp:136] Iteration 7700, lr = 0.00879687, m = 0.9
I0801 13:31:24.795656  3162 solver.cpp:353] Iteration 7800 (63.4415 iter/s, 1.57626s/100 iter), loss = 0.00234699
I0801 13:31:24.795704  3162 solver.cpp:375]     Train net output #0: loss = 0.00234699 (* 1 = 0.00234699 loss)
I0801 13:31:24.795712  3162 sgd_solver.cpp:136] Iteration 7800, lr = 0.00878125, m = 0.9
I0801 13:31:26.384944  3162 solver.cpp:353] Iteration 7900 (62.9233 iter/s, 1.58924s/100 iter), loss = 0.00164509
I0801 13:31:26.384973  3162 solver.cpp:375]     Train net output #0: loss = 0.00164509 (* 1 = 0.00164509 loss)
I0801 13:31:26.384979  3162 sgd_solver.cpp:136] Iteration 7900, lr = 0.00876562, m = 0.9
I0801 13:31:27.944139  3162 solver.cpp:550] Iteration 8000, Testing net (#0)
I0801 13:31:28.779789  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.918824
I0801 13:31:28.779808  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 13:31:28.779815  3162 solver.cpp:635]     Test net output #2: loss = 0.297845 (* 1 = 0.297845 loss)
I0801 13:31:28.779834  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.835672s
I0801 13:31:28.795506  3162 solver.cpp:353] Iteration 8000 (41.4853 iter/s, 2.41049s/100 iter), loss = 0.00351008
I0801 13:31:28.795526  3162 solver.cpp:375]     Train net output #0: loss = 0.00351007 (* 1 = 0.00351007 loss)
I0801 13:31:28.795531  3162 sgd_solver.cpp:136] Iteration 8000, lr = 0.00875, m = 0.9
I0801 13:31:30.358840  3162 solver.cpp:353] Iteration 8100 (63.968 iter/s, 1.56328s/100 iter), loss = 0.00187104
I0801 13:31:30.358866  3162 solver.cpp:375]     Train net output #0: loss = 0.00187104 (* 1 = 0.00187104 loss)
I0801 13:31:30.358872  3162 sgd_solver.cpp:136] Iteration 8100, lr = 0.00873438, m = 0.9
I0801 13:31:31.911736  3162 solver.cpp:353] Iteration 8200 (64.398 iter/s, 1.55284s/100 iter), loss = 0.000656529
I0801 13:31:31.911764  3162 solver.cpp:375]     Train net output #0: loss = 0.000656528 (* 1 = 0.000656528 loss)
I0801 13:31:31.911772  3162 sgd_solver.cpp:136] Iteration 8200, lr = 0.00871875, m = 0.9
I0801 13:31:33.498020  3162 solver.cpp:353] Iteration 8300 (63.0423 iter/s, 1.58624s/100 iter), loss = 0.00230785
I0801 13:31:33.498047  3162 solver.cpp:375]     Train net output #0: loss = 0.00230785 (* 1 = 0.00230785 loss)
I0801 13:31:33.498052  3162 sgd_solver.cpp:136] Iteration 8300, lr = 0.00870312, m = 0.9
I0801 13:31:35.075213  3162 solver.cpp:353] Iteration 8400 (63.4057 iter/s, 1.57714s/100 iter), loss = 0.00184507
I0801 13:31:35.075264  3162 solver.cpp:375]     Train net output #0: loss = 0.00184507 (* 1 = 0.00184507 loss)
I0801 13:31:35.075279  3162 sgd_solver.cpp:136] Iteration 8400, lr = 0.0086875, m = 0.9
I0801 13:31:36.637528  3162 solver.cpp:353] Iteration 8500 (64.0097 iter/s, 1.56226s/100 iter), loss = 0.00219064
I0801 13:31:36.637554  3162 solver.cpp:375]     Train net output #0: loss = 0.00219064 (* 1 = 0.00219064 loss)
I0801 13:31:36.637559  3162 sgd_solver.cpp:136] Iteration 8500, lr = 0.00867188, m = 0.9
I0801 13:31:38.204964  3162 solver.cpp:353] Iteration 8600 (63.8003 iter/s, 1.56739s/100 iter), loss = 0.000392103
I0801 13:31:38.204993  3162 solver.cpp:375]     Train net output #0: loss = 0.000392103 (* 1 = 0.000392103 loss)
I0801 13:31:38.204998  3162 sgd_solver.cpp:136] Iteration 8600, lr = 0.00865625, m = 0.9
I0801 13:31:39.785315  3162 solver.cpp:353] Iteration 8700 (63.2792 iter/s, 1.5803s/100 iter), loss = 0.00195381
I0801 13:31:39.785339  3162 solver.cpp:375]     Train net output #0: loss = 0.00195381 (* 1 = 0.00195381 loss)
I0801 13:31:39.785343  3162 sgd_solver.cpp:136] Iteration 8700, lr = 0.00864062, m = 0.9
I0801 13:31:41.371623  3162 solver.cpp:353] Iteration 8800 (63.0413 iter/s, 1.58626s/100 iter), loss = 0.00111174
I0801 13:31:41.371651  3162 solver.cpp:375]     Train net output #0: loss = 0.00111174 (* 1 = 0.00111174 loss)
I0801 13:31:41.371659  3162 sgd_solver.cpp:136] Iteration 8800, lr = 0.008625, m = 0.9
I0801 13:31:42.942541  3162 solver.cpp:353] Iteration 8900 (63.6592 iter/s, 1.57086s/100 iter), loss = 0.000753854
I0801 13:31:42.942569  3162 solver.cpp:375]     Train net output #0: loss = 0.000753854 (* 1 = 0.000753854 loss)
I0801 13:31:42.942576  3162 sgd_solver.cpp:136] Iteration 8900, lr = 0.00860937, m = 0.9
I0801 13:31:44.499622  3162 solver.cpp:550] Iteration 9000, Testing net (#0)
I0801 13:31:45.337705  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.915001
I0801 13:31:45.337724  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996176
I0801 13:31:45.337729  3162 solver.cpp:635]     Test net output #2: loss = 0.29666 (* 1 = 0.29666 loss)
I0801 13:31:45.337748  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.838103s
I0801 13:31:45.353518  3162 solver.cpp:353] Iteration 9000 (41.4782 iter/s, 2.41091s/100 iter), loss = 0.00147056
I0801 13:31:45.353538  3162 solver.cpp:375]     Train net output #0: loss = 0.00147056 (* 1 = 0.00147056 loss)
I0801 13:31:45.353543  3162 sgd_solver.cpp:136] Iteration 9000, lr = 0.00859375, m = 0.9
I0801 13:31:46.716778  3155 data_reader.cpp:264] Starting prefetch of epoch 2
I0801 13:31:46.921679  3162 solver.cpp:353] Iteration 9100 (63.7712 iter/s, 1.56811s/100 iter), loss = 0.00204251
I0801 13:31:46.921777  3162 solver.cpp:375]     Train net output #0: loss = 0.00204251 (* 1 = 0.00204251 loss)
I0801 13:31:46.921785  3162 sgd_solver.cpp:136] Iteration 9100, lr = 0.00857813, m = 0.9
I0801 13:31:48.493361  3162 solver.cpp:353] Iteration 9200 (63.628 iter/s, 1.57163s/100 iter), loss = 0.00299746
I0801 13:31:48.493409  3162 solver.cpp:375]     Train net output #0: loss = 0.00299746 (* 1 = 0.00299746 loss)
I0801 13:31:48.493422  3162 sgd_solver.cpp:136] Iteration 9200, lr = 0.0085625, m = 0.9
I0801 13:31:50.051386  3162 solver.cpp:353] Iteration 9300 (64.1859 iter/s, 1.55798s/100 iter), loss = 0.00117492
I0801 13:31:50.051414  3162 solver.cpp:375]     Train net output #0: loss = 0.00117492 (* 1 = 0.00117492 loss)
I0801 13:31:50.051420  3162 sgd_solver.cpp:136] Iteration 9300, lr = 0.00854687, m = 0.9
I0801 13:31:51.611081  3162 solver.cpp:353] Iteration 9400 (64.1171 iter/s, 1.55965s/100 iter), loss = 0.00224546
I0801 13:31:51.611107  3162 solver.cpp:375]     Train net output #0: loss = 0.00224546 (* 1 = 0.00224546 loss)
I0801 13:31:51.611114  3162 sgd_solver.cpp:136] Iteration 9400, lr = 0.00853125, m = 0.9
I0801 13:31:53.184434  3162 solver.cpp:353] Iteration 9500 (63.5606 iter/s, 1.5733s/100 iter), loss = 0.000844637
I0801 13:31:53.184459  3162 solver.cpp:375]     Train net output #0: loss = 0.000844638 (* 1 = 0.000844638 loss)
I0801 13:31:53.184465  3162 sgd_solver.cpp:136] Iteration 9500, lr = 0.00851563, m = 0.9
I0801 13:31:54.759958  3162 solver.cpp:353] Iteration 9600 (63.473 iter/s, 1.57547s/100 iter), loss = 0.00232089
I0801 13:31:54.759980  3162 solver.cpp:375]     Train net output #0: loss = 0.00232089 (* 1 = 0.00232089 loss)
I0801 13:31:54.759985  3162 sgd_solver.cpp:136] Iteration 9600, lr = 0.0085, m = 0.9
I0801 13:31:56.331617  3162 solver.cpp:353] Iteration 9700 (63.629 iter/s, 1.57161s/100 iter), loss = 0.00185868
I0801 13:31:56.331642  3162 solver.cpp:375]     Train net output #0: loss = 0.00185868 (* 1 = 0.00185868 loss)
I0801 13:31:56.331648  3162 sgd_solver.cpp:136] Iteration 9700, lr = 0.00848437, m = 0.9
I0801 13:31:57.898722  3162 solver.cpp:353] Iteration 9800 (63.8139 iter/s, 1.56706s/100 iter), loss = 0.00121665
I0801 13:31:57.898748  3162 solver.cpp:375]     Train net output #0: loss = 0.00121665 (* 1 = 0.00121665 loss)
I0801 13:31:57.898754  3162 sgd_solver.cpp:136] Iteration 9800, lr = 0.00846875, m = 0.9
I0801 13:31:59.478427  3162 solver.cpp:353] Iteration 9900 (63.3049 iter/s, 1.57966s/100 iter), loss = 0.000913426
I0801 13:31:59.478454  3162 solver.cpp:375]     Train net output #0: loss = 0.000913427 (* 1 = 0.000913427 loss)
I0801 13:31:59.478461  3162 sgd_solver.cpp:136] Iteration 9900, lr = 0.00845312, m = 0.9
I0801 13:32:01.048964  3162 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/cifar10_jacintonet11v2_iter_10000.caffemodel
I0801 13:32:01.064894  3162 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/cifar10_jacintonet11v2_iter_10000.solverstate
I0801 13:32:01.068630  3162 solver.cpp:550] Iteration 10000, Testing net (#0)
I0801 13:32:01.883955  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.915295
I0801 13:32:01.883973  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 13:32:01.883980  3162 solver.cpp:635]     Test net output #2: loss = 0.31164 (* 1 = 0.31164 loss)
I0801 13:32:01.883996  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.815342s
I0801 13:32:01.899680  3162 solver.cpp:353] Iteration 10000 (41.3021 iter/s, 2.42118s/100 iter), loss = 0.00173234
I0801 13:32:01.899701  3162 solver.cpp:375]     Train net output #0: loss = 0.00173234 (* 1 = 0.00173234 loss)
I0801 13:32:01.899706  3162 sgd_solver.cpp:136] Iteration 10000, lr = 0.0084375, m = 0.9
I0801 13:32:03.482095  3162 solver.cpp:353] Iteration 10100 (63.1966 iter/s, 1.58236s/100 iter), loss = 0.00226461
I0801 13:32:03.482121  3162 solver.cpp:375]     Train net output #0: loss = 0.00226461 (* 1 = 0.00226461 loss)
I0801 13:32:03.482127  3162 sgd_solver.cpp:136] Iteration 10100, lr = 0.00842187, m = 0.9
I0801 13:32:05.081702  3162 solver.cpp:353] Iteration 10200 (62.5173 iter/s, 1.59956s/100 iter), loss = 0.00208562
I0801 13:32:05.081727  3162 solver.cpp:375]     Train net output #0: loss = 0.00208563 (* 1 = 0.00208563 loss)
I0801 13:32:05.081732  3162 sgd_solver.cpp:136] Iteration 10200, lr = 0.00840625, m = 0.9
I0801 13:32:06.648265  3162 solver.cpp:353] Iteration 10300 (63.8361 iter/s, 1.56651s/100 iter), loss = 0.0011056
I0801 13:32:06.648293  3162 solver.cpp:375]     Train net output #0: loss = 0.0011056 (* 1 = 0.0011056 loss)
I0801 13:32:06.648298  3162 sgd_solver.cpp:136] Iteration 10300, lr = 0.00839063, m = 0.9
I0801 13:32:08.229797  3162 solver.cpp:353] Iteration 10400 (63.2317 iter/s, 1.58148s/100 iter), loss = 0.00124456
I0801 13:32:08.229843  3162 solver.cpp:375]     Train net output #0: loss = 0.00124456 (* 1 = 0.00124456 loss)
I0801 13:32:08.229851  3162 sgd_solver.cpp:136] Iteration 10400, lr = 0.008375, m = 0.9
I0801 13:32:09.796406  3162 solver.cpp:353] Iteration 10500 (63.8342 iter/s, 1.56656s/100 iter), loss = 0.00142947
I0801 13:32:09.796430  3162 solver.cpp:375]     Train net output #0: loss = 0.00142947 (* 1 = 0.00142947 loss)
I0801 13:32:09.796437  3162 sgd_solver.cpp:136] Iteration 10500, lr = 0.00835937, m = 0.9
I0801 13:32:11.378278  3162 solver.cpp:353] Iteration 10600 (63.2182 iter/s, 1.58182s/100 iter), loss = 0.00137402
I0801 13:32:11.378322  3162 solver.cpp:375]     Train net output #0: loss = 0.00137403 (* 1 = 0.00137403 loss)
I0801 13:32:11.378334  3162 sgd_solver.cpp:136] Iteration 10600, lr = 0.00834375, m = 0.9
I0801 13:32:12.945981  3162 solver.cpp:353] Iteration 10700 (63.7896 iter/s, 1.56765s/100 iter), loss = 0.00254085
I0801 13:32:12.946007  3162 solver.cpp:375]     Train net output #0: loss = 0.00254085 (* 1 = 0.00254085 loss)
I0801 13:32:12.946012  3162 sgd_solver.cpp:136] Iteration 10700, lr = 0.00832812, m = 0.9
I0801 13:32:14.519613  3162 solver.cpp:353] Iteration 10800 (63.5493 iter/s, 1.57358s/100 iter), loss = 0.00321216
I0801 13:32:14.519639  3162 solver.cpp:375]     Train net output #0: loss = 0.00321217 (* 1 = 0.00321217 loss)
I0801 13:32:14.519644  3162 sgd_solver.cpp:136] Iteration 10800, lr = 0.0083125, m = 0.9
I0801 13:32:16.086491  3162 solver.cpp:353] Iteration 10900 (63.8231 iter/s, 1.56683s/100 iter), loss = 0.000936151
I0801 13:32:16.086518  3162 solver.cpp:375]     Train net output #0: loss = 0.000936151 (* 1 = 0.000936151 loss)
I0801 13:32:16.086522  3162 sgd_solver.cpp:136] Iteration 10900, lr = 0.00829687, m = 0.9
I0801 13:32:17.651856  3162 solver.cpp:550] Iteration 11000, Testing net (#0)
I0801 13:32:18.484928  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.915589
I0801 13:32:18.484947  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 13:32:18.484952  3162 solver.cpp:635]     Test net output #2: loss = 0.303572 (* 1 = 0.303572 loss)
I0801 13:32:18.484968  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.83309s
I0801 13:32:18.501391  3162 solver.cpp:353] Iteration 11000 (41.4108 iter/s, 2.41483s/100 iter), loss = 0.00143811
I0801 13:32:18.501408  3162 solver.cpp:375]     Train net output #0: loss = 0.00143811 (* 1 = 0.00143811 loss)
I0801 13:32:18.501412  3162 sgd_solver.cpp:136] Iteration 11000, lr = 0.00828125, m = 0.9
I0801 13:32:20.089097  3162 solver.cpp:353] Iteration 11100 (62.986 iter/s, 1.58765s/100 iter), loss = 0.00113562
I0801 13:32:20.089148  3162 solver.cpp:375]     Train net output #0: loss = 0.00113562 (* 1 = 0.00113562 loss)
I0801 13:32:20.089162  3162 sgd_solver.cpp:136] Iteration 11100, lr = 0.00826562, m = 0.9
I0801 13:32:21.663923  3162 solver.cpp:353] Iteration 11200 (63.5011 iter/s, 1.57478s/100 iter), loss = 0.00220204
I0801 13:32:21.663949  3162 solver.cpp:375]     Train net output #0: loss = 0.00220204 (* 1 = 0.00220204 loss)
I0801 13:32:21.663954  3162 sgd_solver.cpp:136] Iteration 11200, lr = 0.00825, m = 0.9
I0801 13:32:23.242085  3162 solver.cpp:353] Iteration 11300 (63.3668 iter/s, 1.57811s/100 iter), loss = 0.00159607
I0801 13:32:23.242110  3162 solver.cpp:375]     Train net output #0: loss = 0.00159607 (* 1 = 0.00159607 loss)
I0801 13:32:23.242115  3162 sgd_solver.cpp:136] Iteration 11300, lr = 0.00823438, m = 0.9
I0801 13:32:24.815724  3162 solver.cpp:353] Iteration 11400 (63.5491 iter/s, 1.57359s/100 iter), loss = 0.00215345
I0801 13:32:24.815775  3162 solver.cpp:375]     Train net output #0: loss = 0.00215345 (* 1 = 0.00215345 loss)
I0801 13:32:24.815789  3162 sgd_solver.cpp:136] Iteration 11400, lr = 0.00821875, m = 0.9
I0801 13:32:26.391762  3162 solver.cpp:353] Iteration 11500 (63.4522 iter/s, 1.57599s/100 iter), loss = 0.00318156
I0801 13:32:26.391788  3162 solver.cpp:375]     Train net output #0: loss = 0.00318156 (* 1 = 0.00318156 loss)
I0801 13:32:26.391793  3162 sgd_solver.cpp:136] Iteration 11500, lr = 0.00820312, m = 0.9
I0801 13:32:27.954838  3162 solver.cpp:353] Iteration 11600 (63.9786 iter/s, 1.56302s/100 iter), loss = 0.00238473
I0801 13:32:27.954869  3162 solver.cpp:375]     Train net output #0: loss = 0.00238473 (* 1 = 0.00238473 loss)
I0801 13:32:27.954877  3162 sgd_solver.cpp:136] Iteration 11600, lr = 0.0081875, m = 0.9
I0801 13:32:29.541326  3162 solver.cpp:353] Iteration 11700 (63.0342 iter/s, 1.58644s/100 iter), loss = 0.000946664
I0801 13:32:29.541352  3162 solver.cpp:375]     Train net output #0: loss = 0.000946662 (* 1 = 0.000946662 loss)
I0801 13:32:29.541358  3162 sgd_solver.cpp:136] Iteration 11700, lr = 0.00817188, m = 0.9
I0801 13:32:31.109436  3162 solver.cpp:353] Iteration 11800 (63.7731 iter/s, 1.56806s/100 iter), loss = 0.00117587
I0801 13:32:31.109462  3162 solver.cpp:375]     Train net output #0: loss = 0.00117587 (* 1 = 0.00117587 loss)
I0801 13:32:31.109467  3162 sgd_solver.cpp:136] Iteration 11800, lr = 0.00815625, m = 0.9
I0801 13:32:32.686770  3162 solver.cpp:353] Iteration 11900 (63.4001 iter/s, 1.57728s/100 iter), loss = 0.00222217
I0801 13:32:32.686795  3162 solver.cpp:375]     Train net output #0: loss = 0.00222217 (* 1 = 0.00222217 loss)
I0801 13:32:32.686800  3162 sgd_solver.cpp:136] Iteration 11900, lr = 0.00814062, m = 0.9
I0801 13:32:34.256557  3162 solver.cpp:550] Iteration 12000, Testing net (#0)
I0801 13:32:35.094554  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.909707
I0801 13:32:35.094574  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.994412
I0801 13:32:35.094581  3162 solver.cpp:635]     Test net output #2: loss = 0.316603 (* 1 = 0.316603 loss)
I0801 13:32:35.094601  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.83802s
I0801 13:32:35.110218  3162 solver.cpp:353] Iteration 12000 (41.2647 iter/s, 2.42338s/100 iter), loss = 0.000600822
I0801 13:32:35.110252  3162 solver.cpp:375]     Train net output #0: loss = 0.00060082 (* 1 = 0.00060082 loss)
I0801 13:32:35.110266  3162 sgd_solver.cpp:136] Iteration 12000, lr = 0.008125, m = 0.9
I0801 13:32:36.673475  3162 solver.cpp:353] Iteration 12100 (63.971 iter/s, 1.56321s/100 iter), loss = 0.000988703
I0801 13:32:36.673501  3162 solver.cpp:375]     Train net output #0: loss = 0.000988702 (* 1 = 0.000988702 loss)
I0801 13:32:36.673506  3162 sgd_solver.cpp:136] Iteration 12100, lr = 0.00810937, m = 0.9
I0801 13:32:38.237421  3162 solver.cpp:353] Iteration 12200 (63.9429 iter/s, 1.56389s/100 iter), loss = 0.00309733
I0801 13:32:38.237445  3162 solver.cpp:375]     Train net output #0: loss = 0.00309733 (* 1 = 0.00309733 loss)
I0801 13:32:38.237452  3162 sgd_solver.cpp:136] Iteration 12200, lr = 0.00809375, m = 0.9
I0801 13:32:39.807603  3162 solver.cpp:353] Iteration 12300 (63.6887 iter/s, 1.57014s/100 iter), loss = 0.00202324
I0801 13:32:39.807631  3162 solver.cpp:375]     Train net output #0: loss = 0.00202324 (* 1 = 0.00202324 loss)
I0801 13:32:39.807636  3162 sgd_solver.cpp:136] Iteration 12300, lr = 0.00807813, m = 0.9
I0801 13:32:41.384300  3162 solver.cpp:353] Iteration 12400 (63.4259 iter/s, 1.57664s/100 iter), loss = 0.00176809
I0801 13:32:41.384330  3162 solver.cpp:375]     Train net output #0: loss = 0.00176809 (* 1 = 0.00176809 loss)
I0801 13:32:41.384336  3162 sgd_solver.cpp:136] Iteration 12400, lr = 0.0080625, m = 0.9
I0801 13:32:42.944650  3162 solver.cpp:353] Iteration 12500 (64.0903 iter/s, 1.5603s/100 iter), loss = 0.00111036
I0801 13:32:42.944797  3162 solver.cpp:375]     Train net output #0: loss = 0.00111035 (* 1 = 0.00111035 loss)
I0801 13:32:42.944821  3162 sgd_solver.cpp:136] Iteration 12500, lr = 0.00804687, m = 0.9
I0801 13:32:44.518271  3162 solver.cpp:353] Iteration 12600 (63.5496 iter/s, 1.57357s/100 iter), loss = 0.0011039
I0801 13:32:44.518296  3162 solver.cpp:375]     Train net output #0: loss = 0.00110389 (* 1 = 0.00110389 loss)
I0801 13:32:44.518302  3162 sgd_solver.cpp:136] Iteration 12600, lr = 0.00803125, m = 0.9
I0801 13:32:46.097681  3162 solver.cpp:353] Iteration 12700 (63.3168 iter/s, 1.57936s/100 iter), loss = 0.0011304
I0801 13:32:46.097707  3162 solver.cpp:375]     Train net output #0: loss = 0.00113039 (* 1 = 0.00113039 loss)
I0801 13:32:46.097713  3162 sgd_solver.cpp:136] Iteration 12700, lr = 0.00801562, m = 0.9
I0801 13:32:47.666867  3162 solver.cpp:353] Iteration 12800 (63.7293 iter/s, 1.56914s/100 iter), loss = 0.00278072
I0801 13:32:47.666945  3162 solver.cpp:375]     Train net output #0: loss = 0.00278072 (* 1 = 0.00278072 loss)
I0801 13:32:47.666954  3162 sgd_solver.cpp:136] Iteration 12800, lr = 0.008, m = 0.9
I0801 13:32:49.229045  3162 solver.cpp:353] Iteration 12900 (64.0152 iter/s, 1.56213s/100 iter), loss = 0.00136278
I0801 13:32:49.229070  3162 solver.cpp:375]     Train net output #0: loss = 0.00136277 (* 1 = 0.00136277 loss)
I0801 13:32:49.229074  3162 sgd_solver.cpp:136] Iteration 12900, lr = 0.00798437, m = 0.9
I0801 13:32:50.783123  3162 solver.cpp:550] Iteration 13000, Testing net (#0)
I0801 13:32:51.616657  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.91206
I0801 13:32:51.616675  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 13:32:51.616680  3162 solver.cpp:635]     Test net output #2: loss = 0.315384 (* 1 = 0.315384 loss)
I0801 13:32:51.616695  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.833548s
I0801 13:32:51.633438  3162 solver.cpp:353] Iteration 13000 (41.5918 iter/s, 2.40432s/100 iter), loss = 0.000915098
I0801 13:32:51.633458  3162 solver.cpp:375]     Train net output #0: loss = 0.000915094 (* 1 = 0.000915094 loss)
I0801 13:32:51.633462  3162 sgd_solver.cpp:136] Iteration 13000, lr = 0.00796875, m = 0.9
I0801 13:32:53.195667  3162 solver.cpp:353] Iteration 13100 (64.0132 iter/s, 1.56218s/100 iter), loss = 0.000996621
I0801 13:32:53.195691  3162 solver.cpp:375]     Train net output #0: loss = 0.000996617 (* 1 = 0.000996617 loss)
I0801 13:32:53.195698  3162 sgd_solver.cpp:136] Iteration 13100, lr = 0.00795313, m = 0.9
I0801 13:32:54.757623  3162 solver.cpp:353] Iteration 13200 (64.0242 iter/s, 1.56191s/100 iter), loss = 0.00246374
I0801 13:32:54.757650  3162 solver.cpp:375]     Train net output #0: loss = 0.00246374 (* 1 = 0.00246374 loss)
I0801 13:32:54.757658  3162 sgd_solver.cpp:136] Iteration 13200, lr = 0.0079375, m = 0.9
I0801 13:32:56.347180  3162 solver.cpp:353] Iteration 13300 (62.9126 iter/s, 1.58951s/100 iter), loss = 0.00218026
I0801 13:32:56.347204  3162 solver.cpp:375]     Train net output #0: loss = 0.00218025 (* 1 = 0.00218025 loss)
I0801 13:32:56.347209  3162 sgd_solver.cpp:136] Iteration 13300, lr = 0.00792187, m = 0.9
I0801 13:32:57.912459  3162 solver.cpp:353] Iteration 13400 (63.8884 iter/s, 1.56523s/100 iter), loss = 0.000563878
I0801 13:32:57.912484  3162 solver.cpp:375]     Train net output #0: loss = 0.000563874 (* 1 = 0.000563874 loss)
I0801 13:32:57.912490  3162 sgd_solver.cpp:136] Iteration 13400, lr = 0.00790625, m = 0.9
I0801 13:32:59.474351  3162 solver.cpp:353] Iteration 13500 (64.027 iter/s, 1.56184s/100 iter), loss = 0.00158472
I0801 13:32:59.474405  3162 solver.cpp:375]     Train net output #0: loss = 0.00158472 (* 1 = 0.00158472 loss)
I0801 13:32:59.474418  3162 sgd_solver.cpp:136] Iteration 13500, lr = 0.00789062, m = 0.9
I0801 13:33:01.066910  3162 solver.cpp:353] Iteration 13600 (62.794 iter/s, 1.59251s/100 iter), loss = 0.000385782
I0801 13:33:01.066934  3162 solver.cpp:375]     Train net output #0: loss = 0.000385777 (* 1 = 0.000385777 loss)
I0801 13:33:01.066939  3162 sgd_solver.cpp:136] Iteration 13600, lr = 0.007875, m = 0.9
I0801 13:33:01.581765  3155 data_reader.cpp:264] Starting prefetch of epoch 3
I0801 13:33:02.642241  3162 solver.cpp:353] Iteration 13700 (63.4807 iter/s, 1.57528s/100 iter), loss = 0.00186768
I0801 13:33:02.642268  3162 solver.cpp:375]     Train net output #0: loss = 0.00186768 (* 1 = 0.00186768 loss)
I0801 13:33:02.642274  3162 sgd_solver.cpp:136] Iteration 13700, lr = 0.00785937, m = 0.9
I0801 13:33:04.226573  3162 solver.cpp:353] Iteration 13800 (63.1202 iter/s, 1.58428s/100 iter), loss = 0.00135685
I0801 13:33:04.226627  3162 solver.cpp:375]     Train net output #0: loss = 0.00135685 (* 1 = 0.00135685 loss)
I0801 13:33:04.226642  3162 sgd_solver.cpp:136] Iteration 13800, lr = 0.00784375, m = 0.9
I0801 13:33:05.804574  3162 solver.cpp:353] Iteration 13900 (63.3733 iter/s, 1.57795s/100 iter), loss = 0.00110649
I0801 13:33:05.804713  3162 solver.cpp:375]     Train net output #0: loss = 0.00110649 (* 1 = 0.00110649 loss)
I0801 13:33:05.804744  3162 sgd_solver.cpp:136] Iteration 13900, lr = 0.00782812, m = 0.9
I0801 13:33:07.360246  3162 solver.cpp:550] Iteration 14000, Testing net (#0)
I0801 13:33:08.195405  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.91706
I0801 13:33:08.195427  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 13:33:08.195433  3162 solver.cpp:635]     Test net output #2: loss = 0.304221 (* 1 = 0.304221 loss)
I0801 13:33:08.195454  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.835185s
I0801 13:33:08.211040  3162 solver.cpp:353] Iteration 14000 (41.5559 iter/s, 2.4064s/100 iter), loss = 0.00235621
I0801 13:33:08.211057  3162 solver.cpp:375]     Train net output #0: loss = 0.00235621 (* 1 = 0.00235621 loss)
I0801 13:33:08.211061  3162 sgd_solver.cpp:136] Iteration 14000, lr = 0.0078125, m = 0.9
I0801 13:33:09.784709  3162 solver.cpp:353] Iteration 14100 (63.5478 iter/s, 1.57362s/100 iter), loss = 0.00133989
I0801 13:33:09.784760  3162 solver.cpp:375]     Train net output #0: loss = 0.00133989 (* 1 = 0.00133989 loss)
I0801 13:33:09.784780  3162 sgd_solver.cpp:136] Iteration 14100, lr = 0.00779688, m = 0.9
I0801 13:33:11.368782  3162 solver.cpp:353] Iteration 14200 (63.1305 iter/s, 1.58402s/100 iter), loss = 0.000713062
I0801 13:33:11.368809  3162 solver.cpp:375]     Train net output #0: loss = 0.000713058 (* 1 = 0.000713058 loss)
I0801 13:33:11.368822  3162 sgd_solver.cpp:136] Iteration 14200, lr = 0.00778125, m = 0.9
I0801 13:33:12.928057  3162 solver.cpp:353] Iteration 14300 (64.1343 iter/s, 1.55923s/100 iter), loss = 0.00188834
I0801 13:33:12.928083  3162 solver.cpp:375]     Train net output #0: loss = 0.00188834 (* 1 = 0.00188834 loss)
I0801 13:33:12.928091  3162 sgd_solver.cpp:136] Iteration 14300, lr = 0.00776563, m = 0.9
I0801 13:33:14.492655  3162 solver.cpp:353] Iteration 14400 (63.9161 iter/s, 1.56455s/100 iter), loss = 0.00282532
I0801 13:33:14.492702  3162 solver.cpp:375]     Train net output #0: loss = 0.00282532 (* 1 = 0.00282532 loss)
I0801 13:33:14.492714  3162 sgd_solver.cpp:136] Iteration 14400, lr = 0.00775, m = 0.9
I0801 13:33:16.060449  3162 solver.cpp:353] Iteration 14500 (63.786 iter/s, 1.56774s/100 iter), loss = 0.00132761
I0801 13:33:16.060477  3162 solver.cpp:375]     Train net output #0: loss = 0.00132761 (* 1 = 0.00132761 loss)
I0801 13:33:16.060484  3162 sgd_solver.cpp:136] Iteration 14500, lr = 0.00773437, m = 0.9
I0801 13:33:17.622560  3162 solver.cpp:353] Iteration 14600 (64.0181 iter/s, 1.56206s/100 iter), loss = 0.00160064
I0801 13:33:17.622611  3162 solver.cpp:375]     Train net output #0: loss = 0.00160063 (* 1 = 0.00160063 loss)
I0801 13:33:17.622623  3162 sgd_solver.cpp:136] Iteration 14600, lr = 0.00771875, m = 0.9
I0801 13:33:19.186396  3162 solver.cpp:353] Iteration 14700 (63.9472 iter/s, 1.56379s/100 iter), loss = 0.00202809
I0801 13:33:19.186467  3162 solver.cpp:375]     Train net output #0: loss = 0.00202808 (* 1 = 0.00202808 loss)
I0801 13:33:19.186473  3162 sgd_solver.cpp:136] Iteration 14700, lr = 0.00770312, m = 0.9
I0801 13:33:20.746340  3162 solver.cpp:353] Iteration 14800 (64.1069 iter/s, 1.55989s/100 iter), loss = 0.00102306
I0801 13:33:20.746368  3162 solver.cpp:375]     Train net output #0: loss = 0.00102305 (* 1 = 0.00102305 loss)
I0801 13:33:20.746374  3162 sgd_solver.cpp:136] Iteration 14800, lr = 0.0076875, m = 0.9
I0801 13:33:22.325613  3162 solver.cpp:353] Iteration 14900 (63.3223 iter/s, 1.57922s/100 iter), loss = 0.0032764
I0801 13:33:22.325639  3162 solver.cpp:375]     Train net output #0: loss = 0.00327639 (* 1 = 0.00327639 loss)
I0801 13:33:22.325645  3162 sgd_solver.cpp:136] Iteration 14900, lr = 0.00767187, m = 0.9
I0801 13:33:23.872746  3162 solver.cpp:550] Iteration 15000, Testing net (#0)
I0801 13:33:24.713594  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.912648
I0801 13:33:24.713613  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 13:33:24.713618  3162 solver.cpp:635]     Test net output #2: loss = 0.303266 (* 1 = 0.303266 loss)
I0801 13:33:24.713634  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.840864s
I0801 13:33:24.729264  3162 solver.cpp:353] Iteration 15000 (41.6046 iter/s, 2.40358s/100 iter), loss = 0.00249473
I0801 13:33:24.729282  3162 solver.cpp:375]     Train net output #0: loss = 0.00249472 (* 1 = 0.00249472 loss)
I0801 13:33:24.729287  3162 sgd_solver.cpp:136] Iteration 15000, lr = 0.00765625, m = 0.9
I0801 13:33:26.334262  3162 solver.cpp:353] Iteration 15100 (62.3074 iter/s, 1.60495s/100 iter), loss = 0.00172766
I0801 13:33:26.334290  3162 solver.cpp:375]     Train net output #0: loss = 0.00172766 (* 1 = 0.00172766 loss)
I0801 13:33:26.334296  3162 sgd_solver.cpp:136] Iteration 15100, lr = 0.00764062, m = 0.9
I0801 13:33:27.908195  3162 solver.cpp:353] Iteration 15200 (63.5372 iter/s, 1.57388s/100 iter), loss = 0.000468749
I0801 13:33:27.908218  3162 solver.cpp:375]     Train net output #0: loss = 0.000468743 (* 1 = 0.000468743 loss)
I0801 13:33:27.908222  3162 sgd_solver.cpp:136] Iteration 15200, lr = 0.007625, m = 0.9
I0801 13:33:29.483650  3162 solver.cpp:353] Iteration 15300 (63.4756 iter/s, 1.57541s/100 iter), loss = 0.00093651
I0801 13:33:29.483675  3162 solver.cpp:375]     Train net output #0: loss = 0.000936505 (* 1 = 0.000936505 loss)
I0801 13:33:29.483681  3162 sgd_solver.cpp:136] Iteration 15300, lr = 0.00760937, m = 0.9
I0801 13:33:31.047931  3162 solver.cpp:353] Iteration 15400 (63.9291 iter/s, 1.56423s/100 iter), loss = 0.00665306
I0801 13:33:31.047958  3162 solver.cpp:375]     Train net output #0: loss = 0.00665305 (* 1 = 0.00665305 loss)
I0801 13:33:31.047965  3162 sgd_solver.cpp:136] Iteration 15400, lr = 0.00759375, m = 0.9
I0801 13:33:32.626648  3162 solver.cpp:353] Iteration 15500 (63.3447 iter/s, 1.57866s/100 iter), loss = 0.00197322
I0801 13:33:32.626698  3162 solver.cpp:375]     Train net output #0: loss = 0.00197321 (* 1 = 0.00197321 loss)
I0801 13:33:32.626710  3162 sgd_solver.cpp:136] Iteration 15500, lr = 0.00757812, m = 0.9
I0801 13:33:34.202169  3162 solver.cpp:353] Iteration 15600 (63.473 iter/s, 1.57547s/100 iter), loss = 0.000598932
I0801 13:33:34.202195  3162 solver.cpp:375]     Train net output #0: loss = 0.000598927 (* 1 = 0.000598927 loss)
I0801 13:33:34.202201  3162 sgd_solver.cpp:136] Iteration 15600, lr = 0.0075625, m = 0.9
I0801 13:33:35.786623  3162 solver.cpp:353] Iteration 15700 (63.1152 iter/s, 1.58441s/100 iter), loss = 0.0030882
I0801 13:33:35.786649  3162 solver.cpp:375]     Train net output #0: loss = 0.0030882 (* 1 = 0.0030882 loss)
I0801 13:33:35.786655  3162 sgd_solver.cpp:136] Iteration 15700, lr = 0.00754687, m = 0.9
I0801 13:33:37.364832  3162 solver.cpp:353] Iteration 15800 (63.365 iter/s, 1.57816s/100 iter), loss = 0.00111859
I0801 13:33:37.364980  3162 solver.cpp:375]     Train net output #0: loss = 0.00111858 (* 1 = 0.00111858 loss)
I0801 13:33:37.364998  3162 sgd_solver.cpp:136] Iteration 15800, lr = 0.00753125, m = 0.9
I0801 13:33:38.952788  3162 solver.cpp:353] Iteration 15900 (62.976 iter/s, 1.58791s/100 iter), loss = 0.00281346
I0801 13:33:38.952842  3162 solver.cpp:375]     Train net output #0: loss = 0.00281346 (* 1 = 0.00281346 loss)
I0801 13:33:38.952857  3162 sgd_solver.cpp:136] Iteration 15900, lr = 0.00751562, m = 0.9
I0801 13:33:40.507925  3162 solver.cpp:550] Iteration 16000, Testing net (#0)
I0801 13:33:41.342123  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.916766
I0801 13:33:41.342140  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 13:33:41.342145  3162 solver.cpp:635]     Test net output #2: loss = 0.291256 (* 1 = 0.291256 loss)
I0801 13:33:41.342162  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.834216s
I0801 13:33:41.357769  3162 solver.cpp:353] Iteration 16000 (41.5816 iter/s, 2.40491s/100 iter), loss = 0.00201662
I0801 13:33:41.357786  3162 solver.cpp:375]     Train net output #0: loss = 0.00201661 (* 1 = 0.00201661 loss)
I0801 13:33:41.357790  3162 sgd_solver.cpp:136] Iteration 16000, lr = 0.0075, m = 0.9
I0801 13:33:42.927525  3162 solver.cpp:353] Iteration 16100 (63.7064 iter/s, 1.5697s/100 iter), loss = 0.000833457
I0801 13:33:42.927554  3162 solver.cpp:375]     Train net output #0: loss = 0.000833451 (* 1 = 0.000833451 loss)
I0801 13:33:42.927562  3162 sgd_solver.cpp:136] Iteration 16100, lr = 0.00748438, m = 0.9
I0801 13:33:44.505568  3162 solver.cpp:353] Iteration 16200 (63.3716 iter/s, 1.57799s/100 iter), loss = 0.00126809
I0801 13:33:44.505594  3162 solver.cpp:375]     Train net output #0: loss = 0.00126808 (* 1 = 0.00126808 loss)
I0801 13:33:44.505601  3162 sgd_solver.cpp:136] Iteration 16200, lr = 0.00746875, m = 0.9
I0801 13:33:46.078114  3162 solver.cpp:353] Iteration 16300 (63.5932 iter/s, 1.57249s/100 iter), loss = 0.00415631
I0801 13:33:46.078141  3162 solver.cpp:375]     Train net output #0: loss = 0.00415631 (* 1 = 0.00415631 loss)
I0801 13:33:46.078148  3162 sgd_solver.cpp:136] Iteration 16300, lr = 0.00745312, m = 0.9
I0801 13:33:47.640938  3162 solver.cpp:353] Iteration 16400 (63.9887 iter/s, 1.56278s/100 iter), loss = 0.00241172
I0801 13:33:47.640992  3162 solver.cpp:375]     Train net output #0: loss = 0.00241171 (* 1 = 0.00241171 loss)
I0801 13:33:47.641016  3162 sgd_solver.cpp:136] Iteration 16400, lr = 0.0074375, m = 0.9
I0801 13:33:49.220197  3162 solver.cpp:353] Iteration 16500 (63.3229 iter/s, 1.57921s/100 iter), loss = 0.00157382
I0801 13:33:49.220273  3162 solver.cpp:375]     Train net output #0: loss = 0.00157382 (* 1 = 0.00157382 loss)
I0801 13:33:49.220280  3162 sgd_solver.cpp:136] Iteration 16500, lr = 0.00742187, m = 0.9
I0801 13:33:50.785255  3162 solver.cpp:353] Iteration 16600 (63.8974 iter/s, 1.56501s/100 iter), loss = 0.00242527
I0801 13:33:50.785305  3162 solver.cpp:375]     Train net output #0: loss = 0.00242527 (* 1 = 0.00242527 loss)
I0801 13:33:50.785317  3162 sgd_solver.cpp:136] Iteration 16600, lr = 0.00740625, m = 0.9
I0801 13:33:52.352520  3162 solver.cpp:353] Iteration 16700 (63.8074 iter/s, 1.56722s/100 iter), loss = 0.00531678
I0801 13:33:52.352547  3162 solver.cpp:375]     Train net output #0: loss = 0.00531677 (* 1 = 0.00531677 loss)
I0801 13:33:52.352553  3162 sgd_solver.cpp:136] Iteration 16700, lr = 0.00739062, m = 0.9
I0801 13:33:53.922783  3162 solver.cpp:353] Iteration 16800 (63.6855 iter/s, 1.57022s/100 iter), loss = 0.00352906
I0801 13:33:53.922808  3162 solver.cpp:375]     Train net output #0: loss = 0.00352906 (* 1 = 0.00352906 loss)
I0801 13:33:53.922814  3162 sgd_solver.cpp:136] Iteration 16800, lr = 0.007375, m = 0.9
I0801 13:33:55.513481  3162 solver.cpp:353] Iteration 16900 (62.8675 iter/s, 1.59065s/100 iter), loss = 0.00219706
I0801 13:33:55.513535  3162 solver.cpp:375]     Train net output #0: loss = 0.00219705 (* 1 = 0.00219705 loss)
I0801 13:33:55.513550  3162 sgd_solver.cpp:136] Iteration 16900, lr = 0.00735937, m = 0.9
I0801 13:33:57.079848  3162 solver.cpp:550] Iteration 17000, Testing net (#0)
I0801 13:33:57.910871  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.920883
I0801 13:33:57.910889  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 13:33:57.910894  3162 solver.cpp:635]     Test net output #2: loss = 0.26985 (* 1 = 0.26985 loss)
I0801 13:33:57.910909  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.831038s
I0801 13:33:57.926388  3162 solver.cpp:353] Iteration 17000 (41.445 iter/s, 2.41284s/100 iter), loss = 0.0026438
I0801 13:33:57.926409  3162 solver.cpp:375]     Train net output #0: loss = 0.00264379 (* 1 = 0.00264379 loss)
I0801 13:33:57.926414  3162 sgd_solver.cpp:136] Iteration 17000, lr = 0.00734375, m = 0.9
I0801 13:33:59.496019  3162 solver.cpp:353] Iteration 17100 (63.7114 iter/s, 1.56958s/100 iter), loss = 0.00154093
I0801 13:33:59.496044  3162 solver.cpp:375]     Train net output #0: loss = 0.00154092 (* 1 = 0.00154092 loss)
I0801 13:33:59.496048  3162 sgd_solver.cpp:136] Iteration 17100, lr = 0.00732813, m = 0.9
I0801 13:34:01.065747  3162 solver.cpp:353] Iteration 17200 (63.7072 iter/s, 1.56968s/100 iter), loss = 0.00070843
I0801 13:34:01.065773  3162 solver.cpp:375]     Train net output #0: loss = 0.000708421 (* 1 = 0.000708421 loss)
I0801 13:34:01.065778  3162 sgd_solver.cpp:136] Iteration 17200, lr = 0.0073125, m = 0.9
I0801 13:34:02.649802  3162 solver.cpp:353] Iteration 17300 (63.1311 iter/s, 1.58401s/100 iter), loss = 0.00158507
I0801 13:34:02.649829  3162 solver.cpp:375]     Train net output #0: loss = 0.00158506 (* 1 = 0.00158506 loss)
I0801 13:34:02.649837  3162 sgd_solver.cpp:136] Iteration 17300, lr = 0.00729688, m = 0.9
I0801 13:34:04.239154  3162 solver.cpp:353] Iteration 17400 (62.9208 iter/s, 1.5893s/100 iter), loss = 0.00180337
I0801 13:34:04.239178  3162 solver.cpp:375]     Train net output #0: loss = 0.00180336 (* 1 = 0.00180336 loss)
I0801 13:34:04.239183  3162 sgd_solver.cpp:136] Iteration 17400, lr = 0.00728125, m = 0.9
I0801 13:34:05.825044  3162 solver.cpp:353] Iteration 17500 (63.058 iter/s, 1.58584s/100 iter), loss = 0.0020986
I0801 13:34:05.825093  3162 solver.cpp:375]     Train net output #0: loss = 0.0020986 (* 1 = 0.0020986 loss)
I0801 13:34:05.825106  3162 sgd_solver.cpp:136] Iteration 17500, lr = 0.00726563, m = 0.9
I0801 13:34:07.412231  3162 solver.cpp:353] Iteration 17600 (63.0064 iter/s, 1.58714s/100 iter), loss = 0.0019625
I0801 13:34:07.412258  3162 solver.cpp:375]     Train net output #0: loss = 0.00196249 (* 1 = 0.00196249 loss)
I0801 13:34:07.412264  3162 sgd_solver.cpp:136] Iteration 17600, lr = 0.00725, m = 0.9
I0801 13:34:08.976567  3162 solver.cpp:353] Iteration 17700 (63.9269 iter/s, 1.56429s/100 iter), loss = 0.0016693
I0801 13:34:08.976619  3162 solver.cpp:375]     Train net output #0: loss = 0.0016693 (* 1 = 0.0016693 loss)
I0801 13:34:08.976632  3162 sgd_solver.cpp:136] Iteration 17700, lr = 0.00723437, m = 0.9
I0801 13:34:10.548921  3162 solver.cpp:353] Iteration 17800 (63.6009 iter/s, 1.5723s/100 iter), loss = 0.00311991
I0801 13:34:10.548972  3162 solver.cpp:375]     Train net output #0: loss = 0.00311991 (* 1 = 0.00311991 loss)
I0801 13:34:10.548985  3162 sgd_solver.cpp:136] Iteration 17800, lr = 0.00721875, m = 0.9
I0801 13:34:12.117951  3162 solver.cpp:353] Iteration 17900 (63.7356 iter/s, 1.56898s/100 iter), loss = 0.00559395
I0801 13:34:12.117977  3162 solver.cpp:375]     Train net output #0: loss = 0.00559394 (* 1 = 0.00559394 loss)
I0801 13:34:12.117983  3162 sgd_solver.cpp:136] Iteration 17900, lr = 0.00720312, m = 0.9
I0801 13:34:13.688712  3162 solver.cpp:550] Iteration 18000, Testing net (#0)
I0801 13:34:14.165210  3160 data_reader.cpp:264] Starting prefetch of epoch 2
I0801 13:34:14.525331  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.922942
I0801 13:34:14.525352  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 13:34:14.525358  3162 solver.cpp:635]     Test net output #2: loss = 0.263901 (* 1 = 0.263901 loss)
I0801 13:34:14.525377  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.836641s
I0801 13:34:14.541443  3162 solver.cpp:353] Iteration 18000 (41.264 iter/s, 2.42342s/100 iter), loss = 0.000623171
I0801 13:34:14.541462  3162 solver.cpp:375]     Train net output #0: loss = 0.000623163 (* 1 = 0.000623163 loss)
I0801 13:34:14.541468  3162 sgd_solver.cpp:136] Iteration 18000, lr = 0.0071875, m = 0.9
I0801 13:34:16.126477  3162 solver.cpp:353] Iteration 18100 (63.0923 iter/s, 1.58498s/100 iter), loss = 0.000662073
I0801 13:34:16.126545  3162 solver.cpp:375]     Train net output #0: loss = 0.000662066 (* 1 = 0.000662066 loss)
I0801 13:34:16.126564  3162 sgd_solver.cpp:136] Iteration 18100, lr = 0.00717187, m = 0.9
I0801 13:34:17.765187  3162 solver.cpp:353] Iteration 18200 (61.0255 iter/s, 1.63866s/100 iter), loss = 0.00146008
I0801 13:34:17.765213  3162 solver.cpp:375]     Train net output #0: loss = 0.00146007 (* 1 = 0.00146007 loss)
I0801 13:34:17.765219  3162 sgd_solver.cpp:136] Iteration 18200, lr = 0.00715625, m = 0.9
I0801 13:34:19.355630  3162 solver.cpp:353] Iteration 18300 (62.8775 iter/s, 1.59039s/100 iter), loss = 0.00298968
I0801 13:34:19.355727  3162 solver.cpp:375]     Train net output #0: loss = 0.00298968 (* 1 = 0.00298968 loss)
I0801 13:34:19.355736  3162 sgd_solver.cpp:136] Iteration 18300, lr = 0.00714062, m = 0.9
I0801 13:34:20.945196  3162 solver.cpp:353] Iteration 18400 (62.9121 iter/s, 1.58952s/100 iter), loss = 0.00151914
I0801 13:34:20.945246  3162 solver.cpp:375]     Train net output #0: loss = 0.00151913 (* 1 = 0.00151913 loss)
I0801 13:34:20.945261  3162 sgd_solver.cpp:136] Iteration 18400, lr = 0.007125, m = 0.9
I0801 13:34:22.523524  3162 solver.cpp:353] Iteration 18500 (63.3602 iter/s, 1.57828s/100 iter), loss = 0.000660336
I0801 13:34:22.523674  3162 solver.cpp:375]     Train net output #0: loss = 0.000660327 (* 1 = 0.000660327 loss)
I0801 13:34:22.523692  3162 sgd_solver.cpp:136] Iteration 18500, lr = 0.00710937, m = 0.9
I0801 13:34:24.118630  3162 solver.cpp:353] Iteration 18600 (62.6937 iter/s, 1.59506s/100 iter), loss = 0.00188875
I0801 13:34:24.118655  3162 solver.cpp:375]     Train net output #0: loss = 0.00188874 (* 1 = 0.00188874 loss)
I0801 13:34:24.118661  3162 sgd_solver.cpp:136] Iteration 18600, lr = 0.00709375, m = 0.9
I0801 13:34:25.722347  3162 solver.cpp:353] Iteration 18700 (62.3571 iter/s, 1.60367s/100 iter), loss = 0.00235744
I0801 13:34:25.722373  3162 solver.cpp:375]     Train net output #0: loss = 0.00235743 (* 1 = 0.00235743 loss)
I0801 13:34:25.722379  3162 sgd_solver.cpp:136] Iteration 18700, lr = 0.00707812, m = 0.9
I0801 13:34:27.334364  3162 solver.cpp:353] Iteration 18800 (62.036 iter/s, 1.61197s/100 iter), loss = 0.000901946
I0801 13:34:27.334389  3162 solver.cpp:375]     Train net output #0: loss = 0.000901937 (* 1 = 0.000901937 loss)
I0801 13:34:27.334395  3162 sgd_solver.cpp:136] Iteration 18800, lr = 0.0070625, m = 0.9
I0801 13:34:28.908154  3162 solver.cpp:353] Iteration 18900 (63.543 iter/s, 1.57374s/100 iter), loss = 0.00125271
I0801 13:34:28.908181  3162 solver.cpp:375]     Train net output #0: loss = 0.0012527 (* 1 = 0.0012527 loss)
I0801 13:34:28.908187  3162 sgd_solver.cpp:136] Iteration 18900, lr = 0.00704687, m = 0.9
I0801 13:34:30.455989  3162 solver.cpp:550] Iteration 19000, Testing net (#0)
I0801 13:34:31.289330  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.919119
I0801 13:34:31.289350  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 13:34:31.289355  3162 solver.cpp:635]     Test net output #2: loss = 0.288479 (* 1 = 0.288479 loss)
I0801 13:34:31.289374  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.833362s
I0801 13:34:31.304822  3162 solver.cpp:353] Iteration 19000 (41.7258 iter/s, 2.3966s/100 iter), loss = 0.00221078
I0801 13:34:31.304841  3162 solver.cpp:375]     Train net output #0: loss = 0.00221077 (* 1 = 0.00221077 loss)
I0801 13:34:31.304846  3162 sgd_solver.cpp:136] Iteration 19000, lr = 0.00703125, m = 0.9
I0801 13:34:32.890051  3162 solver.cpp:353] Iteration 19100 (63.0844 iter/s, 1.58518s/100 iter), loss = 0.00450826
I0801 13:34:32.890077  3162 solver.cpp:375]     Train net output #0: loss = 0.00450825 (* 1 = 0.00450825 loss)
I0801 13:34:32.890082  3162 sgd_solver.cpp:136] Iteration 19100, lr = 0.00701563, m = 0.9
I0801 13:34:34.462971  3162 solver.cpp:353] Iteration 19200 (63.578 iter/s, 1.57287s/100 iter), loss = 0.000936962
I0801 13:34:34.462998  3162 solver.cpp:375]     Train net output #0: loss = 0.000936951 (* 1 = 0.000936951 loss)
I0801 13:34:34.463004  3162 sgd_solver.cpp:136] Iteration 19200, lr = 0.007, m = 0.9
I0801 13:34:36.028941  3162 solver.cpp:353] Iteration 19300 (63.8602 iter/s, 1.56592s/100 iter), loss = 0.00150343
I0801 13:34:36.028969  3162 solver.cpp:375]     Train net output #0: loss = 0.00150342 (* 1 = 0.00150342 loss)
I0801 13:34:36.028975  3162 sgd_solver.cpp:136] Iteration 19300, lr = 0.00698437, m = 0.9
I0801 13:34:37.622730  3162 solver.cpp:353] Iteration 19400 (62.7456 iter/s, 1.59374s/100 iter), loss = 0.00143186
I0801 13:34:37.622781  3162 solver.cpp:375]     Train net output #0: loss = 0.00143185 (* 1 = 0.00143185 loss)
I0801 13:34:37.622789  3162 sgd_solver.cpp:136] Iteration 19400, lr = 0.00696875, m = 0.9
I0801 13:34:39.198586  3162 solver.cpp:353] Iteration 19500 (63.4597 iter/s, 1.5758s/100 iter), loss = 0.00154799
I0801 13:34:39.198613  3162 solver.cpp:375]     Train net output #0: loss = 0.00154798 (* 1 = 0.00154798 loss)
I0801 13:34:39.198621  3162 sgd_solver.cpp:136] Iteration 19500, lr = 0.00695312, m = 0.9
I0801 13:34:40.762833  3162 solver.cpp:353] Iteration 19600 (63.9305 iter/s, 1.5642s/100 iter), loss = 0.0053359
I0801 13:34:40.762857  3162 solver.cpp:375]     Train net output #0: loss = 0.00533589 (* 1 = 0.00533589 loss)
I0801 13:34:40.762864  3162 sgd_solver.cpp:136] Iteration 19600, lr = 0.0069375, m = 0.9
I0801 13:34:42.343276  3162 solver.cpp:353] Iteration 19700 (63.2754 iter/s, 1.58039s/100 iter), loss = 0.00104835
I0801 13:34:42.343302  3162 solver.cpp:375]     Train net output #0: loss = 0.00104833 (* 1 = 0.00104833 loss)
I0801 13:34:42.343308  3162 sgd_solver.cpp:136] Iteration 19700, lr = 0.00692187, m = 0.9
I0801 13:34:43.920456  3162 solver.cpp:353] Iteration 19800 (63.4063 iter/s, 1.57713s/100 iter), loss = 0.00113683
I0801 13:34:43.920482  3162 solver.cpp:375]     Train net output #0: loss = 0.00113681 (* 1 = 0.00113681 loss)
I0801 13:34:43.920488  3162 sgd_solver.cpp:136] Iteration 19800, lr = 0.00690625, m = 0.9
I0801 13:34:45.487604  3162 solver.cpp:353] Iteration 19900 (63.8121 iter/s, 1.5671s/100 iter), loss = 0.00261222
I0801 13:34:45.487656  3162 solver.cpp:375]     Train net output #0: loss = 0.00261221 (* 1 = 0.00261221 loss)
I0801 13:34:45.487669  3162 sgd_solver.cpp:136] Iteration 19900, lr = 0.00689062, m = 0.9
I0801 13:34:47.054153  3162 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/cifar10_jacintonet11v2_iter_20000.caffemodel
I0801 13:34:47.062362  3162 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/cifar10_jacintonet11v2_iter_20000.solverstate
I0801 13:34:47.066025  3162 solver.cpp:550] Iteration 20000, Testing net (#0)
I0801 13:34:47.894582  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.916178
I0801 13:34:47.894603  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997353
I0801 13:34:47.894608  3162 solver.cpp:635]     Test net output #2: loss = 0.294117 (* 1 = 0.294117 loss)
I0801 13:34:47.894625  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.828575s
I0801 13:34:47.910117  3162 solver.cpp:353] Iteration 20000 (41.2807 iter/s, 2.42244s/100 iter), loss = 0.00179456
I0801 13:34:47.910137  3162 solver.cpp:375]     Train net output #0: loss = 0.00179455 (* 1 = 0.00179455 loss)
I0801 13:34:47.910143  3162 sgd_solver.cpp:136] Iteration 20000, lr = 0.006875, m = 0.9
I0801 13:34:49.479415  3162 solver.cpp:353] Iteration 20100 (63.7249 iter/s, 1.56925s/100 iter), loss = 0.0014933
I0801 13:34:49.479528  3162 solver.cpp:375]     Train net output #0: loss = 0.00149329 (* 1 = 0.00149329 loss)
I0801 13:34:49.479545  3162 sgd_solver.cpp:136] Iteration 20100, lr = 0.00685938, m = 0.9
I0801 13:34:51.056761  3162 solver.cpp:353] Iteration 20200 (63.3996 iter/s, 1.5773s/100 iter), loss = 0.00172633
I0801 13:34:51.056813  3162 solver.cpp:375]     Train net output #0: loss = 0.00172632 (* 1 = 0.00172632 loss)
I0801 13:34:51.056833  3162 sgd_solver.cpp:136] Iteration 20200, lr = 0.00684375, m = 0.9
I0801 13:34:52.628010  3162 solver.cpp:353] Iteration 20300 (63.6456 iter/s, 1.5712s/100 iter), loss = 0.000962344
I0801 13:34:52.628036  3162 solver.cpp:375]     Train net output #0: loss = 0.000962334 (* 1 = 0.000962334 loss)
I0801 13:34:52.628041  3162 sgd_solver.cpp:136] Iteration 20300, lr = 0.00682813, m = 0.9
I0801 13:34:54.236181  3162 solver.cpp:353] Iteration 20400 (62.1844 iter/s, 1.60812s/100 iter), loss = 0.00453277
I0801 13:34:54.236205  3162 solver.cpp:375]     Train net output #0: loss = 0.00453276 (* 1 = 0.00453276 loss)
I0801 13:34:54.236212  3162 sgd_solver.cpp:136] Iteration 20400, lr = 0.0068125, m = 0.9
I0801 13:34:55.811347  3162 solver.cpp:353] Iteration 20500 (63.4874 iter/s, 1.57512s/100 iter), loss = 0.0027373
I0801 13:34:55.811372  3162 solver.cpp:375]     Train net output #0: loss = 0.00273729 (* 1 = 0.00273729 loss)
I0801 13:34:55.811378  3162 sgd_solver.cpp:136] Iteration 20500, lr = 0.00679688, m = 0.9
I0801 13:34:57.394142  3162 solver.cpp:353] Iteration 20600 (63.1814 iter/s, 1.58275s/100 iter), loss = 0.00314268
I0801 13:34:57.394167  3162 solver.cpp:375]     Train net output #0: loss = 0.00314267 (* 1 = 0.00314267 loss)
I0801 13:34:57.394173  3162 sgd_solver.cpp:136] Iteration 20600, lr = 0.00678125, m = 0.9
I0801 13:34:58.966241  3162 solver.cpp:353] Iteration 20700 (63.6113 iter/s, 1.57205s/100 iter), loss = 0.00176934
I0801 13:34:58.966266  3162 solver.cpp:375]     Train net output #0: loss = 0.00176933 (* 1 = 0.00176933 loss)
I0801 13:34:58.966272  3162 sgd_solver.cpp:136] Iteration 20700, lr = 0.00676562, m = 0.9
I0801 13:35:00.529036  3162 solver.cpp:353] Iteration 20800 (63.9898 iter/s, 1.56275s/100 iter), loss = 0.00192885
I0801 13:35:00.529062  3162 solver.cpp:375]     Train net output #0: loss = 0.00192884 (* 1 = 0.00192884 loss)
I0801 13:35:00.529067  3162 sgd_solver.cpp:136] Iteration 20800, lr = 0.00675, m = 0.9
I0801 13:35:02.120262  3162 solver.cpp:353] Iteration 20900 (62.8467 iter/s, 1.59117s/100 iter), loss = 0.000963729
I0801 13:35:02.120291  3162 solver.cpp:375]     Train net output #0: loss = 0.000963719 (* 1 = 0.000963719 loss)
I0801 13:35:02.120298  3162 sgd_solver.cpp:136] Iteration 20900, lr = 0.00673437, m = 0.9
I0801 13:35:03.676352  3162 solver.cpp:550] Iteration 21000, Testing net (#0)
I0801 13:35:04.510782  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.913825
I0801 13:35:04.510802  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 13:35:04.510807  3162 solver.cpp:635]     Test net output #2: loss = 0.307761 (* 1 = 0.307761 loss)
I0801 13:35:04.510824  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.834448s
I0801 13:35:04.526720  3162 solver.cpp:353] Iteration 21000 (41.556 iter/s, 2.40639s/100 iter), loss = 0.00189566
I0801 13:35:04.526741  3162 solver.cpp:375]     Train net output #0: loss = 0.00189565 (* 1 = 0.00189565 loss)
I0801 13:35:04.526744  3162 sgd_solver.cpp:136] Iteration 21000, lr = 0.00671875, m = 0.9
I0801 13:35:06.108706  3162 solver.cpp:353] Iteration 21100 (63.2137 iter/s, 1.58194s/100 iter), loss = 0.000865045
I0801 13:35:06.108734  3162 solver.cpp:375]     Train net output #0: loss = 0.000865034 (* 1 = 0.000865034 loss)
I0801 13:35:06.108738  3162 sgd_solver.cpp:136] Iteration 21100, lr = 0.00670313, m = 0.9
I0801 13:35:07.691102  3162 solver.cpp:353] Iteration 21200 (63.1972 iter/s, 1.58235s/100 iter), loss = 0.00182832
I0801 13:35:07.691126  3162 solver.cpp:375]     Train net output #0: loss = 0.00182831 (* 1 = 0.00182831 loss)
I0801 13:35:07.691133  3162 sgd_solver.cpp:136] Iteration 21200, lr = 0.0066875, m = 0.9
I0801 13:35:09.258183  3162 solver.cpp:353] Iteration 21300 (63.815 iter/s, 1.56703s/100 iter), loss = 0.00110769
I0801 13:35:09.258235  3162 solver.cpp:375]     Train net output #0: loss = 0.00110768 (* 1 = 0.00110768 loss)
I0801 13:35:09.258246  3162 sgd_solver.cpp:136] Iteration 21300, lr = 0.00667187, m = 0.9
I0801 13:35:10.827497  3162 solver.cpp:353] Iteration 21400 (63.7241 iter/s, 1.56927s/100 iter), loss = 0.00063296
I0801 13:35:10.827522  3162 solver.cpp:375]     Train net output #0: loss = 0.000632949 (* 1 = 0.000632949 loss)
I0801 13:35:10.827527  3162 sgd_solver.cpp:136] Iteration 21400, lr = 0.00665625, m = 0.9
I0801 13:35:12.425302  3162 solver.cpp:353] Iteration 21500 (62.5879 iter/s, 1.59775s/100 iter), loss = 0.00137809
I0801 13:35:12.425326  3162 solver.cpp:375]     Train net output #0: loss = 0.00137808 (* 1 = 0.00137808 loss)
I0801 13:35:12.425333  3162 sgd_solver.cpp:136] Iteration 21500, lr = 0.00664062, m = 0.9
I0801 13:35:13.989990  3162 solver.cpp:353] Iteration 21600 (63.9123 iter/s, 1.56464s/100 iter), loss = 0.00249254
I0801 13:35:13.990015  3162 solver.cpp:375]     Train net output #0: loss = 0.00249252 (* 1 = 0.00249252 loss)
I0801 13:35:13.990022  3162 sgd_solver.cpp:136] Iteration 21600, lr = 0.006625, m = 0.9
I0801 13:35:15.558779  3162 solver.cpp:353] Iteration 21700 (63.7455 iter/s, 1.56874s/100 iter), loss = 0.00496568
I0801 13:35:15.558847  3162 solver.cpp:375]     Train net output #0: loss = 0.00496567 (* 1 = 0.00496567 loss)
I0801 13:35:15.558867  3162 sgd_solver.cpp:136] Iteration 21700, lr = 0.00660937, m = 0.9
I0801 13:35:17.133287  3162 solver.cpp:353] Iteration 21800 (63.5139 iter/s, 1.57446s/100 iter), loss = 0.00127497
I0801 13:35:17.133312  3162 solver.cpp:375]     Train net output #0: loss = 0.00127496 (* 1 = 0.00127496 loss)
I0801 13:35:17.133317  3162 sgd_solver.cpp:136] Iteration 21800, lr = 0.00659375, m = 0.9
I0801 13:35:18.716269  3162 solver.cpp:353] Iteration 21900 (63.1739 iter/s, 1.58293s/100 iter), loss = 0.000735967
I0801 13:35:18.716295  3162 solver.cpp:375]     Train net output #0: loss = 0.000735954 (* 1 = 0.000735954 loss)
I0801 13:35:18.716301  3162 sgd_solver.cpp:136] Iteration 21900, lr = 0.00657812, m = 0.9
I0801 13:35:20.278791  3162 solver.cpp:550] Iteration 22000, Testing net (#0)
I0801 13:35:20.693555  3160 data_reader.cpp:264] Starting prefetch of epoch 3
I0801 13:35:21.112946  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.909119
I0801 13:35:21.112967  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 13:35:21.112972  3162 solver.cpp:635]     Test net output #2: loss = 0.328989 (* 1 = 0.328989 loss)
I0801 13:35:21.112988  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.834175s
I0801 13:35:21.133836  3162 solver.cpp:353] Iteration 22000 (41.3651 iter/s, 2.4175s/100 iter), loss = 0.00116915
I0801 13:35:21.133886  3162 solver.cpp:375]     Train net output #0: loss = 0.00116914 (* 1 = 0.00116914 loss)
I0801 13:35:21.133898  3162 sgd_solver.cpp:136] Iteration 22000, lr = 0.0065625, m = 0.9
I0801 13:35:22.738128  3162 solver.cpp:353] Iteration 22100 (62.3346 iter/s, 1.60424s/100 iter), loss = 0.00215396
I0801 13:35:22.738175  3162 solver.cpp:375]     Train net output #0: loss = 0.00215395 (* 1 = 0.00215395 loss)
I0801 13:35:22.738188  3162 sgd_solver.cpp:136] Iteration 22100, lr = 0.00654687, m = 0.9
I0801 13:35:24.329382  3162 solver.cpp:353] Iteration 22200 (62.8455 iter/s, 1.5912s/100 iter), loss = 0.00181298
I0801 13:35:24.329408  3162 solver.cpp:375]     Train net output #0: loss = 0.00181296 (* 1 = 0.00181296 loss)
I0801 13:35:24.329414  3162 sgd_solver.cpp:136] Iteration 22200, lr = 0.00653125, m = 0.9
I0801 13:35:25.896111  3162 solver.cpp:353] Iteration 22300 (63.8292 iter/s, 1.56668s/100 iter), loss = 0.000230271
I0801 13:35:25.896136  3162 solver.cpp:375]     Train net output #0: loss = 0.000230256 (* 1 = 0.000230256 loss)
I0801 13:35:25.896142  3162 sgd_solver.cpp:136] Iteration 22300, lr = 0.00651562, m = 0.9
I0801 13:35:27.482998  3162 solver.cpp:353] Iteration 22400 (63.0185 iter/s, 1.58684s/100 iter), loss = 0.00165331
I0801 13:35:27.483050  3162 solver.cpp:375]     Train net output #0: loss = 0.0016533 (* 1 = 0.0016533 loss)
I0801 13:35:27.483064  3162 sgd_solver.cpp:136] Iteration 22400, lr = 0.0065, m = 0.9
I0801 13:35:29.052687  3162 solver.cpp:353] Iteration 22500 (63.7089 iter/s, 1.56964s/100 iter), loss = 0.00123862
I0801 13:35:29.052713  3162 solver.cpp:375]     Train net output #0: loss = 0.00123861 (* 1 = 0.00123861 loss)
I0801 13:35:29.052718  3162 sgd_solver.cpp:136] Iteration 22500, lr = 0.00648437, m = 0.9
I0801 13:35:30.622566  3162 solver.cpp:353] Iteration 22600 (63.7012 iter/s, 1.56983s/100 iter), loss = 0.000938061
I0801 13:35:30.622591  3162 solver.cpp:375]     Train net output #0: loss = 0.000938045 (* 1 = 0.000938045 loss)
I0801 13:35:30.622596  3162 sgd_solver.cpp:136] Iteration 22600, lr = 0.00646875, m = 0.9
I0801 13:35:32.191764  3162 solver.cpp:353] Iteration 22700 (63.7287 iter/s, 1.56915s/100 iter), loss = 0.00381101
I0801 13:35:32.191788  3162 solver.cpp:375]     Train net output #0: loss = 0.00381099 (* 1 = 0.00381099 loss)
I0801 13:35:32.191794  3162 sgd_solver.cpp:136] Iteration 22700, lr = 0.00645312, m = 0.9
I0801 13:35:33.782351  3162 solver.cpp:353] Iteration 22800 (62.8719 iter/s, 1.59054s/100 iter), loss = 0.0038699
I0801 13:35:33.782398  3162 solver.cpp:375]     Train net output #0: loss = 0.00386988 (* 1 = 0.00386988 loss)
I0801 13:35:33.782405  3162 sgd_solver.cpp:136] Iteration 22800, lr = 0.0064375, m = 0.9
I0801 13:35:35.351768  3162 solver.cpp:353] Iteration 22900 (63.72 iter/s, 1.56937s/100 iter), loss = 0.000424948
I0801 13:35:35.351794  3162 solver.cpp:375]     Train net output #0: loss = 0.00042493 (* 1 = 0.00042493 loss)
I0801 13:35:35.351799  3162 sgd_solver.cpp:136] Iteration 22900, lr = 0.00642187, m = 0.9
I0801 13:35:36.891412  3162 solver.cpp:550] Iteration 23000, Testing net (#0)
I0801 13:35:37.741916  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.910001
I0801 13:35:37.741936  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 13:35:37.741941  3162 solver.cpp:635]     Test net output #2: loss = 0.313046 (* 1 = 0.313046 loss)
I0801 13:35:37.741960  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.850526s
I0801 13:35:37.757463  3162 solver.cpp:353] Iteration 23000 (41.5692 iter/s, 2.40563s/100 iter), loss = 0.00142508
I0801 13:35:37.757509  3162 solver.cpp:375]     Train net output #0: loss = 0.00142506 (* 1 = 0.00142506 loss)
I0801 13:35:37.757526  3162 sgd_solver.cpp:136] Iteration 23000, lr = 0.00640625, m = 0.9
I0801 13:35:39.319633  3162 solver.cpp:353] Iteration 23100 (64.0156 iter/s, 1.56212s/100 iter), loss = 0.00276654
I0801 13:35:39.319658  3162 solver.cpp:375]     Train net output #0: loss = 0.00276653 (* 1 = 0.00276653 loss)
I0801 13:35:39.319664  3162 sgd_solver.cpp:136] Iteration 23100, lr = 0.00639063, m = 0.9
I0801 13:35:40.885732  3162 solver.cpp:353] Iteration 23200 (63.8549 iter/s, 1.56605s/100 iter), loss = 0.0022474
I0801 13:35:40.885758  3162 solver.cpp:375]     Train net output #0: loss = 0.00224738 (* 1 = 0.00224738 loss)
I0801 13:35:40.885762  3162 sgd_solver.cpp:136] Iteration 23200, lr = 0.006375, m = 0.9
I0801 13:35:42.463582  3162 solver.cpp:353] Iteration 23300 (63.3794 iter/s, 1.5778s/100 iter), loss = 0.00169306
I0801 13:35:42.463629  3162 solver.cpp:375]     Train net output #0: loss = 0.00169304 (* 1 = 0.00169304 loss)
I0801 13:35:42.463640  3162 sgd_solver.cpp:136] Iteration 23300, lr = 0.00635938, m = 0.9
I0801 13:35:44.041713  3162 solver.cpp:353] Iteration 23400 (63.368 iter/s, 1.57808s/100 iter), loss = 0.00179238
I0801 13:35:44.041764  3162 solver.cpp:375]     Train net output #0: loss = 0.00179236 (* 1 = 0.00179236 loss)
I0801 13:35:44.041777  3162 sgd_solver.cpp:136] Iteration 23400, lr = 0.00634375, m = 0.9
I0801 13:35:45.615854  3162 solver.cpp:353] Iteration 23500 (63.5287 iter/s, 1.57409s/100 iter), loss = 0.000936574
I0801 13:35:45.615880  3162 solver.cpp:375]     Train net output #0: loss = 0.000936556 (* 1 = 0.000936556 loss)
I0801 13:35:45.615886  3162 sgd_solver.cpp:136] Iteration 23500, lr = 0.00632813, m = 0.9
I0801 13:35:47.184586  3162 solver.cpp:353] Iteration 23600 (63.7478 iter/s, 1.56868s/100 iter), loss = 0.0013329
I0801 13:35:47.184610  3162 solver.cpp:375]     Train net output #0: loss = 0.00133288 (* 1 = 0.00133288 loss)
I0801 13:35:47.184617  3162 sgd_solver.cpp:136] Iteration 23600, lr = 0.0063125, m = 0.9
I0801 13:35:48.746670  3162 solver.cpp:353] Iteration 23700 (64.019 iter/s, 1.56204s/100 iter), loss = 0.00252372
I0801 13:35:48.746696  3162 solver.cpp:375]     Train net output #0: loss = 0.00252371 (* 1 = 0.00252371 loss)
I0801 13:35:48.746703  3162 sgd_solver.cpp:136] Iteration 23700, lr = 0.00629687, m = 0.9
I0801 13:35:50.326304  3162 solver.cpp:353] Iteration 23800 (63.3078 iter/s, 1.57958s/100 iter), loss = 0.000803195
I0801 13:35:50.326400  3162 solver.cpp:375]     Train net output #0: loss = 0.000803179 (* 1 = 0.000803179 loss)
I0801 13:35:50.326407  3162 sgd_solver.cpp:136] Iteration 23800, lr = 0.00628125, m = 0.9
I0801 13:35:51.884249  3162 solver.cpp:353] Iteration 23900 (64.1892 iter/s, 1.55789s/100 iter), loss = 0.000968432
I0801 13:35:51.884275  3162 solver.cpp:375]     Train net output #0: loss = 0.000968417 (* 1 = 0.000968417 loss)
I0801 13:35:51.884281  3162 sgd_solver.cpp:136] Iteration 23900, lr = 0.00626562, m = 0.9
I0801 13:35:53.443436  3162 solver.cpp:550] Iteration 24000, Testing net (#0)
I0801 13:35:54.276695  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.907354
I0801 13:35:54.276713  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 13:35:54.276718  3162 solver.cpp:635]     Test net output #2: loss = 0.323635 (* 1 = 0.323635 loss)
I0801 13:35:54.276736  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.833276s
I0801 13:35:54.301069  3162 solver.cpp:353] Iteration 24000 (41.3779 iter/s, 2.41675s/100 iter), loss = 0.00329714
I0801 13:35:54.301092  3162 solver.cpp:375]     Train net output #0: loss = 0.00329712 (* 1 = 0.00329712 loss)
I0801 13:35:54.301096  3162 sgd_solver.cpp:136] Iteration 24000, lr = 0.00625, m = 0.9
I0801 13:35:55.866780  3162 solver.cpp:353] Iteration 24100 (63.8706 iter/s, 1.56566s/100 iter), loss = 0.00163425
I0801 13:35:55.866833  3162 solver.cpp:375]     Train net output #0: loss = 0.00163423 (* 1 = 0.00163423 loss)
I0801 13:35:55.866848  3162 sgd_solver.cpp:136] Iteration 24100, lr = 0.00623438, m = 0.9
I0801 13:35:57.454303  3162 solver.cpp:353] Iteration 24200 (62.9932 iter/s, 1.58747s/100 iter), loss = 0.00154251
I0801 13:35:57.454327  3162 solver.cpp:375]     Train net output #0: loss = 0.0015425 (* 1 = 0.0015425 loss)
I0801 13:35:57.454334  3162 sgd_solver.cpp:136] Iteration 24200, lr = 0.00621875, m = 0.9
I0801 13:35:59.027142  3162 solver.cpp:353] Iteration 24300 (63.5812 iter/s, 1.57279s/100 iter), loss = 0.00136563
I0801 13:35:59.027166  3162 solver.cpp:375]     Train net output #0: loss = 0.00136562 (* 1 = 0.00136562 loss)
I0801 13:35:59.027170  3162 sgd_solver.cpp:136] Iteration 24300, lr = 0.00620312, m = 0.9
I0801 13:36:00.606956  3162 solver.cpp:353] Iteration 24400 (63.3007 iter/s, 1.57976s/100 iter), loss = 0.0027343
I0801 13:36:00.606982  3162 solver.cpp:375]     Train net output #0: loss = 0.00273429 (* 1 = 0.00273429 loss)
I0801 13:36:00.606989  3162 sgd_solver.cpp:136] Iteration 24400, lr = 0.0061875, m = 0.9
I0801 13:36:02.173779  3162 solver.cpp:353] Iteration 24500 (63.8253 iter/s, 1.56678s/100 iter), loss = 0.000771487
I0801 13:36:02.173807  3162 solver.cpp:375]     Train net output #0: loss = 0.000771475 (* 1 = 0.000771475 loss)
I0801 13:36:02.173811  3162 sgd_solver.cpp:136] Iteration 24500, lr = 0.00617187, m = 0.9
I0801 13:36:03.742211  3162 solver.cpp:353] Iteration 24600 (63.7599 iter/s, 1.56838s/100 iter), loss = 0.00227917
I0801 13:36:03.742238  3162 solver.cpp:375]     Train net output #0: loss = 0.00227915 (* 1 = 0.00227915 loss)
I0801 13:36:03.742244  3162 sgd_solver.cpp:136] Iteration 24600, lr = 0.00615625, m = 0.9
I0801 13:36:05.327760  3162 solver.cpp:353] Iteration 24700 (63.0715 iter/s, 1.5855s/100 iter), loss = 0.00190813
I0801 13:36:05.327786  3162 solver.cpp:375]     Train net output #0: loss = 0.00190812 (* 1 = 0.00190812 loss)
I0801 13:36:05.327791  3162 sgd_solver.cpp:136] Iteration 24700, lr = 0.00614062, m = 0.9
I0801 13:36:06.889142  3162 solver.cpp:353] Iteration 24800 (64.0478 iter/s, 1.56133s/100 iter), loss = 0.00148341
I0801 13:36:06.889169  3162 solver.cpp:375]     Train net output #0: loss = 0.0014834 (* 1 = 0.0014834 loss)
I0801 13:36:06.889176  3162 sgd_solver.cpp:136] Iteration 24800, lr = 0.006125, m = 0.9
I0801 13:36:08.465796  3162 solver.cpp:353] Iteration 24900 (63.4274 iter/s, 1.5766s/100 iter), loss = 0.00087245
I0801 13:36:08.465819  3162 solver.cpp:375]     Train net output #0: loss = 0.000872442 (* 1 = 0.000872442 loss)
I0801 13:36:08.465823  3162 sgd_solver.cpp:136] Iteration 24900, lr = 0.00610937, m = 0.9
I0801 13:36:10.016957  3162 solver.cpp:550] Iteration 25000, Testing net (#0)
I0801 13:36:10.849700  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.905001
I0801 13:36:10.849720  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995294
I0801 13:36:10.849725  3162 solver.cpp:635]     Test net output #2: loss = 0.348231 (* 1 = 0.348231 loss)
I0801 13:36:10.849740  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.83276s
I0801 13:36:10.866247  3162 solver.cpp:353] Iteration 25000 (41.6601 iter/s, 2.40038s/100 iter), loss = 0.00223768
I0801 13:36:10.866266  3162 solver.cpp:375]     Train net output #0: loss = 0.00223768 (* 1 = 0.00223768 loss)
I0801 13:36:10.866269  3162 sgd_solver.cpp:136] Iteration 25000, lr = 0.00609375, m = 0.9
I0801 13:36:12.443892  3162 solver.cpp:353] Iteration 25100 (63.3877 iter/s, 1.57759s/100 iter), loss = 0.00128196
I0801 13:36:12.443915  3162 solver.cpp:375]     Train net output #0: loss = 0.00128195 (* 1 = 0.00128195 loss)
I0801 13:36:12.443919  3162 sgd_solver.cpp:136] Iteration 25100, lr = 0.00607812, m = 0.9
I0801 13:36:14.006191  3162 solver.cpp:353] Iteration 25200 (64.0102 iter/s, 1.56225s/100 iter), loss = 0.00386459
I0801 13:36:14.006218  3162 solver.cpp:375]     Train net output #0: loss = 0.00386458 (* 1 = 0.00386458 loss)
I0801 13:36:14.006224  3162 sgd_solver.cpp:136] Iteration 25200, lr = 0.0060625, m = 0.9
I0801 13:36:15.585216  3162 solver.cpp:353] Iteration 25300 (63.3322 iter/s, 1.57898s/100 iter), loss = 0.000680805
I0801 13:36:15.585268  3162 solver.cpp:375]     Train net output #0: loss = 0.000680796 (* 1 = 0.000680796 loss)
I0801 13:36:15.585299  3162 sgd_solver.cpp:136] Iteration 25300, lr = 0.00604687, m = 0.9
I0801 13:36:17.174904  3162 solver.cpp:353] Iteration 25400 (62.9075 iter/s, 1.58964s/100 iter), loss = 0.00114861
I0801 13:36:17.174934  3162 solver.cpp:375]     Train net output #0: loss = 0.0011486 (* 1 = 0.0011486 loss)
I0801 13:36:17.174940  3162 sgd_solver.cpp:136] Iteration 25400, lr = 0.00603125, m = 0.9
I0801 13:36:18.749426  3162 solver.cpp:353] Iteration 25500 (63.5134 iter/s, 1.57447s/100 iter), loss = 0.00367265
I0801 13:36:18.749451  3162 solver.cpp:375]     Train net output #0: loss = 0.00367264 (* 1 = 0.00367264 loss)
I0801 13:36:18.749456  3162 sgd_solver.cpp:136] Iteration 25500, lr = 0.00601562, m = 0.9
I0801 13:36:20.324941  3162 solver.cpp:353] Iteration 25600 (63.4732 iter/s, 1.57547s/100 iter), loss = 0.00181789
I0801 13:36:20.324990  3162 solver.cpp:375]     Train net output #0: loss = 0.00181788 (* 1 = 0.00181788 loss)
I0801 13:36:20.325003  3162 sgd_solver.cpp:136] Iteration 25600, lr = 0.006, m = 0.9
I0801 13:36:21.877605  3162 solver.cpp:353] Iteration 25700 (64.4075 iter/s, 1.55261s/100 iter), loss = 0.000208859
I0801 13:36:21.877691  3162 solver.cpp:375]     Train net output #0: loss = 0.000208847 (* 1 = 0.000208847 loss)
I0801 13:36:21.877698  3162 sgd_solver.cpp:136] Iteration 25700, lr = 0.00598437, m = 0.9
I0801 13:36:23.441684  3162 solver.cpp:353] Iteration 25800 (63.9374 iter/s, 1.56403s/100 iter), loss = 0.00167767
I0801 13:36:23.441709  3162 solver.cpp:375]     Train net output #0: loss = 0.00167766 (* 1 = 0.00167766 loss)
I0801 13:36:23.441715  3162 sgd_solver.cpp:136] Iteration 25800, lr = 0.00596875, m = 0.9
I0801 13:36:25.009162  3162 solver.cpp:353] Iteration 25900 (63.7987 iter/s, 1.56743s/100 iter), loss = 0.000373563
I0801 13:36:25.009189  3162 solver.cpp:375]     Train net output #0: loss = 0.00037355 (* 1 = 0.00037355 loss)
I0801 13:36:25.009196  3162 sgd_solver.cpp:136] Iteration 25900, lr = 0.00595312, m = 0.9
I0801 13:36:26.560359  3162 solver.cpp:550] Iteration 26000, Testing net (#0)
I0801 13:36:27.401494  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.902648
I0801 13:36:27.401510  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 13:36:27.401515  3162 solver.cpp:635]     Test net output #2: loss = 0.344368 (* 1 = 0.344368 loss)
I0801 13:36:27.401533  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.841151s
I0801 13:36:27.417439  3162 solver.cpp:353] Iteration 26000 (41.5247 iter/s, 2.40821s/100 iter), loss = 0.00135849
I0801 13:36:27.417459  3162 solver.cpp:375]     Train net output #0: loss = 0.00135847 (* 1 = 0.00135847 loss)
I0801 13:36:27.417465  3162 sgd_solver.cpp:136] Iteration 26000, lr = 0.0059375, m = 0.9
I0801 13:36:28.979737  3162 solver.cpp:353] Iteration 26100 (64.0103 iter/s, 1.56225s/100 iter), loss = 0.00030438
I0801 13:36:28.979761  3162 solver.cpp:375]     Train net output #0: loss = 0.000304366 (* 1 = 0.000304366 loss)
I0801 13:36:28.979768  3162 sgd_solver.cpp:136] Iteration 26100, lr = 0.00592188, m = 0.9
I0801 13:36:30.552006  3162 solver.cpp:353] Iteration 26200 (63.6043 iter/s, 1.57222s/100 iter), loss = 0.00125945
I0801 13:36:30.552032  3162 solver.cpp:375]     Train net output #0: loss = 0.00125944 (* 1 = 0.00125944 loss)
I0801 13:36:30.552038  3162 sgd_solver.cpp:136] Iteration 26200, lr = 0.00590625, m = 0.9
I0801 13:36:32.138422  3162 solver.cpp:353] Iteration 26300 (63.0373 iter/s, 1.58636s/100 iter), loss = 0.000798849
I0801 13:36:32.138475  3162 solver.cpp:375]     Train net output #0: loss = 0.000798837 (* 1 = 0.000798837 loss)
I0801 13:36:32.138489  3162 sgd_solver.cpp:136] Iteration 26300, lr = 0.00589063, m = 0.9
I0801 13:36:33.731420  3162 solver.cpp:353] Iteration 26400 (62.7766 iter/s, 1.59295s/100 iter), loss = 0.00145864
I0801 13:36:33.731444  3162 solver.cpp:375]     Train net output #0: loss = 0.00145863 (* 1 = 0.00145863 loss)
I0801 13:36:33.731451  3162 sgd_solver.cpp:136] Iteration 26400, lr = 0.005875, m = 0.9
I0801 13:36:35.318101  3162 solver.cpp:353] Iteration 26500 (63.0266 iter/s, 1.58663s/100 iter), loss = 0.00122978
I0801 13:36:35.318130  3162 solver.cpp:375]     Train net output #0: loss = 0.00122977 (* 1 = 0.00122977 loss)
I0801 13:36:35.318135  3162 sgd_solver.cpp:136] Iteration 26500, lr = 0.00585938, m = 0.9
I0801 13:36:35.504281  3155 data_reader.cpp:264] Starting prefetch of epoch 4
I0801 13:36:36.901480  3162 solver.cpp:353] Iteration 26600 (63.1581 iter/s, 1.58333s/100 iter), loss = 0.0060073
I0801 13:36:36.901507  3162 solver.cpp:375]     Train net output #0: loss = 0.00600729 (* 1 = 0.00600729 loss)
I0801 13:36:36.901513  3162 sgd_solver.cpp:136] Iteration 26600, lr = 0.00584375, m = 0.9
I0801 13:36:38.482816  3162 solver.cpp:353] Iteration 26700 (63.2395 iter/s, 1.58129s/100 iter), loss = 0.000729036
I0801 13:36:38.482842  3162 solver.cpp:375]     Train net output #0: loss = 0.000729024 (* 1 = 0.000729024 loss)
I0801 13:36:38.482849  3162 sgd_solver.cpp:136] Iteration 26700, lr = 0.00582812, m = 0.9
I0801 13:36:40.071686  3162 solver.cpp:353] Iteration 26800 (62.9398 iter/s, 1.58882s/100 iter), loss = 0.00188542
I0801 13:36:40.071713  3162 solver.cpp:375]     Train net output #0: loss = 0.0018854 (* 1 = 0.0018854 loss)
I0801 13:36:40.071743  3162 sgd_solver.cpp:136] Iteration 26800, lr = 0.0058125, m = 0.9
I0801 13:36:41.649560  3162 solver.cpp:353] Iteration 26900 (63.3786 iter/s, 1.57782s/100 iter), loss = 0.000398044
I0801 13:36:41.649592  3162 solver.cpp:375]     Train net output #0: loss = 0.000398032 (* 1 = 0.000398032 loss)
I0801 13:36:41.649598  3162 sgd_solver.cpp:136] Iteration 26900, lr = 0.00579687, m = 0.9
I0801 13:36:43.198582  3162 solver.cpp:550] Iteration 27000, Testing net (#0)
I0801 13:36:44.036263  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.908236
I0801 13:36:44.036285  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 13:36:44.036290  3162 solver.cpp:635]     Test net output #2: loss = 0.349026 (* 1 = 0.349026 loss)
I0801 13:36:44.036353  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.837748s
I0801 13:36:44.052059  3162 solver.cpp:353] Iteration 27000 (41.6245 iter/s, 2.40243s/100 iter), loss = 0.000798737
I0801 13:36:44.052078  3162 solver.cpp:375]     Train net output #0: loss = 0.000798724 (* 1 = 0.000798724 loss)
I0801 13:36:44.052084  3162 sgd_solver.cpp:136] Iteration 27000, lr = 0.00578125, m = 0.9
I0801 13:36:45.639225  3162 solver.cpp:353] Iteration 27100 (63.0074 iter/s, 1.58712s/100 iter), loss = 0.00192432
I0801 13:36:45.639253  3162 solver.cpp:375]     Train net output #0: loss = 0.00192431 (* 1 = 0.00192431 loss)
I0801 13:36:45.639261  3162 sgd_solver.cpp:136] Iteration 27100, lr = 0.00576563, m = 0.9
I0801 13:36:47.220161  3162 solver.cpp:353] Iteration 27200 (63.2557 iter/s, 1.58089s/100 iter), loss = 0.00094211
I0801 13:36:47.220212  3162 solver.cpp:375]     Train net output #0: loss = 0.000942096 (* 1 = 0.000942096 loss)
I0801 13:36:47.220232  3162 sgd_solver.cpp:136] Iteration 27200, lr = 0.00575, m = 0.9
I0801 13:36:48.796099  3162 solver.cpp:353] Iteration 27300 (63.4563 iter/s, 1.57589s/100 iter), loss = 0.00098815
I0801 13:36:48.796128  3162 solver.cpp:375]     Train net output #0: loss = 0.000988137 (* 1 = 0.000988137 loss)
I0801 13:36:48.796133  3162 sgd_solver.cpp:136] Iteration 27300, lr = 0.00573438, m = 0.9
I0801 13:36:50.369688  3162 solver.cpp:353] Iteration 27400 (63.551 iter/s, 1.57354s/100 iter), loss = 0.000479659
I0801 13:36:50.369760  3162 solver.cpp:375]     Train net output #0: loss = 0.000479645 (* 1 = 0.000479645 loss)
I0801 13:36:50.369781  3162 sgd_solver.cpp:136] Iteration 27400, lr = 0.00571875, m = 0.9
I0801 13:36:51.936031  3162 solver.cpp:353] Iteration 27500 (63.8449 iter/s, 1.5663s/100 iter), loss = 0.00140472
I0801 13:36:51.936133  3162 solver.cpp:375]     Train net output #0: loss = 0.0014047 (* 1 = 0.0014047 loss)
I0801 13:36:51.936152  3162 sgd_solver.cpp:136] Iteration 27500, lr = 0.00570312, m = 0.9
I0801 13:36:53.499089  3162 solver.cpp:353] Iteration 27600 (63.9792 iter/s, 1.56301s/100 iter), loss = 0.00295227
I0801 13:36:53.499114  3162 solver.cpp:375]     Train net output #0: loss = 0.00295226 (* 1 = 0.00295226 loss)
I0801 13:36:53.499120  3162 sgd_solver.cpp:136] Iteration 27600, lr = 0.0056875, m = 0.9
I0801 13:36:55.064117  3162 solver.cpp:353] Iteration 27700 (63.8987 iter/s, 1.56498s/100 iter), loss = 0.000319496
I0801 13:36:55.064170  3162 solver.cpp:375]     Train net output #0: loss = 0.000319483 (* 1 = 0.000319483 loss)
I0801 13:36:55.064184  3162 sgd_solver.cpp:136] Iteration 27700, lr = 0.00567187, m = 0.9
I0801 13:36:56.640965  3162 solver.cpp:353] Iteration 27800 (63.4196 iter/s, 1.5768s/100 iter), loss = 0.000888276
I0801 13:36:56.640990  3162 solver.cpp:375]     Train net output #0: loss = 0.000888262 (* 1 = 0.000888262 loss)
I0801 13:36:56.640995  3162 sgd_solver.cpp:136] Iteration 27800, lr = 0.00565625, m = 0.9
I0801 13:36:58.230782  3162 solver.cpp:353] Iteration 27900 (62.9024 iter/s, 1.58977s/100 iter), loss = 0.000906093
I0801 13:36:58.230811  3162 solver.cpp:375]     Train net output #0: loss = 0.000906078 (* 1 = 0.000906078 loss)
I0801 13:36:58.230816  3162 sgd_solver.cpp:136] Iteration 27900, lr = 0.00564062, m = 0.9
I0801 13:36:59.783843  3162 solver.cpp:550] Iteration 28000, Testing net (#0)
I0801 13:37:00.617080  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.901766
I0801 13:37:00.617100  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995294
I0801 13:37:00.617105  3162 solver.cpp:635]     Test net output #2: loss = 0.344178 (* 1 = 0.344178 loss)
I0801 13:37:00.617118  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.833252s
I0801 13:37:00.632686  3162 solver.cpp:353] Iteration 28000 (41.6348 iter/s, 2.40184s/100 iter), loss = 0.00166902
I0801 13:37:00.632705  3162 solver.cpp:375]     Train net output #0: loss = 0.00166901 (* 1 = 0.00166901 loss)
I0801 13:37:00.632709  3162 sgd_solver.cpp:136] Iteration 28000, lr = 0.005625, m = 0.9
I0801 13:37:02.198653  3162 solver.cpp:353] Iteration 28100 (63.8604 iter/s, 1.56592s/100 iter), loss = 0.00327241
I0801 13:37:02.198678  3162 solver.cpp:375]     Train net output #0: loss = 0.00327239 (* 1 = 0.00327239 loss)
I0801 13:37:02.198681  3162 sgd_solver.cpp:136] Iteration 28100, lr = 0.00560937, m = 0.9
I0801 13:37:03.773459  3162 solver.cpp:353] Iteration 28200 (63.502 iter/s, 1.57475s/100 iter), loss = 0.000307535
I0801 13:37:03.773641  3162 solver.cpp:375]     Train net output #0: loss = 0.000307519 (* 1 = 0.000307519 loss)
I0801 13:37:03.773660  3162 sgd_solver.cpp:136] Iteration 28200, lr = 0.00559375, m = 0.9
I0801 13:37:05.346474  3162 solver.cpp:353] Iteration 28300 (63.5741 iter/s, 1.57297s/100 iter), loss = 0.000818517
I0801 13:37:05.346498  3162 solver.cpp:375]     Train net output #0: loss = 0.000818501 (* 1 = 0.000818501 loss)
I0801 13:37:05.346503  3162 sgd_solver.cpp:136] Iteration 28300, lr = 0.00557812, m = 0.9
I0801 13:37:06.909046  3162 solver.cpp:353] Iteration 28400 (63.999 iter/s, 1.56252s/100 iter), loss = 0.000916163
I0801 13:37:06.909108  3162 solver.cpp:375]     Train net output #0: loss = 0.000916147 (* 1 = 0.000916147 loss)
I0801 13:37:06.909127  3162 sgd_solver.cpp:136] Iteration 28400, lr = 0.0055625, m = 0.9
I0801 13:37:08.477705  3162 solver.cpp:353] Iteration 28500 (63.7507 iter/s, 1.56861s/100 iter), loss = 0.00202818
I0801 13:37:08.477731  3162 solver.cpp:375]     Train net output #0: loss = 0.00202816 (* 1 = 0.00202816 loss)
I0801 13:37:08.477736  3162 sgd_solver.cpp:136] Iteration 28500, lr = 0.00554687, m = 0.9
I0801 13:37:10.036187  3162 solver.cpp:353] Iteration 28600 (64.167 iter/s, 1.55843s/100 iter), loss = 0.0010555
I0801 13:37:10.036236  3162 solver.cpp:375]     Train net output #0: loss = 0.00105548 (* 1 = 0.00105548 loss)
I0801 13:37:10.036249  3162 sgd_solver.cpp:136] Iteration 28600, lr = 0.00553125, m = 0.9
I0801 13:37:11.610000  3162 solver.cpp:353] Iteration 28700 (63.542 iter/s, 1.57376s/100 iter), loss = 0.00153006
I0801 13:37:11.610028  3162 solver.cpp:375]     Train net output #0: loss = 0.00153005 (* 1 = 0.00153005 loss)
I0801 13:37:11.610033  3162 sgd_solver.cpp:136] Iteration 28700, lr = 0.00551562, m = 0.9
I0801 13:37:13.187621  3162 solver.cpp:353] Iteration 28800 (63.3885 iter/s, 1.57757s/100 iter), loss = 0.00183215
I0801 13:37:13.187650  3162 solver.cpp:375]     Train net output #0: loss = 0.00183214 (* 1 = 0.00183214 loss)
I0801 13:37:13.187657  3162 sgd_solver.cpp:136] Iteration 28800, lr = 0.0055, m = 0.9
I0801 13:37:14.766568  3162 solver.cpp:353] Iteration 28900 (63.3354 iter/s, 1.5789s/100 iter), loss = 0.0027007
I0801 13:37:14.766594  3162 solver.cpp:375]     Train net output #0: loss = 0.00270069 (* 1 = 0.00270069 loss)
I0801 13:37:14.766599  3162 sgd_solver.cpp:136] Iteration 28900, lr = 0.00548437, m = 0.9
I0801 13:37:16.336758  3162 solver.cpp:550] Iteration 29000, Testing net (#0)
I0801 13:37:17.168149  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.914119
I0801 13:37:17.168169  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 13:37:17.168176  3162 solver.cpp:635]     Test net output #2: loss = 0.297321 (* 1 = 0.297321 loss)
I0801 13:37:17.168196  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.831414s
I0801 13:37:17.184496  3162 solver.cpp:353] Iteration 29000 (41.3589 iter/s, 2.41786s/100 iter), loss = 0.00456476
I0801 13:37:17.184518  3162 solver.cpp:375]     Train net output #0: loss = 0.00456475 (* 1 = 0.00456475 loss)
I0801 13:37:17.184525  3162 sgd_solver.cpp:136] Iteration 29000, lr = 0.00546875, m = 0.9
I0801 13:37:18.765697  3162 solver.cpp:353] Iteration 29100 (63.2451 iter/s, 1.58115s/100 iter), loss = 0.0168719
I0801 13:37:18.765722  3162 solver.cpp:375]     Train net output #0: loss = 0.0168719 (* 1 = 0.0168719 loss)
I0801 13:37:18.765727  3162 sgd_solver.cpp:136] Iteration 29100, lr = 0.00545313, m = 0.9
I0801 13:37:20.360671  3162 solver.cpp:353] Iteration 29200 (62.699 iter/s, 1.59492s/100 iter), loss = 0.00132883
I0801 13:37:20.360697  3162 solver.cpp:375]     Train net output #0: loss = 0.00132882 (* 1 = 0.00132882 loss)
I0801 13:37:20.360704  3162 sgd_solver.cpp:136] Iteration 29200, lr = 0.0054375, m = 0.9
I0801 13:37:21.937532  3162 solver.cpp:353] Iteration 29300 (63.419 iter/s, 1.57681s/100 iter), loss = 0.00261831
I0801 13:37:21.937608  3162 solver.cpp:375]     Train net output #0: loss = 0.0026183 (* 1 = 0.0026183 loss)
I0801 13:37:21.937613  3162 sgd_solver.cpp:136] Iteration 29300, lr = 0.00542188, m = 0.9
I0801 13:37:23.512759  3162 solver.cpp:353] Iteration 29400 (63.4849 iter/s, 1.57518s/100 iter), loss = 0.00407552
I0801 13:37:23.512784  3162 solver.cpp:375]     Train net output #0: loss = 0.00407551 (* 1 = 0.00407551 loss)
I0801 13:37:23.512790  3162 sgd_solver.cpp:136] Iteration 29400, lr = 0.00540625, m = 0.9
I0801 13:37:25.080785  3162 solver.cpp:353] Iteration 29500 (63.7764 iter/s, 1.56798s/100 iter), loss = 0.00230512
I0801 13:37:25.080852  3162 solver.cpp:375]     Train net output #0: loss = 0.00230511 (* 1 = 0.00230511 loss)
I0801 13:37:25.080869  3162 sgd_solver.cpp:136] Iteration 29500, lr = 0.00539062, m = 0.9
I0801 13:37:26.645603  3162 solver.cpp:353] Iteration 29600 (63.9072 iter/s, 1.56477s/100 iter), loss = 0.000542094
I0801 13:37:26.645628  3162 solver.cpp:375]     Train net output #0: loss = 0.000542087 (* 1 = 0.000542087 loss)
I0801 13:37:26.645634  3162 sgd_solver.cpp:136] Iteration 29600, lr = 0.005375, m = 0.9
I0801 13:37:28.220088  3162 solver.cpp:353] Iteration 29700 (63.5148 iter/s, 1.57444s/100 iter), loss = 0.00657517
I0801 13:37:28.220113  3162 solver.cpp:375]     Train net output #0: loss = 0.00657516 (* 1 = 0.00657516 loss)
I0801 13:37:28.220119  3162 sgd_solver.cpp:136] Iteration 29700, lr = 0.00535937, m = 0.9
I0801 13:37:29.798686  3162 solver.cpp:353] Iteration 29800 (63.3495 iter/s, 1.57855s/100 iter), loss = 0.00243559
I0801 13:37:29.798712  3162 solver.cpp:375]     Train net output #0: loss = 0.00243559 (* 1 = 0.00243559 loss)
I0801 13:37:29.798717  3162 sgd_solver.cpp:136] Iteration 29800, lr = 0.00534375, m = 0.9
I0801 13:37:31.377461  3162 solver.cpp:353] Iteration 29900 (63.3422 iter/s, 1.57873s/100 iter), loss = 0.00124994
I0801 13:37:31.377528  3162 solver.cpp:375]     Train net output #0: loss = 0.00124993 (* 1 = 0.00124993 loss)
I0801 13:37:31.377562  3162 sgd_solver.cpp:136] Iteration 29900, lr = 0.00532812, m = 0.9
I0801 13:37:32.924229  3162 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/cifar10_jacintonet11v2_iter_30000.caffemodel
I0801 13:37:32.932258  3162 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/cifar10_jacintonet11v2_iter_30000.solverstate
I0801 13:37:32.935839  3162 solver.cpp:550] Iteration 30000, Testing net (#0)
I0801 13:37:33.757733  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.913825
I0801 13:37:33.757753  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 13:37:33.757761  3162 solver.cpp:635]     Test net output #2: loss = 0.29831 (* 1 = 0.29831 loss)
I0801 13:37:33.757776  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.821914s
I0801 13:37:33.773289  3162 solver.cpp:353] Iteration 30000 (41.7404 iter/s, 2.39576s/100 iter), loss = 0.00158852
I0801 13:37:33.773308  3162 solver.cpp:375]     Train net output #0: loss = 0.00158851 (* 1 = 0.00158851 loss)
I0801 13:37:33.773313  3162 sgd_solver.cpp:136] Iteration 30000, lr = 0.0053125, m = 0.9
I0801 13:37:35.341737  3162 solver.cpp:353] Iteration 30100 (63.7594 iter/s, 1.5684s/100 iter), loss = 0.00317809
I0801 13:37:35.341784  3162 solver.cpp:375]     Train net output #0: loss = 0.00317809 (* 1 = 0.00317809 loss)
I0801 13:37:35.341796  3162 sgd_solver.cpp:136] Iteration 30100, lr = 0.00529688, m = 0.9
I0801 13:37:36.889783  3162 solver.cpp:353] Iteration 30200 (64.5996 iter/s, 1.548s/100 iter), loss = 0.00267509
I0801 13:37:36.889808  3162 solver.cpp:375]     Train net output #0: loss = 0.00267509 (* 1 = 0.00267509 loss)
I0801 13:37:36.889812  3162 sgd_solver.cpp:136] Iteration 30200, lr = 0.00528125, m = 0.9
I0801 13:37:38.450132  3162 solver.cpp:353] Iteration 30300 (64.0902 iter/s, 1.5603s/100 iter), loss = 0.0025633
I0801 13:37:38.450158  3162 solver.cpp:375]     Train net output #0: loss = 0.0025633 (* 1 = 0.0025633 loss)
I0801 13:37:38.450163  3162 sgd_solver.cpp:136] Iteration 30300, lr = 0.00526563, m = 0.9
I0801 13:37:40.015164  3162 solver.cpp:353] Iteration 30400 (63.8984 iter/s, 1.56498s/100 iter), loss = 0.00371974
I0801 13:37:40.015187  3162 solver.cpp:375]     Train net output #0: loss = 0.00371974 (* 1 = 0.00371974 loss)
I0801 13:37:40.015192  3162 sgd_solver.cpp:136] Iteration 30400, lr = 0.00525, m = 0.9
I0801 13:37:41.592931  3162 solver.cpp:353] Iteration 30500 (63.3827 iter/s, 1.57772s/100 iter), loss = 0.00086553
I0801 13:37:41.593004  3162 solver.cpp:375]     Train net output #0: loss = 0.000865525 (* 1 = 0.000865525 loss)
I0801 13:37:41.593026  3162 sgd_solver.cpp:136] Iteration 30500, lr = 0.00523437, m = 0.9
I0801 13:37:43.163537  3162 solver.cpp:353] Iteration 30600 (63.6717 iter/s, 1.57056s/100 iter), loss = 0.000598142
I0801 13:37:43.163563  3162 solver.cpp:375]     Train net output #0: loss = 0.000598139 (* 1 = 0.000598139 loss)
I0801 13:37:43.163568  3162 sgd_solver.cpp:136] Iteration 30600, lr = 0.00521875, m = 0.9
I0801 13:37:44.725179  3162 solver.cpp:353] Iteration 30700 (64.0371 iter/s, 1.56159s/100 iter), loss = 0.000652803
I0801 13:37:44.725203  3162 solver.cpp:375]     Train net output #0: loss = 0.000652801 (* 1 = 0.000652801 loss)
I0801 13:37:44.725208  3162 sgd_solver.cpp:136] Iteration 30700, lr = 0.00520312, m = 0.9
I0801 13:37:46.301169  3162 solver.cpp:353] Iteration 30800 (63.4541 iter/s, 1.57594s/100 iter), loss = 0.00104314
I0801 13:37:46.301198  3162 solver.cpp:375]     Train net output #0: loss = 0.00104313 (* 1 = 0.00104313 loss)
I0801 13:37:46.301204  3162 sgd_solver.cpp:136] Iteration 30800, lr = 0.0051875, m = 0.9
I0801 13:37:47.873076  3162 solver.cpp:353] Iteration 30900 (63.619 iter/s, 1.57186s/100 iter), loss = 0.00514514
I0801 13:37:47.873102  3162 solver.cpp:375]     Train net output #0: loss = 0.00514514 (* 1 = 0.00514514 loss)
I0801 13:37:47.873107  3162 sgd_solver.cpp:136] Iteration 30900, lr = 0.00517187, m = 0.9
I0801 13:37:49.441807  3162 solver.cpp:550] Iteration 31000, Testing net (#0)
I0801 13:37:49.725514  3160 data_reader.cpp:264] Starting prefetch of epoch 4
I0801 13:37:50.274772  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.919707
I0801 13:37:50.274801  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997353
I0801 13:37:50.274811  3162 solver.cpp:635]     Test net output #2: loss = 0.281995 (* 1 = 0.281995 loss)
I0801 13:37:50.274837  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.833005s
I0801 13:37:50.293512  3162 solver.cpp:353] Iteration 31000 (41.3161 iter/s, 2.42036s/100 iter), loss = 0.000757176
I0801 13:37:50.293540  3162 solver.cpp:375]     Train net output #0: loss = 0.000757171 (* 1 = 0.000757171 loss)
I0801 13:37:50.293545  3162 sgd_solver.cpp:136] Iteration 31000, lr = 0.00515625, m = 0.9
I0801 13:37:51.857398  3162 solver.cpp:353] Iteration 31100 (63.9453 iter/s, 1.56384s/100 iter), loss = 0.00374733
I0801 13:37:51.857425  3162 solver.cpp:375]     Train net output #0: loss = 0.00374732 (* 1 = 0.00374732 loss)
I0801 13:37:51.857431  3162 sgd_solver.cpp:136] Iteration 31100, lr = 0.00514062, m = 0.9
I0801 13:37:53.437690  3162 solver.cpp:353] Iteration 31200 (63.2813 iter/s, 1.58025s/100 iter), loss = 0.00181581
I0801 13:37:53.439328  3162 solver.cpp:375]     Train net output #0: loss = 0.00181581 (* 1 = 0.00181581 loss)
I0801 13:37:53.439335  3162 sgd_solver.cpp:136] Iteration 31200, lr = 0.005125, m = 0.9
I0801 13:37:54.997088  3162 solver.cpp:353] Iteration 31300 (64.1294 iter/s, 1.55935s/100 iter), loss = 0.000940504
I0801 13:37:54.997113  3162 solver.cpp:375]     Train net output #0: loss = 0.000940501 (* 1 = 0.000940501 loss)
I0801 13:37:54.997118  3162 sgd_solver.cpp:136] Iteration 31300, lr = 0.00510937, m = 0.9
I0801 13:37:56.565480  3162 solver.cpp:353] Iteration 31400 (63.7615 iter/s, 1.56834s/100 iter), loss = 0.00162418
I0801 13:37:56.565506  3162 solver.cpp:375]     Train net output #0: loss = 0.00162418 (* 1 = 0.00162418 loss)
I0801 13:37:56.565512  3162 sgd_solver.cpp:136] Iteration 31400, lr = 0.00509375, m = 0.9
I0801 13:37:58.132678  3162 solver.cpp:353] Iteration 31500 (63.8101 iter/s, 1.56715s/100 iter), loss = 0.00418708
I0801 13:37:58.132702  3162 solver.cpp:375]     Train net output #0: loss = 0.00418708 (* 1 = 0.00418708 loss)
I0801 13:37:58.132706  3162 sgd_solver.cpp:136] Iteration 31500, lr = 0.00507812, m = 0.9
I0801 13:37:59.818749  3162 solver.cpp:353] Iteration 31600 (59.3115 iter/s, 1.68602s/100 iter), loss = 0.0021307
I0801 13:37:59.818842  3162 solver.cpp:375]     Train net output #0: loss = 0.0021307 (* 1 = 0.0021307 loss)
I0801 13:37:59.818873  3162 sgd_solver.cpp:136] Iteration 31600, lr = 0.0050625, m = 0.9
I0801 13:38:01.444239  3162 solver.cpp:353] Iteration 31700 (61.5218 iter/s, 1.62544s/100 iter), loss = 0.000879557
I0801 13:38:01.444264  3162 solver.cpp:375]     Train net output #0: loss = 0.000879556 (* 1 = 0.000879556 loss)
I0801 13:38:01.444268  3162 sgd_solver.cpp:136] Iteration 31700, lr = 0.00504687, m = 0.9
I0801 13:38:03.141391  3162 solver.cpp:353] Iteration 31800 (58.9242 iter/s, 1.69709s/100 iter), loss = 0.000958173
I0801 13:38:03.141430  3162 solver.cpp:375]     Train net output #0: loss = 0.000958172 (* 1 = 0.000958172 loss)
I0801 13:38:03.141439  3162 sgd_solver.cpp:136] Iteration 31800, lr = 0.00503125, m = 0.9
I0801 13:38:04.903939  3162 solver.cpp:353] Iteration 31900 (56.7376 iter/s, 1.7625s/100 iter), loss = 0.00144018
I0801 13:38:04.903987  3162 solver.cpp:375]     Train net output #0: loss = 0.00144018 (* 1 = 0.00144018 loss)
I0801 13:38:04.904001  3162 sgd_solver.cpp:136] Iteration 31900, lr = 0.00501562, m = 0.9
I0801 13:38:06.519161  3162 solver.cpp:550] Iteration 32000, Testing net (#0)
I0801 13:38:07.364393  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.911177
I0801 13:38:07.364420  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 13:38:07.364434  3162 solver.cpp:635]     Test net output #2: loss = 0.33074 (* 1 = 0.33074 loss)
I0801 13:38:07.364477  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.845284s
I0801 13:38:07.385143  3162 solver.cpp:353] Iteration 32000 (40.3045 iter/s, 2.48111s/100 iter), loss = 0.00119466
I0801 13:38:07.385205  3162 solver.cpp:375]     Train net output #0: loss = 0.00119466 (* 1 = 0.00119466 loss)
I0801 13:38:07.385221  3162 sgd_solver.cpp:136] Iteration 32000, lr = 0.005, m = 0.9
I0801 13:38:09.033972  3162 solver.cpp:353] Iteration 32100 (60.6506 iter/s, 1.64879s/100 iter), loss = 0.00118984
I0801 13:38:09.033999  3162 solver.cpp:375]     Train net output #0: loss = 0.00118984 (* 1 = 0.00118984 loss)
I0801 13:38:09.034006  3162 sgd_solver.cpp:136] Iteration 32100, lr = 0.00498438, m = 0.9
I0801 13:38:10.734344  3162 solver.cpp:353] Iteration 32200 (58.8125 iter/s, 1.70032s/100 iter), loss = 0.00241458
I0801 13:38:10.734371  3162 solver.cpp:375]     Train net output #0: loss = 0.00241458 (* 1 = 0.00241458 loss)
I0801 13:38:10.734378  3162 sgd_solver.cpp:136] Iteration 32200, lr = 0.00496875, m = 0.9
I0801 13:38:12.400414  3162 solver.cpp:353] Iteration 32300 (60.0235 iter/s, 1.66601s/100 iter), loss = 0.00212096
I0801 13:38:12.400449  3162 solver.cpp:375]     Train net output #0: loss = 0.00212096 (* 1 = 0.00212096 loss)
I0801 13:38:12.400454  3162 sgd_solver.cpp:136] Iteration 32300, lr = 0.00495313, m = 0.9
I0801 13:38:14.191614  3162 solver.cpp:353] Iteration 32400 (55.8301 iter/s, 1.79115s/100 iter), loss = 0.00080271
I0801 13:38:14.191638  3162 solver.cpp:375]     Train net output #0: loss = 0.000802709 (* 1 = 0.000802709 loss)
I0801 13:38:14.191643  3162 sgd_solver.cpp:136] Iteration 32400, lr = 0.0049375, m = 0.9
I0801 13:38:15.857960  3162 solver.cpp:353] Iteration 32500 (60.0135 iter/s, 1.66629s/100 iter), loss = 0.0012079
I0801 13:38:15.857986  3162 solver.cpp:375]     Train net output #0: loss = 0.0012079 (* 1 = 0.0012079 loss)
I0801 13:38:15.857991  3162 sgd_solver.cpp:136] Iteration 32500, lr = 0.00492187, m = 0.9
I0801 13:38:17.566371  3162 solver.cpp:353] Iteration 32600 (58.5359 iter/s, 1.70835s/100 iter), loss = 0.00196988
I0801 13:38:17.566493  3162 solver.cpp:375]     Train net output #0: loss = 0.00196988 (* 1 = 0.00196988 loss)
I0801 13:38:17.566511  3162 sgd_solver.cpp:136] Iteration 32600, lr = 0.00490625, m = 0.9
I0801 13:38:19.345716  3162 solver.cpp:353] Iteration 32700 (56.2024 iter/s, 1.77928s/100 iter), loss = 0.000622774
I0801 13:38:19.346271  3162 solver.cpp:375]     Train net output #0: loss = 0.000622775 (* 1 = 0.000622775 loss)
I0801 13:38:19.346303  3162 sgd_solver.cpp:136] Iteration 32700, lr = 0.00489062, m = 0.9
I0801 13:38:21.141953  3162 solver.cpp:353] Iteration 32800 (55.6734 iter/s, 1.79619s/100 iter), loss = 0.00353842
I0801 13:38:21.141976  3162 solver.cpp:375]     Train net output #0: loss = 0.00353842 (* 1 = 0.00353842 loss)
I0801 13:38:21.141983  3162 sgd_solver.cpp:136] Iteration 32800, lr = 0.004875, m = 0.9
I0801 13:38:22.745998  3162 solver.cpp:353] Iteration 32900 (62.3444 iter/s, 1.60399s/100 iter), loss = 0.00171669
I0801 13:38:22.746023  3162 solver.cpp:375]     Train net output #0: loss = 0.0017167 (* 1 = 0.0017167 loss)
I0801 13:38:22.746029  3162 sgd_solver.cpp:136] Iteration 32900, lr = 0.00485937, m = 0.9
I0801 13:38:24.321533  3162 solver.cpp:550] Iteration 33000, Testing net (#0)
I0801 13:38:25.239856  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.909707
I0801 13:38:25.239876  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 13:38:25.239881  3162 solver.cpp:635]     Test net output #2: loss = 0.335162 (* 1 = 0.335162 loss)
I0801 13:38:25.239898  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.918342s
I0801 13:38:25.255494  3162 solver.cpp:353] Iteration 33000 (39.8498 iter/s, 2.50942s/100 iter), loss = 0.00329251
I0801 13:38:25.255511  3162 solver.cpp:375]     Train net output #0: loss = 0.00329251 (* 1 = 0.00329251 loss)
I0801 13:38:25.255515  3162 sgd_solver.cpp:136] Iteration 33000, lr = 0.00484375, m = 0.9
I0801 13:38:27.018158  3162 solver.cpp:353] Iteration 33100 (56.7344 iter/s, 1.7626s/100 iter), loss = 0.00697683
I0801 13:38:27.018203  3162 solver.cpp:375]     Train net output #0: loss = 0.00697683 (* 1 = 0.00697683 loss)
I0801 13:38:27.018208  3162 sgd_solver.cpp:136] Iteration 33100, lr = 0.00482813, m = 0.9
I0801 13:38:28.784464  3162 solver.cpp:353] Iteration 33200 (56.6168 iter/s, 1.76626s/100 iter), loss = 0.00028278
I0801 13:38:28.784489  3162 solver.cpp:375]     Train net output #0: loss = 0.000282784 (* 1 = 0.000282784 loss)
I0801 13:38:28.784495  3162 sgd_solver.cpp:136] Iteration 33200, lr = 0.0048125, m = 0.9
I0801 13:38:30.504385  3162 solver.cpp:353] Iteration 33300 (58.144 iter/s, 1.71987s/100 iter), loss = 0.00162913
I0801 13:38:30.504412  3162 solver.cpp:375]     Train net output #0: loss = 0.00162914 (* 1 = 0.00162914 loss)
I0801 13:38:30.504420  3162 sgd_solver.cpp:136] Iteration 33300, lr = 0.00479688, m = 0.9
I0801 13:38:32.194854  3162 solver.cpp:353] Iteration 33400 (59.1576 iter/s, 1.6904s/100 iter), loss = 0.00335986
I0801 13:38:32.194934  3162 solver.cpp:375]     Train net output #0: loss = 0.00335987 (* 1 = 0.00335987 loss)
I0801 13:38:32.194959  3162 sgd_solver.cpp:136] Iteration 33400, lr = 0.00478125, m = 0.9
I0801 13:38:33.853703  3162 solver.cpp:353] Iteration 33500 (60.2846 iter/s, 1.6588s/100 iter), loss = 0.00169337
I0801 13:38:33.853746  3162 solver.cpp:375]     Train net output #0: loss = 0.00169338 (* 1 = 0.00169338 loss)
I0801 13:38:33.853757  3162 sgd_solver.cpp:136] Iteration 33500, lr = 0.00476563, m = 0.9
I0801 13:38:35.552662  3162 solver.cpp:353] Iteration 33600 (58.8612 iter/s, 1.69891s/100 iter), loss = 0.00060496
I0801 13:38:35.552692  3162 solver.cpp:375]     Train net output #0: loss = 0.000604963 (* 1 = 0.000604963 loss)
I0801 13:38:35.552700  3162 sgd_solver.cpp:136] Iteration 33600, lr = 0.00475, m = 0.9
I0801 13:38:37.186064  3162 solver.cpp:353] Iteration 33700 (61.2239 iter/s, 1.63335s/100 iter), loss = 0.00171959
I0801 13:38:37.186089  3162 solver.cpp:375]     Train net output #0: loss = 0.00171959 (* 1 = 0.00171959 loss)
I0801 13:38:37.186095  3162 sgd_solver.cpp:136] Iteration 33700, lr = 0.00473437, m = 0.9
I0801 13:38:38.921067  3162 solver.cpp:353] Iteration 33800 (57.6384 iter/s, 1.73495s/100 iter), loss = 0.00118606
I0801 13:38:38.921092  3162 solver.cpp:375]     Train net output #0: loss = 0.00118607 (* 1 = 0.00118607 loss)
I0801 13:38:38.921097  3162 sgd_solver.cpp:136] Iteration 33800, lr = 0.00471875, m = 0.9
I0801 13:38:40.677613  3162 solver.cpp:353] Iteration 33900 (56.9321 iter/s, 1.75648s/100 iter), loss = 0.00296368
I0801 13:38:40.677655  3162 solver.cpp:375]     Train net output #0: loss = 0.00296368 (* 1 = 0.00296368 loss)
I0801 13:38:40.677664  3162 sgd_solver.cpp:136] Iteration 33900, lr = 0.00470312, m = 0.9
I0801 13:38:42.374754  3162 solver.cpp:550] Iteration 34000, Testing net (#0)
I0801 13:38:43.356022  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.90853
I0801 13:38:43.356041  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 13:38:43.356048  3162 solver.cpp:635]     Test net output #2: loss = 0.352253 (* 1 = 0.352253 loss)
I0801 13:38:43.356065  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.981288s
I0801 13:38:43.371742  3162 solver.cpp:353] Iteration 34000 (37.1187 iter/s, 2.69406s/100 iter), loss = 0.00195717
I0801 13:38:43.371775  3162 solver.cpp:375]     Train net output #0: loss = 0.00195717 (* 1 = 0.00195717 loss)
I0801 13:38:43.371781  3162 sgd_solver.cpp:136] Iteration 34000, lr = 0.0046875, m = 0.9
I0801 13:38:44.988729  3162 solver.cpp:353] Iteration 34100 (61.8456 iter/s, 1.61693s/100 iter), loss = 0.0014722
I0801 13:38:44.988788  3162 solver.cpp:375]     Train net output #0: loss = 0.0014722 (* 1 = 0.0014722 loss)
I0801 13:38:44.988803  3162 sgd_solver.cpp:136] Iteration 34100, lr = 0.00467187, m = 0.9
I0801 13:38:46.630722  3162 solver.cpp:353] Iteration 34200 (60.9034 iter/s, 1.64195s/100 iter), loss = 0.0028856
I0801 13:38:46.630779  3162 solver.cpp:375]     Train net output #0: loss = 0.0028856 (* 1 = 0.0028856 loss)
I0801 13:38:46.630791  3162 sgd_solver.cpp:136] Iteration 34200, lr = 0.00465625, m = 0.9
I0801 13:38:48.239861  3162 solver.cpp:353] Iteration 34300 (62.1469 iter/s, 1.60909s/100 iter), loss = 0.000534971
I0801 13:38:48.239887  3162 solver.cpp:375]     Train net output #0: loss = 0.000534972 (* 1 = 0.000534972 loss)
I0801 13:38:48.239893  3162 sgd_solver.cpp:136] Iteration 34300, lr = 0.00464062, m = 0.9
I0801 13:38:49.983104  3162 solver.cpp:353] Iteration 34400 (57.3661 iter/s, 1.74319s/100 iter), loss = 0.00246556
I0801 13:38:49.983129  3162 solver.cpp:375]     Train net output #0: loss = 0.00246556 (* 1 = 0.00246556 loss)
I0801 13:38:49.983134  3162 sgd_solver.cpp:136] Iteration 34400, lr = 0.004625, m = 0.9
I0801 13:38:51.673835  3162 solver.cpp:353] Iteration 34500 (59.1484 iter/s, 1.69066s/100 iter), loss = 0.0005024
I0801 13:38:51.673933  3162 solver.cpp:375]     Train net output #0: loss = 0.000502401 (* 1 = 0.000502401 loss)
I0801 13:38:51.673964  3162 sgd_solver.cpp:136] Iteration 34500, lr = 0.00460937, m = 0.9
I0801 13:38:53.336881  3162 solver.cpp:353] Iteration 34600 (60.1322 iter/s, 1.663s/100 iter), loss = 0.00292516
I0801 13:38:53.336910  3162 solver.cpp:375]     Train net output #0: loss = 0.00292516 (* 1 = 0.00292516 loss)
I0801 13:38:53.336916  3162 sgd_solver.cpp:136] Iteration 34600, lr = 0.00459375, m = 0.9
I0801 13:38:54.968257  3162 solver.cpp:353] Iteration 34700 (61.2998 iter/s, 1.63133s/100 iter), loss = 0.00204724
I0801 13:38:54.968333  3162 solver.cpp:375]     Train net output #0: loss = 0.00204725 (* 1 = 0.00204725 loss)
I0801 13:38:54.968340  3162 sgd_solver.cpp:136] Iteration 34700, lr = 0.00457812, m = 0.9
I0801 13:38:56.549706  3162 solver.cpp:353] Iteration 34800 (63.2352 iter/s, 1.5814s/100 iter), loss = 0.00777014
I0801 13:38:56.549731  3162 solver.cpp:375]     Train net output #0: loss = 0.00777014 (* 1 = 0.00777014 loss)
I0801 13:38:56.549738  3162 sgd_solver.cpp:136] Iteration 34800, lr = 0.0045625, m = 0.9
I0801 13:38:58.116303  3162 solver.cpp:353] Iteration 34900 (63.8346 iter/s, 1.56655s/100 iter), loss = 0.00119366
I0801 13:38:58.116331  3162 solver.cpp:375]     Train net output #0: loss = 0.00119366 (* 1 = 0.00119366 loss)
I0801 13:38:58.116338  3162 sgd_solver.cpp:136] Iteration 34900, lr = 0.00454687, m = 0.9
I0801 13:38:59.665186  3162 solver.cpp:550] Iteration 35000, Testing net (#0)
I0801 13:38:59.887994  3160 data_reader.cpp:264] Starting prefetch of epoch 5
I0801 13:39:00.517067  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.891766
I0801 13:39:00.517103  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 13:39:00.517115  3162 solver.cpp:635]     Test net output #2: loss = 0.42495 (* 1 = 0.42495 loss)
I0801 13:39:00.517148  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.851933s
I0801 13:39:00.534488  3162 solver.cpp:353] Iteration 35000 (41.3546 iter/s, 2.41811s/100 iter), loss = 0.00495565
I0801 13:39:00.534519  3162 solver.cpp:375]     Train net output #0: loss = 0.00495565 (* 1 = 0.00495565 loss)
I0801 13:39:00.534526  3162 sgd_solver.cpp:136] Iteration 35000, lr = 0.00453125, m = 0.9
I0801 13:39:02.169075  3162 solver.cpp:353] Iteration 35100 (61.1794 iter/s, 1.63454s/100 iter), loss = 0.00404985
I0801 13:39:02.169122  3162 solver.cpp:375]     Train net output #0: loss = 0.00404985 (* 1 = 0.00404985 loss)
I0801 13:39:02.169134  3162 sgd_solver.cpp:136] Iteration 35100, lr = 0.00451563, m = 0.9
I0801 13:39:03.921278  3162 solver.cpp:353] Iteration 35200 (57.0728 iter/s, 1.75215s/100 iter), loss = 0.00162285
I0801 13:39:03.921304  3162 solver.cpp:375]     Train net output #0: loss = 0.00162284 (* 1 = 0.00162284 loss)
I0801 13:39:03.921308  3162 sgd_solver.cpp:136] Iteration 35200, lr = 0.0045, m = 0.9
I0801 13:39:05.500864  3162 solver.cpp:353] Iteration 35300 (63.3098 iter/s, 1.57954s/100 iter), loss = 0.0006806
I0801 13:39:05.500916  3162 solver.cpp:375]     Train net output #0: loss = 0.000680599 (* 1 = 0.000680599 loss)
I0801 13:39:05.500933  3162 sgd_solver.cpp:136] Iteration 35300, lr = 0.00448438, m = 0.9
I0801 13:39:07.074770  3162 solver.cpp:353] Iteration 35400 (63.5382 iter/s, 1.57386s/100 iter), loss = 0.000681427
I0801 13:39:07.074825  3162 solver.cpp:375]     Train net output #0: loss = 0.000681427 (* 1 = 0.000681427 loss)
I0801 13:39:07.074839  3162 sgd_solver.cpp:136] Iteration 35400, lr = 0.00446875, m = 0.9
I0801 13:39:08.652256  3162 solver.cpp:353] Iteration 35500 (63.3941 iter/s, 1.57743s/100 iter), loss = 0.00218876
I0801 13:39:08.652307  3162 solver.cpp:375]     Train net output #0: loss = 0.00218876 (* 1 = 0.00218876 loss)
I0801 13:39:08.652323  3162 sgd_solver.cpp:136] Iteration 35500, lr = 0.00445312, m = 0.9
I0801 13:39:10.255499  3162 solver.cpp:353] Iteration 35600 (62.3754 iter/s, 1.6032s/100 iter), loss = 0.00210413
I0801 13:39:10.255524  3162 solver.cpp:375]     Train net output #0: loss = 0.00210414 (* 1 = 0.00210414 loss)
I0801 13:39:10.255529  3162 sgd_solver.cpp:136] Iteration 35600, lr = 0.0044375, m = 0.9
I0801 13:39:12.117563  3162 solver.cpp:353] Iteration 35700 (53.7056 iter/s, 1.862s/100 iter), loss = 0.155426
I0801 13:39:12.117585  3162 solver.cpp:375]     Train net output #0: loss = 0.155426 (* 1 = 0.155426 loss)
I0801 13:39:12.117589  3162 sgd_solver.cpp:136] Iteration 35700, lr = 0.00442187, m = 0.9
I0801 13:39:13.741785  3162 solver.cpp:353] Iteration 35800 (61.5699 iter/s, 1.62417s/100 iter), loss = 0.0520642
I0801 13:39:13.741811  3162 solver.cpp:375]     Train net output #0: loss = 0.0520643 (* 1 = 0.0520643 loss)
I0801 13:39:13.741840  3162 sgd_solver.cpp:136] Iteration 35800, lr = 0.00440625, m = 0.9
I0801 13:39:15.428097  3162 solver.cpp:353] Iteration 35900 (59.3028 iter/s, 1.68626s/100 iter), loss = 0.0238419
I0801 13:39:15.428153  3162 solver.cpp:375]     Train net output #0: loss = 0.0238421 (* 1 = 0.0238421 loss)
I0801 13:39:15.428167  3162 sgd_solver.cpp:136] Iteration 35900, lr = 0.00439062, m = 0.9
I0801 13:39:17.137547  3162 solver.cpp:550] Iteration 36000, Testing net (#0)
I0801 13:39:17.970355  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.805001
I0801 13:39:17.970373  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.99
I0801 13:39:17.970381  3162 solver.cpp:635]     Test net output #2: loss = 0.750817 (* 1 = 0.750817 loss)
I0801 13:39:17.970398  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.832828s
I0801 13:39:17.986057  3162 solver.cpp:353] Iteration 36000 (39.0948 iter/s, 2.55789s/100 iter), loss = 0.0666959
I0801 13:39:17.986075  3162 solver.cpp:375]     Train net output #0: loss = 0.0666962 (* 1 = 0.0666962 loss)
I0801 13:39:17.986081  3162 sgd_solver.cpp:136] Iteration 36000, lr = 0.004375, m = 0.9
I0801 13:39:19.671316  3162 solver.cpp:353] Iteration 36100 (59.3399 iter/s, 1.68521s/100 iter), loss = 0.192064
I0801 13:39:19.671346  3162 solver.cpp:375]     Train net output #0: loss = 0.192064 (* 1 = 0.192064 loss)
I0801 13:39:19.671352  3162 sgd_solver.cpp:136] Iteration 36100, lr = 0.00435938, m = 0.9
I0801 13:39:21.346091  3162 solver.cpp:353] Iteration 36200 (59.7115 iter/s, 1.67472s/100 iter), loss = 0.0527203
I0801 13:39:21.346117  3162 solver.cpp:375]     Train net output #0: loss = 0.0527204 (* 1 = 0.0527204 loss)
I0801 13:39:21.346122  3162 sgd_solver.cpp:136] Iteration 36200, lr = 0.00434375, m = 0.9
I0801 13:39:22.985620  3162 solver.cpp:353] Iteration 36300 (60.9951 iter/s, 1.63948s/100 iter), loss = 0.0219281
I0801 13:39:22.985642  3162 solver.cpp:375]     Train net output #0: loss = 0.0219282 (* 1 = 0.0219282 loss)
I0801 13:39:22.985648  3162 sgd_solver.cpp:136] Iteration 36300, lr = 0.00432813, m = 0.9
I0801 13:39:24.644286  3162 solver.cpp:353] Iteration 36400 (60.2912 iter/s, 1.65862s/100 iter), loss = 0.120613
I0801 13:39:24.644314  3162 solver.cpp:375]     Train net output #0: loss = 0.120613 (* 1 = 0.120613 loss)
I0801 13:39:24.644320  3162 sgd_solver.cpp:136] Iteration 36400, lr = 0.0043125, m = 0.9
I0801 13:39:26.287356  3162 solver.cpp:353] Iteration 36500 (60.8637 iter/s, 1.64302s/100 iter), loss = 0.0653825
I0801 13:39:26.287519  3162 solver.cpp:375]     Train net output #0: loss = 0.0653826 (* 1 = 0.0653826 loss)
I0801 13:39:26.287546  3162 sgd_solver.cpp:136] Iteration 36500, lr = 0.00429688, m = 0.9
I0801 13:39:27.966568  3162 solver.cpp:353] Iteration 36600 (59.5537 iter/s, 1.67916s/100 iter), loss = 0.127271
I0801 13:39:27.966621  3162 solver.cpp:375]     Train net output #0: loss = 0.127271 (* 1 = 0.127271 loss)
I0801 13:39:27.966632  3162 sgd_solver.cpp:136] Iteration 36600, lr = 0.00428125, m = 0.9
I0801 13:39:29.666169  3162 solver.cpp:353] Iteration 36700 (58.8391 iter/s, 1.69955s/100 iter), loss = 0.0563792
I0801 13:39:29.666246  3162 solver.cpp:375]     Train net output #0: loss = 0.0563792 (* 1 = 0.0563792 loss)
I0801 13:39:29.666268  3162 sgd_solver.cpp:136] Iteration 36700, lr = 0.00426562, m = 0.9
I0801 13:39:31.291872  3162 solver.cpp:353] Iteration 36800 (61.5138 iter/s, 1.62565s/100 iter), loss = 0.0200769
I0801 13:39:31.291898  3162 solver.cpp:375]     Train net output #0: loss = 0.0200769 (* 1 = 0.0200769 loss)
I0801 13:39:31.291903  3162 sgd_solver.cpp:136] Iteration 36800, lr = 0.00425, m = 0.9
I0801 13:39:33.036584  3162 solver.cpp:353] Iteration 36900 (57.3179 iter/s, 1.74466s/100 iter), loss = 0.0518104
I0801 13:39:33.036682  3162 solver.cpp:375]     Train net output #0: loss = 0.0518104 (* 1 = 0.0518104 loss)
I0801 13:39:33.036691  3162 sgd_solver.cpp:136] Iteration 36900, lr = 0.00423437, m = 0.9
I0801 13:39:34.864212  3162 solver.cpp:550] Iteration 37000, Testing net (#0)
I0801 13:39:35.732789  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.850589
I0801 13:39:35.732805  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.993824
I0801 13:39:35.732813  3162 solver.cpp:635]     Test net output #2: loss = 0.544351 (* 1 = 0.544351 loss)
I0801 13:39:35.732847  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.868611s
I0801 13:39:35.757055  3162 solver.cpp:353] Iteration 37000 (36.7594 iter/s, 2.72039s/100 iter), loss = 0.0954478
I0801 13:39:35.757081  3162 solver.cpp:375]     Train net output #0: loss = 0.0954479 (* 1 = 0.0954479 loss)
I0801 13:39:35.757086  3162 sgd_solver.cpp:136] Iteration 37000, lr = 0.00421875, m = 0.9
I0801 13:39:37.386126  3162 solver.cpp:353] Iteration 37100 (61.3867 iter/s, 1.62902s/100 iter), loss = 0.0535867
I0801 13:39:37.386160  3162 solver.cpp:375]     Train net output #0: loss = 0.0535867 (* 1 = 0.0535867 loss)
I0801 13:39:37.386165  3162 sgd_solver.cpp:136] Iteration 37100, lr = 0.00420313, m = 0.9
I0801 13:39:39.183184  3162 solver.cpp:353] Iteration 37200 (55.6482 iter/s, 1.797s/100 iter), loss = 0.0263588
I0801 13:39:39.183212  3162 solver.cpp:375]     Train net output #0: loss = 0.0263588 (* 1 = 0.0263588 loss)
I0801 13:39:39.183218  3162 sgd_solver.cpp:136] Iteration 37200, lr = 0.0041875, m = 0.9
I0801 13:39:40.967257  3162 solver.cpp:353] Iteration 37300 (56.0533 iter/s, 1.78402s/100 iter), loss = 0.0551731
I0801 13:39:40.967283  3162 solver.cpp:375]     Train net output #0: loss = 0.0551731 (* 1 = 0.0551731 loss)
I0801 13:39:40.967288  3162 sgd_solver.cpp:136] Iteration 37300, lr = 0.00417187, m = 0.9
I0801 13:39:42.678665  3162 solver.cpp:353] Iteration 37400 (58.4335 iter/s, 1.71135s/100 iter), loss = 0.0327556
I0801 13:39:42.678712  3162 solver.cpp:375]     Train net output #0: loss = 0.0327556 (* 1 = 0.0327556 loss)
I0801 13:39:42.678725  3162 sgd_solver.cpp:136] Iteration 37400, lr = 0.00415625, m = 0.9
I0801 13:39:44.325409  3162 solver.cpp:353] Iteration 37500 (60.7276 iter/s, 1.6467s/100 iter), loss = 0.0441725
I0801 13:39:44.325439  3162 solver.cpp:375]     Train net output #0: loss = 0.0441726 (* 1 = 0.0441726 loss)
I0801 13:39:44.325445  3162 sgd_solver.cpp:136] Iteration 37500, lr = 0.00414062, m = 0.9
I0801 13:39:46.025084  3162 solver.cpp:353] Iteration 37600 (58.8367 iter/s, 1.69962s/100 iter), loss = 0.0324173
I0801 13:39:46.025110  3162 solver.cpp:375]     Train net output #0: loss = 0.0324173 (* 1 = 0.0324173 loss)
I0801 13:39:46.025113  3162 sgd_solver.cpp:136] Iteration 37600, lr = 0.004125, m = 0.9
I0801 13:39:47.749119  3162 solver.cpp:353] Iteration 37700 (58.0054 iter/s, 1.72398s/100 iter), loss = 0.00871978
I0801 13:39:47.749198  3162 solver.cpp:375]     Train net output #0: loss = 0.00871981 (* 1 = 0.00871981 loss)
I0801 13:39:47.749224  3162 sgd_solver.cpp:136] Iteration 37700, lr = 0.00410937, m = 0.9
I0801 13:39:49.355520  3162 solver.cpp:353] Iteration 37800 (62.2528 iter/s, 1.60635s/100 iter), loss = 0.0311927
I0801 13:39:49.355550  3162 solver.cpp:375]     Train net output #0: loss = 0.0311927 (* 1 = 0.0311927 loss)
I0801 13:39:49.355556  3162 sgd_solver.cpp:136] Iteration 37800, lr = 0.00409375, m = 0.9
I0801 13:39:51.077220  3162 solver.cpp:353] Iteration 37900 (58.0842 iter/s, 1.72164s/100 iter), loss = 0.0205516
I0801 13:39:51.077308  3162 solver.cpp:375]     Train net output #0: loss = 0.0205516 (* 1 = 0.0205516 loss)
I0801 13:39:51.077334  3162 sgd_solver.cpp:136] Iteration 37900, lr = 0.00407812, m = 0.9
I0801 13:39:52.865202  3162 solver.cpp:550] Iteration 38000, Testing net (#0)
I0801 13:39:53.744098  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.874707
I0801 13:39:53.744141  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.992941
I0801 13:39:53.744153  3162 solver.cpp:635]     Test net output #2: loss = 0.506074 (* 1 = 0.506074 loss)
I0801 13:39:53.744189  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.878962s
I0801 13:39:53.765185  3162 solver.cpp:353] Iteration 38000 (37.204 iter/s, 2.68788s/100 iter), loss = 0.00288516
I0801 13:39:53.765239  3162 solver.cpp:375]     Train net output #0: loss = 0.00288526 (* 1 = 0.00288526 loss)
I0801 13:39:53.765260  3162 sgd_solver.cpp:136] Iteration 38000, lr = 0.0040625, m = 0.9
I0801 13:39:55.367113  3162 solver.cpp:353] Iteration 38100 (62.4267 iter/s, 1.60188s/100 iter), loss = 0.00676392
I0801 13:39:55.367136  3162 solver.cpp:375]     Train net output #0: loss = 0.00676404 (* 1 = 0.00676404 loss)
I0801 13:39:55.367142  3162 sgd_solver.cpp:136] Iteration 38100, lr = 0.00404688, m = 0.9
I0801 13:39:57.024411  3162 solver.cpp:353] Iteration 38200 (60.3411 iter/s, 1.65725s/100 iter), loss = 0.0156398
I0801 13:39:57.024556  3162 solver.cpp:375]     Train net output #0: loss = 0.0156399 (* 1 = 0.0156399 loss)
I0801 13:39:57.024581  3162 sgd_solver.cpp:136] Iteration 38200, lr = 0.00403125, m = 0.9
I0801 13:39:58.822015  3162 solver.cpp:353] Iteration 38300 (55.6315 iter/s, 1.79754s/100 iter), loss = 0.0275954
I0801 13:39:58.822060  3162 solver.cpp:375]     Train net output #0: loss = 0.0275955 (* 1 = 0.0275955 loss)
I0801 13:39:58.822082  3162 sgd_solver.cpp:136] Iteration 38300, lr = 0.00401562, m = 0.9
I0801 13:40:00.478654  3162 solver.cpp:353] Iteration 38400 (60.3651 iter/s, 1.65659s/100 iter), loss = 0.00720645
I0801 13:40:00.478768  3162 solver.cpp:375]     Train net output #0: loss = 0.00720658 (* 1 = 0.00720658 loss)
I0801 13:40:00.478782  3162 sgd_solver.cpp:136] Iteration 38400, lr = 0.004, m = 0.9
I0801 13:40:02.128168  3162 solver.cpp:353] Iteration 38500 (60.6256 iter/s, 1.64947s/100 iter), loss = 0.0737231
I0801 13:40:02.128195  3162 solver.cpp:375]     Train net output #0: loss = 0.0737232 (* 1 = 0.0737232 loss)
I0801 13:40:02.128199  3162 sgd_solver.cpp:136] Iteration 38500, lr = 0.00398437, m = 0.9
I0801 13:40:03.808524  3162 solver.cpp:353] Iteration 38600 (59.5132 iter/s, 1.6803s/100 iter), loss = 0.0198419
I0801 13:40:03.808583  3162 solver.cpp:375]     Train net output #0: loss = 0.019842 (* 1 = 0.019842 loss)
I0801 13:40:03.808598  3162 sgd_solver.cpp:136] Iteration 38600, lr = 0.00396875, m = 0.9
I0801 13:40:05.442548  3162 solver.cpp:353] Iteration 38700 (61.2005 iter/s, 1.63397s/100 iter), loss = 0.0172191
I0801 13:40:05.442576  3162 solver.cpp:375]     Train net output #0: loss = 0.0172192 (* 1 = 0.0172192 loss)
I0801 13:40:05.442584  3162 sgd_solver.cpp:136] Iteration 38700, lr = 0.00395312, m = 0.9
I0801 13:40:07.018795  3162 solver.cpp:353] Iteration 38800 (63.4439 iter/s, 1.5762s/100 iter), loss = 0.00217083
I0801 13:40:07.018818  3162 solver.cpp:375]     Train net output #0: loss = 0.00217088 (* 1 = 0.00217088 loss)
I0801 13:40:07.018823  3162 sgd_solver.cpp:136] Iteration 38800, lr = 0.0039375, m = 0.9
I0801 13:40:08.587514  3162 solver.cpp:353] Iteration 38900 (63.7482 iter/s, 1.56867s/100 iter), loss = 0.0169003
I0801 13:40:08.587570  3162 solver.cpp:375]     Train net output #0: loss = 0.0169004 (* 1 = 0.0169004 loss)
I0801 13:40:08.587584  3162 sgd_solver.cpp:136] Iteration 38900, lr = 0.00392187, m = 0.9
I0801 13:40:10.156637  3162 solver.cpp:550] Iteration 39000, Testing net (#0)
I0801 13:40:10.992246  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.885589
I0801 13:40:10.992264  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.993235
I0801 13:40:10.992269  3162 solver.cpp:635]     Test net output #2: loss = 0.436306 (* 1 = 0.436306 loss)
I0801 13:40:10.992285  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.835625s
I0801 13:40:11.008044  3162 solver.cpp:353] Iteration 39000 (41.3145 iter/s, 2.42046s/100 iter), loss = 0.059026
I0801 13:40:11.008061  3162 solver.cpp:375]     Train net output #0: loss = 0.059026 (* 1 = 0.059026 loss)
I0801 13:40:11.008065  3162 sgd_solver.cpp:136] Iteration 39000, lr = 0.00390625, m = 0.9
I0801 13:40:12.713440  3162 solver.cpp:353] Iteration 39100 (58.6393 iter/s, 1.70534s/100 iter), loss = 0.0205703
I0801 13:40:12.713526  3162 solver.cpp:375]     Train net output #0: loss = 0.0205704 (* 1 = 0.0205704 loss)
I0801 13:40:12.713546  3162 sgd_solver.cpp:136] Iteration 39100, lr = 0.00389063, m = 0.9
I0801 13:40:14.425623  3162 solver.cpp:353] Iteration 39200 (58.4068 iter/s, 1.71213s/100 iter), loss = 0.00251298
I0801 13:40:14.425675  3162 solver.cpp:375]     Train net output #0: loss = 0.00251305 (* 1 = 0.00251305 loss)
I0801 13:40:14.425690  3162 sgd_solver.cpp:136] Iteration 39200, lr = 0.003875, m = 0.9
I0801 13:40:15.994472  3162 solver.cpp:353] Iteration 39300 (63.7431 iter/s, 1.5688s/100 iter), loss = 0.012378
I0801 13:40:15.994498  3162 solver.cpp:375]     Train net output #0: loss = 0.0123781 (* 1 = 0.0123781 loss)
I0801 13:40:15.994503  3162 sgd_solver.cpp:136] Iteration 39300, lr = 0.00385938, m = 0.9
I0801 13:40:17.402776  3155 data_reader.cpp:264] Starting prefetch of epoch 5
I0801 13:40:17.562026  3162 solver.cpp:353] Iteration 39400 (63.7956 iter/s, 1.56751s/100 iter), loss = 0.00425621
I0801 13:40:17.562049  3162 solver.cpp:375]     Train net output #0: loss = 0.00425631 (* 1 = 0.00425631 loss)
I0801 13:40:17.562053  3162 sgd_solver.cpp:136] Iteration 39400, lr = 0.00384375, m = 0.9
I0801 13:40:19.131225  3162 solver.cpp:353] Iteration 39500 (63.7288 iter/s, 1.56915s/100 iter), loss = 0.0399989
I0801 13:40:19.131249  3162 solver.cpp:375]     Train net output #0: loss = 0.039999 (* 1 = 0.039999 loss)
I0801 13:40:19.131254  3162 sgd_solver.cpp:136] Iteration 39500, lr = 0.00382812, m = 0.9
I0801 13:40:20.691617  3162 solver.cpp:353] Iteration 39600 (64.0884 iter/s, 1.56035s/100 iter), loss = 0.0356467
I0801 13:40:20.691643  3162 solver.cpp:375]     Train net output #0: loss = 0.0356468 (* 1 = 0.0356468 loss)
I0801 13:40:20.691648  3162 sgd_solver.cpp:136] Iteration 39600, lr = 0.0038125, m = 0.9
I0801 13:40:22.279026  3162 solver.cpp:353] Iteration 39700 (62.9979 iter/s, 1.58736s/100 iter), loss = 0.00320679
I0801 13:40:22.279078  3162 solver.cpp:375]     Train net output #0: loss = 0.00320689 (* 1 = 0.00320689 loss)
I0801 13:40:22.279093  3162 sgd_solver.cpp:136] Iteration 39700, lr = 0.00379687, m = 0.9
I0801 13:40:23.911515  3162 solver.cpp:353] Iteration 39800 (61.2583 iter/s, 1.63243s/100 iter), loss = 0.00271452
I0801 13:40:23.911561  3162 solver.cpp:375]     Train net output #0: loss = 0.00271463 (* 1 = 0.00271463 loss)
I0801 13:40:23.911583  3162 sgd_solver.cpp:136] Iteration 39800, lr = 0.00378125, m = 0.9
I0801 13:40:25.554805  3162 solver.cpp:353] Iteration 39900 (60.856 iter/s, 1.64322s/100 iter), loss = 0.0087018
I0801 13:40:25.554867  3162 solver.cpp:375]     Train net output #0: loss = 0.00870191 (* 1 = 0.00870191 loss)
I0801 13:40:25.554888  3162 sgd_solver.cpp:136] Iteration 39900, lr = 0.00376562, m = 0.9
I0801 13:40:27.153556  3162 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/cifar10_jacintonet11v2_iter_40000.caffemodel
I0801 13:40:27.163272  3162 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/cifar10_jacintonet11v2_iter_40000.solverstate
I0801 13:40:27.168110  3162 solver.cpp:550] Iteration 40000, Testing net (#0)
I0801 13:40:28.092874  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.874707
I0801 13:40:28.092895  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.992059
I0801 13:40:28.092900  3162 solver.cpp:635]     Test net output #2: loss = 0.510542 (* 1 = 0.510542 loss)
I0801 13:40:28.092916  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.92478s
I0801 13:40:28.108687  3162 solver.cpp:353] Iteration 40000 (39.1571 iter/s, 2.55381s/100 iter), loss = 0.00683019
I0801 13:40:28.108742  3162 solver.cpp:375]     Train net output #0: loss = 0.00683031 (* 1 = 0.00683031 loss)
I0801 13:40:28.108758  3162 sgd_solver.cpp:136] Iteration 40000, lr = 0.00375, m = 0.9
I0801 13:40:29.769644  3162 solver.cpp:353] Iteration 40100 (60.2082 iter/s, 1.6609s/100 iter), loss = 0.00309148
I0801 13:40:29.769675  3162 solver.cpp:375]     Train net output #0: loss = 0.0030916 (* 1 = 0.0030916 loss)
I0801 13:40:29.769682  3162 sgd_solver.cpp:136] Iteration 40100, lr = 0.00373438, m = 0.9
I0801 13:40:31.363916  3162 solver.cpp:353] Iteration 40200 (62.7264 iter/s, 1.59423s/100 iter), loss = 0.00163039
I0801 13:40:31.363943  3162 solver.cpp:375]     Train net output #0: loss = 0.00163051 (* 1 = 0.00163051 loss)
I0801 13:40:31.363950  3162 sgd_solver.cpp:136] Iteration 40200, lr = 0.00371875, m = 0.9
I0801 13:40:33.046895  3162 solver.cpp:353] Iteration 40300 (59.4204 iter/s, 1.68292s/100 iter), loss = 0.00245952
I0801 13:40:33.046919  3162 solver.cpp:375]     Train net output #0: loss = 0.00245964 (* 1 = 0.00245964 loss)
I0801 13:40:33.046923  3162 sgd_solver.cpp:136] Iteration 40300, lr = 0.00370313, m = 0.9
I0801 13:40:34.673785  3162 solver.cpp:353] Iteration 40400 (61.4688 iter/s, 1.62684s/100 iter), loss = 0.0452328
I0801 13:40:34.673810  3162 solver.cpp:375]     Train net output #0: loss = 0.0452329 (* 1 = 0.0452329 loss)
I0801 13:40:34.673815  3162 sgd_solver.cpp:136] Iteration 40400, lr = 0.0036875, m = 0.9
I0801 13:40:36.364244  3162 solver.cpp:353] Iteration 40500 (59.1575 iter/s, 1.6904s/100 iter), loss = 0.00162398
I0801 13:40:36.364295  3162 solver.cpp:375]     Train net output #0: loss = 0.0016241 (* 1 = 0.0016241 loss)
I0801 13:40:36.364307  3162 sgd_solver.cpp:136] Iteration 40500, lr = 0.00367187, m = 0.9
I0801 13:40:38.141518  3162 solver.cpp:353] Iteration 40600 (56.2677 iter/s, 1.77722s/100 iter), loss = 0.000638246
I0801 13:40:38.141573  3162 solver.cpp:375]     Train net output #0: loss = 0.000638369 (* 1 = 0.000638369 loss)
I0801 13:40:38.141588  3162 sgd_solver.cpp:136] Iteration 40600, lr = 0.00365625, m = 0.9
I0801 13:40:39.778069  3162 solver.cpp:353] Iteration 40700 (61.1062 iter/s, 1.63649s/100 iter), loss = 0.00405577
I0801 13:40:39.778237  3162 solver.cpp:375]     Train net output #0: loss = 0.00405589 (* 1 = 0.00405589 loss)
I0801 13:40:39.778259  3162 sgd_solver.cpp:136] Iteration 40700, lr = 0.00364062, m = 0.9
I0801 13:40:41.454694  3162 solver.cpp:353] Iteration 40800 (59.6455 iter/s, 1.67657s/100 iter), loss = 0.00169312
I0801 13:40:41.454766  3162 solver.cpp:375]     Train net output #0: loss = 0.00169324 (* 1 = 0.00169324 loss)
I0801 13:40:41.454783  3162 sgd_solver.cpp:136] Iteration 40800, lr = 0.003625, m = 0.9
I0801 13:40:43.153105  3162 solver.cpp:353] Iteration 40900 (58.8803 iter/s, 1.69836s/100 iter), loss = 0.00575983
I0801 13:40:43.153131  3162 solver.cpp:375]     Train net output #0: loss = 0.00575995 (* 1 = 0.00575995 loss)
I0801 13:40:43.153137  3162 sgd_solver.cpp:136] Iteration 40900, lr = 0.00360937, m = 0.9
I0801 13:40:44.766434  3162 solver.cpp:550] Iteration 41000, Testing net (#0)
I0801 13:40:45.658963  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.906178
I0801 13:40:45.659029  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995294
I0801 13:40:45.659276  3162 solver.cpp:635]     Test net output #2: loss = 0.362849 (* 1 = 0.362849 loss)
I0801 13:40:45.659581  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.893102s
I0801 13:40:45.676623  3162 solver.cpp:353] Iteration 41000 (39.6285 iter/s, 2.52344s/100 iter), loss = 0.00131682
I0801 13:40:45.676666  3162 solver.cpp:375]     Train net output #0: loss = 0.00131693 (* 1 = 0.00131693 loss)
I0801 13:40:45.676681  3162 sgd_solver.cpp:136] Iteration 41000, lr = 0.00359375, m = 0.9
I0801 13:40:47.397800  3162 solver.cpp:353] Iteration 41100 (58.1015 iter/s, 1.72113s/100 iter), loss = 0.000673118
I0801 13:40:47.397827  3162 solver.cpp:375]     Train net output #0: loss = 0.000673232 (* 1 = 0.000673232 loss)
I0801 13:40:47.397832  3162 sgd_solver.cpp:136] Iteration 41100, lr = 0.00357813, m = 0.9
I0801 13:40:49.117645  3162 solver.cpp:353] Iteration 41200 (58.1466 iter/s, 1.71979s/100 iter), loss = 0.00128522
I0801 13:40:49.117671  3162 solver.cpp:375]     Train net output #0: loss = 0.00128533 (* 1 = 0.00128533 loss)
I0801 13:40:49.117676  3162 sgd_solver.cpp:136] Iteration 41200, lr = 0.0035625, m = 0.9
I0801 13:40:50.819411  3162 solver.cpp:353] Iteration 41300 (58.7643 iter/s, 1.70171s/100 iter), loss = 0.000984248
I0801 13:40:50.819434  3162 solver.cpp:375]     Train net output #0: loss = 0.000984364 (* 1 = 0.000984364 loss)
I0801 13:40:50.819439  3162 sgd_solver.cpp:136] Iteration 41300, lr = 0.00354687, m = 0.9
I0801 13:40:52.446076  3162 solver.cpp:353] Iteration 41400 (61.4775 iter/s, 1.62661s/100 iter), loss = 0.00213586
I0801 13:40:52.446108  3162 solver.cpp:375]     Train net output #0: loss = 0.00213597 (* 1 = 0.00213597 loss)
I0801 13:40:52.446112  3162 sgd_solver.cpp:136] Iteration 41400, lr = 0.00353125, m = 0.9
I0801 13:40:54.134475  3162 solver.cpp:353] Iteration 41500 (59.2294 iter/s, 1.68835s/100 iter), loss = 0.0057573
I0801 13:40:54.134502  3162 solver.cpp:375]     Train net output #0: loss = 0.00575741 (* 1 = 0.00575741 loss)
I0801 13:40:54.134508  3162 sgd_solver.cpp:136] Iteration 41500, lr = 0.00351562, m = 0.9
I0801 13:40:55.785329  3162 solver.cpp:353] Iteration 41600 (60.5766 iter/s, 1.6508s/100 iter), loss = 0.0025656
I0801 13:40:55.785393  3162 solver.cpp:375]     Train net output #0: loss = 0.00256571 (* 1 = 0.00256571 loss)
I0801 13:40:55.785411  3162 sgd_solver.cpp:136] Iteration 41600, lr = 0.0035, m = 0.9
I0801 13:40:57.420375  3162 solver.cpp:353] Iteration 41700 (61.1623 iter/s, 1.63499s/100 iter), loss = 0.0487198
I0801 13:40:57.420459  3162 solver.cpp:375]     Train net output #0: loss = 0.0487199 (* 1 = 0.0487199 loss)
I0801 13:40:57.420473  3162 sgd_solver.cpp:136] Iteration 41700, lr = 0.00348437, m = 0.9
I0801 13:40:59.084676  3162 solver.cpp:353] Iteration 41800 (60.0871 iter/s, 1.66425s/100 iter), loss = 0.00133475
I0801 13:40:59.084702  3162 solver.cpp:375]     Train net output #0: loss = 0.00133486 (* 1 = 0.00133486 loss)
I0801 13:40:59.084708  3162 sgd_solver.cpp:136] Iteration 41800, lr = 0.00346875, m = 0.9
I0801 13:41:00.858556  3162 solver.cpp:353] Iteration 41900 (56.3758 iter/s, 1.77381s/100 iter), loss = 0.00175435
I0801 13:41:00.858605  3162 solver.cpp:375]     Train net output #0: loss = 0.00175447 (* 1 = 0.00175447 loss)
I0801 13:41:00.858717  3162 sgd_solver.cpp:136] Iteration 41900, lr = 0.00345312, m = 0.9
I0801 13:41:02.437942  3162 solver.cpp:550] Iteration 42000, Testing net (#0)
I0801 13:41:03.429474  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.906766
I0801 13:41:03.429497  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 13:41:03.429502  3162 solver.cpp:635]     Test net output #2: loss = 0.361323 (* 1 = 0.361323 loss)
I0801 13:41:03.429517  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.991549s
I0801 13:41:03.445490  3162 solver.cpp:353] Iteration 42000 (38.6568 iter/s, 2.58687s/100 iter), loss = 0.00300552
I0801 13:41:03.445508  3162 solver.cpp:375]     Train net output #0: loss = 0.00300564 (* 1 = 0.00300564 loss)
I0801 13:41:03.445514  3162 sgd_solver.cpp:136] Iteration 42000, lr = 0.0034375, m = 0.9
I0801 13:41:05.109112  3162 solver.cpp:353] Iteration 42100 (60.112 iter/s, 1.66356s/100 iter), loss = 0.00192978
I0801 13:41:05.109205  3162 solver.cpp:375]     Train net output #0: loss = 0.00192989 (* 1 = 0.00192989 loss)
I0801 13:41:05.109236  3162 sgd_solver.cpp:136] Iteration 42100, lr = 0.00342188, m = 0.9
I0801 13:41:06.747817  3162 solver.cpp:353] Iteration 42200 (61.0257 iter/s, 1.63865s/100 iter), loss = 0.000785716
I0801 13:41:06.747848  3162 solver.cpp:375]     Train net output #0: loss = 0.000785838 (* 1 = 0.000785838 loss)
I0801 13:41:06.747853  3162 sgd_solver.cpp:136] Iteration 42200, lr = 0.00340625, m = 0.9
I0801 13:41:08.543383  3162 solver.cpp:353] Iteration 42300 (55.6947 iter/s, 1.7955s/100 iter), loss = 0.000862253
I0801 13:41:08.543517  3162 solver.cpp:375]     Train net output #0: loss = 0.000862375 (* 1 = 0.000862375 loss)
I0801 13:41:08.543536  3162 sgd_solver.cpp:136] Iteration 42300, lr = 0.00339063, m = 0.9
I0801 13:41:10.200898  3162 solver.cpp:353] Iteration 42400 (60.3328 iter/s, 1.65747s/100 iter), loss = 0.00214077
I0801 13:41:10.200923  3162 solver.cpp:375]     Train net output #0: loss = 0.0021409 (* 1 = 0.0021409 loss)
I0801 13:41:10.200929  3162 sgd_solver.cpp:136] Iteration 42400, lr = 0.003375, m = 0.9
I0801 13:41:11.911299  3162 solver.cpp:353] Iteration 42500 (58.4677 iter/s, 1.71035s/100 iter), loss = 0.0075202
I0801 13:41:11.911350  3162 solver.cpp:375]     Train net output #0: loss = 0.00752032 (* 1 = 0.00752032 loss)
I0801 13:41:11.911370  3162 sgd_solver.cpp:136] Iteration 42500, lr = 0.00335937, m = 0.9
I0801 13:41:13.634366  3162 solver.cpp:353] Iteration 42600 (58.0385 iter/s, 1.72299s/100 iter), loss = 0.00570642
I0801 13:41:13.634430  3162 solver.cpp:375]     Train net output #0: loss = 0.00570655 (* 1 = 0.00570655 loss)
I0801 13:41:13.634449  3162 sgd_solver.cpp:136] Iteration 42600, lr = 0.00334375, m = 0.9
I0801 13:41:15.357712  3162 solver.cpp:353] Iteration 42700 (58.0281 iter/s, 1.7233s/100 iter), loss = 0.00473276
I0801 13:41:15.357739  3162 solver.cpp:375]     Train net output #0: loss = 0.00473288 (* 1 = 0.00473288 loss)
I0801 13:41:15.357745  3162 sgd_solver.cpp:136] Iteration 42700, lr = 0.00332812, m = 0.9
I0801 13:41:16.954197  3162 solver.cpp:353] Iteration 42800 (62.6395 iter/s, 1.59644s/100 iter), loss = 0.000943755
I0801 13:41:16.954222  3162 solver.cpp:375]     Train net output #0: loss = 0.00094388 (* 1 = 0.00094388 loss)
I0801 13:41:16.954228  3162 sgd_solver.cpp:136] Iteration 42800, lr = 0.0033125, m = 0.9
I0801 13:41:18.527747  3162 solver.cpp:353] Iteration 42900 (63.5526 iter/s, 1.5735s/100 iter), loss = 0.00628885
I0801 13:41:18.527772  3162 solver.cpp:375]     Train net output #0: loss = 0.00628897 (* 1 = 0.00628897 loss)
I0801 13:41:18.527778  3162 sgd_solver.cpp:136] Iteration 42900, lr = 0.00329687, m = 0.9
I0801 13:41:20.073931  3162 solver.cpp:550] Iteration 43000, Testing net (#0)
I0801 13:41:20.907050  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.913531
I0801 13:41:20.907069  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 13:41:20.907076  3162 solver.cpp:635]     Test net output #2: loss = 0.3247 (* 1 = 0.3247 loss)
I0801 13:41:20.907094  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.833139s
I0801 13:41:20.922844  3162 solver.cpp:353] Iteration 43000 (41.7532 iter/s, 2.39503s/100 iter), loss = 0.000706747
I0801 13:41:20.922864  3162 solver.cpp:375]     Train net output #0: loss = 0.000706873 (* 1 = 0.000706873 loss)
I0801 13:41:20.922869  3162 sgd_solver.cpp:136] Iteration 43000, lr = 0.00328125, m = 0.9
I0801 13:41:22.492360  3162 solver.cpp:353] Iteration 43100 (63.7161 iter/s, 1.56946s/100 iter), loss = 0.000193988
I0801 13:41:22.492388  3162 solver.cpp:375]     Train net output #0: loss = 0.000194113 (* 1 = 0.000194113 loss)
I0801 13:41:22.492394  3162 sgd_solver.cpp:136] Iteration 43100, lr = 0.00326563, m = 0.9
I0801 13:41:24.204766  3162 solver.cpp:353] Iteration 43200 (58.3994 iter/s, 1.71235s/100 iter), loss = 0.00508896
I0801 13:41:24.204813  3162 solver.cpp:375]     Train net output #0: loss = 0.00508909 (* 1 = 0.00508909 loss)
I0801 13:41:24.204831  3162 sgd_solver.cpp:136] Iteration 43200, lr = 0.00325, m = 0.9
I0801 13:41:25.805055  3162 solver.cpp:353] Iteration 43300 (62.4904 iter/s, 1.60025s/100 iter), loss = 0.0428959
I0801 13:41:25.805084  3162 solver.cpp:375]     Train net output #0: loss = 0.0428961 (* 1 = 0.0428961 loss)
I0801 13:41:25.805091  3162 sgd_solver.cpp:136] Iteration 43300, lr = 0.00323438, m = 0.9
I0801 13:41:27.368271  3162 solver.cpp:353] Iteration 43400 (63.9727 iter/s, 1.56317s/100 iter), loss = 0.00152221
I0801 13:41:27.368297  3162 solver.cpp:375]     Train net output #0: loss = 0.00152234 (* 1 = 0.00152234 loss)
I0801 13:41:27.368304  3162 sgd_solver.cpp:136] Iteration 43400, lr = 0.00321875, m = 0.9
I0801 13:41:28.944165  3162 solver.cpp:353] Iteration 43500 (63.4581 iter/s, 1.57584s/100 iter), loss = 0.000266026
I0801 13:41:28.944272  3162 solver.cpp:375]     Train net output #0: loss = 0.000266156 (* 1 = 0.000266156 loss)
I0801 13:41:28.944285  3162 sgd_solver.cpp:136] Iteration 43500, lr = 0.00320312, m = 0.9
I0801 13:41:30.537282  3162 solver.cpp:353] Iteration 43600 (62.7725 iter/s, 1.59306s/100 iter), loss = 0.00107396
I0801 13:41:30.537369  3162 solver.cpp:375]     Train net output #0: loss = 0.00107409 (* 1 = 0.00107409 loss)
I0801 13:41:30.537395  3162 sgd_solver.cpp:136] Iteration 43600, lr = 0.0031875, m = 0.9
I0801 13:41:32.210388  3162 solver.cpp:353] Iteration 43700 (59.7707 iter/s, 1.67306s/100 iter), loss = 0.000559897
I0801 13:41:32.210443  3162 solver.cpp:375]     Train net output #0: loss = 0.000560027 (* 1 = 0.000560027 loss)
I0801 13:41:32.210456  3162 sgd_solver.cpp:136] Iteration 43700, lr = 0.00317187, m = 0.9
I0801 13:41:33.997359  3162 solver.cpp:353] Iteration 43800 (55.9624 iter/s, 1.78691s/100 iter), loss = 0.000115311
I0801 13:41:33.997385  3162 solver.cpp:375]     Train net output #0: loss = 0.000115447 (* 1 = 0.000115447 loss)
I0801 13:41:33.997388  3162 sgd_solver.cpp:136] Iteration 43800, lr = 0.00315625, m = 0.9
I0801 13:41:35.620261  3162 solver.cpp:353] Iteration 43900 (61.6204 iter/s, 1.62284s/100 iter), loss = 0.000924004
I0801 13:41:35.620353  3162 solver.cpp:375]     Train net output #0: loss = 0.000924133 (* 1 = 0.000924133 loss)
I0801 13:41:35.620379  3162 sgd_solver.cpp:136] Iteration 43900, lr = 0.00314062, m = 0.9
I0801 13:41:36.211024  3155 data_reader.cpp:264] Starting prefetch of epoch 6
I0801 13:41:37.299839  3162 solver.cpp:550] Iteration 44000, Testing net (#0)
I0801 13:41:38.162240  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.920589
I0801 13:41:38.162281  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997353
I0801 13:41:38.162293  3162 solver.cpp:635]     Test net output #2: loss = 0.299588 (* 1 = 0.299588 loss)
I0801 13:41:38.162325  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.862459s
I0801 13:41:38.186105  3162 solver.cpp:353] Iteration 44000 (38.9746 iter/s, 2.56577s/100 iter), loss = 0.00177889
I0801 13:41:38.186134  3162 solver.cpp:375]     Train net output #0: loss = 0.00177902 (* 1 = 0.00177902 loss)
I0801 13:41:38.186139  3162 sgd_solver.cpp:136] Iteration 44000, lr = 0.003125, m = 0.9
I0801 13:41:39.883288  3162 solver.cpp:353] Iteration 44100 (58.923 iter/s, 1.69713s/100 iter), loss = 0.0014717
I0801 13:41:39.883316  3162 solver.cpp:375]     Train net output #0: loss = 0.00147183 (* 1 = 0.00147183 loss)
I0801 13:41:39.883322  3162 sgd_solver.cpp:136] Iteration 44100, lr = 0.00310938, m = 0.9
I0801 13:41:41.607561  3162 solver.cpp:353] Iteration 44200 (57.9973 iter/s, 1.72422s/100 iter), loss = 0.000366713
I0801 13:41:41.607587  3162 solver.cpp:375]     Train net output #0: loss = 0.000366843 (* 1 = 0.000366843 loss)
I0801 13:41:41.607594  3162 sgd_solver.cpp:136] Iteration 44200, lr = 0.00309375, m = 0.9
I0801 13:41:43.256124  3162 solver.cpp:353] Iteration 44300 (60.6608 iter/s, 1.64851s/100 iter), loss = 0.00116798
I0801 13:41:43.256197  3162 solver.cpp:375]     Train net output #0: loss = 0.00116811 (* 1 = 0.00116811 loss)
I0801 13:41:43.256230  3162 sgd_solver.cpp:136] Iteration 44300, lr = 0.00307812, m = 0.9
I0801 13:41:44.997181  3162 solver.cpp:353] Iteration 44400 (57.4384 iter/s, 1.74099s/100 iter), loss = 0.00349851
I0801 13:41:44.997227  3162 solver.cpp:375]     Train net output #0: loss = 0.00349864 (* 1 = 0.00349864 loss)
I0801 13:41:44.997236  3162 sgd_solver.cpp:136] Iteration 44400, lr = 0.0030625, m = 0.9
I0801 13:41:46.604364  3162 solver.cpp:353] Iteration 44500 (62.2229 iter/s, 1.60713s/100 iter), loss = 0.000682049
I0801 13:41:46.604586  3162 solver.cpp:375]     Train net output #0: loss = 0.000682179 (* 1 = 0.000682179 loss)
I0801 13:41:46.604677  3162 sgd_solver.cpp:136] Iteration 44500, lr = 0.00304687, m = 0.9
I0801 13:41:48.321514  3162 solver.cpp:353] Iteration 44600 (58.2375 iter/s, 1.71711s/100 iter), loss = 0.00198234
I0801 13:41:48.321538  3162 solver.cpp:375]     Train net output #0: loss = 0.00198247 (* 1 = 0.00198247 loss)
I0801 13:41:48.321563  3162 sgd_solver.cpp:136] Iteration 44600, lr = 0.00303125, m = 0.9
I0801 13:41:50.024631  3162 solver.cpp:353] Iteration 44700 (58.7179 iter/s, 1.70306s/100 iter), loss = 0.000152428
I0801 13:41:50.024659  3162 solver.cpp:375]     Train net output #0: loss = 0.000152557 (* 1 = 0.000152557 loss)
I0801 13:41:50.024667  3162 sgd_solver.cpp:136] Iteration 44700, lr = 0.00301562, m = 0.9
I0801 13:41:51.802703  3162 solver.cpp:353] Iteration 44800 (56.2423 iter/s, 1.77802s/100 iter), loss = 0.00346247
I0801 13:41:51.802734  3162 solver.cpp:375]     Train net output #0: loss = 0.0034626 (* 1 = 0.0034626 loss)
I0801 13:41:51.802742  3162 sgd_solver.cpp:136] Iteration 44800, lr = 0.003, m = 0.9
I0801 13:41:53.395320  3162 solver.cpp:353] Iteration 44900 (62.7918 iter/s, 1.59256s/100 iter), loss = 0.00280417
I0801 13:41:53.395412  3162 solver.cpp:375]     Train net output #0: loss = 0.0028043 (* 1 = 0.0028043 loss)
I0801 13:41:53.395421  3162 sgd_solver.cpp:136] Iteration 44900, lr = 0.00298437, m = 0.9
I0801 13:41:55.035684  3162 solver.cpp:550] Iteration 45000, Testing net (#0)
I0801 13:41:55.954360  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.919119
I0801 13:41:55.954378  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997647
I0801 13:41:55.954383  3162 solver.cpp:635]     Test net output #2: loss = 0.317746 (* 1 = 0.317746 loss)
I0801 13:41:55.954399  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.91869s
I0801 13:41:55.970777  3162 solver.cpp:353] Iteration 45000 (38.8293 iter/s, 2.57538s/100 iter), loss = 0.00110971
I0801 13:41:55.970798  3162 solver.cpp:375]     Train net output #0: loss = 0.00110984 (* 1 = 0.00110984 loss)
I0801 13:41:55.970803  3162 sgd_solver.cpp:136] Iteration 45000, lr = 0.00296875, m = 0.9
I0801 13:41:57.613937  3162 solver.cpp:353] Iteration 45100 (60.8602 iter/s, 1.64311s/100 iter), loss = 0.00105821
I0801 13:41:57.613976  3162 solver.cpp:375]     Train net output #0: loss = 0.00105834 (* 1 = 0.00105834 loss)
I0801 13:41:57.613982  3162 sgd_solver.cpp:136] Iteration 45100, lr = 0.00295313, m = 0.9
I0801 13:41:59.300936  3162 solver.cpp:353] Iteration 45200 (59.2787 iter/s, 1.68695s/100 iter), loss = 0.00127226
I0801 13:41:59.301185  3162 solver.cpp:375]     Train net output #0: loss = 0.00127239 (* 1 = 0.00127239 loss)
I0801 13:41:59.301195  3162 sgd_solver.cpp:136] Iteration 45200, lr = 0.0029375, m = 0.9
I0801 13:42:01.107919  3162 solver.cpp:353] Iteration 45300 (55.3426 iter/s, 1.80693s/100 iter), loss = 0.000400358
I0801 13:42:01.107947  3162 solver.cpp:375]     Train net output #0: loss = 0.000400489 (* 1 = 0.000400489 loss)
I0801 13:42:01.107954  3162 sgd_solver.cpp:136] Iteration 45300, lr = 0.00292188, m = 0.9
I0801 13:42:02.739167  3162 solver.cpp:353] Iteration 45400 (61.3047 iter/s, 1.6312s/100 iter), loss = 0.00091782
I0801 13:42:02.739197  3162 solver.cpp:375]     Train net output #0: loss = 0.000917949 (* 1 = 0.000917949 loss)
I0801 13:42:02.739204  3162 sgd_solver.cpp:136] Iteration 45400, lr = 0.00290625, m = 0.9
I0801 13:42:04.369397  3162 solver.cpp:353] Iteration 45500 (61.3429 iter/s, 1.63018s/100 iter), loss = 0.000671595
I0801 13:42:04.369424  3162 solver.cpp:375]     Train net output #0: loss = 0.000671726 (* 1 = 0.000671726 loss)
I0801 13:42:04.369431  3162 sgd_solver.cpp:136] Iteration 45500, lr = 0.00289063, m = 0.9
I0801 13:42:05.981248  3162 solver.cpp:353] Iteration 45600 (62.0428 iter/s, 1.61179s/100 iter), loss = 0.00123627
I0801 13:42:05.981294  3162 solver.cpp:375]     Train net output #0: loss = 0.0012364 (* 1 = 0.0012364 loss)
I0801 13:42:05.981317  3162 sgd_solver.cpp:136] Iteration 45600, lr = 0.002875, m = 0.9
I0801 13:42:07.759845  3162 solver.cpp:353] Iteration 45700 (56.2261 iter/s, 1.77853s/100 iter), loss = 0.00128022
I0801 13:42:07.760129  3162 solver.cpp:375]     Train net output #0: loss = 0.00128035 (* 1 = 0.00128035 loss)
I0801 13:42:07.760257  3162 sgd_solver.cpp:136] Iteration 45700, lr = 0.00285937, m = 0.9
I0801 13:42:09.404652  3162 solver.cpp:353] Iteration 45800 (60.799 iter/s, 1.64476s/100 iter), loss = 0.000407946
I0801 13:42:09.404741  3162 solver.cpp:375]     Train net output #0: loss = 0.000408077 (* 1 = 0.000408077 loss)
I0801 13:42:09.404748  3162 sgd_solver.cpp:136] Iteration 45800, lr = 0.00284375, m = 0.9
I0801 13:42:11.109827  3162 solver.cpp:353] Iteration 45900 (58.6469 iter/s, 1.70512s/100 iter), loss = 0.00242511
I0801 13:42:11.109850  3162 solver.cpp:375]     Train net output #0: loss = 0.00242524 (* 1 = 0.00242524 loss)
I0801 13:42:11.109854  3162 sgd_solver.cpp:136] Iteration 45900, lr = 0.00282812, m = 0.9
I0801 13:42:12.747606  3162 solver.cpp:550] Iteration 46000, Testing net (#0)
I0801 13:42:13.617817  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.910001
I0801 13:42:13.617843  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 13:42:13.617851  3162 solver.cpp:635]     Test net output #2: loss = 0.331918 (* 1 = 0.331918 loss)
I0801 13:42:13.617874  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.87025s
I0801 13:42:13.638016  3162 solver.cpp:353] Iteration 46000 (39.5553 iter/s, 2.52811s/100 iter), loss = 0.00168187
I0801 13:42:13.638053  3162 solver.cpp:375]     Train net output #0: loss = 0.001682 (* 1 = 0.001682 loss)
I0801 13:42:13.638069  3162 sgd_solver.cpp:136] Iteration 46000, lr = 0.0028125, m = 0.9
I0801 13:42:15.262856  3162 solver.cpp:353] Iteration 46100 (61.5463 iter/s, 1.62479s/100 iter), loss = 0.00138357
I0801 13:42:15.262881  3162 solver.cpp:375]     Train net output #0: loss = 0.0013837 (* 1 = 0.0013837 loss)
I0801 13:42:15.262887  3162 sgd_solver.cpp:136] Iteration 46100, lr = 0.00279688, m = 0.9
I0801 13:42:16.923267  3162 solver.cpp:353] Iteration 46200 (60.228 iter/s, 1.66036s/100 iter), loss = 0.000780777
I0801 13:42:16.923319  3162 solver.cpp:375]     Train net output #0: loss = 0.000780909 (* 1 = 0.000780909 loss)
I0801 13:42:16.923333  3162 sgd_solver.cpp:136] Iteration 46200, lr = 0.00278125, m = 0.9
I0801 13:42:18.669873  3162 solver.cpp:353] Iteration 46300 (57.2558 iter/s, 1.74655s/100 iter), loss = 0.000649345
I0801 13:42:18.669937  3162 solver.cpp:375]     Train net output #0: loss = 0.000649477 (* 1 = 0.000649477 loss)
I0801 13:42:18.669957  3162 sgd_solver.cpp:136] Iteration 46300, lr = 0.00276563, m = 0.9
I0801 13:42:20.454071  3162 solver.cpp:353] Iteration 46400 (56.0492 iter/s, 1.78415s/100 iter), loss = 0.000218571
I0801 13:42:20.454097  3162 solver.cpp:375]     Train net output #0: loss = 0.000218704 (* 1 = 0.000218704 loss)
I0801 13:42:20.454103  3162 sgd_solver.cpp:136] Iteration 46400, lr = 0.00275, m = 0.9
I0801 13:42:22.157699  3162 solver.cpp:353] Iteration 46500 (58.7001 iter/s, 1.70358s/100 iter), loss = 0.001244
I0801 13:42:22.157724  3162 solver.cpp:375]     Train net output #0: loss = 0.00124413 (* 1 = 0.00124413 loss)
I0801 13:42:22.157730  3162 sgd_solver.cpp:136] Iteration 46500, lr = 0.00273437, m = 0.9
I0801 13:42:23.838973  3162 solver.cpp:353] Iteration 46600 (59.4806 iter/s, 1.68122s/100 iter), loss = 0.000676376
I0801 13:42:23.839046  3162 solver.cpp:375]     Train net output #0: loss = 0.00067651 (* 1 = 0.00067651 loss)
I0801 13:42:23.839066  3162 sgd_solver.cpp:136] Iteration 46600, lr = 0.00271875, m = 0.9
I0801 13:42:25.524070  3162 solver.cpp:353] Iteration 46700 (59.3457 iter/s, 1.68504s/100 iter), loss = 0.00146967
I0801 13:42:25.524143  3162 solver.cpp:375]     Train net output #0: loss = 0.00146981 (* 1 = 0.00146981 loss)
I0801 13:42:25.524158  3162 sgd_solver.cpp:136] Iteration 46700, lr = 0.00270312, m = 0.9
I0801 13:42:27.093744  3162 solver.cpp:353] Iteration 46800 (63.7095 iter/s, 1.56963s/100 iter), loss = 0.000499066
I0801 13:42:27.093773  3162 solver.cpp:375]     Train net output #0: loss = 0.0004992 (* 1 = 0.0004992 loss)
I0801 13:42:27.093780  3162 sgd_solver.cpp:136] Iteration 46800, lr = 0.0026875, m = 0.9
I0801 13:42:28.689327  3162 solver.cpp:353] Iteration 46900 (62.6749 iter/s, 1.59554s/100 iter), loss = 0.000178366
I0801 13:42:28.689353  3162 solver.cpp:375]     Train net output #0: loss = 0.000178502 (* 1 = 0.000178502 loss)
I0801 13:42:28.689359  3162 sgd_solver.cpp:136] Iteration 46900, lr = 0.00267187, m = 0.9
I0801 13:42:30.246856  3162 solver.cpp:550] Iteration 47000, Testing net (#0)
I0801 13:42:31.079294  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.907942
I0801 13:42:31.079313  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 13:42:31.079319  3162 solver.cpp:635]     Test net output #2: loss = 0.346737 (* 1 = 0.346737 loss)
I0801 13:42:31.079334  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.832455s
I0801 13:42:31.095064  3162 solver.cpp:353] Iteration 47000 (41.5686 iter/s, 2.40566s/100 iter), loss = 0.00126874
I0801 13:42:31.095084  3162 solver.cpp:375]     Train net output #0: loss = 0.00126888 (* 1 = 0.00126888 loss)
I0801 13:42:31.095090  3162 sgd_solver.cpp:136] Iteration 47000, lr = 0.00265625, m = 0.9
I0801 13:42:32.754015  3162 solver.cpp:353] Iteration 47100 (60.2815 iter/s, 1.65888s/100 iter), loss = 0.00160353
I0801 13:42:32.754098  3162 solver.cpp:375]     Train net output #0: loss = 0.00160367 (* 1 = 0.00160367 loss)
I0801 13:42:32.754125  3162 sgd_solver.cpp:136] Iteration 47100, lr = 0.00264063, m = 0.9
I0801 13:42:34.348410  3162 solver.cpp:353] Iteration 47200 (62.7214 iter/s, 1.59435s/100 iter), loss = 0.00103847
I0801 13:42:34.348434  3162 solver.cpp:375]     Train net output #0: loss = 0.0010386 (* 1 = 0.0010386 loss)
I0801 13:42:34.348440  3162 sgd_solver.cpp:136] Iteration 47200, lr = 0.002625, m = 0.9
I0801 13:42:36.033289  3162 solver.cpp:353] Iteration 47300 (59.3533 iter/s, 1.68483s/100 iter), loss = 0.000257937
I0801 13:42:36.033316  3162 solver.cpp:375]     Train net output #0: loss = 0.000258072 (* 1 = 0.000258072 loss)
I0801 13:42:36.033323  3162 sgd_solver.cpp:136] Iteration 47300, lr = 0.00260938, m = 0.9
I0801 13:42:37.603328  3162 solver.cpp:353] Iteration 47400 (63.6946 iter/s, 1.56999s/100 iter), loss = 0.0012193
I0801 13:42:37.603351  3162 solver.cpp:375]     Train net output #0: loss = 0.00121943 (* 1 = 0.00121943 loss)
I0801 13:42:37.603356  3162 sgd_solver.cpp:136] Iteration 47400, lr = 0.00259375, m = 0.9
I0801 13:42:39.174629  3162 solver.cpp:353] Iteration 47500 (63.6435 iter/s, 1.57125s/100 iter), loss = 0.00108784
I0801 13:42:39.174655  3162 solver.cpp:375]     Train net output #0: loss = 0.00108798 (* 1 = 0.00108798 loss)
I0801 13:42:39.174661  3162 sgd_solver.cpp:136] Iteration 47500, lr = 0.00257812, m = 0.9
I0801 13:42:40.791870  3162 solver.cpp:353] Iteration 47600 (61.8358 iter/s, 1.61719s/100 iter), loss = 0.000155928
I0801 13:42:40.792098  3162 solver.cpp:375]     Train net output #0: loss = 0.000156062 (* 1 = 0.000156062 loss)
I0801 13:42:40.792224  3162 sgd_solver.cpp:136] Iteration 47600, lr = 0.0025625, m = 0.9
I0801 13:42:42.439376  3162 solver.cpp:353] Iteration 47700 (60.6996 iter/s, 1.64746s/100 iter), loss = 0.0021478
I0801 13:42:42.439429  3162 solver.cpp:375]     Train net output #0: loss = 0.00214793 (* 1 = 0.00214793 loss)
I0801 13:42:42.439437  3162 sgd_solver.cpp:136] Iteration 47700, lr = 0.00254687, m = 0.9
I0801 13:42:44.146163  3162 solver.cpp:353] Iteration 47800 (58.5915 iter/s, 1.70673s/100 iter), loss = 0.00156237
I0801 13:42:44.146189  3162 solver.cpp:375]     Train net output #0: loss = 0.00156251 (* 1 = 0.00156251 loss)
I0801 13:42:44.146195  3162 sgd_solver.cpp:136] Iteration 47800, lr = 0.00253125, m = 0.9
I0801 13:42:45.877737  3162 solver.cpp:353] Iteration 47900 (57.7526 iter/s, 1.73152s/100 iter), loss = 0.000995956
I0801 13:42:45.877761  3162 solver.cpp:375]     Train net output #0: loss = 0.000996089 (* 1 = 0.000996089 loss)
I0801 13:42:45.877768  3162 sgd_solver.cpp:136] Iteration 47900, lr = 0.00251562, m = 0.9
I0801 13:42:47.457856  3162 solver.cpp:550] Iteration 48000, Testing net (#0)
I0801 13:42:47.488243  3160 data_reader.cpp:264] Starting prefetch of epoch 6
I0801 13:42:48.466490  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.910295
I0801 13:42:48.466513  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 13:42:48.466519  3162 solver.cpp:635]     Test net output #2: loss = 0.340712 (* 1 = 0.340712 loss)
I0801 13:42:48.466766  3162 solver.cpp:305] [MultiGPU] Tests completed in 1.00866s
I0801 13:42:48.482089  3162 solver.cpp:353] Iteration 48000 (38.3984 iter/s, 2.60428s/100 iter), loss = 0.00158598
I0801 13:42:48.482120  3162 solver.cpp:375]     Train net output #0: loss = 0.00158611 (* 1 = 0.00158611 loss)
I0801 13:42:48.482125  3162 sgd_solver.cpp:136] Iteration 48000, lr = 0.0025, m = 0.9
I0801 13:42:50.080060  3162 solver.cpp:353] Iteration 48100 (62.5818 iter/s, 1.59791s/100 iter), loss = 0.00135707
I0801 13:42:50.080103  3162 solver.cpp:375]     Train net output #0: loss = 0.0013572 (* 1 = 0.0013572 loss)
I0801 13:42:50.080221  3162 sgd_solver.cpp:136] Iteration 48100, lr = 0.00248438, m = 0.9
I0801 13:42:51.834782  3162 solver.cpp:353] Iteration 48200 (56.9909 iter/s, 1.75467s/100 iter), loss = 0.00153219
I0801 13:42:51.835047  3162 solver.cpp:375]     Train net output #0: loss = 0.00153232 (* 1 = 0.00153232 loss)
I0801 13:42:51.835172  3162 sgd_solver.cpp:136] Iteration 48200, lr = 0.00246875, m = 0.9
I0801 13:42:53.538575  3162 solver.cpp:353] Iteration 48300 (58.6942 iter/s, 1.70375s/100 iter), loss = 0.000675027
I0801 13:42:53.538600  3162 solver.cpp:375]     Train net output #0: loss = 0.000675161 (* 1 = 0.000675161 loss)
I0801 13:42:53.538606  3162 sgd_solver.cpp:136] Iteration 48300, lr = 0.00245313, m = 0.9
I0801 13:42:55.236089  3162 solver.cpp:353] Iteration 48400 (58.9117 iter/s, 1.69746s/100 iter), loss = 0.000178911
I0801 13:42:55.236161  3162 solver.cpp:375]     Train net output #0: loss = 0.000179045 (* 1 = 0.000179045 loss)
I0801 13:42:55.236181  3162 sgd_solver.cpp:136] Iteration 48400, lr = 0.0024375, m = 0.9
I0801 13:42:56.902595  3162 solver.cpp:353] Iteration 48500 (60.0077 iter/s, 1.66645s/100 iter), loss = 0.000420602
I0801 13:42:56.902670  3162 solver.cpp:375]     Train net output #0: loss = 0.000420735 (* 1 = 0.000420735 loss)
I0801 13:42:56.902695  3162 sgd_solver.cpp:136] Iteration 48500, lr = 0.00242188, m = 0.9
I0801 13:42:58.521833  3162 solver.cpp:353] Iteration 48600 (61.7592 iter/s, 1.61919s/100 iter), loss = 0.00126469
I0801 13:42:58.521895  3162 solver.cpp:375]     Train net output #0: loss = 0.00126482 (* 1 = 0.00126482 loss)
I0801 13:42:58.521914  3162 sgd_solver.cpp:136] Iteration 48600, lr = 0.00240625, m = 0.9
I0801 13:43:00.237936  3162 solver.cpp:353] Iteration 48700 (58.2736 iter/s, 1.71604s/100 iter), loss = 0.000549521
I0801 13:43:00.237987  3162 solver.cpp:375]     Train net output #0: loss = 0.000549655 (* 1 = 0.000549655 loss)
I0801 13:43:00.238198  3162 sgd_solver.cpp:136] Iteration 48700, lr = 0.00239062, m = 0.9
I0801 13:43:01.888628  3162 solver.cpp:353] Iteration 48800 (60.5827 iter/s, 1.65064s/100 iter), loss = 0.00071877
I0801 13:43:01.888784  3162 solver.cpp:375]     Train net output #0: loss = 0.000718904 (* 1 = 0.000718904 loss)
I0801 13:43:01.888811  3162 sgd_solver.cpp:136] Iteration 48800, lr = 0.002375, m = 0.9
I0801 13:43:03.500634  3162 solver.cpp:353] Iteration 48900 (62.0362 iter/s, 1.61196s/100 iter), loss = 0.00104895
I0801 13:43:03.500660  3162 solver.cpp:375]     Train net output #0: loss = 0.00104909 (* 1 = 0.00104909 loss)
I0801 13:43:03.500666  3162 sgd_solver.cpp:136] Iteration 48900, lr = 0.00235937, m = 0.9
I0801 13:43:05.203639  3162 solver.cpp:550] Iteration 49000, Testing net (#0)
I0801 13:43:06.151350  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.910001
I0801 13:43:06.151370  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995588
I0801 13:43:06.151374  3162 solver.cpp:635]     Test net output #2: loss = 0.341068 (* 1 = 0.341068 loss)
I0801 13:43:06.151391  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.947724s
I0801 13:43:06.167027  3162 solver.cpp:353] Iteration 49000 (37.505 iter/s, 2.66631s/100 iter), loss = 0.00331568
I0801 13:43:06.167043  3162 solver.cpp:375]     Train net output #0: loss = 0.00331581 (* 1 = 0.00331581 loss)
I0801 13:43:06.167047  3162 sgd_solver.cpp:136] Iteration 49000, lr = 0.00234375, m = 0.9
I0801 13:43:07.853847  3162 solver.cpp:353] Iteration 49100 (59.2855 iter/s, 1.68675s/100 iter), loss = 0.00102733
I0801 13:43:07.853898  3162 solver.cpp:375]     Train net output #0: loss = 0.00102747 (* 1 = 0.00102747 loss)
I0801 13:43:07.853906  3162 sgd_solver.cpp:136] Iteration 49100, lr = 0.00232813, m = 0.9
I0801 13:43:09.455504  3162 solver.cpp:353] Iteration 49200 (62.4369 iter/s, 1.60162s/100 iter), loss = 0.00150719
I0801 13:43:09.455556  3162 solver.cpp:375]     Train net output #0: loss = 0.00150732 (* 1 = 0.00150732 loss)
I0801 13:43:09.455571  3162 sgd_solver.cpp:136] Iteration 49200, lr = 0.0023125, m = 0.9
I0801 13:43:11.106542  3162 solver.cpp:353] Iteration 49300 (60.57 iter/s, 1.65098s/100 iter), loss = 0.000505843
I0801 13:43:11.106570  3162 solver.cpp:375]     Train net output #0: loss = 0.000505978 (* 1 = 0.000505978 loss)
I0801 13:43:11.106576  3162 sgd_solver.cpp:136] Iteration 49300, lr = 0.00229687, m = 0.9
I0801 13:43:12.784241  3162 solver.cpp:353] Iteration 49400 (59.6074 iter/s, 1.67764s/100 iter), loss = 0.000741314
I0801 13:43:12.784342  3162 solver.cpp:375]     Train net output #0: loss = 0.00074145 (* 1 = 0.00074145 loss)
I0801 13:43:12.784369  3162 sgd_solver.cpp:136] Iteration 49400, lr = 0.00228125, m = 0.9
I0801 13:43:14.535714  3162 solver.cpp:353] Iteration 49500 (57.0964 iter/s, 1.75142s/100 iter), loss = 0.000677509
I0801 13:43:14.535742  3162 solver.cpp:375]     Train net output #0: loss = 0.000677643 (* 1 = 0.000677643 loss)
I0801 13:43:14.535748  3162 sgd_solver.cpp:136] Iteration 49500, lr = 0.00226562, m = 0.9
I0801 13:43:16.221545  3162 solver.cpp:353] Iteration 49600 (59.3198 iter/s, 1.68578s/100 iter), loss = 0.000919108
I0801 13:43:16.221596  3162 solver.cpp:375]     Train net output #0: loss = 0.000919243 (* 1 = 0.000919243 loss)
I0801 13:43:16.221607  3162 sgd_solver.cpp:136] Iteration 49600, lr = 0.00225, m = 0.9
I0801 13:43:17.885617  3162 solver.cpp:353] Iteration 49700 (60.0954 iter/s, 1.66402s/100 iter), loss = 0.000867596
I0801 13:43:17.885643  3162 solver.cpp:375]     Train net output #0: loss = 0.000867731 (* 1 = 0.000867731 loss)
I0801 13:43:17.885650  3162 sgd_solver.cpp:136] Iteration 49700, lr = 0.00223437, m = 0.9
I0801 13:43:19.583712  3162 solver.cpp:353] Iteration 49800 (58.8914 iter/s, 1.69804s/100 iter), loss = 0.000457131
I0801 13:43:19.583740  3162 solver.cpp:375]     Train net output #0: loss = 0.000457266 (* 1 = 0.000457266 loss)
I0801 13:43:19.583747  3162 sgd_solver.cpp:136] Iteration 49800, lr = 0.00221875, m = 0.9
I0801 13:43:21.267417  3162 solver.cpp:353] Iteration 49900 (59.3947 iter/s, 1.68365s/100 iter), loss = 0.00024708
I0801 13:43:21.267441  3162 solver.cpp:375]     Train net output #0: loss = 0.000247215 (* 1 = 0.000247215 loss)
I0801 13:43:21.267446  3162 sgd_solver.cpp:136] Iteration 49900, lr = 0.00220312, m = 0.9
I0801 13:43:22.869583  3162 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/cifar10_jacintonet11v2_iter_50000.caffemodel
I0801 13:43:22.879567  3162 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/cifar10_jacintonet11v2_iter_50000.solverstate
I0801 13:43:22.883487  3162 solver.cpp:550] Iteration 50000, Testing net (#0)
I0801 13:43:23.728260  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.905589
I0801 13:43:23.728277  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995294
I0801 13:43:23.728283  3162 solver.cpp:635]     Test net output #2: loss = 0.362569 (* 1 = 0.362569 loss)
I0801 13:43:23.728302  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.844791s
I0801 13:43:23.746763  3162 solver.cpp:353] Iteration 50000 (40.3345 iter/s, 2.47927s/100 iter), loss = 0.000739925
I0801 13:43:23.746786  3162 solver.cpp:375]     Train net output #0: loss = 0.000740061 (* 1 = 0.000740061 loss)
I0801 13:43:23.746790  3162 sgd_solver.cpp:136] Iteration 50000, lr = 0.0021875, m = 0.9
I0801 13:43:25.412663  3162 solver.cpp:353] Iteration 50100 (60.0295 iter/s, 1.66585s/100 iter), loss = 0.000256935
I0801 13:43:25.412691  3162 solver.cpp:375]     Train net output #0: loss = 0.000257071 (* 1 = 0.000257071 loss)
I0801 13:43:25.412698  3162 sgd_solver.cpp:136] Iteration 50100, lr = 0.00217188, m = 0.9
I0801 13:43:27.057118  3162 solver.cpp:353] Iteration 50200 (60.8122 iter/s, 1.64441s/100 iter), loss = 0.0036674
I0801 13:43:27.057145  3162 solver.cpp:375]     Train net output #0: loss = 0.00366753 (* 1 = 0.00366753 loss)
I0801 13:43:27.057152  3162 sgd_solver.cpp:136] Iteration 50200, lr = 0.00215625, m = 0.9
I0801 13:43:28.733896  3162 solver.cpp:353] Iteration 50300 (59.6405 iter/s, 1.67671s/100 iter), loss = 0.000221225
I0801 13:43:28.733953  3162 solver.cpp:375]     Train net output #0: loss = 0.000221363 (* 1 = 0.000221363 loss)
I0801 13:43:28.733965  3162 sgd_solver.cpp:136] Iteration 50300, lr = 0.00214063, m = 0.9
I0801 13:43:30.422533  3162 solver.cpp:353] Iteration 50400 (59.221 iter/s, 1.68859s/100 iter), loss = 0.00101477
I0801 13:43:30.422559  3162 solver.cpp:375]     Train net output #0: loss = 0.0010149 (* 1 = 0.0010149 loss)
I0801 13:43:30.422564  3162 sgd_solver.cpp:136] Iteration 50400, lr = 0.002125, m = 0.9
I0801 13:43:32.042439  3162 solver.cpp:353] Iteration 50500 (61.7339 iter/s, 1.61986s/100 iter), loss = 0.0024834
I0801 13:43:32.042520  3162 solver.cpp:375]     Train net output #0: loss = 0.00248353 (* 1 = 0.00248353 loss)
I0801 13:43:32.042527  3162 sgd_solver.cpp:136] Iteration 50500, lr = 0.00210937, m = 0.9
I0801 13:43:33.724272  3162 solver.cpp:353] Iteration 50600 (59.4609 iter/s, 1.68178s/100 iter), loss = 0.00128911
I0801 13:43:33.724300  3162 solver.cpp:375]     Train net output #0: loss = 0.00128925 (* 1 = 0.00128925 loss)
I0801 13:43:33.724308  3162 sgd_solver.cpp:136] Iteration 50600, lr = 0.00209375, m = 0.9
I0801 13:43:35.381302  3162 solver.cpp:353] Iteration 50700 (60.351 iter/s, 1.65697s/100 iter), loss = 0.000791598
I0801 13:43:35.381350  3162 solver.cpp:375]     Train net output #0: loss = 0.000791736 (* 1 = 0.000791736 loss)
I0801 13:43:35.381362  3162 sgd_solver.cpp:136] Iteration 50700, lr = 0.00207812, m = 0.9
I0801 13:43:36.974944  3162 solver.cpp:353] Iteration 50800 (62.7512 iter/s, 1.5936s/100 iter), loss = 0.000438652
I0801 13:43:36.975010  3162 solver.cpp:375]     Train net output #0: loss = 0.00043879 (* 1 = 0.00043879 loss)
I0801 13:43:36.975030  3162 sgd_solver.cpp:136] Iteration 50800, lr = 0.0020625, m = 0.9
I0801 13:43:38.534957  3162 solver.cpp:353] Iteration 50900 (64.1041 iter/s, 1.55996s/100 iter), loss = 0.000174204
I0801 13:43:38.534981  3162 solver.cpp:375]     Train net output #0: loss = 0.000174341 (* 1 = 0.000174341 loss)
I0801 13:43:38.534986  3162 sgd_solver.cpp:136] Iteration 50900, lr = 0.00204687, m = 0.9
I0801 13:43:40.085543  3162 solver.cpp:550] Iteration 51000, Testing net (#0)
I0801 13:43:40.918507  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.910589
I0801 13:43:40.918527  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995588
I0801 13:43:40.918532  3162 solver.cpp:635]     Test net output #2: loss = 0.349791 (* 1 = 0.349791 loss)
I0801 13:43:40.918550  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.832983s
I0801 13:43:40.934376  3162 solver.cpp:353] Iteration 51000 (41.678 iter/s, 2.39935s/100 iter), loss = 0.00075372
I0801 13:43:40.934399  3162 solver.cpp:375]     Train net output #0: loss = 0.000753856 (* 1 = 0.000753856 loss)
I0801 13:43:40.934403  3162 sgd_solver.cpp:136] Iteration 51000, lr = 0.00203125, m = 0.9
I0801 13:43:42.644898  3162 solver.cpp:353] Iteration 51100 (58.4638 iter/s, 1.71046s/100 iter), loss = 0.000837216
I0801 13:43:42.644937  3162 solver.cpp:375]     Train net output #0: loss = 0.000837352 (* 1 = 0.000837352 loss)
I0801 13:43:42.644945  3162 sgd_solver.cpp:136] Iteration 51100, lr = 0.00201563, m = 0.9
I0801 13:43:44.287966  3162 solver.cpp:353] Iteration 51200 (60.8639 iter/s, 1.64301s/100 iter), loss = 0.00045923
I0801 13:43:44.288055  3162 solver.cpp:375]     Train net output #0: loss = 0.000459366 (* 1 = 0.000459366 loss)
I0801 13:43:44.288084  3162 sgd_solver.cpp:136] Iteration 51200, lr = 0.002, m = 0.9
I0801 13:43:46.039134  3162 solver.cpp:353] Iteration 51300 (57.1064 iter/s, 1.75112s/100 iter), loss = 0.00157965
I0801 13:43:46.039160  3162 solver.cpp:375]     Train net output #0: loss = 0.00157978 (* 1 = 0.00157978 loss)
I0801 13:43:46.039166  3162 sgd_solver.cpp:136] Iteration 51300, lr = 0.00198438, m = 0.9
I0801 13:43:47.625402  3162 solver.cpp:353] Iteration 51400 (63.043 iter/s, 1.58622s/100 iter), loss = 0.000651837
I0801 13:43:47.625465  3162 solver.cpp:375]     Train net output #0: loss = 0.000651974 (* 1 = 0.000651974 loss)
I0801 13:43:47.625484  3162 sgd_solver.cpp:136] Iteration 51400, lr = 0.00196875, m = 0.9
I0801 13:43:49.186872  3162 solver.cpp:353] Iteration 51500 (64.0442 iter/s, 1.56142s/100 iter), loss = 0.000356192
I0801 13:43:49.186936  3162 solver.cpp:375]     Train net output #0: loss = 0.000356328 (* 1 = 0.000356328 loss)
I0801 13:43:49.186954  3162 sgd_solver.cpp:136] Iteration 51500, lr = 0.00195312, m = 0.9
I0801 13:43:50.773639  3162 solver.cpp:353] Iteration 51600 (63.0233 iter/s, 1.58672s/100 iter), loss = 0.000493204
I0801 13:43:50.773663  3162 solver.cpp:375]     Train net output #0: loss = 0.000493341 (* 1 = 0.000493341 loss)
I0801 13:43:50.773669  3162 sgd_solver.cpp:136] Iteration 51600, lr = 0.0019375, m = 0.9
I0801 13:43:52.337167  3162 solver.cpp:353] Iteration 51700 (63.96 iter/s, 1.56348s/100 iter), loss = 0.00111509
I0801 13:43:52.337191  3162 solver.cpp:375]     Train net output #0: loss = 0.00111522 (* 1 = 0.00111522 loss)
I0801 13:43:52.337198  3162 sgd_solver.cpp:136] Iteration 51700, lr = 0.00192187, m = 0.9
I0801 13:43:53.999502  3162 solver.cpp:353] Iteration 51800 (60.1581 iter/s, 1.66229s/100 iter), loss = 0.00095207
I0801 13:43:53.999763  3162 solver.cpp:375]     Train net output #0: loss = 0.000952208 (* 1 = 0.000952208 loss)
I0801 13:43:53.999771  3162 sgd_solver.cpp:136] Iteration 51800, lr = 0.00190625, m = 0.9
I0801 13:43:55.620753  3162 solver.cpp:353] Iteration 51900 (61.6828 iter/s, 1.6212s/100 iter), loss = 0.000142626
I0801 13:43:55.620833  3162 solver.cpp:375]     Train net output #0: loss = 0.000142763 (* 1 = 0.000142763 loss)
I0801 13:43:55.620857  3162 sgd_solver.cpp:136] Iteration 51900, lr = 0.00189062, m = 0.9
I0801 13:43:57.297513  3162 solver.cpp:550] Iteration 52000, Testing net (#0)
I0801 13:43:58.067265  3160 data_reader.cpp:264] Starting prefetch of epoch 7
I0801 13:43:58.180057  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.907354
I0801 13:43:58.180074  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 13:43:58.180079  3162 solver.cpp:635]     Test net output #2: loss = 0.340996 (* 1 = 0.340996 loss)
I0801 13:43:58.180099  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.882567s
I0801 13:43:58.195694  3162 solver.cpp:353] Iteration 52000 (38.8369 iter/s, 2.57487s/100 iter), loss = 0.000604304
I0801 13:43:58.195713  3162 solver.cpp:375]     Train net output #0: loss = 0.000604441 (* 1 = 0.000604441 loss)
I0801 13:43:58.195716  3162 sgd_solver.cpp:136] Iteration 52000, lr = 0.001875, m = 0.9
I0801 13:43:59.848172  3162 solver.cpp:353] Iteration 52100 (60.5171 iter/s, 1.65242s/100 iter), loss = 0.00056832
I0801 13:43:59.848201  3162 solver.cpp:375]     Train net output #0: loss = 0.000568455 (* 1 = 0.000568455 loss)
I0801 13:43:59.848207  3162 sgd_solver.cpp:136] Iteration 52100, lr = 0.00185938, m = 0.9
I0801 13:44:01.485837  3162 solver.cpp:353] Iteration 52200 (61.0645 iter/s, 1.63761s/100 iter), loss = 0.000400705
I0801 13:44:01.485863  3162 solver.cpp:375]     Train net output #0: loss = 0.000400841 (* 1 = 0.000400841 loss)
I0801 13:44:01.485868  3162 sgd_solver.cpp:136] Iteration 52200, lr = 0.00184375, m = 0.9
I0801 13:44:03.213965  3162 solver.cpp:353] Iteration 52300 (57.8678 iter/s, 1.72808s/100 iter), loss = 0.000682341
I0801 13:44:03.214069  3162 solver.cpp:375]     Train net output #0: loss = 0.000682476 (* 1 = 0.000682476 loss)
I0801 13:44:03.214085  3162 sgd_solver.cpp:136] Iteration 52300, lr = 0.00182813, m = 0.9
I0801 13:44:04.874238  3162 solver.cpp:353] Iteration 52400 (60.233 iter/s, 1.66022s/100 iter), loss = 0.000902179
I0801 13:44:04.874266  3162 solver.cpp:375]     Train net output #0: loss = 0.000902314 (* 1 = 0.000902314 loss)
I0801 13:44:04.874274  3162 sgd_solver.cpp:136] Iteration 52400, lr = 0.0018125, m = 0.9
I0801 13:44:06.473001  3162 solver.cpp:353] Iteration 52500 (62.5504 iter/s, 1.59871s/100 iter), loss = 0.000473948
I0801 13:44:06.473177  3162 solver.cpp:375]     Train net output #0: loss = 0.000474083 (* 1 = 0.000474083 loss)
I0801 13:44:06.473255  3162 sgd_solver.cpp:136] Iteration 52500, lr = 0.00179687, m = 0.9
I0801 13:44:08.066215  3162 solver.cpp:353] Iteration 52600 (62.7683 iter/s, 1.59316s/100 iter), loss = 0.00430296
I0801 13:44:08.066258  3162 solver.cpp:375]     Train net output #0: loss = 0.0043031 (* 1 = 0.0043031 loss)
I0801 13:44:08.066279  3162 sgd_solver.cpp:136] Iteration 52600, lr = 0.00178125, m = 0.9
I0801 13:44:09.782471  3162 solver.cpp:353] Iteration 52700 (58.2683 iter/s, 1.7162s/100 iter), loss = 0.000131277
I0801 13:44:09.782521  3162 solver.cpp:375]     Train net output #0: loss = 0.000131412 (* 1 = 0.000131412 loss)
I0801 13:44:09.782534  3162 sgd_solver.cpp:136] Iteration 52700, lr = 0.00176562, m = 0.9
I0801 13:44:11.558751  3162 solver.cpp:353] Iteration 52800 (56.2991 iter/s, 1.77623s/100 iter), loss = 0.000648517
I0801 13:44:11.558776  3162 solver.cpp:375]     Train net output #0: loss = 0.000648653 (* 1 = 0.000648653 loss)
I0801 13:44:11.558784  3162 sgd_solver.cpp:136] Iteration 52800, lr = 0.00175, m = 0.9
I0801 13:44:13.213408  3162 solver.cpp:353] Iteration 52900 (60.4374 iter/s, 1.6546s/100 iter), loss = 0.0005505
I0801 13:44:13.213439  3162 solver.cpp:375]     Train net output #0: loss = 0.000550636 (* 1 = 0.000550636 loss)
I0801 13:44:13.213448  3162 sgd_solver.cpp:136] Iteration 52900, lr = 0.00173437, m = 0.9
I0801 13:44:14.901338  3162 solver.cpp:550] Iteration 53000, Testing net (#0)
I0801 13:44:15.773753  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.915295
I0801 13:44:15.773773  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995294
I0801 13:44:15.773778  3162 solver.cpp:635]     Test net output #2: loss = 0.323728 (* 1 = 0.323728 loss)
I0801 13:44:15.773795  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.872434s
I0801 13:44:15.789299  3162 solver.cpp:353] Iteration 53000 (38.8227 iter/s, 2.57581s/100 iter), loss = 0.00217085
I0801 13:44:15.789316  3162 solver.cpp:375]     Train net output #0: loss = 0.00217099 (* 1 = 0.00217099 loss)
I0801 13:44:15.789322  3162 sgd_solver.cpp:136] Iteration 53000, lr = 0.00171875, m = 0.9
I0801 13:44:17.404556  3162 solver.cpp:353] Iteration 53100 (61.9117 iter/s, 1.6152s/100 iter), loss = 0.00082083
I0801 13:44:17.404582  3162 solver.cpp:375]     Train net output #0: loss = 0.000820966 (* 1 = 0.000820966 loss)
I0801 13:44:17.404588  3162 sgd_solver.cpp:136] Iteration 53100, lr = 0.00170313, m = 0.9
I0801 13:44:19.072906  3162 solver.cpp:353] Iteration 53200 (59.9413 iter/s, 1.6683s/100 iter), loss = 0.000684126
I0801 13:44:19.072931  3162 solver.cpp:375]     Train net output #0: loss = 0.000684261 (* 1 = 0.000684261 loss)
I0801 13:44:19.072937  3162 sgd_solver.cpp:136] Iteration 53200, lr = 0.0016875, m = 0.9
I0801 13:44:20.735505  3162 solver.cpp:353] Iteration 53300 (60.1489 iter/s, 1.66254s/100 iter), loss = 0.00126845
I0801 13:44:20.735553  3162 solver.cpp:375]     Train net output #0: loss = 0.00126858 (* 1 = 0.00126858 loss)
I0801 13:44:20.735572  3162 sgd_solver.cpp:136] Iteration 53300, lr = 0.00167188, m = 0.9
I0801 13:44:22.364574  3162 solver.cpp:353] Iteration 53400 (61.3865 iter/s, 1.62902s/100 iter), loss = 0.000630443
I0801 13:44:22.364599  3162 solver.cpp:375]     Train net output #0: loss = 0.000630577 (* 1 = 0.000630577 loss)
I0801 13:44:22.364605  3162 sgd_solver.cpp:136] Iteration 53400, lr = 0.00165625, m = 0.9
I0801 13:44:24.079638  3162 solver.cpp:353] Iteration 53500 (58.3085 iter/s, 1.71501s/100 iter), loss = 0.00164472
I0801 13:44:24.079664  3162 solver.cpp:375]     Train net output #0: loss = 0.00164485 (* 1 = 0.00164485 loss)
I0801 13:44:24.079670  3162 sgd_solver.cpp:136] Iteration 53500, lr = 0.00164062, m = 0.9
I0801 13:44:25.716122  3162 solver.cpp:353] Iteration 53600 (61.1086 iter/s, 1.63643s/100 iter), loss = 0.000188614
I0801 13:44:25.716150  3162 solver.cpp:375]     Train net output #0: loss = 0.000188748 (* 1 = 0.000188748 loss)
I0801 13:44:25.716156  3162 sgd_solver.cpp:136] Iteration 53600, lr = 0.001625, m = 0.9
I0801 13:44:27.302887  3162 solver.cpp:353] Iteration 53700 (63.0233 iter/s, 1.58671s/100 iter), loss = 0.00104392
I0801 13:44:27.302928  3162 solver.cpp:375]     Train net output #0: loss = 0.00104406 (* 1 = 0.00104406 loss)
I0801 13:44:27.302935  3162 sgd_solver.cpp:136] Iteration 53700, lr = 0.00160937, m = 0.9
I0801 13:44:28.939327  3162 solver.cpp:353] Iteration 53800 (61.1102 iter/s, 1.63639s/100 iter), loss = 0.00145785
I0801 13:44:28.939354  3162 solver.cpp:375]     Train net output #0: loss = 0.00145799 (* 1 = 0.00145799 loss)
I0801 13:44:28.939360  3162 sgd_solver.cpp:136] Iteration 53800, lr = 0.00159375, m = 0.9
I0801 13:44:30.524838  3162 solver.cpp:353] Iteration 53900 (63.0731 iter/s, 1.58546s/100 iter), loss = 0.000961353
I0801 13:44:30.524878  3162 solver.cpp:375]     Train net output #0: loss = 0.000961489 (* 1 = 0.000961489 loss)
I0801 13:44:30.524890  3162 sgd_solver.cpp:136] Iteration 53900, lr = 0.00157812, m = 0.9
I0801 13:44:32.130167  3162 solver.cpp:550] Iteration 54000, Testing net (#0)
I0801 13:44:32.994294  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.918236
I0801 13:44:32.994323  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 13:44:32.994330  3162 solver.cpp:635]     Test net output #2: loss = 0.294463 (* 1 = 0.294463 loss)
I0801 13:44:32.994350  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.864159s
I0801 13:44:33.010334  3162 solver.cpp:353] Iteration 54000 (40.2347 iter/s, 2.48541s/100 iter), loss = 0.0011502
I0801 13:44:33.010386  3162 solver.cpp:375]     Train net output #0: loss = 0.00115034 (* 1 = 0.00115034 loss)
I0801 13:44:33.010403  3162 sgd_solver.cpp:136] Iteration 54000, lr = 0.0015625, m = 0.9
I0801 13:44:34.582283  3162 solver.cpp:353] Iteration 54100 (63.6172 iter/s, 1.5719s/100 iter), loss = 0.000455005
I0801 13:44:34.582360  3162 solver.cpp:375]     Train net output #0: loss = 0.000455141 (* 1 = 0.000455141 loss)
I0801 13:44:34.582365  3162 sgd_solver.cpp:136] Iteration 54100, lr = 0.00154688, m = 0.9
I0801 13:44:36.281616  3162 solver.cpp:353] Iteration 54200 (58.8485 iter/s, 1.69928s/100 iter), loss = 0.000608006
I0801 13:44:36.281662  3162 solver.cpp:375]     Train net output #0: loss = 0.000608143 (* 1 = 0.000608143 loss)
I0801 13:44:36.281683  3162 sgd_solver.cpp:136] Iteration 54200, lr = 0.00153125, m = 0.9
I0801 13:44:37.868492  3162 solver.cpp:353] Iteration 54300 (63.019 iter/s, 1.58682s/100 iter), loss = 0.000591646
I0801 13:44:37.868568  3162 solver.cpp:375]     Train net output #0: loss = 0.000591784 (* 1 = 0.000591784 loss)
I0801 13:44:37.868588  3162 sgd_solver.cpp:136] Iteration 54300, lr = 0.00151563, m = 0.9
I0801 13:44:39.550293  3162 solver.cpp:353] Iteration 54400 (59.4618 iter/s, 1.68175s/100 iter), loss = 0.00224078
I0801 13:44:39.550318  3162 solver.cpp:375]     Train net output #0: loss = 0.00224091 (* 1 = 0.00224091 loss)
I0801 13:44:39.550323  3162 sgd_solver.cpp:136] Iteration 54400, lr = 0.0015, m = 0.9
I0801 13:44:41.268268  3162 solver.cpp:353] Iteration 54500 (58.2098 iter/s, 1.71792s/100 iter), loss = 0.00114785
I0801 13:44:41.268331  3162 solver.cpp:375]     Train net output #0: loss = 0.00114799 (* 1 = 0.00114799 loss)
I0801 13:44:41.268349  3162 sgd_solver.cpp:136] Iteration 54500, lr = 0.00148437, m = 0.9
I0801 13:44:42.940306  3162 solver.cpp:353] Iteration 54600 (59.8095 iter/s, 1.67198s/100 iter), loss = 0.000524731
I0801 13:44:42.940536  3162 solver.cpp:375]     Train net output #0: loss = 0.000524868 (* 1 = 0.000524868 loss)
I0801 13:44:42.940670  3162 sgd_solver.cpp:136] Iteration 54600, lr = 0.00146875, m = 0.9
I0801 13:44:44.581524  3162 solver.cpp:353] Iteration 54700 (60.9322 iter/s, 1.64117s/100 iter), loss = 0.00238353
I0801 13:44:44.581547  3162 solver.cpp:375]     Train net output #0: loss = 0.00238367 (* 1 = 0.00238367 loss)
I0801 13:44:44.581552  3162 sgd_solver.cpp:136] Iteration 54700, lr = 0.00145312, m = 0.9
I0801 13:44:46.294838  3162 solver.cpp:353] Iteration 54800 (58.3682 iter/s, 1.71326s/100 iter), loss = 0.00382023
I0801 13:44:46.294864  3162 solver.cpp:375]     Train net output #0: loss = 0.00382036 (* 1 = 0.00382036 loss)
I0801 13:44:46.294870  3162 sgd_solver.cpp:136] Iteration 54800, lr = 0.0014375, m = 0.9
I0801 13:44:47.865986  3162 solver.cpp:353] Iteration 54900 (63.6496 iter/s, 1.5711s/100 iter), loss = 0.000898492
I0801 13:44:47.866013  3162 solver.cpp:375]     Train net output #0: loss = 0.00089863 (* 1 = 0.00089863 loss)
I0801 13:44:47.866019  3162 sgd_solver.cpp:136] Iteration 54900, lr = 0.00142187, m = 0.9
I0801 13:44:49.414680  3162 solver.cpp:550] Iteration 55000, Testing net (#0)
I0801 13:44:50.252034  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.91853
I0801 13:44:50.252053  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 13:44:50.252058  3162 solver.cpp:635]     Test net output #2: loss = 0.286537 (* 1 = 0.286537 loss)
I0801 13:44:50.252074  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.837369s
I0801 13:44:50.267709  3162 solver.cpp:353] Iteration 55000 (41.638 iter/s, 2.40165s/100 iter), loss = 0.000916867
I0801 13:44:50.267724  3162 solver.cpp:375]     Train net output #0: loss = 0.000917004 (* 1 = 0.000917004 loss)
I0801 13:44:50.267729  3162 sgd_solver.cpp:136] Iteration 55000, lr = 0.00140625, m = 0.9
I0801 13:44:51.848708  3162 solver.cpp:353] Iteration 55100 (63.2533 iter/s, 1.58094s/100 iter), loss = 0.000842907
I0801 13:44:51.848739  3162 solver.cpp:375]     Train net output #0: loss = 0.000843044 (* 1 = 0.000843044 loss)
I0801 13:44:51.848747  3162 sgd_solver.cpp:136] Iteration 55100, lr = 0.00139063, m = 0.9
I0801 13:44:53.491247  3162 solver.cpp:353] Iteration 55200 (60.8832 iter/s, 1.64249s/100 iter), loss = 0.00148851
I0801 13:44:53.491317  3162 solver.cpp:375]     Train net output #0: loss = 0.00148865 (* 1 = 0.00148865 loss)
I0801 13:44:53.491338  3162 sgd_solver.cpp:136] Iteration 55200, lr = 0.001375, m = 0.9
I0801 13:44:55.176470  3162 solver.cpp:353] Iteration 55300 (59.3411 iter/s, 1.68517s/100 iter), loss = 0.000737463
I0801 13:44:55.176491  3162 solver.cpp:375]     Train net output #0: loss = 0.000737601 (* 1 = 0.000737601 loss)
I0801 13:44:55.176496  3162 sgd_solver.cpp:136] Iteration 55300, lr = 0.00135938, m = 0.9
I0801 13:44:56.768203  3162 solver.cpp:353] Iteration 55400 (62.8266 iter/s, 1.59168s/100 iter), loss = 0.000680306
I0801 13:44:56.768232  3162 solver.cpp:375]     Train net output #0: loss = 0.000680443 (* 1 = 0.000680443 loss)
I0801 13:44:56.768239  3162 sgd_solver.cpp:136] Iteration 55400, lr = 0.00134375, m = 0.9
I0801 13:44:58.328851  3162 solver.cpp:353] Iteration 55500 (64.078 iter/s, 1.5606s/100 iter), loss = 0.000894445
I0801 13:44:58.328876  3162 solver.cpp:375]     Train net output #0: loss = 0.000894582 (* 1 = 0.000894582 loss)
I0801 13:44:58.328881  3162 sgd_solver.cpp:136] Iteration 55500, lr = 0.00132813, m = 0.9
I0801 13:44:59.905804  3162 solver.cpp:353] Iteration 55600 (63.4154 iter/s, 1.5769s/100 iter), loss = 0.000544839
I0801 13:44:59.905831  3162 solver.cpp:375]     Train net output #0: loss = 0.000544976 (* 1 = 0.000544976 loss)
I0801 13:44:59.905836  3162 sgd_solver.cpp:136] Iteration 55600, lr = 0.0013125, m = 0.9
I0801 13:45:01.519470  3162 solver.cpp:353] Iteration 55700 (61.9726 iter/s, 1.61362s/100 iter), loss = 0.000429219
I0801 13:45:01.519533  3162 solver.cpp:375]     Train net output #0: loss = 0.000429356 (* 1 = 0.000429356 loss)
I0801 13:45:01.519551  3162 sgd_solver.cpp:136] Iteration 55700, lr = 0.00129687, m = 0.9
I0801 13:45:03.191597  3162 solver.cpp:353] Iteration 55800 (59.8059 iter/s, 1.67208s/100 iter), loss = 0.000715096
I0801 13:45:03.191622  3162 solver.cpp:375]     Train net output #0: loss = 0.000715233 (* 1 = 0.000715233 loss)
I0801 13:45:03.191627  3162 sgd_solver.cpp:136] Iteration 55800, lr = 0.00128125, m = 0.9
I0801 13:45:04.794059  3162 solver.cpp:353] Iteration 55900 (62.4065 iter/s, 1.6024s/100 iter), loss = 0.00147684
I0801 13:45:04.794431  3162 solver.cpp:375]     Train net output #0: loss = 0.00147698 (* 1 = 0.00147698 loss)
I0801 13:45:04.794453  3162 sgd_solver.cpp:136] Iteration 55900, lr = 0.00126562, m = 0.9
I0801 13:45:06.423166  3162 solver.cpp:550] Iteration 56000, Testing net (#0)
I0801 13:45:07.295747  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.919413
I0801 13:45:07.295766  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 13:45:07.295771  3162 solver.cpp:635]     Test net output #2: loss = 0.293835 (* 1 = 0.293835 loss)
I0801 13:45:07.295789  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.872598s
I0801 13:45:07.311409  3162 solver.cpp:353] Iteration 56000 (39.7254 iter/s, 2.51728s/100 iter), loss = 0.000102655
I0801 13:45:07.311427  3162 solver.cpp:375]     Train net output #0: loss = 0.000102793 (* 1 = 0.000102793 loss)
I0801 13:45:07.311431  3162 sgd_solver.cpp:136] Iteration 56000, lr = 0.00125, m = 0.9
I0801 13:45:08.213677  3155 data_reader.cpp:264] Starting prefetch of epoch 7
I0801 13:45:08.965605  3162 solver.cpp:353] Iteration 56100 (60.4543 iter/s, 1.65414s/100 iter), loss = 0.00221284
I0801 13:45:08.965628  3162 solver.cpp:375]     Train net output #0: loss = 0.00221297 (* 1 = 0.00221297 loss)
I0801 13:45:08.965632  3162 sgd_solver.cpp:136] Iteration 56100, lr = 0.00123438, m = 0.9
I0801 13:45:10.586272  3162 solver.cpp:353] Iteration 56200 (61.705 iter/s, 1.62061s/100 iter), loss = 0.000375064
I0801 13:45:10.586302  3162 solver.cpp:375]     Train net output #0: loss = 0.0003752 (* 1 = 0.0003752 loss)
I0801 13:45:10.586308  3162 sgd_solver.cpp:136] Iteration 56200, lr = 0.00121875, m = 0.9
I0801 13:45:12.174783  3162 solver.cpp:353] Iteration 56300 (62.9544 iter/s, 1.58845s/100 iter), loss = 0.000410269
I0801 13:45:12.174856  3162 solver.cpp:375]     Train net output #0: loss = 0.000410405 (* 1 = 0.000410405 loss)
I0801 13:45:12.174873  3162 sgd_solver.cpp:136] Iteration 56300, lr = 0.00120313, m = 0.9
I0801 13:45:13.841225  3162 solver.cpp:353] Iteration 56400 (60.0101 iter/s, 1.66639s/100 iter), loss = 0.00145035
I0801 13:45:13.841406  3162 solver.cpp:375]     Train net output #0: loss = 0.00145048 (* 1 = 0.00145048 loss)
I0801 13:45:13.841521  3162 sgd_solver.cpp:136] Iteration 56400, lr = 0.0011875, m = 0.9
I0801 13:45:15.520261  3162 solver.cpp:353] Iteration 56500 (59.5596 iter/s, 1.67899s/100 iter), loss = 0.00101778
I0801 13:45:15.520308  3162 solver.cpp:375]     Train net output #0: loss = 0.00101791 (* 1 = 0.00101791 loss)
I0801 13:45:15.520321  3162 sgd_solver.cpp:136] Iteration 56500, lr = 0.00117187, m = 0.9
I0801 13:45:17.164057  3162 solver.cpp:353] Iteration 56600 (60.8367 iter/s, 1.64375s/100 iter), loss = 0.000601969
I0801 13:45:17.164088  3162 solver.cpp:375]     Train net output #0: loss = 0.000602105 (* 1 = 0.000602105 loss)
I0801 13:45:17.164093  3162 sgd_solver.cpp:136] Iteration 56600, lr = 0.00115625, m = 0.9
I0801 13:45:18.760139  3162 solver.cpp:353] Iteration 56700 (62.6558 iter/s, 1.59602s/100 iter), loss = 0.00110084
I0801 13:45:18.760226  3162 solver.cpp:375]     Train net output #0: loss = 0.00110097 (* 1 = 0.00110097 loss)
I0801 13:45:18.760251  3162 sgd_solver.cpp:136] Iteration 56700, lr = 0.00114062, m = 0.9
I0801 13:45:20.463424  3162 solver.cpp:353] Iteration 56800 (58.7117 iter/s, 1.70324s/100 iter), loss = 0.000655694
I0801 13:45:20.463452  3162 solver.cpp:375]     Train net output #0: loss = 0.00065583 (* 1 = 0.00065583 loss)
I0801 13:45:20.463457  3162 sgd_solver.cpp:136] Iteration 56800, lr = 0.001125, m = 0.9
I0801 13:45:22.149739  3162 solver.cpp:353] Iteration 56900 (59.3033 iter/s, 1.68625s/100 iter), loss = 0.000568809
I0801 13:45:22.149818  3162 solver.cpp:375]     Train net output #0: loss = 0.000568945 (* 1 = 0.000568945 loss)
I0801 13:45:22.149840  3162 sgd_solver.cpp:136] Iteration 56900, lr = 0.00110937, m = 0.9
I0801 13:45:23.759310  3162 solver.cpp:550] Iteration 57000, Testing net (#0)
I0801 13:45:24.736634  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.91853
I0801 13:45:24.736655  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997353
I0801 13:45:24.736680  3162 solver.cpp:635]     Test net output #2: loss = 0.304993 (* 1 = 0.304993 loss)
I0801 13:45:24.736696  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.97737s
I0801 13:45:24.752930  3162 solver.cpp:353] Iteration 57000 (38.4154 iter/s, 2.60313s/100 iter), loss = 0.00304667
I0801 13:45:24.752949  3162 solver.cpp:375]     Train net output #0: loss = 0.00304681 (* 1 = 0.00304681 loss)
I0801 13:45:24.752954  3162 sgd_solver.cpp:136] Iteration 57000, lr = 0.00109375, m = 0.9
I0801 13:45:26.336927  3162 solver.cpp:353] Iteration 57100 (63.1335 iter/s, 1.58395s/100 iter), loss = 0.00244975
I0801 13:45:26.336952  3162 solver.cpp:375]     Train net output #0: loss = 0.00244989 (* 1 = 0.00244989 loss)
I0801 13:45:26.336957  3162 sgd_solver.cpp:136] Iteration 57100, lr = 0.00107813, m = 0.9
I0801 13:45:28.054162  3162 solver.cpp:353] Iteration 57200 (58.2349 iter/s, 1.71718s/100 iter), loss = 0.00152488
I0801 13:45:28.054186  3162 solver.cpp:375]     Train net output #0: loss = 0.00152501 (* 1 = 0.00152501 loss)
I0801 13:45:28.054193  3162 sgd_solver.cpp:136] Iteration 57200, lr = 0.0010625, m = 0.9
I0801 13:45:29.770164  3162 solver.cpp:353] Iteration 57300 (58.2768 iter/s, 1.71595s/100 iter), loss = 0.00158709
I0801 13:45:29.770193  3162 solver.cpp:375]     Train net output #0: loss = 0.00158723 (* 1 = 0.00158723 loss)
I0801 13:45:29.770200  3162 sgd_solver.cpp:136] Iteration 57300, lr = 0.00104688, m = 0.9
I0801 13:45:31.414824  3162 solver.cpp:353] Iteration 57400 (60.8047 iter/s, 1.64461s/100 iter), loss = 0.00106836
I0801 13:45:31.414849  3162 solver.cpp:375]     Train net output #0: loss = 0.0010685 (* 1 = 0.0010685 loss)
I0801 13:45:31.414855  3162 sgd_solver.cpp:136] Iteration 57400, lr = 0.00103125, m = 0.9
I0801 13:45:33.045073  3162 solver.cpp:353] Iteration 57500 (61.3423 iter/s, 1.6302s/100 iter), loss = 0.000887686
I0801 13:45:33.045099  3162 solver.cpp:375]     Train net output #0: loss = 0.000887821 (* 1 = 0.000887821 loss)
I0801 13:45:33.045104  3162 sgd_solver.cpp:136] Iteration 57500, lr = 0.00101562, m = 0.9
I0801 13:45:34.608155  3162 solver.cpp:353] Iteration 57600 (63.9783 iter/s, 1.56303s/100 iter), loss = 0.00187435
I0801 13:45:34.608183  3162 solver.cpp:375]     Train net output #0: loss = 0.00187448 (* 1 = 0.00187448 loss)
I0801 13:45:34.608191  3162 sgd_solver.cpp:136] Iteration 57600, lr = 0.001, m = 0.9
I0801 13:45:36.311223  3162 solver.cpp:353] Iteration 57700 (58.7194 iter/s, 1.70302s/100 iter), loss = 0.000185922
I0801 13:45:36.311269  3162 solver.cpp:375]     Train net output #0: loss = 0.000186058 (* 1 = 0.000186058 loss)
I0801 13:45:36.311275  3162 sgd_solver.cpp:136] Iteration 57700, lr = 0.000984375, m = 0.9
I0801 13:45:37.978587  3162 solver.cpp:353] Iteration 57800 (59.9767 iter/s, 1.66731s/100 iter), loss = 0.000851888
I0801 13:45:37.978652  3162 solver.cpp:375]     Train net output #0: loss = 0.000852025 (* 1 = 0.000852025 loss)
I0801 13:45:37.978678  3162 sgd_solver.cpp:136] Iteration 57800, lr = 0.00096875, m = 0.9
I0801 13:45:39.602505  3162 solver.cpp:353] Iteration 57900 (61.5813 iter/s, 1.62387s/100 iter), loss = 0.00104307
I0801 13:45:39.602558  3162 solver.cpp:375]     Train net output #0: loss = 0.00104321 (* 1 = 0.00104321 loss)
I0801 13:45:39.602573  3162 sgd_solver.cpp:136] Iteration 57900, lr = 0.000953125, m = 0.9
I0801 13:45:41.174137  3162 solver.cpp:550] Iteration 58000, Testing net (#0)
I0801 13:45:42.032374  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.908236
I0801 13:45:42.032393  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 13:45:42.032398  3162 solver.cpp:635]     Test net output #2: loss = 0.339138 (* 1 = 0.339138 loss)
I0801 13:45:42.032415  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.858263s
I0801 13:45:42.048105  3162 solver.cpp:353] Iteration 58000 (40.891 iter/s, 2.44553s/100 iter), loss = 0.00116856
I0801 13:45:42.048123  3162 solver.cpp:375]     Train net output #0: loss = 0.0011687 (* 1 = 0.0011687 loss)
I0801 13:45:42.048130  3162 sgd_solver.cpp:136] Iteration 58000, lr = 0.0009375, m = 0.9
I0801 13:45:43.688973  3162 solver.cpp:353] Iteration 58100 (60.9452 iter/s, 1.64082s/100 iter), loss = 0.00549615
I0801 13:45:43.688998  3162 solver.cpp:375]     Train net output #0: loss = 0.00549628 (* 1 = 0.00549628 loss)
I0801 13:45:43.689004  3162 sgd_solver.cpp:136] Iteration 58100, lr = 0.000921875, m = 0.9
I0801 13:45:45.364521  3162 solver.cpp:353] Iteration 58200 (59.6841 iter/s, 1.67549s/100 iter), loss = 0.00182245
I0801 13:45:45.364614  3162 solver.cpp:375]     Train net output #0: loss = 0.00182259 (* 1 = 0.00182259 loss)
I0801 13:45:45.364644  3162 sgd_solver.cpp:136] Iteration 58200, lr = 0.00090625, m = 0.9
I0801 13:45:47.048308  3162 solver.cpp:353] Iteration 58300 (59.3918 iter/s, 1.68373s/100 iter), loss = 0.000983475
I0801 13:45:47.048549  3162 solver.cpp:375]     Train net output #0: loss = 0.000983613 (* 1 = 0.000983613 loss)
I0801 13:45:47.048907  3162 sgd_solver.cpp:136] Iteration 58300, lr = 0.000890625, m = 0.9
I0801 13:45:48.637640  3162 solver.cpp:353] Iteration 58400 (62.9214 iter/s, 1.58928s/100 iter), loss = 0.000537124
I0801 13:45:48.637670  3162 solver.cpp:375]     Train net output #0: loss = 0.000537262 (* 1 = 0.000537262 loss)
I0801 13:45:48.637676  3162 sgd_solver.cpp:136] Iteration 58400, lr = 0.000875, m = 0.9
I0801 13:45:50.286171  3162 solver.cpp:353] Iteration 58500 (60.662 iter/s, 1.64848s/100 iter), loss = 0.00204948
I0801 13:45:50.286200  3162 solver.cpp:375]     Train net output #0: loss = 0.00204962 (* 1 = 0.00204962 loss)
I0801 13:45:50.286206  3162 sgd_solver.cpp:136] Iteration 58500, lr = 0.000859375, m = 0.9
I0801 13:45:51.982018  3162 solver.cpp:353] Iteration 58600 (58.9698 iter/s, 1.69578s/100 iter), loss = 0.000231225
I0801 13:45:51.982087  3162 solver.cpp:375]     Train net output #0: loss = 0.000231362 (* 1 = 0.000231362 loss)
I0801 13:45:51.982106  3162 sgd_solver.cpp:136] Iteration 58600, lr = 0.00084375, m = 0.9
I0801 13:45:53.676705  3162 solver.cpp:353] Iteration 58700 (59.0096 iter/s, 1.69464s/100 iter), loss = 0.000909585
I0801 13:45:53.676754  3162 solver.cpp:375]     Train net output #0: loss = 0.000909722 (* 1 = 0.000909722 loss)
I0801 13:45:53.676767  3162 sgd_solver.cpp:136] Iteration 58700, lr = 0.000828125, m = 0.9
I0801 13:45:55.394945  3162 solver.cpp:353] Iteration 58800 (58.201 iter/s, 1.71818s/100 iter), loss = 0.00287022
I0801 13:45:55.395040  3162 solver.cpp:375]     Train net output #0: loss = 0.00287036 (* 1 = 0.00287036 loss)
I0801 13:45:55.395063  3162 sgd_solver.cpp:136] Iteration 58800, lr = 0.0008125, m = 0.9
I0801 13:45:57.009697  3162 solver.cpp:353] Iteration 58900 (61.9309 iter/s, 1.6147s/100 iter), loss = 0.00160161
I0801 13:45:57.009724  3162 solver.cpp:375]     Train net output #0: loss = 0.00160175 (* 1 = 0.00160175 loss)
I0801 13:45:57.009730  3162 sgd_solver.cpp:136] Iteration 58900, lr = 0.000796875, m = 0.9
I0801 13:45:58.552067  3162 solver.cpp:550] Iteration 59000, Testing net (#0)
I0801 13:45:59.386802  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.907354
I0801 13:45:59.386822  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997353
I0801 13:45:59.386828  3162 solver.cpp:635]     Test net output #2: loss = 0.338581 (* 1 = 0.338581 loss)
I0801 13:45:59.386847  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.834758s
I0801 13:45:59.402396  3162 solver.cpp:353] Iteration 59000 (41.795 iter/s, 2.39263s/100 iter), loss = 0.000933278
I0801 13:45:59.402415  3162 solver.cpp:375]     Train net output #0: loss = 0.000933416 (* 1 = 0.000933416 loss)
I0801 13:45:59.402420  3162 sgd_solver.cpp:136] Iteration 59000, lr = 0.00078125, m = 0.9
I0801 13:46:00.976445  3162 solver.cpp:353] Iteration 59100 (63.5325 iter/s, 1.574s/100 iter), loss = 0.000874505
I0801 13:46:00.976471  3162 solver.cpp:375]     Train net output #0: loss = 0.000874642 (* 1 = 0.000874642 loss)
I0801 13:46:00.976477  3162 sgd_solver.cpp:136] Iteration 59100, lr = 0.000765625, m = 0.9
I0801 13:46:02.593515  3162 solver.cpp:353] Iteration 59200 (61.8421 iter/s, 1.61702s/100 iter), loss = 0.00275622
I0801 13:46:02.593542  3162 solver.cpp:375]     Train net output #0: loss = 0.00275635 (* 1 = 0.00275635 loss)
I0801 13:46:02.593549  3162 sgd_solver.cpp:136] Iteration 59200, lr = 0.00075, m = 0.9
I0801 13:46:04.330885  3162 solver.cpp:353] Iteration 59300 (57.5601 iter/s, 1.73732s/100 iter), loss = 0.000464212
I0801 13:46:04.330965  3162 solver.cpp:375]     Train net output #0: loss = 0.000464348 (* 1 = 0.000464348 loss)
I0801 13:46:04.330988  3162 sgd_solver.cpp:136] Iteration 59300, lr = 0.000734375, m = 0.9
I0801 13:46:05.995328  3162 solver.cpp:353] Iteration 59400 (60.082 iter/s, 1.66439s/100 iter), loss = 0.00178427
I0801 13:46:05.995358  3162 solver.cpp:375]     Train net output #0: loss = 0.0017844 (* 1 = 0.0017844 loss)
I0801 13:46:05.995363  3162 sgd_solver.cpp:136] Iteration 59400, lr = 0.00071875, m = 0.9
I0801 13:46:07.582315  3162 solver.cpp:353] Iteration 59500 (63.0144 iter/s, 1.58694s/100 iter), loss = 0.000767793
I0801 13:46:07.582412  3162 solver.cpp:375]     Train net output #0: loss = 0.000767929 (* 1 = 0.000767929 loss)
I0801 13:46:07.582424  3162 sgd_solver.cpp:136] Iteration 59500, lr = 0.000703125, m = 0.9
I0801 13:46:09.154914  3162 solver.cpp:353] Iteration 59600 (63.5911 iter/s, 1.57255s/100 iter), loss = 0.0024679
I0801 13:46:09.154976  3162 solver.cpp:375]     Train net output #0: loss = 0.00246804 (* 1 = 0.00246804 loss)
I0801 13:46:09.154994  3162 sgd_solver.cpp:136] Iteration 59600, lr = 0.0006875, m = 0.9
I0801 13:46:10.723644  3162 solver.cpp:353] Iteration 59700 (63.7478 iter/s, 1.56868s/100 iter), loss = 0.00257634
I0801 13:46:10.723671  3162 solver.cpp:375]     Train net output #0: loss = 0.00257647 (* 1 = 0.00257647 loss)
I0801 13:46:10.723677  3162 sgd_solver.cpp:136] Iteration 59700, lr = 0.000671875, m = 0.9
I0801 13:46:12.304626  3162 solver.cpp:353] Iteration 59800 (63.2538 iter/s, 1.58093s/100 iter), loss = 0.0010244
I0801 13:46:12.304679  3162 solver.cpp:375]     Train net output #0: loss = 0.00102454 (* 1 = 0.00102454 loss)
I0801 13:46:12.304694  3162 sgd_solver.cpp:136] Iteration 59800, lr = 0.00065625, m = 0.9
I0801 13:46:13.868911  3162 solver.cpp:353] Iteration 59900 (63.929 iter/s, 1.56424s/100 iter), loss = 0.000952152
I0801 13:46:13.868937  3162 solver.cpp:375]     Train net output #0: loss = 0.000952286 (* 1 = 0.000952286 loss)
I0801 13:46:13.868942  3162 sgd_solver.cpp:136] Iteration 59900, lr = 0.000640625, m = 0.9
I0801 13:46:15.506772  3162 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/cifar10_jacintonet11v2_iter_60000.caffemodel
I0801 13:46:15.514756  3162 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/cifar10_jacintonet11v2_iter_60000.solverstate
I0801 13:46:15.518360  3162 solver.cpp:550] Iteration 60000, Testing net (#0)
I0801 13:46:16.342375  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.905589
I0801 13:46:16.342396  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 13:46:16.342401  3162 solver.cpp:635]     Test net output #2: loss = 0.354346 (* 1 = 0.354346 loss)
I0801 13:46:16.342417  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.824032s
I0801 13:46:16.358114  3162 solver.cpp:353] Iteration 60000 (40.1747 iter/s, 2.48913s/100 iter), loss = 0.00204092
I0801 13:46:16.358130  3162 solver.cpp:375]     Train net output #0: loss = 0.00204105 (* 1 = 0.00204105 loss)
I0801 13:46:16.358134  3162 sgd_solver.cpp:136] Iteration 60000, lr = 0.000625, m = 0.9
I0801 13:46:18.068044  3162 solver.cpp:353] Iteration 60100 (58.4837 iter/s, 1.70988s/100 iter), loss = 0.000826949
I0801 13:46:18.068069  3162 solver.cpp:375]     Train net output #0: loss = 0.000827083 (* 1 = 0.000827083 loss)
I0801 13:46:18.068073  3162 sgd_solver.cpp:136] Iteration 60100, lr = 0.000609375, m = 0.9
I0801 13:46:19.704610  3162 solver.cpp:353] Iteration 60200 (61.1055 iter/s, 1.63651s/100 iter), loss = 0.00106044
I0801 13:46:19.704638  3162 solver.cpp:375]     Train net output #0: loss = 0.00106058 (* 1 = 0.00106058 loss)
I0801 13:46:19.704645  3162 sgd_solver.cpp:136] Iteration 60200, lr = 0.00059375, m = 0.9
I0801 13:46:21.354585  3162 solver.cpp:353] Iteration 60300 (60.6089 iter/s, 1.64992s/100 iter), loss = 0.000670495
I0801 13:46:21.354609  3162 solver.cpp:375]     Train net output #0: loss = 0.000670628 (* 1 = 0.000670628 loss)
I0801 13:46:21.354614  3162 sgd_solver.cpp:136] Iteration 60300, lr = 0.000578125, m = 0.9
I0801 13:46:22.947965  3162 solver.cpp:353] Iteration 60400 (62.7617 iter/s, 1.59333s/100 iter), loss = 0.000904247
I0801 13:46:22.947995  3162 solver.cpp:375]     Train net output #0: loss = 0.00090438 (* 1 = 0.00090438 loss)
I0801 13:46:22.948001  3162 sgd_solver.cpp:136] Iteration 60400, lr = 0.0005625, m = 0.9
I0801 13:46:24.582021  3162 solver.cpp:353] Iteration 60500 (61.1992 iter/s, 1.63401s/100 iter), loss = 0.00173767
I0801 13:46:24.582070  3162 solver.cpp:375]     Train net output #0: loss = 0.0017378 (* 1 = 0.0017378 loss)
I0801 13:46:24.582098  3162 sgd_solver.cpp:136] Iteration 60500, lr = 0.000546875, m = 0.9
I0801 13:46:26.278993  3162 solver.cpp:353] Iteration 60600 (58.9307 iter/s, 1.69691s/100 iter), loss = 0.00203452
I0801 13:46:26.279062  3162 solver.cpp:375]     Train net output #0: loss = 0.00203466 (* 1 = 0.00203466 loss)
I0801 13:46:26.279083  3162 sgd_solver.cpp:136] Iteration 60600, lr = 0.00053125, m = 0.9
I0801 13:46:26.326967  3155 data_reader.cpp:264] Starting prefetch of epoch 8
I0801 13:46:27.859788  3162 solver.cpp:353] Iteration 60700 (63.2613 iter/s, 1.58075s/100 iter), loss = 0.00102988
I0801 13:46:27.859840  3162 solver.cpp:375]     Train net output #0: loss = 0.00103001 (* 1 = 0.00103001 loss)
I0801 13:46:27.859851  3162 sgd_solver.cpp:136] Iteration 60700, lr = 0.000515625, m = 0.9
I0801 13:46:29.481102  3162 solver.cpp:353] Iteration 60800 (61.6802 iter/s, 1.62127s/100 iter), loss = 0.00137992
I0801 13:46:29.481127  3162 solver.cpp:375]     Train net output #0: loss = 0.00138005 (* 1 = 0.00138005 loss)
I0801 13:46:29.481132  3162 sgd_solver.cpp:136] Iteration 60800, lr = 0.0005, m = 0.9
I0801 13:46:31.118185  3162 solver.cpp:353] Iteration 60900 (61.0861 iter/s, 1.63703s/100 iter), loss = 0.000350291
I0801 13:46:31.118238  3162 solver.cpp:375]     Train net output #0: loss = 0.000350424 (* 1 = 0.000350424 loss)
I0801 13:46:31.118253  3162 sgd_solver.cpp:136] Iteration 60900, lr = 0.000484375, m = 0.9
I0801 13:46:32.707386  3162 solver.cpp:550] Iteration 61000, Testing net (#0)
I0801 13:46:33.586587  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.90353
I0801 13:46:33.586607  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 13:46:33.586612  3162 solver.cpp:635]     Test net output #2: loss = 0.35269 (* 1 = 0.35269 loss)
I0801 13:46:33.586627  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.879216s
I0801 13:46:33.602342  3162 solver.cpp:353] Iteration 61000 (40.2563 iter/s, 2.48408s/100 iter), loss = 0.0020512
I0801 13:46:33.602360  3162 solver.cpp:375]     Train net output #0: loss = 0.00205133 (* 1 = 0.00205133 loss)
I0801 13:46:33.602365  3162 sgd_solver.cpp:136] Iteration 61000, lr = 0.00046875, m = 0.9
I0801 13:46:35.217396  3162 solver.cpp:353] Iteration 61100 (61.9196 iter/s, 1.615s/100 iter), loss = 0.000705706
I0801 13:46:35.217483  3162 solver.cpp:375]     Train net output #0: loss = 0.00070584 (* 1 = 0.00070584 loss)
I0801 13:46:35.217506  3162 sgd_solver.cpp:136] Iteration 61100, lr = 0.000453125, m = 0.9
I0801 13:46:36.889499  3162 solver.cpp:353] Iteration 61200 (59.8067 iter/s, 1.67205s/100 iter), loss = 0.00171059
I0801 13:46:36.889524  3162 solver.cpp:375]     Train net output #0: loss = 0.00171073 (* 1 = 0.00171073 loss)
I0801 13:46:36.889528  3162 sgd_solver.cpp:136] Iteration 61200, lr = 0.0004375, m = 0.9
I0801 13:46:38.569823  3162 solver.cpp:353] Iteration 61300 (59.5143 iter/s, 1.68027s/100 iter), loss = 0.00106814
I0801 13:46:38.569948  3162 solver.cpp:375]     Train net output #0: loss = 0.00106828 (* 1 = 0.00106828 loss)
I0801 13:46:38.569959  3162 sgd_solver.cpp:136] Iteration 61300, lr = 0.000421875, m = 0.9
I0801 13:46:40.226923  3162 solver.cpp:353] Iteration 61400 (60.3484 iter/s, 1.65705s/100 iter), loss = 0.00115873
I0801 13:46:40.226968  3162 solver.cpp:375]     Train net output #0: loss = 0.00115887 (* 1 = 0.00115887 loss)
I0801 13:46:40.226987  3162 sgd_solver.cpp:136] Iteration 61400, lr = 0.00040625, m = 0.9
I0801 13:46:41.934173  3162 solver.cpp:353] Iteration 61500 (58.5756 iter/s, 1.7072s/100 iter), loss = 0.00132242
I0801 13:46:41.934207  3162 solver.cpp:375]     Train net output #0: loss = 0.00132255 (* 1 = 0.00132255 loss)
I0801 13:46:41.934214  3162 sgd_solver.cpp:136] Iteration 61500, lr = 0.000390625, m = 0.9
I0801 13:46:43.534689  3162 solver.cpp:353] Iteration 61600 (62.4817 iter/s, 1.60047s/100 iter), loss = 0.00144664
I0801 13:46:43.534739  3162 solver.cpp:375]     Train net output #0: loss = 0.00144677 (* 1 = 0.00144677 loss)
I0801 13:46:43.534754  3162 sgd_solver.cpp:136] Iteration 61600, lr = 0.000375, m = 0.9
I0801 13:46:45.188684  3162 solver.cpp:353] Iteration 61700 (60.4616 iter/s, 1.65394s/100 iter), loss = 0.000623951
I0801 13:46:45.188742  3162 solver.cpp:375]     Train net output #0: loss = 0.000624085 (* 1 = 0.000624085 loss)
I0801 13:46:45.188758  3162 sgd_solver.cpp:136] Iteration 61700, lr = 0.000359375, m = 0.9
I0801 13:46:46.812023  3162 solver.cpp:353] Iteration 61800 (61.6038 iter/s, 1.62328s/100 iter), loss = 0.00077011
I0801 13:46:46.812119  3162 solver.cpp:375]     Train net output #0: loss = 0.000770244 (* 1 = 0.000770244 loss)
I0801 13:46:46.812146  3162 sgd_solver.cpp:136] Iteration 61800, lr = 0.00034375, m = 0.9
I0801 13:46:48.430866  3162 solver.cpp:353] Iteration 61900 (61.7741 iter/s, 1.6188s/100 iter), loss = 0.00180879
I0801 13:46:48.430891  3162 solver.cpp:375]     Train net output #0: loss = 0.00180893 (* 1 = 0.00180893 loss)
I0801 13:46:48.430894  3162 sgd_solver.cpp:136] Iteration 61900, lr = 0.000328125, m = 0.9
I0801 13:46:50.117517  3162 solver.cpp:550] Iteration 62000, Testing net (#0)
I0801 13:46:50.985618  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.903236
I0801 13:46:50.985641  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 13:46:50.985646  3162 solver.cpp:635]     Test net output #2: loss = 0.366849 (* 1 = 0.366849 loss)
I0801 13:46:50.985662  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.868124s
I0801 13:46:51.001480  3162 solver.cpp:353] Iteration 62000 (38.9024 iter/s, 2.57054s/100 iter), loss = 0.000507733
I0801 13:46:51.001507  3162 solver.cpp:375]     Train net output #0: loss = 0.000507867 (* 1 = 0.000507867 loss)
I0801 13:46:51.001513  3162 sgd_solver.cpp:136] Iteration 62000, lr = 0.0003125, m = 0.9
I0801 13:46:52.584156  3162 solver.cpp:353] Iteration 62100 (63.1861 iter/s, 1.58263s/100 iter), loss = 0.000505252
I0801 13:46:52.584183  3162 solver.cpp:375]     Train net output #0: loss = 0.000505384 (* 1 = 0.000505384 loss)
I0801 13:46:52.584189  3162 sgd_solver.cpp:136] Iteration 62100, lr = 0.000296875, m = 0.9
I0801 13:46:54.307184  3162 solver.cpp:353] Iteration 62200 (58.0395 iter/s, 1.72297s/100 iter), loss = 0.00120934
I0801 13:46:54.307274  3162 solver.cpp:375]     Train net output #0: loss = 0.00120947 (* 1 = 0.00120947 loss)
I0801 13:46:54.307304  3162 sgd_solver.cpp:136] Iteration 62200, lr = 0.00028125, m = 0.9
I0801 13:46:56.010308  3162 solver.cpp:353] Iteration 62300 (58.7176 iter/s, 1.70307s/100 iter), loss = 0.00107623
I0801 13:46:56.010417  3162 solver.cpp:375]     Train net output #0: loss = 0.00107636 (* 1 = 0.00107636 loss)
I0801 13:46:56.010447  3162 sgd_solver.cpp:136] Iteration 62300, lr = 0.000265625, m = 0.9
I0801 13:46:57.636804  3162 solver.cpp:353] Iteration 62400 (61.4837 iter/s, 1.62645s/100 iter), loss = 0.000746316
I0801 13:46:57.636835  3162 solver.cpp:375]     Train net output #0: loss = 0.000746447 (* 1 = 0.000746447 loss)
I0801 13:46:57.636842  3162 sgd_solver.cpp:136] Iteration 62400, lr = 0.00025, m = 0.9
I0801 13:46:59.278522  3162 solver.cpp:353] Iteration 62500 (60.9137 iter/s, 1.64167s/100 iter), loss = 0.000474796
I0801 13:46:59.278547  3162 solver.cpp:375]     Train net output #0: loss = 0.000474927 (* 1 = 0.000474927 loss)
I0801 13:46:59.278550  3162 sgd_solver.cpp:136] Iteration 62500, lr = 0.000234375, m = 0.9
I0801 13:47:01.085181  3162 solver.cpp:353] Iteration 62600 (55.3525 iter/s, 1.8066s/100 iter), loss = 0.000650956
I0801 13:47:01.085207  3162 solver.cpp:375]     Train net output #0: loss = 0.000651087 (* 1 = 0.000651087 loss)
I0801 13:47:01.085213  3162 sgd_solver.cpp:136] Iteration 62600, lr = 0.00021875, m = 0.9
I0801 13:47:02.744630  3162 solver.cpp:353] Iteration 62700 (60.2627 iter/s, 1.6594s/100 iter), loss = 0.00149603
I0801 13:47:02.744657  3162 solver.cpp:375]     Train net output #0: loss = 0.00149616 (* 1 = 0.00149616 loss)
I0801 13:47:02.744664  3162 sgd_solver.cpp:136] Iteration 62700, lr = 0.000203125, m = 0.9
I0801 13:47:04.422838  3162 solver.cpp:353] Iteration 62800 (59.5893 iter/s, 1.67815s/100 iter), loss = 0.00229836
I0801 13:47:04.422889  3162 solver.cpp:375]     Train net output #0: loss = 0.00229849 (* 1 = 0.00229849 loss)
I0801 13:47:04.422904  3162 sgd_solver.cpp:136] Iteration 62800, lr = 0.0001875, m = 0.9
I0801 13:47:06.099673  3162 solver.cpp:353] Iteration 62900 (59.6381 iter/s, 1.67678s/100 iter), loss = 0.00161587
I0801 13:47:06.099699  3162 solver.cpp:375]     Train net output #0: loss = 0.001616 (* 1 = 0.001616 loss)
I0801 13:47:06.099704  3162 sgd_solver.cpp:136] Iteration 62900, lr = 0.000171875, m = 0.9
I0801 13:47:07.668948  3162 solver.cpp:550] Iteration 63000, Testing net (#0)
I0801 13:47:08.502861  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.90706
I0801 13:47:08.502881  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 13:47:08.502887  3162 solver.cpp:635]     Test net output #2: loss = 0.358423 (* 1 = 0.358423 loss)
I0801 13:47:08.502904  3162 solver.cpp:305] [MultiGPU] Tests completed in 0.833932s
I0801 13:47:08.521960  3162 solver.cpp:353] Iteration 63000 (41.2845 iter/s, 2.42222s/100 iter), loss = 0.000912424
I0801 13:47:08.521978  3162 solver.cpp:375]     Train net output #0: loss = 0.000912556 (* 1 = 0.000912556 loss)
I0801 13:47:08.521983  3162 sgd_solver.cpp:136] Iteration 63000, lr = 0.00015625, m = 0.9
I0801 13:47:10.106366  3162 solver.cpp:353] Iteration 63100 (63.1171 iter/s, 1.58436s/100 iter), loss = 0.00114126
I0801 13:47:10.106429  3162 solver.cpp:375]     Train net output #0: loss = 0.00114139 (* 1 = 0.00114139 loss)
I0801 13:47:10.106439  3162 sgd_solver.cpp:136] Iteration 63100, lr = 0.000140625, m = 0.9
I0801 13:47:11.701206  3162 solver.cpp:353] Iteration 63200 (62.7042 iter/s, 1.59479s/100 iter), loss = 0.00169459
I0801 13:47:11.701231  3162 solver.cpp:375]     Train net output #0: loss = 0.00169472 (* 1 = 0.00169472 loss)
I0801 13:47:11.701237  3162 sgd_solver.cpp:136] Iteration 63200, lr = 0.000125, m = 0.9
I0801 13:47:13.289443  3162 solver.cpp:353] Iteration 63300 (62.9649 iter/s, 1.58819s/100 iter), loss = 0.00183363
I0801 13:47:13.289469  3162 solver.cpp:375]     Train net output #0: loss = 0.00183376 (* 1 = 0.00183376 loss)
I0801 13:47:13.289472  3162 sgd_solver.cpp:136] Iteration 63300, lr = 0.000109375, m = 0.9
I0801 13:47:14.932013  3162 solver.cpp:353] Iteration 63400 (60.882 iter/s, 1.64252s/100 iter), loss = 0.000551688
I0801 13:47:14.932065  3162 solver.cpp:375]     Train net output #0: loss = 0.000551818 (* 1 = 0.000551818 loss)
I0801 13:47:14.932080  3162 sgd_solver.cpp:136] Iteration 63400, lr = 9.37498e-05, m = 0.9
I0801 13:47:16.617338  3162 solver.cpp:353] Iteration 63500 (59.3376 iter/s, 1.68527s/100 iter), loss = 0.00110375
I0801 13:47:16.617388  3162 solver.cpp:375]     Train net output #0: loss = 0.00110388 (* 1 = 0.00110388 loss)
I0801 13:47:16.617403  3162 sgd_solver.cpp:136] Iteration 63500, lr = 7.8125e-05, m = 0.9
I0801 13:47:18.196970  3162 solver.cpp:353] Iteration 63600 (63.3078 iter/s, 1.57958s/100 iter), loss = 0.000320617
I0801 13:47:18.196995  3162 solver.cpp:375]     Train net output #0: loss = 0.000320749 (* 1 = 0.000320749 loss)
I0801 13:47:18.197001  3162 sgd_solver.cpp:136] Iteration 63600, lr = 6.25002e-05, m = 0.9
I0801 13:47:19.762063  3162 solver.cpp:353] Iteration 63700 (63.896 iter/s, 1.56504s/100 iter), loss = 0.00137995
I0801 13:47:19.762130  3162 solver.cpp:375]     Train net output #0: loss = 0.00138008 (* 1 = 0.00138008 loss)
I0801 13:47:19.762151  3162 sgd_solver.cpp:136] Iteration 63700, lr = 4.68749e-05, m = 0.9
I0801 13:47:21.324304  3162 solver.cpp:353] Iteration 63800 (64.0127 iter/s, 1.56219s/100 iter), loss = 0.00173982
I0801 13:47:21.324332  3162 solver.cpp:375]     Train net output #0: loss = 0.00173995 (* 1 = 0.00173995 loss)
I0801 13:47:21.324338  3162 sgd_solver.cpp:136] Iteration 63800, lr = 3.12501e-05, m = 0.9
I0801 13:47:22.892196  3162 solver.cpp:353] Iteration 63900 (63.7818 iter/s, 1.56784s/100 iter), loss = 0.000415953
I0801 13:47:22.892220  3162 solver.cpp:375]     Train net output #0: loss = 0.000416084 (* 1 = 0.000416084 loss)
I0801 13:47:22.892225  3162 sgd_solver.cpp:136] Iteration 63900, lr = 1.56248e-05, m = 0.9
I0801 13:47:24.514693  3162 solver.cpp:353] Iteration 63999 (61.0191 iter/s, 1.62244s/99 iter), loss = 0.000428463
I0801 13:47:24.514716  3162 solver.cpp:375]     Train net output #0: loss = 0.000428594 (* 1 = 0.000428594 loss)
I0801 13:47:24.514843  3162 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/cifar10_jacintonet11v2_iter_64000.caffemodel
I0801 13:47:24.522917  3162 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/cifar10_jacintonet11v2_iter_64000.solverstate
I0801 13:47:24.531424  3162 solver.cpp:527] Iteration 64000, loss = 0.000431558
I0801 13:47:24.531445  3162 solver.cpp:550] Iteration 64000, Testing net (#0)
I0801 13:47:25.355577  3162 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.90853
I0801 13:47:25.355597  3162 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995294
I0801 13:47:25.355602  3162 solver.cpp:635]     Test net output #2: loss = 0.347261 (* 1 = 0.347261 loss)
I0801 13:47:25.358602  3122 parallel.cpp:73] Root Solver performance on device 0: 58.8 * 22 = 1294 img/sec (64000 itr in 1088 sec)
I0801 13:47:25.358614  3122 parallel.cpp:78]      Solver performance on device 1: 58.8 * 22 = 1294 img/sec (64000 itr in 1088 sec)
I0801 13:47:25.358618  3122 parallel.cpp:78]      Solver performance on device 2: 58.8 * 22 = 1294 img/sec (64000 itr in 1088 sec)
I0801 13:47:25.358640  3122 parallel.cpp:81] Overall multi-GPU performance: 3880.74 img/sec
I0801 13:47:25.441818  3122 caffe.cpp:247] Optimization Done in 18m 12s
I0801 13:47:26.329907 24466 caffe.cpp:608] This is NVCaffe 0.16.3 started at Tue Aug  1 13:47:26 2017
I0801 13:47:26.330034 24466 caffe.cpp:611] CuDNN version: 6021
I0801 13:47:26.330039 24466 caffe.cpp:612] CuBLAS version: 8000
I0801 13:47:26.330040 24466 caffe.cpp:613] CUDA version: 8000
I0801 13:47:26.330041 24466 caffe.cpp:614] CUDA driver version: 8000
I0801 13:47:26.581715 24466 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0801 13:47:26.582283 24466 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0801 13:47:26.582803 24466 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0801 13:47:26.583317 24466 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0801 13:47:26.583325 24466 caffe.cpp:208] Using GPUs 0, 1, 2
I0801 13:47:26.583647 24466 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0801 13:47:26.583971 24466 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0801 13:47:26.584291 24466 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0801 13:47:26.584326 24466 solver.cpp:42] Solver data type: FLOAT
I0801 13:47:26.584360 24466 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: true
iter_size: 1
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.02
sparsity_step_iter: 1000
sparsity_start_iter: 4000
sparsity_start_factor: 0
I0801 13:47:26.591029 24466 solver.cpp:77] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/train.prototxt
I0801 13:47:26.591450 24466 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0801 13:47:26.591457 24466 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0801 13:47:26.591512 24466 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0801 13:47:26.591699 24466 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 22
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0801 13:47:26.591804 24466 net.cpp:104] Using FLOAT as default forward math type
I0801 13:47:26.591809 24466 net.cpp:110] Using FLOAT as default backward math type
I0801 13:47:26.591812 24466 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0801 13:47:26.591815 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.591856 24466 net.cpp:184] Created Layer data (0)
I0801 13:47:26.591861 24466 net.cpp:530] data -> data
I0801 13:47:26.591873 24466 net.cpp:530] data -> label
I0801 13:47:26.591900 24466 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0801 13:47:26.591917 24466 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:47:26.593327 24487 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0801 13:47:26.594347 24466 data_layer.cpp:184] [0] ReshapePrefetch 22, 3, 32, 32
I0801 13:47:26.594415 24466 data_layer.cpp:208] [0] Output data size: 22, 3, 32, 32
I0801 13:47:26.594421 24466 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:47:26.594442 24466 net.cpp:245] Setting up data
I0801 13:47:26.594451 24466 net.cpp:252] TRAIN Top shape for layer 0 'data' 22 3 32 32 (67584)
I0801 13:47:26.594463 24466 net.cpp:252] TRAIN Top shape for layer 0 'data' 22 (22)
I0801 13:47:26.594470 24466 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0801 13:47:26.594475 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.594487 24466 net.cpp:184] Created Layer data/bias (1)
I0801 13:47:26.594492 24466 net.cpp:561] data/bias <- data
I0801 13:47:26.594501 24466 net.cpp:530] data/bias -> data/bias
I0801 13:47:26.596505 24466 net.cpp:245] Setting up data/bias
I0801 13:47:26.596515 24466 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 22 3 32 32 (67584)
I0801 13:47:26.596525 24466 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0801 13:47:26.596529 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.596544 24466 net.cpp:184] Created Layer conv1a (2)
I0801 13:47:26.596549 24466 net.cpp:561] conv1a <- data/bias
I0801 13:47:26.596552 24466 net.cpp:530] conv1a -> conv1a
I0801 13:47:26.892632 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.15G, req 0G)
I0801 13:47:26.892652 24466 net.cpp:245] Setting up conv1a
I0801 13:47:26.892659 24466 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 22 32 32 32 (720896)
I0801 13:47:26.892670 24466 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0801 13:47:26.892675 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.892688 24466 net.cpp:184] Created Layer conv1a/bn (3)
I0801 13:47:26.892693 24466 net.cpp:561] conv1a/bn <- conv1a
I0801 13:47:26.892699 24466 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0801 13:47:26.893388 24466 net.cpp:245] Setting up conv1a/bn
I0801 13:47:26.893398 24466 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 22 32 32 32 (720896)
I0801 13:47:26.893414 24466 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0801 13:47:26.893417 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.893425 24466 net.cpp:184] Created Layer conv1a/relu (4)
I0801 13:47:26.893429 24466 net.cpp:561] conv1a/relu <- conv1a
I0801 13:47:26.893432 24466 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0801 13:47:26.893447 24466 net.cpp:245] Setting up conv1a/relu
I0801 13:47:26.893451 24466 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 22 32 32 32 (720896)
I0801 13:47:26.893456 24466 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0801 13:47:26.893460 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.893471 24466 net.cpp:184] Created Layer conv1b (5)
I0801 13:47:26.893473 24466 net.cpp:561] conv1b <- conv1a
I0801 13:47:26.893478 24466 net.cpp:530] conv1b -> conv1b
I0801 13:47:26.900804 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 1 3  (limit 8.13G, req 0G)
I0801 13:47:26.900822 24466 net.cpp:245] Setting up conv1b
I0801 13:47:26.900830 24466 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 22 32 32 32 (720896)
I0801 13:47:26.900838 24466 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0801 13:47:26.900843 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.900861 24466 net.cpp:184] Created Layer conv1b/bn (6)
I0801 13:47:26.900864 24466 net.cpp:561] conv1b/bn <- conv1b
I0801 13:47:26.900868 24466 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0801 13:47:26.901500 24466 net.cpp:245] Setting up conv1b/bn
I0801 13:47:26.901510 24466 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 22 32 32 32 (720896)
I0801 13:47:26.901518 24466 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0801 13:47:26.901522 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.901528 24466 net.cpp:184] Created Layer conv1b/relu (7)
I0801 13:47:26.901531 24466 net.cpp:561] conv1b/relu <- conv1b
I0801 13:47:26.901535 24466 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0801 13:47:26.901541 24466 net.cpp:245] Setting up conv1b/relu
I0801 13:47:26.901546 24466 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 22 32 32 32 (720896)
I0801 13:47:26.901551 24466 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0801 13:47:26.901554 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.901563 24466 net.cpp:184] Created Layer pool1 (8)
I0801 13:47:26.901566 24466 net.cpp:561] pool1 <- conv1b
I0801 13:47:26.901569 24466 net.cpp:530] pool1 -> pool1
I0801 13:47:26.901635 24466 net.cpp:245] Setting up pool1
I0801 13:47:26.901641 24466 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 22 32 32 32 (720896)
I0801 13:47:26.901645 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0801 13:47:26.901649 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.901664 24466 net.cpp:184] Created Layer res2a_branch2a (9)
I0801 13:47:26.901669 24466 net.cpp:561] res2a_branch2a <- pool1
I0801 13:47:26.901674 24466 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0801 13:47:26.913063 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0G)
I0801 13:47:26.913074 24466 net.cpp:245] Setting up res2a_branch2a
I0801 13:47:26.913079 24466 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 22 64 32 32 (1441792)
I0801 13:47:26.913085 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0801 13:47:26.913089 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.913094 24466 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0801 13:47:26.913095 24466 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0801 13:47:26.913099 24466 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0801 13:47:26.913724 24466 net.cpp:245] Setting up res2a_branch2a/bn
I0801 13:47:26.913733 24466 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 22 64 32 32 (1441792)
I0801 13:47:26.913738 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0801 13:47:26.913740 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.913744 24466 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0801 13:47:26.913746 24466 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0801 13:47:26.913748 24466 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0801 13:47:26.913751 24466 net.cpp:245] Setting up res2a_branch2a/relu
I0801 13:47:26.913754 24466 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 22 64 32 32 (1441792)
I0801 13:47:26.913756 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0801 13:47:26.913758 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.913763 24466 net.cpp:184] Created Layer res2a_branch2b (12)
I0801 13:47:26.913766 24466 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0801 13:47:26.913769 24466 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0801 13:47:26.920666 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.1G, req 0G)
I0801 13:47:26.920680 24466 net.cpp:245] Setting up res2a_branch2b
I0801 13:47:26.920686 24466 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 22 64 32 32 (1441792)
I0801 13:47:26.920691 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0801 13:47:26.920696 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.920703 24466 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0801 13:47:26.920707 24466 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0801 13:47:26.920711 24466 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0801 13:47:26.921383 24466 net.cpp:245] Setting up res2a_branch2b/bn
I0801 13:47:26.921392 24466 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 22 64 32 32 (1441792)
I0801 13:47:26.921398 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0801 13:47:26.921401 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.921406 24466 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0801 13:47:26.921408 24466 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0801 13:47:26.921411 24466 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0801 13:47:26.921414 24466 net.cpp:245] Setting up res2a_branch2b/relu
I0801 13:47:26.921418 24466 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 22 64 32 32 (1441792)
I0801 13:47:26.921421 24466 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0801 13:47:26.921422 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.921428 24466 net.cpp:184] Created Layer pool2 (15)
I0801 13:47:26.921432 24466 net.cpp:561] pool2 <- res2a_branch2b
I0801 13:47:26.921433 24466 net.cpp:530] pool2 -> pool2
I0801 13:47:26.921499 24466 net.cpp:245] Setting up pool2
I0801 13:47:26.921505 24466 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 22 64 16 16 (360448)
I0801 13:47:26.921509 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0801 13:47:26.921511 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.921519 24466 net.cpp:184] Created Layer res3a_branch2a (16)
I0801 13:47:26.921521 24466 net.cpp:561] res3a_branch2a <- pool2
I0801 13:47:26.921525 24466 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0801 13:47:26.932653 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 8.09G, req 0G)
I0801 13:47:26.932669 24466 net.cpp:245] Setting up res3a_branch2a
I0801 13:47:26.932675 24466 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 22 128 16 16 (720896)
I0801 13:47:26.932680 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0801 13:47:26.932684 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.932693 24466 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0801 13:47:26.932696 24466 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0801 13:47:26.932699 24466 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0801 13:47:26.933353 24466 net.cpp:245] Setting up res3a_branch2a/bn
I0801 13:47:26.933362 24466 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 22 128 16 16 (720896)
I0801 13:47:26.933370 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0801 13:47:26.933374 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.933378 24466 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0801 13:47:26.933382 24466 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0801 13:47:26.933384 24466 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0801 13:47:26.933388 24466 net.cpp:245] Setting up res3a_branch2a/relu
I0801 13:47:26.933400 24466 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 22 128 16 16 (720896)
I0801 13:47:26.933403 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0801 13:47:26.933406 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.933414 24466 net.cpp:184] Created Layer res3a_branch2b (19)
I0801 13:47:26.933418 24466 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0801 13:47:26.933419 24466 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0801 13:47:26.938220 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.08G, req 0G)
I0801 13:47:26.938231 24466 net.cpp:245] Setting up res3a_branch2b
I0801 13:47:26.938236 24466 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 22 128 16 16 (720896)
I0801 13:47:26.938241 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0801 13:47:26.938244 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.938249 24466 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0801 13:47:26.938252 24466 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0801 13:47:26.938256 24466 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0801 13:47:26.938843 24466 net.cpp:245] Setting up res3a_branch2b/bn
I0801 13:47:26.938849 24466 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 22 128 16 16 (720896)
I0801 13:47:26.938855 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0801 13:47:26.938858 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.938863 24466 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0801 13:47:26.938865 24466 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0801 13:47:26.938868 24466 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0801 13:47:26.938871 24466 net.cpp:245] Setting up res3a_branch2b/relu
I0801 13:47:26.938874 24466 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 22 128 16 16 (720896)
I0801 13:47:26.938876 24466 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0801 13:47:26.938879 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.938882 24466 net.cpp:184] Created Layer pool3 (22)
I0801 13:47:26.938885 24466 net.cpp:561] pool3 <- res3a_branch2b
I0801 13:47:26.938887 24466 net.cpp:530] pool3 -> pool3
I0801 13:47:26.938946 24466 net.cpp:245] Setting up pool3
I0801 13:47:26.938951 24466 net.cpp:252] TRAIN Top shape for layer 22 'pool3' 22 128 16 16 (720896)
I0801 13:47:26.938953 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0801 13:47:26.938956 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.938961 24466 net.cpp:184] Created Layer res4a_branch2a (23)
I0801 13:47:26.938964 24466 net.cpp:561] res4a_branch2a <- pool3
I0801 13:47:26.938966 24466 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0801 13:47:26.958015 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.05G, req 0G)
I0801 13:47:26.958034 24466 net.cpp:245] Setting up res4a_branch2a
I0801 13:47:26.958039 24466 net.cpp:252] TRAIN Top shape for layer 23 'res4a_branch2a' 22 256 16 16 (1441792)
I0801 13:47:26.958045 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0801 13:47:26.958050 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.958063 24466 net.cpp:184] Created Layer res4a_branch2a/bn (24)
I0801 13:47:26.958066 24466 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0801 13:47:26.958070 24466 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0801 13:47:26.958765 24466 net.cpp:245] Setting up res4a_branch2a/bn
I0801 13:47:26.958773 24466 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a/bn' 22 256 16 16 (1441792)
I0801 13:47:26.958789 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0801 13:47:26.958792 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.958796 24466 net.cpp:184] Created Layer res4a_branch2a/relu (25)
I0801 13:47:26.958798 24466 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0801 13:47:26.958801 24466 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0801 13:47:26.958804 24466 net.cpp:245] Setting up res4a_branch2a/relu
I0801 13:47:26.958808 24466 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/relu' 22 256 16 16 (1441792)
I0801 13:47:26.958811 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0801 13:47:26.958814 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.958822 24466 net.cpp:184] Created Layer res4a_branch2b (26)
I0801 13:47:26.958824 24466 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0801 13:47:26.958827 24466 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0801 13:47:26.967553 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.04G, req 0G)
I0801 13:47:26.967566 24466 net.cpp:245] Setting up res4a_branch2b
I0801 13:47:26.967571 24466 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2b' 22 256 16 16 (1441792)
I0801 13:47:26.967576 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0801 13:47:26.967579 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.967583 24466 net.cpp:184] Created Layer res4a_branch2b/bn (27)
I0801 13:47:26.967586 24466 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0801 13:47:26.967589 24466 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0801 13:47:26.968232 24466 net.cpp:245] Setting up res4a_branch2b/bn
I0801 13:47:26.968240 24466 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b/bn' 22 256 16 16 (1441792)
I0801 13:47:26.968246 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0801 13:47:26.968250 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.968253 24466 net.cpp:184] Created Layer res4a_branch2b/relu (28)
I0801 13:47:26.968256 24466 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0801 13:47:26.968258 24466 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0801 13:47:26.968262 24466 net.cpp:245] Setting up res4a_branch2b/relu
I0801 13:47:26.968266 24466 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/relu' 22 256 16 16 (1441792)
I0801 13:47:26.968267 24466 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0801 13:47:26.968271 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.968274 24466 net.cpp:184] Created Layer pool4 (29)
I0801 13:47:26.968276 24466 net.cpp:561] pool4 <- res4a_branch2b
I0801 13:47:26.968279 24466 net.cpp:530] pool4 -> pool4
I0801 13:47:26.968349 24466 net.cpp:245] Setting up pool4
I0801 13:47:26.968355 24466 net.cpp:252] TRAIN Top shape for layer 29 'pool4' 22 256 8 8 (360448)
I0801 13:47:26.968358 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0801 13:47:26.968361 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.968370 24466 net.cpp:184] Created Layer res5a_branch2a (30)
I0801 13:47:26.968374 24466 net.cpp:561] res5a_branch2a <- pool4
I0801 13:47:26.968376 24466 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0801 13:47:27.013043 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.02G, req 0.01G)
I0801 13:47:27.013062 24466 net.cpp:245] Setting up res5a_branch2a
I0801 13:47:27.013067 24466 net.cpp:252] TRAIN Top shape for layer 30 'res5a_branch2a' 22 512 8 8 (720896)
I0801 13:47:27.013087 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0801 13:47:27.013090 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.013098 24466 net.cpp:184] Created Layer res5a_branch2a/bn (31)
I0801 13:47:27.013100 24466 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0801 13:47:27.013104 24466 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0801 13:47:27.013770 24466 net.cpp:245] Setting up res5a_branch2a/bn
I0801 13:47:27.013779 24466 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a/bn' 22 512 8 8 (720896)
I0801 13:47:27.013784 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0801 13:47:27.013787 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.013792 24466 net.cpp:184] Created Layer res5a_branch2a/relu (32)
I0801 13:47:27.013794 24466 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0801 13:47:27.013797 24466 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0801 13:47:27.013800 24466 net.cpp:245] Setting up res5a_branch2a/relu
I0801 13:47:27.013803 24466 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/relu' 22 512 8 8 (720896)
I0801 13:47:27.013804 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0801 13:47:27.013808 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.013819 24466 net.cpp:184] Created Layer res5a_branch2b (33)
I0801 13:47:27.013823 24466 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0801 13:47:27.013824 24466 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0801 13:47:27.033175 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8G, req 0.01G)
I0801 13:47:27.033190 24466 net.cpp:245] Setting up res5a_branch2b
I0801 13:47:27.033195 24466 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2b' 22 512 8 8 (720896)
I0801 13:47:27.033205 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0801 13:47:27.033208 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.033221 24466 net.cpp:184] Created Layer res5a_branch2b/bn (34)
I0801 13:47:27.033226 24466 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0801 13:47:27.033228 24466 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0801 13:47:27.033893 24466 net.cpp:245] Setting up res5a_branch2b/bn
I0801 13:47:27.033901 24466 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b/bn' 22 512 8 8 (720896)
I0801 13:47:27.033908 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0801 13:47:27.033911 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.033915 24466 net.cpp:184] Created Layer res5a_branch2b/relu (35)
I0801 13:47:27.033918 24466 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0801 13:47:27.033921 24466 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0801 13:47:27.033926 24466 net.cpp:245] Setting up res5a_branch2b/relu
I0801 13:47:27.033928 24466 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/relu' 22 512 8 8 (720896)
I0801 13:47:27.033931 24466 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0801 13:47:27.033933 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.033938 24466 net.cpp:184] Created Layer pool5 (36)
I0801 13:47:27.033941 24466 net.cpp:561] pool5 <- res5a_branch2b
I0801 13:47:27.033943 24466 net.cpp:530] pool5 -> pool5
I0801 13:47:27.033968 24466 net.cpp:245] Setting up pool5
I0801 13:47:27.033973 24466 net.cpp:252] TRAIN Top shape for layer 36 'pool5' 22 512 1 1 (11264)
I0801 13:47:27.033977 24466 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0801 13:47:27.033978 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.033993 24466 net.cpp:184] Created Layer fc10 (37)
I0801 13:47:27.033996 24466 net.cpp:561] fc10 <- pool5
I0801 13:47:27.033998 24466 net.cpp:530] fc10 -> fc10
I0801 13:47:27.034286 24466 net.cpp:245] Setting up fc10
I0801 13:47:27.034293 24466 net.cpp:252] TRAIN Top shape for layer 37 'fc10' 22 10 (220)
I0801 13:47:27.034298 24466 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0801 13:47:27.034301 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.034312 24466 net.cpp:184] Created Layer loss (38)
I0801 13:47:27.034314 24466 net.cpp:561] loss <- fc10
I0801 13:47:27.034317 24466 net.cpp:561] loss <- label
I0801 13:47:27.034322 24466 net.cpp:530] loss -> loss
I0801 13:47:27.034478 24466 net.cpp:245] Setting up loss
I0801 13:47:27.034485 24466 net.cpp:252] TRAIN Top shape for layer 38 'loss' (1)
I0801 13:47:27.034487 24466 net.cpp:256]     with loss weight 1
I0801 13:47:27.034492 24466 net.cpp:323] loss needs backward computation.
I0801 13:47:27.034495 24466 net.cpp:323] fc10 needs backward computation.
I0801 13:47:27.034497 24466 net.cpp:323] pool5 needs backward computation.
I0801 13:47:27.034500 24466 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0801 13:47:27.034502 24466 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0801 13:47:27.034504 24466 net.cpp:323] res5a_branch2b needs backward computation.
I0801 13:47:27.034507 24466 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0801 13:47:27.034509 24466 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0801 13:47:27.034512 24466 net.cpp:323] res5a_branch2a needs backward computation.
I0801 13:47:27.034513 24466 net.cpp:323] pool4 needs backward computation.
I0801 13:47:27.034517 24466 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0801 13:47:27.034518 24466 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0801 13:47:27.034520 24466 net.cpp:323] res4a_branch2b needs backward computation.
I0801 13:47:27.034523 24466 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0801 13:47:27.034525 24466 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0801 13:47:27.034528 24466 net.cpp:323] res4a_branch2a needs backward computation.
I0801 13:47:27.034529 24466 net.cpp:323] pool3 needs backward computation.
I0801 13:47:27.034531 24466 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0801 13:47:27.034534 24466 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0801 13:47:27.034535 24466 net.cpp:323] res3a_branch2b needs backward computation.
I0801 13:47:27.034538 24466 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0801 13:47:27.034540 24466 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0801 13:47:27.034543 24466 net.cpp:323] res3a_branch2a needs backward computation.
I0801 13:47:27.034545 24466 net.cpp:323] pool2 needs backward computation.
I0801 13:47:27.034548 24466 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0801 13:47:27.034549 24466 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0801 13:47:27.034551 24466 net.cpp:323] res2a_branch2b needs backward computation.
I0801 13:47:27.034554 24466 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0801 13:47:27.034556 24466 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0801 13:47:27.034559 24466 net.cpp:323] res2a_branch2a needs backward computation.
I0801 13:47:27.034560 24466 net.cpp:323] pool1 needs backward computation.
I0801 13:47:27.034562 24466 net.cpp:323] conv1b/relu needs backward computation.
I0801 13:47:27.034564 24466 net.cpp:323] conv1b/bn needs backward computation.
I0801 13:47:27.034566 24466 net.cpp:323] conv1b needs backward computation.
I0801 13:47:27.034569 24466 net.cpp:323] conv1a/relu needs backward computation.
I0801 13:47:27.034570 24466 net.cpp:323] conv1a/bn needs backward computation.
I0801 13:47:27.034572 24466 net.cpp:323] conv1a needs backward computation.
I0801 13:47:27.034580 24466 net.cpp:325] data/bias does not need backward computation.
I0801 13:47:27.034584 24466 net.cpp:325] data does not need backward computation.
I0801 13:47:27.034586 24466 net.cpp:367] This network produces output loss
I0801 13:47:27.034615 24466 net.cpp:389] Top memory (TRAIN) required for data: 121110528 diff: 121110536
I0801 13:47:27.034617 24466 net.cpp:392] Bottom memory (TRAIN) required for data: 121110528 diff: 121110528
I0801 13:47:27.034620 24466 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 80740352 diff: 80740352
I0801 13:47:27.034621 24466 net.cpp:398] Parameters memory (TRAIN) required for data: 9450960 diff: 9450960
I0801 13:47:27.034623 24466 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0801 13:47:27.034626 24466 net.cpp:407] Network initialization done.
I0801 13:47:27.034976 24466 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/test.prototxt
W0801 13:47:27.035027 24466 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0801 13:47:27.035150 24466 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 17
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0801 13:47:27.035238 24466 net.cpp:104] Using FLOAT as default forward math type
I0801 13:47:27.035243 24466 net.cpp:110] Using FLOAT as default backward math type
I0801 13:47:27.035245 24466 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0801 13:47:27.035248 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.035257 24466 net.cpp:184] Created Layer data (0)
I0801 13:47:27.035260 24466 net.cpp:530] data -> data
I0801 13:47:27.035264 24466 net.cpp:530] data -> label
I0801 13:47:27.035269 24466 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0801 13:47:27.035274 24466 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:47:27.036044 24520 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0801 13:47:27.036108 24466 data_layer.cpp:184] (0) ReshapePrefetch 17, 3, 32, 32
I0801 13:47:27.036170 24466 data_layer.cpp:208] (0) Output data size: 17, 3, 32, 32
I0801 13:47:27.036175 24466 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:47:27.036186 24466 net.cpp:245] Setting up data
I0801 13:47:27.036191 24466 net.cpp:252] TEST Top shape for layer 0 'data' 17 3 32 32 (52224)
I0801 13:47:27.036195 24466 net.cpp:252] TEST Top shape for layer 0 'data' 17 (17)
I0801 13:47:27.036196 24466 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0801 13:47:27.036200 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.036209 24466 net.cpp:184] Created Layer label_data_1_split (1)
I0801 13:47:27.036211 24466 net.cpp:561] label_data_1_split <- label
I0801 13:47:27.036214 24466 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0801 13:47:27.036217 24466 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0801 13:47:27.036221 24466 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0801 13:47:27.036283 24466 net.cpp:245] Setting up label_data_1_split
I0801 13:47:27.036288 24466 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0801 13:47:27.036290 24466 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0801 13:47:27.036293 24466 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0801 13:47:27.036295 24466 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0801 13:47:27.036298 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.036303 24466 net.cpp:184] Created Layer data/bias (2)
I0801 13:47:27.036305 24466 net.cpp:561] data/bias <- data
I0801 13:47:27.036308 24466 net.cpp:530] data/bias -> data/bias
I0801 13:47:27.036432 24466 net.cpp:245] Setting up data/bias
I0801 13:47:27.036439 24466 net.cpp:252] TEST Top shape for layer 2 'data/bias' 17 3 32 32 (52224)
I0801 13:47:27.036443 24466 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0801 13:47:27.036447 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.036453 24466 net.cpp:184] Created Layer conv1a (3)
I0801 13:47:27.036456 24466 net.cpp:561] conv1a <- data/bias
I0801 13:47:27.036458 24466 net.cpp:530] conv1a -> conv1a
I0801 13:47:27.036857 24521 data_layer.cpp:97] (0) Parser threads: 1
I0801 13:47:27.036865 24521 data_layer.cpp:99] (0) Transformer threads: 1
I0801 13:47:27.039867 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8G, req 0.01G)
I0801 13:47:27.039881 24466 net.cpp:245] Setting up conv1a
I0801 13:47:27.039888 24466 net.cpp:252] TEST Top shape for layer 3 'conv1a' 17 32 32 32 (557056)
I0801 13:47:27.039896 24466 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0801 13:47:27.039899 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.039906 24466 net.cpp:184] Created Layer conv1a/bn (4)
I0801 13:47:27.039911 24466 net.cpp:561] conv1a/bn <- conv1a
I0801 13:47:27.039914 24466 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0801 13:47:27.040726 24466 net.cpp:245] Setting up conv1a/bn
I0801 13:47:27.040736 24466 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 17 32 32 32 (557056)
I0801 13:47:27.040745 24466 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0801 13:47:27.040748 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.040752 24466 net.cpp:184] Created Layer conv1a/relu (5)
I0801 13:47:27.040755 24466 net.cpp:561] conv1a/relu <- conv1a
I0801 13:47:27.040757 24466 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0801 13:47:27.040762 24466 net.cpp:245] Setting up conv1a/relu
I0801 13:47:27.040766 24466 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 17 32 32 32 (557056)
I0801 13:47:27.040767 24466 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0801 13:47:27.040771 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.040781 24466 net.cpp:184] Created Layer conv1b (6)
I0801 13:47:27.040783 24466 net.cpp:561] conv1b <- conv1a
I0801 13:47:27.040786 24466 net.cpp:530] conv1b -> conv1b
I0801 13:47:27.043931 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8G, req 0.01G)
I0801 13:47:27.043941 24466 net.cpp:245] Setting up conv1b
I0801 13:47:27.043946 24466 net.cpp:252] TEST Top shape for layer 6 'conv1b' 17 32 32 32 (557056)
I0801 13:47:27.043952 24466 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0801 13:47:27.043964 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.043972 24466 net.cpp:184] Created Layer conv1b/bn (7)
I0801 13:47:27.043974 24466 net.cpp:561] conv1b/bn <- conv1b
I0801 13:47:27.043978 24466 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0801 13:47:27.044643 24466 net.cpp:245] Setting up conv1b/bn
I0801 13:47:27.044651 24466 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 17 32 32 32 (557056)
I0801 13:47:27.044656 24466 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0801 13:47:27.044659 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.044663 24466 net.cpp:184] Created Layer conv1b/relu (8)
I0801 13:47:27.044667 24466 net.cpp:561] conv1b/relu <- conv1b
I0801 13:47:27.044669 24466 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0801 13:47:27.044673 24466 net.cpp:245] Setting up conv1b/relu
I0801 13:47:27.044677 24466 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 17 32 32 32 (557056)
I0801 13:47:27.044679 24466 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0801 13:47:27.044682 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.044687 24466 net.cpp:184] Created Layer pool1 (9)
I0801 13:47:27.044689 24466 net.cpp:561] pool1 <- conv1b
I0801 13:47:27.044692 24466 net.cpp:530] pool1 -> pool1
I0801 13:47:27.044757 24466 net.cpp:245] Setting up pool1
I0801 13:47:27.044762 24466 net.cpp:252] TEST Top shape for layer 9 'pool1' 17 32 32 32 (557056)
I0801 13:47:27.044765 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0801 13:47:27.044767 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.044778 24466 net.cpp:184] Created Layer res2a_branch2a (10)
I0801 13:47:27.044781 24466 net.cpp:561] res2a_branch2a <- pool1
I0801 13:47:27.044785 24466 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0801 13:47:27.048420 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.99G, req 0.01G)
I0801 13:47:27.048430 24466 net.cpp:245] Setting up res2a_branch2a
I0801 13:47:27.048435 24466 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 17 64 32 32 (1114112)
I0801 13:47:27.048441 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0801 13:47:27.048444 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.048450 24466 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0801 13:47:27.048454 24466 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0801 13:47:27.048456 24466 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0801 13:47:27.049150 24466 net.cpp:245] Setting up res2a_branch2a/bn
I0801 13:47:27.049159 24466 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 17 64 32 32 (1114112)
I0801 13:47:27.049165 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0801 13:47:27.049167 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.049171 24466 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0801 13:47:27.049175 24466 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0801 13:47:27.049176 24466 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0801 13:47:27.049182 24466 net.cpp:245] Setting up res2a_branch2a/relu
I0801 13:47:27.049185 24466 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 17 64 32 32 (1114112)
I0801 13:47:27.049188 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0801 13:47:27.049191 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.049197 24466 net.cpp:184] Created Layer res2a_branch2b (13)
I0801 13:47:27.049201 24466 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0801 13:47:27.049203 24466 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0801 13:47:27.052304 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.98G, req 0.01G)
I0801 13:47:27.052312 24466 net.cpp:245] Setting up res2a_branch2b
I0801 13:47:27.052316 24466 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 17 64 32 32 (1114112)
I0801 13:47:27.052321 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0801 13:47:27.052325 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.052330 24466 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0801 13:47:27.052333 24466 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0801 13:47:27.052336 24466 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0801 13:47:27.053000 24466 net.cpp:245] Setting up res2a_branch2b/bn
I0801 13:47:27.053009 24466 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 17 64 32 32 (1114112)
I0801 13:47:27.053014 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0801 13:47:27.053017 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.053021 24466 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0801 13:47:27.053025 24466 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0801 13:47:27.053027 24466 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0801 13:47:27.053031 24466 net.cpp:245] Setting up res2a_branch2b/relu
I0801 13:47:27.053035 24466 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 17 64 32 32 (1114112)
I0801 13:47:27.053036 24466 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0801 13:47:27.053040 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.053043 24466 net.cpp:184] Created Layer pool2 (16)
I0801 13:47:27.053046 24466 net.cpp:561] pool2 <- res2a_branch2b
I0801 13:47:27.053048 24466 net.cpp:530] pool2 -> pool2
I0801 13:47:27.053108 24466 net.cpp:245] Setting up pool2
I0801 13:47:27.053112 24466 net.cpp:252] TEST Top shape for layer 16 'pool2' 17 64 16 16 (278528)
I0801 13:47:27.053115 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0801 13:47:27.053118 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.053124 24466 net.cpp:184] Created Layer res3a_branch2a (17)
I0801 13:47:27.053128 24466 net.cpp:561] res3a_branch2a <- pool2
I0801 13:47:27.053129 24466 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0801 13:47:27.059074 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.97G, req 0.01G)
I0801 13:47:27.059085 24466 net.cpp:245] Setting up res3a_branch2a
I0801 13:47:27.059090 24466 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 17 128 16 16 (557056)
I0801 13:47:27.059095 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0801 13:47:27.059098 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.059103 24466 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0801 13:47:27.059106 24466 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0801 13:47:27.059110 24466 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0801 13:47:27.059762 24466 net.cpp:245] Setting up res3a_branch2a/bn
I0801 13:47:27.059769 24466 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 17 128 16 16 (557056)
I0801 13:47:27.059777 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0801 13:47:27.059780 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.059784 24466 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0801 13:47:27.059787 24466 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0801 13:47:27.059792 24466 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0801 13:47:27.059795 24466 net.cpp:245] Setting up res3a_branch2a/relu
I0801 13:47:27.059806 24466 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 17 128 16 16 (557056)
I0801 13:47:27.059809 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0801 13:47:27.059811 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.059818 24466 net.cpp:184] Created Layer res3a_branch2b (20)
I0801 13:47:27.059821 24466 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0801 13:47:27.059823 24466 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0801 13:47:27.063128 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.97G, req 0.01G)
I0801 13:47:27.063138 24466 net.cpp:245] Setting up res3a_branch2b
I0801 13:47:27.063143 24466 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 17 128 16 16 (557056)
I0801 13:47:27.063146 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0801 13:47:27.063150 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.063159 24466 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0801 13:47:27.063163 24466 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0801 13:47:27.063165 24466 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0801 13:47:27.063822 24466 net.cpp:245] Setting up res3a_branch2b/bn
I0801 13:47:27.063829 24466 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 17 128 16 16 (557056)
I0801 13:47:27.063835 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0801 13:47:27.063838 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.063841 24466 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0801 13:47:27.063844 24466 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0801 13:47:27.063848 24466 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0801 13:47:27.063851 24466 net.cpp:245] Setting up res3a_branch2b/relu
I0801 13:47:27.063854 24466 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 17 128 16 16 (557056)
I0801 13:47:27.063858 24466 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0801 13:47:27.063859 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.063863 24466 net.cpp:184] Created Layer pool3 (23)
I0801 13:47:27.063866 24466 net.cpp:561] pool3 <- res3a_branch2b
I0801 13:47:27.063868 24466 net.cpp:530] pool3 -> pool3
I0801 13:47:27.063928 24466 net.cpp:245] Setting up pool3
I0801 13:47:27.063932 24466 net.cpp:252] TEST Top shape for layer 23 'pool3' 17 128 16 16 (557056)
I0801 13:47:27.063935 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0801 13:47:27.063938 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.063953 24466 net.cpp:184] Created Layer res4a_branch2a (24)
I0801 13:47:27.063956 24466 net.cpp:561] res4a_branch2a <- pool3
I0801 13:47:27.063959 24466 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0801 13:47:27.074946 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0.01G)
I0801 13:47:27.074961 24466 net.cpp:245] Setting up res4a_branch2a
I0801 13:47:27.074966 24466 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 17 256 16 16 (1114112)
I0801 13:47:27.074973 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0801 13:47:27.074976 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.074985 24466 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0801 13:47:27.074987 24466 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0801 13:47:27.074990 24466 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0801 13:47:27.075716 24466 net.cpp:245] Setting up res4a_branch2a/bn
I0801 13:47:27.075723 24466 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 17 256 16 16 (1114112)
I0801 13:47:27.075739 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0801 13:47:27.075743 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.075747 24466 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0801 13:47:27.075748 24466 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0801 13:47:27.075752 24466 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0801 13:47:27.075754 24466 net.cpp:245] Setting up res4a_branch2a/relu
I0801 13:47:27.075757 24466 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 17 256 16 16 (1114112)
I0801 13:47:27.075759 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0801 13:47:27.075762 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.075773 24466 net.cpp:184] Created Layer res4a_branch2b (27)
I0801 13:47:27.075775 24466 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0801 13:47:27.075778 24466 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0801 13:47:27.081995 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.95G, req 0.01G)
I0801 13:47:27.082006 24466 net.cpp:245] Setting up res4a_branch2b
I0801 13:47:27.082011 24466 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 17 256 16 16 (1114112)
I0801 13:47:27.082015 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0801 13:47:27.082018 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.082025 24466 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0801 13:47:27.082026 24466 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0801 13:47:27.082029 24466 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0801 13:47:27.082783 24466 net.cpp:245] Setting up res4a_branch2b/bn
I0801 13:47:27.082792 24466 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 17 256 16 16 (1114112)
I0801 13:47:27.082798 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0801 13:47:27.082800 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.082803 24466 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0801 13:47:27.082806 24466 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0801 13:47:27.082808 24466 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0801 13:47:27.082813 24466 net.cpp:245] Setting up res4a_branch2b/relu
I0801 13:47:27.082814 24466 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 17 256 16 16 (1114112)
I0801 13:47:27.082816 24466 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0801 13:47:27.082818 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.082823 24466 net.cpp:184] Created Layer pool4 (30)
I0801 13:47:27.082824 24466 net.cpp:561] pool4 <- res4a_branch2b
I0801 13:47:27.082828 24466 net.cpp:530] pool4 -> pool4
I0801 13:47:27.082895 24466 net.cpp:245] Setting up pool4
I0801 13:47:27.082900 24466 net.cpp:252] TEST Top shape for layer 30 'pool4' 17 256 8 8 (278528)
I0801 13:47:27.082901 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0801 13:47:27.082904 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.082911 24466 net.cpp:184] Created Layer res5a_branch2a (31)
I0801 13:47:27.082912 24466 net.cpp:561] res5a_branch2a <- pool4
I0801 13:47:27.082914 24466 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0801 13:47:27.115054 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.94G, req 0.01G)
I0801 13:47:27.115072 24466 net.cpp:245] Setting up res5a_branch2a
I0801 13:47:27.115077 24466 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 17 512 8 8 (557056)
I0801 13:47:27.115083 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0801 13:47:27.115100 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.115109 24466 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0801 13:47:27.115113 24466 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0801 13:47:27.115118 24466 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0801 13:47:27.115838 24466 net.cpp:245] Setting up res5a_branch2a/bn
I0801 13:47:27.115845 24466 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 17 512 8 8 (557056)
I0801 13:47:27.115851 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0801 13:47:27.115854 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.115859 24466 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0801 13:47:27.115861 24466 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0801 13:47:27.115864 24466 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0801 13:47:27.115867 24466 net.cpp:245] Setting up res5a_branch2a/relu
I0801 13:47:27.115869 24466 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 17 512 8 8 (557056)
I0801 13:47:27.115871 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0801 13:47:27.115875 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.115881 24466 net.cpp:184] Created Layer res5a_branch2b (34)
I0801 13:47:27.115883 24466 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0801 13:47:27.115886 24466 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0801 13:47:27.133141 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.93G, req 0.01G)
I0801 13:47:27.133159 24466 net.cpp:245] Setting up res5a_branch2b
I0801 13:47:27.133165 24466 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 17 512 8 8 (557056)
I0801 13:47:27.133178 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0801 13:47:27.133183 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.133194 24466 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0801 13:47:27.133198 24466 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0801 13:47:27.133203 24466 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0801 13:47:27.134007 24466 net.cpp:245] Setting up res5a_branch2b/bn
I0801 13:47:27.134017 24466 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 17 512 8 8 (557056)
I0801 13:47:27.134026 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0801 13:47:27.134029 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.134038 24466 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0801 13:47:27.134042 24466 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0801 13:47:27.134045 24466 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0801 13:47:27.134050 24466 net.cpp:245] Setting up res5a_branch2b/relu
I0801 13:47:27.134053 24466 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 17 512 8 8 (557056)
I0801 13:47:27.134057 24466 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0801 13:47:27.134059 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.134065 24466 net.cpp:184] Created Layer pool5 (37)
I0801 13:47:27.134068 24466 net.cpp:561] pool5 <- res5a_branch2b
I0801 13:47:27.134070 24466 net.cpp:530] pool5 -> pool5
I0801 13:47:27.134099 24466 net.cpp:245] Setting up pool5
I0801 13:47:27.134104 24466 net.cpp:252] TEST Top shape for layer 37 'pool5' 17 512 1 1 (8704)
I0801 13:47:27.134106 24466 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0801 13:47:27.134110 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.134116 24466 net.cpp:184] Created Layer fc10 (38)
I0801 13:47:27.134130 24466 net.cpp:561] fc10 <- pool5
I0801 13:47:27.134132 24466 net.cpp:530] fc10 -> fc10
I0801 13:47:27.134435 24466 net.cpp:245] Setting up fc10
I0801 13:47:27.134443 24466 net.cpp:252] TEST Top shape for layer 38 'fc10' 17 10 (170)
I0801 13:47:27.134447 24466 layer_factory.hpp:136] Creating layer 'fc10_fc10_0_split' of type 'Split'
I0801 13:47:27.134450 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.134454 24466 net.cpp:184] Created Layer fc10_fc10_0_split (39)
I0801 13:47:27.134457 24466 net.cpp:561] fc10_fc10_0_split <- fc10
I0801 13:47:27.134460 24466 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0801 13:47:27.134464 24466 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0801 13:47:27.134467 24466 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0801 13:47:27.134541 24466 net.cpp:245] Setting up fc10_fc10_0_split
I0801 13:47:27.134546 24466 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0801 13:47:27.134548 24466 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0801 13:47:27.134552 24466 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0801 13:47:27.134555 24466 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0801 13:47:27.134557 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.134567 24466 net.cpp:184] Created Layer loss (40)
I0801 13:47:27.134570 24466 net.cpp:561] loss <- fc10_fc10_0_split_0
I0801 13:47:27.134572 24466 net.cpp:561] loss <- label_data_1_split_0
I0801 13:47:27.134577 24466 net.cpp:530] loss -> loss
I0801 13:47:27.134747 24466 net.cpp:245] Setting up loss
I0801 13:47:27.134754 24466 net.cpp:252] TEST Top shape for layer 40 'loss' (1)
I0801 13:47:27.134758 24466 net.cpp:256]     with loss weight 1
I0801 13:47:27.134764 24466 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0801 13:47:27.134769 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.134783 24466 net.cpp:184] Created Layer accuracy/top1 (41)
I0801 13:47:27.134788 24466 net.cpp:561] accuracy/top1 <- fc10_fc10_0_split_1
I0801 13:47:27.134791 24466 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0801 13:47:27.134795 24466 net.cpp:530] accuracy/top1 -> accuracy/top1
I0801 13:47:27.134802 24466 net.cpp:245] Setting up accuracy/top1
I0801 13:47:27.134806 24466 net.cpp:252] TEST Top shape for layer 41 'accuracy/top1' (1)
I0801 13:47:27.134810 24466 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0801 13:47:27.134814 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.134819 24466 net.cpp:184] Created Layer accuracy/top5 (42)
I0801 13:47:27.134824 24466 net.cpp:561] accuracy/top5 <- fc10_fc10_0_split_2
I0801 13:47:27.134827 24466 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0801 13:47:27.134832 24466 net.cpp:530] accuracy/top5 -> accuracy/top5
I0801 13:47:27.134838 24466 net.cpp:245] Setting up accuracy/top5
I0801 13:47:27.134842 24466 net.cpp:252] TEST Top shape for layer 42 'accuracy/top5' (1)
I0801 13:47:27.134846 24466 net.cpp:325] accuracy/top5 does not need backward computation.
I0801 13:47:27.134851 24466 net.cpp:325] accuracy/top1 does not need backward computation.
I0801 13:47:27.134855 24466 net.cpp:323] loss needs backward computation.
I0801 13:47:27.134860 24466 net.cpp:323] fc10_fc10_0_split needs backward computation.
I0801 13:47:27.134863 24466 net.cpp:323] fc10 needs backward computation.
I0801 13:47:27.134867 24466 net.cpp:323] pool5 needs backward computation.
I0801 13:47:27.134871 24466 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0801 13:47:27.134874 24466 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0801 13:47:27.134878 24466 net.cpp:323] res5a_branch2b needs backward computation.
I0801 13:47:27.134882 24466 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0801 13:47:27.134891 24466 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0801 13:47:27.134896 24466 net.cpp:323] res5a_branch2a needs backward computation.
I0801 13:47:27.134901 24466 net.cpp:323] pool4 needs backward computation.
I0801 13:47:27.134904 24466 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0801 13:47:27.134907 24466 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0801 13:47:27.134912 24466 net.cpp:323] res4a_branch2b needs backward computation.
I0801 13:47:27.134915 24466 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0801 13:47:27.134919 24466 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0801 13:47:27.134923 24466 net.cpp:323] res4a_branch2a needs backward computation.
I0801 13:47:27.134927 24466 net.cpp:323] pool3 needs backward computation.
I0801 13:47:27.134932 24466 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0801 13:47:27.134935 24466 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0801 13:47:27.134939 24466 net.cpp:323] res3a_branch2b needs backward computation.
I0801 13:47:27.134943 24466 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0801 13:47:27.134946 24466 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0801 13:47:27.134950 24466 net.cpp:323] res3a_branch2a needs backward computation.
I0801 13:47:27.134954 24466 net.cpp:323] pool2 needs backward computation.
I0801 13:47:27.134958 24466 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0801 13:47:27.134961 24466 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0801 13:47:27.134965 24466 net.cpp:323] res2a_branch2b needs backward computation.
I0801 13:47:27.134969 24466 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0801 13:47:27.134973 24466 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0801 13:47:27.134976 24466 net.cpp:323] res2a_branch2a needs backward computation.
I0801 13:47:27.134980 24466 net.cpp:323] pool1 needs backward computation.
I0801 13:47:27.134984 24466 net.cpp:323] conv1b/relu needs backward computation.
I0801 13:47:27.134989 24466 net.cpp:323] conv1b/bn needs backward computation.
I0801 13:47:27.134992 24466 net.cpp:323] conv1b needs backward computation.
I0801 13:47:27.134995 24466 net.cpp:323] conv1a/relu needs backward computation.
I0801 13:47:27.134999 24466 net.cpp:323] conv1a/bn needs backward computation.
I0801 13:47:27.135004 24466 net.cpp:323] conv1a needs backward computation.
I0801 13:47:27.135007 24466 net.cpp:325] data/bias does not need backward computation.
I0801 13:47:27.135012 24466 net.cpp:325] label_data_1_split does not need backward computation.
I0801 13:47:27.135016 24466 net.cpp:325] data does not need backward computation.
I0801 13:47:27.135020 24466 net.cpp:367] This network produces output accuracy/top1
I0801 13:47:27.135025 24466 net.cpp:367] This network produces output accuracy/top5
I0801 13:47:27.135027 24466 net.cpp:367] This network produces output loss
I0801 13:47:27.135057 24466 net.cpp:389] Top memory (TEST) required for data: 93585408 diff: 8
I0801 13:47:27.135061 24466 net.cpp:392] Bottom memory (TEST) required for data: 93585408 diff: 93585408
I0801 13:47:27.135064 24466 net.cpp:395] Shared (in-place) memory (TEST) by data: 62390272 diff: 62390272
I0801 13:47:27.135067 24466 net.cpp:398] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0801 13:47:27.135071 24466 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0801 13:47:27.135076 24466 net.cpp:407] Network initialization done.
I0801 13:47:27.135128 24466 solver.cpp:56] Solver scaffolding done.
I0801 13:47:27.139250 24466 caffe.cpp:137] Finetuning from training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/cifar10_jacintonet11v2_iter_64000.caffemodel
I0801 13:47:27.143579 24466 net.cpp:1089] Copying source layer data Type:Data #blobs=0
I0801 13:47:27.143599 24466 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0801 13:47:27.143635 24466 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0801 13:47:27.143658 24466 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.143918 24466 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0801 13:47:27.143923 24466 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0801 13:47:27.143934 24466 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.144088 24466 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0801 13:47:27.144093 24466 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0801 13:47:27.144096 24466 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0801 13:47:27.144114 24466 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.144268 24466 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0801 13:47:27.144273 24466 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0801 13:47:27.144286 24466 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.144428 24466 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0801 13:47:27.144433 24466 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0801 13:47:27.144438 24466 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0801 13:47:27.144476 24466 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.144608 24466 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0801 13:47:27.144613 24466 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0801 13:47:27.144635 24466 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.144755 24466 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0801 13:47:27.144759 24466 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0801 13:47:27.144763 24466 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0801 13:47:27.144888 24466 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.145015 24466 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0801 13:47:27.145020 24466 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0801 13:47:27.145081 24466 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.145202 24466 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0801 13:47:27.145207 24466 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0801 13:47:27.145210 24466 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0801 13:47:27.145592 24466 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.145725 24466 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0801 13:47:27.145730 24466 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0801 13:47:27.145889 24466 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.146013 24466 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0801 13:47:27.146019 24466 net.cpp:1089] Copying source layer pool5 Type:Pooling #blobs=0
I0801 13:47:27.146023 24466 net.cpp:1089] Copying source layer fc10 Type:InnerProduct #blobs=2
I0801 13:47:27.146035 24466 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0801 13:47:27.148739 24466 net.cpp:1089] Copying source layer data Type:Data #blobs=0
I0801 13:47:27.148758 24466 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0801 13:47:27.148789 24466 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0801 13:47:27.148803 24466 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.149056 24466 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0801 13:47:27.149071 24466 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0801 13:47:27.149085 24466 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.149235 24466 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0801 13:47:27.149241 24466 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0801 13:47:27.149245 24466 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0801 13:47:27.149261 24466 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.149413 24466 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0801 13:47:27.149420 24466 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0801 13:47:27.149435 24466 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.149581 24466 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0801 13:47:27.149586 24466 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0801 13:47:27.149590 24466 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0801 13:47:27.149629 24466 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.149767 24466 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0801 13:47:27.149772 24466 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0801 13:47:27.149796 24466 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.149914 24466 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0801 13:47:27.149919 24466 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0801 13:47:27.149922 24466 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0801 13:47:27.150037 24466 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.150164 24466 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0801 13:47:27.150169 24466 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0801 13:47:27.150228 24466 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.150358 24466 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0801 13:47:27.150363 24466 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0801 13:47:27.150367 24466 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0801 13:47:27.150707 24466 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.150835 24466 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0801 13:47:27.150840 24466 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0801 13:47:27.150990 24466 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.151120 24466 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0801 13:47:27.151125 24466 net.cpp:1089] Copying source layer pool5 Type:Pooling #blobs=0
I0801 13:47:27.151129 24466 net.cpp:1089] Copying source layer fc10 Type:InnerProduct #blobs=2
I0801 13:47:27.151140 24466 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0801 13:47:27.151204 24466 parallel.cpp:108] [0 - 0] P2pSync adding callback
I0801 13:47:27.151214 24466 parallel.cpp:108] [1 - 1] P2pSync adding callback
I0801 13:47:27.151218 24466 parallel.cpp:108] [2 - 2] P2pSync adding callback
I0801 13:47:27.151221 24466 parallel.cpp:61] Starting Optimization
I0801 13:47:27.151226 24466 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:47:27.151252 24466 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:47:27.151278 24466 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:47:27.151942 24522 device_alternate.hpp:116] NVML initialized on thread 140400203826944
I0801 13:47:27.164360 24522 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0801 13:47:27.164420 24523 device_alternate.hpp:116] NVML initialized on thread 140400195434240
I0801 13:47:27.165562 24523 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0801 13:47:27.165578 24524 device_alternate.hpp:116] NVML initialized on thread 140400187041536
I0801 13:47:27.166246 24524 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0801 13:47:27.169977 24523 solver.cpp:42] Solver data type: FLOAT
W0801 13:47:27.170514 24523 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0801 13:47:27.170626 24523 net.cpp:104] Using FLOAT as default forward math type
I0801 13:47:27.170634 24523 net.cpp:110] Using FLOAT as default backward math type
I0801 13:47:27.170677 24523 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0801 13:47:27.170694 24523 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:47:27.174593 24524 solver.cpp:42] Solver data type: FLOAT
W0801 13:47:27.174952 24524 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0801 13:47:27.175016 24524 net.cpp:104] Using FLOAT as default forward math type
I0801 13:47:27.175020 24524 net.cpp:110] Using FLOAT as default backward math type
I0801 13:47:27.175042 24524 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0801 13:47:27.175050 24524 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:47:27.175292 24525 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0801 13:47:27.176281 24527 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0801 13:47:27.176419 24523 data_layer.cpp:184] [1] ReshapePrefetch 22, 3, 32, 32
I0801 13:47:27.177389 24524 data_layer.cpp:184] [2] ReshapePrefetch 22, 3, 32, 32
I0801 13:47:27.177464 24524 data_layer.cpp:208] [2] Output data size: 22, 3, 32, 32
I0801 13:47:27.177469 24524 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:47:27.177531 24523 data_layer.cpp:208] [1] Output data size: 22, 3, 32, 32
I0801 13:47:27.177541 24523 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:47:27.605844 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.25G, req 0G)
I0801 13:47:27.630036 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.25G, req 0G)
I0801 13:47:27.632150 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.23G, req 0G)
I0801 13:47:27.639448 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.23G, req 0G)
I0801 13:47:27.646047 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.21G, req 0G)
I0801 13:47:27.651338 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.21G, req 0G)
I0801 13:47:27.658550 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.19G, req 0G)
I0801 13:47:27.680593 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.19G, req 0G)
I0801 13:47:27.698572 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.18G, req 0.01G)
I0801 13:47:27.703321 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.18G, req 0.01G)
I0801 13:47:27.705633 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0.01G)
I0801 13:47:27.709846 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0.01G)
I0801 13:47:27.728874 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.15G, req 0.01G)
I0801 13:47:27.730916 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.15G, req 0.01G)
I0801 13:47:27.739718 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.14G, req 0.01G)
I0801 13:47:27.743654 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.14G, req 0.01G)
I0801 13:47:27.787050 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0.01G)
I0801 13:47:27.790648 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0.01G)
I0801 13:47:27.808516 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8.1G, req 0.01G)
I0801 13:47:27.811020 24524 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/test.prototxt
W0801 13:47:27.811067 24524 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0801 13:47:27.811173 24524 net.cpp:104] Using FLOAT as default forward math type
I0801 13:47:27.811178 24524 net.cpp:110] Using FLOAT as default backward math type
I0801 13:47:27.811194 24524 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0801 13:47:27.811204 24524 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:47:27.812021 24543 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0801 13:47:27.812157 24524 data_layer.cpp:184] (2) ReshapePrefetch 17, 3, 32, 32
I0801 13:47:27.812347 24524 data_layer.cpp:208] (2) Output data size: 17, 3, 32, 32
I0801 13:47:27.812355 24524 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:47:27.812383 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8.1G, req 0.01G)
I0801 13:47:27.813047 24544 data_layer.cpp:97] (2) Parser threads: 1
I0801 13:47:27.813055 24544 data_layer.cpp:99] (2) Transformer threads: 1
I0801 13:47:27.815155 24523 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/test.prototxt
W0801 13:47:27.815207 24523 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0801 13:47:27.815305 24523 net.cpp:104] Using FLOAT as default forward math type
I0801 13:47:27.815310 24523 net.cpp:110] Using FLOAT as default backward math type
I0801 13:47:27.815326 24523 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0801 13:47:27.815335 24523 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:47:27.816347 24545 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0801 13:47:27.816426 24523 data_layer.cpp:184] (1) ReshapePrefetch 17, 3, 32, 32
I0801 13:47:27.816517 24523 data_layer.cpp:208] (1) Output data size: 17, 3, 32, 32
I0801 13:47:27.816522 24523 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:47:27.817234 24546 data_layer.cpp:97] (1) Parser threads: 1
I0801 13:47:27.817240 24546 data_layer.cpp:99] (1) Transformer threads: 1
I0801 13:47:27.819483 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8.1G, req 0.01G)
I0801 13:47:27.820569 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8.1G, req 0.01G)
I0801 13:47:27.825067 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.09G, req 0.01G)
I0801 13:47:27.825990 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.09G, req 0.01G)
I0801 13:47:27.830546 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0.01G)
I0801 13:47:27.832154 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0.01G)
I0801 13:47:27.835326 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.08G, req 0.01G)
I0801 13:47:27.837288 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.08G, req 0.01G)
I0801 13:47:27.844128 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.07G, req 0.01G)
I0801 13:47:27.845638 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.07G, req 0.01G)
I0801 13:47:27.850589 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.06G, req 0.01G)
I0801 13:47:27.850803 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.06G, req 0.01G)
I0801 13:47:27.863771 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0.01G)
I0801 13:47:27.865610 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0.01G)
I0801 13:47:27.871058 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.05G, req 0.01G)
I0801 13:47:27.872862 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.05G, req 0.01G)
I0801 13:47:27.905182 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.03G, req 0.01G)
I0801 13:47:27.911846 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.03G, req 0.01G)
I0801 13:47:27.924582 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8.02G, req 0.01G)
I0801 13:47:27.926373 24523 solver.cpp:56] Solver scaffolding done.
I0801 13:47:27.929606 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8.02G, req 0.01G)
I0801 13:47:27.932032 24524 solver.cpp:56] Solver scaffolding done.
I0801 13:47:27.978685 24523 parallel.cpp:164] [1 - 1] P2pSync adding callback
I0801 13:47:27.978711 24524 parallel.cpp:164] [2 - 2] P2pSync adding callback
I0801 13:47:27.978745 24522 parallel.cpp:164] [0 - 0] P2pSync adding callback
I0801 13:47:28.184315 24522 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:47:28.188086 24524 solver.cpp:479] Solving jacintonet11v2_train
I0801 13:47:28.188097 24524 solver.cpp:480] Learning Rate Policy: poly
I0801 13:47:28.191659 24523 solver.cpp:479] Solving jacintonet11v2_train
I0801 13:47:28.191671 24523 solver.cpp:480] Learning Rate Policy: poly
I0801 13:47:28.198138 24522 solver.cpp:479] Solving jacintonet11v2_train
I0801 13:47:28.198151 24522 solver.cpp:480] Learning Rate Policy: poly
I0801 13:47:28.205061 24523 solver.cpp:268] Starting Optimization on GPU 1
I0801 13:47:28.205066 24524 solver.cpp:268] Starting Optimization on GPU 2
I0801 13:47:28.205093 24522 solver.cpp:268] Starting Optimization on GPU 0
I0801 13:47:28.205219 24547 device_alternate.hpp:116] NVML initialized on thread 140399423227648
I0801 13:47:28.205240 24547 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0801 13:47:28.205278 24522 solver.cpp:550] Iteration 0, Testing net (#0)
I0801 13:47:28.205919 24548 device_alternate.hpp:116] NVML initialized on thread 140399431620352
I0801 13:47:28.205931 24548 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0801 13:47:28.205951 24549 device_alternate.hpp:116] NVML initialized on thread 140399414834944
I0801 13:47:28.205965 24549 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0801 13:47:28.215507 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.98G, req 0.01G)
I0801 13:47:28.215780 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.98G, req 0.01G)
I0801 13:47:28.221525 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.97G, req 0.01G)
I0801 13:47:28.221905 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.97G, req 0.01G)
I0801 13:47:28.222198 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 7.92G, req 0G)
I0801 13:47:28.229967 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0.01G)
I0801 13:47:28.230974 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.9G, req 0G)
I0801 13:47:28.231447 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0.01G)
I0801 13:47:28.237018 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.95G, req 0.01G)
I0801 13:47:28.239029 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.95G, req 0.01G)
I0801 13:47:28.241389 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.89G, req 0G)
I0801 13:47:28.243556 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.93G, req 0.01G)
I0801 13:47:28.244863 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.93G, req 0.01G)
I0801 13:47:28.247980 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0G)
I0801 13:47:28.249577 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.92G, req 0.01G)
I0801 13:47:28.251190 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.92G, req 0.01G)
I0801 13:47:28.256014 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.86G, req 0G)
I0801 13:47:28.259230 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.91G, req 0.01G)
I0801 13:47:28.260452 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.91G, req 0.01G)
I0801 13:47:28.260726 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.85G, req 0G)
I0801 13:47:28.265595 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.9G, req 0.01G)
I0801 13:47:28.267150 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.9G, req 0.01G)
I0801 13:47:28.269918 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.84G, req 0G)
I0801 13:47:28.274474 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.83G, req 0G)
I0801 13:47:28.275120 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.88G, req 0.01G)
I0801 13:47:28.276909 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.88G, req 0.01G)
I0801 13:47:28.282027 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0.01G)
I0801 13:47:28.283643 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0.01G)
I0801 13:47:28.285234 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.81G, req 0G)
I0801 13:47:28.289938 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.8G, req 0G)
I0801 13:47:28.292285 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 1
I0801 13:47:28.292294 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0801 13:47:28.292299 24522 solver.cpp:635]     Test net output #2: loss = 0.0169944 (* 1 = 0.0169944 loss)
I0801 13:47:28.292301 24522 solver.cpp:295] [MultiGPU] Initial Test completed
I0801 13:47:28.292318 24524 blocking_queue.cpp:40] Data layer prefetch queue empty
I0801 13:47:28.301594 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.87G, req 0.01G)
I0801 13:47:28.302810 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.87G, req 0.01G)
I0801 13:47:28.303113 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.8G, req 0G)
I0801 13:47:28.310590 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.86G, req 0.01G)
I0801 13:47:28.312160 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.86G, req 0.01G)
I0801 13:47:28.312436 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.79G, req 0G)
I0801 13:47:28.322090 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.85G, req 0.01G)
I0801 13:47:28.324101 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.85G, req 0.01G)
I0801 13:47:28.325076 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.77G, req 0G)
I0801 13:47:28.329833 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0.01G)
I0801 13:47:28.332134 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0.01G)
I0801 13:47:28.333263 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.76G, req 0G)
I0801 13:47:28.340365 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.82G, req 0.01G)
I0801 13:47:28.343231 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.82G, req 0.01G)
I0801 13:47:28.344734 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.75G, req 0.01G)
I0801 13:47:28.347297 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.81G, req 0.01G)
I0801 13:47:28.350663 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.81G, req 0.01G)
I0801 13:47:28.351954 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.74G, req 0.01G)
I0801 13:47:28.362238 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.8G, req 0.01G)
I0801 13:47:28.365849 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.8G, req 0.01G)
I0801 13:47:28.367575 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.72G, req 0.01G)
I0801 13:47:28.369717 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.78G, req 0.01G)
I0801 13:47:28.374197 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.78G, req 0.01G)
I0801 13:47:28.375723 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.71G, req 0.01G)
I0801 13:47:28.389847 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.77G, req 0.01G)
I0801 13:47:28.394136 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.77G, req 0.01G)
I0801 13:47:28.396167 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.69G, req 0.01G)
I0801 13:47:28.397151 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.76G, req 0.01G)
I0801 13:47:28.402323 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.76G, req 0.01G)
I0801 13:47:28.404592 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.68G, req 0.01G)
I0801 13:47:28.423187 24529 data_layer.cpp:97] [1] Parser threads: 1
I0801 13:47:28.423202 24529 data_layer.cpp:99] [1] Transformer threads: 1
I0801 13:47:28.434536 24488 data_layer.cpp:97] [0] Parser threads: 1
I0801 13:47:28.434551 24488 data_layer.cpp:99] [0] Transformer threads: 1
I0801 13:47:28.435797 24528 data_layer.cpp:97] [2] Parser threads: 1
I0801 13:47:28.435811 24528 data_layer.cpp:99] [2] Transformer threads: 1
I0801 13:47:28.441738 24522 solver.cpp:358] Iteration 0 (0.149399 s), loss = 0.000534833
I0801 13:47:28.441758 24522 solver.cpp:375]     Train net output #0: loss = 0.000534833 (* 1 = 0.000534833 loss)
I0801 13:47:28.441764 24522 sgd_solver.cpp:136] Iteration 0, lr = 0.01, m = 0.9
I0801 13:47:28.469102 24522 solver.cpp:358] Iteration 1 (0.0273572 s), loss = 0.00206478
I0801 13:47:28.469130 24522 solver.cpp:375]     Train net output #0: loss = 0.00206478 (* 1 = 0.00206478 loss)
I0801 13:47:28.479344 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 7.06G, req 0.01G)
I0801 13:47:28.480532 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 6.97G, req 0.01G)
I0801 13:47:28.481356 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 7.06G, req 0.01G)
I0801 13:47:28.488248 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.42G, req 0.01G)
I0801 13:47:28.490032 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.33G, req 0.01G)
I0801 13:47:28.490975 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.42G, req 0.01G)
I0801 13:47:28.502267 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.42G, req 0.01G)
I0801 13:47:28.504338 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.33G, req 0.01G)
I0801 13:47:28.504865 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.42G, req 0.01G)
I0801 13:47:28.509312 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.01G)
I0801 13:47:28.511430 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.33G, req 0.01G)
I0801 13:47:28.515529 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.01G)
I0801 13:47:28.519434 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.42G, req 0.01G)
I0801 13:47:28.524116 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.33G, req 0.01G)
I0801 13:47:28.524274 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.01G)
I0801 13:47:28.526057 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.42G, req 0.01G)
I0801 13:47:28.528772 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 0  (limit 6.33G, req 0.01G)
I0801 13:47:28.530575 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 0  (limit 6.42G, req 0.01G)
I0801 13:47:28.550166 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.42G, req 0.02G)
I0801 13:47:28.552214 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.33G, req 0.02G)
I0801 13:47:28.554939 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.42G, req 0.02G)
I0801 13:47:28.558421 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.02G)
I0801 13:47:28.559480 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.33G, req 0.02G)
I0801 13:47:28.563061 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.02G)
I0801 13:47:28.594821 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.33G, req 0.03G)
I0801 13:47:28.595392 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.42G, req 0.03G)
I0801 13:47:28.599827 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.42G, req 0.03G)
I0801 13:47:28.603843 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 5 5  (limit 6.33G, req 0.03G)
I0801 13:47:28.604480 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.42G, req 0.03G)
I0801 13:47:28.608052 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.42G, req 0.03G)
I0801 13:47:28.619050 24522 solver.cpp:358] Iteration 2 (0.149937 s), loss = 0.000599201
I0801 13:47:28.619074 24524 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0801 13:47:28.619077 24522 solver.cpp:375]     Train net output #0: loss = 0.000599201 (* 1 = 0.000599201 loss)
I0801 13:47:28.619084 24523 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0801 13:47:28.619364 24522 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0801 13:47:30.179291 24522 solver.cpp:353] Iteration 100 (62.8129 iter/s, 1.56019s/98 iter), loss = 0.000651761
I0801 13:47:30.179316 24522 solver.cpp:375]     Train net output #0: loss = 0.000651762 (* 1 = 0.000651762 loss)
I0801 13:47:30.179322 24522 sgd_solver.cpp:136] Iteration 100, lr = 0.00998437, m = 0.9
I0801 13:47:31.753546 24522 solver.cpp:353] Iteration 200 (63.524 iter/s, 1.57421s/100 iter), loss = 0.00582214
I0801 13:47:31.753602 24522 solver.cpp:375]     Train net output #0: loss = 0.00582214 (* 1 = 0.00582214 loss)
I0801 13:47:31.753609 24522 sgd_solver.cpp:136] Iteration 200, lr = 0.00996875, m = 0.9
I0801 13:47:33.341953 24522 solver.cpp:353] Iteration 300 (62.9581 iter/s, 1.58836s/100 iter), loss = 0.000438397
I0801 13:47:33.341980 24522 solver.cpp:375]     Train net output #0: loss = 0.000438398 (* 1 = 0.000438398 loss)
I0801 13:47:33.341985 24522 sgd_solver.cpp:136] Iteration 300, lr = 0.00995312, m = 0.9
I0801 13:47:34.923933 24522 solver.cpp:353] Iteration 400 (63.2139 iter/s, 1.58193s/100 iter), loss = 0.00109726
I0801 13:47:34.923959 24522 solver.cpp:375]     Train net output #0: loss = 0.00109726 (* 1 = 0.00109726 loss)
I0801 13:47:34.923964 24522 sgd_solver.cpp:136] Iteration 400, lr = 0.0099375, m = 0.9
I0801 13:47:36.513214 24522 solver.cpp:353] Iteration 500 (62.9235 iter/s, 1.58923s/100 iter), loss = 0.00251341
I0801 13:47:36.513263 24522 solver.cpp:375]     Train net output #0: loss = 0.00251341 (* 1 = 0.00251341 loss)
I0801 13:47:36.513278 24522 sgd_solver.cpp:136] Iteration 500, lr = 0.00992187, m = 0.9
I0801 13:47:38.115597 24522 solver.cpp:353] Iteration 600 (62.4091 iter/s, 1.60233s/100 iter), loss = 0.02622
I0801 13:47:38.115619 24522 solver.cpp:375]     Train net output #0: loss = 0.02622 (* 1 = 0.02622 loss)
I0801 13:47:38.115623 24522 sgd_solver.cpp:136] Iteration 600, lr = 0.00990625, m = 0.9
I0801 13:47:39.699136 24522 solver.cpp:353] Iteration 700 (63.1516 iter/s, 1.58349s/100 iter), loss = 0.329899
I0801 13:47:39.699173 24522 solver.cpp:375]     Train net output #0: loss = 0.329899 (* 1 = 0.329899 loss)
I0801 13:47:39.699179 24522 sgd_solver.cpp:136] Iteration 700, lr = 0.00989062, m = 0.9
I0801 13:47:40.545948 24487 data_reader.cpp:264] Starting prefetch of epoch 1
I0801 13:47:41.275439 24522 solver.cpp:353] Iteration 800 (63.4415 iter/s, 1.57626s/100 iter), loss = 0.357397
I0801 13:47:41.275485 24522 solver.cpp:375]     Train net output #0: loss = 0.357397 (* 1 = 0.357397 loss)
I0801 13:47:41.275496 24522 sgd_solver.cpp:136] Iteration 800, lr = 0.009875, m = 0.9
I0801 13:47:42.872835 24522 solver.cpp:353] Iteration 900 (62.6039 iter/s, 1.59734s/100 iter), loss = 0.0303912
I0801 13:47:42.872861 24522 solver.cpp:375]     Train net output #0: loss = 0.030391 (* 1 = 0.030391 loss)
I0801 13:47:42.872867 24522 sgd_solver.cpp:136] Iteration 900, lr = 0.00985937, m = 0.9
I0801 13:47:44.455584 24522 solver.cpp:404] Sparsity after update:
I0801 13:47:44.460052 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:47:44.460064 24522 net.cpp:2270] conv1a_param_0(0) 
I0801 13:47:44.460078 24522 net.cpp:2270] conv1b_param_0(0) 
I0801 13:47:44.460083 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:47:44.460085 24522 net.cpp:2270] res2a_branch2a_param_0(0) 
I0801 13:47:44.460089 24522 net.cpp:2270] res2a_branch2b_param_0(0) 
I0801 13:47:44.460093 24522 net.cpp:2270] res3a_branch2a_param_0(0) 
I0801 13:47:44.460095 24522 net.cpp:2270] res3a_branch2b_param_0(0) 
I0801 13:47:44.460099 24522 net.cpp:2270] res4a_branch2a_param_0(0) 
I0801 13:47:44.460103 24522 net.cpp:2270] res4a_branch2b_param_0(0) 
I0801 13:47:44.460105 24522 net.cpp:2270] res5a_branch2a_param_0(0) 
I0801 13:47:44.460108 24522 net.cpp:2270] res5a_branch2b_param_0(0) 
I0801 13:47:44.460111 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0801 13:47:44.460121 24522 solver.cpp:550] Iteration 1000, Testing net (#0)
I0801 13:47:45.284344 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.655588
I0801 13:47:45.284363 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.946765
I0801 13:47:45.284368 24522 solver.cpp:635]     Test net output #2: loss = 1.32485 (* 1 = 1.32485 loss)
I0801 13:47:45.284381 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.824232s
I0801 13:47:45.305763 24522 solver.cpp:353] Iteration 1000 (41.104 iter/s, 2.43286s/100 iter), loss = 0.868658
I0801 13:47:45.305786 24522 solver.cpp:375]     Train net output #0: loss = 0.868658 (* 1 = 0.868658 loss)
I0801 13:47:45.305794 24522 sgd_solver.cpp:136] Iteration 1000, lr = 0.00984375, m = 0.9
I0801 13:47:46.903251 24522 solver.cpp:353] Iteration 1100 (62.6002 iter/s, 1.59744s/100 iter), loss = 0.60125
I0801 13:47:46.903301 24522 solver.cpp:375]     Train net output #0: loss = 0.60125 (* 1 = 0.60125 loss)
I0801 13:47:46.903312 24522 sgd_solver.cpp:136] Iteration 1100, lr = 0.00982813, m = 0.9
I0801 13:47:48.494046 24522 solver.cpp:353] Iteration 1200 (62.8636 iter/s, 1.59074s/100 iter), loss = 0.443024
I0801 13:47:48.494072 24522 solver.cpp:375]     Train net output #0: loss = 0.443024 (* 1 = 0.443024 loss)
I0801 13:47:48.494078 24522 sgd_solver.cpp:136] Iteration 1200, lr = 0.0098125, m = 0.9
I0801 13:47:50.071902 24522 solver.cpp:353] Iteration 1300 (63.3792 iter/s, 1.5778s/100 iter), loss = 0.133561
I0801 13:47:50.071924 24522 solver.cpp:375]     Train net output #0: loss = 0.133561 (* 1 = 0.133561 loss)
I0801 13:47:50.071929 24522 sgd_solver.cpp:136] Iteration 1300, lr = 0.00979687, m = 0.9
I0801 13:47:51.656603 24522 solver.cpp:353] Iteration 1400 (63.1053 iter/s, 1.58465s/100 iter), loss = 0.216859
I0801 13:47:51.656627 24522 solver.cpp:375]     Train net output #0: loss = 0.216859 (* 1 = 0.216859 loss)
I0801 13:47:51.656630 24522 sgd_solver.cpp:136] Iteration 1400, lr = 0.00978125, m = 0.9
I0801 13:47:53.241462 24522 solver.cpp:353] Iteration 1500 (63.099 iter/s, 1.58481s/100 iter), loss = 0.0974944
I0801 13:47:53.241487 24522 solver.cpp:375]     Train net output #0: loss = 0.0974946 (* 1 = 0.0974946 loss)
I0801 13:47:53.243049 24522 sgd_solver.cpp:136] Iteration 1500, lr = 0.00976562, m = 0.9
I0801 13:47:54.831667 24522 solver.cpp:353] Iteration 1600 (62.8871 iter/s, 1.59015s/100 iter), loss = 0.198752
I0801 13:47:54.831707 24522 solver.cpp:375]     Train net output #0: loss = 0.198752 (* 1 = 0.198752 loss)
I0801 13:47:54.831712 24522 sgd_solver.cpp:136] Iteration 1600, lr = 0.00975, m = 0.9
I0801 13:47:56.419111 24522 solver.cpp:353] Iteration 1700 (62.9961 iter/s, 1.5874s/100 iter), loss = 0.322344
I0801 13:47:56.419229 24522 solver.cpp:375]     Train net output #0: loss = 0.322344 (* 1 = 0.322344 loss)
I0801 13:47:56.419237 24522 sgd_solver.cpp:136] Iteration 1700, lr = 0.00973437, m = 0.9
I0801 13:47:58.010743 24522 solver.cpp:353] Iteration 1800 (62.8306 iter/s, 1.59158s/100 iter), loss = 0.176526
I0801 13:47:58.010768 24522 solver.cpp:375]     Train net output #0: loss = 0.176526 (* 1 = 0.176526 loss)
I0801 13:47:58.010774 24522 sgd_solver.cpp:136] Iteration 1800, lr = 0.00971875, m = 0.9
I0801 13:47:59.592622 24522 solver.cpp:353] Iteration 1900 (63.218 iter/s, 1.58183s/100 iter), loss = 0.0715274
I0801 13:47:59.592648 24522 solver.cpp:375]     Train net output #0: loss = 0.0715274 (* 1 = 0.0715274 loss)
I0801 13:47:59.592654 24522 sgd_solver.cpp:136] Iteration 1900, lr = 0.00970312, m = 0.9
I0801 13:48:01.176017 24522 solver.cpp:404] Sparsity after update:
I0801 13:48:01.177908 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:48:01.177919 24522 net.cpp:2270] conv1a_param_0(0) 
I0801 13:48:01.177929 24522 net.cpp:2270] conv1b_param_0(0) 
I0801 13:48:01.177934 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:48:01.177937 24522 net.cpp:2270] res2a_branch2a_param_0(0) 
I0801 13:48:01.177942 24522 net.cpp:2270] res2a_branch2b_param_0(0) 
I0801 13:48:01.177945 24522 net.cpp:2270] res3a_branch2a_param_0(0) 
I0801 13:48:01.177948 24522 net.cpp:2270] res3a_branch2b_param_0(0) 
I0801 13:48:01.177953 24522 net.cpp:2270] res4a_branch2a_param_0(0) 
I0801 13:48:01.177956 24522 net.cpp:2270] res4a_branch2b_param_0(0) 
I0801 13:48:01.177959 24522 net.cpp:2270] res5a_branch2a_param_0(0) 
I0801 13:48:01.177963 24522 net.cpp:2270] res5a_branch2b_param_0(0) 
I0801 13:48:01.177966 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0801 13:48:01.177978 24522 solver.cpp:550] Iteration 2000, Testing net (#0)
I0801 13:48:01.995277 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.786472
I0801 13:48:01.995297 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.983235
I0801 13:48:01.995302 24522 solver.cpp:635]     Test net output #2: loss = 0.820027 (* 1 = 0.820027 loss)
I0801 13:48:01.995318 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.817312s
I0801 13:48:02.010977 24522 solver.cpp:353] Iteration 2000 (41.3516 iter/s, 2.41829s/100 iter), loss = 0.286471
I0801 13:48:02.010996 24522 solver.cpp:375]     Train net output #0: loss = 0.286471 (* 1 = 0.286471 loss)
I0801 13:48:02.011001 24522 sgd_solver.cpp:136] Iteration 2000, lr = 0.0096875, m = 0.9
I0801 13:48:03.592183 24522 solver.cpp:353] Iteration 2100 (63.245 iter/s, 1.58115s/100 iter), loss = 0.206765
I0801 13:48:03.592208 24522 solver.cpp:375]     Train net output #0: loss = 0.206765 (* 1 = 0.206765 loss)
I0801 13:48:03.592216 24522 sgd_solver.cpp:136] Iteration 2100, lr = 0.00967188, m = 0.9
I0801 13:48:05.187500 24522 solver.cpp:353] Iteration 2200 (62.6855 iter/s, 1.59526s/100 iter), loss = 0.0686356
I0801 13:48:05.187553 24522 solver.cpp:375]     Train net output #0: loss = 0.0686357 (* 1 = 0.0686357 loss)
I0801 13:48:05.187568 24522 sgd_solver.cpp:136] Iteration 2200, lr = 0.00965625, m = 0.9
I0801 13:48:06.773969 24522 solver.cpp:353] Iteration 2300 (63.035 iter/s, 1.58642s/100 iter), loss = 0.107024
I0801 13:48:06.773994 24522 solver.cpp:375]     Train net output #0: loss = 0.107024 (* 1 = 0.107024 loss)
I0801 13:48:06.773998 24522 sgd_solver.cpp:136] Iteration 2300, lr = 0.00964062, m = 0.9
I0801 13:48:08.365300 24522 solver.cpp:353] Iteration 2400 (62.8424 iter/s, 1.59128s/100 iter), loss = 0.206561
I0801 13:48:08.365324 24522 solver.cpp:375]     Train net output #0: loss = 0.206561 (* 1 = 0.206561 loss)
I0801 13:48:08.365329 24522 sgd_solver.cpp:136] Iteration 2400, lr = 0.009625, m = 0.9
I0801 13:48:09.951424 24522 solver.cpp:353] Iteration 2500 (63.0488 iter/s, 1.58607s/100 iter), loss = 0.10762
I0801 13:48:09.951448 24522 solver.cpp:375]     Train net output #0: loss = 0.10762 (* 1 = 0.10762 loss)
I0801 13:48:09.951452 24522 sgd_solver.cpp:136] Iteration 2500, lr = 0.00960938, m = 0.9
I0801 13:48:11.526700 24522 solver.cpp:353] Iteration 2600 (63.4829 iter/s, 1.57523s/100 iter), loss = 0.142416
I0801 13:48:11.526724 24522 solver.cpp:375]     Train net output #0: loss = 0.142416 (* 1 = 0.142416 loss)
I0801 13:48:11.526729 24522 sgd_solver.cpp:136] Iteration 2600, lr = 0.00959375, m = 0.9
I0801 13:48:13.108774 24522 solver.cpp:353] Iteration 2700 (63.2102 iter/s, 1.58202s/100 iter), loss = 0.0513683
I0801 13:48:13.108822 24522 solver.cpp:375]     Train net output #0: loss = 0.0513684 (* 1 = 0.0513684 loss)
I0801 13:48:13.108835 24522 sgd_solver.cpp:136] Iteration 2700, lr = 0.00957812, m = 0.9
I0801 13:48:14.693248 24522 solver.cpp:353] Iteration 2800 (63.1144 iter/s, 1.58442s/100 iter), loss = 0.130509
I0801 13:48:14.693272 24522 solver.cpp:375]     Train net output #0: loss = 0.130509 (* 1 = 0.130509 loss)
I0801 13:48:14.693277 24522 sgd_solver.cpp:136] Iteration 2800, lr = 0.0095625, m = 0.9
I0801 13:48:16.292270 24522 solver.cpp:353] Iteration 2900 (62.54 iter/s, 1.59898s/100 iter), loss = 0.148904
I0801 13:48:16.292296 24522 solver.cpp:375]     Train net output #0: loss = 0.148904 (* 1 = 0.148904 loss)
I0801 13:48:16.292301 24522 sgd_solver.cpp:136] Iteration 2900, lr = 0.00954687, m = 0.9
I0801 13:48:17.854998 24522 solver.cpp:404] Sparsity after update:
I0801 13:48:17.857084 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:48:17.857101 24522 net.cpp:2270] conv1a_param_0(0) 
I0801 13:48:17.857107 24522 net.cpp:2270] conv1b_param_0(0) 
I0801 13:48:17.857110 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:48:17.857111 24522 net.cpp:2270] res2a_branch2a_param_0(0) 
I0801 13:48:17.857113 24522 net.cpp:2270] res2a_branch2b_param_0(0) 
I0801 13:48:17.857115 24522 net.cpp:2270] res3a_branch2a_param_0(0) 
I0801 13:48:17.857117 24522 net.cpp:2270] res3a_branch2b_param_0(0) 
I0801 13:48:17.857120 24522 net.cpp:2270] res4a_branch2a_param_0(0) 
I0801 13:48:17.857123 24522 net.cpp:2270] res4a_branch2b_param_0(0) 
I0801 13:48:17.857125 24522 net.cpp:2270] res5a_branch2a_param_0(0) 
I0801 13:48:17.857127 24522 net.cpp:2270] res5a_branch2b_param_0(0) 
I0801 13:48:17.857128 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0801 13:48:17.857137 24522 solver.cpp:550] Iteration 3000, Testing net (#0)
I0801 13:48:18.672202 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.843237
I0801 13:48:18.672220 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.991765
I0801 13:48:18.672225 24522 solver.cpp:635]     Test net output #2: loss = 0.548438 (* 1 = 0.548438 loss)
I0801 13:48:18.672240 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.815076s
I0801 13:48:18.688145 24522 solver.cpp:353] Iteration 3000 (41.7396 iter/s, 2.3958s/100 iter), loss = 0.068511
I0801 13:48:18.688163 24522 solver.cpp:375]     Train net output #0: loss = 0.068511 (* 1 = 0.068511 loss)
I0801 13:48:18.688169 24522 sgd_solver.cpp:136] Iteration 3000, lr = 0.00953125, m = 0.9
I0801 13:48:20.272599 24522 solver.cpp:353] Iteration 3100 (63.1152 iter/s, 1.5844s/100 iter), loss = 0.0336655
I0801 13:48:20.272649 24522 solver.cpp:375]     Train net output #0: loss = 0.0336654 (* 1 = 0.0336654 loss)
I0801 13:48:20.272661 24522 sgd_solver.cpp:136] Iteration 3100, lr = 0.00951563, m = 0.9
I0801 13:48:21.866626 24522 solver.cpp:353] Iteration 3200 (62.7362 iter/s, 1.59398s/100 iter), loss = 0.0705109
I0801 13:48:21.866652 24522 solver.cpp:375]     Train net output #0: loss = 0.0705108 (* 1 = 0.0705108 loss)
I0801 13:48:21.866657 24522 sgd_solver.cpp:136] Iteration 3200, lr = 0.0095, m = 0.9
I0801 13:48:23.465806 24522 solver.cpp:353] Iteration 3300 (62.534 iter/s, 1.59913s/100 iter), loss = 0.105086
I0801 13:48:23.465831 24522 solver.cpp:375]     Train net output #0: loss = 0.105086 (* 1 = 0.105086 loss)
I0801 13:48:23.465837 24522 sgd_solver.cpp:136] Iteration 3300, lr = 0.00948437, m = 0.9
I0801 13:48:25.047185 24522 solver.cpp:353] Iteration 3400 (63.238 iter/s, 1.58133s/100 iter), loss = 0.0295291
I0801 13:48:25.047211 24522 solver.cpp:375]     Train net output #0: loss = 0.029529 (* 1 = 0.029529 loss)
I0801 13:48:25.047235 24522 sgd_solver.cpp:136] Iteration 3400, lr = 0.00946875, m = 0.9
I0801 13:48:26.619125 24522 solver.cpp:353] Iteration 3500 (63.6175 iter/s, 1.57189s/100 iter), loss = 0.0694349
I0801 13:48:26.619246 24522 solver.cpp:375]     Train net output #0: loss = 0.0694347 (* 1 = 0.0694347 loss)
I0801 13:48:26.619261 24522 sgd_solver.cpp:136] Iteration 3500, lr = 0.00945312, m = 0.9
I0801 13:48:28.199262 24522 solver.cpp:353] Iteration 3600 (63.2876 iter/s, 1.58009s/100 iter), loss = 0.0974736
I0801 13:48:28.199292 24522 solver.cpp:375]     Train net output #0: loss = 0.0974734 (* 1 = 0.0974734 loss)
I0801 13:48:28.199300 24522 sgd_solver.cpp:136] Iteration 3600, lr = 0.0094375, m = 0.9
I0801 13:48:29.776561 24522 solver.cpp:353] Iteration 3700 (63.4016 iter/s, 1.57725s/100 iter), loss = 0.102818
I0801 13:48:29.776589 24522 solver.cpp:375]     Train net output #0: loss = 0.102818 (* 1 = 0.102818 loss)
I0801 13:48:29.776595 24522 sgd_solver.cpp:136] Iteration 3700, lr = 0.00942187, m = 0.9
I0801 13:48:31.362355 24522 solver.cpp:353] Iteration 3800 (63.0618 iter/s, 1.58575s/100 iter), loss = 0.171872
I0801 13:48:31.362380 24522 solver.cpp:375]     Train net output #0: loss = 0.171872 (* 1 = 0.171872 loss)
I0801 13:48:31.362386 24522 sgd_solver.cpp:136] Iteration 3800, lr = 0.00940625, m = 0.9
I0801 13:48:32.938639 24522 solver.cpp:353] Iteration 3900 (63.4424 iter/s, 1.57623s/100 iter), loss = 0.141611
I0801 13:48:32.938663 24522 solver.cpp:375]     Train net output #0: loss = 0.14161 (* 1 = 0.14161 loss)
I0801 13:48:32.938668 24522 sgd_solver.cpp:136] Iteration 3900, lr = 0.00939062, m = 0.9
I0801 13:48:34.518767 24522 solver.cpp:404] Sparsity after update:
I0801 13:48:34.520565 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:48:34.520575 24522 net.cpp:2270] conv1a_param_0(0) 
I0801 13:48:34.520581 24522 net.cpp:2270] conv1b_param_0(0) 
I0801 13:48:34.520584 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:48:34.520586 24522 net.cpp:2270] res2a_branch2a_param_0(0) 
I0801 13:48:34.520588 24522 net.cpp:2270] res2a_branch2b_param_0(0) 
I0801 13:48:34.520591 24522 net.cpp:2270] res3a_branch2a_param_0(0) 
I0801 13:48:34.520593 24522 net.cpp:2270] res3a_branch2b_param_0(0) 
I0801 13:48:34.520596 24522 net.cpp:2270] res4a_branch2a_param_0(0) 
I0801 13:48:34.520597 24522 net.cpp:2270] res4a_branch2b_param_0(0) 
I0801 13:48:34.520601 24522 net.cpp:2270] res5a_branch2a_param_0(0) 
I0801 13:48:34.520602 24522 net.cpp:2270] res5a_branch2b_param_0(0) 
I0801 13:48:34.520604 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0801 13:48:34.520612 24522 solver.cpp:550] Iteration 4000, Testing net (#0)
I0801 13:48:35.347121 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.829119
I0801 13:48:35.347141 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.99
I0801 13:48:35.347146 24522 solver.cpp:635]     Test net output #2: loss = 0.645051 (* 1 = 0.645051 loss)
I0801 13:48:35.347160 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.82652s
I0801 13:48:35.363096 24549 solver.cpp:450] Finding and applying sparsity: 0.02
I0801 13:48:55.879442 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:48:55.881356 24522 solver.cpp:353] Iteration 4000 (4.3588 iter/s, 22.9421s/100 iter), loss = 0.101566
I0801 13:48:55.881374 24522 solver.cpp:375]     Train net output #0: loss = 0.101566 (* 1 = 0.101566 loss)
I0801 13:48:55.881382 24522 sgd_solver.cpp:136] Iteration 4000, lr = 0.009375, m = 0.9
I0801 13:48:57.702824 24522 solver.cpp:353] Iteration 4100 (54.9024 iter/s, 1.82141s/100 iter), loss = 0.0278589
I0801 13:48:57.702904 24522 solver.cpp:375]     Train net output #0: loss = 0.0278587 (* 1 = 0.0278587 loss)
I0801 13:48:57.702911 24522 sgd_solver.cpp:136] Iteration 4100, lr = 0.00935937, m = 0.9
I0801 13:48:59.289101 24522 solver.cpp:353] Iteration 4200 (63.0427 iter/s, 1.58623s/100 iter), loss = 0.267889
I0801 13:48:59.289125 24522 solver.cpp:375]     Train net output #0: loss = 0.267889 (* 1 = 0.267889 loss)
I0801 13:48:59.289130 24522 sgd_solver.cpp:136] Iteration 4200, lr = 0.00934375, m = 0.9
I0801 13:49:00.852496 24522 solver.cpp:353] Iteration 4300 (63.9655 iter/s, 1.56334s/100 iter), loss = 0.0601831
I0801 13:49:00.852524 24522 solver.cpp:375]     Train net output #0: loss = 0.060183 (* 1 = 0.060183 loss)
I0801 13:49:00.852530 24522 sgd_solver.cpp:136] Iteration 4300, lr = 0.00932813, m = 0.9
I0801 13:49:02.431870 24522 solver.cpp:353] Iteration 4400 (63.3181 iter/s, 1.57933s/100 iter), loss = 0.267935
I0801 13:49:02.431896 24522 solver.cpp:375]     Train net output #0: loss = 0.267935 (* 1 = 0.267935 loss)
I0801 13:49:02.431902 24522 sgd_solver.cpp:136] Iteration 4400, lr = 0.0093125, m = 0.9
I0801 13:49:04.000684 24522 solver.cpp:353] Iteration 4500 (63.7446 iter/s, 1.56876s/100 iter), loss = 0.0253828
I0801 13:49:04.000748 24522 solver.cpp:375]     Train net output #0: loss = 0.0253826 (* 1 = 0.0253826 loss)
I0801 13:49:04.000766 24522 sgd_solver.cpp:136] Iteration 4500, lr = 0.00929687, m = 0.9
I0801 13:49:05.579517 24522 solver.cpp:353] Iteration 4600 (63.3399 iter/s, 1.57878s/100 iter), loss = 0.0383846
I0801 13:49:05.579543 24522 solver.cpp:375]     Train net output #0: loss = 0.0383843 (* 1 = 0.0383843 loss)
I0801 13:49:05.579550 24522 sgd_solver.cpp:136] Iteration 4600, lr = 0.00928125, m = 0.9
I0801 13:49:07.160811 24522 solver.cpp:353] Iteration 4700 (63.2412 iter/s, 1.58125s/100 iter), loss = 0.039688
I0801 13:49:07.160841 24522 solver.cpp:375]     Train net output #0: loss = 0.0396878 (* 1 = 0.0396878 loss)
I0801 13:49:07.160847 24522 sgd_solver.cpp:136] Iteration 4700, lr = 0.00926562, m = 0.9
I0801 13:49:08.753296 24522 solver.cpp:353] Iteration 4800 (62.797 iter/s, 1.59243s/100 iter), loss = 0.164733
I0801 13:49:08.753320 24522 solver.cpp:375]     Train net output #0: loss = 0.164733 (* 1 = 0.164733 loss)
I0801 13:49:08.753325 24522 sgd_solver.cpp:136] Iteration 4800, lr = 0.00925, m = 0.9
I0801 13:49:10.342250 24522 solver.cpp:353] Iteration 4900 (62.9365 iter/s, 1.5889s/100 iter), loss = 0.136636
I0801 13:49:10.342277 24522 solver.cpp:375]     Train net output #0: loss = 0.136636 (* 1 = 0.136636 loss)
I0801 13:49:10.342280 24522 sgd_solver.cpp:136] Iteration 4900, lr = 0.00923437, m = 0.9
I0801 13:49:11.914005 24522 solver.cpp:404] Sparsity after update:
I0801 13:49:11.915362 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:49:11.915382 24522 net.cpp:2270] conv1a_param_0(0) 
I0801 13:49:11.915390 24522 net.cpp:2270] conv1b_param_0(0.00694) 
I0801 13:49:11.915393 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:49:11.915396 24522 net.cpp:2270] res2a_branch2a_param_0(0.0174) 
I0801 13:49:11.915400 24522 net.cpp:2270] res2a_branch2b_param_0(0.0139) 
I0801 13:49:11.915405 24522 net.cpp:2270] res3a_branch2a_param_0(0.0191) 
I0801 13:49:11.915407 24522 net.cpp:2270] res3a_branch2b_param_0(0.0174) 
I0801 13:49:11.915410 24522 net.cpp:2270] res4a_branch2a_param_0(0.02) 
I0801 13:49:11.915413 24522 net.cpp:2270] res4a_branch2b_param_0(0.0191) 
I0801 13:49:11.915416 24522 net.cpp:2270] res5a_branch2a_param_0(0.0198) 
I0801 13:49:11.915419 24522 net.cpp:2270] res5a_branch2b_param_0(0.0198) 
I0801 13:49:11.915423 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (46254/2.3599e+06) 0.0196
I0801 13:49:11.915437 24522 solver.cpp:550] Iteration 5000, Testing net (#0)
I0801 13:49:12.586000 24520 data_reader.cpp:264] Starting prefetch of epoch 1
I0801 13:49:12.728308 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.86706
I0801 13:49:12.728328 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995
I0801 13:49:12.728333 24522 solver.cpp:635]     Test net output #2: loss = 0.465939 (* 1 = 0.465939 loss)
I0801 13:49:12.728361 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.812898s
I0801 13:49:12.743962 24549 solver.cpp:450] Finding and applying sparsity: 0.04
I0801 13:49:33.584513 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:49:33.586470 24522 solver.cpp:353] Iteration 5000 (4.30226 iter/s, 23.2436s/100 iter), loss = 0.0490053
I0801 13:49:33.586493 24522 solver.cpp:375]     Train net output #0: loss = 0.0490049 (* 1 = 0.0490049 loss)
I0801 13:49:33.586501 24522 sgd_solver.cpp:136] Iteration 5000, lr = 0.00921875, m = 0.9
I0801 13:49:35.416996 24522 solver.cpp:353] Iteration 5100 (54.6309 iter/s, 1.83047s/100 iter), loss = 0.0253344
I0801 13:49:35.417047 24522 solver.cpp:375]     Train net output #0: loss = 0.025334 (* 1 = 0.025334 loss)
I0801 13:49:35.417059 24522 sgd_solver.cpp:136] Iteration 5100, lr = 0.00920312, m = 0.9
I0801 13:49:37.016727 24522 solver.cpp:353] Iteration 5200 (62.5125 iter/s, 1.59968s/100 iter), loss = 0.024886
I0801 13:49:37.016753 24522 solver.cpp:375]     Train net output #0: loss = 0.0248856 (* 1 = 0.0248856 loss)
I0801 13:49:37.016759 24522 sgd_solver.cpp:136] Iteration 5200, lr = 0.0091875, m = 0.9
I0801 13:49:38.616523 24522 solver.cpp:353] Iteration 5300 (62.51 iter/s, 1.59974s/100 iter), loss = 0.0160695
I0801 13:49:38.616547 24522 solver.cpp:375]     Train net output #0: loss = 0.0160691 (* 1 = 0.0160691 loss)
I0801 13:49:38.616552 24522 sgd_solver.cpp:136] Iteration 5300, lr = 0.00917188, m = 0.9
I0801 13:49:40.205629 24522 solver.cpp:353] Iteration 5400 (62.9304 iter/s, 1.58906s/100 iter), loss = 0.0603123
I0801 13:49:40.205654 24522 solver.cpp:375]     Train net output #0: loss = 0.0603119 (* 1 = 0.0603119 loss)
I0801 13:49:40.205660 24522 sgd_solver.cpp:136] Iteration 5400, lr = 0.00915625, m = 0.9
I0801 13:49:41.791944 24522 solver.cpp:353] Iteration 5500 (63.0413 iter/s, 1.58626s/100 iter), loss = 0.0314787
I0801 13:49:41.791972 24522 solver.cpp:375]     Train net output #0: loss = 0.0314783 (* 1 = 0.0314783 loss)
I0801 13:49:41.791980 24522 sgd_solver.cpp:136] Iteration 5500, lr = 0.00914062, m = 0.9
I0801 13:49:43.375090 24522 solver.cpp:353] Iteration 5600 (63.1673 iter/s, 1.5831s/100 iter), loss = 0.0160304
I0801 13:49:43.375116 24522 solver.cpp:375]     Train net output #0: loss = 0.01603 (* 1 = 0.01603 loss)
I0801 13:49:43.375123 24522 sgd_solver.cpp:136] Iteration 5600, lr = 0.009125, m = 0.9
I0801 13:49:44.982098 24522 solver.cpp:353] Iteration 5700 (62.2293 iter/s, 1.60696s/100 iter), loss = 0.0360691
I0801 13:49:44.982125 24522 solver.cpp:375]     Train net output #0: loss = 0.0360687 (* 1 = 0.0360687 loss)
I0801 13:49:44.982132 24522 sgd_solver.cpp:136] Iteration 5700, lr = 0.00910938, m = 0.9
I0801 13:49:46.580613 24522 solver.cpp:353] Iteration 5800 (62.5601 iter/s, 1.59846s/100 iter), loss = 0.25557
I0801 13:49:46.580638 24522 solver.cpp:375]     Train net output #0: loss = 0.25557 (* 1 = 0.25557 loss)
I0801 13:49:46.580643 24522 sgd_solver.cpp:136] Iteration 5800, lr = 0.00909375, m = 0.9
I0801 13:49:48.161201 24522 solver.cpp:353] Iteration 5900 (63.2695 iter/s, 1.58054s/100 iter), loss = 0.0340505
I0801 13:49:48.161223 24522 solver.cpp:375]     Train net output #0: loss = 0.0340501 (* 1 = 0.0340501 loss)
I0801 13:49:48.161227 24522 sgd_solver.cpp:136] Iteration 5900, lr = 0.00907812, m = 0.9
I0801 13:49:49.733209 24522 solver.cpp:404] Sparsity after update:
I0801 13:49:49.735102 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:49:49.735113 24522 net.cpp:2270] conv1a_param_0(0.0125) 
I0801 13:49:49.735126 24522 net.cpp:2270] conv1b_param_0(0.0139) 
I0801 13:49:49.735131 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:49:49.735134 24522 net.cpp:2270] res2a_branch2a_param_0(0.0382) 
I0801 13:49:49.735138 24522 net.cpp:2270] res2a_branch2b_param_0(0.0347) 
I0801 13:49:49.735141 24522 net.cpp:2270] res3a_branch2a_param_0(0.0399) 
I0801 13:49:49.735144 24522 net.cpp:2270] res3a_branch2b_param_0(0.0382) 
I0801 13:49:49.735149 24522 net.cpp:2270] res4a_branch2a_param_0(0.0399) 
I0801 13:49:49.735153 24522 net.cpp:2270] res4a_branch2b_param_0(0.0399) 
I0801 13:49:49.735157 24522 net.cpp:2270] res5a_branch2a_param_0(0.0397) 
I0801 13:49:49.735162 24522 net.cpp:2270] res5a_branch2b_param_0(0.0399) 
I0801 13:49:49.735165 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (93479/2.3599e+06) 0.0396
I0801 13:49:49.735188 24522 solver.cpp:550] Iteration 6000, Testing net (#0)
I0801 13:49:50.553411 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.865295
I0801 13:49:50.553431 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.994412
I0801 13:49:50.553436 24522 solver.cpp:635]     Test net output #2: loss = 0.493651 (* 1 = 0.493651 loss)
I0801 13:49:50.553457 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.81824s
I0801 13:49:50.572526 24549 solver.cpp:450] Finding and applying sparsity: 0.06
I0801 13:50:11.473465 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:50:11.475404 24522 solver.cpp:353] Iteration 6000 (4.28935 iter/s, 23.3136s/100 iter), loss = 0.00997581
I0801 13:50:11.475422 24522 solver.cpp:375]     Train net output #0: loss = 0.00997545 (* 1 = 0.00997545 loss)
I0801 13:50:11.475427 24522 sgd_solver.cpp:136] Iteration 6000, lr = 0.0090625, m = 0.9
I0801 13:50:13.393908 24522 solver.cpp:353] Iteration 6100 (52.1255 iter/s, 1.91845s/100 iter), loss = 0.0885491
I0801 13:50:13.393934 24522 solver.cpp:375]     Train net output #0: loss = 0.0885488 (* 1 = 0.0885488 loss)
I0801 13:50:13.393939 24522 sgd_solver.cpp:136] Iteration 6100, lr = 0.00904687, m = 0.9
I0801 13:50:14.989401 24522 solver.cpp:353] Iteration 6200 (62.6787 iter/s, 1.59544s/100 iter), loss = 0.134932
I0801 13:50:14.989433 24522 solver.cpp:375]     Train net output #0: loss = 0.134931 (* 1 = 0.134931 loss)
I0801 13:50:14.989439 24522 sgd_solver.cpp:136] Iteration 6200, lr = 0.00903125, m = 0.9
I0801 13:50:16.575060 24522 solver.cpp:353] Iteration 6300 (63.0673 iter/s, 1.58561s/100 iter), loss = 0.00373273
I0801 13:50:16.575089 24522 solver.cpp:375]     Train net output #0: loss = 0.00373237 (* 1 = 0.00373237 loss)
I0801 13:50:16.575096 24522 sgd_solver.cpp:136] Iteration 6300, lr = 0.00901563, m = 0.9
I0801 13:50:18.147588 24522 solver.cpp:353] Iteration 6400 (63.5938 iter/s, 1.57248s/100 iter), loss = 0.0401329
I0801 13:50:18.147613 24522 solver.cpp:375]     Train net output #0: loss = 0.0401325 (* 1 = 0.0401325 loss)
I0801 13:50:18.147619 24522 sgd_solver.cpp:136] Iteration 6400, lr = 0.009, m = 0.9
I0801 13:50:19.720974 24522 solver.cpp:353] Iteration 6500 (63.5591 iter/s, 1.57334s/100 iter), loss = 0.0665383
I0801 13:50:19.720999 24522 solver.cpp:375]     Train net output #0: loss = 0.0665379 (* 1 = 0.0665379 loss)
I0801 13:50:19.721006 24522 sgd_solver.cpp:136] Iteration 6500, lr = 0.00898437, m = 0.9
I0801 13:50:21.303812 24522 solver.cpp:353] Iteration 6600 (63.1798 iter/s, 1.58278s/100 iter), loss = 0.175659
I0801 13:50:21.303843 24522 solver.cpp:375]     Train net output #0: loss = 0.175659 (* 1 = 0.175659 loss)
I0801 13:50:21.303848 24522 sgd_solver.cpp:136] Iteration 6600, lr = 0.00896875, m = 0.9
I0801 13:50:22.888906 24522 solver.cpp:353] Iteration 6700 (63.0897 iter/s, 1.58505s/100 iter), loss = 0.00646419
I0801 13:50:22.888933 24522 solver.cpp:375]     Train net output #0: loss = 0.00646384 (* 1 = 0.00646384 loss)
I0801 13:50:22.888938 24522 sgd_solver.cpp:136] Iteration 6700, lr = 0.00895312, m = 0.9
I0801 13:50:24.462033 24522 solver.cpp:353] Iteration 6800 (63.5696 iter/s, 1.57308s/100 iter), loss = 0.0760818
I0801 13:50:24.462082 24522 solver.cpp:375]     Train net output #0: loss = 0.0760815 (* 1 = 0.0760815 loss)
I0801 13:50:24.462095 24522 sgd_solver.cpp:136] Iteration 6800, lr = 0.0089375, m = 0.9
I0801 13:50:26.041519 24522 solver.cpp:353] Iteration 6900 (63.3137 iter/s, 1.57944s/100 iter), loss = 0.197167
I0801 13:50:26.041548 24522 solver.cpp:375]     Train net output #0: loss = 0.197167 (* 1 = 0.197167 loss)
I0801 13:50:26.041554 24522 sgd_solver.cpp:136] Iteration 6900, lr = 0.00892187, m = 0.9
I0801 13:50:27.608525 24522 solver.cpp:404] Sparsity after update:
I0801 13:50:27.610123 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:50:27.610133 24522 net.cpp:2270] conv1a_param_0(0.025) 
I0801 13:50:27.610143 24522 net.cpp:2270] conv1b_param_0(0.0273) 
I0801 13:50:27.610148 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:50:27.610152 24522 net.cpp:2270] res2a_branch2a_param_0(0.059) 
I0801 13:50:27.610157 24522 net.cpp:2270] res2a_branch2b_param_0(0.0556) 
I0801 13:50:27.610162 24522 net.cpp:2270] res3a_branch2a_param_0(0.059) 
I0801 13:50:27.610165 24522 net.cpp:2270] res3a_branch2b_param_0(0.059) 
I0801 13:50:27.610169 24522 net.cpp:2270] res4a_branch2a_param_0(0.0599) 
I0801 13:50:27.610173 24522 net.cpp:2270] res4a_branch2b_param_0(0.059) 
I0801 13:50:27.610177 24522 net.cpp:2270] res5a_branch2a_param_0(0.0596) 
I0801 13:50:27.610182 24522 net.cpp:2270] res5a_branch2b_param_0(0.0598) 
I0801 13:50:27.610186 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (140212/2.3599e+06) 0.0594
I0801 13:50:27.610208 24522 solver.cpp:550] Iteration 7000, Testing net (#0)
I0801 13:50:28.417639 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.874413
I0801 13:50:28.417659 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.994118
I0801 13:50:28.417665 24522 solver.cpp:635]     Test net output #2: loss = 0.469216 (* 1 = 0.469216 loss)
I0801 13:50:28.417683 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.807447s
I0801 13:50:28.433418 24549 solver.cpp:450] Finding and applying sparsity: 0.08
I0801 13:50:48.890005 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:50:48.891901 24522 solver.cpp:353] Iteration 7000 (4.37642 iter/s, 22.8497s/100 iter), loss = 0.00813367
I0801 13:50:48.891926 24522 solver.cpp:375]     Train net output #0: loss = 0.00813329 (* 1 = 0.00813329 loss)
I0801 13:50:48.891934 24522 sgd_solver.cpp:136] Iteration 7000, lr = 0.00890625, m = 0.9
I0801 13:50:50.777755 24522 solver.cpp:353] Iteration 7100 (53.0281 iter/s, 1.88579s/100 iter), loss = 0.0493996
I0801 13:50:50.777778 24522 solver.cpp:375]     Train net output #0: loss = 0.0493993 (* 1 = 0.0493993 loss)
I0801 13:50:50.777782 24522 sgd_solver.cpp:136] Iteration 7100, lr = 0.00889063, m = 0.9
I0801 13:50:52.368500 24522 solver.cpp:353] Iteration 7200 (62.8655 iter/s, 1.5907s/100 iter), loss = 0.0734331
I0801 13:50:52.368527 24522 solver.cpp:375]     Train net output #0: loss = 0.0734326 (* 1 = 0.0734326 loss)
I0801 13:50:52.368531 24522 sgd_solver.cpp:136] Iteration 7200, lr = 0.008875, m = 0.9
I0801 13:50:53.949064 24522 solver.cpp:353] Iteration 7300 (63.2705 iter/s, 1.58051s/100 iter), loss = 0.152591
I0801 13:50:53.949113 24522 solver.cpp:375]     Train net output #0: loss = 0.152591 (* 1 = 0.152591 loss)
I0801 13:50:53.949127 24522 sgd_solver.cpp:136] Iteration 7300, lr = 0.00885937, m = 0.9
I0801 13:50:55.538905 24522 solver.cpp:353] Iteration 7400 (62.9015 iter/s, 1.58979s/100 iter), loss = 0.00469898
I0801 13:50:55.538957 24522 solver.cpp:375]     Train net output #0: loss = 0.0046985 (* 1 = 0.0046985 loss)
I0801 13:50:55.538970 24522 sgd_solver.cpp:136] Iteration 7400, lr = 0.00884375, m = 0.9
I0801 13:50:57.135196 24522 solver.cpp:353] Iteration 7500 (62.6472 iter/s, 1.59624s/100 iter), loss = 0.0361564
I0801 13:50:57.135222 24522 solver.cpp:375]     Train net output #0: loss = 0.036156 (* 1 = 0.036156 loss)
I0801 13:50:57.135228 24522 sgd_solver.cpp:136] Iteration 7500, lr = 0.00882812, m = 0.9
I0801 13:50:58.727721 24522 solver.cpp:353] Iteration 7600 (62.7952 iter/s, 1.59248s/100 iter), loss = 0.0873567
I0801 13:50:58.727748 24522 solver.cpp:375]     Train net output #0: loss = 0.0873561 (* 1 = 0.0873561 loss)
I0801 13:50:58.727756 24522 sgd_solver.cpp:136] Iteration 7600, lr = 0.0088125, m = 0.9
I0801 13:51:00.358357 24522 solver.cpp:353] Iteration 7700 (61.3278 iter/s, 1.63058s/100 iter), loss = 0.00261982
I0801 13:51:00.358383 24522 solver.cpp:375]     Train net output #0: loss = 0.00261929 (* 1 = 0.00261929 loss)
I0801 13:51:00.358388 24522 sgd_solver.cpp:136] Iteration 7700, lr = 0.00879687, m = 0.9
I0801 13:51:01.987216 24522 solver.cpp:353] Iteration 7800 (61.3945 iter/s, 1.62881s/100 iter), loss = 0.0142309
I0801 13:51:01.987267 24522 solver.cpp:375]     Train net output #0: loss = 0.0142303 (* 1 = 0.0142303 loss)
I0801 13:51:01.987279 24522 sgd_solver.cpp:136] Iteration 7800, lr = 0.00878125, m = 0.9
I0801 13:51:03.597430 24522 solver.cpp:353] Iteration 7900 (62.1056 iter/s, 1.61016s/100 iter), loss = 0.113554
I0801 13:51:03.597456 24522 solver.cpp:375]     Train net output #0: loss = 0.113553 (* 1 = 0.113553 loss)
I0801 13:51:03.597462 24522 sgd_solver.cpp:136] Iteration 7900, lr = 0.00876562, m = 0.9
I0801 13:51:05.217314 24522 solver.cpp:404] Sparsity after update:
I0801 13:51:05.218951 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:51:05.218961 24522 net.cpp:2270] conv1a_param_0(0.0367) 
I0801 13:51:05.218967 24522 net.cpp:2270] conv1b_param_0(0.0339) 
I0801 13:51:05.218969 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:51:05.218971 24522 net.cpp:2270] res2a_branch2a_param_0(0.0799) 
I0801 13:51:05.218974 24522 net.cpp:2270] res2a_branch2b_param_0(0.0764) 
I0801 13:51:05.218976 24522 net.cpp:2270] res3a_branch2a_param_0(0.0799) 
I0801 13:51:05.218978 24522 net.cpp:2270] res3a_branch2b_param_0(0.0799) 
I0801 13:51:05.218981 24522 net.cpp:2270] res4a_branch2a_param_0(0.0799) 
I0801 13:51:05.218983 24522 net.cpp:2270] res4a_branch2b_param_0(0.0798) 
I0801 13:51:05.218986 24522 net.cpp:2270] res5a_branch2a_param_0(0.0795) 
I0801 13:51:05.218988 24522 net.cpp:2270] res5a_branch2b_param_0(0.0798) 
I0801 13:51:05.218991 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (187363/2.3599e+06) 0.0794
I0801 13:51:05.219012 24522 solver.cpp:550] Iteration 8000, Testing net (#0)
I0801 13:51:06.027817 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.856178
I0801 13:51:06.027838 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995588
I0801 13:51:06.027843 24522 solver.cpp:635]     Test net output #2: loss = 0.555665 (* 1 = 0.555665 loss)
I0801 13:51:06.027866 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.808827s
I0801 13:51:06.045866 24549 solver.cpp:450] Finding and applying sparsity: 0.1
I0801 13:51:27.131675 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:51:27.133592 24522 solver.cpp:353] Iteration 8000 (4.2489 iter/s, 23.5355s/100 iter), loss = 0.0636911
I0801 13:51:27.133615 24522 solver.cpp:375]     Train net output #0: loss = 0.0636906 (* 1 = 0.0636906 loss)
I0801 13:51:27.133623 24522 sgd_solver.cpp:136] Iteration 8000, lr = 0.00875, m = 0.9
I0801 13:51:28.965428 24522 solver.cpp:353] Iteration 8100 (54.5918 iter/s, 1.83178s/100 iter), loss = 0.0425892
I0801 13:51:28.965453 24522 solver.cpp:375]     Train net output #0: loss = 0.0425887 (* 1 = 0.0425887 loss)
I0801 13:51:28.965458 24522 sgd_solver.cpp:136] Iteration 8100, lr = 0.00873438, m = 0.9
I0801 13:51:30.567754 24522 solver.cpp:353] Iteration 8200 (62.4112 iter/s, 1.60228s/100 iter), loss = 0.00256245
I0801 13:51:30.567777 24522 solver.cpp:375]     Train net output #0: loss = 0.00256192 (* 1 = 0.00256192 loss)
I0801 13:51:30.567781 24522 sgd_solver.cpp:136] Iteration 8200, lr = 0.00871875, m = 0.9
I0801 13:51:32.162586 24522 solver.cpp:353] Iteration 8300 (62.7045 iter/s, 1.59478s/100 iter), loss = 0.0987593
I0801 13:51:32.162633 24522 solver.cpp:375]     Train net output #0: loss = 0.0987587 (* 1 = 0.0987587 loss)
I0801 13:51:32.162644 24522 sgd_solver.cpp:136] Iteration 8300, lr = 0.00870312, m = 0.9
I0801 13:51:33.751250 24522 solver.cpp:353] Iteration 8400 (62.948 iter/s, 1.58861s/100 iter), loss = 0.0758556
I0801 13:51:33.751304 24522 solver.cpp:375]     Train net output #0: loss = 0.0758551 (* 1 = 0.0758551 loss)
I0801 13:51:33.751319 24522 sgd_solver.cpp:136] Iteration 8400, lr = 0.0086875, m = 0.9
I0801 13:51:35.330265 24522 solver.cpp:353] Iteration 8500 (63.3327 iter/s, 1.57896s/100 iter), loss = 0.10946
I0801 13:51:35.330289 24522 solver.cpp:375]     Train net output #0: loss = 0.10946 (* 1 = 0.10946 loss)
I0801 13:51:35.330294 24522 sgd_solver.cpp:136] Iteration 8500, lr = 0.00867188, m = 0.9
I0801 13:51:36.910332 24522 solver.cpp:353] Iteration 8600 (63.2903 iter/s, 1.58002s/100 iter), loss = 0.00420942
I0801 13:51:36.910382 24522 solver.cpp:375]     Train net output #0: loss = 0.00420892 (* 1 = 0.00420892 loss)
I0801 13:51:36.910392 24522 sgd_solver.cpp:136] Iteration 8600, lr = 0.00865625, m = 0.9
I0801 13:51:38.507546 24522 solver.cpp:353] Iteration 8700 (62.611 iter/s, 1.59716s/100 iter), loss = 0.0117062
I0801 13:51:38.507572 24522 solver.cpp:375]     Train net output #0: loss = 0.0117057 (* 1 = 0.0117057 loss)
I0801 13:51:38.507577 24522 sgd_solver.cpp:136] Iteration 8700, lr = 0.00864062, m = 0.9
I0801 13:51:40.087502 24522 solver.cpp:353] Iteration 8800 (63.295 iter/s, 1.5799s/100 iter), loss = 0.061608
I0801 13:51:40.087529 24522 solver.cpp:375]     Train net output #0: loss = 0.0616075 (* 1 = 0.0616075 loss)
I0801 13:51:40.087537 24522 sgd_solver.cpp:136] Iteration 8800, lr = 0.008625, m = 0.9
I0801 13:51:41.661341 24522 solver.cpp:353] Iteration 8900 (63.5409 iter/s, 1.57379s/100 iter), loss = 0.177315
I0801 13:51:41.661366 24522 solver.cpp:375]     Train net output #0: loss = 0.177315 (* 1 = 0.177315 loss)
I0801 13:51:41.661372 24522 sgd_solver.cpp:136] Iteration 8900, lr = 0.00860937, m = 0.9
I0801 13:51:43.235909 24522 solver.cpp:404] Sparsity after update:
I0801 13:51:43.237548 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:51:43.237557 24522 net.cpp:2270] conv1a_param_0(0.0371) 
I0801 13:51:43.237566 24522 net.cpp:2270] conv1b_param_0(0.0273) 
I0801 13:51:43.237571 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:51:43.237576 24522 net.cpp:2270] res2a_branch2a_param_0(0.0972) 
I0801 13:51:43.237581 24522 net.cpp:2270] res2a_branch2b_param_0(0.0972) 
I0801 13:51:43.237584 24522 net.cpp:2270] res3a_branch2a_param_0(0.099) 
I0801 13:51:43.237589 24522 net.cpp:2270] res3a_branch2b_param_0(0.0972) 
I0801 13:51:43.237593 24522 net.cpp:2270] res4a_branch2a_param_0(0.0998) 
I0801 13:51:43.237597 24522 net.cpp:2270] res4a_branch2b_param_0(0.099) 
I0801 13:51:43.237601 24522 net.cpp:2270] res5a_branch2a_param_0(0.0991) 
I0801 13:51:43.237606 24522 net.cpp:2270] res5a_branch2b_param_0(0.0998) 
I0801 13:51:43.237610 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (233478/2.3599e+06) 0.0989
I0801 13:51:43.237634 24522 solver.cpp:550] Iteration 9000, Testing net (#0)
I0801 13:51:44.083238 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.890001
I0801 13:51:44.083261 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.994118
I0801 13:51:44.083271 24522 solver.cpp:635]     Test net output #2: loss = 0.452671 (* 1 = 0.452671 loss)
I0801 13:51:44.083297 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.845632s
I0801 13:51:44.101447 24549 solver.cpp:450] Finding and applying sparsity: 0.12
I0801 13:52:04.706492 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:52:04.708408 24522 solver.cpp:353] Iteration 9000 (4.33907 iter/s, 23.0464s/100 iter), loss = 0.00604583
I0801 13:52:04.708426 24522 solver.cpp:375]     Train net output #0: loss = 0.00604533 (* 1 = 0.00604533 loss)
I0801 13:52:04.708432 24522 sgd_solver.cpp:136] Iteration 9000, lr = 0.00859375, m = 0.9
I0801 13:52:06.327913 24487 data_reader.cpp:264] Starting prefetch of epoch 2
I0801 13:52:06.535255 24522 solver.cpp:353] Iteration 9100 (54.7409 iter/s, 1.82679s/100 iter), loss = 0.044862
I0801 13:52:06.535284 24522 solver.cpp:375]     Train net output #0: loss = 0.0448615 (* 1 = 0.0448615 loss)
I0801 13:52:06.535289 24522 sgd_solver.cpp:136] Iteration 9100, lr = 0.00857813, m = 0.9
I0801 13:52:08.141448 24522 solver.cpp:353] Iteration 9200 (62.261 iter/s, 1.60614s/100 iter), loss = 0.0270382
I0801 13:52:08.141470 24522 solver.cpp:375]     Train net output #0: loss = 0.0270377 (* 1 = 0.0270377 loss)
I0801 13:52:08.141474 24522 sgd_solver.cpp:136] Iteration 9200, lr = 0.0085625, m = 0.9
I0801 13:52:09.712714 24522 solver.cpp:353] Iteration 9300 (63.6449 iter/s, 1.57122s/100 iter), loss = 0.018198
I0801 13:52:09.712766 24522 solver.cpp:375]     Train net output #0: loss = 0.0181974 (* 1 = 0.0181974 loss)
I0801 13:52:09.712783 24522 sgd_solver.cpp:136] Iteration 9300, lr = 0.00854687, m = 0.9
I0801 13:52:11.308285 24522 solver.cpp:353] Iteration 9400 (62.6754 iter/s, 1.59552s/100 iter), loss = 0.194368
I0801 13:52:11.308332 24522 solver.cpp:375]     Train net output #0: loss = 0.194368 (* 1 = 0.194368 loss)
I0801 13:52:11.308346 24522 sgd_solver.cpp:136] Iteration 9400, lr = 0.00853125, m = 0.9
I0801 13:52:12.912261 24522 solver.cpp:353] Iteration 9500 (62.3471 iter/s, 1.60392s/100 iter), loss = 0.00284404
I0801 13:52:12.912288 24522 solver.cpp:375]     Train net output #0: loss = 0.00284352 (* 1 = 0.00284352 loss)
I0801 13:52:12.912294 24522 sgd_solver.cpp:136] Iteration 9500, lr = 0.00851563, m = 0.9
I0801 13:52:14.489341 24522 solver.cpp:353] Iteration 9600 (63.4104 iter/s, 1.57703s/100 iter), loss = 0.00472684
I0801 13:52:14.489364 24522 solver.cpp:375]     Train net output #0: loss = 0.00472632 (* 1 = 0.00472632 loss)
I0801 13:52:14.489369 24522 sgd_solver.cpp:136] Iteration 9600, lr = 0.0085, m = 0.9
I0801 13:52:16.074249 24522 solver.cpp:353] Iteration 9700 (63.0969 iter/s, 1.58486s/100 iter), loss = 0.0140821
I0801 13:52:16.074275 24522 solver.cpp:375]     Train net output #0: loss = 0.0140815 (* 1 = 0.0140815 loss)
I0801 13:52:16.074282 24522 sgd_solver.cpp:136] Iteration 9700, lr = 0.00848437, m = 0.9
I0801 13:52:17.661612 24522 solver.cpp:353] Iteration 9800 (62.9996 iter/s, 1.58731s/100 iter), loss = 0.0271521
I0801 13:52:17.661672 24522 solver.cpp:375]     Train net output #0: loss = 0.0271516 (* 1 = 0.0271516 loss)
I0801 13:52:17.661689 24522 sgd_solver.cpp:136] Iteration 9800, lr = 0.00846875, m = 0.9
I0801 13:52:19.241834 24522 solver.cpp:353] Iteration 9900 (63.2842 iter/s, 1.58017s/100 iter), loss = 0.0127208
I0801 13:52:19.241858 24522 solver.cpp:375]     Train net output #0: loss = 0.0127203 (* 1 = 0.0127203 loss)
I0801 13:52:19.241861 24522 sgd_solver.cpp:136] Iteration 9900, lr = 0.00845312, m = 0.9
I0801 13:52:20.808187 24522 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_10000.caffemodel
I0801 13:52:20.826392 24522 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_10000.solverstate
I0801 13:52:20.830085 24522 solver.cpp:404] Sparsity after update:
I0801 13:52:20.831630 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:52:20.831638 24522 net.cpp:2270] conv1a_param_0(0.0496) 
I0801 13:52:20.831647 24522 net.cpp:2270] conv1b_param_0(0.0382) 
I0801 13:52:20.831652 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:52:20.831655 24522 net.cpp:2270] res2a_branch2a_param_0(0.118) 
I0801 13:52:20.831660 24522 net.cpp:2270] res2a_branch2b_param_0(0.118) 
I0801 13:52:20.831665 24522 net.cpp:2270] res3a_branch2a_param_0(0.12) 
I0801 13:52:20.831676 24522 net.cpp:2270] res3a_branch2b_param_0(0.118) 
I0801 13:52:20.831681 24522 net.cpp:2270] res4a_branch2a_param_0(0.12) 
I0801 13:52:20.831686 24522 net.cpp:2270] res4a_branch2b_param_0(0.12) 
I0801 13:52:20.831689 24522 net.cpp:2270] res5a_branch2a_param_0(0.119) 
I0801 13:52:20.831693 24522 net.cpp:2270] res5a_branch2b_param_0(0.12) 
I0801 13:52:20.831698 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (280589/2.3599e+06) 0.119
I0801 13:52:20.831709 24522 solver.cpp:550] Iteration 10000, Testing net (#0)
I0801 13:52:21.623633 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.879119
I0801 13:52:21.623653 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 13:52:21.623659 24522 solver.cpp:635]     Test net output #2: loss = 0.462688 (* 1 = 0.462688 loss)
I0801 13:52:21.623677 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.791939s
I0801 13:52:21.639384 24549 solver.cpp:450] Finding and applying sparsity: 0.14
I0801 13:52:41.768389 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:52:41.770337 24522 solver.cpp:353] Iteration 10000 (4.43894 iter/s, 22.5279s/100 iter), loss = 0.019464
I0801 13:52:41.770355 24522 solver.cpp:375]     Train net output #0: loss = 0.0194634 (* 1 = 0.0194634 loss)
I0801 13:52:41.770364 24522 sgd_solver.cpp:136] Iteration 10000, lr = 0.0084375, m = 0.9
I0801 13:52:43.621383 24522 solver.cpp:353] Iteration 10100 (54.0252 iter/s, 1.85099s/100 iter), loss = 0.00561036
I0801 13:52:43.621430 24522 solver.cpp:375]     Train net output #0: loss = 0.00560979 (* 1 = 0.00560979 loss)
I0801 13:52:43.621443 24522 sgd_solver.cpp:136] Iteration 10100, lr = 0.00842187, m = 0.9
I0801 13:52:45.231158 24522 solver.cpp:353] Iteration 10200 (62.1225 iter/s, 1.60972s/100 iter), loss = 0.0049929
I0801 13:52:45.231184 24522 solver.cpp:375]     Train net output #0: loss = 0.00499233 (* 1 = 0.00499233 loss)
I0801 13:52:45.231189 24522 sgd_solver.cpp:136] Iteration 10200, lr = 0.00840625, m = 0.9
I0801 13:52:46.800251 24522 solver.cpp:353] Iteration 10300 (63.733 iter/s, 1.56905s/100 iter), loss = 0.00173509
I0801 13:52:46.800277 24522 solver.cpp:375]     Train net output #0: loss = 0.00173452 (* 1 = 0.00173452 loss)
I0801 13:52:46.800282 24522 sgd_solver.cpp:136] Iteration 10300, lr = 0.00839063, m = 0.9
I0801 13:52:48.375249 24522 solver.cpp:353] Iteration 10400 (63.4941 iter/s, 1.57495s/100 iter), loss = 0.00155696
I0801 13:52:48.375277 24522 solver.cpp:375]     Train net output #0: loss = 0.00155639 (* 1 = 0.00155639 loss)
I0801 13:52:48.375283 24522 sgd_solver.cpp:136] Iteration 10400, lr = 0.008375, m = 0.9
I0801 13:52:49.958428 24522 solver.cpp:353] Iteration 10500 (63.1662 iter/s, 1.58313s/100 iter), loss = 0.0358158
I0801 13:52:49.958457 24522 solver.cpp:375]     Train net output #0: loss = 0.0358152 (* 1 = 0.0358152 loss)
I0801 13:52:49.958463 24522 sgd_solver.cpp:136] Iteration 10500, lr = 0.00835937, m = 0.9
I0801 13:52:51.542914 24522 solver.cpp:353] Iteration 10600 (63.1139 iter/s, 1.58444s/100 iter), loss = 0.0181446
I0801 13:52:51.542943 24522 solver.cpp:375]     Train net output #0: loss = 0.018144 (* 1 = 0.018144 loss)
I0801 13:52:51.542950 24522 sgd_solver.cpp:136] Iteration 10600, lr = 0.00834375, m = 0.9
I0801 13:52:53.116212 24522 solver.cpp:353] Iteration 10700 (63.5627 iter/s, 1.57325s/100 iter), loss = 0.0869192
I0801 13:52:53.116236 24522 solver.cpp:375]     Train net output #0: loss = 0.0869186 (* 1 = 0.0869186 loss)
I0801 13:52:53.116241 24522 sgd_solver.cpp:136] Iteration 10700, lr = 0.00832812, m = 0.9
I0801 13:52:54.704728 24522 solver.cpp:353] Iteration 10800 (62.9539 iter/s, 1.58846s/100 iter), loss = 0.02794
I0801 13:52:54.704754 24522 solver.cpp:375]     Train net output #0: loss = 0.0279395 (* 1 = 0.0279395 loss)
I0801 13:52:54.704759 24522 sgd_solver.cpp:136] Iteration 10800, lr = 0.0083125, m = 0.9
I0801 13:52:56.299711 24522 solver.cpp:353] Iteration 10900 (62.6985 iter/s, 1.59493s/100 iter), loss = 0.0697004
I0801 13:52:56.299764 24522 solver.cpp:375]     Train net output #0: loss = 0.0696998 (* 1 = 0.0696998 loss)
I0801 13:52:56.299778 24522 sgd_solver.cpp:136] Iteration 10900, lr = 0.00829687, m = 0.9
I0801 13:52:57.875438 24522 solver.cpp:404] Sparsity after update:
I0801 13:52:57.877035 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:52:57.877044 24522 net.cpp:2270] conv1a_param_0(0.06) 
I0801 13:52:57.877051 24522 net.cpp:2270] conv1b_param_0(0.104) 
I0801 13:52:57.877053 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:52:57.877055 24522 net.cpp:2270] res2a_branch2a_param_0(0.139) 
I0801 13:52:57.877059 24522 net.cpp:2270] res2a_branch2b_param_0(0.139) 
I0801 13:52:57.877061 24522 net.cpp:2270] res3a_branch2a_param_0(0.139) 
I0801 13:52:57.877063 24522 net.cpp:2270] res3a_branch2b_param_0(0.139) 
I0801 13:52:57.877064 24522 net.cpp:2270] res4a_branch2a_param_0(0.14) 
I0801 13:52:57.877066 24522 net.cpp:2270] res4a_branch2b_param_0(0.139) 
I0801 13:52:57.877068 24522 net.cpp:2270] res5a_branch2a_param_0(0.139) 
I0801 13:52:57.877070 24522 net.cpp:2270] res5a_branch2b_param_0(0.14) 
I0801 13:52:57.877073 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (327396/2.3599e+06) 0.139
I0801 13:52:57.877090 24522 solver.cpp:550] Iteration 11000, Testing net (#0)
I0801 13:52:58.686298 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.879413
I0801 13:52:58.686317 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.993529
I0801 13:52:58.686322 24522 solver.cpp:635]     Test net output #2: loss = 0.489197 (* 1 = 0.489197 loss)
I0801 13:52:58.686336 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.809219s
I0801 13:52:58.701864 24549 solver.cpp:450] Finding and applying sparsity: 0.16
I0801 13:53:18.956146 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:53:18.962685 24522 solver.cpp:353] Iteration 11000 (4.41261 iter/s, 22.6623s/100 iter), loss = 0.00418054
I0801 13:53:18.962710 24522 solver.cpp:375]     Train net output #0: loss = 0.0041799 (* 1 = 0.0041799 loss)
I0801 13:53:18.962718 24522 sgd_solver.cpp:136] Iteration 11000, lr = 0.00828125, m = 0.9
I0801 13:53:20.818753 24522 solver.cpp:353] Iteration 11100 (53.879 iter/s, 1.85601s/100 iter), loss = 0.00273855
I0801 13:53:20.818776 24522 solver.cpp:375]     Train net output #0: loss = 0.00273792 (* 1 = 0.00273792 loss)
I0801 13:53:20.818783 24522 sgd_solver.cpp:136] Iteration 11100, lr = 0.00826562, m = 0.9
I0801 13:53:22.397426 24522 solver.cpp:353] Iteration 11200 (63.3464 iter/s, 1.57862s/100 iter), loss = 0.00614682
I0801 13:53:22.397454 24522 solver.cpp:375]     Train net output #0: loss = 0.00614619 (* 1 = 0.00614619 loss)
I0801 13:53:22.397460 24522 sgd_solver.cpp:136] Iteration 11200, lr = 0.00825, m = 0.9
I0801 13:53:23.977322 24522 solver.cpp:353] Iteration 11300 (63.2972 iter/s, 1.57985s/100 iter), loss = 0.0379475
I0801 13:53:23.977350 24522 solver.cpp:375]     Train net output #0: loss = 0.0379469 (* 1 = 0.0379469 loss)
I0801 13:53:23.977356 24522 sgd_solver.cpp:136] Iteration 11300, lr = 0.00823438, m = 0.9
I0801 13:53:25.558254 24522 solver.cpp:353] Iteration 11400 (63.2559 iter/s, 1.58088s/100 iter), loss = 0.00605096
I0801 13:53:25.558303 24522 solver.cpp:375]     Train net output #0: loss = 0.00605031 (* 1 = 0.00605031 loss)
I0801 13:53:25.558315 24522 sgd_solver.cpp:136] Iteration 11400, lr = 0.00821875, m = 0.9
I0801 13:53:27.145300 24522 solver.cpp:353] Iteration 11500 (63.0121 iter/s, 1.587s/100 iter), loss = 0.0296246
I0801 13:53:27.145329 24522 solver.cpp:375]     Train net output #0: loss = 0.029624 (* 1 = 0.029624 loss)
I0801 13:53:27.145336 24522 sgd_solver.cpp:136] Iteration 11500, lr = 0.00820312, m = 0.9
I0801 13:53:28.762676 24522 solver.cpp:353] Iteration 11600 (61.8305 iter/s, 1.61733s/100 iter), loss = 0.00596845
I0801 13:53:28.762701 24522 solver.cpp:375]     Train net output #0: loss = 0.0059678 (* 1 = 0.0059678 loss)
I0801 13:53:28.762706 24522 sgd_solver.cpp:136] Iteration 11600, lr = 0.0081875, m = 0.9
I0801 13:53:30.372643 24522 solver.cpp:353] Iteration 11700 (62.115 iter/s, 1.60992s/100 iter), loss = 0.00206239
I0801 13:53:30.372668 24522 solver.cpp:375]     Train net output #0: loss = 0.00206173 (* 1 = 0.00206173 loss)
I0801 13:53:30.372673 24522 sgd_solver.cpp:136] Iteration 11700, lr = 0.00817188, m = 0.9
I0801 13:53:31.955338 24522 solver.cpp:353] Iteration 11800 (63.1854 iter/s, 1.58264s/100 iter), loss = 0.00612272
I0801 13:53:31.955363 24522 solver.cpp:375]     Train net output #0: loss = 0.00612208 (* 1 = 0.00612208 loss)
I0801 13:53:31.955369 24522 sgd_solver.cpp:136] Iteration 11800, lr = 0.00815625, m = 0.9
I0801 13:53:33.543051 24522 solver.cpp:353] Iteration 11900 (62.9857 iter/s, 1.58766s/100 iter), loss = 0.00345716
I0801 13:53:33.543076 24522 solver.cpp:375]     Train net output #0: loss = 0.00345652 (* 1 = 0.00345652 loss)
I0801 13:53:33.543082 24522 sgd_solver.cpp:136] Iteration 11900, lr = 0.00814062, m = 0.9
I0801 13:53:35.118019 24522 solver.cpp:404] Sparsity after update:
I0801 13:53:35.119595 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:53:35.119603 24522 net.cpp:2270] conv1a_param_0(0.0613) 
I0801 13:53:35.119611 24522 net.cpp:2270] conv1b_param_0(0.115) 
I0801 13:53:35.119612 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:53:35.119616 24522 net.cpp:2270] res2a_branch2a_param_0(0.16) 
I0801 13:53:35.119618 24522 net.cpp:2270] res2a_branch2b_param_0(0.16) 
I0801 13:53:35.119622 24522 net.cpp:2270] res3a_branch2a_param_0(0.16) 
I0801 13:53:35.119626 24522 net.cpp:2270] res3a_branch2b_param_0(0.16) 
I0801 13:53:35.119629 24522 net.cpp:2270] res4a_branch2a_param_0(0.16) 
I0801 13:53:35.119632 24522 net.cpp:2270] res4a_branch2b_param_0(0.16) 
I0801 13:53:35.119637 24522 net.cpp:2270] res5a_branch2a_param_0(0.158) 
I0801 13:53:35.119640 24522 net.cpp:2270] res5a_branch2b_param_0(0.16) 
I0801 13:53:35.119644 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (374110/2.3599e+06) 0.159
I0801 13:53:35.119673 24522 solver.cpp:550] Iteration 12000, Testing net (#0)
I0801 13:53:35.927697 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.886178
I0801 13:53:35.927717 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995294
I0801 13:53:35.927722 24522 solver.cpp:635]     Test net output #2: loss = 0.475343 (* 1 = 0.475343 loss)
I0801 13:53:35.927736 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.808037s
I0801 13:53:35.943308 24549 solver.cpp:450] Finding and applying sparsity: 0.18
I0801 13:53:55.797487 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:53:55.799330 24522 solver.cpp:353] Iteration 12000 (4.49324 iter/s, 22.2557s/100 iter), loss = 0.0141433
I0801 13:53:55.799355 24522 solver.cpp:375]     Train net output #0: loss = 0.0141427 (* 1 = 0.0141427 loss)
I0801 13:53:55.799363 24522 sgd_solver.cpp:136] Iteration 12000, lr = 0.008125, m = 0.9
I0801 13:53:57.640625 24522 solver.cpp:353] Iteration 12100 (54.3113 iter/s, 1.84124s/100 iter), loss = 0.00646575
I0801 13:53:57.640650 24522 solver.cpp:375]     Train net output #0: loss = 0.00646508 (* 1 = 0.00646508 loss)
I0801 13:53:57.640656 24522 sgd_solver.cpp:136] Iteration 12100, lr = 0.00810937, m = 0.9
I0801 13:53:59.226744 24522 solver.cpp:353] Iteration 12200 (63.049 iter/s, 1.58607s/100 iter), loss = 0.00407551
I0801 13:53:59.226771 24522 solver.cpp:375]     Train net output #0: loss = 0.00407485 (* 1 = 0.00407485 loss)
I0801 13:53:59.226778 24522 sgd_solver.cpp:136] Iteration 12200, lr = 0.00809375, m = 0.9
I0801 13:54:00.822548 24522 solver.cpp:353] Iteration 12300 (62.6663 iter/s, 1.59575s/100 iter), loss = 0.0106629
I0801 13:54:00.822573 24522 solver.cpp:375]     Train net output #0: loss = 0.0106622 (* 1 = 0.0106622 loss)
I0801 13:54:00.822579 24522 sgd_solver.cpp:136] Iteration 12300, lr = 0.00807813, m = 0.9
I0801 13:54:02.400616 24522 solver.cpp:353] Iteration 12400 (63.3706 iter/s, 1.57802s/100 iter), loss = 0.00690153
I0801 13:54:02.400676 24522 solver.cpp:375]     Train net output #0: loss = 0.00690087 (* 1 = 0.00690087 loss)
I0801 13:54:02.400694 24522 sgd_solver.cpp:136] Iteration 12400, lr = 0.0080625, m = 0.9
I0801 13:54:03.985097 24522 solver.cpp:353] Iteration 12500 (63.1141 iter/s, 1.58443s/100 iter), loss = 0.000736088
I0801 13:54:03.985123 24522 solver.cpp:375]     Train net output #0: loss = 0.000735427 (* 1 = 0.000735427 loss)
I0801 13:54:03.985128 24522 sgd_solver.cpp:136] Iteration 12500, lr = 0.00804687, m = 0.9
I0801 13:54:05.571285 24522 solver.cpp:353] Iteration 12600 (63.0462 iter/s, 1.58614s/100 iter), loss = 0.129925
I0801 13:54:05.571310 24522 solver.cpp:375]     Train net output #0: loss = 0.129924 (* 1 = 0.129924 loss)
I0801 13:54:05.571315 24522 sgd_solver.cpp:136] Iteration 12600, lr = 0.00803125, m = 0.9
I0801 13:54:07.160998 24522 solver.cpp:353] Iteration 12700 (62.9063 iter/s, 1.58967s/100 iter), loss = 0.00184096
I0801 13:54:07.161023 24522 solver.cpp:375]     Train net output #0: loss = 0.00184029 (* 1 = 0.00184029 loss)
I0801 13:54:07.161028 24522 sgd_solver.cpp:136] Iteration 12700, lr = 0.00801562, m = 0.9
I0801 13:54:08.755039 24522 solver.cpp:353] Iteration 12800 (62.7356 iter/s, 1.59399s/100 iter), loss = 0.00403571
I0801 13:54:08.755067 24522 solver.cpp:375]     Train net output #0: loss = 0.00403503 (* 1 = 0.00403503 loss)
I0801 13:54:08.755074 24522 sgd_solver.cpp:136] Iteration 12800, lr = 0.008, m = 0.9
I0801 13:54:10.333870 24522 solver.cpp:353] Iteration 12900 (63.3401 iter/s, 1.57878s/100 iter), loss = 0.0089635
I0801 13:54:10.333902 24522 solver.cpp:375]     Train net output #0: loss = 0.00896281 (* 1 = 0.00896281 loss)
I0801 13:54:10.333909 24522 sgd_solver.cpp:136] Iteration 12900, lr = 0.00798437, m = 0.9
I0801 13:54:11.898337 24522 solver.cpp:404] Sparsity after update:
I0801 13:54:11.900096 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:54:11.900105 24522 net.cpp:2270] conv1a_param_0(0.0729) 
I0801 13:54:11.900115 24522 net.cpp:2270] conv1b_param_0(0.125) 
I0801 13:54:11.900120 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:54:11.900123 24522 net.cpp:2270] res2a_branch2a_param_0(0.177) 
I0801 13:54:11.900127 24522 net.cpp:2270] res2a_branch2b_param_0(0.174) 
I0801 13:54:11.900131 24522 net.cpp:2270] res3a_branch2a_param_0(0.179) 
I0801 13:54:11.900135 24522 net.cpp:2270] res3a_branch2b_param_0(0.177) 
I0801 13:54:11.900140 24522 net.cpp:2270] res4a_branch2a_param_0(0.18) 
I0801 13:54:11.900144 24522 net.cpp:2270] res4a_branch2b_param_0(0.179) 
I0801 13:54:11.900148 24522 net.cpp:2270] res5a_branch2a_param_0(0.177) 
I0801 13:54:11.900151 24522 net.cpp:2270] res5a_branch2b_param_0(0.179) 
I0801 13:54:11.900156 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (419373/2.3599e+06) 0.178
I0801 13:54:11.900182 24522 solver.cpp:550] Iteration 13000, Testing net (#0)
I0801 13:54:12.707868 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.896472
I0801 13:54:12.707886 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995
I0801 13:54:12.707893 24522 solver.cpp:635]     Test net output #2: loss = 0.416913 (* 1 = 0.416913 loss)
I0801 13:54:12.707914 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.807706s
I0801 13:54:12.725986 24549 solver.cpp:450] Finding and applying sparsity: 0.2
I0801 13:54:32.868176 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:54:32.870106 24522 solver.cpp:353] Iteration 13000 (4.43742 iter/s, 22.5356s/100 iter), loss = 0.0015026
I0801 13:54:32.870126 24522 solver.cpp:375]     Train net output #0: loss = 0.00150192 (* 1 = 0.00150192 loss)
I0801 13:54:32.870133 24522 sgd_solver.cpp:136] Iteration 13000, lr = 0.00796875, m = 0.9
I0801 13:54:34.666558 24522 solver.cpp:353] Iteration 13100 (55.6671 iter/s, 1.79639s/100 iter), loss = 0.0182494
I0801 13:54:34.666582 24522 solver.cpp:375]     Train net output #0: loss = 0.0182487 (* 1 = 0.0182487 loss)
I0801 13:54:34.666589 24522 sgd_solver.cpp:136] Iteration 13100, lr = 0.00795313, m = 0.9
I0801 13:54:36.255295 24522 solver.cpp:353] Iteration 13200 (62.9452 iter/s, 1.58868s/100 iter), loss = 0.00488502
I0801 13:54:36.255364 24522 solver.cpp:375]     Train net output #0: loss = 0.00488433 (* 1 = 0.00488433 loss)
I0801 13:54:36.255383 24522 sgd_solver.cpp:136] Iteration 13200, lr = 0.0079375, m = 0.9
I0801 13:54:37.842905 24522 solver.cpp:353] Iteration 13300 (62.9897 iter/s, 1.58756s/100 iter), loss = 0.00324234
I0801 13:54:37.842954 24522 solver.cpp:375]     Train net output #0: loss = 0.00324164 (* 1 = 0.00324164 loss)
I0801 13:54:37.842967 24522 sgd_solver.cpp:136] Iteration 13300, lr = 0.00792187, m = 0.9
I0801 13:54:39.423236 24522 solver.cpp:353] Iteration 13400 (63.2799 iter/s, 1.58028s/100 iter), loss = 0.000507683
I0801 13:54:39.423265 24522 solver.cpp:375]     Train net output #0: loss = 0.000506988 (* 1 = 0.000506988 loss)
I0801 13:54:39.423272 24522 sgd_solver.cpp:136] Iteration 13400, lr = 0.00790625, m = 0.9
I0801 13:54:40.998116 24522 solver.cpp:353] Iteration 13500 (63.499 iter/s, 1.57483s/100 iter), loss = 0.00423786
I0801 13:54:40.998138 24522 solver.cpp:375]     Train net output #0: loss = 0.00423716 (* 1 = 0.00423716 loss)
I0801 13:54:40.998142 24522 sgd_solver.cpp:136] Iteration 13500, lr = 0.00789062, m = 0.9
I0801 13:54:42.579310 24522 solver.cpp:353] Iteration 13600 (63.2452 iter/s, 1.58115s/100 iter), loss = 0.000787131
I0801 13:54:42.579335 24522 solver.cpp:375]     Train net output #0: loss = 0.000786428 (* 1 = 0.000786428 loss)
I0801 13:54:42.579341 24522 sgd_solver.cpp:136] Iteration 13600, lr = 0.007875, m = 0.9
I0801 13:54:43.100240 24487 data_reader.cpp:264] Starting prefetch of epoch 3
I0801 13:54:44.164624 24522 solver.cpp:353] Iteration 13700 (63.0809 iter/s, 1.58527s/100 iter), loss = 0.00111901
I0801 13:54:44.164652 24522 solver.cpp:375]     Train net output #0: loss = 0.00111831 (* 1 = 0.00111831 loss)
I0801 13:54:44.164660 24522 sgd_solver.cpp:136] Iteration 13700, lr = 0.00785937, m = 0.9
I0801 13:54:45.757823 24522 solver.cpp:353] Iteration 13800 (62.7688 iter/s, 1.59315s/100 iter), loss = 0.00202114
I0801 13:54:45.757853 24522 solver.cpp:375]     Train net output #0: loss = 0.00202045 (* 1 = 0.00202045 loss)
I0801 13:54:45.757858 24522 sgd_solver.cpp:136] Iteration 13800, lr = 0.00784375, m = 0.9
I0801 13:54:47.347936 24522 solver.cpp:353] Iteration 13900 (62.8906 iter/s, 1.59006s/100 iter), loss = 0.00124864
I0801 13:54:47.347959 24522 solver.cpp:375]     Train net output #0: loss = 0.00124795 (* 1 = 0.00124795 loss)
I0801 13:54:47.347964 24522 sgd_solver.cpp:136] Iteration 13900, lr = 0.00782812, m = 0.9
I0801 13:54:48.899562 24522 solver.cpp:404] Sparsity after update:
I0801 13:54:48.901207 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:54:48.901216 24522 net.cpp:2270] conv1a_param_0(0.0838) 
I0801 13:54:48.901222 24522 net.cpp:2270] conv1b_param_0(0.146) 
I0801 13:54:48.901224 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:54:48.901228 24522 net.cpp:2270] res2a_branch2a_param_0(0.198) 
I0801 13:54:48.901232 24522 net.cpp:2270] res2a_branch2b_param_0(0.194) 
I0801 13:54:48.901237 24522 net.cpp:2270] res3a_branch2a_param_0(0.2) 
I0801 13:54:48.901240 24522 net.cpp:2270] res3a_branch2b_param_0(0.198) 
I0801 13:54:48.901242 24522 net.cpp:2270] res4a_branch2a_param_0(0.2) 
I0801 13:54:48.901245 24522 net.cpp:2270] res4a_branch2b_param_0(0.2) 
I0801 13:54:48.901249 24522 net.cpp:2270] res5a_branch2a_param_0(0.198) 
I0801 13:54:48.901253 24522 net.cpp:2270] res5a_branch2b_param_0(0.2) 
I0801 13:54:48.901268 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (468129/2.3599e+06) 0.198
I0801 13:54:48.901278 24522 solver.cpp:550] Iteration 14000, Testing net (#0)
I0801 13:54:49.716732 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.897942
I0801 13:54:49.716753 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.993824
I0801 13:54:49.716758 24522 solver.cpp:635]     Test net output #2: loss = 0.434368 (* 1 = 0.434368 loss)
I0801 13:54:49.716771 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.815466s
I0801 13:54:49.732343 24549 solver.cpp:450] Finding and applying sparsity: 0.22
I0801 13:55:10.056449 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:55:10.058365 24522 solver.cpp:353] Iteration 14000 (4.40339 iter/s, 22.7098s/100 iter), loss = 0.0746899
I0801 13:55:10.058384 24522 solver.cpp:375]     Train net output #0: loss = 0.0746892 (* 1 = 0.0746892 loss)
I0801 13:55:10.058390 24522 sgd_solver.cpp:136] Iteration 14000, lr = 0.0078125, m = 0.9
I0801 13:55:11.886555 24522 solver.cpp:353] Iteration 14100 (54.7006 iter/s, 1.82813s/100 iter), loss = 0.00371545
I0801 13:55:11.886582 24522 solver.cpp:375]     Train net output #0: loss = 0.00371474 (* 1 = 0.00371474 loss)
I0801 13:55:11.886589 24522 sgd_solver.cpp:136] Iteration 14100, lr = 0.00779688, m = 0.9
I0801 13:55:13.489128 24522 solver.cpp:353] Iteration 14200 (62.4017 iter/s, 1.60252s/100 iter), loss = 0.00108607
I0801 13:55:13.489173 24522 solver.cpp:375]     Train net output #0: loss = 0.00108537 (* 1 = 0.00108537 loss)
I0801 13:55:13.489186 24522 sgd_solver.cpp:136] Iteration 14200, lr = 0.00778125, m = 0.9
I0801 13:55:15.073106 24522 solver.cpp:353] Iteration 14300 (63.1343 iter/s, 1.58392s/100 iter), loss = 0.00541784
I0801 13:55:15.073134 24522 solver.cpp:375]     Train net output #0: loss = 0.00541714 (* 1 = 0.00541714 loss)
I0801 13:55:15.073141 24522 sgd_solver.cpp:136] Iteration 14300, lr = 0.00776563, m = 0.9
I0801 13:55:16.650166 24522 solver.cpp:353] Iteration 14400 (63.411 iter/s, 1.57701s/100 iter), loss = 0.00494886
I0801 13:55:16.650195 24522 solver.cpp:375]     Train net output #0: loss = 0.00494817 (* 1 = 0.00494817 loss)
I0801 13:55:16.650200 24522 sgd_solver.cpp:136] Iteration 14400, lr = 0.00775, m = 0.9
I0801 13:55:18.222170 24522 solver.cpp:353] Iteration 14500 (63.615 iter/s, 1.57196s/100 iter), loss = 0.000253498
I0801 13:55:18.222218 24522 solver.cpp:375]     Train net output #0: loss = 0.000252815 (* 1 = 0.000252815 loss)
I0801 13:55:18.222230 24522 sgd_solver.cpp:136] Iteration 14500, lr = 0.00773437, m = 0.9
I0801 13:55:19.824872 24522 solver.cpp:353] Iteration 14600 (62.3967 iter/s, 1.60265s/100 iter), loss = 0.00158708
I0801 13:55:19.824921 24522 solver.cpp:375]     Train net output #0: loss = 0.0015864 (* 1 = 0.0015864 loss)
I0801 13:55:19.824934 24522 sgd_solver.cpp:136] Iteration 14600, lr = 0.00771875, m = 0.9
I0801 13:55:21.410377 24522 solver.cpp:353] Iteration 14700 (63.0734 iter/s, 1.58545s/100 iter), loss = 0.00110149
I0801 13:55:21.410410 24522 solver.cpp:375]     Train net output #0: loss = 0.0011008 (* 1 = 0.0011008 loss)
I0801 13:55:21.410418 24522 sgd_solver.cpp:136] Iteration 14700, lr = 0.00770312, m = 0.9
I0801 13:55:22.980284 24522 solver.cpp:353] Iteration 14800 (63.7 iter/s, 1.56986s/100 iter), loss = 0.00253098
I0801 13:55:22.980309 24522 solver.cpp:375]     Train net output #0: loss = 0.00253029 (* 1 = 0.00253029 loss)
I0801 13:55:22.980316 24522 sgd_solver.cpp:136] Iteration 14800, lr = 0.0076875, m = 0.9
I0801 13:55:24.583889 24522 solver.cpp:353] Iteration 14900 (62.3615 iter/s, 1.60355s/100 iter), loss = 0.00933057
I0801 13:55:24.583915 24522 solver.cpp:375]     Train net output #0: loss = 0.00932988 (* 1 = 0.00932988 loss)
I0801 13:55:24.583920 24522 sgd_solver.cpp:136] Iteration 14900, lr = 0.00767187, m = 0.9
I0801 13:55:26.157485 24522 solver.cpp:404] Sparsity after update:
I0801 13:55:26.159116 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:55:26.159123 24522 net.cpp:2270] conv1a_param_0(0.0954) 
I0801 13:55:26.159129 24522 net.cpp:2270] conv1b_param_0(0.156) 
I0801 13:55:26.159133 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:55:26.159137 24522 net.cpp:2270] res2a_branch2a_param_0(0.219) 
I0801 13:55:26.159139 24522 net.cpp:2270] res2a_branch2b_param_0(0.215) 
I0801 13:55:26.159143 24522 net.cpp:2270] res3a_branch2a_param_0(0.219) 
I0801 13:55:26.159152 24522 net.cpp:2270] res3a_branch2b_param_0(0.219) 
I0801 13:55:26.159157 24522 net.cpp:2270] res4a_branch2a_param_0(0.22) 
I0801 13:55:26.159162 24522 net.cpp:2270] res4a_branch2b_param_0(0.219) 
I0801 13:55:26.159164 24522 net.cpp:2270] res5a_branch2a_param_0(0.215) 
I0801 13:55:26.159170 24522 net.cpp:2270] res5a_branch2b_param_0(0.219) 
I0801 13:55:26.159175 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (510611/2.3599e+06) 0.216
I0801 13:55:26.159198 24522 solver.cpp:550] Iteration 15000, Testing net (#0)
I0801 13:55:26.971050 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.892942
I0801 13:55:26.971070 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995294
I0801 13:55:26.971076 24522 solver.cpp:635]     Test net output #2: loss = 0.444005 (* 1 = 0.444005 loss)
I0801 13:55:26.971092 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.811868s
I0801 13:55:26.986596 24549 solver.cpp:450] Finding and applying sparsity: 0.24
I0801 13:55:46.458355 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:55:46.460271 24522 solver.cpp:353] Iteration 15000 (4.57127 iter/s, 21.8758s/100 iter), loss = 0.00190435
I0801 13:55:46.460289 24522 solver.cpp:375]     Train net output #0: loss = 0.00190366 (* 1 = 0.00190366 loss)
I0801 13:55:46.460295 24522 sgd_solver.cpp:136] Iteration 15000, lr = 0.00765625, m = 0.9
I0801 13:55:48.317044 24522 solver.cpp:353] Iteration 15100 (53.8586 iter/s, 1.85672s/100 iter), loss = 0.000148196
I0801 13:55:48.317068 24522 solver.cpp:375]     Train net output #0: loss = 0.000147504 (* 1 = 0.000147504 loss)
I0801 13:55:48.317073 24522 sgd_solver.cpp:136] Iteration 15100, lr = 0.00764062, m = 0.9
I0801 13:55:49.914598 24522 solver.cpp:353] Iteration 15200 (62.5977 iter/s, 1.5975s/100 iter), loss = 0.0015119
I0801 13:55:49.914645 24522 solver.cpp:375]     Train net output #0: loss = 0.0015112 (* 1 = 0.0015112 loss)
I0801 13:55:49.914657 24522 sgd_solver.cpp:136] Iteration 15200, lr = 0.007625, m = 0.9
I0801 13:55:51.504467 24522 solver.cpp:353] Iteration 15300 (62.9003 iter/s, 1.58982s/100 iter), loss = 0.00547053
I0801 13:55:51.504523 24522 solver.cpp:375]     Train net output #0: loss = 0.00546984 (* 1 = 0.00546984 loss)
I0801 13:55:51.504539 24522 sgd_solver.cpp:136] Iteration 15300, lr = 0.00760937, m = 0.9
I0801 13:55:53.081877 24522 solver.cpp:353] Iteration 15400 (63.3971 iter/s, 1.57736s/100 iter), loss = 0.0100126
I0801 13:55:53.081904 24522 solver.cpp:375]     Train net output #0: loss = 0.0100119 (* 1 = 0.0100119 loss)
I0801 13:55:53.081908 24522 sgd_solver.cpp:136] Iteration 15400, lr = 0.00759375, m = 0.9
I0801 13:55:54.679930 24522 solver.cpp:353] Iteration 15500 (62.5782 iter/s, 1.598s/100 iter), loss = 0.00443748
I0801 13:55:54.679985 24522 solver.cpp:375]     Train net output #0: loss = 0.00443678 (* 1 = 0.00443678 loss)
I0801 13:55:54.679999 24522 sgd_solver.cpp:136] Iteration 15500, lr = 0.00757812, m = 0.9
I0801 13:55:56.289757 24522 solver.cpp:353] Iteration 15600 (62.1204 iter/s, 1.60978s/100 iter), loss = 0.00679582
I0801 13:55:56.289780 24522 solver.cpp:375]     Train net output #0: loss = 0.00679514 (* 1 = 0.00679514 loss)
I0801 13:55:56.289785 24522 sgd_solver.cpp:136] Iteration 15600, lr = 0.0075625, m = 0.9
I0801 13:55:57.877805 24522 solver.cpp:353] Iteration 15700 (62.9724 iter/s, 1.588s/100 iter), loss = 0.000648394
I0801 13:55:57.877830 24522 solver.cpp:375]     Train net output #0: loss = 0.000647708 (* 1 = 0.000647708 loss)
I0801 13:55:57.877833 24522 sgd_solver.cpp:136] Iteration 15700, lr = 0.00754687, m = 0.9
I0801 13:55:59.468271 24522 solver.cpp:353] Iteration 15800 (62.8765 iter/s, 1.59042s/100 iter), loss = 0.000742716
I0801 13:55:59.468298 24522 solver.cpp:375]     Train net output #0: loss = 0.000742026 (* 1 = 0.000742026 loss)
I0801 13:55:59.468304 24522 sgd_solver.cpp:136] Iteration 15800, lr = 0.00753125, m = 0.9
I0801 13:56:01.057001 24522 solver.cpp:353] Iteration 15900 (62.9453 iter/s, 1.58868s/100 iter), loss = 0.00565604
I0801 13:56:01.057025 24522 solver.cpp:375]     Train net output #0: loss = 0.00565536 (* 1 = 0.00565536 loss)
I0801 13:56:01.057029 24522 sgd_solver.cpp:136] Iteration 15900, lr = 0.00751562, m = 0.9
I0801 13:56:02.623759 24522 solver.cpp:404] Sparsity after update:
I0801 13:56:02.625677 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:56:02.625689 24522 net.cpp:2270] conv1a_param_0(0.0962) 
I0801 13:56:02.625697 24522 net.cpp:2270] conv1b_param_0(0.177) 
I0801 13:56:02.625702 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:56:02.625707 24522 net.cpp:2270] res2a_branch2a_param_0(0.24) 
I0801 13:56:02.625712 24522 net.cpp:2270] res2a_branch2b_param_0(0.236) 
I0801 13:56:02.625717 24522 net.cpp:2270] res3a_branch2a_param_0(0.24) 
I0801 13:56:02.625721 24522 net.cpp:2270] res3a_branch2b_param_0(0.24) 
I0801 13:56:02.625726 24522 net.cpp:2270] res4a_branch2a_param_0(0.24) 
I0801 13:56:02.625730 24522 net.cpp:2270] res4a_branch2b_param_0(0.24) 
I0801 13:56:02.625735 24522 net.cpp:2270] res5a_branch2a_param_0(0.236) 
I0801 13:56:02.625738 24522 net.cpp:2270] res5a_branch2b_param_0(0.239) 
I0801 13:56:02.625743 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (559723/2.3599e+06) 0.237
I0801 13:56:02.625771 24522 solver.cpp:550] Iteration 16000, Testing net (#0)
I0801 13:56:03.476935 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.905295
I0801 13:56:03.476953 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 13:56:03.476958 24522 solver.cpp:635]     Test net output #2: loss = 0.382189 (* 1 = 0.382189 loss)
I0801 13:56:03.476974 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.851174s
I0801 13:56:03.497334 24549 solver.cpp:450] Finding and applying sparsity: 0.26
I0801 13:56:21.726263 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:56:21.728179 24522 solver.cpp:353] Iteration 16000 (4.83779 iter/s, 20.6706s/100 iter), loss = 0.00498618
I0801 13:56:21.728199 24522 solver.cpp:375]     Train net output #0: loss = 0.0049855 (* 1 = 0.0049855 loss)
I0801 13:56:21.728204 24522 sgd_solver.cpp:136] Iteration 16000, lr = 0.0075, m = 0.9
I0801 13:56:23.539106 24522 solver.cpp:353] Iteration 16100 (55.2221 iter/s, 1.81087s/100 iter), loss = 0.000448718
I0801 13:56:23.539129 24522 solver.cpp:375]     Train net output #0: loss = 0.000448034 (* 1 = 0.000448034 loss)
I0801 13:56:23.539136 24522 sgd_solver.cpp:136] Iteration 16100, lr = 0.00748438, m = 0.9
I0801 13:56:25.124323 24522 solver.cpp:353] Iteration 16200 (63.0848 iter/s, 1.58517s/100 iter), loss = 0.00204645
I0801 13:56:25.124349 24522 solver.cpp:375]     Train net output #0: loss = 0.00204577 (* 1 = 0.00204577 loss)
I0801 13:56:25.124356 24522 sgd_solver.cpp:136] Iteration 16200, lr = 0.00746875, m = 0.9
I0801 13:56:26.715216 24522 solver.cpp:353] Iteration 16300 (62.8597 iter/s, 1.59084s/100 iter), loss = 0.00216135
I0801 13:56:26.715245 24522 solver.cpp:375]     Train net output #0: loss = 0.00216066 (* 1 = 0.00216066 loss)
I0801 13:56:26.715250 24522 sgd_solver.cpp:136] Iteration 16300, lr = 0.00745312, m = 0.9
I0801 13:56:28.322585 24522 solver.cpp:353] Iteration 16400 (62.2154 iter/s, 1.60732s/100 iter), loss = 0.000297942
I0801 13:56:28.322613 24522 solver.cpp:375]     Train net output #0: loss = 0.000297254 (* 1 = 0.000297254 loss)
I0801 13:56:28.322618 24522 sgd_solver.cpp:136] Iteration 16400, lr = 0.0074375, m = 0.9
I0801 13:56:29.910639 24522 solver.cpp:353] Iteration 16500 (62.9722 iter/s, 1.588s/100 iter), loss = 0.00249876
I0801 13:56:29.910666 24522 solver.cpp:375]     Train net output #0: loss = 0.00249807 (* 1 = 0.00249807 loss)
I0801 13:56:29.910675 24522 sgd_solver.cpp:136] Iteration 16500, lr = 0.00742187, m = 0.9
I0801 13:56:31.492115 24522 solver.cpp:353] Iteration 16600 (63.2341 iter/s, 1.58143s/100 iter), loss = 0.0026068
I0801 13:56:31.492141 24522 solver.cpp:375]     Train net output #0: loss = 0.00260611 (* 1 = 0.00260611 loss)
I0801 13:56:31.492147 24522 sgd_solver.cpp:136] Iteration 16600, lr = 0.00740625, m = 0.9
I0801 13:56:33.075289 24522 solver.cpp:353] Iteration 16700 (63.1661 iter/s, 1.58313s/100 iter), loss = 0.0143397
I0801 13:56:33.075337 24522 solver.cpp:375]     Train net output #0: loss = 0.0143391 (* 1 = 0.0143391 loss)
I0801 13:56:33.075350 24522 sgd_solver.cpp:136] Iteration 16700, lr = 0.00739062, m = 0.9
I0801 13:56:34.683002 24522 solver.cpp:353] Iteration 16800 (62.2021 iter/s, 1.60766s/100 iter), loss = 0.00202817
I0801 13:56:34.683027 24522 solver.cpp:375]     Train net output #0: loss = 0.00202749 (* 1 = 0.00202749 loss)
I0801 13:56:34.683032 24522 sgd_solver.cpp:136] Iteration 16800, lr = 0.007375, m = 0.9
I0801 13:56:36.256855 24522 solver.cpp:353] Iteration 16900 (63.5404 iter/s, 1.5738s/100 iter), loss = 0.00129148
I0801 13:56:36.256882 24522 solver.cpp:375]     Train net output #0: loss = 0.0012908 (* 1 = 0.0012908 loss)
I0801 13:56:36.256888 24522 sgd_solver.cpp:136] Iteration 16900, lr = 0.00735937, m = 0.9
I0801 13:56:37.830687 24522 solver.cpp:404] Sparsity after update:
I0801 13:56:37.832265 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:56:37.832273 24522 net.cpp:2270] conv1a_param_0(0.104) 
I0801 13:56:37.832283 24522 net.cpp:2270] conv1b_param_0(0.187) 
I0801 13:56:37.832288 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:56:37.832293 24522 net.cpp:2270] res2a_branch2a_param_0(0.257) 
I0801 13:56:37.832300 24522 net.cpp:2270] res2a_branch2b_param_0(0.257) 
I0801 13:56:37.832304 24522 net.cpp:2270] res3a_branch2a_param_0(0.259) 
I0801 13:56:37.832307 24522 net.cpp:2270] res3a_branch2b_param_0(0.257) 
I0801 13:56:37.832309 24522 net.cpp:2270] res4a_branch2a_param_0(0.26) 
I0801 13:56:37.832314 24522 net.cpp:2270] res4a_branch2b_param_0(0.259) 
I0801 13:56:37.832316 24522 net.cpp:2270] res5a_branch2a_param_0(0.253) 
I0801 13:56:37.832320 24522 net.cpp:2270] res5a_branch2b_param_0(0.258) 
I0801 13:56:37.832325 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (602331/2.3599e+06) 0.255
I0801 13:56:37.832350 24522 solver.cpp:550] Iteration 17000, Testing net (#0)
I0801 13:56:38.640887 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.916766
I0801 13:56:38.640907 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 13:56:38.640913 24522 solver.cpp:635]     Test net output #2: loss = 0.331567 (* 1 = 0.331567 loss)
I0801 13:56:38.640929 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.808553s
I0801 13:56:38.656332 24549 solver.cpp:450] Finding and applying sparsity: 0.28
I0801 13:56:55.725636 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:56:55.727560 24522 solver.cpp:353] Iteration 17000 (5.13606 iter/s, 19.4702s/100 iter), loss = 0.000756713
I0801 13:56:55.727576 24522 solver.cpp:375]     Train net output #0: loss = 0.000756031 (* 1 = 0.000756031 loss)
I0801 13:56:55.727581 24522 sgd_solver.cpp:136] Iteration 17000, lr = 0.00734375, m = 0.9
I0801 13:56:57.569291 24522 solver.cpp:353] Iteration 17100 (54.2984 iter/s, 1.84167s/100 iter), loss = 0.00433522
I0801 13:56:57.569315 24522 solver.cpp:375]     Train net output #0: loss = 0.00433454 (* 1 = 0.00433454 loss)
I0801 13:56:57.569321 24522 sgd_solver.cpp:136] Iteration 17100, lr = 0.00732813, m = 0.9
I0801 13:56:59.154312 24522 solver.cpp:353] Iteration 17200 (63.0927 iter/s, 1.58497s/100 iter), loss = 0.000114296
I0801 13:56:59.154338 24522 solver.cpp:375]     Train net output #0: loss = 0.000113617 (* 1 = 0.000113617 loss)
I0801 13:56:59.154345 24522 sgd_solver.cpp:136] Iteration 17200, lr = 0.0073125, m = 0.9
I0801 13:57:00.737691 24522 solver.cpp:353] Iteration 17300 (63.158 iter/s, 1.58333s/100 iter), loss = 0.000381985
I0801 13:57:00.737717 24522 solver.cpp:375]     Train net output #0: loss = 0.000381311 (* 1 = 0.000381311 loss)
I0801 13:57:00.737722 24522 sgd_solver.cpp:136] Iteration 17300, lr = 0.00729688, m = 0.9
I0801 13:57:02.313124 24522 solver.cpp:353] Iteration 17400 (63.4766 iter/s, 1.57538s/100 iter), loss = 0.00426756
I0801 13:57:02.313148 24522 solver.cpp:375]     Train net output #0: loss = 0.00426689 (* 1 = 0.00426689 loss)
I0801 13:57:02.313151 24522 sgd_solver.cpp:136] Iteration 17400, lr = 0.00728125, m = 0.9
I0801 13:57:03.922804 24522 solver.cpp:353] Iteration 17500 (62.126 iter/s, 1.60963s/100 iter), loss = 0.00130369
I0801 13:57:03.922832 24522 solver.cpp:375]     Train net output #0: loss = 0.00130301 (* 1 = 0.00130301 loss)
I0801 13:57:03.922837 24522 sgd_solver.cpp:136] Iteration 17500, lr = 0.00726563, m = 0.9
I0801 13:57:05.513883 24522 solver.cpp:353] Iteration 17600 (62.8525 iter/s, 1.59103s/100 iter), loss = 0.00159859
I0801 13:57:05.513907 24522 solver.cpp:375]     Train net output #0: loss = 0.00159792 (* 1 = 0.00159792 loss)
I0801 13:57:05.513911 24522 sgd_solver.cpp:136] Iteration 17600, lr = 0.00725, m = 0.9
I0801 13:57:07.091053 24522 solver.cpp:353] Iteration 17700 (63.4067 iter/s, 1.57712s/100 iter), loss = 0.000897399
I0801 13:57:07.091079 24522 solver.cpp:375]     Train net output #0: loss = 0.000896723 (* 1 = 0.000896723 loss)
I0801 13:57:07.091085 24522 sgd_solver.cpp:136] Iteration 17700, lr = 0.00723437, m = 0.9
I0801 13:57:08.676369 24522 solver.cpp:353] Iteration 17800 (63.0809 iter/s, 1.58527s/100 iter), loss = 0.000582915
I0801 13:57:08.676391 24522 solver.cpp:375]     Train net output #0: loss = 0.000582237 (* 1 = 0.000582237 loss)
I0801 13:57:08.676398 24522 sgd_solver.cpp:136] Iteration 17800, lr = 0.00721875, m = 0.9
I0801 13:57:10.255614 24522 solver.cpp:353] Iteration 17900 (63.3234 iter/s, 1.5792s/100 iter), loss = 0.00998031
I0801 13:57:10.255638 24522 solver.cpp:375]     Train net output #0: loss = 0.00997963 (* 1 = 0.00997963 loss)
I0801 13:57:10.255645 24522 sgd_solver.cpp:136] Iteration 17900, lr = 0.00720312, m = 0.9
I0801 13:57:11.841758 24522 solver.cpp:404] Sparsity after update:
I0801 13:57:11.843515 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:57:11.843523 24522 net.cpp:2270] conv1a_param_0(0.115) 
I0801 13:57:11.843528 24522 net.cpp:2270] conv1b_param_0(0.208) 
I0801 13:57:11.843531 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:57:11.843533 24522 net.cpp:2270] res2a_branch2a_param_0(0.278) 
I0801 13:57:11.843536 24522 net.cpp:2270] res2a_branch2b_param_0(0.278) 
I0801 13:57:11.843538 24522 net.cpp:2270] res3a_branch2a_param_0(0.28) 
I0801 13:57:11.843540 24522 net.cpp:2270] res3a_branch2b_param_0(0.278) 
I0801 13:57:11.843542 24522 net.cpp:2270] res4a_branch2a_param_0(0.28) 
I0801 13:57:11.843544 24522 net.cpp:2270] res4a_branch2b_param_0(0.28) 
I0801 13:57:11.843546 24522 net.cpp:2270] res5a_branch2a_param_0(0.273) 
I0801 13:57:11.843549 24522 net.cpp:2270] res5a_branch2b_param_0(0.277) 
I0801 13:57:11.843551 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (648018/2.3599e+06) 0.275
I0801 13:57:11.843580 24522 solver.cpp:550] Iteration 18000, Testing net (#0)
I0801 13:57:12.316167 24520 data_reader.cpp:264] Starting prefetch of epoch 2
I0801 13:57:12.651342 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.918824
I0801 13:57:12.651361 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 13:57:12.651367 24522 solver.cpp:635]     Test net output #2: loss = 0.344375 (* 1 = 0.344375 loss)
I0801 13:57:12.651386 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.807777s
I0801 13:57:12.666775 24549 solver.cpp:450] Finding and applying sparsity: 0.3
I0801 13:57:30.058078 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:57:30.059989 24522 solver.cpp:353] Iteration 18000 (5.04953 iter/s, 19.8038s/100 iter), loss = 0.00144948
I0801 13:57:30.060006 24522 solver.cpp:375]     Train net output #0: loss = 0.0014488 (* 1 = 0.0014488 loss)
I0801 13:57:30.060011 24522 sgd_solver.cpp:136] Iteration 18000, lr = 0.0071875, m = 0.9
I0801 13:57:31.906461 24522 solver.cpp:353] Iteration 18100 (54.1591 iter/s, 1.84641s/100 iter), loss = 0.00037554
I0801 13:57:31.906487 24522 solver.cpp:375]     Train net output #0: loss = 0.000374862 (* 1 = 0.000374862 loss)
I0801 13:57:31.906493 24522 sgd_solver.cpp:136] Iteration 18100, lr = 0.00717187, m = 0.9
I0801 13:57:33.497629 24522 solver.cpp:353] Iteration 18200 (62.849 iter/s, 1.59111s/100 iter), loss = 0.000225688
I0801 13:57:33.497658 24522 solver.cpp:375]     Train net output #0: loss = 0.000225008 (* 1 = 0.000225008 loss)
I0801 13:57:33.497664 24522 sgd_solver.cpp:136] Iteration 18200, lr = 0.00715625, m = 0.9
I0801 13:57:35.080292 24522 solver.cpp:353] Iteration 18300 (63.1866 iter/s, 1.58262s/100 iter), loss = 0.00360334
I0801 13:57:35.080320 24522 solver.cpp:375]     Train net output #0: loss = 0.00360265 (* 1 = 0.00360265 loss)
I0801 13:57:35.080327 24522 sgd_solver.cpp:136] Iteration 18300, lr = 0.00714062, m = 0.9
I0801 13:57:36.662710 24522 solver.cpp:353] Iteration 18400 (63.1964 iter/s, 1.58237s/100 iter), loss = 0.00109441
I0801 13:57:36.662734 24522 solver.cpp:375]     Train net output #0: loss = 0.00109373 (* 1 = 0.00109373 loss)
I0801 13:57:36.662739 24522 sgd_solver.cpp:136] Iteration 18400, lr = 0.007125, m = 0.9
I0801 13:57:38.255584 24522 solver.cpp:353] Iteration 18500 (62.7816 iter/s, 1.59282s/100 iter), loss = 0.000474275
I0801 13:57:38.255633 24522 solver.cpp:375]     Train net output #0: loss = 0.000473593 (* 1 = 0.000473593 loss)
I0801 13:57:38.255645 24522 sgd_solver.cpp:136] Iteration 18500, lr = 0.00710937, m = 0.9
I0801 13:57:39.841053 24522 solver.cpp:353] Iteration 18600 (63.0748 iter/s, 1.58542s/100 iter), loss = 0.0027402
I0801 13:57:39.841078 24522 solver.cpp:375]     Train net output #0: loss = 0.00273952 (* 1 = 0.00273952 loss)
I0801 13:57:39.841084 24522 sgd_solver.cpp:136] Iteration 18600, lr = 0.00709375, m = 0.9
I0801 13:57:41.435984 24522 solver.cpp:353] Iteration 18700 (62.7006 iter/s, 1.59488s/100 iter), loss = 0.000393333
I0801 13:57:41.436009 24522 solver.cpp:375]     Train net output #0: loss = 0.000392651 (* 1 = 0.000392651 loss)
I0801 13:57:41.436015 24522 sgd_solver.cpp:136] Iteration 18700, lr = 0.00707812, m = 0.9
I0801 13:57:43.007207 24522 solver.cpp:353] Iteration 18800 (63.6467 iter/s, 1.57117s/100 iter), loss = 0.000203356
I0801 13:57:43.007233 24522 solver.cpp:375]     Train net output #0: loss = 0.000202672 (* 1 = 0.000202672 loss)
I0801 13:57:43.007238 24522 sgd_solver.cpp:136] Iteration 18800, lr = 0.0070625, m = 0.9
I0801 13:57:44.594230 24522 solver.cpp:353] Iteration 18900 (63.0131 iter/s, 1.58697s/100 iter), loss = 0.00201048
I0801 13:57:44.594255 24522 solver.cpp:375]     Train net output #0: loss = 0.0020098 (* 1 = 0.0020098 loss)
I0801 13:57:44.594261 24522 sgd_solver.cpp:136] Iteration 18900, lr = 0.00704687, m = 0.9
I0801 13:57:46.166934 24522 solver.cpp:404] Sparsity after update:
I0801 13:57:46.168529 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:57:46.168537 24522 net.cpp:2270] conv1a_param_0(0.129) 
I0801 13:57:46.168543 24522 net.cpp:2270] conv1b_param_0(0.219) 
I0801 13:57:46.168546 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:57:46.168548 24522 net.cpp:2270] res2a_branch2a_param_0(0.299) 
I0801 13:57:46.168550 24522 net.cpp:2270] res2a_branch2b_param_0(0.299) 
I0801 13:57:46.168552 24522 net.cpp:2270] res3a_branch2a_param_0(0.299) 
I0801 13:57:46.168555 24522 net.cpp:2270] res3a_branch2b_param_0(0.299) 
I0801 13:57:46.168556 24522 net.cpp:2270] res4a_branch2a_param_0(0.299) 
I0801 13:57:46.168558 24522 net.cpp:2270] res4a_branch2b_param_0(0.299) 
I0801 13:57:46.168560 24522 net.cpp:2270] res5a_branch2a_param_0(0.293) 
I0801 13:57:46.168562 24522 net.cpp:2270] res5a_branch2b_param_0(0.298) 
I0801 13:57:46.168565 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (695432/2.3599e+06) 0.295
I0801 13:57:46.168584 24522 solver.cpp:550] Iteration 19000, Testing net (#0)
I0801 13:57:46.978953 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.915001
I0801 13:57:46.978971 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 13:57:46.978976 24522 solver.cpp:635]     Test net output #2: loss = 0.352645 (* 1 = 0.352645 loss)
I0801 13:57:46.978991 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.81038s
I0801 13:57:46.994683 24549 solver.cpp:450] Finding and applying sparsity: 0.32
I0801 13:58:02.749421 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:58:02.751353 24522 solver.cpp:353] Iteration 19000 (5.50763 iter/s, 18.1566s/100 iter), loss = 0.000496591
I0801 13:58:02.751372 24522 solver.cpp:375]     Train net output #0: loss = 0.000495906 (* 1 = 0.000495906 loss)
I0801 13:58:02.751379 24522 sgd_solver.cpp:136] Iteration 19000, lr = 0.00703125, m = 0.9
I0801 13:58:04.537607 24522 solver.cpp:353] Iteration 19100 (55.9848 iter/s, 1.7862s/100 iter), loss = 0.000308639
I0801 13:58:04.537859 24522 solver.cpp:375]     Train net output #0: loss = 0.000307952 (* 1 = 0.000307952 loss)
I0801 13:58:04.537866 24522 sgd_solver.cpp:136] Iteration 19100, lr = 0.00701563, m = 0.9
I0801 13:58:06.115314 24522 solver.cpp:353] Iteration 19200 (63.3851 iter/s, 1.57766s/100 iter), loss = 0.00019857
I0801 13:58:06.115365 24522 solver.cpp:375]     Train net output #0: loss = 0.000197882 (* 1 = 0.000197882 loss)
I0801 13:58:06.115378 24522 sgd_solver.cpp:136] Iteration 19200, lr = 0.007, m = 0.9
I0801 13:58:07.705724 24522 solver.cpp:353] Iteration 19300 (62.8789 iter/s, 1.59036s/100 iter), loss = 0.0019933
I0801 13:58:07.705770 24522 solver.cpp:375]     Train net output #0: loss = 0.00199261 (* 1 = 0.00199261 loss)
I0801 13:58:07.705785 24522 sgd_solver.cpp:136] Iteration 19300, lr = 0.00698437, m = 0.9
I0801 13:58:09.286679 24522 solver.cpp:353] Iteration 19400 (63.2549 iter/s, 1.58091s/100 iter), loss = 0.00596585
I0801 13:58:09.286701 24522 solver.cpp:375]     Train net output #0: loss = 0.00596516 (* 1 = 0.00596516 loss)
I0801 13:58:09.286707 24522 sgd_solver.cpp:136] Iteration 19400, lr = 0.00696875, m = 0.9
I0801 13:58:10.888733 24522 solver.cpp:353] Iteration 19500 (62.4218 iter/s, 1.60201s/100 iter), loss = 0.000232034
I0801 13:58:10.888758 24522 solver.cpp:375]     Train net output #0: loss = 0.000231345 (* 1 = 0.000231345 loss)
I0801 13:58:10.888764 24522 sgd_solver.cpp:136] Iteration 19500, lr = 0.00695312, m = 0.9
I0801 13:58:12.475275 24522 solver.cpp:353] Iteration 19600 (63.0322 iter/s, 1.58649s/100 iter), loss = 0.000536199
I0801 13:58:12.475298 24522 solver.cpp:375]     Train net output #0: loss = 0.000535511 (* 1 = 0.000535511 loss)
I0801 13:58:12.475304 24522 sgd_solver.cpp:136] Iteration 19600, lr = 0.0069375, m = 0.9
I0801 13:58:14.051605 24522 solver.cpp:353] Iteration 19700 (63.4404 iter/s, 1.57628s/100 iter), loss = 0.000480874
I0801 13:58:14.051630 24522 solver.cpp:375]     Train net output #0: loss = 0.000480187 (* 1 = 0.000480187 loss)
I0801 13:58:14.051635 24522 sgd_solver.cpp:136] Iteration 19700, lr = 0.00692187, m = 0.9
I0801 13:58:15.632913 24522 solver.cpp:353] Iteration 19800 (63.2408 iter/s, 1.58126s/100 iter), loss = 0.000496421
I0801 13:58:15.632977 24522 solver.cpp:375]     Train net output #0: loss = 0.000495733 (* 1 = 0.000495733 loss)
I0801 13:58:15.632994 24522 sgd_solver.cpp:136] Iteration 19800, lr = 0.00690625, m = 0.9
I0801 13:58:17.224186 24522 solver.cpp:353] Iteration 19900 (62.8447 iter/s, 1.59122s/100 iter), loss = 0.00133293
I0801 13:58:17.224210 24522 solver.cpp:375]     Train net output #0: loss = 0.00133224 (* 1 = 0.00133224 loss)
I0801 13:58:17.224216 24522 sgd_solver.cpp:136] Iteration 19900, lr = 0.00689062, m = 0.9
I0801 13:58:18.787400 24522 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_20000.caffemodel
I0801 13:58:18.797099 24522 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_20000.solverstate
I0801 13:58:18.801990 24522 solver.cpp:404] Sparsity after update:
I0801 13:58:18.804904 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:58:18.804916 24522 net.cpp:2270] conv1a_param_0(0.143) 
I0801 13:58:18.804927 24522 net.cpp:2270] conv1b_param_0(0.239) 
I0801 13:58:18.804941 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:58:18.804955 24522 net.cpp:2270] res2a_branch2a_param_0(0.319) 
I0801 13:58:18.804965 24522 net.cpp:2270] res2a_branch2b_param_0(0.319) 
I0801 13:58:18.804973 24522 net.cpp:2270] res3a_branch2a_param_0(0.319) 
I0801 13:58:18.804993 24522 net.cpp:2270] res3a_branch2b_param_0(0.319) 
I0801 13:58:18.805003 24522 net.cpp:2270] res4a_branch2a_param_0(0.319) 
I0801 13:58:18.805013 24522 net.cpp:2270] res4a_branch2b_param_0(0.319) 
I0801 13:58:18.805022 24522 net.cpp:2270] res5a_branch2a_param_0(0.307) 
I0801 13:58:18.805030 24522 net.cpp:2270] res5a_branch2b_param_0(0.313) 
I0801 13:58:18.805042 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (732930/2.3599e+06) 0.311
I0801 13:58:18.805058 24522 solver.cpp:550] Iteration 20000, Testing net (#0)
I0801 13:58:19.597762 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.914707
I0801 13:58:19.597780 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 13:58:19.597785 24522 solver.cpp:635]     Test net output #2: loss = 0.34708 (* 1 = 0.34708 loss)
I0801 13:58:19.597803 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.792718s
I0801 13:58:19.613271 24549 solver.cpp:450] Finding and applying sparsity: 0.34
I0801 13:58:35.654861 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:58:35.660473 24522 solver.cpp:353] Iteration 20000 (5.42424 iter/s, 18.4358s/100 iter), loss = 0.000365448
I0801 13:58:35.660490 24522 solver.cpp:375]     Train net output #0: loss = 0.000364761 (* 1 = 0.000364761 loss)
I0801 13:58:35.660496 24522 sgd_solver.cpp:136] Iteration 20000, lr = 0.006875, m = 0.9
I0801 13:58:37.502418 24522 solver.cpp:353] Iteration 20100 (54.2922 iter/s, 1.84189s/100 iter), loss = 0.000430974
I0801 13:58:37.502468 24522 solver.cpp:375]     Train net output #0: loss = 0.000430287 (* 1 = 0.000430287 loss)
I0801 13:58:37.502482 24522 sgd_solver.cpp:136] Iteration 20100, lr = 0.00685938, m = 0.9
I0801 13:58:39.111367 24522 solver.cpp:353] Iteration 20200 (62.1544 iter/s, 1.6089s/100 iter), loss = 0.00107454
I0801 13:58:39.111393 24522 solver.cpp:375]     Train net output #0: loss = 0.00107385 (* 1 = 0.00107385 loss)
I0801 13:58:39.111399 24522 sgd_solver.cpp:136] Iteration 20200, lr = 0.00684375, m = 0.9
I0801 13:58:40.749510 24522 solver.cpp:353] Iteration 20300 (61.0466 iter/s, 1.63809s/100 iter), loss = 0.000396971
I0801 13:58:40.749557 24522 solver.cpp:375]     Train net output #0: loss = 0.000396286 (* 1 = 0.000396286 loss)
I0801 13:58:40.749567 24522 sgd_solver.cpp:136] Iteration 20300, lr = 0.00682813, m = 0.9
I0801 13:58:42.376346 24522 solver.cpp:353] Iteration 20400 (61.4709 iter/s, 1.62679s/100 iter), loss = 0.0013729
I0801 13:58:42.376371 24522 solver.cpp:375]     Train net output #0: loss = 0.00137222 (* 1 = 0.00137222 loss)
I0801 13:58:42.376377 24522 sgd_solver.cpp:136] Iteration 20400, lr = 0.0068125, m = 0.9
I0801 13:58:43.978286 24522 solver.cpp:353] Iteration 20500 (62.4263 iter/s, 1.60189s/100 iter), loss = 0.00158632
I0801 13:58:43.978312 24522 solver.cpp:375]     Train net output #0: loss = 0.00158563 (* 1 = 0.00158563 loss)
I0801 13:58:43.978317 24522 sgd_solver.cpp:136] Iteration 20500, lr = 0.00679688, m = 0.9
I0801 13:58:45.573654 24522 solver.cpp:353] Iteration 20600 (62.6835 iter/s, 1.59531s/100 iter), loss = 0.000759837
I0801 13:58:45.573683 24522 solver.cpp:375]     Train net output #0: loss = 0.000759153 (* 1 = 0.000759153 loss)
I0801 13:58:45.573689 24522 sgd_solver.cpp:136] Iteration 20600, lr = 0.00678125, m = 0.9
I0801 13:58:47.181362 24522 solver.cpp:353] Iteration 20700 (62.2024 iter/s, 1.60766s/100 iter), loss = 0.00299108
I0801 13:58:47.181430 24522 solver.cpp:375]     Train net output #0: loss = 0.0029904 (* 1 = 0.0029904 loss)
I0801 13:58:47.181449 24522 sgd_solver.cpp:136] Iteration 20700, lr = 0.00676562, m = 0.9
I0801 13:58:48.755483 24522 solver.cpp:353] Iteration 20800 (63.5294 iter/s, 1.57407s/100 iter), loss = 0.0014712
I0801 13:58:48.755508 24522 solver.cpp:375]     Train net output #0: loss = 0.00147051 (* 1 = 0.00147051 loss)
I0801 13:58:48.755514 24522 sgd_solver.cpp:136] Iteration 20800, lr = 0.00675, m = 0.9
I0801 13:58:50.329507 24522 solver.cpp:353] Iteration 20900 (63.5334 iter/s, 1.57397s/100 iter), loss = 0.00284135
I0801 13:58:50.329533 24522 solver.cpp:375]     Train net output #0: loss = 0.00284066 (* 1 = 0.00284066 loss)
I0801 13:58:50.329540 24522 sgd_solver.cpp:136] Iteration 20900, lr = 0.00673437, m = 0.9
I0801 13:58:51.899386 24522 solver.cpp:404] Sparsity after update:
I0801 13:58:51.901020 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:58:51.901029 24522 net.cpp:2270] conv1a_param_0(0.138) 
I0801 13:58:51.901036 24522 net.cpp:2270] conv1b_param_0(0.25) 
I0801 13:58:51.901039 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:58:51.901043 24522 net.cpp:2270] res2a_branch2a_param_0(0.337) 
I0801 13:58:51.901051 24522 net.cpp:2270] res2a_branch2b_param_0(0.333) 
I0801 13:58:51.901057 24522 net.cpp:2270] res3a_branch2a_param_0(0.339) 
I0801 13:58:51.901062 24522 net.cpp:2270] res3a_branch2b_param_0(0.337) 
I0801 13:58:51.901065 24522 net.cpp:2270] res4a_branch2a_param_0(0.339) 
I0801 13:58:51.901069 24522 net.cpp:2270] res4a_branch2b_param_0(0.339) 
I0801 13:58:51.901073 24522 net.cpp:2270] res5a_branch2a_param_0(0.324) 
I0801 13:58:51.901077 24522 net.cpp:2270] res5a_branch2b_param_0(0.333) 
I0801 13:58:51.901082 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (776960/2.3599e+06) 0.329
I0801 13:58:51.901104 24522 solver.cpp:550] Iteration 21000, Testing net (#0)
I0801 13:58:52.711611 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.908825
I0801 13:58:52.711630 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997353
I0801 13:58:52.711637 24522 solver.cpp:635]     Test net output #2: loss = 0.367676 (* 1 = 0.367676 loss)
I0801 13:58:52.711655 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.810524s
I0801 13:58:52.727190 24549 solver.cpp:450] Finding and applying sparsity: 0.36
I0801 13:59:09.764240 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:59:09.766211 24522 solver.cpp:353] Iteration 21000 (5.14505 iter/s, 19.4362s/100 iter), loss = 0.000756806
I0801 13:59:09.766228 24522 solver.cpp:375]     Train net output #0: loss = 0.000756126 (* 1 = 0.000756126 loss)
I0801 13:59:09.766234 24522 sgd_solver.cpp:136] Iteration 21000, lr = 0.00671875, m = 0.9
I0801 13:59:11.567276 24522 solver.cpp:353] Iteration 21100 (55.5245 iter/s, 1.80101s/100 iter), loss = 0.000562546
I0801 13:59:11.567301 24522 solver.cpp:375]     Train net output #0: loss = 0.000561866 (* 1 = 0.000561866 loss)
I0801 13:59:11.567306 24522 sgd_solver.cpp:136] Iteration 21100, lr = 0.00670313, m = 0.9
I0801 13:59:13.159042 24522 solver.cpp:353] Iteration 21200 (62.8252 iter/s, 1.59172s/100 iter), loss = 0.000511892
I0801 13:59:13.159068 24522 solver.cpp:375]     Train net output #0: loss = 0.000511212 (* 1 = 0.000511212 loss)
I0801 13:59:13.159072 24522 sgd_solver.cpp:136] Iteration 21200, lr = 0.0066875, m = 0.9
I0801 13:59:14.751349 24522 solver.cpp:353] Iteration 21300 (62.8039 iter/s, 1.59226s/100 iter), loss = 0.000537717
I0801 13:59:14.751374 24522 solver.cpp:375]     Train net output #0: loss = 0.000537038 (* 1 = 0.000537038 loss)
I0801 13:59:14.751379 24522 sgd_solver.cpp:136] Iteration 21300, lr = 0.00667187, m = 0.9
I0801 13:59:16.345367 24522 solver.cpp:353] Iteration 21400 (62.7365 iter/s, 1.59397s/100 iter), loss = 0.000267403
I0801 13:59:16.345418 24522 solver.cpp:375]     Train net output #0: loss = 0.000266724 (* 1 = 0.000266724 loss)
I0801 13:59:16.345427 24522 sgd_solver.cpp:136] Iteration 21400, lr = 0.00665625, m = 0.9
I0801 13:59:17.931365 24522 solver.cpp:353] Iteration 21500 (63.0539 iter/s, 1.58595s/100 iter), loss = 0.00149765
I0801 13:59:17.931411 24522 solver.cpp:375]     Train net output #0: loss = 0.00149697 (* 1 = 0.00149697 loss)
I0801 13:59:17.931422 24522 sgd_solver.cpp:136] Iteration 21500, lr = 0.00664062, m = 0.9
I0801 13:59:19.516399 24522 solver.cpp:353] Iteration 21600 (63.092 iter/s, 1.58499s/100 iter), loss = 0.000861838
I0801 13:59:19.516427 24522 solver.cpp:375]     Train net output #0: loss = 0.000861159 (* 1 = 0.000861159 loss)
I0801 13:59:19.516434 24522 sgd_solver.cpp:136] Iteration 21600, lr = 0.006625, m = 0.9
I0801 13:59:21.099822 24522 solver.cpp:353] Iteration 21700 (63.1564 iter/s, 1.58337s/100 iter), loss = 0.00154741
I0801 13:59:21.099846 24522 solver.cpp:375]     Train net output #0: loss = 0.00154673 (* 1 = 0.00154673 loss)
I0801 13:59:21.099853 24522 sgd_solver.cpp:136] Iteration 21700, lr = 0.00660937, m = 0.9
I0801 13:59:22.695293 24522 solver.cpp:353] Iteration 21800 (62.6793 iter/s, 1.59542s/100 iter), loss = 0.000359854
I0801 13:59:22.695343 24522 solver.cpp:375]     Train net output #0: loss = 0.000359174 (* 1 = 0.000359174 loss)
I0801 13:59:22.695361 24522 sgd_solver.cpp:136] Iteration 21800, lr = 0.00659375, m = 0.9
I0801 13:59:24.301708 24522 solver.cpp:353] Iteration 21900 (62.2524 iter/s, 1.60636s/100 iter), loss = 0.00283178
I0801 13:59:24.301736 24522 solver.cpp:375]     Train net output #0: loss = 0.00283109 (* 1 = 0.00283109 loss)
I0801 13:59:24.301743 24522 sgd_solver.cpp:136] Iteration 21900, lr = 0.00657812, m = 0.9
I0801 13:59:25.868069 24522 solver.cpp:404] Sparsity after update:
I0801 13:59:25.869652 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:59:25.869662 24522 net.cpp:2270] conv1a_param_0(0.149) 
I0801 13:59:25.869669 24522 net.cpp:2270] conv1b_param_0(0.26) 
I0801 13:59:25.869673 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:59:25.869678 24522 net.cpp:2270] res2a_branch2a_param_0(0.358) 
I0801 13:59:25.869681 24522 net.cpp:2270] res2a_branch2b_param_0(0.354) 
I0801 13:59:25.869684 24522 net.cpp:2270] res3a_branch2a_param_0(0.359) 
I0801 13:59:25.869688 24522 net.cpp:2270] res3a_branch2b_param_0(0.358) 
I0801 13:59:25.869690 24522 net.cpp:2270] res4a_branch2a_param_0(0.359) 
I0801 13:59:25.869693 24522 net.cpp:2270] res4a_branch2b_param_0(0.359) 
I0801 13:59:25.869696 24522 net.cpp:2270] res5a_branch2a_param_0(0.347) 
I0801 13:59:25.869699 24522 net.cpp:2270] res5a_branch2b_param_0(0.357) 
I0801 13:59:25.869716 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (828998/2.3599e+06) 0.351
I0801 13:59:25.869729 24522 solver.cpp:550] Iteration 22000, Testing net (#0)
I0801 13:59:26.283421 24520 data_reader.cpp:264] Starting prefetch of epoch 3
I0801 13:59:26.689656 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.907942
I0801 13:59:26.689674 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997941
I0801 13:59:26.689682 24522 solver.cpp:635]     Test net output #2: loss = 0.36923 (* 1 = 0.36923 loss)
I0801 13:59:26.689700 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.819944s
I0801 13:59:26.705417 24549 solver.cpp:450] Finding and applying sparsity: 0.38
I0801 13:59:42.596545 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:59:42.598449 24522 solver.cpp:353] Iteration 22000 (5.46561 iter/s, 18.2962s/100 iter), loss = 0.00113687
I0801 13:59:42.598469 24522 solver.cpp:375]     Train net output #0: loss = 0.00113619 (* 1 = 0.00113619 loss)
I0801 13:59:42.598477 24522 sgd_solver.cpp:136] Iteration 22000, lr = 0.0065625, m = 0.9
I0801 13:59:44.440263 24522 solver.cpp:353] Iteration 22100 (54.296 iter/s, 1.84176s/100 iter), loss = 0.0013199
I0801 13:59:44.440286 24522 solver.cpp:375]     Train net output #0: loss = 0.00131922 (* 1 = 0.00131922 loss)
I0801 13:59:44.440290 24522 sgd_solver.cpp:136] Iteration 22100, lr = 0.00654687, m = 0.9
I0801 13:59:46.028795 24522 solver.cpp:353] Iteration 22200 (62.9533 iter/s, 1.58848s/100 iter), loss = 0.000506676
I0801 13:59:46.028825 24522 solver.cpp:375]     Train net output #0: loss = 0.000505997 (* 1 = 0.000505997 loss)
I0801 13:59:46.028828 24522 sgd_solver.cpp:136] Iteration 22200, lr = 0.00653125, m = 0.9
I0801 13:59:47.622074 24522 solver.cpp:353] Iteration 22300 (62.7655 iter/s, 1.59323s/100 iter), loss = 0.00015816
I0801 13:59:47.622103 24522 solver.cpp:375]     Train net output #0: loss = 0.00015748 (* 1 = 0.00015748 loss)
I0801 13:59:47.622110 24522 sgd_solver.cpp:136] Iteration 22300, lr = 0.00651562, m = 0.9
I0801 13:59:49.206606 24522 solver.cpp:353] Iteration 22400 (63.1121 iter/s, 1.58448s/100 iter), loss = 0.000416087
I0801 13:59:49.206653 24522 solver.cpp:375]     Train net output #0: loss = 0.000415403 (* 1 = 0.000415403 loss)
I0801 13:59:49.206662 24522 sgd_solver.cpp:136] Iteration 22400, lr = 0.0065, m = 0.9
I0801 13:59:50.813978 24522 solver.cpp:353] Iteration 22500 (62.2153 iter/s, 1.60732s/100 iter), loss = 0.000283355
I0801 13:59:50.814025 24522 solver.cpp:375]     Train net output #0: loss = 0.00028267 (* 1 = 0.00028267 loss)
I0801 13:59:50.814040 24522 sgd_solver.cpp:136] Iteration 22500, lr = 0.00648437, m = 0.9
I0801 13:59:52.408310 24522 solver.cpp:353] Iteration 22600 (62.7241 iter/s, 1.59428s/100 iter), loss = 0.00102661
I0801 13:59:52.408360 24522 solver.cpp:375]     Train net output #0: loss = 0.00102593 (* 1 = 0.00102593 loss)
I0801 13:59:52.408371 24522 sgd_solver.cpp:136] Iteration 22600, lr = 0.00646875, m = 0.9
I0801 13:59:53.990494 24522 solver.cpp:353] Iteration 22700 (63.2058 iter/s, 1.58213s/100 iter), loss = 0.0051047
I0801 13:59:53.990530 24522 solver.cpp:375]     Train net output #0: loss = 0.00510402 (* 1 = 0.00510402 loss)
I0801 13:59:53.990538 24522 sgd_solver.cpp:136] Iteration 22700, lr = 0.00645312, m = 0.9
I0801 13:59:55.593287 24522 solver.cpp:353] Iteration 22800 (62.3931 iter/s, 1.60274s/100 iter), loss = 0.00132588
I0801 13:59:55.593338 24522 solver.cpp:375]     Train net output #0: loss = 0.0013252 (* 1 = 0.0013252 loss)
I0801 13:59:55.593363 24522 sgd_solver.cpp:136] Iteration 22800, lr = 0.0064375, m = 0.9
I0801 13:59:57.189414 24522 solver.cpp:353] Iteration 22900 (62.6538 iter/s, 1.59607s/100 iter), loss = 7.60802e-05
I0801 13:59:57.189604 24522 solver.cpp:375]     Train net output #0: loss = 7.53939e-05 (* 1 = 7.53939e-05 loss)
I0801 13:59:57.189682 24522 sgd_solver.cpp:136] Iteration 22900, lr = 0.00642187, m = 0.9
I0801 13:59:58.751377 24522 solver.cpp:404] Sparsity after update:
I0801 13:59:58.753204 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:59:58.753214 24522 net.cpp:2270] conv1a_param_0(0.16) 
I0801 13:59:58.753223 24522 net.cpp:2270] conv1b_param_0(0.281) 
I0801 13:59:58.753237 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:59:58.753242 24522 net.cpp:2270] res2a_branch2a_param_0(0.378) 
I0801 13:59:58.753245 24522 net.cpp:2270] res2a_branch2b_param_0(0.375) 
I0801 13:59:58.753249 24522 net.cpp:2270] res3a_branch2a_param_0(0.378) 
I0801 13:59:58.753257 24522 net.cpp:2270] res3a_branch2b_param_0(0.378) 
I0801 13:59:58.753260 24522 net.cpp:2270] res4a_branch2a_param_0(0.379) 
I0801 13:59:58.753264 24522 net.cpp:2270] res4a_branch2b_param_0(0.378) 
I0801 13:59:58.753268 24522 net.cpp:2270] res5a_branch2a_param_0(0.361) 
I0801 13:59:58.753271 24522 net.cpp:2270] res5a_branch2b_param_0(0.373) 
I0801 13:59:58.753275 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (866878/2.3599e+06) 0.367
I0801 13:59:58.753301 24522 solver.cpp:550] Iteration 23000, Testing net (#0)
I0801 13:59:59.569330 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.910295
I0801 13:59:59.569350 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 13:59:59.569355 24522 solver.cpp:635]     Test net output #2: loss = 0.346102 (* 1 = 0.346102 loss)
I0801 13:59:59.569370 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.81604s
I0801 13:59:59.584956 24549 solver.cpp:450] Finding and applying sparsity: 0.4
I0801 14:00:16.261052 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:00:16.262981 24522 solver.cpp:353] Iteration 23000 (5.243 iter/s, 19.073s/100 iter), loss = 0.00203507
I0801 14:00:16.263005 24522 solver.cpp:375]     Train net output #0: loss = 0.00203438 (* 1 = 0.00203438 loss)
I0801 14:00:16.263011 24522 sgd_solver.cpp:136] Iteration 23000, lr = 0.00640625, m = 0.9
I0801 14:00:18.072160 24522 solver.cpp:353] Iteration 23100 (55.2754 iter/s, 1.80912s/100 iter), loss = 0.00179673
I0801 14:00:18.072185 24522 solver.cpp:375]     Train net output #0: loss = 0.00179605 (* 1 = 0.00179605 loss)
I0801 14:00:18.072190 24522 sgd_solver.cpp:136] Iteration 23100, lr = 0.00639063, m = 0.9
I0801 14:00:19.657762 24522 solver.cpp:353] Iteration 23200 (63.0696 iter/s, 1.58555s/100 iter), loss = 0.0020372
I0801 14:00:19.657788 24522 solver.cpp:375]     Train net output #0: loss = 0.00203651 (* 1 = 0.00203651 loss)
I0801 14:00:19.657794 24522 sgd_solver.cpp:136] Iteration 23200, lr = 0.006375, m = 0.9
I0801 14:00:21.257598 24522 solver.cpp:353] Iteration 23300 (62.5083 iter/s, 1.59979s/100 iter), loss = 0.000830606
I0801 14:00:21.257627 24522 solver.cpp:375]     Train net output #0: loss = 0.000829923 (* 1 = 0.000829923 loss)
I0801 14:00:21.257632 24522 sgd_solver.cpp:136] Iteration 23300, lr = 0.00635938, m = 0.9
I0801 14:00:22.842555 24522 solver.cpp:353] Iteration 23400 (63.0952 iter/s, 1.58491s/100 iter), loss = 0.000698262
I0801 14:00:22.842578 24522 solver.cpp:375]     Train net output #0: loss = 0.000697579 (* 1 = 0.000697579 loss)
I0801 14:00:22.842583 24522 sgd_solver.cpp:136] Iteration 23400, lr = 0.00634375, m = 0.9
I0801 14:00:24.424775 24522 solver.cpp:353] Iteration 23500 (63.2043 iter/s, 1.58217s/100 iter), loss = 0.000264578
I0801 14:00:24.424829 24522 solver.cpp:375]     Train net output #0: loss = 0.000263896 (* 1 = 0.000263896 loss)
I0801 14:00:24.424844 24522 sgd_solver.cpp:136] Iteration 23500, lr = 0.00632813, m = 0.9
I0801 14:00:25.999186 24522 solver.cpp:353] Iteration 23600 (63.5177 iter/s, 1.57436s/100 iter), loss = 0.00102565
I0801 14:00:25.999212 24522 solver.cpp:375]     Train net output #0: loss = 0.00102496 (* 1 = 0.00102496 loss)
I0801 14:00:25.999218 24522 sgd_solver.cpp:136] Iteration 23600, lr = 0.0063125, m = 0.9
I0801 14:00:27.586519 24522 solver.cpp:353] Iteration 23700 (63.0007 iter/s, 1.58728s/100 iter), loss = 0.00135949
I0801 14:00:27.586567 24522 solver.cpp:375]     Train net output #0: loss = 0.00135881 (* 1 = 0.00135881 loss)
I0801 14:00:27.586580 24522 sgd_solver.cpp:136] Iteration 23700, lr = 0.00629687, m = 0.9
I0801 14:00:29.196213 24522 solver.cpp:353] Iteration 23800 (62.1255 iter/s, 1.60964s/100 iter), loss = 0.00185312
I0801 14:00:29.196240 24522 solver.cpp:375]     Train net output #0: loss = 0.00185244 (* 1 = 0.00185244 loss)
I0801 14:00:29.196247 24522 sgd_solver.cpp:136] Iteration 23800, lr = 0.00628125, m = 0.9
I0801 14:00:30.781970 24522 solver.cpp:353] Iteration 23900 (63.0634 iter/s, 1.58571s/100 iter), loss = 0.00177764
I0801 14:00:30.782044 24522 solver.cpp:375]     Train net output #0: loss = 0.00177696 (* 1 = 0.00177696 loss)
I0801 14:00:30.782064 24522 sgd_solver.cpp:136] Iteration 23900, lr = 0.00626562, m = 0.9
I0801 14:00:32.335335 24522 solver.cpp:404] Sparsity after update:
I0801 14:00:32.336989 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:00:32.336998 24522 net.cpp:2270] conv1a_param_0(0.171) 
I0801 14:00:32.337005 24522 net.cpp:2270] conv1b_param_0(0.316) 
I0801 14:00:32.337007 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:00:32.337009 24522 net.cpp:2270] res2a_branch2a_param_0(0.399) 
I0801 14:00:32.337011 24522 net.cpp:2270] res2a_branch2b_param_0(0.396) 
I0801 14:00:32.337013 24522 net.cpp:2270] res3a_branch2a_param_0(0.399) 
I0801 14:00:32.337015 24522 net.cpp:2270] res3a_branch2b_param_0(0.399) 
I0801 14:00:32.337023 24522 net.cpp:2270] res4a_branch2a_param_0(0.399) 
I0801 14:00:32.337024 24522 net.cpp:2270] res4a_branch2b_param_0(0.399) 
I0801 14:00:32.337026 24522 net.cpp:2270] res5a_branch2a_param_0(0.386) 
I0801 14:00:32.337031 24522 net.cpp:2270] res5a_branch2b_param_0(0.395) 
I0801 14:00:32.337033 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (920650/2.3599e+06) 0.39
I0801 14:00:32.337056 24522 solver.cpp:550] Iteration 24000, Testing net (#0)
I0801 14:00:33.145632 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.905883
I0801 14:00:33.145651 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 14:00:33.145656 24522 solver.cpp:635]     Test net output #2: loss = 0.355252 (* 1 = 0.355252 loss)
I0801 14:00:33.145670 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.808586s
I0801 14:00:33.161176 24549 solver.cpp:450] Finding and applying sparsity: 0.42
I0801 14:00:50.090773 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:00:50.092679 24522 solver.cpp:353] Iteration 24000 (5.17862 iter/s, 19.3102s/100 iter), loss = 0.00269516
I0801 14:00:50.092696 24522 solver.cpp:375]     Train net output #0: loss = 0.00269448 (* 1 = 0.00269448 loss)
I0801 14:00:50.092703 24522 sgd_solver.cpp:136] Iteration 24000, lr = 0.00625, m = 0.9
I0801 14:00:51.916821 24522 solver.cpp:353] Iteration 24100 (54.8221 iter/s, 1.82408s/100 iter), loss = 0.00157487
I0801 14:00:51.916846 24522 solver.cpp:375]     Train net output #0: loss = 0.00157419 (* 1 = 0.00157419 loss)
I0801 14:00:51.916852 24522 sgd_solver.cpp:136] Iteration 24100, lr = 0.00623438, m = 0.9
I0801 14:00:53.497592 24522 solver.cpp:353] Iteration 24200 (63.2623 iter/s, 1.58072s/100 iter), loss = 0.00141924
I0801 14:00:53.497618 24522 solver.cpp:375]     Train net output #0: loss = 0.00141856 (* 1 = 0.00141856 loss)
I0801 14:00:53.497624 24522 sgd_solver.cpp:136] Iteration 24200, lr = 0.00621875, m = 0.9
I0801 14:00:55.088201 24522 solver.cpp:353] Iteration 24300 (62.871 iter/s, 1.59056s/100 iter), loss = 0.000578608
I0801 14:00:55.088224 24522 solver.cpp:375]     Train net output #0: loss = 0.000577928 (* 1 = 0.000577928 loss)
I0801 14:00:55.088230 24522 sgd_solver.cpp:136] Iteration 24300, lr = 0.00620312, m = 0.9
I0801 14:00:56.686486 24522 solver.cpp:353] Iteration 24400 (62.5689 iter/s, 1.59824s/100 iter), loss = 0.00076415
I0801 14:00:56.686511 24522 solver.cpp:375]     Train net output #0: loss = 0.00076347 (* 1 = 0.00076347 loss)
I0801 14:00:56.686517 24522 sgd_solver.cpp:136] Iteration 24400, lr = 0.0061875, m = 0.9
I0801 14:00:58.261090 24522 solver.cpp:353] Iteration 24500 (63.51 iter/s, 1.57455s/100 iter), loss = 0.00047016
I0801 14:00:58.261140 24522 solver.cpp:375]     Train net output #0: loss = 0.000469478 (* 1 = 0.000469478 loss)
I0801 14:00:58.261153 24522 sgd_solver.cpp:136] Iteration 24500, lr = 0.00617187, m = 0.9
I0801 14:00:59.853148 24522 solver.cpp:353] Iteration 24600 (62.8137 iter/s, 1.59201s/100 iter), loss = 0.00197787
I0801 14:00:59.853170 24522 solver.cpp:375]     Train net output #0: loss = 0.00197719 (* 1 = 0.00197719 loss)
I0801 14:00:59.853176 24522 sgd_solver.cpp:136] Iteration 24600, lr = 0.00615625, m = 0.9
I0801 14:01:01.441236 24522 solver.cpp:353] Iteration 24700 (62.9708 iter/s, 1.58804s/100 iter), loss = 0.00088127
I0801 14:01:01.441259 24522 solver.cpp:375]     Train net output #0: loss = 0.000880588 (* 1 = 0.000880588 loss)
I0801 14:01:01.441263 24522 sgd_solver.cpp:136] Iteration 24700, lr = 0.00614062, m = 0.9
I0801 14:01:03.023525 24522 solver.cpp:353] Iteration 24800 (63.2016 iter/s, 1.58224s/100 iter), loss = 0.000719587
I0801 14:01:03.023552 24522 solver.cpp:375]     Train net output #0: loss = 0.000718907 (* 1 = 0.000718907 loss)
I0801 14:01:03.023558 24522 sgd_solver.cpp:136] Iteration 24800, lr = 0.006125, m = 0.9
I0801 14:01:04.597826 24522 solver.cpp:353] Iteration 24900 (63.5222 iter/s, 1.57425s/100 iter), loss = 5.16188e-05
I0801 14:01:04.597852 24522 solver.cpp:375]     Train net output #0: loss = 5.09386e-05 (* 1 = 5.09386e-05 loss)
I0801 14:01:04.597858 24522 sgd_solver.cpp:136] Iteration 24900, lr = 0.00610937, m = 0.9
I0801 14:01:06.180382 24522 solver.cpp:404] Sparsity after update:
I0801 14:01:06.182008 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:01:06.182016 24522 net.cpp:2270] conv1a_param_0(0.176) 
I0801 14:01:06.182024 24522 net.cpp:2270] conv1b_param_0(0.352) 
I0801 14:01:06.182029 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:01:06.182034 24522 net.cpp:2270] res2a_branch2a_param_0(0.417) 
I0801 14:01:06.182039 24522 net.cpp:2270] res2a_branch2b_param_0(0.417) 
I0801 14:01:06.182042 24522 net.cpp:2270] res3a_branch2a_param_0(0.418) 
I0801 14:01:06.182046 24522 net.cpp:2270] res3a_branch2b_param_0(0.417) 
I0801 14:01:06.182051 24522 net.cpp:2270] res4a_branch2a_param_0(0.419) 
I0801 14:01:06.182056 24522 net.cpp:2270] res4a_branch2b_param_0(0.418) 
I0801 14:01:06.182061 24522 net.cpp:2270] res5a_branch2a_param_0(0.396) 
I0801 14:01:06.182065 24522 net.cpp:2270] res5a_branch2b_param_0(0.414) 
I0801 14:01:06.182070 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (956290/2.3599e+06) 0.405
I0801 14:01:06.183681 24522 solver.cpp:550] Iteration 25000, Testing net (#0)
I0801 14:01:06.989956 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.90853
I0801 14:01:06.989975 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.994706
I0801 14:01:06.989981 24522 solver.cpp:635]     Test net output #2: loss = 0.373472 (* 1 = 0.373472 loss)
I0801 14:01:06.989997 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.806288s
I0801 14:01:07.005564 24549 solver.cpp:450] Finding and applying sparsity: 0.44
I0801 14:01:23.681139 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:01:23.683244 24522 solver.cpp:353] Iteration 25000 (5.23975 iter/s, 19.0849s/100 iter), loss = 0.000589835
I0801 14:01:23.683264 24522 solver.cpp:375]     Train net output #0: loss = 0.000589153 (* 1 = 0.000589153 loss)
I0801 14:01:23.683274 24522 sgd_solver.cpp:136] Iteration 25000, lr = 0.00609375, m = 0.9
I0801 14:01:25.503509 24522 solver.cpp:353] Iteration 25100 (54.9388 iter/s, 1.82021s/100 iter), loss = 0.000248309
I0801 14:01:25.503535 24522 solver.cpp:375]     Train net output #0: loss = 0.000247629 (* 1 = 0.000247629 loss)
I0801 14:01:25.503540 24522 sgd_solver.cpp:136] Iteration 25100, lr = 0.00607812, m = 0.9
I0801 14:01:27.095652 24522 solver.cpp:353] Iteration 25200 (62.8105 iter/s, 1.59209s/100 iter), loss = 0.00324748
I0801 14:01:27.095677 24522 solver.cpp:375]     Train net output #0: loss = 0.0032468 (* 1 = 0.0032468 loss)
I0801 14:01:27.095685 24522 sgd_solver.cpp:136] Iteration 25200, lr = 0.0060625, m = 0.9
I0801 14:01:28.663059 24522 solver.cpp:353] Iteration 25300 (63.8015 iter/s, 1.56736s/100 iter), loss = 0.00015576
I0801 14:01:28.663084 24522 solver.cpp:375]     Train net output #0: loss = 0.000155079 (* 1 = 0.000155079 loss)
I0801 14:01:28.663090 24522 sgd_solver.cpp:136] Iteration 25300, lr = 0.00604687, m = 0.9
I0801 14:01:30.247774 24522 solver.cpp:353] Iteration 25400 (63.1048 iter/s, 1.58466s/100 iter), loss = 0.00015903
I0801 14:01:30.247802 24522 solver.cpp:375]     Train net output #0: loss = 0.000158349 (* 1 = 0.000158349 loss)
I0801 14:01:30.247805 24522 sgd_solver.cpp:136] Iteration 25400, lr = 0.00603125, m = 0.9
I0801 14:01:31.844226 24522 solver.cpp:353] Iteration 25500 (62.6408 iter/s, 1.5964s/100 iter), loss = 0.00114769
I0801 14:01:31.844251 24522 solver.cpp:375]     Train net output #0: loss = 0.00114701 (* 1 = 0.00114701 loss)
I0801 14:01:31.844256 24522 sgd_solver.cpp:136] Iteration 25500, lr = 0.00601562, m = 0.9
I0801 14:01:33.431915 24522 solver.cpp:353] Iteration 25600 (62.9866 iter/s, 1.58764s/100 iter), loss = 0.00179576
I0801 14:01:33.431941 24522 solver.cpp:375]     Train net output #0: loss = 0.00179508 (* 1 = 0.00179508 loss)
I0801 14:01:33.431946 24522 sgd_solver.cpp:136] Iteration 25600, lr = 0.006, m = 0.9
I0801 14:01:35.015244 24522 solver.cpp:353] Iteration 25700 (63.16 iter/s, 1.58328s/100 iter), loss = 0.000338449
I0801 14:01:35.015291 24522 solver.cpp:375]     Train net output #0: loss = 0.000337767 (* 1 = 0.000337767 loss)
I0801 14:01:35.015300 24522 sgd_solver.cpp:136] Iteration 25700, lr = 0.00598437, m = 0.9
I0801 14:01:36.604722 24522 solver.cpp:353] Iteration 25800 (62.9158 iter/s, 1.58943s/100 iter), loss = 0.00127182
I0801 14:01:36.604748 24522 solver.cpp:375]     Train net output #0: loss = 0.00127114 (* 1 = 0.00127114 loss)
I0801 14:01:36.604753 24522 sgd_solver.cpp:136] Iteration 25800, lr = 0.00596875, m = 0.9
I0801 14:01:38.195598 24522 solver.cpp:353] Iteration 25900 (62.8604 iter/s, 1.59083s/100 iter), loss = 6.05088e-05
I0801 14:01:38.195623 24522 solver.cpp:375]     Train net output #0: loss = 5.98277e-05 (* 1 = 5.98277e-05 loss)
I0801 14:01:38.195628 24522 sgd_solver.cpp:136] Iteration 25900, lr = 0.00595312, m = 0.9
I0801 14:01:39.767071 24522 solver.cpp:404] Sparsity after update:
I0801 14:01:39.769075 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:01:39.769088 24522 net.cpp:2270] conv1a_param_0(0.183) 
I0801 14:01:39.769096 24522 net.cpp:2270] conv1b_param_0(0.363) 
I0801 14:01:39.769100 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:01:39.769104 24522 net.cpp:2270] res2a_branch2a_param_0(0.437) 
I0801 14:01:39.769109 24522 net.cpp:2270] res2a_branch2b_param_0(0.438) 
I0801 14:01:39.769112 24522 net.cpp:2270] res3a_branch2a_param_0(0.439) 
I0801 14:01:39.769115 24522 net.cpp:2270] res3a_branch2b_param_0(0.438) 
I0801 14:01:39.769119 24522 net.cpp:2270] res4a_branch2a_param_0(0.439) 
I0801 14:01:39.769124 24522 net.cpp:2270] res4a_branch2b_param_0(0.439) 
I0801 14:01:39.769126 24522 net.cpp:2270] res5a_branch2a_param_0(0.416) 
I0801 14:01:39.769130 24522 net.cpp:2270] res5a_branch2b_param_0(0.433) 
I0801 14:01:39.769134 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.00221e+06/2.3599e+06) 0.425
I0801 14:01:39.769160 24522 solver.cpp:550] Iteration 26000, Testing net (#0)
I0801 14:01:40.577688 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.910001
I0801 14:01:40.577708 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995588
I0801 14:01:40.577714 24522 solver.cpp:635]     Test net output #2: loss = 0.372696 (* 1 = 0.372696 loss)
I0801 14:01:40.577730 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.808544s
I0801 14:01:40.594132 24549 solver.cpp:450] Finding and applying sparsity: 0.46
I0801 14:01:58.013207 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:01:58.015166 24522 solver.cpp:353] Iteration 26000 (5.04566 iter/s, 19.819s/100 iter), loss = 0.00121238
I0801 14:01:58.015185 24522 solver.cpp:375]     Train net output #0: loss = 0.0012117 (* 1 = 0.0012117 loss)
I0801 14:01:58.015193 24522 sgd_solver.cpp:136] Iteration 26000, lr = 0.0059375, m = 0.9
I0801 14:01:59.840090 24522 solver.cpp:353] Iteration 26100 (54.7986 iter/s, 1.82487s/100 iter), loss = 0.00205299
I0801 14:01:59.840113 24522 solver.cpp:375]     Train net output #0: loss = 0.00205231 (* 1 = 0.00205231 loss)
I0801 14:01:59.840118 24522 sgd_solver.cpp:136] Iteration 26100, lr = 0.00592188, m = 0.9
I0801 14:02:01.428261 24522 solver.cpp:353] Iteration 26200 (62.9675 iter/s, 1.58812s/100 iter), loss = 0.00045883
I0801 14:02:01.428288 24522 solver.cpp:375]     Train net output #0: loss = 0.000458149 (* 1 = 0.000458149 loss)
I0801 14:02:01.428297 24522 sgd_solver.cpp:136] Iteration 26200, lr = 0.00590625, m = 0.9
I0801 14:02:03.018095 24522 solver.cpp:353] Iteration 26300 (62.9016 iter/s, 1.58978s/100 iter), loss = 0.000833806
I0801 14:02:03.018115 24522 solver.cpp:375]     Train net output #0: loss = 0.000833125 (* 1 = 0.000833125 loss)
I0801 14:02:03.018120 24522 sgd_solver.cpp:136] Iteration 26300, lr = 0.00589063, m = 0.9
I0801 14:02:04.686153 24522 solver.cpp:353] Iteration 26400 (59.9519 iter/s, 1.668s/100 iter), loss = 0.000658299
I0801 14:02:04.686199 24522 solver.cpp:375]     Train net output #0: loss = 0.000657618 (* 1 = 0.000657618 loss)
I0801 14:02:04.686223 24522 sgd_solver.cpp:136] Iteration 26400, lr = 0.005875, m = 0.9
I0801 14:02:06.366571 24522 solver.cpp:353] Iteration 26500 (59.5108 iter/s, 1.68037s/100 iter), loss = 0.00105419
I0801 14:02:06.366600 24522 solver.cpp:375]     Train net output #0: loss = 0.0010535 (* 1 = 0.0010535 loss)
I0801 14:02:06.366605 24522 sgd_solver.cpp:136] Iteration 26500, lr = 0.00585938, m = 0.9
I0801 14:02:06.557858 24487 data_reader.cpp:264] Starting prefetch of epoch 4
I0801 14:02:07.963193 24522 solver.cpp:353] Iteration 26600 (62.6341 iter/s, 1.59657s/100 iter), loss = 0.000371216
I0801 14:02:07.963238 24522 solver.cpp:375]     Train net output #0: loss = 0.000370534 (* 1 = 0.000370534 loss)
I0801 14:02:07.963250 24522 sgd_solver.cpp:136] Iteration 26600, lr = 0.00584375, m = 0.9
I0801 14:02:09.620812 24522 solver.cpp:353] Iteration 26700 (60.3294 iter/s, 1.65757s/100 iter), loss = 0.000280486
I0801 14:02:09.620843 24522 solver.cpp:375]     Train net output #0: loss = 0.000279804 (* 1 = 0.000279804 loss)
I0801 14:02:09.620848 24522 sgd_solver.cpp:136] Iteration 26700, lr = 0.00582812, m = 0.9
I0801 14:02:11.279279 24522 solver.cpp:353] Iteration 26800 (60.2985 iter/s, 1.65841s/100 iter), loss = 0.00187292
I0801 14:02:11.279302 24522 solver.cpp:375]     Train net output #0: loss = 0.00187223 (* 1 = 0.00187223 loss)
I0801 14:02:11.279309 24522 sgd_solver.cpp:136] Iteration 26800, lr = 0.0058125, m = 0.9
I0801 14:02:12.873612 24522 solver.cpp:353] Iteration 26900 (62.7246 iter/s, 1.59427s/100 iter), loss = 0.000148487
I0801 14:02:12.873666 24522 solver.cpp:375]     Train net output #0: loss = 0.000147803 (* 1 = 0.000147803 loss)
I0801 14:02:12.873675 24522 sgd_solver.cpp:136] Iteration 26900, lr = 0.00579687, m = 0.9
I0801 14:02:14.457173 24522 solver.cpp:404] Sparsity after update:
I0801 14:02:14.458777 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:02:14.458786 24522 net.cpp:2270] conv1a_param_0(0.201) 
I0801 14:02:14.458796 24522 net.cpp:2270] conv1b_param_0(0.401) 
I0801 14:02:14.458801 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:02:14.458806 24522 net.cpp:2270] res2a_branch2a_param_0(0.458) 
I0801 14:02:14.458809 24522 net.cpp:2270] res2a_branch2b_param_0(0.458) 
I0801 14:02:14.458813 24522 net.cpp:2270] res3a_branch2a_param_0(0.458) 
I0801 14:02:14.458818 24522 net.cpp:2270] res3a_branch2b_param_0(0.458) 
I0801 14:02:14.458822 24522 net.cpp:2270] res4a_branch2a_param_0(0.459) 
I0801 14:02:14.458827 24522 net.cpp:2270] res4a_branch2b_param_0(0.458) 
I0801 14:02:14.458829 24522 net.cpp:2270] res5a_branch2a_param_0(0.438) 
I0801 14:02:14.458842 24522 net.cpp:2270] res5a_branch2b_param_0(0.454) 
I0801 14:02:14.458847 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.05178e+06/2.3599e+06) 0.446
I0801 14:02:14.458858 24522 solver.cpp:550] Iteration 27000, Testing net (#0)
I0801 14:02:15.353816 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.909119
I0801 14:02:15.353857 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995588
I0801 14:02:15.353874 24522 solver.cpp:635]     Test net output #2: loss = 0.371314 (* 1 = 0.371314 loss)
I0801 14:02:15.353905 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.895014s
I0801 14:02:15.374022 24549 solver.cpp:450] Finding and applying sparsity: 0.48
I0801 14:02:35.550765 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:02:35.552743 24522 solver.cpp:353] Iteration 27000 (4.40946 iter/s, 22.6785s/100 iter), loss = 0.00244515
I0801 14:02:35.552764 24522 solver.cpp:375]     Train net output #0: loss = 0.00244447 (* 1 = 0.00244447 loss)
I0801 14:02:35.552773 24522 sgd_solver.cpp:136] Iteration 27000, lr = 0.00578125, m = 0.9
I0801 14:02:37.381306 24522 solver.cpp:353] Iteration 27100 (54.6896 iter/s, 1.8285s/100 iter), loss = 0.00411485
I0801 14:02:37.381335 24522 solver.cpp:375]     Train net output #0: loss = 0.00411417 (* 1 = 0.00411417 loss)
I0801 14:02:37.381342 24522 sgd_solver.cpp:136] Iteration 27100, lr = 0.00576563, m = 0.9
I0801 14:02:38.981580 24522 solver.cpp:353] Iteration 27200 (62.4912 iter/s, 1.60022s/100 iter), loss = 0.000874988
I0801 14:02:38.981601 24522 solver.cpp:375]     Train net output #0: loss = 0.000874304 (* 1 = 0.000874304 loss)
I0801 14:02:38.981606 24522 sgd_solver.cpp:136] Iteration 27200, lr = 0.00575, m = 0.9
I0801 14:02:40.569185 24522 solver.cpp:353] Iteration 27300 (62.9899 iter/s, 1.58756s/100 iter), loss = 0.00108895
I0801 14:02:40.569208 24522 solver.cpp:375]     Train net output #0: loss = 0.00108827 (* 1 = 0.00108827 loss)
I0801 14:02:40.569216 24522 sgd_solver.cpp:136] Iteration 27300, lr = 0.00573438, m = 0.9
I0801 14:02:42.152681 24522 solver.cpp:353] Iteration 27400 (63.1533 iter/s, 1.58345s/100 iter), loss = 0.000591896
I0801 14:02:42.152709 24522 solver.cpp:375]     Train net output #0: loss = 0.00059121 (* 1 = 0.00059121 loss)
I0801 14:02:42.152714 24522 sgd_solver.cpp:136] Iteration 27400, lr = 0.00571875, m = 0.9
I0801 14:02:43.725005 24522 solver.cpp:353] Iteration 27500 (63.6023 iter/s, 1.57227s/100 iter), loss = 0.000968419
I0801 14:02:43.725036 24522 solver.cpp:375]     Train net output #0: loss = 0.000967733 (* 1 = 0.000967733 loss)
I0801 14:02:43.725044 24522 sgd_solver.cpp:136] Iteration 27500, lr = 0.00570312, m = 0.9
I0801 14:02:45.313719 24522 solver.cpp:353] Iteration 27600 (62.9461 iter/s, 1.58866s/100 iter), loss = 0.00258634
I0801 14:02:45.313771 24522 solver.cpp:375]     Train net output #0: loss = 0.00258566 (* 1 = 0.00258566 loss)
I0801 14:02:45.313784 24522 sgd_solver.cpp:136] Iteration 27600, lr = 0.0056875, m = 0.9
I0801 14:02:46.907027 24522 solver.cpp:353] Iteration 27700 (62.7644 iter/s, 1.59326s/100 iter), loss = 0.00035698
I0801 14:02:46.907074 24522 solver.cpp:375]     Train net output #0: loss = 0.000356294 (* 1 = 0.000356294 loss)
I0801 14:02:46.907230 24522 sgd_solver.cpp:136] Iteration 27700, lr = 0.00567187, m = 0.9
I0801 14:02:48.494879 24522 solver.cpp:353] Iteration 27800 (62.9801 iter/s, 1.5878s/100 iter), loss = 0.00169794
I0801 14:02:48.494928 24522 solver.cpp:375]     Train net output #0: loss = 0.00169725 (* 1 = 0.00169725 loss)
I0801 14:02:48.494940 24522 sgd_solver.cpp:136] Iteration 27800, lr = 0.00565625, m = 0.9
I0801 14:02:50.100929 24522 solver.cpp:353] Iteration 27900 (62.2665 iter/s, 1.606s/100 iter), loss = 0.000554908
I0801 14:02:50.100952 24522 solver.cpp:375]     Train net output #0: loss = 0.000554223 (* 1 = 0.000554223 loss)
I0801 14:02:50.100956 24522 sgd_solver.cpp:136] Iteration 27900, lr = 0.00564062, m = 0.9
I0801 14:02:51.673272 24522 solver.cpp:404] Sparsity after update:
I0801 14:02:51.674899 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:02:51.674907 24522 net.cpp:2270] conv1a_param_0(0.205) 
I0801 14:02:51.674914 24522 net.cpp:2270] conv1b_param_0(0.457) 
I0801 14:02:51.674917 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:02:51.674921 24522 net.cpp:2270] res2a_branch2a_param_0(0.479) 
I0801 14:02:51.674922 24522 net.cpp:2270] res2a_branch2b_param_0(0.479) 
I0801 14:02:51.674926 24522 net.cpp:2270] res3a_branch2a_param_0(0.479) 
I0801 14:02:51.674927 24522 net.cpp:2270] res3a_branch2b_param_0(0.479) 
I0801 14:02:51.674929 24522 net.cpp:2270] res4a_branch2a_param_0(0.479) 
I0801 14:02:51.674932 24522 net.cpp:2270] res4a_branch2b_param_0(0.479) 
I0801 14:02:51.674935 24522 net.cpp:2270] res5a_branch2a_param_0(0.455) 
I0801 14:02:51.674937 24522 net.cpp:2270] res5a_branch2b_param_0(0.476) 
I0801 14:02:51.674939 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.09693e+06/2.3599e+06) 0.465
I0801 14:02:51.674959 24522 solver.cpp:550] Iteration 28000, Testing net (#0)
I0801 14:02:52.486750 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.911472
I0801 14:02:52.486773 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 14:02:52.486781 24522 solver.cpp:635]     Test net output #2: loss = 0.369701 (* 1 = 0.369701 loss)
I0801 14:02:52.486804 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.811816s
I0801 14:02:52.508786 24549 solver.cpp:450] Finding and applying sparsity: 0.5
I0801 14:03:11.925927 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:03:11.927820 24522 solver.cpp:353] Iteration 28000 (4.58163 iter/s, 21.8263s/100 iter), loss = 0.0033412
I0801 14:03:11.927845 24522 solver.cpp:375]     Train net output #0: loss = 0.00334052 (* 1 = 0.00334052 loss)
I0801 14:03:11.927855 24522 sgd_solver.cpp:136] Iteration 28000, lr = 0.005625, m = 0.9
I0801 14:03:13.726022 24522 solver.cpp:353] Iteration 28100 (55.613 iter/s, 1.79814s/100 iter), loss = 0.00172988
I0801 14:03:13.726073 24522 solver.cpp:375]     Train net output #0: loss = 0.00172919 (* 1 = 0.00172919 loss)
I0801 14:03:13.726089 24522 sgd_solver.cpp:136] Iteration 28100, lr = 0.00560937, m = 0.9
I0801 14:03:15.316104 24522 solver.cpp:353] Iteration 28200 (62.8917 iter/s, 1.59003s/100 iter), loss = 0.000275048
I0801 14:03:15.316130 24522 solver.cpp:375]     Train net output #0: loss = 0.000274363 (* 1 = 0.000274363 loss)
I0801 14:03:15.316136 24522 sgd_solver.cpp:136] Iteration 28200, lr = 0.00559375, m = 0.9
I0801 14:03:16.930045 24522 solver.cpp:353] Iteration 28300 (61.9622 iter/s, 1.61389s/100 iter), loss = 0.000165993
I0801 14:03:16.930073 24522 solver.cpp:375]     Train net output #0: loss = 0.000165308 (* 1 = 0.000165308 loss)
I0801 14:03:16.930078 24522 sgd_solver.cpp:136] Iteration 28300, lr = 0.00557812, m = 0.9
I0801 14:03:18.510673 24522 solver.cpp:353] Iteration 28400 (63.268 iter/s, 1.58058s/100 iter), loss = 0.00266383
I0801 14:03:18.510720 24522 solver.cpp:375]     Train net output #0: loss = 0.00266314 (* 1 = 0.00266314 loss)
I0801 14:03:18.510732 24522 sgd_solver.cpp:136] Iteration 28400, lr = 0.0055625, m = 0.9
I0801 14:03:20.090108 24522 solver.cpp:353] Iteration 28500 (63.3158 iter/s, 1.57939s/100 iter), loss = 0.00164701
I0801 14:03:20.090133 24522 solver.cpp:375]     Train net output #0: loss = 0.00164632 (* 1 = 0.00164632 loss)
I0801 14:03:20.090139 24522 sgd_solver.cpp:136] Iteration 28500, lr = 0.00554687, m = 0.9
I0801 14:03:21.669046 24522 solver.cpp:353] Iteration 28600 (63.3358 iter/s, 1.57889s/100 iter), loss = 0.000725548
I0801 14:03:21.669070 24522 solver.cpp:375]     Train net output #0: loss = 0.000724866 (* 1 = 0.000724866 loss)
I0801 14:03:21.669075 24522 sgd_solver.cpp:136] Iteration 28600, lr = 0.00553125, m = 0.9
I0801 14:03:23.253962 24522 solver.cpp:353] Iteration 28700 (63.0968 iter/s, 1.58487s/100 iter), loss = 0.00091369
I0801 14:03:23.254016 24522 solver.cpp:375]     Train net output #0: loss = 0.000913008 (* 1 = 0.000913008 loss)
I0801 14:03:23.254024 24522 sgd_solver.cpp:136] Iteration 28700, lr = 0.00551562, m = 0.9
I0801 14:03:24.857395 24522 solver.cpp:353] Iteration 28800 (62.3683 iter/s, 1.60338s/100 iter), loss = 0.000768929
I0801 14:03:24.857422 24522 solver.cpp:375]     Train net output #0: loss = 0.000768246 (* 1 = 0.000768246 loss)
I0801 14:03:24.857430 24522 sgd_solver.cpp:136] Iteration 28800, lr = 0.0055, m = 0.9
I0801 14:03:26.449237 24522 solver.cpp:353] Iteration 28900 (62.8223 iter/s, 1.59179s/100 iter), loss = 0.00406896
I0801 14:03:26.449288 24522 solver.cpp:375]     Train net output #0: loss = 0.00406828 (* 1 = 0.00406828 loss)
I0801 14:03:26.449301 24522 sgd_solver.cpp:136] Iteration 28900, lr = 0.00548437, m = 0.9
I0801 14:03:28.035491 24522 solver.cpp:404] Sparsity after update:
I0801 14:03:28.037149 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:03:28.037158 24522 net.cpp:2270] conv1a_param_0(0.213) 
I0801 14:03:28.037165 24522 net.cpp:2270] conv1b_param_0(0.5) 
I0801 14:03:28.037168 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:03:28.037171 24522 net.cpp:2270] res2a_branch2a_param_0(0.5) 
I0801 14:03:28.037173 24522 net.cpp:2270] res2a_branch2b_param_0(0.5) 
I0801 14:03:28.037175 24522 net.cpp:2270] res3a_branch2a_param_0(0.5) 
I0801 14:03:28.037178 24522 net.cpp:2270] res3a_branch2b_param_0(0.5) 
I0801 14:03:28.037180 24522 net.cpp:2270] res4a_branch2a_param_0(0.5) 
I0801 14:03:28.037183 24522 net.cpp:2270] res4a_branch2b_param_0(0.5) 
I0801 14:03:28.037184 24522 net.cpp:2270] res5a_branch2a_param_0(0.477) 
I0801 14:03:28.037187 24522 net.cpp:2270] res5a_branch2b_param_0(0.497) 
I0801 14:03:28.037189 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.14827e+06/2.3599e+06) 0.487
I0801 14:03:28.037212 24522 solver.cpp:550] Iteration 29000, Testing net (#0)
I0801 14:03:28.843344 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.915589
I0801 14:03:28.843364 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 14:03:28.843369 24522 solver.cpp:635]     Test net output #2: loss = 0.33576 (* 1 = 0.33576 loss)
I0801 14:03:28.843382 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.806142s
I0801 14:03:28.859042 24549 solver.cpp:450] Finding and applying sparsity: 0.52
I0801 14:03:48.609366 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:03:48.611320 24522 solver.cpp:353] Iteration 29000 (4.51234 iter/s, 22.1615s/100 iter), loss = 0.000157744
I0801 14:03:48.611340 24522 solver.cpp:375]     Train net output #0: loss = 0.000157061 (* 1 = 0.000157061 loss)
I0801 14:03:48.611344 24522 sgd_solver.cpp:136] Iteration 29000, lr = 0.00546875, m = 0.9
I0801 14:03:50.405907 24522 solver.cpp:353] Iteration 29100 (55.725 iter/s, 1.79453s/100 iter), loss = 0.0015412
I0801 14:03:50.405987 24522 solver.cpp:375]     Train net output #0: loss = 0.00154051 (* 1 = 0.00154051 loss)
I0801 14:03:50.406009 24522 sgd_solver.cpp:136] Iteration 29100, lr = 0.00545313, m = 0.9
I0801 14:03:52.009536 24522 solver.cpp:353] Iteration 29200 (62.3605 iter/s, 1.60358s/100 iter), loss = 0.000392898
I0801 14:03:52.009583 24522 solver.cpp:375]     Train net output #0: loss = 0.000392217 (* 1 = 0.000392217 loss)
I0801 14:03:52.009591 24522 sgd_solver.cpp:136] Iteration 29200, lr = 0.0054375, m = 0.9
I0801 14:03:53.586814 24522 solver.cpp:353] Iteration 29300 (63.4024 iter/s, 1.57723s/100 iter), loss = 0.000653554
I0801 14:03:53.586935 24522 solver.cpp:375]     Train net output #0: loss = 0.000652872 (* 1 = 0.000652872 loss)
I0801 14:03:53.586942 24522 sgd_solver.cpp:136] Iteration 29300, lr = 0.00542188, m = 0.9
I0801 14:03:55.176892 24522 solver.cpp:353] Iteration 29400 (62.892 iter/s, 1.59003s/100 iter), loss = 0.00232988
I0801 14:03:55.176957 24522 solver.cpp:375]     Train net output #0: loss = 0.0023292 (* 1 = 0.0023292 loss)
I0801 14:03:55.176975 24522 sgd_solver.cpp:136] Iteration 29400, lr = 0.00540625, m = 0.9
I0801 14:03:56.744509 24522 solver.cpp:353] Iteration 29500 (63.7932 iter/s, 1.56757s/100 iter), loss = 0.00128368
I0801 14:03:56.744532 24522 solver.cpp:375]     Train net output #0: loss = 0.001283 (* 1 = 0.001283 loss)
I0801 14:03:56.744537 24522 sgd_solver.cpp:136] Iteration 29500, lr = 0.00539062, m = 0.9
I0801 14:03:58.336319 24522 solver.cpp:353] Iteration 29600 (62.8236 iter/s, 1.59176s/100 iter), loss = 0.000563355
I0801 14:03:58.336349 24522 solver.cpp:375]     Train net output #0: loss = 0.000562674 (* 1 = 0.000562674 loss)
I0801 14:03:58.336355 24522 sgd_solver.cpp:136] Iteration 29600, lr = 0.005375, m = 0.9
I0801 14:03:59.924002 24522 solver.cpp:353] Iteration 29700 (62.987 iter/s, 1.58763s/100 iter), loss = 0.00328812
I0801 14:03:59.924062 24522 solver.cpp:375]     Train net output #0: loss = 0.00328744 (* 1 = 0.00328744 loss)
I0801 14:03:59.924082 24522 sgd_solver.cpp:136] Iteration 29700, lr = 0.00535937, m = 0.9
I0801 14:04:01.507500 24522 solver.cpp:353] Iteration 29800 (63.1533 iter/s, 1.58345s/100 iter), loss = 0.00116331
I0801 14:04:01.507529 24522 solver.cpp:375]     Train net output #0: loss = 0.00116262 (* 1 = 0.00116262 loss)
I0801 14:04:01.507534 24522 sgd_solver.cpp:136] Iteration 29800, lr = 0.00534375, m = 0.9
I0801 14:04:03.082367 24522 solver.cpp:353] Iteration 29900 (63.4994 iter/s, 1.57482s/100 iter), loss = 0.00119165
I0801 14:04:03.082415 24522 solver.cpp:375]     Train net output #0: loss = 0.00119096 (* 1 = 0.00119096 loss)
I0801 14:04:03.082427 24522 sgd_solver.cpp:136] Iteration 29900, lr = 0.00532812, m = 0.9
I0801 14:04:04.637586 24522 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_30000.caffemodel
I0801 14:04:04.645647 24522 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_30000.solverstate
I0801 14:04:04.649188 24522 solver.cpp:404] Sparsity after update:
I0801 14:04:04.651032 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:04:04.651041 24522 net.cpp:2270] conv1a_param_0(0.217) 
I0801 14:04:04.651051 24522 net.cpp:2270] conv1b_param_0(0.498) 
I0801 14:04:04.651056 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:04:04.651059 24522 net.cpp:2270] res2a_branch2a_param_0(0.517) 
I0801 14:04:04.651063 24522 net.cpp:2270] res2a_branch2b_param_0(0.514) 
I0801 14:04:04.651067 24522 net.cpp:2270] res3a_branch2a_param_0(0.519) 
I0801 14:04:04.651080 24522 net.cpp:2270] res3a_branch2b_param_0(0.517) 
I0801 14:04:04.651085 24522 net.cpp:2270] res4a_branch2a_param_0(0.52) 
I0801 14:04:04.651089 24522 net.cpp:2270] res4a_branch2b_param_0(0.519) 
I0801 14:04:04.651093 24522 net.cpp:2270] res5a_branch2a_param_0(0.494) 
I0801 14:04:04.651098 24522 net.cpp:2270] res5a_branch2b_param_0(0.517) 
I0801 14:04:04.651103 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.1906e+06/2.3599e+06) 0.505
I0801 14:04:04.651113 24522 solver.cpp:550] Iteration 30000, Testing net (#0)
I0801 14:04:05.451701 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.91853
I0801 14:04:05.451720 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 14:04:05.451727 24522 solver.cpp:635]     Test net output #2: loss = 0.321198 (* 1 = 0.321198 loss)
I0801 14:04:05.451743 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.800602s
I0801 14:04:05.467329 24549 solver.cpp:450] Finding and applying sparsity: 0.54
I0801 14:04:25.876719 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:04:25.878650 24522 solver.cpp:353] Iteration 30000 (4.38681 iter/s, 22.7956s/100 iter), loss = 0.00103505
I0801 14:04:25.878669 24522 solver.cpp:375]     Train net output #0: loss = 0.00103437 (* 1 = 0.00103437 loss)
I0801 14:04:25.878674 24522 sgd_solver.cpp:136] Iteration 30000, lr = 0.0053125, m = 0.9
I0801 14:04:27.723160 24522 solver.cpp:353] Iteration 30100 (54.2168 iter/s, 1.84445s/100 iter), loss = 0.00212161
I0801 14:04:27.723182 24522 solver.cpp:375]     Train net output #0: loss = 0.00212093 (* 1 = 0.00212093 loss)
I0801 14:04:27.723186 24522 sgd_solver.cpp:136] Iteration 30100, lr = 0.00529688, m = 0.9
I0801 14:04:29.311976 24522 solver.cpp:353] Iteration 30200 (62.9419 iter/s, 1.58877s/100 iter), loss = 0.00112022
I0801 14:04:29.312002 24522 solver.cpp:375]     Train net output #0: loss = 0.00111954 (* 1 = 0.00111954 loss)
I0801 14:04:29.312008 24522 sgd_solver.cpp:136] Iteration 30200, lr = 0.00528125, m = 0.9
I0801 14:04:30.884639 24522 solver.cpp:353] Iteration 30300 (63.5884 iter/s, 1.57261s/100 iter), loss = 0.000508138
I0801 14:04:30.884663 24522 solver.cpp:375]     Train net output #0: loss = 0.000507456 (* 1 = 0.000507456 loss)
I0801 14:04:30.884670 24522 sgd_solver.cpp:136] Iteration 30300, lr = 0.00526563, m = 0.9
I0801 14:04:32.478104 24522 solver.cpp:353] Iteration 30400 (62.7584 iter/s, 1.59341s/100 iter), loss = 0.00455598
I0801 14:04:32.478133 24522 solver.cpp:375]     Train net output #0: loss = 0.0045553 (* 1 = 0.0045553 loss)
I0801 14:04:32.478137 24522 sgd_solver.cpp:136] Iteration 30400, lr = 0.00525, m = 0.9
I0801 14:04:34.062613 24522 solver.cpp:353] Iteration 30500 (63.1131 iter/s, 1.58446s/100 iter), loss = 0.00179638
I0801 14:04:34.062640 24522 solver.cpp:375]     Train net output #0: loss = 0.00179569 (* 1 = 0.00179569 loss)
I0801 14:04:34.062644 24522 sgd_solver.cpp:136] Iteration 30500, lr = 0.00523437, m = 0.9
I0801 14:04:35.645773 24522 solver.cpp:353] Iteration 30600 (63.1667 iter/s, 1.58311s/100 iter), loss = 0.00195572
I0801 14:04:35.645823 24522 solver.cpp:375]     Train net output #0: loss = 0.00195503 (* 1 = 0.00195503 loss)
I0801 14:04:35.645835 24522 sgd_solver.cpp:136] Iteration 30600, lr = 0.00521875, m = 0.9
I0801 14:04:37.243264 24522 solver.cpp:353] Iteration 30700 (62.6001 iter/s, 1.59744s/100 iter), loss = 0.000156336
I0801 14:04:37.243293 24522 solver.cpp:375]     Train net output #0: loss = 0.000155652 (* 1 = 0.000155652 loss)
I0801 14:04:37.243297 24522 sgd_solver.cpp:136] Iteration 30700, lr = 0.00520312, m = 0.9
I0801 14:04:38.834988 24522 solver.cpp:353] Iteration 30800 (62.827 iter/s, 1.59167s/100 iter), loss = 0.000737585
I0801 14:04:38.835013 24522 solver.cpp:375]     Train net output #0: loss = 0.000736901 (* 1 = 0.000736901 loss)
I0801 14:04:38.835019 24522 sgd_solver.cpp:136] Iteration 30800, lr = 0.0051875, m = 0.9
I0801 14:04:40.426770 24522 solver.cpp:353] Iteration 30900 (62.8246 iter/s, 1.59173s/100 iter), loss = 0.00145899
I0801 14:04:40.426795 24522 solver.cpp:375]     Train net output #0: loss = 0.0014583 (* 1 = 0.0014583 loss)
I0801 14:04:40.426800 24522 sgd_solver.cpp:136] Iteration 30900, lr = 0.00517187, m = 0.9
I0801 14:04:41.985601 24522 solver.cpp:404] Sparsity after update:
I0801 14:04:41.987268 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:04:41.987277 24522 net.cpp:2270] conv1a_param_0(0.234) 
I0801 14:04:41.987287 24522 net.cpp:2270] conv1b_param_0(0.528) 
I0801 14:04:41.987296 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:04:41.987301 24522 net.cpp:2270] res2a_branch2a_param_0(0.538) 
I0801 14:04:41.987309 24522 net.cpp:2270] res2a_branch2b_param_0(0.534) 
I0801 14:04:41.987314 24522 net.cpp:2270] res3a_branch2a_param_0(0.54) 
I0801 14:04:41.987321 24522 net.cpp:2270] res3a_branch2b_param_0(0.538) 
I0801 14:04:41.987326 24522 net.cpp:2270] res4a_branch2a_param_0(0.54) 
I0801 14:04:41.987334 24522 net.cpp:2270] res4a_branch2b_param_0(0.54) 
I0801 14:04:41.987337 24522 net.cpp:2270] res5a_branch2a_param_0(0.514) 
I0801 14:04:41.987345 24522 net.cpp:2270] res5a_branch2b_param_0(0.537) 
I0801 14:04:41.987349 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.23835e+06/2.3599e+06) 0.525
I0801 14:04:41.987375 24522 solver.cpp:550] Iteration 31000, Testing net (#0)
I0801 14:04:42.271692 24520 data_reader.cpp:264] Starting prefetch of epoch 4
I0801 14:04:42.803303 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.920883
I0801 14:04:42.803321 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.998529
I0801 14:04:42.803328 24522 solver.cpp:635]     Test net output #2: loss = 0.308905 (* 1 = 0.308905 loss)
I0801 14:04:42.803345 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.815942s
I0801 14:04:42.818851 24549 solver.cpp:450] Finding and applying sparsity: 0.56
I0801 14:05:04.068400 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:05:04.070423 24522 solver.cpp:353] Iteration 31000 (4.22959 iter/s, 23.643s/100 iter), loss = 0.000266724
I0801 14:05:04.070443 24522 solver.cpp:375]     Train net output #0: loss = 0.00026604 (* 1 = 0.00026604 loss)
I0801 14:05:04.070448 24522 sgd_solver.cpp:136] Iteration 31000, lr = 0.00515625, m = 0.9
I0801 14:05:05.888232 24522 solver.cpp:353] Iteration 31100 (55.013 iter/s, 1.81775s/100 iter), loss = 0.00394839
I0801 14:05:05.888258 24522 solver.cpp:375]     Train net output #0: loss = 0.0039477 (* 1 = 0.0039477 loss)
I0801 14:05:05.888264 24522 sgd_solver.cpp:136] Iteration 31100, lr = 0.00514062, m = 0.9
I0801 14:05:07.480041 24522 solver.cpp:353] Iteration 31200 (62.8237 iter/s, 1.59175s/100 iter), loss = 0.000950691
I0801 14:05:07.480064 24522 solver.cpp:375]     Train net output #0: loss = 0.000950009 (* 1 = 0.000950009 loss)
I0801 14:05:07.480069 24522 sgd_solver.cpp:136] Iteration 31200, lr = 0.005125, m = 0.9
I0801 14:05:09.084508 24522 solver.cpp:353] Iteration 31300 (62.3279 iter/s, 1.60442s/100 iter), loss = 0.00140777
I0801 14:05:09.084561 24522 solver.cpp:375]     Train net output #0: loss = 0.00140709 (* 1 = 0.00140709 loss)
I0801 14:05:09.084574 24522 sgd_solver.cpp:136] Iteration 31300, lr = 0.00510937, m = 0.9
I0801 14:05:10.679778 24522 solver.cpp:353] Iteration 31400 (62.6874 iter/s, 1.59522s/100 iter), loss = 0.00301608
I0801 14:05:10.679848 24522 solver.cpp:375]     Train net output #0: loss = 0.00301539 (* 1 = 0.00301539 loss)
I0801 14:05:10.679869 24522 sgd_solver.cpp:136] Iteration 31400, lr = 0.00509375, m = 0.9
I0801 14:05:12.254474 24522 solver.cpp:353] Iteration 31500 (63.5064 iter/s, 1.57464s/100 iter), loss = 0.00108426
I0801 14:05:12.254498 24522 solver.cpp:375]     Train net output #0: loss = 0.00108357 (* 1 = 0.00108357 loss)
I0801 14:05:12.254503 24522 sgd_solver.cpp:136] Iteration 31500, lr = 0.00507812, m = 0.9
I0801 14:05:13.825881 24522 solver.cpp:353] Iteration 31600 (63.6393 iter/s, 1.57136s/100 iter), loss = 0.00465516
I0801 14:05:13.825908 24522 solver.cpp:375]     Train net output #0: loss = 0.00465447 (* 1 = 0.00465447 loss)
I0801 14:05:13.825913 24522 sgd_solver.cpp:136] Iteration 31600, lr = 0.0050625, m = 0.9
I0801 14:05:15.397774 24522 solver.cpp:353] Iteration 31700 (63.6195 iter/s, 1.57185s/100 iter), loss = 0.000435742
I0801 14:05:15.397825 24522 solver.cpp:375]     Train net output #0: loss = 0.000435058 (* 1 = 0.000435058 loss)
I0801 14:05:15.397836 24522 sgd_solver.cpp:136] Iteration 31700, lr = 0.00504687, m = 0.9
I0801 14:05:16.979081 24522 solver.cpp:353] Iteration 31800 (63.2408 iter/s, 1.58126s/100 iter), loss = 0.000659436
I0801 14:05:16.979130 24522 solver.cpp:375]     Train net output #0: loss = 0.000658751 (* 1 = 0.000658751 loss)
I0801 14:05:16.979142 24522 sgd_solver.cpp:136] Iteration 31800, lr = 0.00503125, m = 0.9
I0801 14:05:18.561480 24522 solver.cpp:353] Iteration 31900 (63.1972 iter/s, 1.58235s/100 iter), loss = 0.00112373
I0801 14:05:18.561525 24522 solver.cpp:375]     Train net output #0: loss = 0.00112305 (* 1 = 0.00112305 loss)
I0801 14:05:18.561533 24522 sgd_solver.cpp:136] Iteration 31900, lr = 0.00501562, m = 0.9
I0801 14:05:20.123788 24522 solver.cpp:404] Sparsity after update:
I0801 14:05:20.125413 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:05:20.125422 24522 net.cpp:2270] conv1a_param_0(0.239) 
I0801 14:05:20.125429 24522 net.cpp:2270] conv1b_param_0(0.556) 
I0801 14:05:20.125432 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:05:20.125434 24522 net.cpp:2270] res2a_branch2a_param_0(0.559) 
I0801 14:05:20.125438 24522 net.cpp:2270] res2a_branch2b_param_0(0.554) 
I0801 14:05:20.125442 24522 net.cpp:2270] res3a_branch2a_param_0(0.559) 
I0801 14:05:20.125447 24522 net.cpp:2270] res3a_branch2b_param_0(0.559) 
I0801 14:05:20.125452 24522 net.cpp:2270] res4a_branch2a_param_0(0.56) 
I0801 14:05:20.125454 24522 net.cpp:2270] res4a_branch2b_param_0(0.559) 
I0801 14:05:20.125458 24522 net.cpp:2270] res5a_branch2a_param_0(0.53) 
I0801 14:05:20.125461 24522 net.cpp:2270] res5a_branch2b_param_0(0.557) 
I0801 14:05:20.125465 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.28034e+06/2.3599e+06) 0.543
I0801 14:05:20.125494 24522 solver.cpp:550] Iteration 32000, Testing net (#0)
I0801 14:05:20.933603 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.909413
I0801 14:05:20.933620 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 14:05:20.933625 24522 solver.cpp:635]     Test net output #2: loss = 0.363852 (* 1 = 0.363852 loss)
I0801 14:05:20.933640 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.808118s
I0801 14:05:20.949180 24549 solver.cpp:450] Finding and applying sparsity: 0.58
I0801 14:05:43.964661 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:05:43.967488 24522 solver.cpp:353] Iteration 32000 (3.93619 iter/s, 25.4053s/100 iter), loss = 0.00194346
I0801 14:05:43.967510 24522 solver.cpp:375]     Train net output #0: loss = 0.00194278 (* 1 = 0.00194278 loss)
I0801 14:05:43.967519 24522 sgd_solver.cpp:136] Iteration 32000, lr = 0.005, m = 0.9
I0801 14:05:46.122074 24522 solver.cpp:353] Iteration 32100 (46.4144 iter/s, 2.15451s/100 iter), loss = 0.000252229
I0801 14:05:46.122162 24522 solver.cpp:375]     Train net output #0: loss = 0.000251544 (* 1 = 0.000251544 loss)
I0801 14:05:46.122192 24522 sgd_solver.cpp:136] Iteration 32100, lr = 0.00498438, m = 0.9
I0801 14:05:47.732707 24522 solver.cpp:353] Iteration 32200 (62.0891 iter/s, 1.61059s/100 iter), loss = 0.00838257
I0801 14:05:47.732755 24522 solver.cpp:375]     Train net output #0: loss = 0.00838189 (* 1 = 0.00838189 loss)
I0801 14:05:47.732764 24522 sgd_solver.cpp:136] Iteration 32200, lr = 0.00496875, m = 0.9
I0801 14:05:49.363518 24522 solver.cpp:353] Iteration 32300 (61.3212 iter/s, 1.63076s/100 iter), loss = 0.00626172
I0801 14:05:49.363543 24522 solver.cpp:375]     Train net output #0: loss = 0.00626103 (* 1 = 0.00626103 loss)
I0801 14:05:49.363549 24522 sgd_solver.cpp:136] Iteration 32300, lr = 0.00495313, m = 0.9
I0801 14:05:51.162928 24522 solver.cpp:353] Iteration 32400 (55.5756 iter/s, 1.79935s/100 iter), loss = 0.000468706
I0801 14:05:51.162966 24522 solver.cpp:375]     Train net output #0: loss = 0.000468021 (* 1 = 0.000468021 loss)
I0801 14:05:51.162976 24522 sgd_solver.cpp:136] Iteration 32400, lr = 0.0049375, m = 0.9
I0801 14:05:52.836819 24522 solver.cpp:353] Iteration 32500 (59.7431 iter/s, 1.67383s/100 iter), loss = 0.000786399
I0801 14:05:52.836846 24522 solver.cpp:375]     Train net output #0: loss = 0.000785715 (* 1 = 0.000785715 loss)
I0801 14:05:52.836851 24522 sgd_solver.cpp:136] Iteration 32500, lr = 0.00492187, m = 0.9
I0801 14:05:54.526526 24522 solver.cpp:353] Iteration 32600 (59.1836 iter/s, 1.68966s/100 iter), loss = 0.00193591
I0801 14:05:54.526551 24522 solver.cpp:375]     Train net output #0: loss = 0.00193522 (* 1 = 0.00193522 loss)
I0801 14:05:54.526556 24522 sgd_solver.cpp:136] Iteration 32600, lr = 0.00490625, m = 0.9
I0801 14:05:56.232028 24522 solver.cpp:353] Iteration 32700 (58.6356 iter/s, 1.70545s/100 iter), loss = 0.000320954
I0801 14:05:56.232077 24522 solver.cpp:375]     Train net output #0: loss = 0.00032027 (* 1 = 0.00032027 loss)
I0801 14:05:56.232089 24522 sgd_solver.cpp:136] Iteration 32700, lr = 0.00489062, m = 0.9
I0801 14:05:57.937086 24522 solver.cpp:353] Iteration 32800 (58.6508 iter/s, 1.70501s/100 iter), loss = 0.000869558
I0801 14:05:57.937135 24522 solver.cpp:375]     Train net output #0: loss = 0.000868874 (* 1 = 0.000868874 loss)
I0801 14:05:57.937147 24522 sgd_solver.cpp:136] Iteration 32800, lr = 0.004875, m = 0.9
I0801 14:05:59.645192 24522 solver.cpp:353] Iteration 32900 (58.5462 iter/s, 1.70805s/100 iter), loss = 0.00133214
I0801 14:05:59.645215 24522 solver.cpp:375]     Train net output #0: loss = 0.00133146 (* 1 = 0.00133146 loss)
I0801 14:05:59.645220 24522 sgd_solver.cpp:136] Iteration 32900, lr = 0.00485937, m = 0.9
I0801 14:06:01.405740 24522 solver.cpp:404] Sparsity after update:
I0801 14:06:01.408229 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:06:01.408248 24522 net.cpp:2270] conv1a_param_0(0.248) 
I0801 14:06:01.408265 24522 net.cpp:2270] conv1b_param_0(0.569) 
I0801 14:06:01.408272 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:06:01.408280 24522 net.cpp:2270] res2a_branch2a_param_0(0.58) 
I0801 14:06:01.408287 24522 net.cpp:2270] res2a_branch2b_param_0(0.573) 
I0801 14:06:01.408293 24522 net.cpp:2270] res3a_branch2a_param_0(0.58) 
I0801 14:06:01.408300 24522 net.cpp:2270] res3a_branch2b_param_0(0.58) 
I0801 14:06:01.408308 24522 net.cpp:2270] res4a_branch2a_param_0(0.58) 
I0801 14:06:01.408314 24522 net.cpp:2270] res4a_branch2b_param_0(0.58) 
I0801 14:06:01.408323 24522 net.cpp:2270] res5a_branch2a_param_0(0.549) 
I0801 14:06:01.408329 24522 net.cpp:2270] res5a_branch2b_param_0(0.577) 
I0801 14:06:01.408336 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.32676e+06/2.3599e+06) 0.562
I0801 14:06:01.408372 24522 solver.cpp:550] Iteration 33000, Testing net (#0)
I0801 14:06:02.259765 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.907648
I0801 14:06:02.259806 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997353
I0801 14:06:02.259824 24522 solver.cpp:635]     Test net output #2: loss = 0.377064 (* 1 = 0.377064 loss)
I0801 14:06:02.259860 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.851454s
I0801 14:06:02.276757 24549 solver.cpp:450] Finding and applying sparsity: 0.6
I0801 14:06:32.441519 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:06:32.443683 24522 solver.cpp:353] Iteration 33000 (3.04901 iter/s, 32.7976s/100 iter), loss = 0.00166459
I0801 14:06:32.443716 24522 solver.cpp:375]     Train net output #0: loss = 0.00166391 (* 1 = 0.00166391 loss)
I0801 14:06:32.443725 24522 sgd_solver.cpp:136] Iteration 33000, lr = 0.00484375, m = 0.9
I0801 14:06:34.456316 24522 solver.cpp:353] Iteration 33100 (49.6879 iter/s, 2.01256s/100 iter), loss = 0.010466
I0801 14:06:34.456360 24522 solver.cpp:375]     Train net output #0: loss = 0.0104653 (* 1 = 0.0104653 loss)
I0801 14:06:34.456504 24522 sgd_solver.cpp:136] Iteration 33100, lr = 0.00482813, m = 0.9
I0801 14:06:36.163357 24522 solver.cpp:353] Iteration 33200 (58.5834 iter/s, 1.70697s/100 iter), loss = 0.000222067
I0801 14:06:36.163646 24522 solver.cpp:375]     Train net output #0: loss = 0.000221384 (* 1 = 0.000221384 loss)
I0801 14:06:36.163805 24522 sgd_solver.cpp:136] Iteration 33200, lr = 0.0048125, m = 0.9
I0801 14:06:37.785192 24522 solver.cpp:353] Iteration 33300 (61.6602 iter/s, 1.62179s/100 iter), loss = 0.00110086
I0801 14:06:37.785220 24522 solver.cpp:375]     Train net output #0: loss = 0.00110017 (* 1 = 0.00110017 loss)
I0801 14:06:37.785228 24522 sgd_solver.cpp:136] Iteration 33300, lr = 0.00479688, m = 0.9
I0801 14:06:39.391163 24522 solver.cpp:353] Iteration 33400 (62.2697 iter/s, 1.60592s/100 iter), loss = 0.00038006
I0801 14:06:39.391186 24522 solver.cpp:375]     Train net output #0: loss = 0.000379376 (* 1 = 0.000379376 loss)
I0801 14:06:39.391191 24522 sgd_solver.cpp:136] Iteration 33400, lr = 0.00478125, m = 0.9
I0801 14:06:40.991514 24522 solver.cpp:353] Iteration 33500 (62.4884 iter/s, 1.6003s/100 iter), loss = 0.00146767
I0801 14:06:40.991544 24522 solver.cpp:375]     Train net output #0: loss = 0.00146699 (* 1 = 0.00146699 loss)
I0801 14:06:40.991551 24522 sgd_solver.cpp:136] Iteration 33500, lr = 0.00476563, m = 0.9
I0801 14:06:42.586542 24522 solver.cpp:353] Iteration 33600 (62.6967 iter/s, 1.59498s/100 iter), loss = 0.000466732
I0801 14:06:42.586565 24522 solver.cpp:375]     Train net output #0: loss = 0.00046605 (* 1 = 0.00046605 loss)
I0801 14:06:42.586570 24522 sgd_solver.cpp:136] Iteration 33600, lr = 0.00475, m = 0.9
I0801 14:06:44.175961 24522 solver.cpp:353] Iteration 33700 (62.9182 iter/s, 1.58937s/100 iter), loss = 0.00111176
I0801 14:06:44.175992 24522 solver.cpp:375]     Train net output #0: loss = 0.00111108 (* 1 = 0.00111108 loss)
I0801 14:06:44.175998 24522 sgd_solver.cpp:136] Iteration 33700, lr = 0.00473437, m = 0.9
I0801 14:06:45.890115 24522 solver.cpp:353] Iteration 33800 (58.3397 iter/s, 1.7141s/100 iter), loss = 0.00480793
I0801 14:06:45.890161 24522 solver.cpp:375]     Train net output #0: loss = 0.00480724 (* 1 = 0.00480724 loss)
I0801 14:06:45.890172 24522 sgd_solver.cpp:136] Iteration 33800, lr = 0.00471875, m = 0.9
I0801 14:06:47.577992 24522 solver.cpp:353] Iteration 33900 (59.2478 iter/s, 1.68783s/100 iter), loss = 0.00157025
I0801 14:06:47.578019 24522 solver.cpp:375]     Train net output #0: loss = 0.00156957 (* 1 = 0.00156957 loss)
I0801 14:06:47.578025 24522 sgd_solver.cpp:136] Iteration 33900, lr = 0.00470312, m = 0.9
I0801 14:06:49.134011 24522 solver.cpp:404] Sparsity after update:
I0801 14:06:49.135589 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:06:49.135597 24522 net.cpp:2270] conv1a_param_0(0.25) 
I0801 14:06:49.135604 24522 net.cpp:2270] conv1b_param_0(0.596) 
I0801 14:06:49.135607 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:06:49.135609 24522 net.cpp:2270] res2a_branch2a_param_0(0.597) 
I0801 14:06:49.135612 24522 net.cpp:2270] res2a_branch2b_param_0(0.591) 
I0801 14:06:49.135614 24522 net.cpp:2270] res3a_branch2a_param_0(0.599) 
I0801 14:06:49.135617 24522 net.cpp:2270] res3a_branch2b_param_0(0.597) 
I0801 14:06:49.135618 24522 net.cpp:2270] res4a_branch2a_param_0(0.6) 
I0801 14:06:49.135622 24522 net.cpp:2270] res4a_branch2b_param_0(0.599) 
I0801 14:06:49.135623 24522 net.cpp:2270] res5a_branch2a_param_0(0.565) 
I0801 14:06:49.135627 24522 net.cpp:2270] res5a_branch2b_param_0(0.599) 
I0801 14:06:49.135628 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.36923e+06/2.3599e+06) 0.58
I0801 14:06:49.135646 24522 solver.cpp:550] Iteration 34000, Testing net (#0)
I0801 14:06:49.942524 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.899413
I0801 14:06:49.942543 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 14:06:49.942548 24522 solver.cpp:635]     Test net output #2: loss = 0.395503 (* 1 = 0.395503 loss)
I0801 14:06:49.942561 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.806887s
I0801 14:06:49.958849 24549 solver.cpp:450] Finding and applying sparsity: 0.62
I0801 14:07:20.742476 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:07:20.744415 24522 solver.cpp:353] Iteration 34000 (3.01518 iter/s, 33.1655s/100 iter), loss = 0.00122027
I0801 14:07:20.744432 24522 solver.cpp:375]     Train net output #0: loss = 0.00121958 (* 1 = 0.00121958 loss)
I0801 14:07:20.744437 24522 sgd_solver.cpp:136] Iteration 34000, lr = 0.0046875, m = 0.9
I0801 14:07:22.673514 24522 solver.cpp:353] Iteration 34100 (51.8393 iter/s, 1.92904s/100 iter), loss = 0.00212065
I0801 14:07:22.673562 24522 solver.cpp:375]     Train net output #0: loss = 0.00211996 (* 1 = 0.00211996 loss)
I0801 14:07:22.673573 24522 sgd_solver.cpp:136] Iteration 34100, lr = 0.00467187, m = 0.9
I0801 14:07:24.374264 24522 solver.cpp:353] Iteration 34200 (58.8001 iter/s, 1.70068s/100 iter), loss = 0.00101198
I0801 14:07:24.374346 24522 solver.cpp:375]     Train net output #0: loss = 0.00101129 (* 1 = 0.00101129 loss)
I0801 14:07:24.374363 24522 sgd_solver.cpp:136] Iteration 34200, lr = 0.00465625, m = 0.9
I0801 14:07:26.109272 24522 solver.cpp:353] Iteration 34300 (57.6381 iter/s, 1.73496s/100 iter), loss = 0.000643793
I0801 14:07:26.109298 24522 solver.cpp:375]     Train net output #0: loss = 0.000643105 (* 1 = 0.000643105 loss)
I0801 14:07:26.109304 24522 sgd_solver.cpp:136] Iteration 34300, lr = 0.00464062, m = 0.9
I0801 14:07:27.798760 24522 solver.cpp:353] Iteration 34400 (59.1915 iter/s, 1.68943s/100 iter), loss = 0.00125563
I0801 14:07:27.798785 24522 solver.cpp:375]     Train net output #0: loss = 0.00125494 (* 1 = 0.00125494 loss)
I0801 14:07:27.798791 24522 sgd_solver.cpp:136] Iteration 34400, lr = 0.004625, m = 0.9
I0801 14:07:29.510670 24522 solver.cpp:353] Iteration 34500 (58.4163 iter/s, 1.71185s/100 iter), loss = 0.000458118
I0801 14:07:29.510715 24522 solver.cpp:375]     Train net output #0: loss = 0.000457431 (* 1 = 0.000457431 loss)
I0801 14:07:29.510725 24522 sgd_solver.cpp:136] Iteration 34500, lr = 0.00460937, m = 0.9
I0801 14:07:31.218269 24522 solver.cpp:353] Iteration 34600 (58.5646 iter/s, 1.70752s/100 iter), loss = 0.00183656
I0801 14:07:31.219427 24522 solver.cpp:375]     Train net output #0: loss = 0.00183587 (* 1 = 0.00183587 loss)
I0801 14:07:31.219542 24522 sgd_solver.cpp:136] Iteration 34600, lr = 0.00459375, m = 0.9
I0801 14:07:32.923406 24522 solver.cpp:353] Iteration 34700 (58.6477 iter/s, 1.7051s/100 iter), loss = 0.00396041
I0801 14:07:32.923429 24522 solver.cpp:375]     Train net output #0: loss = 0.00395972 (* 1 = 0.00395972 loss)
I0801 14:07:32.923435 24522 sgd_solver.cpp:136] Iteration 34700, lr = 0.00457812, m = 0.9
I0801 14:07:34.652912 24522 solver.cpp:353] Iteration 34800 (57.8218 iter/s, 1.72945s/100 iter), loss = 0.00166477
I0801 14:07:34.652935 24522 solver.cpp:375]     Train net output #0: loss = 0.00166408 (* 1 = 0.00166408 loss)
I0801 14:07:34.652942 24522 sgd_solver.cpp:136] Iteration 34800, lr = 0.0045625, m = 0.9
I0801 14:07:36.352766 24522 solver.cpp:353] Iteration 34900 (58.8307 iter/s, 1.69979s/100 iter), loss = 0.000569476
I0801 14:07:36.352807 24522 solver.cpp:375]     Train net output #0: loss = 0.00056879 (* 1 = 0.00056879 loss)
I0801 14:07:36.352823 24522 sgd_solver.cpp:136] Iteration 34900, lr = 0.00454687, m = 0.9
I0801 14:07:38.052497 24522 solver.cpp:404] Sparsity after update:
I0801 14:07:38.054111 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:07:38.054119 24522 net.cpp:2270] conv1a_param_0(0.26) 
I0801 14:07:38.054126 24522 net.cpp:2270] conv1b_param_0(0.609) 
I0801 14:07:38.054128 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:07:38.054131 24522 net.cpp:2270] res2a_branch2a_param_0(0.618) 
I0801 14:07:38.054133 24522 net.cpp:2270] res2a_branch2b_param_0(0.608) 
I0801 14:07:38.054136 24522 net.cpp:2270] res3a_branch2a_param_0(0.62) 
I0801 14:07:38.054139 24522 net.cpp:2270] res3a_branch2b_param_0(0.618) 
I0801 14:07:38.054142 24522 net.cpp:2270] res4a_branch2a_param_0(0.62) 
I0801 14:07:38.054145 24522 net.cpp:2270] res4a_branch2b_param_0(0.62) 
I0801 14:07:38.054148 24522 net.cpp:2270] res5a_branch2a_param_0(0.588) 
I0801 14:07:38.054152 24522 net.cpp:2270] res5a_branch2b_param_0(0.618) 
I0801 14:07:38.054155 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.42039e+06/2.3599e+06) 0.602
I0801 14:07:38.054184 24522 solver.cpp:550] Iteration 35000, Testing net (#0)
I0801 14:07:38.284687 24520 data_reader.cpp:264] Starting prefetch of epoch 5
I0801 14:07:38.917353 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.89706
I0801 14:07:38.917371 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 14:07:38.917376 24522 solver.cpp:635]     Test net output #2: loss = 0.4105 (* 1 = 0.4105 loss)
I0801 14:07:38.917392 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.863178s
I0801 14:07:38.933218 24549 solver.cpp:450] Finding and applying sparsity: 0.64
I0801 14:08:10.141633 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:08:10.143838 24522 solver.cpp:353] Iteration 35000 (2.95945 iter/s, 33.7901s/100 iter), loss = 0.0178318
I0801 14:08:10.143877 24522 solver.cpp:375]     Train net output #0: loss = 0.0178311 (* 1 = 0.0178311 loss)
I0801 14:08:10.143892 24522 sgd_solver.cpp:136] Iteration 35000, lr = 0.00453125, m = 0.9
I0801 14:08:12.214037 24522 solver.cpp:353] Iteration 35100 (48.3061 iter/s, 2.07013s/100 iter), loss = 0.00150618
I0801 14:08:12.214076 24522 solver.cpp:375]     Train net output #0: loss = 0.0015055 (* 1 = 0.0015055 loss)
I0801 14:08:12.214088 24522 sgd_solver.cpp:136] Iteration 35100, lr = 0.00451563, m = 0.9
I0801 14:08:13.854209 24522 solver.cpp:353] Iteration 35200 (60.971 iter/s, 1.64012s/100 iter), loss = 0.00050316
I0801 14:08:13.854261 24522 solver.cpp:375]     Train net output #0: loss = 0.000502479 (* 1 = 0.000502479 loss)
I0801 14:08:13.854285 24522 sgd_solver.cpp:136] Iteration 35200, lr = 0.0045, m = 0.9
I0801 14:08:15.659462 24522 solver.cpp:353] Iteration 35300 (55.3957 iter/s, 1.8052s/100 iter), loss = 0.00171614
I0801 14:08:15.659487 24522 solver.cpp:375]     Train net output #0: loss = 0.00171545 (* 1 = 0.00171545 loss)
I0801 14:08:15.659492 24522 sgd_solver.cpp:136] Iteration 35300, lr = 0.00448438, m = 0.9
I0801 14:08:17.321251 24522 solver.cpp:353] Iteration 35400 (60.1783 iter/s, 1.66173s/100 iter), loss = 0.000770561
I0801 14:08:17.321300 24522 solver.cpp:375]     Train net output #0: loss = 0.000769875 (* 1 = 0.000769875 loss)
I0801 14:08:17.321315 24522 sgd_solver.cpp:136] Iteration 35400, lr = 0.00446875, m = 0.9
I0801 14:08:18.946945 24522 solver.cpp:353] Iteration 35500 (61.5141 iter/s, 1.62564s/100 iter), loss = 0.00129598
I0801 14:08:18.946972 24522 solver.cpp:375]     Train net output #0: loss = 0.00129529 (* 1 = 0.00129529 loss)
I0801 14:08:18.946979 24522 sgd_solver.cpp:136] Iteration 35500, lr = 0.00445312, m = 0.9
I0801 14:08:20.677410 24522 solver.cpp:353] Iteration 35600 (57.79 iter/s, 1.7304s/100 iter), loss = 0.0021757
I0801 14:08:20.677480 24522 solver.cpp:375]     Train net output #0: loss = 0.00217501 (* 1 = 0.00217501 loss)
I0801 14:08:20.677500 24522 sgd_solver.cpp:136] Iteration 35600, lr = 0.0044375, m = 0.9
I0801 14:08:22.414221 24522 solver.cpp:353] Iteration 35700 (57.5788 iter/s, 1.73675s/100 iter), loss = 0.0013021
I0801 14:08:22.414263 24522 solver.cpp:375]     Train net output #0: loss = 0.00130141 (* 1 = 0.00130141 loss)
I0801 14:08:22.414273 24522 sgd_solver.cpp:136] Iteration 35700, lr = 0.00442187, m = 0.9
I0801 14:08:24.130125 24522 solver.cpp:353] Iteration 35800 (58.28 iter/s, 1.71586s/100 iter), loss = 0.00560029
I0801 14:08:24.130147 24522 solver.cpp:375]     Train net output #0: loss = 0.00559961 (* 1 = 0.00559961 loss)
I0801 14:08:24.130153 24522 sgd_solver.cpp:136] Iteration 35800, lr = 0.00440625, m = 0.9
I0801 14:08:25.793414 24522 solver.cpp:353] Iteration 35900 (60.1238 iter/s, 1.66324s/100 iter), loss = 0.000155516
I0801 14:08:25.793833 24522 solver.cpp:375]     Train net output #0: loss = 0.000154831 (* 1 = 0.000154831 loss)
I0801 14:08:25.793853 24522 sgd_solver.cpp:136] Iteration 35900, lr = 0.00439062, m = 0.9
I0801 14:08:27.436554 24522 solver.cpp:404] Sparsity after update:
I0801 14:08:27.438174 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:08:27.438185 24522 net.cpp:2270] conv1a_param_0(0.255) 
I0801 14:08:27.438194 24522 net.cpp:2270] conv1b_param_0(0.635) 
I0801 14:08:27.438199 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:08:27.438205 24522 net.cpp:2270] res2a_branch2a_param_0(0.639) 
I0801 14:08:27.438208 24522 net.cpp:2270] res2a_branch2b_param_0(0.623) 
I0801 14:08:27.438212 24522 net.cpp:2270] res3a_branch2a_param_0(0.639) 
I0801 14:08:27.438216 24522 net.cpp:2270] res3a_branch2b_param_0(0.639) 
I0801 14:08:27.438220 24522 net.cpp:2270] res4a_branch2a_param_0(0.64) 
I0801 14:08:27.438225 24522 net.cpp:2270] res4a_branch2b_param_0(0.639) 
I0801 14:08:27.438228 24522 net.cpp:2270] res5a_branch2a_param_0(0.604) 
I0801 14:08:27.438233 24522 net.cpp:2270] res5a_branch2b_param_0(0.639) 
I0801 14:08:27.438236 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.46319e+06/2.3599e+06) 0.62
I0801 14:08:27.438261 24522 solver.cpp:550] Iteration 36000, Testing net (#0)
I0801 14:08:27.461385 24521 blocking_queue.cpp:40] Waiting for datum
I0801 14:08:28.329282 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.902354
I0801 14:08:28.329311 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997353
I0801 14:08:28.329319 24522 solver.cpp:635]     Test net output #2: loss = 0.382221 (* 1 = 0.382221 loss)
I0801 14:08:28.329342 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.891048s
I0801 14:08:28.355633 24549 solver.cpp:450] Finding and applying sparsity: 0.66
I0801 14:09:01.426973 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:09:01.428946 24522 solver.cpp:353] Iteration 36000 (2.80627 iter/s, 35.6345s/100 iter), loss = 0.000940949
I0801 14:09:01.428966 24522 solver.cpp:375]     Train net output #0: loss = 0.000940263 (* 1 = 0.000940263 loss)
I0801 14:09:01.428973 24522 sgd_solver.cpp:136] Iteration 36000, lr = 0.004375, m = 0.9
I0801 14:09:03.330200 24522 solver.cpp:353] Iteration 36100 (52.5987 iter/s, 1.90119s/100 iter), loss = 0.000881535
I0801 14:09:03.330343 24522 solver.cpp:375]     Train net output #0: loss = 0.000880851 (* 1 = 0.000880851 loss)
I0801 14:09:03.330394 24522 sgd_solver.cpp:136] Iteration 36100, lr = 0.00435938, m = 0.9
I0801 14:09:05.014261 24522 solver.cpp:353] Iteration 36200 (59.3822 iter/s, 1.68401s/100 iter), loss = 0.000535197
I0801 14:09:05.014310 24522 solver.cpp:375]     Train net output #0: loss = 0.000534513 (* 1 = 0.000534513 loss)
I0801 14:09:05.014323 24522 sgd_solver.cpp:136] Iteration 36200, lr = 0.00434375, m = 0.9
I0801 14:09:06.740676 24522 solver.cpp:353] Iteration 36300 (57.9254 iter/s, 1.72636s/100 iter), loss = 0.000626915
I0801 14:09:06.740702 24522 solver.cpp:375]     Train net output #0: loss = 0.000626229 (* 1 = 0.000626229 loss)
I0801 14:09:06.740708 24522 sgd_solver.cpp:136] Iteration 36300, lr = 0.00432813, m = 0.9
I0801 14:09:08.377117 24522 solver.cpp:353] Iteration 36400 (61.1101 iter/s, 1.63639s/100 iter), loss = 0.00291853
I0801 14:09:08.377143 24522 solver.cpp:375]     Train net output #0: loss = 0.00291784 (* 1 = 0.00291784 loss)
I0801 14:09:08.377148 24522 sgd_solver.cpp:136] Iteration 36400, lr = 0.0043125, m = 0.9
I0801 14:09:09.952302 24522 solver.cpp:353] Iteration 36500 (63.4867 iter/s, 1.57513s/100 iter), loss = 0.00191148
I0801 14:09:09.952354 24522 solver.cpp:375]     Train net output #0: loss = 0.0019108 (* 1 = 0.0019108 loss)
I0801 14:09:09.952366 24522 sgd_solver.cpp:136] Iteration 36500, lr = 0.00429688, m = 0.9
I0801 14:09:11.556238 24522 solver.cpp:353] Iteration 36600 (62.3486 iter/s, 1.60389s/100 iter), loss = 0.00161355
I0801 14:09:11.556300 24522 solver.cpp:375]     Train net output #0: loss = 0.00161286 (* 1 = 0.00161286 loss)
I0801 14:09:11.556318 24522 sgd_solver.cpp:136] Iteration 36600, lr = 0.00428125, m = 0.9
I0801 14:09:13.129235 24522 solver.cpp:353] Iteration 36700 (63.575 iter/s, 1.57294s/100 iter), loss = 0.00100522
I0801 14:09:13.129261 24522 solver.cpp:375]     Train net output #0: loss = 0.00100454 (* 1 = 0.00100454 loss)
I0801 14:09:13.129266 24522 sgd_solver.cpp:136] Iteration 36700, lr = 0.00426562, m = 0.9
I0801 14:09:14.788202 24522 solver.cpp:353] Iteration 36800 (60.2803 iter/s, 1.65892s/100 iter), loss = 0.000662595
I0801 14:09:14.788226 24522 solver.cpp:375]     Train net output #0: loss = 0.000661914 (* 1 = 0.000661914 loss)
I0801 14:09:14.788231 24522 sgd_solver.cpp:136] Iteration 36800, lr = 0.00425, m = 0.9
I0801 14:09:16.426545 24522 solver.cpp:353] Iteration 36900 (61.0392 iter/s, 1.63829s/100 iter), loss = 0.00266728
I0801 14:09:16.426579 24522 solver.cpp:375]     Train net output #0: loss = 0.00266659 (* 1 = 0.00266659 loss)
I0801 14:09:16.426585 24522 sgd_solver.cpp:136] Iteration 36900, lr = 0.00423437, m = 0.9
I0801 14:09:18.112174 24522 solver.cpp:404] Sparsity after update:
I0801 14:09:18.113868 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:09:18.113876 24522 net.cpp:2270] conv1a_param_0(0.278) 
I0801 14:09:18.113883 24522 net.cpp:2270] conv1b_param_0(0.648) 
I0801 14:09:18.113886 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:09:18.113891 24522 net.cpp:2270] res2a_branch2a_param_0(0.66) 
I0801 14:09:18.113894 24522 net.cpp:2270] res2a_branch2b_param_0(0.636) 
I0801 14:09:18.113898 24522 net.cpp:2270] res3a_branch2a_param_0(0.66) 
I0801 14:09:18.113901 24522 net.cpp:2270] res3a_branch2b_param_0(0.659) 
I0801 14:09:18.113905 24522 net.cpp:2270] res4a_branch2a_param_0(0.66) 
I0801 14:09:18.113909 24522 net.cpp:2270] res4a_branch2b_param_0(0.66) 
I0801 14:09:18.113912 24522 net.cpp:2270] res5a_branch2a_param_0(0.623) 
I0801 14:09:18.113916 24522 net.cpp:2270] res5a_branch2b_param_0(0.659) 
I0801 14:09:18.113921 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.50816e+06/2.3599e+06) 0.639
I0801 14:09:18.113946 24522 solver.cpp:550] Iteration 37000, Testing net (#0)
I0801 14:09:18.988343 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.906472
I0801 14:09:18.988360 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 14:09:18.988365 24522 solver.cpp:635]     Test net output #2: loss = 0.37651 (* 1 = 0.37651 loss)
I0801 14:09:18.988382 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.874406s
I0801 14:09:19.004165 24549 solver.cpp:450] Finding and applying sparsity: 0.68
I0801 14:09:55.190399 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:09:55.192335 24522 solver.cpp:353] Iteration 37000 (2.57967 iter/s, 38.7647s/100 iter), loss = 0.0028661
I0801 14:09:55.192355 24522 solver.cpp:375]     Train net output #0: loss = 0.00286542 (* 1 = 0.00286542 loss)
I0801 14:09:55.192363 24522 sgd_solver.cpp:136] Iteration 37000, lr = 0.00421875, m = 0.9
I0801 14:09:57.214676 24522 solver.cpp:353] Iteration 37100 (49.4493 iter/s, 2.02227s/100 iter), loss = 0.00875254
I0801 14:09:57.214705 24522 solver.cpp:375]     Train net output #0: loss = 0.00875185 (* 1 = 0.00875185 loss)
I0801 14:09:57.214711 24522 sgd_solver.cpp:136] Iteration 37100, lr = 0.00420313, m = 0.9
I0801 14:09:58.991840 24522 solver.cpp:353] Iteration 37200 (56.2711 iter/s, 1.77711s/100 iter), loss = 0.00189968
I0801 14:09:58.991866 24522 solver.cpp:375]     Train net output #0: loss = 0.001899 (* 1 = 0.001899 loss)
I0801 14:09:58.991873 24522 sgd_solver.cpp:136] Iteration 37200, lr = 0.0041875, m = 0.9
I0801 14:10:00.771112 24522 solver.cpp:353] Iteration 37300 (56.2046 iter/s, 1.77921s/100 iter), loss = 0.000891273
I0801 14:10:00.771138 24522 solver.cpp:375]     Train net output #0: loss = 0.000890591 (* 1 = 0.000890591 loss)
I0801 14:10:00.771142 24522 sgd_solver.cpp:136] Iteration 37300, lr = 0.00417187, m = 0.9
I0801 14:10:02.515305 24522 solver.cpp:353] Iteration 37400 (57.3353 iter/s, 1.74413s/100 iter), loss = 0.00224734
I0801 14:10:02.515349 24522 solver.cpp:375]     Train net output #0: loss = 0.00224666 (* 1 = 0.00224666 loss)
I0801 14:10:02.515360 24522 sgd_solver.cpp:136] Iteration 37400, lr = 0.00415625, m = 0.9
I0801 14:10:04.140736 24522 solver.cpp:353] Iteration 37500 (61.5238 iter/s, 1.62539s/100 iter), loss = 0.000962321
I0801 14:10:04.140764 24522 solver.cpp:375]     Train net output #0: loss = 0.000961639 (* 1 = 0.000961639 loss)
I0801 14:10:04.140771 24522 sgd_solver.cpp:136] Iteration 37500, lr = 0.00414062, m = 0.9
I0801 14:10:05.977324 24522 solver.cpp:353] Iteration 37600 (54.4506 iter/s, 1.83653s/100 iter), loss = 0.00236573
I0801 14:10:05.977378 24522 solver.cpp:375]     Train net output #0: loss = 0.00236505 (* 1 = 0.00236505 loss)
I0801 14:10:05.977391 24522 sgd_solver.cpp:136] Iteration 37600, lr = 0.004125, m = 0.9
I0801 14:10:07.672257 24522 solver.cpp:353] Iteration 37700 (59.0013 iter/s, 1.69488s/100 iter), loss = 0.000909939
I0801 14:10:07.672286 24522 solver.cpp:375]     Train net output #0: loss = 0.00090926 (* 1 = 0.00090926 loss)
I0801 14:10:07.672291 24522 sgd_solver.cpp:136] Iteration 37700, lr = 0.00410937, m = 0.9
I0801 14:10:09.256923 24522 solver.cpp:353] Iteration 37800 (63.1068 iter/s, 1.58462s/100 iter), loss = 0.00248125
I0801 14:10:09.256947 24522 solver.cpp:375]     Train net output #0: loss = 0.00248057 (* 1 = 0.00248057 loss)
I0801 14:10:09.256953 24522 sgd_solver.cpp:136] Iteration 37800, lr = 0.00409375, m = 0.9
I0801 14:10:10.830955 24522 solver.cpp:353] Iteration 37900 (63.5332 iter/s, 1.57398s/100 iter), loss = 0.000365001
I0801 14:10:10.830981 24522 solver.cpp:375]     Train net output #0: loss = 0.000364321 (* 1 = 0.000364321 loss)
I0801 14:10:10.830987 24522 sgd_solver.cpp:136] Iteration 37900, lr = 0.00407812, m = 0.9
I0801 14:10:12.401451 24522 solver.cpp:404] Sparsity after update:
I0801 14:10:12.403046 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:10:12.403055 24522 net.cpp:2270] conv1a_param_0(0.283) 
I0801 14:10:12.403064 24522 net.cpp:2270] conv1b_param_0(0.661) 
I0801 14:10:12.403069 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:10:12.403072 24522 net.cpp:2270] res2a_branch2a_param_0(0.677) 
I0801 14:10:12.403075 24522 net.cpp:2270] res2a_branch2b_param_0(0.644) 
I0801 14:10:12.403079 24522 net.cpp:2270] res3a_branch2a_param_0(0.679) 
I0801 14:10:12.403084 24522 net.cpp:2270] res3a_branch2b_param_0(0.675) 
I0801 14:10:12.403087 24522 net.cpp:2270] res4a_branch2a_param_0(0.68) 
I0801 14:10:12.403091 24522 net.cpp:2270] res4a_branch2b_param_0(0.679) 
I0801 14:10:12.403095 24522 net.cpp:2270] res5a_branch2a_param_0(0.642) 
I0801 14:10:12.403100 24522 net.cpp:2270] res5a_branch2b_param_0(0.678) 
I0801 14:10:12.403105 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.55303e+06/2.3599e+06) 0.658
I0801 14:10:12.403123 24522 solver.cpp:550] Iteration 38000, Testing net (#0)
I0801 14:10:13.209719 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.903236
I0801 14:10:13.209738 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 14:10:13.209744 24522 solver.cpp:635]     Test net output #2: loss = 0.408611 (* 1 = 0.408611 loss)
I0801 14:10:13.209764 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.806612s
I0801 14:10:13.225162 24549 solver.cpp:450] Finding and applying sparsity: 0.7
I0801 14:10:49.397742 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:10:49.400092 24522 solver.cpp:353] Iteration 38000 (2.59282 iter/s, 38.568s/100 iter), loss = 0.00074572
I0801 14:10:49.400135 24522 solver.cpp:375]     Train net output #0: loss = 0.000745041 (* 1 = 0.000745041 loss)
I0801 14:10:49.400151 24522 sgd_solver.cpp:136] Iteration 38000, lr = 0.0040625, m = 0.9
I0801 14:10:51.456254 24522 solver.cpp:353] Iteration 38100 (48.6359 iter/s, 2.05609s/100 iter), loss = 0.00136866
I0801 14:10:51.456324 24522 solver.cpp:375]     Train net output #0: loss = 0.00136798 (* 1 = 0.00136798 loss)
I0801 14:10:51.456343 24522 sgd_solver.cpp:136] Iteration 38100, lr = 0.00404688, m = 0.9
I0801 14:10:53.212067 24522 solver.cpp:353] Iteration 38200 (56.9553 iter/s, 1.75576s/100 iter), loss = 0.00255316
I0801 14:10:53.212118 24522 solver.cpp:375]     Train net output #0: loss = 0.00255249 (* 1 = 0.00255249 loss)
I0801 14:10:53.212131 24522 sgd_solver.cpp:136] Iteration 38200, lr = 0.00403125, m = 0.9
I0801 14:10:54.967011 24522 solver.cpp:353] Iteration 38300 (56.9837 iter/s, 1.75489s/100 iter), loss = 0.00273153
I0801 14:10:54.967062 24522 solver.cpp:375]     Train net output #0: loss = 0.00273085 (* 1 = 0.00273085 loss)
I0801 14:10:54.967075 24522 sgd_solver.cpp:136] Iteration 38300, lr = 0.00401562, m = 0.9
I0801 14:10:56.653669 24522 solver.cpp:353] Iteration 38400 (59.2907 iter/s, 1.6866s/100 iter), loss = 0.000153298
I0801 14:10:56.653694 24522 solver.cpp:375]     Train net output #0: loss = 0.000152624 (* 1 = 0.000152624 loss)
I0801 14:10:56.653699 24522 sgd_solver.cpp:136] Iteration 38400, lr = 0.004, m = 0.9
I0801 14:10:58.332831 24522 solver.cpp:353] Iteration 38500 (59.5553 iter/s, 1.67911s/100 iter), loss = 0.0178774
I0801 14:10:58.332892 24522 solver.cpp:375]     Train net output #0: loss = 0.0178768 (* 1 = 0.0178768 loss)
I0801 14:10:58.332909 24522 sgd_solver.cpp:136] Iteration 38500, lr = 0.00398437, m = 0.9
I0801 14:10:59.938215 24522 solver.cpp:353] Iteration 38600 (62.2924 iter/s, 1.60533s/100 iter), loss = 0.003889
I0801 14:10:59.938268 24522 solver.cpp:375]     Train net output #0: loss = 0.00388832 (* 1 = 0.00388832 loss)
I0801 14:10:59.938282 24522 sgd_solver.cpp:136] Iteration 38600, lr = 0.00396875, m = 0.9
I0801 14:11:01.585222 24522 solver.cpp:353] Iteration 38700 (60.7182 iter/s, 1.64695s/100 iter), loss = 0.00149916
I0801 14:11:01.585250 24522 solver.cpp:375]     Train net output #0: loss = 0.00149849 (* 1 = 0.00149849 loss)
I0801 14:11:01.585256 24522 sgd_solver.cpp:136] Iteration 38700, lr = 0.00395312, m = 0.9
I0801 14:11:03.204519 24522 solver.cpp:353] Iteration 38800 (61.7575 iter/s, 1.61924s/100 iter), loss = 0.000409661
I0801 14:11:03.204568 24522 solver.cpp:375]     Train net output #0: loss = 0.000408985 (* 1 = 0.000408985 loss)
I0801 14:11:03.204579 24522 sgd_solver.cpp:136] Iteration 38800, lr = 0.0039375, m = 0.9
I0801 14:11:04.907641 24522 solver.cpp:353] Iteration 38900 (58.7174 iter/s, 1.70307s/100 iter), loss = 0.00306468
I0801 14:11:04.907666 24522 solver.cpp:375]     Train net output #0: loss = 0.00306401 (* 1 = 0.00306401 loss)
I0801 14:11:04.907671 24522 sgd_solver.cpp:136] Iteration 38900, lr = 0.00392187, m = 0.9
I0801 14:11:06.602929 24522 solver.cpp:404] Sparsity after update:
I0801 14:11:06.605270 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:11:06.605286 24522 net.cpp:2270] conv1a_param_0(0.306) 
I0801 14:11:06.605301 24522 net.cpp:2270] conv1b_param_0(0.684) 
I0801 14:11:06.605312 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:11:06.605322 24522 net.cpp:2270] res2a_branch2a_param_0(0.698) 
I0801 14:11:06.605329 24522 net.cpp:2270] res2a_branch2b_param_0(0.654) 
I0801 14:11:06.605339 24522 net.cpp:2270] res3a_branch2a_param_0(0.7) 
I0801 14:11:06.605348 24522 net.cpp:2270] res3a_branch2b_param_0(0.692) 
I0801 14:11:06.605358 24522 net.cpp:2270] res4a_branch2a_param_0(0.7) 
I0801 14:11:06.605367 24522 net.cpp:2270] res4a_branch2b_param_0(0.7) 
I0801 14:11:06.605376 24522 net.cpp:2270] res5a_branch2a_param_0(0.658) 
I0801 14:11:06.605384 24522 net.cpp:2270] res5a_branch2b_param_0(0.699) 
I0801 14:11:06.605393 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.59683e+06/2.3599e+06) 0.677
I0801 14:11:06.605448 24522 solver.cpp:550] Iteration 39000, Testing net (#0)
I0801 14:11:07.426787 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.902648
I0801 14:11:07.426822 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995588
I0801 14:11:07.426836 24522 solver.cpp:635]     Test net output #2: loss = 0.411482 (* 1 = 0.411482 loss)
I0801 14:11:07.426862 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.821387s
I0801 14:11:07.443611 24549 solver.cpp:450] Finding and applying sparsity: 0.72
I0801 14:11:45.012931 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:11:45.014853 24522 solver.cpp:353] Iteration 39000 (2.49339 iter/s, 40.1061s/100 iter), loss = 0.00184714
I0801 14:11:45.014870 24522 solver.cpp:375]     Train net output #0: loss = 0.00184647 (* 1 = 0.00184647 loss)
I0801 14:11:45.014876 24522 sgd_solver.cpp:136] Iteration 39000, lr = 0.00390625, m = 0.9
I0801 14:11:46.896061 24522 solver.cpp:353] Iteration 39100 (53.159 iter/s, 1.88115s/100 iter), loss = 0.000735196
I0801 14:11:46.896086 24522 solver.cpp:375]     Train net output #0: loss = 0.00073452 (* 1 = 0.00073452 loss)
I0801 14:11:46.896091 24522 sgd_solver.cpp:136] Iteration 39100, lr = 0.00389063, m = 0.9
I0801 14:11:48.571569 24522 solver.cpp:353] Iteration 39200 (59.6855 iter/s, 1.67545s/100 iter), loss = 0.00034909
I0801 14:11:48.571642 24522 solver.cpp:375]     Train net output #0: loss = 0.000348415 (* 1 = 0.000348415 loss)
I0801 14:11:48.571661 24522 sgd_solver.cpp:136] Iteration 39200, lr = 0.003875, m = 0.9
I0801 14:11:50.240010 24522 solver.cpp:353] Iteration 39300 (59.9381 iter/s, 1.66839s/100 iter), loss = 0.0030348
I0801 14:11:50.240032 24522 solver.cpp:375]     Train net output #0: loss = 0.00303413 (* 1 = 0.00303413 loss)
I0801 14:11:50.240037 24522 sgd_solver.cpp:136] Iteration 39300, lr = 0.00385938, m = 0.9
I0801 14:11:51.764344 24487 data_reader.cpp:264] Starting prefetch of epoch 5
I0801 14:11:51.928975 24522 solver.cpp:353] Iteration 39400 (59.2099 iter/s, 1.68891s/100 iter), loss = 0.0163882
I0801 14:11:51.929021 24522 solver.cpp:375]     Train net output #0: loss = 0.0163875 (* 1 = 0.0163875 loss)
I0801 14:11:51.929031 24522 sgd_solver.cpp:136] Iteration 39400, lr = 0.00384375, m = 0.9
I0801 14:11:53.525588 24522 solver.cpp:353] Iteration 39500 (62.6347 iter/s, 1.59656s/100 iter), loss = 0.00590614
I0801 14:11:53.525627 24522 solver.cpp:375]     Train net output #0: loss = 0.00590547 (* 1 = 0.00590547 loss)
I0801 14:11:53.525638 24522 sgd_solver.cpp:136] Iteration 39500, lr = 0.00382812, m = 0.9
I0801 14:11:55.149586 24522 solver.cpp:353] Iteration 39600 (61.5784 iter/s, 1.62395s/100 iter), loss = 0.00156271
I0801 14:11:55.149611 24522 solver.cpp:375]     Train net output #0: loss = 0.00156204 (* 1 = 0.00156204 loss)
I0801 14:11:55.149617 24522 sgd_solver.cpp:136] Iteration 39600, lr = 0.0038125, m = 0.9
I0801 14:11:56.832955 24522 solver.cpp:353] Iteration 39700 (59.4065 iter/s, 1.68332s/100 iter), loss = 0.00151379
I0801 14:11:56.832981 24522 solver.cpp:375]     Train net output #0: loss = 0.00151312 (* 1 = 0.00151312 loss)
I0801 14:11:56.832988 24522 sgd_solver.cpp:136] Iteration 39700, lr = 0.00379687, m = 0.9
I0801 14:11:58.533768 24522 solver.cpp:353] Iteration 39800 (58.7984 iter/s, 1.70073s/100 iter), loss = 0.00136631
I0801 14:11:58.533939 24522 solver.cpp:375]     Train net output #0: loss = 0.00136564 (* 1 = 0.00136564 loss)
I0801 14:11:58.533975 24522 sgd_solver.cpp:136] Iteration 39800, lr = 0.00378125, m = 0.9
I0801 14:12:00.318666 24522 solver.cpp:353] Iteration 39900 (56.0268 iter/s, 1.78486s/100 iter), loss = 0.00534749
I0801 14:12:00.318691 24522 solver.cpp:375]     Train net output #0: loss = 0.00534681 (* 1 = 0.00534681 loss)
I0801 14:12:00.318696 24522 sgd_solver.cpp:136] Iteration 39900, lr = 0.00376562, m = 0.9
I0801 14:12:02.071998 24522 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_40000.caffemodel
I0801 14:12:02.080021 24522 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_40000.solverstate
I0801 14:12:02.083555 24522 solver.cpp:404] Sparsity after update:
I0801 14:12:02.085233 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:12:02.085245 24522 net.cpp:2270] conv1a_param_0(0.294) 
I0801 14:12:02.085259 24522 net.cpp:2270] conv1b_param_0(0.696) 
I0801 14:12:02.085263 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:12:02.085268 24522 net.cpp:2270] res2a_branch2a_param_0(0.719) 
I0801 14:12:02.085273 24522 net.cpp:2270] res2a_branch2b_param_0(0.663) 
I0801 14:12:02.085295 24522 net.cpp:2270] res3a_branch2a_param_0(0.719) 
I0801 14:12:02.085301 24522 net.cpp:2270] res3a_branch2b_param_0(0.708) 
I0801 14:12:02.085305 24522 net.cpp:2270] res4a_branch2a_param_0(0.72) 
I0801 14:12:02.085310 24522 net.cpp:2270] res4a_branch2b_param_0(0.719) 
I0801 14:12:02.085314 24522 net.cpp:2270] res5a_branch2a_param_0(0.681) 
I0801 14:12:02.085319 24522 net.cpp:2270] res5a_branch2b_param_0(0.719) 
I0801 14:12:02.085324 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.64632e+06/2.3599e+06) 0.698
I0801 14:12:02.085336 24522 solver.cpp:550] Iteration 40000, Testing net (#0)
I0801 14:12:02.883498 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.906766
I0801 14:12:02.883517 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 14:12:02.883523 24522 solver.cpp:635]     Test net output #2: loss = 0.386997 (* 1 = 0.386997 loss)
I0801 14:12:02.883541 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.798176s
I0801 14:12:02.899124 24549 solver.cpp:450] Finding and applying sparsity: 0.74
I0801 14:12:37.964088 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:12:37.966007 24522 solver.cpp:353] Iteration 40000 (2.65631 iter/s, 37.6463s/100 iter), loss = 0.000424887
I0801 14:12:37.966027 24522 solver.cpp:375]     Train net output #0: loss = 0.000424212 (* 1 = 0.000424212 loss)
I0801 14:12:37.966033 24522 sgd_solver.cpp:136] Iteration 40000, lr = 0.00375, m = 0.9
I0801 14:12:39.869068 24522 solver.cpp:353] Iteration 40100 (52.5487 iter/s, 1.903s/100 iter), loss = 0.00453139
I0801 14:12:39.869097 24522 solver.cpp:375]     Train net output #0: loss = 0.00453071 (* 1 = 0.00453071 loss)
I0801 14:12:39.869103 24522 sgd_solver.cpp:136] Iteration 40100, lr = 0.00373438, m = 0.9
I0801 14:12:41.444445 24522 solver.cpp:353] Iteration 40200 (63.4789 iter/s, 1.57533s/100 iter), loss = 0.000411164
I0801 14:12:41.444500 24522 solver.cpp:375]     Train net output #0: loss = 0.000410478 (* 1 = 0.000410478 loss)
I0801 14:12:41.444524 24522 sgd_solver.cpp:136] Iteration 40200, lr = 0.00371875, m = 0.9
I0801 14:12:43.041960 24522 solver.cpp:353] Iteration 40300 (62.5992 iter/s, 1.59746s/100 iter), loss = 0.00524889
I0801 14:12:43.041985 24522 solver.cpp:375]     Train net output #0: loss = 0.00524821 (* 1 = 0.00524821 loss)
I0801 14:12:43.041991 24522 sgd_solver.cpp:136] Iteration 40300, lr = 0.00370313, m = 0.9
I0801 14:12:44.661520 24522 solver.cpp:353] Iteration 40400 (61.747 iter/s, 1.61951s/100 iter), loss = 0.00711263
I0801 14:12:44.661543 24522 solver.cpp:375]     Train net output #0: loss = 0.00711195 (* 1 = 0.00711195 loss)
I0801 14:12:44.661546 24522 sgd_solver.cpp:136] Iteration 40400, lr = 0.0036875, m = 0.9
I0801 14:12:46.243746 24522 solver.cpp:353] Iteration 40500 (63.2043 iter/s, 1.58217s/100 iter), loss = 0.00528817
I0801 14:12:46.243769 24522 solver.cpp:375]     Train net output #0: loss = 0.00528749 (* 1 = 0.00528749 loss)
I0801 14:12:46.243773 24522 sgd_solver.cpp:136] Iteration 40500, lr = 0.00367187, m = 0.9
I0801 14:12:47.813280 24522 solver.cpp:353] Iteration 40600 (63.7152 iter/s, 1.56948s/100 iter), loss = 0.00175298
I0801 14:12:47.813305 24522 solver.cpp:375]     Train net output #0: loss = 0.00175231 (* 1 = 0.00175231 loss)
I0801 14:12:47.813311 24522 sgd_solver.cpp:136] Iteration 40600, lr = 0.00365625, m = 0.9
I0801 14:12:49.391880 24522 solver.cpp:353] Iteration 40700 (63.3493 iter/s, 1.57855s/100 iter), loss = 0.00247242
I0801 14:12:49.391908 24522 solver.cpp:375]     Train net output #0: loss = 0.00247174 (* 1 = 0.00247174 loss)
I0801 14:12:49.391914 24522 sgd_solver.cpp:136] Iteration 40700, lr = 0.00364062, m = 0.9
I0801 14:12:50.970894 24522 solver.cpp:353] Iteration 40800 (63.3327 iter/s, 1.57896s/100 iter), loss = 0.00171148
I0801 14:12:50.970943 24522 solver.cpp:375]     Train net output #0: loss = 0.00171081 (* 1 = 0.00171081 loss)
I0801 14:12:50.970957 24522 sgd_solver.cpp:136] Iteration 40800, lr = 0.003625, m = 0.9
I0801 14:12:52.558768 24522 solver.cpp:353] Iteration 40900 (62.9795 iter/s, 1.58782s/100 iter), loss = 0.00655072
I0801 14:12:52.558790 24522 solver.cpp:375]     Train net output #0: loss = 0.00655005 (* 1 = 0.00655005 loss)
I0801 14:12:52.558794 24522 sgd_solver.cpp:136] Iteration 40900, lr = 0.00360937, m = 0.9
I0801 14:12:54.118893 24522 solver.cpp:404] Sparsity after update:
I0801 14:12:54.120523 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:12:54.120532 24522 net.cpp:2270] conv1a_param_0(0.317) 
I0801 14:12:54.120537 24522 net.cpp:2270] conv1b_param_0(0.716) 
I0801 14:12:54.120539 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:12:54.120542 24522 net.cpp:2270] res2a_branch2a_param_0(0.739) 
I0801 14:12:54.120543 24522 net.cpp:2270] res2a_branch2b_param_0(0.671) 
I0801 14:12:54.120545 24522 net.cpp:2270] res3a_branch2a_param_0(0.739) 
I0801 14:12:54.120548 24522 net.cpp:2270] res3a_branch2b_param_0(0.722) 
I0801 14:12:54.120549 24522 net.cpp:2270] res4a_branch2a_param_0(0.74) 
I0801 14:12:54.120551 24522 net.cpp:2270] res4a_branch2b_param_0(0.739) 
I0801 14:12:54.120553 24522 net.cpp:2270] res5a_branch2a_param_0(0.696) 
I0801 14:12:54.120555 24522 net.cpp:2270] res5a_branch2b_param_0(0.739) 
I0801 14:12:54.120558 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.68711e+06/2.3599e+06) 0.715
I0801 14:12:54.120579 24522 solver.cpp:550] Iteration 41000, Testing net (#0)
I0801 14:12:54.935829 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.907648
I0801 14:12:54.935849 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 14:12:54.935854 24522 solver.cpp:635]     Test net output #2: loss = 0.398976 (* 1 = 0.398976 loss)
I0801 14:12:54.935868 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.815261s
I0801 14:12:54.951495 24549 solver.cpp:450] Finding and applying sparsity: 0.76
I0801 14:13:31.081903 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:13:31.083894 24522 solver.cpp:353] Iteration 41000 (2.59578 iter/s, 38.524s/100 iter), loss = 0.00940124
I0801 14:13:31.083912 24522 solver.cpp:375]     Train net output #0: loss = 0.00940057 (* 1 = 0.00940057 loss)
I0801 14:13:31.083920 24522 sgd_solver.cpp:136] Iteration 41000, lr = 0.00359375, m = 0.9
I0801 14:13:32.926358 24522 solver.cpp:353] Iteration 41100 (54.2769 iter/s, 1.8424s/100 iter), loss = 0.00087284
I0801 14:13:32.926384 24522 solver.cpp:375]     Train net output #0: loss = 0.000872173 (* 1 = 0.000872173 loss)
I0801 14:13:32.926388 24522 sgd_solver.cpp:136] Iteration 41100, lr = 0.00357813, m = 0.9
I0801 14:13:34.503262 24522 solver.cpp:353] Iteration 41200 (63.4174 iter/s, 1.57685s/100 iter), loss = 0.00157554
I0801 14:13:34.503314 24522 solver.cpp:375]     Train net output #0: loss = 0.00157488 (* 1 = 0.00157488 loss)
I0801 14:13:34.503326 24522 sgd_solver.cpp:136] Iteration 41200, lr = 0.0035625, m = 0.9
I0801 14:13:36.103199 24522 solver.cpp:353] Iteration 41300 (62.5045 iter/s, 1.59988s/100 iter), loss = 0.00709238
I0801 14:13:36.103288 24522 solver.cpp:375]     Train net output #0: loss = 0.00709171 (* 1 = 0.00709171 loss)
I0801 14:13:36.103296 24522 sgd_solver.cpp:136] Iteration 41300, lr = 0.00354687, m = 0.9
I0801 14:13:37.713596 24522 solver.cpp:353] Iteration 41400 (62.0985 iter/s, 1.61035s/100 iter), loss = 0.00969981
I0801 14:13:37.713623 24522 solver.cpp:375]     Train net output #0: loss = 0.00969914 (* 1 = 0.00969914 loss)
I0801 14:13:37.713629 24522 sgd_solver.cpp:136] Iteration 41400, lr = 0.00353125, m = 0.9
I0801 14:13:39.285046 24522 solver.cpp:353] Iteration 41500 (63.6376 iter/s, 1.5714s/100 iter), loss = 0.00412774
I0801 14:13:39.285071 24522 solver.cpp:375]     Train net output #0: loss = 0.00412706 (* 1 = 0.00412706 loss)
I0801 14:13:39.285075 24522 sgd_solver.cpp:136] Iteration 41500, lr = 0.00351562, m = 0.9
I0801 14:13:40.877753 24522 solver.cpp:353] Iteration 41600 (62.7883 iter/s, 1.59265s/100 iter), loss = 0.0164018
I0801 14:13:40.877776 24522 solver.cpp:375]     Train net output #0: loss = 0.0164011 (* 1 = 0.0164011 loss)
I0801 14:13:40.877782 24522 sgd_solver.cpp:136] Iteration 41600, lr = 0.0035, m = 0.9
I0801 14:13:42.461941 24522 solver.cpp:353] Iteration 41700 (63.1259 iter/s, 1.58414s/100 iter), loss = 0.0111854
I0801 14:13:42.461992 24522 solver.cpp:375]     Train net output #0: loss = 0.0111847 (* 1 = 0.0111847 loss)
I0801 14:13:42.462005 24522 sgd_solver.cpp:136] Iteration 41700, lr = 0.00348437, m = 0.9
I0801 14:13:44.061113 24522 solver.cpp:353] Iteration 41800 (62.5342 iter/s, 1.59912s/100 iter), loss = 0.00165816
I0801 14:13:44.061187 24522 solver.cpp:375]     Train net output #0: loss = 0.00165749 (* 1 = 0.00165749 loss)
I0801 14:13:44.061214 24522 sgd_solver.cpp:136] Iteration 41800, lr = 0.00346875, m = 0.9
I0801 14:13:45.639443 24522 solver.cpp:353] Iteration 41900 (63.3602 iter/s, 1.57828s/100 iter), loss = 0.0020735
I0801 14:13:45.639469 24522 solver.cpp:375]     Train net output #0: loss = 0.00207283 (* 1 = 0.00207283 loss)
I0801 14:13:45.639474 24522 sgd_solver.cpp:136] Iteration 41900, lr = 0.00345312, m = 0.9
I0801 14:13:47.200484 24522 solver.cpp:404] Sparsity after update:
I0801 14:13:47.202095 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:13:47.202105 24522 net.cpp:2270] conv1a_param_0(0.317) 
I0801 14:13:47.202111 24522 net.cpp:2270] conv1b_param_0(0.727) 
I0801 14:13:47.202112 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:13:47.202116 24522 net.cpp:2270] res2a_branch2a_param_0(0.756) 
I0801 14:13:47.202119 24522 net.cpp:2270] res2a_branch2b_param_0(0.678) 
I0801 14:13:47.202124 24522 net.cpp:2270] res3a_branch2a_param_0(0.758) 
I0801 14:13:47.202128 24522 net.cpp:2270] res3a_branch2b_param_0(0.732) 
I0801 14:13:47.202131 24522 net.cpp:2270] res4a_branch2a_param_0(0.76) 
I0801 14:13:47.202133 24522 net.cpp:2270] res4a_branch2b_param_0(0.758) 
I0801 14:13:47.202137 24522 net.cpp:2270] res5a_branch2a_param_0(0.721) 
I0801 14:13:47.202141 24522 net.cpp:2270] res5a_branch2b_param_0(0.759) 
I0801 14:13:47.202142 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.73967e+06/2.3599e+06) 0.737
I0801 14:13:47.202172 24522 solver.cpp:550] Iteration 42000, Testing net (#0)
I0801 14:13:48.009711 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.911177
I0801 14:13:48.009728 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 14:13:48.009733 24522 solver.cpp:635]     Test net output #2: loss = 0.363961 (* 1 = 0.363961 loss)
I0801 14:13:48.009747 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.807547s
I0801 14:13:48.025243 24549 solver.cpp:450] Finding and applying sparsity: 0.78
I0801 14:14:26.526052 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:14:26.527971 24522 solver.cpp:353] Iteration 42000 (2.44574 iter/s, 40.8874s/100 iter), loss = 0.010395
I0801 14:14:26.527994 24522 solver.cpp:375]     Train net output #0: loss = 0.0103944 (* 1 = 0.0103944 loss)
I0801 14:14:26.528004 24522 sgd_solver.cpp:136] Iteration 42000, lr = 0.0034375, m = 0.9
I0801 14:14:28.376772 24522 solver.cpp:353] Iteration 42100 (54.0909 iter/s, 1.84874s/100 iter), loss = 0.00208231
I0801 14:14:28.376798 24522 solver.cpp:375]     Train net output #0: loss = 0.00208164 (* 1 = 0.00208164 loss)
I0801 14:14:28.376802 24522 sgd_solver.cpp:136] Iteration 42100, lr = 0.00342188, m = 0.9
I0801 14:14:29.963157 24522 solver.cpp:353] Iteration 42200 (63.0384 iter/s, 1.58633s/100 iter), loss = 0.00147983
I0801 14:14:29.963182 24522 solver.cpp:375]     Train net output #0: loss = 0.00147916 (* 1 = 0.00147916 loss)
I0801 14:14:29.963188 24522 sgd_solver.cpp:136] Iteration 42200, lr = 0.00340625, m = 0.9
I0801 14:14:31.533463 24522 solver.cpp:353] Iteration 42300 (63.6839 iter/s, 1.57026s/100 iter), loss = 0.191607
I0801 14:14:31.533488 24522 solver.cpp:375]     Train net output #0: loss = 0.191607 (* 1 = 0.191607 loss)
I0801 14:14:31.533493 24522 sgd_solver.cpp:136] Iteration 42300, lr = 0.00339063, m = 0.9
I0801 14:14:33.124956 24522 solver.cpp:353] Iteration 42400 (62.836 iter/s, 1.59145s/100 iter), loss = 0.00274655
I0801 14:14:33.124984 24522 solver.cpp:375]     Train net output #0: loss = 0.00274586 (* 1 = 0.00274586 loss)
I0801 14:14:33.124989 24522 sgd_solver.cpp:136] Iteration 42400, lr = 0.003375, m = 0.9
I0801 14:14:34.715101 24522 solver.cpp:353] Iteration 42500 (62.8895 iter/s, 1.59009s/100 iter), loss = 0.00266444
I0801 14:14:34.715124 24522 solver.cpp:375]     Train net output #0: loss = 0.00266375 (* 1 = 0.00266375 loss)
I0801 14:14:34.715131 24522 sgd_solver.cpp:136] Iteration 42500, lr = 0.00335937, m = 0.9
I0801 14:14:36.298630 24522 solver.cpp:353] Iteration 42600 (63.152 iter/s, 1.58348s/100 iter), loss = 0.00141241
I0801 14:14:36.298655 24522 solver.cpp:375]     Train net output #0: loss = 0.00141173 (* 1 = 0.00141173 loss)
I0801 14:14:36.298658 24522 sgd_solver.cpp:136] Iteration 42600, lr = 0.00334375, m = 0.9
I0801 14:14:37.886467 24522 solver.cpp:353] Iteration 42700 (62.9808 iter/s, 1.58778s/100 iter), loss = 0.00380942
I0801 14:14:37.886492 24522 solver.cpp:375]     Train net output #0: loss = 0.00380874 (* 1 = 0.00380874 loss)
I0801 14:14:37.886497 24522 sgd_solver.cpp:136] Iteration 42700, lr = 0.00332812, m = 0.9
I0801 14:14:39.458547 24522 solver.cpp:353] Iteration 42800 (63.612 iter/s, 1.57203s/100 iter), loss = 0.0081753
I0801 14:14:39.458572 24522 solver.cpp:375]     Train net output #0: loss = 0.00817463 (* 1 = 0.00817463 loss)
I0801 14:14:39.458578 24522 sgd_solver.cpp:136] Iteration 42800, lr = 0.0033125, m = 0.9
I0801 14:14:41.035668 24522 solver.cpp:353] Iteration 42900 (63.4088 iter/s, 1.57707s/100 iter), loss = 0.0278557
I0801 14:14:41.035693 24522 solver.cpp:375]     Train net output #0: loss = 0.027855 (* 1 = 0.027855 loss)
I0801 14:14:41.035699 24522 sgd_solver.cpp:136] Iteration 42900, lr = 0.00329687, m = 0.9
I0801 14:14:42.603211 24522 solver.cpp:404] Sparsity after update:
I0801 14:14:42.604795 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:14:42.604804 24522 net.cpp:2270] conv1a_param_0(0.328) 
I0801 14:14:42.604830 24522 net.cpp:2270] conv1b_param_0(0.742) 
I0801 14:14:42.604835 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:14:42.604838 24522 net.cpp:2270] res2a_branch2a_param_0(0.776) 
I0801 14:14:42.604842 24522 net.cpp:2270] res2a_branch2b_param_0(0.683) 
I0801 14:14:42.604846 24522 net.cpp:2270] res3a_branch2a_param_0(0.778) 
I0801 14:14:42.604849 24522 net.cpp:2270] res3a_branch2b_param_0(0.741) 
I0801 14:14:42.604852 24522 net.cpp:2270] res4a_branch2a_param_0(0.779) 
I0801 14:14:42.604856 24522 net.cpp:2270] res4a_branch2b_param_0(0.778) 
I0801 14:14:42.604859 24522 net.cpp:2270] res5a_branch2a_param_0(0.742) 
I0801 14:14:42.604863 24522 net.cpp:2270] res5a_branch2b_param_0(0.779) 
I0801 14:14:42.604866 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.788e+06/2.3599e+06) 0.758
I0801 14:14:42.604887 24522 solver.cpp:550] Iteration 43000, Testing net (#0)
I0801 14:14:43.409391 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.907648
I0801 14:14:43.409409 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 14:14:43.409415 24522 solver.cpp:635]     Test net output #2: loss = 0.400647 (* 1 = 0.400647 loss)
I0801 14:14:43.409430 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.804516s
I0801 14:14:43.424891 24549 solver.cpp:450] Finding and applying sparsity: 0.8
I0801 14:15:28.291119 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:15:28.293056 24522 solver.cpp:353] Iteration 43000 (2.11613 iter/s, 47.2561s/100 iter), loss = 0.00154071
I0801 14:15:28.293076 24522 solver.cpp:375]     Train net output #0: loss = 0.00154003 (* 1 = 0.00154003 loss)
I0801 14:15:28.293082 24522 sgd_solver.cpp:136] Iteration 43000, lr = 0.00328125, m = 0.9
I0801 14:15:30.262547 24522 solver.cpp:353] Iteration 43100 (50.7762 iter/s, 1.96943s/100 iter), loss = 0.000349341
I0801 14:15:30.262573 24522 solver.cpp:375]     Train net output #0: loss = 0.000348662 (* 1 = 0.000348662 loss)
I0801 14:15:30.262579 24522 sgd_solver.cpp:136] Iteration 43100, lr = 0.00326563, m = 0.9
I0801 14:15:31.959344 24522 solver.cpp:353] Iteration 43200 (58.9365 iter/s, 1.69674s/100 iter), loss = 0.00637403
I0801 14:15:31.959378 24522 solver.cpp:375]     Train net output #0: loss = 0.00637335 (* 1 = 0.00637335 loss)
I0801 14:15:31.959385 24522 sgd_solver.cpp:136] Iteration 43200, lr = 0.00325, m = 0.9
I0801 14:15:33.587316 24522 solver.cpp:353] Iteration 43300 (61.4285 iter/s, 1.62791s/100 iter), loss = 0.00871517
I0801 14:15:33.587362 24522 solver.cpp:375]     Train net output #0: loss = 0.00871448 (* 1 = 0.00871448 loss)
I0801 14:15:33.587373 24522 sgd_solver.cpp:136] Iteration 43300, lr = 0.00323438, m = 0.9
I0801 14:15:35.245312 24522 solver.cpp:353] Iteration 43400 (60.3155 iter/s, 1.65795s/100 iter), loss = 0.00761329
I0801 14:15:35.245337 24522 solver.cpp:375]     Train net output #0: loss = 0.00761259 (* 1 = 0.00761259 loss)
I0801 14:15:35.245343 24522 sgd_solver.cpp:136] Iteration 43400, lr = 0.00321875, m = 0.9
I0801 14:15:36.926390 24522 solver.cpp:353] Iteration 43500 (59.4876 iter/s, 1.68102s/100 iter), loss = 0.00296994
I0801 14:15:36.926417 24522 solver.cpp:375]     Train net output #0: loss = 0.00296925 (* 1 = 0.00296925 loss)
I0801 14:15:36.926422 24522 sgd_solver.cpp:136] Iteration 43500, lr = 0.00320312, m = 0.9
I0801 14:15:38.513463 24522 solver.cpp:353] Iteration 43600 (63.0111 iter/s, 1.58702s/100 iter), loss = 0.0096679
I0801 14:15:38.513494 24522 solver.cpp:375]     Train net output #0: loss = 0.00966721 (* 1 = 0.00966721 loss)
I0801 14:15:38.513500 24522 sgd_solver.cpp:136] Iteration 43600, lr = 0.0031875, m = 0.9
I0801 14:15:40.127918 24522 solver.cpp:353] Iteration 43700 (61.9423 iter/s, 1.6144s/100 iter), loss = 0.0034968
I0801 14:15:40.127944 24522 solver.cpp:375]     Train net output #0: loss = 0.00349611 (* 1 = 0.00349611 loss)
I0801 14:15:40.127950 24522 sgd_solver.cpp:136] Iteration 43700, lr = 0.00317187, m = 0.9
I0801 14:15:41.826576 24522 solver.cpp:353] Iteration 43800 (58.8734 iter/s, 1.69856s/100 iter), loss = 0.000262671
I0801 14:15:41.827525 24522 solver.cpp:375]     Train net output #0: loss = 0.000261987 (* 1 = 0.000261987 loss)
I0801 14:15:41.827685 24522 sgd_solver.cpp:136] Iteration 43800, lr = 0.00315625, m = 0.9
I0801 14:15:43.438757 24522 solver.cpp:353] Iteration 43900 (62.0291 iter/s, 1.61215s/100 iter), loss = 0.00158668
I0801 14:15:43.438782 24522 solver.cpp:375]     Train net output #0: loss = 0.00158598 (* 1 = 0.00158598 loss)
I0801 14:15:43.438788 24522 sgd_solver.cpp:136] Iteration 43900, lr = 0.00314062, m = 0.9
I0801 14:15:44.056126 24487 data_reader.cpp:264] Starting prefetch of epoch 6
I0801 14:15:45.089807 24522 solver.cpp:404] Sparsity after update:
I0801 14:15:45.091423 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:15:45.091431 24522 net.cpp:2270] conv1a_param_0(0.329) 
I0801 14:15:45.091440 24522 net.cpp:2270] conv1b_param_0(0.748) 
I0801 14:15:45.091445 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:15:45.091449 24522 net.cpp:2270] res2a_branch2a_param_0(0.794) 
I0801 14:15:45.091455 24522 net.cpp:2270] res2a_branch2b_param_0(0.689) 
I0801 14:15:45.091461 24522 net.cpp:2270] res3a_branch2a_param_0(0.795) 
I0801 14:15:45.091467 24522 net.cpp:2270] res3a_branch2b_param_0(0.749) 
I0801 14:15:45.091472 24522 net.cpp:2270] res4a_branch2a_param_0(0.799) 
I0801 14:15:45.091477 24522 net.cpp:2270] res4a_branch2b_param_0(0.796) 
I0801 14:15:45.091482 24522 net.cpp:2270] res5a_branch2a_param_0(0.759) 
I0801 14:15:45.091485 24522 net.cpp:2270] res5a_branch2b_param_0(0.799) 
I0801 14:15:45.091503 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.83015e+06/2.3599e+06) 0.776
I0801 14:15:45.091516 24522 solver.cpp:550] Iteration 44000, Testing net (#0)
I0801 14:15:45.957854 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.906766
I0801 14:15:45.957890 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 14:15:45.957909 24522 solver.cpp:635]     Test net output #2: loss = 0.379241 (* 1 = 0.379241 loss)
I0801 14:15:45.957939 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.86639s
I0801 14:15:45.981411 24549 solver.cpp:450] Finding and applying sparsity: 0.82
I0801 14:16:35.432785 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:16:35.434698 24522 solver.cpp:353] Iteration 44000 (1.92328 iter/s, 51.9945s/100 iter), loss = 0.00441611
I0801 14:16:35.434716 24522 solver.cpp:375]     Train net output #0: loss = 0.00441542 (* 1 = 0.00441542 loss)
I0801 14:16:35.434722 24522 sgd_solver.cpp:136] Iteration 44000, lr = 0.003125, m = 0.9
I0801 14:16:37.317042 24522 solver.cpp:353] Iteration 44100 (53.127 iter/s, 1.88228s/100 iter), loss = 0.00303087
I0801 14:16:37.317068 24522 solver.cpp:375]     Train net output #0: loss = 0.00303017 (* 1 = 0.00303017 loss)
I0801 14:16:37.317075 24522 sgd_solver.cpp:136] Iteration 44100, lr = 0.00310938, m = 0.9
I0801 14:16:39.009191 24522 solver.cpp:353] Iteration 44200 (59.0986 iter/s, 1.69209s/100 iter), loss = 0.013145
I0801 14:16:39.009269 24522 solver.cpp:375]     Train net output #0: loss = 0.0131443 (* 1 = 0.0131443 loss)
I0801 14:16:39.009296 24522 sgd_solver.cpp:136] Iteration 44200, lr = 0.00309375, m = 0.9
I0801 14:16:40.624222 24522 solver.cpp:353] Iteration 44300 (61.9202 iter/s, 1.61498s/100 iter), loss = 0.0693668
I0801 14:16:40.624244 24522 solver.cpp:375]     Train net output #0: loss = 0.0693661 (* 1 = 0.0693661 loss)
I0801 14:16:40.624249 24522 sgd_solver.cpp:136] Iteration 44300, lr = 0.00307812, m = 0.9
I0801 14:16:42.339866 24522 solver.cpp:353] Iteration 44400 (58.2891 iter/s, 1.71559s/100 iter), loss = 0.00209093
I0801 14:16:42.339895 24522 solver.cpp:375]     Train net output #0: loss = 0.00209024 (* 1 = 0.00209024 loss)
I0801 14:16:42.339900 24522 sgd_solver.cpp:136] Iteration 44400, lr = 0.0030625, m = 0.9
I0801 14:16:43.984433 24522 solver.cpp:353] Iteration 44500 (60.8084 iter/s, 1.64451s/100 iter), loss = 0.0071657
I0801 14:16:43.984508 24522 solver.cpp:375]     Train net output #0: loss = 0.007165 (* 1 = 0.007165 loss)
I0801 14:16:43.984530 24522 sgd_solver.cpp:136] Iteration 44500, lr = 0.00304687, m = 0.9
I0801 14:16:45.578915 24522 solver.cpp:353] Iteration 44600 (62.7183 iter/s, 1.59443s/100 iter), loss = 0.0170949
I0801 14:16:45.578969 24522 solver.cpp:375]     Train net output #0: loss = 0.0170942 (* 1 = 0.0170942 loss)
I0801 14:16:45.578979 24522 sgd_solver.cpp:136] Iteration 44600, lr = 0.00303125, m = 0.9
I0801 14:16:47.231216 24522 solver.cpp:353] Iteration 44700 (60.5235 iter/s, 1.65225s/100 iter), loss = 0.00102719
I0801 14:16:47.231242 24522 solver.cpp:375]     Train net output #0: loss = 0.0010265 (* 1 = 0.0010265 loss)
I0801 14:16:47.231248 24522 sgd_solver.cpp:136] Iteration 44700, lr = 0.00301562, m = 0.9
I0801 14:16:48.884407 24522 solver.cpp:353] Iteration 44800 (60.4911 iter/s, 1.65313s/100 iter), loss = 0.00328837
I0801 14:16:48.884459 24522 solver.cpp:375]     Train net output #0: loss = 0.00328768 (* 1 = 0.00328768 loss)
I0801 14:16:48.884475 24522 sgd_solver.cpp:136] Iteration 44800, lr = 0.003, m = 0.9
I0801 14:16:50.607966 24522 solver.cpp:353] Iteration 44900 (58.0213 iter/s, 1.72351s/100 iter), loss = 0.0102283
I0801 14:16:50.607990 24522 solver.cpp:375]     Train net output #0: loss = 0.0102276 (* 1 = 0.0102276 loss)
I0801 14:16:50.607996 24522 sgd_solver.cpp:136] Iteration 44900, lr = 0.00298437, m = 0.9
I0801 14:16:52.250116 24522 solver.cpp:404] Sparsity after update:
I0801 14:16:52.252400 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:16:52.252418 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:16:52.252450 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:16:52.252470 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:16:52.252487 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:16:52.252503 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:16:52.252518 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:16:52.252535 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:16:52.252550 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:16:52.252565 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:16:52.252581 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:16:52.252599 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:16:52.252615 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:16:52.252671 24522 solver.cpp:550] Iteration 45000, Testing net (#0)
I0801 14:16:53.111562 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.890295
I0801 14:16:53.111582 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 14:16:53.111588 24522 solver.cpp:635]     Test net output #2: loss = 0.460744 (* 1 = 0.460744 loss)
I0801 14:16:53.111604 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.858904s
I0801 14:16:53.127462 24522 solver.cpp:353] Iteration 45000 (39.6916 iter/s, 2.51942s/100 iter), loss = 0.00216663
I0801 14:16:53.127480 24522 solver.cpp:375]     Train net output #0: loss = 0.00216594 (* 1 = 0.00216594 loss)
I0801 14:16:53.127483 24522 sgd_solver.cpp:136] Iteration 45000, lr = 0.00296875, m = 0.9
I0801 14:16:54.834941 24522 solver.cpp:353] Iteration 45100 (58.5678 iter/s, 1.70742s/100 iter), loss = 0.00130178
I0801 14:16:54.834969 24522 solver.cpp:375]     Train net output #0: loss = 0.00130109 (* 1 = 0.00130109 loss)
I0801 14:16:54.834975 24522 sgd_solver.cpp:136] Iteration 45100, lr = 0.00295313, m = 0.9
I0801 14:16:56.452070 24522 solver.cpp:353] Iteration 45200 (61.8399 iter/s, 1.61708s/100 iter), loss = 0.0192443
I0801 14:16:56.452096 24522 solver.cpp:375]     Train net output #0: loss = 0.0192436 (* 1 = 0.0192436 loss)
I0801 14:16:56.452102 24522 sgd_solver.cpp:136] Iteration 45200, lr = 0.0029375, m = 0.9
I0801 14:16:58.082116 24522 solver.cpp:353] Iteration 45300 (61.3499 iter/s, 1.62999s/100 iter), loss = 0.000816064
I0801 14:16:58.082147 24522 solver.cpp:375]     Train net output #0: loss = 0.000815364 (* 1 = 0.000815364 loss)
I0801 14:16:58.082154 24522 sgd_solver.cpp:136] Iteration 45300, lr = 0.00292188, m = 0.9
I0801 14:16:59.719563 24522 solver.cpp:353] Iteration 45400 (61.0727 iter/s, 1.63739s/100 iter), loss = 0.0135891
I0801 14:16:59.719594 24522 solver.cpp:375]     Train net output #0: loss = 0.0135884 (* 1 = 0.0135884 loss)
I0801 14:16:59.719600 24522 sgd_solver.cpp:136] Iteration 45400, lr = 0.00290625, m = 0.9
I0801 14:17:01.361320 24522 solver.cpp:353] Iteration 45500 (60.9124 iter/s, 1.6417s/100 iter), loss = 0.00669323
I0801 14:17:01.361346 24522 solver.cpp:375]     Train net output #0: loss = 0.00669253 (* 1 = 0.00669253 loss)
I0801 14:17:01.361353 24522 sgd_solver.cpp:136] Iteration 45500, lr = 0.00289063, m = 0.9
I0801 14:17:03.045578 24522 solver.cpp:353] Iteration 45600 (59.3752 iter/s, 1.6842s/100 iter), loss = 0.00199184
I0801 14:17:03.045604 24522 solver.cpp:375]     Train net output #0: loss = 0.00199114 (* 1 = 0.00199114 loss)
I0801 14:17:03.045608 24522 sgd_solver.cpp:136] Iteration 45600, lr = 0.002875, m = 0.9
I0801 14:17:04.613617 24522 solver.cpp:353] Iteration 45700 (63.7758 iter/s, 1.56799s/100 iter), loss = 0.00368762
I0801 14:17:04.613683 24522 solver.cpp:375]     Train net output #0: loss = 0.00368692 (* 1 = 0.00368692 loss)
I0801 14:17:04.613700 24522 sgd_solver.cpp:136] Iteration 45700, lr = 0.00285937, m = 0.9
I0801 14:17:06.342447 24522 solver.cpp:353] Iteration 45800 (57.8445 iter/s, 1.72877s/100 iter), loss = 0.0120308
I0801 14:17:06.342514 24522 solver.cpp:375]     Train net output #0: loss = 0.0120301 (* 1 = 0.0120301 loss)
I0801 14:17:06.342521 24522 sgd_solver.cpp:136] Iteration 45800, lr = 0.00284375, m = 0.9
I0801 14:17:07.999166 24522 solver.cpp:353] Iteration 45900 (60.3622 iter/s, 1.65667s/100 iter), loss = 0.00030727
I0801 14:17:07.999215 24522 solver.cpp:375]     Train net output #0: loss = 0.000306588 (* 1 = 0.000306588 loss)
I0801 14:17:07.999228 24522 sgd_solver.cpp:136] Iteration 45900, lr = 0.00282812, m = 0.9
I0801 14:17:09.594343 24522 solver.cpp:404] Sparsity after update:
I0801 14:17:09.595916 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:17:09.595924 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:17:09.595932 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:17:09.595937 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:17:09.595942 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:17:09.595945 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:17:09.595949 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:17:09.595953 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:17:09.595957 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:17:09.595962 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:17:09.595965 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:17:09.595970 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:17:09.595974 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:17:09.595985 24522 solver.cpp:550] Iteration 46000, Testing net (#0)
I0801 14:17:10.407780 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.894413
I0801 14:17:10.407799 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 14:17:10.407804 24522 solver.cpp:635]     Test net output #2: loss = 0.423443 (* 1 = 0.423443 loss)
I0801 14:17:10.407820 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.811807s
I0801 14:17:10.427357 24522 solver.cpp:353] Iteration 46000 (41.1842 iter/s, 2.42812s/100 iter), loss = 0.00280235
I0801 14:17:10.427379 24522 solver.cpp:375]     Train net output #0: loss = 0.00280165 (* 1 = 0.00280165 loss)
I0801 14:17:10.427383 24522 sgd_solver.cpp:136] Iteration 46000, lr = 0.0028125, m = 0.9
I0801 14:17:12.006662 24522 solver.cpp:353] Iteration 46100 (63.321 iter/s, 1.57926s/100 iter), loss = 0.00362323
I0801 14:17:12.006690 24522 solver.cpp:375]     Train net output #0: loss = 0.00362253 (* 1 = 0.00362253 loss)
I0801 14:17:12.006695 24522 sgd_solver.cpp:136] Iteration 46100, lr = 0.00279688, m = 0.9
I0801 14:17:13.587709 24522 solver.cpp:353] Iteration 46200 (63.2513 iter/s, 1.581s/100 iter), loss = 0.000994526
I0801 14:17:13.587756 24522 solver.cpp:375]     Train net output #0: loss = 0.000993819 (* 1 = 0.000993819 loss)
I0801 14:17:13.587775 24522 sgd_solver.cpp:136] Iteration 46200, lr = 0.00278125, m = 0.9
I0801 14:17:15.301759 24522 solver.cpp:353] Iteration 46300 (58.3434 iter/s, 1.71399s/100 iter), loss = 0.00218274
I0801 14:17:15.301831 24522 solver.cpp:375]     Train net output #0: loss = 0.00218203 (* 1 = 0.00218203 loss)
I0801 14:17:15.301852 24522 sgd_solver.cpp:136] Iteration 46300, lr = 0.00276563, m = 0.9
I0801 14:17:16.966101 24522 solver.cpp:353] Iteration 46400 (60.0858 iter/s, 1.66429s/100 iter), loss = 0.0371972
I0801 14:17:16.966130 24522 solver.cpp:375]     Train net output #0: loss = 0.0371965 (* 1 = 0.0371965 loss)
I0801 14:17:16.966138 24522 sgd_solver.cpp:136] Iteration 46400, lr = 0.00275, m = 0.9
I0801 14:17:18.594499 24522 solver.cpp:353] Iteration 46500 (61.412 iter/s, 1.62835s/100 iter), loss = 0.00422712
I0801 14:17:18.594527 24522 solver.cpp:375]     Train net output #0: loss = 0.00422642 (* 1 = 0.00422642 loss)
I0801 14:17:18.594532 24522 sgd_solver.cpp:136] Iteration 46500, lr = 0.00273437, m = 0.9
I0801 14:17:20.174248 24522 solver.cpp:353] Iteration 46600 (63.3032 iter/s, 1.5797s/100 iter), loss = 0.000499518
I0801 14:17:20.174274 24522 solver.cpp:375]     Train net output #0: loss = 0.000498813 (* 1 = 0.000498813 loss)
I0801 14:17:20.174304 24522 sgd_solver.cpp:136] Iteration 46600, lr = 0.00271875, m = 0.9
I0801 14:17:21.761479 24522 solver.cpp:353] Iteration 46700 (63.0048 iter/s, 1.58718s/100 iter), loss = 0.00333456
I0801 14:17:21.761505 24522 solver.cpp:375]     Train net output #0: loss = 0.00333386 (* 1 = 0.00333386 loss)
I0801 14:17:21.761512 24522 sgd_solver.cpp:136] Iteration 46700, lr = 0.00270312, m = 0.9
I0801 14:17:23.334183 24522 solver.cpp:353] Iteration 46800 (63.5868 iter/s, 1.57265s/100 iter), loss = 0.00218479
I0801 14:17:23.334208 24522 solver.cpp:375]     Train net output #0: loss = 0.00218408 (* 1 = 0.00218408 loss)
I0801 14:17:23.334213 24522 sgd_solver.cpp:136] Iteration 46800, lr = 0.0026875, m = 0.9
I0801 14:17:24.945040 24522 solver.cpp:353] Iteration 46900 (62.0807 iter/s, 1.61081s/100 iter), loss = 0.000859444
I0801 14:17:24.945070 24522 solver.cpp:375]     Train net output #0: loss = 0.00085874 (* 1 = 0.00085874 loss)
I0801 14:17:24.945075 24522 sgd_solver.cpp:136] Iteration 46900, lr = 0.00267187, m = 0.9
I0801 14:17:26.546816 24522 solver.cpp:404] Sparsity after update:
I0801 14:17:26.548458 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:17:26.548467 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:17:26.548473 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:17:26.548476 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:17:26.548480 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:17:26.548481 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:17:26.548485 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:17:26.548486 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:17:26.548488 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:17:26.548491 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:17:26.548492 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:17:26.548494 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:17:26.548496 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:17:26.548504 24522 solver.cpp:550] Iteration 47000, Testing net (#0)
I0801 14:17:27.433969 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.891178
I0801 14:17:27.433987 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 14:17:27.433992 24522 solver.cpp:635]     Test net output #2: loss = 0.439066 (* 1 = 0.439066 loss)
I0801 14:17:27.434006 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.885472s
I0801 14:17:27.449875 24522 solver.cpp:353] Iteration 47000 (39.924 iter/s, 2.50476s/100 iter), loss = 0.00263097
I0801 14:17:27.449893 24522 solver.cpp:375]     Train net output #0: loss = 0.00263027 (* 1 = 0.00263027 loss)
I0801 14:17:27.449899 24522 sgd_solver.cpp:136] Iteration 47000, lr = 0.00265625, m = 0.9
I0801 14:17:29.122589 24522 solver.cpp:353] Iteration 47100 (59.7851 iter/s, 1.67266s/100 iter), loss = 0.0120141
I0801 14:17:29.122613 24522 solver.cpp:375]     Train net output #0: loss = 0.0120134 (* 1 = 0.0120134 loss)
I0801 14:17:29.122618 24522 sgd_solver.cpp:136] Iteration 47100, lr = 0.00264063, m = 0.9
I0801 14:17:30.738884 24522 solver.cpp:353] Iteration 47200 (61.872 iter/s, 1.61624s/100 iter), loss = 0.004296
I0801 14:17:30.738914 24522 solver.cpp:375]     Train net output #0: loss = 0.0042953 (* 1 = 0.0042953 loss)
I0801 14:17:30.738919 24522 sgd_solver.cpp:136] Iteration 47200, lr = 0.002625, m = 0.9
I0801 14:17:32.417963 24522 solver.cpp:353] Iteration 47300 (59.5582 iter/s, 1.67903s/100 iter), loss = 0.00577798
I0801 14:17:32.417991 24522 solver.cpp:375]     Train net output #0: loss = 0.00577728 (* 1 = 0.00577728 loss)
I0801 14:17:32.417997 24522 sgd_solver.cpp:136] Iteration 47300, lr = 0.00260938, m = 0.9
I0801 14:17:34.117321 24522 solver.cpp:353] Iteration 47400 (58.8478 iter/s, 1.6993s/100 iter), loss = 0.00228793
I0801 14:17:34.117354 24522 solver.cpp:375]     Train net output #0: loss = 0.00228723 (* 1 = 0.00228723 loss)
I0801 14:17:34.117360 24522 sgd_solver.cpp:136] Iteration 47400, lr = 0.00259375, m = 0.9
I0801 14:17:35.716992 24522 solver.cpp:353] Iteration 47500 (62.5148 iter/s, 1.59962s/100 iter), loss = 0.00365444
I0801 14:17:35.717020 24522 solver.cpp:375]     Train net output #0: loss = 0.00365374 (* 1 = 0.00365374 loss)
I0801 14:17:35.717025 24522 sgd_solver.cpp:136] Iteration 47500, lr = 0.00257812, m = 0.9
I0801 14:17:37.393082 24522 solver.cpp:353] Iteration 47600 (59.6645 iter/s, 1.67604s/100 iter), loss = 0.000827092
I0801 14:17:37.393167 24522 solver.cpp:375]     Train net output #0: loss = 0.000826394 (* 1 = 0.000826394 loss)
I0801 14:17:37.393173 24522 sgd_solver.cpp:136] Iteration 47600, lr = 0.0025625, m = 0.9
I0801 14:17:39.101155 24522 solver.cpp:353] Iteration 47700 (58.5475 iter/s, 1.70801s/100 iter), loss = 0.0213427
I0801 14:17:39.101183 24522 solver.cpp:375]     Train net output #0: loss = 0.021342 (* 1 = 0.021342 loss)
I0801 14:17:39.101191 24522 sgd_solver.cpp:136] Iteration 47700, lr = 0.00254687, m = 0.9
I0801 14:17:40.770917 24522 solver.cpp:353] Iteration 47800 (59.8905 iter/s, 1.66971s/100 iter), loss = 0.000689803
I0801 14:17:40.770944 24522 solver.cpp:375]     Train net output #0: loss = 0.000689094 (* 1 = 0.000689094 loss)
I0801 14:17:40.770951 24522 sgd_solver.cpp:136] Iteration 47800, lr = 0.00253125, m = 0.9
I0801 14:17:42.497822 24522 solver.cpp:353] Iteration 47900 (57.9091 iter/s, 1.72684s/100 iter), loss = 0.00046019
I0801 14:17:42.497867 24522 solver.cpp:375]     Train net output #0: loss = 0.000459478 (* 1 = 0.000459478 loss)
I0801 14:17:42.497889 24522 sgd_solver.cpp:136] Iteration 47900, lr = 0.00251562, m = 0.9
I0801 14:17:44.221591 24522 solver.cpp:404] Sparsity after update:
I0801 14:17:44.223492 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:17:44.223505 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:17:44.223513 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:17:44.223518 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:17:44.223522 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:17:44.223526 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:17:44.223531 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:17:44.223533 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:17:44.223538 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:17:44.223541 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:17:44.223546 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:17:44.223549 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:17:44.223554 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:17:44.223565 24522 solver.cpp:550] Iteration 48000, Testing net (#0)
I0801 14:17:44.258483 24520 data_reader.cpp:264] Starting prefetch of epoch 6
I0801 14:17:45.053171 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.89206
I0801 14:17:45.053189 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 14:17:45.053195 24522 solver.cpp:635]     Test net output #2: loss = 0.430369 (* 1 = 0.430369 loss)
I0801 14:17:45.053211 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.829616s
I0801 14:17:45.068876 24522 solver.cpp:353] Iteration 48000 (38.8956 iter/s, 2.57098s/100 iter), loss = 0.00114082
I0801 14:17:45.068910 24522 solver.cpp:375]     Train net output #0: loss = 0.00114011 (* 1 = 0.00114011 loss)
I0801 14:17:45.068925 24522 sgd_solver.cpp:136] Iteration 48000, lr = 0.0025, m = 0.9
I0801 14:17:46.778707 24522 solver.cpp:353] Iteration 48100 (58.4873 iter/s, 1.70977s/100 iter), loss = 0.00418524
I0801 14:17:46.778729 24522 solver.cpp:375]     Train net output #0: loss = 0.00418453 (* 1 = 0.00418453 loss)
I0801 14:17:46.778733 24522 sgd_solver.cpp:136] Iteration 48100, lr = 0.00248438, m = 0.9
I0801 14:17:48.405146 24522 solver.cpp:353] Iteration 48200 (61.4861 iter/s, 1.62638s/100 iter), loss = 0.00690877
I0801 14:17:48.405192 24522 solver.cpp:375]     Train net output #0: loss = 0.00690806 (* 1 = 0.00690806 loss)
I0801 14:17:48.405205 24522 sgd_solver.cpp:136] Iteration 48200, lr = 0.00246875, m = 0.9
I0801 14:17:50.030041 24522 solver.cpp:353] Iteration 48300 (61.5446 iter/s, 1.62484s/100 iter), loss = 0.000810311
I0801 14:17:50.030088 24522 solver.cpp:375]     Train net output #0: loss = 0.000809602 (* 1 = 0.000809602 loss)
I0801 14:17:50.030108 24522 sgd_solver.cpp:136] Iteration 48300, lr = 0.00245313, m = 0.9
I0801 14:17:51.647233 24522 solver.cpp:353] Iteration 48400 (61.8373 iter/s, 1.61715s/100 iter), loss = 0.00223008
I0801 14:17:51.647306 24522 solver.cpp:375]     Train net output #0: loss = 0.00222937 (* 1 = 0.00222937 loss)
I0801 14:17:51.647322 24522 sgd_solver.cpp:136] Iteration 48400, lr = 0.0024375, m = 0.9
I0801 14:17:53.312355 24522 solver.cpp:353] Iteration 48500 (60.0575 iter/s, 1.66507s/100 iter), loss = 0.000370159
I0801 14:17:53.312381 24522 solver.cpp:375]     Train net output #0: loss = 0.000369446 (* 1 = 0.000369446 loss)
I0801 14:17:53.312388 24522 sgd_solver.cpp:136] Iteration 48500, lr = 0.00242188, m = 0.9
I0801 14:17:54.992120 24522 solver.cpp:353] Iteration 48600 (59.5342 iter/s, 1.67971s/100 iter), loss = 0.0039023
I0801 14:17:54.992187 24522 solver.cpp:375]     Train net output #0: loss = 0.00390159 (* 1 = 0.00390159 loss)
I0801 14:17:54.992202 24522 sgd_solver.cpp:136] Iteration 48600, lr = 0.00240625, m = 0.9
I0801 14:17:56.657294 24522 solver.cpp:353] Iteration 48700 (60.0558 iter/s, 1.66512s/100 iter), loss = 0.001293
I0801 14:17:56.657325 24522 solver.cpp:375]     Train net output #0: loss = 0.00129229 (* 1 = 0.00129229 loss)
I0801 14:17:56.657331 24522 sgd_solver.cpp:136] Iteration 48700, lr = 0.00239062, m = 0.9
I0801 14:17:58.392566 24522 solver.cpp:353] Iteration 48800 (57.6297 iter/s, 1.73522s/100 iter), loss = 0.000860412
I0801 14:17:58.392593 24522 solver.cpp:375]     Train net output #0: loss = 0.000859705 (* 1 = 0.000859705 loss)
I0801 14:17:58.392599 24522 sgd_solver.cpp:136] Iteration 48800, lr = 0.002375, m = 0.9
I0801 14:18:00.055704 24522 solver.cpp:353] Iteration 48900 (60.1296 iter/s, 1.66308s/100 iter), loss = 0.00330686
I0801 14:18:00.055769 24522 solver.cpp:375]     Train net output #0: loss = 0.00330615 (* 1 = 0.00330615 loss)
I0801 14:18:00.055790 24522 sgd_solver.cpp:136] Iteration 48900, lr = 0.00235937, m = 0.9
I0801 14:18:01.747787 24522 solver.cpp:404] Sparsity after update:
I0801 14:18:01.749672 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:18:01.749682 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:18:01.749691 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:18:01.749696 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:18:01.749701 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:18:01.749706 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:18:01.749711 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:18:01.749714 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:18:01.749718 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:18:01.749722 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:18:01.749725 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:18:01.749729 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:18:01.749733 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:18:01.749744 24522 solver.cpp:550] Iteration 49000, Testing net (#0)
I0801 14:18:02.578824 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.902942
I0801 14:18:02.578848 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995588
I0801 14:18:02.578853 24522 solver.cpp:635]     Test net output #2: loss = 0.412203 (* 1 = 0.412203 loss)
I0801 14:18:02.578872 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.829099s
I0801 14:18:02.595520 24522 solver.cpp:353] Iteration 49000 (39.3741 iter/s, 2.53974s/100 iter), loss = 0.00834225
I0801 14:18:02.595556 24522 solver.cpp:375]     Train net output #0: loss = 0.00834155 (* 1 = 0.00834155 loss)
I0801 14:18:02.595566 24522 sgd_solver.cpp:136] Iteration 49000, lr = 0.00234375, m = 0.9
I0801 14:18:04.229593 24522 solver.cpp:353] Iteration 49100 (61.1987 iter/s, 1.63402s/100 iter), loss = 0.00212511
I0801 14:18:04.229619 24522 solver.cpp:375]     Train net output #0: loss = 0.00212441 (* 1 = 0.00212441 loss)
I0801 14:18:04.229625 24522 sgd_solver.cpp:136] Iteration 49100, lr = 0.00232813, m = 0.9
I0801 14:18:05.870824 24522 solver.cpp:353] Iteration 49200 (60.9318 iter/s, 1.64118s/100 iter), loss = 0.00224913
I0801 14:18:05.870875 24522 solver.cpp:375]     Train net output #0: loss = 0.00224843 (* 1 = 0.00224843 loss)
I0801 14:18:05.870903 24522 sgd_solver.cpp:136] Iteration 49200, lr = 0.0023125, m = 0.9
I0801 14:18:07.518133 24522 solver.cpp:353] Iteration 49300 (60.707 iter/s, 1.64726s/100 iter), loss = 0.00177838
I0801 14:18:07.518210 24522 solver.cpp:375]     Train net output #0: loss = 0.00177768 (* 1 = 0.00177768 loss)
I0801 14:18:07.518218 24522 sgd_solver.cpp:136] Iteration 49300, lr = 0.00229687, m = 0.9
I0801 14:18:09.174814 24522 solver.cpp:353] Iteration 49400 (60.3636 iter/s, 1.65663s/100 iter), loss = 0.000718735
I0801 14:18:09.174836 24522 solver.cpp:375]     Train net output #0: loss = 0.000718035 (* 1 = 0.000718035 loss)
I0801 14:18:09.174840 24522 sgd_solver.cpp:136] Iteration 49400, lr = 0.00228125, m = 0.9
I0801 14:18:10.771766 24522 solver.cpp:353] Iteration 49500 (62.6214 iter/s, 1.5969s/100 iter), loss = 0.000590206
I0801 14:18:10.771816 24522 solver.cpp:375]     Train net output #0: loss = 0.000589506 (* 1 = 0.000589506 loss)
I0801 14:18:10.771829 24522 sgd_solver.cpp:136] Iteration 49500, lr = 0.00226562, m = 0.9
I0801 14:18:12.404362 24522 solver.cpp:353] Iteration 49600 (61.2541 iter/s, 1.63254s/100 iter), loss = 0.00594818
I0801 14:18:12.404412 24522 solver.cpp:375]     Train net output #0: loss = 0.00594748 (* 1 = 0.00594748 loss)
I0801 14:18:12.404424 24522 sgd_solver.cpp:136] Iteration 49600, lr = 0.00225, m = 0.9
I0801 14:18:14.096853 24522 solver.cpp:353] Iteration 49700 (59.0863 iter/s, 1.69244s/100 iter), loss = 0.000240295
I0801 14:18:14.096902 24522 solver.cpp:375]     Train net output #0: loss = 0.000239594 (* 1 = 0.000239594 loss)
I0801 14:18:14.096913 24522 sgd_solver.cpp:136] Iteration 49700, lr = 0.00223437, m = 0.9
I0801 14:18:15.750527 24522 solver.cpp:353] Iteration 49800 (60.4733 iter/s, 1.65362s/100 iter), loss = 0.000420505
I0801 14:18:15.750556 24522 solver.cpp:375]     Train net output #0: loss = 0.000419805 (* 1 = 0.000419805 loss)
I0801 14:18:15.750562 24522 sgd_solver.cpp:136] Iteration 49800, lr = 0.00221875, m = 0.9
I0801 14:18:17.460803 24522 solver.cpp:353] Iteration 49900 (58.472 iter/s, 1.71022s/100 iter), loss = 7.42731e-05
I0801 14:18:17.460831 24522 solver.cpp:375]     Train net output #0: loss = 7.35756e-05 (* 1 = 7.35756e-05 loss)
I0801 14:18:17.460835 24522 sgd_solver.cpp:136] Iteration 49900, lr = 0.00220312, m = 0.9
I0801 14:18:19.045475 24522 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_50000.caffemodel
I0801 14:18:19.053601 24522 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_50000.solverstate
I0801 14:18:19.057268 24522 solver.cpp:404] Sparsity after update:
I0801 14:18:19.059078 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:18:19.059087 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:18:19.059092 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:18:19.059094 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:18:19.059096 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:18:19.059098 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:18:19.059100 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:18:19.059103 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:18:19.059104 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:18:19.059106 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:18:19.059108 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:18:19.059111 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:18:19.059113 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:18:19.059120 24522 solver.cpp:550] Iteration 50000, Testing net (#0)
I0801 14:18:19.863417 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.895295
I0801 14:18:19.863436 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995294
I0801 14:18:19.863441 24522 solver.cpp:635]     Test net output #2: loss = 0.442876 (* 1 = 0.442876 loss)
I0801 14:18:19.863456 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.804308s
I0801 14:18:19.879592 24522 solver.cpp:353] Iteration 50000 (41.3442 iter/s, 2.41872s/100 iter), loss = 0.000229349
I0801 14:18:19.879623 24522 solver.cpp:375]     Train net output #0: loss = 0.000228651 (* 1 = 0.000228651 loss)
I0801 14:18:19.879628 24522 sgd_solver.cpp:136] Iteration 50000, lr = 0.0021875, m = 0.9
I0801 14:18:21.449702 24522 solver.cpp:353] Iteration 50100 (63.6919 iter/s, 1.57006s/100 iter), loss = 0.000642494
I0801 14:18:21.449753 24522 solver.cpp:375]     Train net output #0: loss = 0.000641796 (* 1 = 0.000641796 loss)
I0801 14:18:21.449767 24522 sgd_solver.cpp:136] Iteration 50100, lr = 0.00217188, m = 0.9
I0801 14:18:23.041909 24522 solver.cpp:353] Iteration 50200 (62.8079 iter/s, 1.59216s/100 iter), loss = 0.0054778
I0801 14:18:23.041934 24522 solver.cpp:375]     Train net output #0: loss = 0.0054771 (* 1 = 0.0054771 loss)
I0801 14:18:23.041940 24522 sgd_solver.cpp:136] Iteration 50200, lr = 0.00215625, m = 0.9
I0801 14:18:24.626925 24522 solver.cpp:353] Iteration 50300 (63.0929 iter/s, 1.58496s/100 iter), loss = 5.7159e-05
I0801 14:18:24.626950 24522 solver.cpp:375]     Train net output #0: loss = 5.64621e-05 (* 1 = 5.64621e-05 loss)
I0801 14:18:24.626957 24522 sgd_solver.cpp:136] Iteration 50300, lr = 0.00214063, m = 0.9
I0801 14:18:26.235194 24522 solver.cpp:353] Iteration 50400 (62.1807 iter/s, 1.60822s/100 iter), loss = 0.000540474
I0801 14:18:26.235225 24522 solver.cpp:375]     Train net output #0: loss = 0.000539779 (* 1 = 0.000539779 loss)
I0801 14:18:26.235232 24522 sgd_solver.cpp:136] Iteration 50400, lr = 0.002125, m = 0.9
I0801 14:18:27.841547 24522 solver.cpp:353] Iteration 50500 (62.2548 iter/s, 1.6063s/100 iter), loss = 0.00529038
I0801 14:18:27.841754 24522 solver.cpp:375]     Train net output #0: loss = 0.00528969 (* 1 = 0.00528969 loss)
I0801 14:18:27.841814 24522 sgd_solver.cpp:136] Iteration 50500, lr = 0.00210937, m = 0.9
I0801 14:18:29.430892 24522 solver.cpp:353] Iteration 50600 (62.921 iter/s, 1.5893s/100 iter), loss = 0.00183118
I0801 14:18:29.430920 24522 solver.cpp:375]     Train net output #0: loss = 0.00183048 (* 1 = 0.00183048 loss)
I0801 14:18:29.430927 24522 sgd_solver.cpp:136] Iteration 50600, lr = 0.00209375, m = 0.9
I0801 14:18:31.008646 24522 solver.cpp:353] Iteration 50700 (63.3833 iter/s, 1.5777s/100 iter), loss = 0.000184943
I0801 14:18:31.008673 24522 solver.cpp:375]     Train net output #0: loss = 0.00018425 (* 1 = 0.00018425 loss)
I0801 14:18:31.008679 24522 sgd_solver.cpp:136] Iteration 50700, lr = 0.00207812, m = 0.9
I0801 14:18:32.612749 24522 solver.cpp:353] Iteration 50800 (62.3421 iter/s, 1.60405s/100 iter), loss = 0.00224082
I0801 14:18:32.612777 24522 solver.cpp:375]     Train net output #0: loss = 0.00224013 (* 1 = 0.00224013 loss)
I0801 14:18:32.612782 24522 sgd_solver.cpp:136] Iteration 50800, lr = 0.0020625, m = 0.9
I0801 14:18:34.199223 24522 solver.cpp:353] Iteration 50900 (63.035 iter/s, 1.58642s/100 iter), loss = 0.00072819
I0801 14:18:34.199249 24522 solver.cpp:375]     Train net output #0: loss = 0.0007275 (* 1 = 0.0007275 loss)
I0801 14:18:34.199255 24522 sgd_solver.cpp:136] Iteration 50900, lr = 0.00204687, m = 0.9
I0801 14:18:35.757325 24522 solver.cpp:404] Sparsity after update:
I0801 14:18:35.758927 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:18:35.758936 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:18:35.758944 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:18:35.758949 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:18:35.758954 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:18:35.758958 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:18:35.758962 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:18:35.758966 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:18:35.758970 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:18:35.758975 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:18:35.758978 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:18:35.758983 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:18:35.758987 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:18:35.758998 24522 solver.cpp:550] Iteration 51000, Testing net (#0)
I0801 14:18:36.610667 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.901178
I0801 14:18:36.610687 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.994706
I0801 14:18:36.610693 24522 solver.cpp:635]     Test net output #2: loss = 0.433684 (* 1 = 0.433684 loss)
I0801 14:18:36.610713 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.851686s
I0801 14:18:36.626394 24522 solver.cpp:353] Iteration 51000 (41.2014 iter/s, 2.4271s/100 iter), loss = 0.00252364
I0801 14:18:36.626411 24522 solver.cpp:375]     Train net output #0: loss = 0.00252295 (* 1 = 0.00252295 loss)
I0801 14:18:36.626417 24522 sgd_solver.cpp:136] Iteration 51000, lr = 0.00203125, m = 0.9
I0801 14:18:38.355356 24522 solver.cpp:353] Iteration 51100 (57.8402 iter/s, 1.7289s/100 iter), loss = 0.000679729
I0801 14:18:38.355465 24522 solver.cpp:375]     Train net output #0: loss = 0.00067904 (* 1 = 0.00067904 loss)
I0801 14:18:38.355478 24522 sgd_solver.cpp:136] Iteration 51100, lr = 0.00201563, m = 0.9
I0801 14:18:39.969552 24522 solver.cpp:353] Iteration 51200 (61.9523 iter/s, 1.61415s/100 iter), loss = 0.000356135
I0801 14:18:39.969624 24522 solver.cpp:375]     Train net output #0: loss = 0.000355447 (* 1 = 0.000355447 loss)
I0801 14:18:39.969640 24522 sgd_solver.cpp:136] Iteration 51200, lr = 0.002, m = 0.9
I0801 14:18:41.599146 24522 solver.cpp:353] Iteration 51300 (61.3674 iter/s, 1.62953s/100 iter), loss = 0.000994919
I0801 14:18:41.599345 24522 solver.cpp:375]     Train net output #0: loss = 0.000994231 (* 1 = 0.000994231 loss)
I0801 14:18:41.599449 24522 sgd_solver.cpp:136] Iteration 51300, lr = 0.00198438, m = 0.9
I0801 14:18:43.245909 24522 solver.cpp:353] Iteration 51400 (60.7269 iter/s, 1.64672s/100 iter), loss = 0.00365058
I0801 14:18:43.245936 24522 solver.cpp:375]     Train net output #0: loss = 0.00364989 (* 1 = 0.00364989 loss)
I0801 14:18:43.245941 24522 sgd_solver.cpp:136] Iteration 51400, lr = 0.00196875, m = 0.9
I0801 14:18:44.926818 24522 solver.cpp:353] Iteration 51500 (59.4935 iter/s, 1.68086s/100 iter), loss = 0.000449854
I0801 14:18:44.926843 24522 solver.cpp:375]     Train net output #0: loss = 0.000449164 (* 1 = 0.000449164 loss)
I0801 14:18:44.926849 24522 sgd_solver.cpp:136] Iteration 51500, lr = 0.00195312, m = 0.9
I0801 14:18:46.739648 24522 solver.cpp:353] Iteration 51600 (55.1641 iter/s, 1.81277s/100 iter), loss = 0.00108968
I0801 14:18:46.739672 24522 solver.cpp:375]     Train net output #0: loss = 0.00108899 (* 1 = 0.00108899 loss)
I0801 14:18:46.739677 24522 sgd_solver.cpp:136] Iteration 51600, lr = 0.0019375, m = 0.9
I0801 14:18:48.373059 24522 solver.cpp:353] Iteration 51700 (61.2235 iter/s, 1.63336s/100 iter), loss = 0.00243366
I0801 14:18:48.373090 24522 solver.cpp:375]     Train net output #0: loss = 0.00243297 (* 1 = 0.00243297 loss)
I0801 14:18:48.373095 24522 sgd_solver.cpp:136] Iteration 51700, lr = 0.00192187, m = 0.9
I0801 14:18:50.083575 24522 solver.cpp:353] Iteration 51800 (58.4637 iter/s, 1.71046s/100 iter), loss = 0.000328123
I0801 14:18:50.083600 24522 solver.cpp:375]     Train net output #0: loss = 0.000327435 (* 1 = 0.000327435 loss)
I0801 14:18:50.083605 24522 sgd_solver.cpp:136] Iteration 51800, lr = 0.00190625, m = 0.9
I0801 14:18:51.761446 24522 solver.cpp:353] Iteration 51900 (59.6013 iter/s, 1.67782s/100 iter), loss = 0.000230751
I0801 14:18:51.761471 24522 solver.cpp:375]     Train net output #0: loss = 0.000230063 (* 1 = 0.000230063 loss)
I0801 14:18:51.761476 24522 sgd_solver.cpp:136] Iteration 51900, lr = 0.00189062, m = 0.9
I0801 14:18:53.420941 24522 solver.cpp:404] Sparsity after update:
I0801 14:18:53.422505 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:18:53.422513 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:18:53.422519 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:18:53.422521 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:18:53.422523 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:18:53.422525 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:18:53.422533 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:18:53.422534 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:18:53.422536 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:18:53.422538 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:18:53.422539 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:18:53.422541 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:18:53.422544 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:18:53.422551 24522 solver.cpp:550] Iteration 52000, Testing net (#0)
I0801 14:18:54.143776 24520 data_reader.cpp:264] Starting prefetch of epoch 7
I0801 14:18:54.233335 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.90353
I0801 14:18:54.233351 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 14:18:54.233373 24522 solver.cpp:635]     Test net output #2: loss = 0.419384 (* 1 = 0.419384 loss)
I0801 14:18:54.233393 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.810812s
I0801 14:18:54.249472 24522 solver.cpp:353] Iteration 52000 (40.1937 iter/s, 2.48795s/100 iter), loss = 0.00122811
I0801 14:18:54.249491 24522 solver.cpp:375]     Train net output #0: loss = 0.00122742 (* 1 = 0.00122742 loss)
I0801 14:18:54.249497 24522 sgd_solver.cpp:136] Iteration 52000, lr = 0.001875, m = 0.9
I0801 14:18:55.996017 24522 solver.cpp:353] Iteration 52100 (57.2578 iter/s, 1.74649s/100 iter), loss = 0.00152307
I0801 14:18:55.996047 24522 solver.cpp:375]     Train net output #0: loss = 0.00152238 (* 1 = 0.00152238 loss)
I0801 14:18:55.996073 24522 sgd_solver.cpp:136] Iteration 52100, lr = 0.00185938, m = 0.9
I0801 14:18:57.664580 24522 solver.cpp:353] Iteration 52200 (59.9338 iter/s, 1.66851s/100 iter), loss = 0.0010322
I0801 14:18:57.664611 24522 solver.cpp:375]     Train net output #0: loss = 0.00103151 (* 1 = 0.00103151 loss)
I0801 14:18:57.664618 24522 sgd_solver.cpp:136] Iteration 52200, lr = 0.00184375, m = 0.9
I0801 14:18:59.292511 24522 solver.cpp:353] Iteration 52300 (61.4295 iter/s, 1.62788s/100 iter), loss = 0.000537884
I0801 14:18:59.292537 24522 solver.cpp:375]     Train net output #0: loss = 0.000537197 (* 1 = 0.000537197 loss)
I0801 14:18:59.292543 24522 sgd_solver.cpp:136] Iteration 52300, lr = 0.00182813, m = 0.9
I0801 14:19:00.917201 24522 solver.cpp:353] Iteration 52400 (61.5524 iter/s, 1.62463s/100 iter), loss = 0.000921667
I0801 14:19:00.917312 24522 solver.cpp:375]     Train net output #0: loss = 0.00092098 (* 1 = 0.00092098 loss)
I0801 14:19:00.917335 24522 sgd_solver.cpp:136] Iteration 52400, lr = 0.0018125, m = 0.9
I0801 14:19:02.633437 24522 solver.cpp:353] Iteration 52500 (58.2688 iter/s, 1.71618s/100 iter), loss = 0.000841595
I0801 14:19:02.633509 24522 solver.cpp:375]     Train net output #0: loss = 0.000840908 (* 1 = 0.000840908 loss)
I0801 14:19:02.633524 24522 sgd_solver.cpp:136] Iteration 52500, lr = 0.00179687, m = 0.9
I0801 14:19:04.297587 24522 solver.cpp:353] Iteration 52600 (60.0926 iter/s, 1.6641s/100 iter), loss = 0.00260883
I0801 14:19:04.297641 24522 solver.cpp:375]     Train net output #0: loss = 0.00260814 (* 1 = 0.00260814 loss)
I0801 14:19:04.297657 24522 sgd_solver.cpp:136] Iteration 52600, lr = 0.00178125, m = 0.9
I0801 14:19:06.012064 24522 solver.cpp:353] Iteration 52700 (58.3287 iter/s, 1.71442s/100 iter), loss = 0.000245146
I0801 14:19:06.012133 24522 solver.cpp:375]     Train net output #0: loss = 0.000244458 (* 1 = 0.000244458 loss)
I0801 14:19:06.012152 24522 sgd_solver.cpp:136] Iteration 52700, lr = 0.00176562, m = 0.9
I0801 14:19:07.720930 24522 solver.cpp:353] Iteration 52800 (58.5201 iter/s, 1.70881s/100 iter), loss = 0.00142103
I0801 14:19:07.720955 24522 solver.cpp:375]     Train net output #0: loss = 0.00142034 (* 1 = 0.00142034 loss)
I0801 14:19:07.720962 24522 sgd_solver.cpp:136] Iteration 52800, lr = 0.00175, m = 0.9
I0801 14:19:09.428107 24522 solver.cpp:353] Iteration 52900 (58.5781 iter/s, 1.70712s/100 iter), loss = 0.000235937
I0801 14:19:09.428182 24522 solver.cpp:375]     Train net output #0: loss = 0.000235251 (* 1 = 0.000235251 loss)
I0801 14:19:09.428189 24522 sgd_solver.cpp:136] Iteration 52900, lr = 0.00173437, m = 0.9
I0801 14:19:11.044386 24522 solver.cpp:404] Sparsity after update:
I0801 14:19:11.046187 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:19:11.046197 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:19:11.046206 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:19:11.046211 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:19:11.046216 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:19:11.046218 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:19:11.046221 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:19:11.046224 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:19:11.046228 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:19:11.046231 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:19:11.046234 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:19:11.046237 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:19:11.046241 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:19:11.046250 24522 solver.cpp:550] Iteration 53000, Testing net (#0)
I0801 14:19:11.928683 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.910295
I0801 14:19:11.928711 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 14:19:11.928719 24522 solver.cpp:635]     Test net output #2: loss = 0.394632 (* 1 = 0.394632 loss)
I0801 14:19:11.928743 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.882461s
I0801 14:19:11.945868 24522 solver.cpp:353] Iteration 53000 (39.7192 iter/s, 2.51768s/100 iter), loss = 0.00126861
I0801 14:19:11.945909 24522 solver.cpp:375]     Train net output #0: loss = 0.00126792 (* 1 = 0.00126792 loss)
I0801 14:19:11.945921 24522 sgd_solver.cpp:136] Iteration 53000, lr = 0.00171875, m = 0.9
I0801 14:19:13.665908 24522 solver.cpp:353] Iteration 53100 (58.1398 iter/s, 1.71999s/100 iter), loss = 0.000434517
I0801 14:19:13.665961 24522 solver.cpp:375]     Train net output #0: loss = 0.00043383 (* 1 = 0.00043383 loss)
I0801 14:19:13.665978 24522 sgd_solver.cpp:136] Iteration 53100, lr = 0.00170313, m = 0.9
I0801 14:19:15.393741 24522 solver.cpp:353] Iteration 53200 (57.8781 iter/s, 1.72777s/100 iter), loss = 0.000196463
I0801 14:19:15.393793 24522 solver.cpp:375]     Train net output #0: loss = 0.000195778 (* 1 = 0.000195778 loss)
I0801 14:19:15.393806 24522 sgd_solver.cpp:136] Iteration 53200, lr = 0.0016875, m = 0.9
I0801 14:19:17.140846 24522 solver.cpp:353] Iteration 53300 (57.2393 iter/s, 1.74705s/100 iter), loss = 7.75023e-05
I0801 14:19:17.140892 24522 solver.cpp:375]     Train net output #0: loss = 7.68179e-05 (* 1 = 7.68179e-05 loss)
I0801 14:19:17.140903 24522 sgd_solver.cpp:136] Iteration 53300, lr = 0.00167188, m = 0.9
I0801 14:19:18.849714 24522 solver.cpp:353] Iteration 53400 (58.5201 iter/s, 1.70882s/100 iter), loss = 0.000684103
I0801 14:19:18.849740 24522 solver.cpp:375]     Train net output #0: loss = 0.000683419 (* 1 = 0.000683419 loss)
I0801 14:19:18.849745 24522 sgd_solver.cpp:136] Iteration 53400, lr = 0.00165625, m = 0.9
I0801 14:19:20.545780 24522 solver.cpp:353] Iteration 53500 (58.9618 iter/s, 1.69601s/100 iter), loss = 0.000975593
I0801 14:19:20.545807 24522 solver.cpp:375]     Train net output #0: loss = 0.000974909 (* 1 = 0.000974909 loss)
I0801 14:19:20.545812 24522 sgd_solver.cpp:136] Iteration 53500, lr = 0.00164062, m = 0.9
I0801 14:19:22.259513 24522 solver.cpp:353] Iteration 53600 (58.3543 iter/s, 1.71367s/100 iter), loss = 0.000919639
I0801 14:19:22.259618 24522 solver.cpp:375]     Train net output #0: loss = 0.000918956 (* 1 = 0.000918956 loss)
I0801 14:19:22.259642 24522 sgd_solver.cpp:136] Iteration 53600, lr = 0.001625, m = 0.9
I0801 14:19:23.870846 24522 solver.cpp:353] Iteration 53700 (62.0622 iter/s, 1.61129s/100 iter), loss = 0.0016663
I0801 14:19:23.870873 24522 solver.cpp:375]     Train net output #0: loss = 0.00166562 (* 1 = 0.00166562 loss)
I0801 14:19:23.870900 24522 sgd_solver.cpp:136] Iteration 53700, lr = 0.00160937, m = 0.9
I0801 14:19:25.553444 24522 solver.cpp:353] Iteration 53800 (59.4338 iter/s, 1.68254s/100 iter), loss = 0.00363187
I0801 14:19:25.553557 24522 solver.cpp:375]     Train net output #0: loss = 0.00363118 (* 1 = 0.00363118 loss)
I0801 14:19:25.553576 24522 sgd_solver.cpp:136] Iteration 53800, lr = 0.00159375, m = 0.9
I0801 14:19:27.320119 24522 solver.cpp:353] Iteration 53900 (56.6052 iter/s, 1.76662s/100 iter), loss = 0.0119178
I0801 14:19:27.320142 24522 solver.cpp:375]     Train net output #0: loss = 0.0119172 (* 1 = 0.0119172 loss)
I0801 14:19:27.320145 24522 sgd_solver.cpp:136] Iteration 53900, lr = 0.00157812, m = 0.9
I0801 14:19:28.903018 24522 solver.cpp:404] Sparsity after update:
I0801 14:19:28.904639 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:19:28.904647 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:19:28.904655 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:19:28.904660 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:19:28.904664 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:19:28.904670 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:19:28.904673 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:19:28.904677 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:19:28.904681 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:19:28.904685 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:19:28.904688 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:19:28.904692 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:19:28.904695 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:19:28.904706 24522 solver.cpp:550] Iteration 54000, Testing net (#0)
I0801 14:19:29.711675 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.915295
I0801 14:19:29.711699 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 14:19:29.711705 24522 solver.cpp:635]     Test net output #2: loss = 0.373765 (* 1 = 0.373765 loss)
I0801 14:19:29.711724 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.80699s
I0801 14:19:29.727924 24522 solver.cpp:353] Iteration 54000 (41.5329 iter/s, 2.40773s/100 iter), loss = 0.000687217
I0801 14:19:29.728178 24522 solver.cpp:375]     Train net output #0: loss = 0.000686533 (* 1 = 0.000686533 loss)
I0801 14:19:29.728185 24522 sgd_solver.cpp:136] Iteration 54000, lr = 0.0015625, m = 0.9
I0801 14:19:31.315908 24522 solver.cpp:353] Iteration 54100 (62.975 iter/s, 1.58793s/100 iter), loss = 0.00132573
I0801 14:19:31.315937 24522 solver.cpp:375]     Train net output #0: loss = 0.00132505 (* 1 = 0.00132505 loss)
I0801 14:19:31.315943 24522 sgd_solver.cpp:136] Iteration 54100, lr = 0.00154688, m = 0.9
I0801 14:19:32.904285 24522 solver.cpp:353] Iteration 54200 (62.9594 iter/s, 1.58833s/100 iter), loss = 0.000376965
I0801 14:19:32.904312 24522 solver.cpp:375]     Train net output #0: loss = 0.000376281 (* 1 = 0.000376281 loss)
I0801 14:19:32.904319 24522 sgd_solver.cpp:136] Iteration 54200, lr = 0.00153125, m = 0.9
I0801 14:19:34.495860 24522 solver.cpp:353] Iteration 54300 (62.8328 iter/s, 1.59153s/100 iter), loss = 0.0021527
I0801 14:19:34.495949 24522 solver.cpp:375]     Train net output #0: loss = 0.00215202 (* 1 = 0.00215202 loss)
I0801 14:19:34.495955 24522 sgd_solver.cpp:136] Iteration 54300, lr = 0.00151563, m = 0.9
I0801 14:19:36.139536 24522 solver.cpp:353] Iteration 54400 (60.8415 iter/s, 1.64362s/100 iter), loss = 0.000795495
I0801 14:19:36.139576 24522 solver.cpp:375]     Train net output #0: loss = 0.00079481 (* 1 = 0.00079481 loss)
I0801 14:19:36.139586 24522 sgd_solver.cpp:136] Iteration 54400, lr = 0.0015, m = 0.9
I0801 14:19:37.780740 24522 solver.cpp:353] Iteration 54500 (60.9326 iter/s, 1.64116s/100 iter), loss = 0.00175581
I0801 14:19:37.780764 24522 solver.cpp:375]     Train net output #0: loss = 0.00175512 (* 1 = 0.00175512 loss)
I0801 14:19:37.780768 24522 sgd_solver.cpp:136] Iteration 54500, lr = 0.00148437, m = 0.9
I0801 14:19:39.435153 24522 solver.cpp:353] Iteration 54600 (60.4464 iter/s, 1.65436s/100 iter), loss = 0.000470823
I0801 14:19:39.435240 24522 solver.cpp:375]     Train net output #0: loss = 0.000470138 (* 1 = 0.000470138 loss)
I0801 14:19:39.435246 24522 sgd_solver.cpp:136] Iteration 54600, lr = 0.00146875, m = 0.9
I0801 14:19:41.012408 24522 solver.cpp:353] Iteration 54700 (63.4033 iter/s, 1.57721s/100 iter), loss = 0.00411865
I0801 14:19:41.012434 24522 solver.cpp:375]     Train net output #0: loss = 0.00411797 (* 1 = 0.00411797 loss)
I0801 14:19:41.012440 24522 sgd_solver.cpp:136] Iteration 54700, lr = 0.00145312, m = 0.9
I0801 14:19:42.579473 24522 solver.cpp:353] Iteration 54800 (63.8156 iter/s, 1.56702s/100 iter), loss = 0.00133246
I0801 14:19:42.579526 24522 solver.cpp:375]     Train net output #0: loss = 0.00133178 (* 1 = 0.00133178 loss)
I0801 14:19:42.579542 24522 sgd_solver.cpp:136] Iteration 54800, lr = 0.0014375, m = 0.9
I0801 14:19:44.172754 24522 solver.cpp:353] Iteration 54900 (62.7657 iter/s, 1.59323s/100 iter), loss = 0.00110558
I0801 14:19:44.172785 24522 solver.cpp:375]     Train net output #0: loss = 0.00110489 (* 1 = 0.00110489 loss)
I0801 14:19:44.172791 24522 sgd_solver.cpp:136] Iteration 54900, lr = 0.00142187, m = 0.9
I0801 14:19:45.748021 24522 solver.cpp:404] Sparsity after update:
I0801 14:19:45.749553 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:19:45.749563 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:19:45.749572 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:19:45.749577 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:19:45.749580 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:19:45.749584 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:19:45.749588 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:19:45.749593 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:19:45.749598 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:19:45.749601 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:19:45.749605 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:19:45.749609 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:19:45.749613 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:19:45.749624 24522 solver.cpp:550] Iteration 55000, Testing net (#0)
I0801 14:19:46.586872 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.917942
I0801 14:19:46.586891 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 14:19:46.586899 24522 solver.cpp:635]     Test net output #2: loss = 0.347735 (* 1 = 0.347735 loss)
I0801 14:19:46.586915 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.837263s
I0801 14:19:46.602589 24522 solver.cpp:353] Iteration 55000 (41.1562 iter/s, 2.42977s/100 iter), loss = 0.00118939
I0801 14:19:46.602607 24522 solver.cpp:375]     Train net output #0: loss = 0.0011887 (* 1 = 0.0011887 loss)
I0801 14:19:46.602612 24522 sgd_solver.cpp:136] Iteration 55000, lr = 0.00140625, m = 0.9
I0801 14:19:48.258368 24522 solver.cpp:353] Iteration 55100 (60.3966 iter/s, 1.65572s/100 iter), loss = 0.00404965
I0801 14:19:48.258394 24522 solver.cpp:375]     Train net output #0: loss = 0.00404897 (* 1 = 0.00404897 loss)
I0801 14:19:48.258400 24522 sgd_solver.cpp:136] Iteration 55100, lr = 0.00139063, m = 0.9
I0801 14:19:49.955258 24522 solver.cpp:353] Iteration 55200 (58.9332 iter/s, 1.69684s/100 iter), loss = 0.00161667
I0801 14:19:49.955284 24522 solver.cpp:375]     Train net output #0: loss = 0.00161598 (* 1 = 0.00161598 loss)
I0801 14:19:49.955289 24522 sgd_solver.cpp:136] Iteration 55200, lr = 0.001375, m = 0.9
I0801 14:19:51.655750 24522 solver.cpp:353] Iteration 55300 (58.8083 iter/s, 1.70044s/100 iter), loss = 0.000898675
I0801 14:19:51.655773 24522 solver.cpp:375]     Train net output #0: loss = 0.000897992 (* 1 = 0.000897992 loss)
I0801 14:19:51.655778 24522 sgd_solver.cpp:136] Iteration 55300, lr = 0.00135938, m = 0.9
I0801 14:19:53.337860 24522 solver.cpp:353] Iteration 55400 (59.4512 iter/s, 1.68205s/100 iter), loss = 0.000742849
I0801 14:19:53.337906 24522 solver.cpp:375]     Train net output #0: loss = 0.000742167 (* 1 = 0.000742167 loss)
I0801 14:19:53.337945 24522 sgd_solver.cpp:136] Iteration 55400, lr = 0.00134375, m = 0.9
I0801 14:19:55.000679 24522 solver.cpp:353] Iteration 55500 (60.1408 iter/s, 1.66276s/100 iter), loss = 0.000307954
I0801 14:19:55.000792 24522 solver.cpp:375]     Train net output #0: loss = 0.000307272 (* 1 = 0.000307272 loss)
I0801 14:19:55.000830 24522 sgd_solver.cpp:136] Iteration 55500, lr = 0.00132813, m = 0.9
I0801 14:19:56.675070 24522 solver.cpp:353] Iteration 55600 (59.725 iter/s, 1.67434s/100 iter), loss = 0.000522966
I0801 14:19:56.675096 24522 solver.cpp:375]     Train net output #0: loss = 0.000522284 (* 1 = 0.000522284 loss)
I0801 14:19:56.675104 24522 sgd_solver.cpp:136] Iteration 55600, lr = 0.0013125, m = 0.9
I0801 14:19:58.372941 24522 solver.cpp:353] Iteration 55700 (58.8992 iter/s, 1.69782s/100 iter), loss = 0.00158203
I0801 14:19:58.372966 24522 solver.cpp:375]     Train net output #0: loss = 0.00158135 (* 1 = 0.00158135 loss)
I0801 14:19:58.372969 24522 sgd_solver.cpp:136] Iteration 55700, lr = 0.00129687, m = 0.9
I0801 14:20:00.094506 24522 solver.cpp:353] Iteration 55800 (58.0885 iter/s, 1.72151s/100 iter), loss = 0.000438428
I0801 14:20:00.094533 24522 solver.cpp:375]     Train net output #0: loss = 0.000437746 (* 1 = 0.000437746 loss)
I0801 14:20:00.094538 24522 sgd_solver.cpp:136] Iteration 55800, lr = 0.00128125, m = 0.9
I0801 14:20:01.788017 24522 solver.cpp:353] Iteration 55900 (59.0508 iter/s, 1.69346s/100 iter), loss = 0.00079541
I0801 14:20:01.788043 24522 solver.cpp:375]     Train net output #0: loss = 0.000794729 (* 1 = 0.000794729 loss)
I0801 14:20:01.788049 24522 sgd_solver.cpp:136] Iteration 55900, lr = 0.00126562, m = 0.9
I0801 14:20:03.419282 24522 solver.cpp:404] Sparsity after update:
I0801 14:20:03.420864 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:20:03.420873 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:20:03.420881 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:20:03.420882 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:20:03.420884 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:20:03.420886 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:20:03.420888 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:20:03.420891 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:20:03.420892 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:20:03.420895 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:20:03.420897 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:20:03.420900 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:20:03.420902 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:20:03.420909 24522 solver.cpp:550] Iteration 56000, Testing net (#0)
I0801 14:20:04.322536 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.917354
I0801 14:20:04.322571 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 14:20:04.322584 24522 solver.cpp:635]     Test net output #2: loss = 0.359344 (* 1 = 0.359344 loss)
I0801 14:20:04.322623 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.901678s
I0801 14:20:04.341557 24522 solver.cpp:353] Iteration 56000 (39.1625 iter/s, 2.55346s/100 iter), loss = 0.000330347
I0801 14:20:04.341601 24522 solver.cpp:375]     Train net output #0: loss = 0.000329666 (* 1 = 0.000329666 loss)
I0801 14:20:04.341615 24522 sgd_solver.cpp:136] Iteration 56000, lr = 0.00125, m = 0.9
I0801 14:20:05.322149 24487 data_reader.cpp:264] Starting prefetch of epoch 7
I0801 14:20:06.064107 24522 solver.cpp:353] Iteration 56100 (58.0553 iter/s, 1.72249s/100 iter), loss = 0.00223643
I0801 14:20:06.064133 24522 solver.cpp:375]     Train net output #0: loss = 0.00223575 (* 1 = 0.00223575 loss)
I0801 14:20:06.064137 24522 sgd_solver.cpp:136] Iteration 56100, lr = 0.00123438, m = 0.9
I0801 14:20:07.719738 24522 solver.cpp:353] Iteration 56200 (60.4018 iter/s, 1.65558s/100 iter), loss = 0.000612436
I0801 14:20:07.719804 24522 solver.cpp:375]     Train net output #0: loss = 0.000611753 (* 1 = 0.000611753 loss)
I0801 14:20:07.719833 24522 sgd_solver.cpp:136] Iteration 56200, lr = 0.00121875, m = 0.9
I0801 14:20:09.345963 24522 solver.cpp:353] Iteration 56300 (61.494 iter/s, 1.62617s/100 iter), loss = 0.000548362
I0801 14:20:09.346012 24522 solver.cpp:375]     Train net output #0: loss = 0.000547679 (* 1 = 0.000547679 loss)
I0801 14:20:09.346025 24522 sgd_solver.cpp:136] Iteration 56300, lr = 0.00120313, m = 0.9
I0801 14:20:10.947382 24522 solver.cpp:353] Iteration 56400 (62.4468 iter/s, 1.60136s/100 iter), loss = 0.00118859
I0801 14:20:10.947531 24522 solver.cpp:375]     Train net output #0: loss = 0.00118791 (* 1 = 0.00118791 loss)
I0801 14:20:10.947549 24522 sgd_solver.cpp:136] Iteration 56400, lr = 0.0011875, m = 0.9
I0801 14:20:12.621181 24522 solver.cpp:353] Iteration 56500 (59.7462 iter/s, 1.67375s/100 iter), loss = 0.00249184
I0801 14:20:12.621206 24522 solver.cpp:375]     Train net output #0: loss = 0.00249115 (* 1 = 0.00249115 loss)
I0801 14:20:12.621210 24522 sgd_solver.cpp:136] Iteration 56500, lr = 0.00117187, m = 0.9
I0801 14:20:14.317194 24522 solver.cpp:353] Iteration 56600 (58.9636 iter/s, 1.69596s/100 iter), loss = 0.00155856
I0801 14:20:14.317246 24522 solver.cpp:375]     Train net output #0: loss = 0.00155788 (* 1 = 0.00155788 loss)
I0801 14:20:14.317260 24522 sgd_solver.cpp:136] Iteration 56600, lr = 0.00115625, m = 0.9
I0801 14:20:15.986099 24522 solver.cpp:353] Iteration 56700 (59.9215 iter/s, 1.66885s/100 iter), loss = 0.000490599
I0801 14:20:15.986122 24522 solver.cpp:375]     Train net output #0: loss = 0.000489917 (* 1 = 0.000489917 loss)
I0801 14:20:15.986129 24522 sgd_solver.cpp:136] Iteration 56700, lr = 0.00114062, m = 0.9
I0801 14:20:17.740176 24522 solver.cpp:353] Iteration 56800 (57.012 iter/s, 1.75402s/100 iter), loss = 0.000975586
I0801 14:20:17.740217 24522 solver.cpp:375]     Train net output #0: loss = 0.000974904 (* 1 = 0.000974904 loss)
I0801 14:20:17.740228 24522 sgd_solver.cpp:136] Iteration 56800, lr = 0.001125, m = 0.9
I0801 14:20:19.365447 24522 solver.cpp:353] Iteration 56900 (61.53 iter/s, 1.62522s/100 iter), loss = 0.000540173
I0801 14:20:19.365469 24522 solver.cpp:375]     Train net output #0: loss = 0.000539492 (* 1 = 0.000539492 loss)
I0801 14:20:19.365473 24522 sgd_solver.cpp:136] Iteration 56900, lr = 0.00110937, m = 0.9
I0801 14:20:20.964462 24522 solver.cpp:404] Sparsity after update:
I0801 14:20:20.966892 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:20:20.966908 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:20:20.966928 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:20:20.966936 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:20:20.966945 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:20:20.966953 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:20:20.966962 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:20:20.966969 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:20:20.966977 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:20:20.966985 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:20:20.966995 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:20:20.967005 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:20:20.967015 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:20:20.967042 24522 solver.cpp:550] Iteration 57000, Testing net (#0)
I0801 14:20:21.858446 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.909707
I0801 14:20:21.858465 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 14:20:21.858472 24522 solver.cpp:635]     Test net output #2: loss = 0.382904 (* 1 = 0.382904 loss)
I0801 14:20:21.858494 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.891422s
I0801 14:20:21.877668 24522 solver.cpp:353] Iteration 57000 (39.8066 iter/s, 2.51215s/100 iter), loss = 0.00109887
I0801 14:20:21.877694 24522 solver.cpp:375]     Train net output #0: loss = 0.00109819 (* 1 = 0.00109819 loss)
I0801 14:20:21.877699 24522 sgd_solver.cpp:136] Iteration 57000, lr = 0.00109375, m = 0.9
I0801 14:20:23.558136 24522 solver.cpp:353] Iteration 57100 (59.5093 iter/s, 1.68041s/100 iter), loss = 0.000436448
I0801 14:20:23.558209 24522 solver.cpp:375]     Train net output #0: loss = 0.000435767 (* 1 = 0.000435767 loss)
I0801 14:20:23.558226 24522 sgd_solver.cpp:136] Iteration 57100, lr = 0.00107813, m = 0.9
I0801 14:20:25.293696 24522 solver.cpp:353] Iteration 57200 (57.6202 iter/s, 1.7355s/100 iter), loss = 0.00138693
I0801 14:20:25.293834 24522 solver.cpp:375]     Train net output #0: loss = 0.00138625 (* 1 = 0.00138625 loss)
I0801 14:20:25.295370 24522 sgd_solver.cpp:136] Iteration 57200, lr = 0.0010625, m = 0.9
I0801 14:20:26.901485 24522 solver.cpp:353] Iteration 57300 (62.1994 iter/s, 1.60773s/100 iter), loss = 0.000443452
I0801 14:20:26.901685 24522 solver.cpp:375]     Train net output #0: loss = 0.00044277 (* 1 = 0.00044277 loss)
I0801 14:20:26.901692 24522 sgd_solver.cpp:136] Iteration 57300, lr = 0.00104688, m = 0.9
I0801 14:20:28.579989 24522 solver.cpp:353] Iteration 57400 (59.5785 iter/s, 1.67846s/100 iter), loss = 0.00109717
I0801 14:20:28.580036 24522 solver.cpp:375]     Train net output #0: loss = 0.00109649 (* 1 = 0.00109649 loss)
I0801 14:20:28.580049 24522 sgd_solver.cpp:136] Iteration 57400, lr = 0.00103125, m = 0.9
I0801 14:20:30.251385 24522 solver.cpp:353] Iteration 57500 (59.8322 iter/s, 1.67134s/100 iter), loss = 0.000879905
I0801 14:20:30.251412 24522 solver.cpp:375]     Train net output #0: loss = 0.000879221 (* 1 = 0.000879221 loss)
I0801 14:20:30.251416 24522 sgd_solver.cpp:136] Iteration 57500, lr = 0.00101562, m = 0.9
I0801 14:20:32.012910 24522 solver.cpp:353] Iteration 57600 (56.771 iter/s, 1.76146s/100 iter), loss = 0.000840449
I0801 14:20:32.013165 24522 solver.cpp:375]     Train net output #0: loss = 0.000839764 (* 1 = 0.000839764 loss)
I0801 14:20:32.013281 24522 sgd_solver.cpp:136] Iteration 57600, lr = 0.001, m = 0.9
I0801 14:20:33.610136 24522 solver.cpp:353] Iteration 57700 (62.6106 iter/s, 1.59717s/100 iter), loss = 0.000397246
I0801 14:20:33.610167 24522 solver.cpp:375]     Train net output #0: loss = 0.000396562 (* 1 = 0.000396562 loss)
I0801 14:20:33.610174 24522 sgd_solver.cpp:136] Iteration 57700, lr = 0.000984375, m = 0.9
I0801 14:20:35.386374 24522 solver.cpp:353] Iteration 57800 (56.3005 iter/s, 1.77618s/100 iter), loss = 0.00046873
I0801 14:20:35.386401 24522 solver.cpp:375]     Train net output #0: loss = 0.000468048 (* 1 = 0.000468048 loss)
I0801 14:20:35.386406 24522 sgd_solver.cpp:136] Iteration 57800, lr = 0.00096875, m = 0.9
I0801 14:20:37.005722 24522 solver.cpp:353] Iteration 57900 (61.7552 iter/s, 1.6193s/100 iter), loss = 0.00146076
I0801 14:20:37.005745 24522 solver.cpp:375]     Train net output #0: loss = 0.00146008 (* 1 = 0.00146008 loss)
I0801 14:20:37.005751 24522 sgd_solver.cpp:136] Iteration 57900, lr = 0.000953125, m = 0.9
I0801 14:20:38.671591 24522 solver.cpp:404] Sparsity after update:
I0801 14:20:38.673188 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:20:38.673195 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:20:38.673203 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:20:38.673207 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:20:38.673209 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:20:38.673211 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:20:38.673214 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:20:38.673216 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:20:38.673218 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:20:38.673220 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:20:38.673223 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:20:38.673225 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:20:38.673228 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:20:38.673235 24522 solver.cpp:550] Iteration 58000, Testing net (#0)
I0801 14:20:39.501029 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.897648
I0801 14:20:39.501047 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 14:20:39.501051 24522 solver.cpp:635]     Test net output #2: loss = 0.41192 (* 1 = 0.41192 loss)
I0801 14:20:39.501067 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.827804s
I0801 14:20:39.516733 24522 solver.cpp:353] Iteration 58000 (39.8258 iter/s, 2.51094s/100 iter), loss = 0.00284762
I0801 14:20:39.516751 24522 solver.cpp:375]     Train net output #0: loss = 0.00284694 (* 1 = 0.00284694 loss)
I0801 14:20:39.516754 24522 sgd_solver.cpp:136] Iteration 58000, lr = 0.0009375, m = 0.9
I0801 14:20:41.090122 24522 solver.cpp:353] Iteration 58100 (63.5591 iter/s, 1.57334s/100 iter), loss = 0.00377831
I0801 14:20:41.090188 24522 solver.cpp:375]     Train net output #0: loss = 0.00377763 (* 1 = 0.00377763 loss)
I0801 14:20:41.090195 24522 sgd_solver.cpp:136] Iteration 58100, lr = 0.000921875, m = 0.9
I0801 14:20:42.680183 24522 solver.cpp:353] Iteration 58200 (62.8927 iter/s, 1.59001s/100 iter), loss = 0.000217579
I0801 14:20:42.680208 24522 solver.cpp:375]     Train net output #0: loss = 0.000216897 (* 1 = 0.000216897 loss)
I0801 14:20:42.680213 24522 sgd_solver.cpp:136] Iteration 58200, lr = 0.00090625, m = 0.9
I0801 14:20:44.252410 24522 solver.cpp:353] Iteration 58300 (63.6062 iter/s, 1.57217s/100 iter), loss = 0.000764377
I0801 14:20:44.252463 24522 solver.cpp:375]     Train net output #0: loss = 0.000763695 (* 1 = 0.000763695 loss)
I0801 14:20:44.252477 24522 sgd_solver.cpp:136] Iteration 58300, lr = 0.000890625, m = 0.9
I0801 14:20:45.945662 24522 solver.cpp:353] Iteration 58400 (59.0597 iter/s, 1.6932s/100 iter), loss = 0.000379177
I0801 14:20:45.945690 24522 solver.cpp:375]     Train net output #0: loss = 0.000378496 (* 1 = 0.000378496 loss)
I0801 14:20:45.945695 24522 sgd_solver.cpp:136] Iteration 58400, lr = 0.000875, m = 0.9
I0801 14:20:47.577828 24522 solver.cpp:353] Iteration 58500 (61.2703 iter/s, 1.63211s/100 iter), loss = 0.00354146
I0801 14:20:47.577859 24522 solver.cpp:375]     Train net output #0: loss = 0.00354077 (* 1 = 0.00354077 loss)
I0801 14:20:47.577865 24522 sgd_solver.cpp:136] Iteration 58500, lr = 0.000859375, m = 0.9
I0801 14:20:49.236249 24522 solver.cpp:353] Iteration 58600 (60.3003 iter/s, 1.65837s/100 iter), loss = 0.000801043
I0801 14:20:49.236280 24522 solver.cpp:375]     Train net output #0: loss = 0.000800363 (* 1 = 0.000800363 loss)
I0801 14:20:49.236284 24522 sgd_solver.cpp:136] Iteration 58600, lr = 0.00084375, m = 0.9
I0801 14:20:50.819182 24522 solver.cpp:353] Iteration 58700 (63.1758 iter/s, 1.58288s/100 iter), loss = 0.000514017
I0801 14:20:50.819232 24522 solver.cpp:375]     Train net output #0: loss = 0.000513336 (* 1 = 0.000513336 loss)
I0801 14:20:50.819244 24522 sgd_solver.cpp:136] Iteration 58700, lr = 0.000828125, m = 0.9
I0801 14:20:52.404875 24522 solver.cpp:353] Iteration 58800 (63.0659 iter/s, 1.58564s/100 iter), loss = 0.00201116
I0801 14:20:52.404913 24522 solver.cpp:375]     Train net output #0: loss = 0.00201048 (* 1 = 0.00201048 loss)
I0801 14:20:52.404924 24522 sgd_solver.cpp:136] Iteration 58800, lr = 0.0008125, m = 0.9
I0801 14:20:53.983258 24522 solver.cpp:353] Iteration 58900 (63.3581 iter/s, 1.57833s/100 iter), loss = 0.000969029
I0801 14:20:53.983283 24522 solver.cpp:375]     Train net output #0: loss = 0.000968348 (* 1 = 0.000968348 loss)
I0801 14:20:53.983286 24522 sgd_solver.cpp:136] Iteration 58900, lr = 0.000796875, m = 0.9
I0801 14:20:55.605139 24522 solver.cpp:404] Sparsity after update:
I0801 14:20:55.606736 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:20:55.606745 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:20:55.606751 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:20:55.606755 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:20:55.606756 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:20:55.606760 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:20:55.606761 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:20:55.606763 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:20:55.606765 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:20:55.606767 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:20:55.606770 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:20:55.606772 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:20:55.606775 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:20:55.606782 24522 solver.cpp:550] Iteration 59000, Testing net (#0)
I0801 14:20:56.419036 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.895883
I0801 14:20:56.419055 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 14:20:56.419060 24522 solver.cpp:635]     Test net output #2: loss = 0.410575 (* 1 = 0.410575 loss)
I0801 14:20:56.419090 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.812279s
I0801 14:20:56.434784 24522 solver.cpp:353] Iteration 59000 (40.7921 iter/s, 2.45146s/100 iter), loss = 0.000709541
I0801 14:20:56.434801 24522 solver.cpp:375]     Train net output #0: loss = 0.000708861 (* 1 = 0.000708861 loss)
I0801 14:20:56.434808 24522 sgd_solver.cpp:136] Iteration 59000, lr = 0.00078125, m = 0.9
I0801 14:20:58.121834 24522 solver.cpp:353] Iteration 59100 (59.2771 iter/s, 1.68699s/100 iter), loss = 0.000790055
I0801 14:20:58.121858 24522 solver.cpp:375]     Train net output #0: loss = 0.000789374 (* 1 = 0.000789374 loss)
I0801 14:20:58.121863 24522 sgd_solver.cpp:136] Iteration 59100, lr = 0.000765625, m = 0.9
I0801 14:20:59.785359 24522 solver.cpp:353] Iteration 59200 (60.1152 iter/s, 1.66347s/100 iter), loss = 0.00106546
I0801 14:20:59.785383 24522 solver.cpp:375]     Train net output #0: loss = 0.00106478 (* 1 = 0.00106478 loss)
I0801 14:20:59.785389 24522 sgd_solver.cpp:136] Iteration 59200, lr = 0.00075, m = 0.9
I0801 14:21:01.467123 24522 solver.cpp:353] Iteration 59300 (59.4633 iter/s, 1.68171s/100 iter), loss = 0.00101248
I0801 14:21:01.467147 24522 solver.cpp:375]     Train net output #0: loss = 0.0010118 (* 1 = 0.0010118 loss)
I0801 14:21:01.467152 24522 sgd_solver.cpp:136] Iteration 59300, lr = 0.000734375, m = 0.9
I0801 14:21:03.068954 24522 solver.cpp:353] Iteration 59400 (62.4304 iter/s, 1.60178s/100 iter), loss = 0.00226807
I0801 14:21:03.068979 24522 solver.cpp:375]     Train net output #0: loss = 0.00226739 (* 1 = 0.00226739 loss)
I0801 14:21:03.068984 24522 sgd_solver.cpp:136] Iteration 59400, lr = 0.00071875, m = 0.9
I0801 14:21:04.765573 24522 solver.cpp:353] Iteration 59500 (58.9427 iter/s, 1.69656s/100 iter), loss = 0.000377451
I0801 14:21:04.765630 24522 solver.cpp:375]     Train net output #0: loss = 0.000376768 (* 1 = 0.000376768 loss)
I0801 14:21:04.765645 24522 sgd_solver.cpp:136] Iteration 59500, lr = 0.000703125, m = 0.9
I0801 14:21:06.430711 24522 solver.cpp:353] Iteration 59600 (60.057 iter/s, 1.66508s/100 iter), loss = 0.00220853
I0801 14:21:06.430733 24522 solver.cpp:375]     Train net output #0: loss = 0.00220785 (* 1 = 0.00220785 loss)
I0801 14:21:06.430737 24522 sgd_solver.cpp:136] Iteration 59600, lr = 0.0006875, m = 0.9
I0801 14:21:08.018141 24522 solver.cpp:353] Iteration 59700 (62.997 iter/s, 1.58738s/100 iter), loss = 0.00176195
I0801 14:21:08.018220 24522 solver.cpp:375]     Train net output #0: loss = 0.00176127 (* 1 = 0.00176127 loss)
I0801 14:21:08.018241 24522 sgd_solver.cpp:136] Iteration 59700, lr = 0.000671875, m = 0.9
I0801 14:21:09.731151 24522 solver.cpp:353] Iteration 59800 (58.3785 iter/s, 1.71296s/100 iter), loss = 0.000802004
I0801 14:21:09.731176 24522 solver.cpp:375]     Train net output #0: loss = 0.000801322 (* 1 = 0.000801322 loss)
I0801 14:21:09.731182 24522 sgd_solver.cpp:136] Iteration 59800, lr = 0.00065625, m = 0.9
I0801 14:21:11.339314 24522 solver.cpp:353] Iteration 59900 (62.1849 iter/s, 1.60811s/100 iter), loss = 0.000409011
I0801 14:21:11.339417 24522 solver.cpp:375]     Train net output #0: loss = 0.000408328 (* 1 = 0.000408328 loss)
I0801 14:21:11.339424 24522 sgd_solver.cpp:136] Iteration 59900, lr = 0.000640625, m = 0.9
I0801 14:21:12.972980 24522 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_60000.caffemodel
I0801 14:21:12.981503 24522 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_60000.solverstate
I0801 14:21:12.985080 24522 solver.cpp:404] Sparsity after update:
I0801 14:21:12.986915 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:21:12.986924 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:21:12.986932 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:21:12.986933 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:21:12.986937 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:21:12.986940 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:21:12.986943 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:21:12.986944 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:21:12.986946 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:21:12.986948 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:21:12.986949 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:21:12.986951 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:21:12.986953 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:21:12.986963 24522 solver.cpp:550] Iteration 60000, Testing net (#0)
I0801 14:21:13.790906 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.892942
I0801 14:21:13.790925 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 14:21:13.790930 24522 solver.cpp:635]     Test net output #2: loss = 0.442116 (* 1 = 0.442116 loss)
I0801 14:21:13.790946 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.803956s
I0801 14:21:13.806895 24522 solver.cpp:353] Iteration 60000 (40.5267 iter/s, 2.46751s/100 iter), loss = 0.000898302
I0801 14:21:13.806912 24522 solver.cpp:375]     Train net output #0: loss = 0.00089762 (* 1 = 0.00089762 loss)
I0801 14:21:13.806916 24522 sgd_solver.cpp:136] Iteration 60000, lr = 0.000625, m = 0.9
I0801 14:21:15.464609 24522 solver.cpp:353] Iteration 60100 (60.3261 iter/s, 1.65766s/100 iter), loss = 0.000589767
I0801 14:21:15.464669 24522 solver.cpp:375]     Train net output #0: loss = 0.000589085 (* 1 = 0.000589085 loss)
I0801 14:21:15.464682 24522 sgd_solver.cpp:136] Iteration 60100, lr = 0.000609375, m = 0.9
I0801 14:21:17.127920 24522 solver.cpp:353] Iteration 60200 (60.1229 iter/s, 1.66326s/100 iter), loss = 0.0017615
I0801 14:21:17.127944 24522 solver.cpp:375]     Train net output #0: loss = 0.00176082 (* 1 = 0.00176082 loss)
I0801 14:21:17.127948 24522 sgd_solver.cpp:136] Iteration 60200, lr = 0.00059375, m = 0.9
I0801 14:21:18.725980 24522 solver.cpp:353] Iteration 60300 (62.5781 iter/s, 1.598s/100 iter), loss = 0.000648605
I0801 14:21:18.726161 24522 solver.cpp:375]     Train net output #0: loss = 0.00064792 (* 1 = 0.00064792 loss)
I0801 14:21:18.726184 24522 sgd_solver.cpp:136] Iteration 60300, lr = 0.000578125, m = 0.9
I0801 14:21:20.466401 24522 solver.cpp:353] Iteration 60400 (57.4593 iter/s, 1.74036s/100 iter), loss = 0.000894886
I0801 14:21:20.466457 24522 solver.cpp:375]     Train net output #0: loss = 0.000894202 (* 1 = 0.000894202 loss)
I0801 14:21:20.466472 24522 sgd_solver.cpp:136] Iteration 60400, lr = 0.0005625, m = 0.9
I0801 14:21:22.125368 24522 solver.cpp:353] Iteration 60500 (60.2806 iter/s, 1.65891s/100 iter), loss = 0.00153954
I0801 14:21:22.125485 24522 solver.cpp:375]     Train net output #0: loss = 0.00153885 (* 1 = 0.00153885 loss)
I0801 14:21:22.125517 24522 sgd_solver.cpp:136] Iteration 60500, lr = 0.000546875, m = 0.9
I0801 14:21:23.722739 24522 solver.cpp:353] Iteration 60600 (62.6047 iter/s, 1.59733s/100 iter), loss = 0.00140742
I0801 14:21:23.722801 24522 solver.cpp:375]     Train net output #0: loss = 0.00140673 (* 1 = 0.00140673 loss)
I0801 14:21:23.722816 24522 sgd_solver.cpp:136] Iteration 60600, lr = 0.00053125, m = 0.9
I0801 14:21:23.769779 24487 data_reader.cpp:264] Starting prefetch of epoch 8
I0801 14:21:25.420588 24522 solver.cpp:353] Iteration 60700 (58.9001 iter/s, 1.69779s/100 iter), loss = 0.00115334
I0801 14:21:25.420677 24522 solver.cpp:375]     Train net output #0: loss = 0.00115265 (* 1 = 0.00115265 loss)
I0801 14:21:25.420704 24522 sgd_solver.cpp:136] Iteration 60700, lr = 0.000515625, m = 0.9
I0801 14:21:27.021435 24522 solver.cpp:353] Iteration 60800 (62.4689 iter/s, 1.6008s/100 iter), loss = 0.00300468
I0801 14:21:27.021526 24522 solver.cpp:375]     Train net output #0: loss = 0.003004 (* 1 = 0.003004 loss)
I0801 14:21:27.021546 24522 sgd_solver.cpp:136] Iteration 60800, lr = 0.0005, m = 0.9
I0801 14:21:28.692592 24522 solver.cpp:353] Iteration 60900 (59.8406 iter/s, 1.67111s/100 iter), loss = 0.000379021
I0801 14:21:28.692615 24522 solver.cpp:375]     Train net output #0: loss = 0.000378337 (* 1 = 0.000378337 loss)
I0801 14:21:28.692621 24522 sgd_solver.cpp:136] Iteration 60900, lr = 0.000484375, m = 0.9
I0801 14:21:30.390024 24522 solver.cpp:404] Sparsity after update:
I0801 14:21:30.391685 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:21:30.391695 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:21:30.391703 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:21:30.391717 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:21:30.391722 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:21:30.391732 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:21:30.391737 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:21:30.391741 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:21:30.391748 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:21:30.391753 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:21:30.391757 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:21:30.391764 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:21:30.391769 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:21:30.391793 24522 solver.cpp:550] Iteration 61000, Testing net (#0)
I0801 14:21:31.211928 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.894119
I0801 14:21:31.211947 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995294
I0801 14:21:31.211953 24522 solver.cpp:635]     Test net output #2: loss = 0.431654 (* 1 = 0.431654 loss)
I0801 14:21:31.211971 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.820149s
I0801 14:21:31.227753 24522 solver.cpp:353] Iteration 61000 (39.4464 iter/s, 2.53509s/100 iter), loss = 0.00122136
I0801 14:21:31.227771 24522 solver.cpp:375]     Train net output #0: loss = 0.00122068 (* 1 = 0.00122068 loss)
I0801 14:21:31.227776 24522 sgd_solver.cpp:136] Iteration 61000, lr = 0.00046875, m = 0.9
I0801 14:21:32.907415 24522 solver.cpp:353] Iteration 61100 (59.5378 iter/s, 1.67961s/100 iter), loss = 0.000702031
I0801 14:21:32.907441 24522 solver.cpp:375]     Train net output #0: loss = 0.000701346 (* 1 = 0.000701346 loss)
I0801 14:21:32.907446 24522 sgd_solver.cpp:136] Iteration 61100, lr = 0.000453125, m = 0.9
I0801 14:21:34.541358 24522 solver.cpp:353] Iteration 61200 (61.2037 iter/s, 1.63389s/100 iter), loss = 0.000449848
I0801 14:21:34.541404 24522 solver.cpp:375]     Train net output #0: loss = 0.000449165 (* 1 = 0.000449165 loss)
I0801 14:21:34.541424 24522 sgd_solver.cpp:136] Iteration 61200, lr = 0.0004375, m = 0.9
I0801 14:21:36.195171 24522 solver.cpp:353] Iteration 61300 (60.4683 iter/s, 1.65376s/100 iter), loss = 0.00162333
I0801 14:21:36.195209 24522 solver.cpp:375]     Train net output #0: loss = 0.00162264 (* 1 = 0.00162264 loss)
I0801 14:21:36.195231 24522 sgd_solver.cpp:136] Iteration 61300, lr = 0.000421875, m = 0.9
I0801 14:21:37.822137 24522 solver.cpp:353] Iteration 61400 (61.4661 iter/s, 1.62691s/100 iter), loss = 0.00137754
I0801 14:21:37.822182 24522 solver.cpp:375]     Train net output #0: loss = 0.00137685 (* 1 = 0.00137685 loss)
I0801 14:21:37.822188 24522 sgd_solver.cpp:136] Iteration 61400, lr = 0.00040625, m = 0.9
I0801 14:21:39.509935 24522 solver.cpp:353] Iteration 61500 (59.2506 iter/s, 1.68775s/100 iter), loss = 0.00248597
I0801 14:21:39.509984 24522 solver.cpp:375]     Train net output #0: loss = 0.00248529 (* 1 = 0.00248529 loss)
I0801 14:21:39.509996 24522 sgd_solver.cpp:136] Iteration 61500, lr = 0.000390625, m = 0.9
I0801 14:21:41.103821 24522 solver.cpp:353] Iteration 61600 (62.7421 iter/s, 1.59383s/100 iter), loss = 0.000967365
I0801 14:21:41.103886 24522 solver.cpp:375]     Train net output #0: loss = 0.00096668 (* 1 = 0.00096668 loss)
I0801 14:21:41.103905 24522 sgd_solver.cpp:136] Iteration 61600, lr = 0.000375, m = 0.9
I0801 14:21:42.763725 24522 solver.cpp:353] Iteration 61700 (60.2462 iter/s, 1.65985s/100 iter), loss = 0.000702511
I0801 14:21:42.763800 24522 solver.cpp:375]     Train net output #0: loss = 0.000701827 (* 1 = 0.000701827 loss)
I0801 14:21:42.763808 24522 sgd_solver.cpp:136] Iteration 61700, lr = 0.000359375, m = 0.9
I0801 14:21:44.493425 24522 solver.cpp:353] Iteration 61800 (57.8153 iter/s, 1.72965s/100 iter), loss = 0.000503246
I0801 14:21:44.493474 24522 solver.cpp:375]     Train net output #0: loss = 0.000502562 (* 1 = 0.000502562 loss)
I0801 14:21:44.493485 24522 sgd_solver.cpp:136] Iteration 61800, lr = 0.00034375, m = 0.9
I0801 14:21:46.206616 24522 solver.cpp:353] Iteration 61900 (58.3726 iter/s, 1.71313s/100 iter), loss = 0.001723
I0801 14:21:46.206647 24522 solver.cpp:375]     Train net output #0: loss = 0.00172231 (* 1 = 0.00172231 loss)
I0801 14:21:46.206655 24522 sgd_solver.cpp:136] Iteration 61900, lr = 0.000328125, m = 0.9
I0801 14:21:47.880311 24522 solver.cpp:404] Sparsity after update:
I0801 14:21:47.882267 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:21:47.882282 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:21:47.882304 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:21:47.882311 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:21:47.882316 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:21:47.882321 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:21:47.882325 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:21:47.882329 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:21:47.882333 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:21:47.882338 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:21:47.882342 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:21:47.882354 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:21:47.882360 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:21:47.882375 24522 solver.cpp:550] Iteration 62000, Testing net (#0)
I0801 14:21:48.691216 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.895295
I0801 14:21:48.691234 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995
I0801 14:21:48.691239 24522 solver.cpp:635]     Test net output #2: loss = 0.44996 (* 1 = 0.44996 loss)
I0801 14:21:48.691254 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.808852s
I0801 14:21:48.710692 24522 solver.cpp:353] Iteration 62000 (39.9361 iter/s, 2.504s/100 iter), loss = 0.000290017
I0801 14:21:48.710713 24522 solver.cpp:375]     Train net output #0: loss = 0.000289332 (* 1 = 0.000289332 loss)
I0801 14:21:48.710719 24522 sgd_solver.cpp:136] Iteration 62000, lr = 0.0003125, m = 0.9
I0801 14:21:50.392333 24522 solver.cpp:353] Iteration 62100 (59.4677 iter/s, 1.68159s/100 iter), loss = 0.00163699
I0801 14:21:50.392361 24522 solver.cpp:375]     Train net output #0: loss = 0.00163631 (* 1 = 0.00163631 loss)
I0801 14:21:50.392366 24522 sgd_solver.cpp:136] Iteration 62100, lr = 0.000296875, m = 0.9
I0801 14:21:51.976951 24522 solver.cpp:353] Iteration 62200 (63.1087 iter/s, 1.58457s/100 iter), loss = 0.000902461
I0801 14:21:51.976979 24522 solver.cpp:375]     Train net output #0: loss = 0.000901777 (* 1 = 0.000901777 loss)
I0801 14:21:51.976985 24522 sgd_solver.cpp:136] Iteration 62200, lr = 0.00028125, m = 0.9
I0801 14:21:53.557356 24522 solver.cpp:353] Iteration 62300 (63.2769 iter/s, 1.58036s/100 iter), loss = 0.00104048
I0801 14:21:53.557382 24522 solver.cpp:375]     Train net output #0: loss = 0.0010398 (* 1 = 0.0010398 loss)
I0801 14:21:53.557387 24522 sgd_solver.cpp:136] Iteration 62300, lr = 0.000265625, m = 0.9
I0801 14:21:55.168427 24522 solver.cpp:353] Iteration 62400 (62.0725 iter/s, 1.61102s/100 iter), loss = 0.000764285
I0801 14:21:55.168454 24522 solver.cpp:375]     Train net output #0: loss = 0.000763599 (* 1 = 0.000763599 loss)
I0801 14:21:55.168462 24522 sgd_solver.cpp:136] Iteration 62400, lr = 0.00025, m = 0.9
I0801 14:21:56.787053 24522 solver.cpp:353] Iteration 62500 (61.7828 iter/s, 1.61857s/100 iter), loss = 0.000269326
I0801 14:21:56.787077 24522 solver.cpp:375]     Train net output #0: loss = 0.000268639 (* 1 = 0.000268639 loss)
I0801 14:21:56.787099 24522 sgd_solver.cpp:136] Iteration 62500, lr = 0.000234375, m = 0.9
I0801 14:21:58.486905 24522 solver.cpp:353] Iteration 62600 (58.8304 iter/s, 1.6998s/100 iter), loss = 0.00164891
I0801 14:21:58.486932 24522 solver.cpp:375]     Train net output #0: loss = 0.00164823 (* 1 = 0.00164823 loss)
I0801 14:21:58.486937 24522 sgd_solver.cpp:136] Iteration 62600, lr = 0.00021875, m = 0.9
I0801 14:22:00.092016 24522 solver.cpp:353] Iteration 62700 (62.303 iter/s, 1.60506s/100 iter), loss = 0.000908071
I0801 14:22:00.092042 24522 solver.cpp:375]     Train net output #0: loss = 0.000907384 (* 1 = 0.000907384 loss)
I0801 14:22:00.092047 24522 sgd_solver.cpp:136] Iteration 62700, lr = 0.000203125, m = 0.9
I0801 14:22:01.663326 24522 solver.cpp:353] Iteration 62800 (63.6431 iter/s, 1.57126s/100 iter), loss = 0.00151747
I0801 14:22:01.663390 24522 solver.cpp:375]     Train net output #0: loss = 0.00151678 (* 1 = 0.00151678 loss)
I0801 14:22:01.663409 24522 sgd_solver.cpp:136] Iteration 62800, lr = 0.0001875, m = 0.9
I0801 14:22:03.249698 24522 solver.cpp:353] Iteration 62900 (63.0389 iter/s, 1.58632s/100 iter), loss = 0.00072543
I0801 14:22:03.249725 24522 solver.cpp:375]     Train net output #0: loss = 0.000724745 (* 1 = 0.000724745 loss)
I0801 14:22:03.249732 24522 sgd_solver.cpp:136] Iteration 62900, lr = 0.000171875, m = 0.9
I0801 14:22:04.825531 24522 solver.cpp:404] Sparsity after update:
I0801 14:22:04.827145 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:22:04.827154 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:22:04.827162 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:22:04.827167 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:22:04.827174 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:22:04.827179 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:22:04.827185 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:22:04.827190 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:22:04.827195 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:22:04.827199 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:22:04.827203 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:22:04.827208 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:22:04.827211 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:22:04.827222 24522 solver.cpp:550] Iteration 63000, Testing net (#0)
I0801 14:22:05.643880 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.900883
I0801 14:22:05.643899 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995
I0801 14:22:05.643906 24522 solver.cpp:635]     Test net output #2: loss = 0.442553 (* 1 = 0.442553 loss)
I0801 14:22:05.643924 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.816674s
I0801 14:22:05.660951 24522 solver.cpp:353] Iteration 63000 (41.4735 iter/s, 2.41118s/100 iter), loss = 0.00178825
I0801 14:22:05.660981 24522 solver.cpp:375]     Train net output #0: loss = 0.00178757 (* 1 = 0.00178757 loss)
I0801 14:22:05.660992 24522 sgd_solver.cpp:136] Iteration 63000, lr = 0.00015625, m = 0.9
I0801 14:22:07.280298 24522 solver.cpp:353] Iteration 63100 (61.7554 iter/s, 1.61929s/100 iter), loss = 0.00159019
I0801 14:22:07.280323 24522 solver.cpp:375]     Train net output #0: loss = 0.00158951 (* 1 = 0.00158951 loss)
I0801 14:22:07.280328 24522 sgd_solver.cpp:136] Iteration 63100, lr = 0.000140625, m = 0.9
I0801 14:22:09.003165 24522 solver.cpp:353] Iteration 63200 (58.0446 iter/s, 1.72281s/100 iter), loss = 0.000952651
I0801 14:22:09.003186 24522 solver.cpp:375]     Train net output #0: loss = 0.000951967 (* 1 = 0.000951967 loss)
I0801 14:22:09.003190 24522 sgd_solver.cpp:136] Iteration 63200, lr = 0.000125, m = 0.9
I0801 14:22:10.647856 24522 solver.cpp:353] Iteration 63300 (60.8036 iter/s, 1.64464s/100 iter), loss = 0.00199844
I0801 14:22:10.647881 24522 solver.cpp:375]     Train net output #0: loss = 0.00199775 (* 1 = 0.00199775 loss)
I0801 14:22:10.647886 24522 sgd_solver.cpp:136] Iteration 63300, lr = 0.000109375, m = 0.9
I0801 14:22:12.386957 24522 solver.cpp:353] Iteration 63400 (57.503 iter/s, 1.73904s/100 iter), loss = 0.000258135
I0801 14:22:12.387025 24522 solver.cpp:375]     Train net output #0: loss = 0.000257451 (* 1 = 0.000257451 loss)
I0801 14:22:12.387042 24522 sgd_solver.cpp:136] Iteration 63400, lr = 9.37498e-05, m = 0.9
I0801 14:22:14.095674 24522 solver.cpp:353] Iteration 63500 (58.5252 iter/s, 1.70866s/100 iter), loss = 0.000511712
I0801 14:22:14.095752 24522 solver.cpp:375]     Train net output #0: loss = 0.000511028 (* 1 = 0.000511028 loss)
I0801 14:22:14.095757 24522 sgd_solver.cpp:136] Iteration 63500, lr = 7.8125e-05, m = 0.9
I0801 14:22:15.673202 24522 solver.cpp:353] Iteration 63600 (63.3924 iter/s, 1.57748s/100 iter), loss = 0.000551771
I0801 14:22:15.673252 24522 solver.cpp:375]     Train net output #0: loss = 0.000551087 (* 1 = 0.000551087 loss)
I0801 14:22:15.673266 24522 sgd_solver.cpp:136] Iteration 63600, lr = 6.25002e-05, m = 0.9
I0801 14:22:17.371356 24522 solver.cpp:353] Iteration 63700 (58.8893 iter/s, 1.6981s/100 iter), loss = 0.000419886
I0801 14:22:17.371381 24522 solver.cpp:375]     Train net output #0: loss = 0.000419201 (* 1 = 0.000419201 loss)
I0801 14:22:17.371386 24522 sgd_solver.cpp:136] Iteration 63700, lr = 4.68749e-05, m = 0.9
I0801 14:22:19.045405 24522 solver.cpp:353] Iteration 63800 (59.7373 iter/s, 1.674s/100 iter), loss = 0.000453459
I0801 14:22:19.045430 24522 solver.cpp:375]     Train net output #0: loss = 0.000452775 (* 1 = 0.000452775 loss)
I0801 14:22:19.045435 24522 sgd_solver.cpp:136] Iteration 63800, lr = 3.12501e-05, m = 0.9
I0801 14:22:20.723067 24522 solver.cpp:353] Iteration 63900 (59.609 iter/s, 1.6776s/100 iter), loss = 0.00280178
I0801 14:22:20.723110 24522 solver.cpp:375]     Train net output #0: loss = 0.00280109 (* 1 = 0.00280109 loss)
I0801 14:22:20.723122 24522 sgd_solver.cpp:136] Iteration 63900, lr = 1.56248e-05, m = 0.9
I0801 14:22:22.356325 24522 solver.cpp:353] Iteration 63999 (60.6175 iter/s, 1.63319s/99 iter), loss = 0.000880025
I0801 14:22:22.356400 24522 solver.cpp:375]     Train net output #0: loss = 0.000879341 (* 1 = 0.000879341 loss)
I0801 14:22:22.356412 24522 solver.cpp:404] Sparsity after update:
I0801 14:22:22.358965 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:22:22.358976 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:22:22.358989 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:22:22.358996 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:22:22.359004 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:22:22.359011 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:22:22.359019 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:22:22.359026 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:22:22.359032 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:22:22.359040 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:22:22.359045 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:22:22.359052 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:22:22.359058 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:22:22.359262 24522 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_64000.caffemodel
I0801 14:22:22.369220 24522 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_64000.solverstate
I0801 14:22:22.378031 24522 solver.cpp:527] Iteration 64000, loss = 0.000240207
I0801 14:22:22.378051 24522 solver.cpp:550] Iteration 64000, Testing net (#0)
I0801 14:22:23.196447 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.902648
I0801 14:22:23.196468 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.994706
I0801 14:22:23.196475 24522 solver.cpp:635]     Test net output #2: loss = 0.42992 (* 1 = 0.42992 loss)
I0801 14:22:23.199661 24466 parallel.cpp:73] Root Solver performance on device 0: 30.57 * 22 = 672.5 img/sec (64000 itr in 2094 sec)
I0801 14:22:23.199681 24466 parallel.cpp:78]      Solver performance on device 1: 30.57 * 22 = 672.5 img/sec (64000 itr in 2094 sec)
I0801 14:22:23.199687 24466 parallel.cpp:78]      Solver performance on device 2: 30.57 * 22 = 672.5 img/sec (64000 itr in 2094 sec)
I0801 14:22:23.199688 24466 parallel.cpp:81] Overall multi-GPU performance: 2017.42 img/sec
I0801 14:22:23.302027 24466 caffe.cpp:247] Optimization Done in 34m 57s
I0801 14:22:24.197372  1577 caffe.cpp:608] This is NVCaffe 0.16.3 started at Tue Aug  1 14:22:23 2017
I0801 14:22:24.197500  1577 caffe.cpp:611] CuDNN version: 6021
I0801 14:22:24.197504  1577 caffe.cpp:612] CuBLAS version: 8000
I0801 14:22:24.197506  1577 caffe.cpp:613] CUDA version: 8000
I0801 14:22:24.197509  1577 caffe.cpp:614] CUDA driver version: 8000
I0801 14:22:24.197515  1577 caffe.cpp:263] Not using GPU #2 for single-GPU function
I0801 14:22:24.197516  1577 caffe.cpp:263] Not using GPU #1 for single-GPU function
I0801 14:22:24.198070  1577 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0801 14:22:24.198626  1577 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0801 14:22:24.198632  1577 caffe.cpp:275] Use GPU with device ID 0
I0801 14:22:24.198954  1577 caffe.cpp:279] GPU device name: GeForce GTX 1080
I0801 14:22:24.200268  1577 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0801 14:22:24.200384  1577 net.cpp:104] Using FLOAT as default forward math type
I0801 14:22:24.200389  1577 net.cpp:110] Using FLOAT as default backward math type
I0801 14:22:24.200392  1577 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0801 14:22:24.200397  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.200438  1577 net.cpp:184] Created Layer data (0)
I0801 14:22:24.200443  1577 net.cpp:530] data -> data
I0801 14:22:24.200451  1577 net.cpp:530] data -> label
I0801 14:22:24.200469  1577 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 50
I0801 14:22:24.200762  1577 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 14:22:24.208415  1605 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0801 14:22:24.209131  1577 data_layer.cpp:184] (0) ReshapePrefetch 50, 3, 32, 32
I0801 14:22:24.209167  1577 data_layer.cpp:208] (0) Output data size: 50, 3, 32, 32
I0801 14:22:24.209172  1577 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 14:22:24.209197  1577 net.cpp:245] Setting up data
I0801 14:22:24.209208  1577 net.cpp:252] TEST Top shape for layer 0 'data' 50 3 32 32 (153600)
I0801 14:22:24.209219  1577 net.cpp:252] TEST Top shape for layer 0 'data' 50 (50)
I0801 14:22:24.209228  1577 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0801 14:22:24.209233  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.209247  1577 net.cpp:184] Created Layer label_data_1_split (1)
I0801 14:22:24.209254  1577 net.cpp:561] label_data_1_split <- label
I0801 14:22:24.209262  1577 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0801 14:22:24.209269  1577 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0801 14:22:24.209273  1577 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0801 14:22:24.209314  1577 net.cpp:245] Setting up label_data_1_split
I0801 14:22:24.209319  1577 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0801 14:22:24.209326  1577 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0801 14:22:24.209331  1577 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0801 14:22:24.209334  1577 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0801 14:22:24.209339  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.209352  1577 net.cpp:184] Created Layer data/bias (2)
I0801 14:22:24.209357  1577 net.cpp:561] data/bias <- data
I0801 14:22:24.209360  1577 net.cpp:530] data/bias -> data/bias
I0801 14:22:24.210371  1606 data_layer.cpp:97] (0) Parser threads: 1
I0801 14:22:24.210379  1606 data_layer.cpp:99] (0) Transformer threads: 1
I0801 14:22:24.211083  1577 net.cpp:245] Setting up data/bias
I0801 14:22:24.211093  1577 net.cpp:252] TEST Top shape for layer 2 'data/bias' 50 3 32 32 (153600)
I0801 14:22:24.211105  1577 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0801 14:22:24.211109  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.211125  1577 net.cpp:184] Created Layer conv1a (3)
I0801 14:22:24.211129  1577 net.cpp:561] conv1a <- data/bias
I0801 14:22:24.211134  1577 net.cpp:530] conv1a -> conv1a
I0801 14:22:24.503372  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 8.15G, req 0G)
I0801 14:22:24.503392  1577 net.cpp:245] Setting up conv1a
I0801 14:22:24.503399  1577 net.cpp:252] TEST Top shape for layer 3 'conv1a' 50 32 32 32 (1638400)
I0801 14:22:24.503412  1577 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0801 14:22:24.503417  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.503432  1577 net.cpp:184] Created Layer conv1a/bn (4)
I0801 14:22:24.503437  1577 net.cpp:561] conv1a/bn <- conv1a
I0801 14:22:24.503442  1577 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0801 14:22:24.503883  1577 net.cpp:245] Setting up conv1a/bn
I0801 14:22:24.503891  1577 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 50 32 32 32 (1638400)
I0801 14:22:24.503901  1577 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0801 14:22:24.503906  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.503912  1577 net.cpp:184] Created Layer conv1a/relu (5)
I0801 14:22:24.503916  1577 net.cpp:561] conv1a/relu <- conv1a
I0801 14:22:24.503921  1577 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0801 14:22:24.503934  1577 net.cpp:245] Setting up conv1a/relu
I0801 14:22:24.503938  1577 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 50 32 32 32 (1638400)
I0801 14:22:24.503942  1577 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0801 14:22:24.503947  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.503957  1577 net.cpp:184] Created Layer conv1b (6)
I0801 14:22:24.503960  1577 net.cpp:561] conv1b <- conv1a
I0801 14:22:24.503965  1577 net.cpp:530] conv1b -> conv1b
I0801 14:22:24.507649  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.13G, req 0G)
I0801 14:22:24.507660  1577 net.cpp:245] Setting up conv1b
I0801 14:22:24.507666  1577 net.cpp:252] TEST Top shape for layer 6 'conv1b' 50 32 32 32 (1638400)
I0801 14:22:24.507674  1577 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0801 14:22:24.507678  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.507688  1577 net.cpp:184] Created Layer conv1b/bn (7)
I0801 14:22:24.507690  1577 net.cpp:561] conv1b/bn <- conv1b
I0801 14:22:24.507695  1577 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0801 14:22:24.508085  1577 net.cpp:245] Setting up conv1b/bn
I0801 14:22:24.508101  1577 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 50 32 32 32 (1638400)
I0801 14:22:24.508111  1577 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0801 14:22:24.508113  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.508121  1577 net.cpp:184] Created Layer conv1b/relu (8)
I0801 14:22:24.508123  1577 net.cpp:561] conv1b/relu <- conv1b
I0801 14:22:24.508128  1577 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0801 14:22:24.508136  1577 net.cpp:245] Setting up conv1b/relu
I0801 14:22:24.508141  1577 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 50 32 32 32 (1638400)
I0801 14:22:24.508144  1577 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0801 14:22:24.508148  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.508157  1577 net.cpp:184] Created Layer pool1 (9)
I0801 14:22:24.508159  1577 net.cpp:561] pool1 <- conv1b
I0801 14:22:24.508164  1577 net.cpp:530] pool1 -> pool1
I0801 14:22:24.508203  1577 net.cpp:245] Setting up pool1
I0801 14:22:24.508209  1577 net.cpp:252] TEST Top shape for layer 9 'pool1' 50 32 32 32 (1638400)
I0801 14:22:24.508214  1577 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0801 14:22:24.508219  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.508226  1577 net.cpp:184] Created Layer res2a_branch2a (10)
I0801 14:22:24.508230  1577 net.cpp:561] res2a_branch2a <- pool1
I0801 14:22:24.508234  1577 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0801 14:22:24.513413  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.11G, req 0G)
I0801 14:22:24.513424  1577 net.cpp:245] Setting up res2a_branch2a
I0801 14:22:24.513430  1577 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 50 64 32 32 (3276800)
I0801 14:22:24.513438  1577 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0801 14:22:24.513442  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.513451  1577 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0801 14:22:24.513455  1577 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0801 14:22:24.513459  1577 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0801 14:22:24.513854  1577 net.cpp:245] Setting up res2a_branch2a/bn
I0801 14:22:24.513861  1577 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 50 64 32 32 (3276800)
I0801 14:22:24.513870  1577 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0801 14:22:24.513875  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.513880  1577 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0801 14:22:24.513883  1577 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0801 14:22:24.513888  1577 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0801 14:22:24.513895  1577 net.cpp:245] Setting up res2a_branch2a/relu
I0801 14:22:24.513898  1577 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 50 64 32 32 (3276800)
I0801 14:22:24.513902  1577 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0801 14:22:24.513906  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.513916  1577 net.cpp:184] Created Layer res2a_branch2b (13)
I0801 14:22:24.513921  1577 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0801 14:22:24.513926  1577 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0801 14:22:24.517082  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.09G, req 0G)
I0801 14:22:24.517093  1577 net.cpp:245] Setting up res2a_branch2b
I0801 14:22:24.517099  1577 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 50 64 32 32 (3276800)
I0801 14:22:24.517114  1577 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0801 14:22:24.517119  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.517127  1577 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0801 14:22:24.517130  1577 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0801 14:22:24.517134  1577 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0801 14:22:24.517534  1577 net.cpp:245] Setting up res2a_branch2b/bn
I0801 14:22:24.517542  1577 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 50 64 32 32 (3276800)
I0801 14:22:24.517552  1577 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0801 14:22:24.517556  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.517562  1577 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0801 14:22:24.517565  1577 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0801 14:22:24.517570  1577 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0801 14:22:24.517576  1577 net.cpp:245] Setting up res2a_branch2b/relu
I0801 14:22:24.517581  1577 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 50 64 32 32 (3276800)
I0801 14:22:24.517585  1577 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0801 14:22:24.517588  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.517596  1577 net.cpp:184] Created Layer pool2 (16)
I0801 14:22:24.517598  1577 net.cpp:561] pool2 <- res2a_branch2b
I0801 14:22:24.517603  1577 net.cpp:530] pool2 -> pool2
I0801 14:22:24.517634  1577 net.cpp:245] Setting up pool2
I0801 14:22:24.517639  1577 net.cpp:252] TEST Top shape for layer 16 'pool2' 50 64 16 16 (819200)
I0801 14:22:24.517643  1577 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0801 14:22:24.517648  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.517657  1577 net.cpp:184] Created Layer res3a_branch2a (17)
I0801 14:22:24.517660  1577 net.cpp:561] res3a_branch2a <- pool2
I0801 14:22:24.517664  1577 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0801 14:22:24.522755  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0G)
I0801 14:22:24.522765  1577 net.cpp:245] Setting up res3a_branch2a
I0801 14:22:24.522773  1577 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 50 128 16 16 (1638400)
I0801 14:22:24.522779  1577 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0801 14:22:24.522783  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.522790  1577 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0801 14:22:24.522794  1577 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0801 14:22:24.522799  1577 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0801 14:22:24.523201  1577 net.cpp:245] Setting up res3a_branch2a/bn
I0801 14:22:24.523208  1577 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 50 128 16 16 (1638400)
I0801 14:22:24.523218  1577 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0801 14:22:24.523221  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.523227  1577 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0801 14:22:24.523231  1577 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0801 14:22:24.523236  1577 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0801 14:22:24.523242  1577 net.cpp:245] Setting up res3a_branch2a/relu
I0801 14:22:24.523247  1577 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 50 128 16 16 (1638400)
I0801 14:22:24.523250  1577 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0801 14:22:24.523254  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.523274  1577 net.cpp:184] Created Layer res3a_branch2b (20)
I0801 14:22:24.523278  1577 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0801 14:22:24.523283  1577 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0801 14:22:24.526175  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.07G, req 0G)
I0801 14:22:24.526185  1577 net.cpp:245] Setting up res3a_branch2b
I0801 14:22:24.526191  1577 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 50 128 16 16 (1638400)
I0801 14:22:24.526198  1577 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0801 14:22:24.526202  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.526209  1577 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0801 14:22:24.526212  1577 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0801 14:22:24.526217  1577 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0801 14:22:24.526608  1577 net.cpp:245] Setting up res3a_branch2b/bn
I0801 14:22:24.526615  1577 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 50 128 16 16 (1638400)
I0801 14:22:24.526623  1577 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0801 14:22:24.526628  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.526633  1577 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0801 14:22:24.526638  1577 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0801 14:22:24.526641  1577 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0801 14:22:24.526648  1577 net.cpp:245] Setting up res3a_branch2b/relu
I0801 14:22:24.526652  1577 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 50 128 16 16 (1638400)
I0801 14:22:24.526655  1577 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0801 14:22:24.526660  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.526666  1577 net.cpp:184] Created Layer pool3 (23)
I0801 14:22:24.526669  1577 net.cpp:561] pool3 <- res3a_branch2b
I0801 14:22:24.526674  1577 net.cpp:530] pool3 -> pool3
I0801 14:22:24.526710  1577 net.cpp:245] Setting up pool3
I0801 14:22:24.526715  1577 net.cpp:252] TEST Top shape for layer 23 'pool3' 50 128 16 16 (1638400)
I0801 14:22:24.526721  1577 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0801 14:22:24.526724  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.526736  1577 net.cpp:184] Created Layer res4a_branch2a (24)
I0801 14:22:24.526741  1577 net.cpp:561] res4a_branch2a <- pool3
I0801 14:22:24.526746  1577 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0801 14:22:24.539494  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0G)
I0801 14:22:24.539505  1577 net.cpp:245] Setting up res4a_branch2a
I0801 14:22:24.539511  1577 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 50 256 16 16 (3276800)
I0801 14:22:24.539518  1577 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0801 14:22:24.539522  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.539530  1577 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0801 14:22:24.539533  1577 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0801 14:22:24.539538  1577 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0801 14:22:24.539950  1577 net.cpp:245] Setting up res4a_branch2a/bn
I0801 14:22:24.539958  1577 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 50 256 16 16 (3276800)
I0801 14:22:24.539966  1577 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0801 14:22:24.539970  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.539975  1577 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0801 14:22:24.539986  1577 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0801 14:22:24.539991  1577 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0801 14:22:24.539997  1577 net.cpp:245] Setting up res4a_branch2a/relu
I0801 14:22:24.540002  1577 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 50 256 16 16 (3276800)
I0801 14:22:24.540006  1577 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0801 14:22:24.540010  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.540019  1577 net.cpp:184] Created Layer res4a_branch2b (27)
I0801 14:22:24.540022  1577 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0801 14:22:24.540026  1577 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0801 14:22:24.546970  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.03G, req 0G)
I0801 14:22:24.546983  1577 net.cpp:245] Setting up res4a_branch2b
I0801 14:22:24.546990  1577 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 50 256 16 16 (3276800)
I0801 14:22:24.546998  1577 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0801 14:22:24.547003  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.547009  1577 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0801 14:22:24.547013  1577 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0801 14:22:24.547019  1577 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0801 14:22:24.547426  1577 net.cpp:245] Setting up res4a_branch2b/bn
I0801 14:22:24.547435  1577 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 50 256 16 16 (3276800)
I0801 14:22:24.547442  1577 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0801 14:22:24.547446  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.547452  1577 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0801 14:22:24.547456  1577 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0801 14:22:24.547461  1577 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0801 14:22:24.547466  1577 net.cpp:245] Setting up res4a_branch2b/relu
I0801 14:22:24.547472  1577 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 50 256 16 16 (3276800)
I0801 14:22:24.547475  1577 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0801 14:22:24.547480  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.547487  1577 net.cpp:184] Created Layer pool4 (30)
I0801 14:22:24.547490  1577 net.cpp:561] pool4 <- res4a_branch2b
I0801 14:22:24.547494  1577 net.cpp:530] pool4 -> pool4
I0801 14:22:24.547533  1577 net.cpp:245] Setting up pool4
I0801 14:22:24.547539  1577 net.cpp:252] TEST Top shape for layer 30 'pool4' 50 256 8 8 (819200)
I0801 14:22:24.547544  1577 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0801 14:22:24.547549  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.547557  1577 net.cpp:184] Created Layer res5a_branch2a (31)
I0801 14:22:24.547561  1577 net.cpp:561] res5a_branch2a <- pool4
I0801 14:22:24.547565  1577 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0801 14:22:24.580353  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.01G, req 0G)
I0801 14:22:24.580368  1577 net.cpp:245] Setting up res5a_branch2a
I0801 14:22:24.580375  1577 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 50 512 8 8 (1638400)
I0801 14:22:24.580384  1577 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0801 14:22:24.580387  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.580397  1577 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0801 14:22:24.580401  1577 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0801 14:22:24.580415  1577 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0801 14:22:24.580862  1577 net.cpp:245] Setting up res5a_branch2a/bn
I0801 14:22:24.580870  1577 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 50 512 8 8 (1638400)
I0801 14:22:24.580878  1577 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0801 14:22:24.580883  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.580888  1577 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0801 14:22:24.580893  1577 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0801 14:22:24.580896  1577 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0801 14:22:24.580902  1577 net.cpp:245] Setting up res5a_branch2a/relu
I0801 14:22:24.580907  1577 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 50 512 8 8 (1638400)
I0801 14:22:24.580911  1577 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0801 14:22:24.580915  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.580925  1577 net.cpp:184] Created Layer res5a_branch2b (34)
I0801 14:22:24.580929  1577 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0801 14:22:24.580934  1577 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0801 14:22:24.596328  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8G, req 0G)
I0801 14:22:24.596339  1577 net.cpp:245] Setting up res5a_branch2b
I0801 14:22:24.596344  1577 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 50 512 8 8 (1638400)
I0801 14:22:24.596356  1577 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0801 14:22:24.596360  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.596374  1577 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0801 14:22:24.596377  1577 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0801 14:22:24.596382  1577 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0801 14:22:24.596804  1577 net.cpp:245] Setting up res5a_branch2b/bn
I0801 14:22:24.596812  1577 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 50 512 8 8 (1638400)
I0801 14:22:24.596829  1577 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0801 14:22:24.596834  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.596840  1577 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0801 14:22:24.596843  1577 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0801 14:22:24.596848  1577 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0801 14:22:24.596855  1577 net.cpp:245] Setting up res5a_branch2b/relu
I0801 14:22:24.596860  1577 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 50 512 8 8 (1638400)
I0801 14:22:24.596864  1577 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0801 14:22:24.596868  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.596875  1577 net.cpp:184] Created Layer pool5 (37)
I0801 14:22:24.596879  1577 net.cpp:561] pool5 <- res5a_branch2b
I0801 14:22:24.596884  1577 net.cpp:530] pool5 -> pool5
I0801 14:22:24.596902  1577 net.cpp:245] Setting up pool5
I0801 14:22:24.596909  1577 net.cpp:252] TEST Top shape for layer 37 'pool5' 50 512 1 1 (25600)
I0801 14:22:24.596912  1577 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0801 14:22:24.596917  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.596925  1577 net.cpp:184] Created Layer fc10 (38)
I0801 14:22:24.596928  1577 net.cpp:561] fc10 <- pool5
I0801 14:22:24.596933  1577 net.cpp:530] fc10 -> fc10
I0801 14:22:24.597127  1577 net.cpp:245] Setting up fc10
I0801 14:22:24.597134  1577 net.cpp:252] TEST Top shape for layer 38 'fc10' 50 10 (500)
I0801 14:22:24.597141  1577 layer_factory.hpp:136] Creating layer 'fc10_fc10_0_split' of type 'Split'
I0801 14:22:24.597152  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.597158  1577 net.cpp:184] Created Layer fc10_fc10_0_split (39)
I0801 14:22:24.597162  1577 net.cpp:561] fc10_fc10_0_split <- fc10
I0801 14:22:24.597168  1577 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0801 14:22:24.597173  1577 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0801 14:22:24.597178  1577 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0801 14:22:24.597209  1577 net.cpp:245] Setting up fc10_fc10_0_split
I0801 14:22:24.597214  1577 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0801 14:22:24.597219  1577 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0801 14:22:24.597224  1577 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 50 10 (500)
I0801 14:22:24.597229  1577 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0801 14:22:24.597232  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.597244  1577 net.cpp:184] Created Layer loss (40)
I0801 14:22:24.597249  1577 net.cpp:561] loss <- fc10_fc10_0_split_0
I0801 14:22:24.597254  1577 net.cpp:561] loss <- label_data_1_split_0
I0801 14:22:24.597259  1577 net.cpp:530] loss -> loss
I0801 14:22:24.597363  1577 net.cpp:245] Setting up loss
I0801 14:22:24.597370  1577 net.cpp:252] TEST Top shape for layer 40 'loss' (1)
I0801 14:22:24.597375  1577 net.cpp:256]     with loss weight 1
I0801 14:22:24.597381  1577 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0801 14:22:24.597385  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.597396  1577 net.cpp:184] Created Layer accuracy/top1 (41)
I0801 14:22:24.597400  1577 net.cpp:561] accuracy/top1 <- fc10_fc10_0_split_1
I0801 14:22:24.597404  1577 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0801 14:22:24.597409  1577 net.cpp:530] accuracy/top1 -> accuracy/top1
I0801 14:22:24.597416  1577 net.cpp:245] Setting up accuracy/top1
I0801 14:22:24.597420  1577 net.cpp:252] TEST Top shape for layer 41 'accuracy/top1' (1)
I0801 14:22:24.597424  1577 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0801 14:22:24.597429  1577 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 14:22:24.597440  1577 net.cpp:184] Created Layer accuracy/top5 (42)
I0801 14:22:24.597443  1577 net.cpp:561] accuracy/top5 <- fc10_fc10_0_split_2
I0801 14:22:24.597447  1577 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0801 14:22:24.597451  1577 net.cpp:530] accuracy/top5 -> accuracy/top5
I0801 14:22:24.597457  1577 net.cpp:245] Setting up accuracy/top5
I0801 14:22:24.597462  1577 net.cpp:252] TEST Top shape for layer 42 'accuracy/top5' (1)
I0801 14:22:24.597466  1577 net.cpp:325] accuracy/top5 does not need backward computation.
I0801 14:22:24.597471  1577 net.cpp:325] accuracy/top1 does not need backward computation.
I0801 14:22:24.597476  1577 net.cpp:323] loss needs backward computation.
I0801 14:22:24.597479  1577 net.cpp:323] fc10_fc10_0_split needs backward computation.
I0801 14:22:24.597483  1577 net.cpp:323] fc10 needs backward computation.
I0801 14:22:24.597487  1577 net.cpp:323] pool5 needs backward computation.
I0801 14:22:24.597491  1577 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0801 14:22:24.597496  1577 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0801 14:22:24.597499  1577 net.cpp:323] res5a_branch2b needs backward computation.
I0801 14:22:24.597503  1577 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0801 14:22:24.597507  1577 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0801 14:22:24.597512  1577 net.cpp:323] res5a_branch2a needs backward computation.
I0801 14:22:24.597515  1577 net.cpp:323] pool4 needs backward computation.
I0801 14:22:24.597519  1577 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0801 14:22:24.597528  1577 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0801 14:22:24.597532  1577 net.cpp:323] res4a_branch2b needs backward computation.
I0801 14:22:24.597537  1577 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0801 14:22:24.597540  1577 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0801 14:22:24.597544  1577 net.cpp:323] res4a_branch2a needs backward computation.
I0801 14:22:24.597548  1577 net.cpp:323] pool3 needs backward computation.
I0801 14:22:24.597553  1577 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0801 14:22:24.597558  1577 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0801 14:22:24.597561  1577 net.cpp:323] res3a_branch2b needs backward computation.
I0801 14:22:24.597565  1577 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0801 14:22:24.597569  1577 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0801 14:22:24.597573  1577 net.cpp:323] res3a_branch2a needs backward computation.
I0801 14:22:24.597578  1577 net.cpp:323] pool2 needs backward computation.
I0801 14:22:24.597581  1577 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0801 14:22:24.597585  1577 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0801 14:22:24.597589  1577 net.cpp:323] res2a_branch2b needs backward computation.
I0801 14:22:24.597594  1577 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0801 14:22:24.597597  1577 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0801 14:22:24.597600  1577 net.cpp:323] res2a_branch2a needs backward computation.
I0801 14:22:24.597605  1577 net.cpp:323] pool1 needs backward computation.
I0801 14:22:24.597609  1577 net.cpp:323] conv1b/relu needs backward computation.
I0801 14:22:24.597614  1577 net.cpp:323] conv1b/bn needs backward computation.
I0801 14:22:24.597616  1577 net.cpp:323] conv1b needs backward computation.
I0801 14:22:24.597620  1577 net.cpp:323] conv1a/relu needs backward computation.
I0801 14:22:24.597625  1577 net.cpp:323] conv1a/bn needs backward computation.
I0801 14:22:24.597628  1577 net.cpp:323] conv1a needs backward computation.
I0801 14:22:24.597633  1577 net.cpp:325] data/bias does not need backward computation.
I0801 14:22:24.597637  1577 net.cpp:325] label_data_1_split does not need backward computation.
I0801 14:22:24.597642  1577 net.cpp:325] data does not need backward computation.
I0801 14:22:24.597646  1577 net.cpp:367] This network produces output accuracy/top1
I0801 14:22:24.597650  1577 net.cpp:367] This network produces output accuracy/top5
I0801 14:22:24.597653  1577 net.cpp:367] This network produces output loss
I0801 14:22:24.597683  1577 net.cpp:389] Top memory (TEST) required for data: 275251200 diff: 8
I0801 14:22:24.597687  1577 net.cpp:392] Bottom memory (TEST) required for data: 275251200 diff: 275251200
I0801 14:22:24.597690  1577 net.cpp:395] Shared (in-place) memory (TEST) by data: 183500800 diff: 183500800
I0801 14:22:24.597693  1577 net.cpp:398] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0801 14:22:24.597697  1577 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0801 14:22:24.597702  1577 net.cpp:407] Network initialization done.
I0801 14:22:24.601003  1577 net.cpp:1089] Copying source layer data Type:Data #blobs=0
I0801 14:22:24.601024  1577 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0801 14:22:24.601059  1577 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0801 14:22:24.601073  1577 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0801 14:22:24.601217  1577 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0801 14:22:24.601223  1577 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0801 14:22:24.601234  1577 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0801 14:22:24.601325  1577 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0801 14:22:24.601330  1577 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0801 14:22:24.601342  1577 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0801 14:22:24.601362  1577 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0801 14:22:24.601456  1577 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0801 14:22:24.601461  1577 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0801 14:22:24.601475  1577 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0801 14:22:24.601562  1577 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0801 14:22:24.601567  1577 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0801 14:22:24.601569  1577 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0801 14:22:24.601610  1577 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0801 14:22:24.601692  1577 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0801 14:22:24.601697  1577 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0801 14:22:24.601722  1577 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0801 14:22:24.601804  1577 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0801 14:22:24.601809  1577 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0801 14:22:24.601811  1577 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0801 14:22:24.601918  1577 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0801 14:22:24.601999  1577 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0801 14:22:24.602005  1577 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0801 14:22:24.602061  1577 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0801 14:22:24.602138  1577 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0801 14:22:24.602143  1577 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0801 14:22:24.602146  1577 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0801 14:22:24.602499  1577 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0801 14:22:24.602584  1577 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0801 14:22:24.602589  1577 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0801 14:22:24.602738  1577 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0801 14:22:24.602821  1577 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0801 14:22:24.602826  1577 net.cpp:1089] Copying source layer pool5 Type:Pooling #blobs=0
I0801 14:22:24.602830  1577 net.cpp:1089] Copying source layer fc10 Type:InnerProduct #blobs=2
I0801 14:22:24.602841  1577 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0801 14:22:24.602887  1577 caffe.cpp:290] Running for 200 iterations.
I0801 14:22:24.605602  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8G, req 0G)
I0801 14:22:24.609104  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.98G, req 0G)
I0801 14:22:24.614636  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0G)
I0801 14:22:24.618613  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.93G, req 0G)
I0801 14:22:24.623805  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.9G, req 0G)
I0801 14:22:24.626929  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0G)
I0801 14:22:24.634404  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.85G, req 0G)
I0801 14:22:24.639351  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.83G, req 0G)
I0801 14:22:24.650172  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 6  (limit 7.79G, req 0.01G)
I0801 14:22:24.657883  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.78G, req 0.01G)
I0801 14:22:24.661156  1577 caffe.cpp:313] Batch 0, accuracy/top1 = 0.94
I0801 14:22:24.661180  1577 caffe.cpp:313] Batch 0, accuracy/top5 = 1
I0801 14:22:24.661190  1577 caffe.cpp:313] Batch 0, loss = 0.124184
I0801 14:22:24.667207  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.74G/1 1  (limit 7.04G, req 0.01G)
I0801 14:22:24.672631  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 1.48G/2 1  (limit 6.3G, req 0.01G)
I0801 14:22:24.684130  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 1.48G/1 6  (limit 6.3G, req 0.01G)
I0801 14:22:24.692947  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 1.48G/2 6  (limit 6.3G, req 0.01G)
I0801 14:22:24.702162  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 1.48G/1 6  (limit 6.3G, req 0.01G)
I0801 14:22:24.707729  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 1.48G/2 6  (limit 6.3G, req 0.01G)
I0801 14:22:24.723266  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 1.48G/1 6  (limit 6.3G, req 0.01G)
I0801 14:22:24.731559  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 1.48G/2 6  (limit 6.3G, req 0.01G)
I0801 14:22:24.755535  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2a' with space 1.48G/1 7  (limit 6.3G, req 0.05G)
I0801 14:22:24.763113  1577 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2b' with space 1.48G/2 6  (limit 6.3G, req 0.05G)
I0801 14:22:24.764528  1577 caffe.cpp:313] Batch 1, accuracy/top1 = 0.88
I0801 14:22:24.764555  1577 caffe.cpp:313] Batch 1, accuracy/top5 = 1
I0801 14:22:24.764567  1577 caffe.cpp:313] Batch 1, loss = 0.631967
I0801 14:22:24.774415  1577 caffe.cpp:313] Batch 2, accuracy/top1 = 0.9
I0801 14:22:24.774451  1577 caffe.cpp:313] Batch 2, accuracy/top5 = 1
I0801 14:22:24.774462  1577 caffe.cpp:313] Batch 2, loss = 0.287833
I0801 14:22:24.784627  1577 caffe.cpp:313] Batch 3, accuracy/top1 = 0.9
I0801 14:22:24.784663  1577 caffe.cpp:313] Batch 3, accuracy/top5 = 1
I0801 14:22:24.784675  1577 caffe.cpp:313] Batch 3, loss = 0.473005
I0801 14:22:24.794651  1577 caffe.cpp:313] Batch 4, accuracy/top1 = 0.92
I0801 14:22:24.794674  1577 caffe.cpp:313] Batch 4, accuracy/top5 = 1
I0801 14:22:24.794683  1577 caffe.cpp:313] Batch 4, loss = 0.444
I0801 14:22:24.804015  1577 caffe.cpp:313] Batch 5, accuracy/top1 = 0.82
I0801 14:22:24.804035  1577 caffe.cpp:313] Batch 5, accuracy/top5 = 1
I0801 14:22:24.804041  1577 caffe.cpp:313] Batch 5, loss = 0.555535
I0801 14:22:24.812973  1577 caffe.cpp:313] Batch 6, accuracy/top1 = 1
I0801 14:22:24.812988  1577 caffe.cpp:313] Batch 6, accuracy/top5 = 1
I0801 14:22:24.812991  1577 caffe.cpp:313] Batch 6, loss = 0.0455181
I0801 14:22:24.821709  1577 caffe.cpp:313] Batch 7, accuracy/top1 = 0.88
I0801 14:22:24.821725  1577 caffe.cpp:313] Batch 7, accuracy/top5 = 1
I0801 14:22:24.821728  1577 caffe.cpp:313] Batch 7, loss = 0.635858
I0801 14:22:24.830292  1577 caffe.cpp:313] Batch 8, accuracy/top1 = 0.94
I0801 14:22:24.830302  1577 caffe.cpp:313] Batch 8, accuracy/top5 = 1
I0801 14:22:24.830303  1577 caffe.cpp:313] Batch 8, loss = 0.241769
I0801 14:22:24.838816  1577 caffe.cpp:313] Batch 9, accuracy/top1 = 0.94
I0801 14:22:24.838826  1577 caffe.cpp:313] Batch 9, accuracy/top5 = 1
I0801 14:22:24.838829  1577 caffe.cpp:313] Batch 9, loss = 0.313418
I0801 14:22:24.847367  1577 caffe.cpp:313] Batch 10, accuracy/top1 = 0.98
I0801 14:22:24.847378  1577 caffe.cpp:313] Batch 10, accuracy/top5 = 1
I0801 14:22:24.847380  1577 caffe.cpp:313] Batch 10, loss = 0.102595
I0801 14:22:24.855865  1577 caffe.cpp:313] Batch 11, accuracy/top1 = 0.94
I0801 14:22:24.855882  1577 caffe.cpp:313] Batch 11, accuracy/top5 = 1
I0801 14:22:24.855885  1577 caffe.cpp:313] Batch 11, loss = 0.199213
I0801 14:22:24.864356  1577 caffe.cpp:313] Batch 12, accuracy/top1 = 0.92
I0801 14:22:24.864364  1577 caffe.cpp:313] Batch 12, accuracy/top5 = 1
I0801 14:22:24.864367  1577 caffe.cpp:313] Batch 12, loss = 0.252476
I0801 14:22:24.872817  1577 caffe.cpp:313] Batch 13, accuracy/top1 = 0.88
I0801 14:22:24.872825  1577 caffe.cpp:313] Batch 13, accuracy/top5 = 1
I0801 14:22:24.872828  1577 caffe.cpp:313] Batch 13, loss = 0.470494
I0801 14:22:24.881343  1577 caffe.cpp:313] Batch 14, accuracy/top1 = 0.86
I0801 14:22:24.881350  1577 caffe.cpp:313] Batch 14, accuracy/top5 = 1
I0801 14:22:24.881353  1577 caffe.cpp:313] Batch 14, loss = 0.774902
I0801 14:22:24.889797  1577 caffe.cpp:313] Batch 15, accuracy/top1 = 0.9
I0801 14:22:24.889804  1577 caffe.cpp:313] Batch 15, accuracy/top5 = 0.98
I0801 14:22:24.889807  1577 caffe.cpp:313] Batch 15, loss = 0.626718
I0801 14:22:24.898317  1577 caffe.cpp:313] Batch 16, accuracy/top1 = 0.96
I0801 14:22:24.898325  1577 caffe.cpp:313] Batch 16, accuracy/top5 = 0.98
I0801 14:22:24.898327  1577 caffe.cpp:313] Batch 16, loss = 0.366777
I0801 14:22:24.906774  1577 caffe.cpp:313] Batch 17, accuracy/top1 = 0.84
I0801 14:22:24.906781  1577 caffe.cpp:313] Batch 17, accuracy/top5 = 0.98
I0801 14:22:24.906783  1577 caffe.cpp:313] Batch 17, loss = 0.613804
I0801 14:22:24.915241  1577 caffe.cpp:313] Batch 18, accuracy/top1 = 0.92
I0801 14:22:24.915251  1577 caffe.cpp:313] Batch 18, accuracy/top5 = 1
I0801 14:22:24.915252  1577 caffe.cpp:313] Batch 18, loss = 0.272792
I0801 14:22:24.923714  1577 caffe.cpp:313] Batch 19, accuracy/top1 = 0.96
I0801 14:22:24.923722  1577 caffe.cpp:313] Batch 19, accuracy/top5 = 1
I0801 14:22:24.923724  1577 caffe.cpp:313] Batch 19, loss = 0.150311
I0801 14:22:24.932212  1577 caffe.cpp:313] Batch 20, accuracy/top1 = 0.9
I0801 14:22:24.932219  1577 caffe.cpp:313] Batch 20, accuracy/top5 = 1
I0801 14:22:24.932222  1577 caffe.cpp:313] Batch 20, loss = 0.538488
I0801 14:22:24.940661  1577 caffe.cpp:313] Batch 21, accuracy/top1 = 0.9
I0801 14:22:24.940670  1577 caffe.cpp:313] Batch 21, accuracy/top5 = 1
I0801 14:22:24.940671  1577 caffe.cpp:313] Batch 21, loss = 0.320419
I0801 14:22:24.949383  1577 caffe.cpp:313] Batch 22, accuracy/top1 = 0.92
I0801 14:22:24.949391  1577 caffe.cpp:313] Batch 22, accuracy/top5 = 1
I0801 14:22:24.949393  1577 caffe.cpp:313] Batch 22, loss = 0.335932
I0801 14:22:24.957835  1577 caffe.cpp:313] Batch 23, accuracy/top1 = 0.88
I0801 14:22:24.957844  1577 caffe.cpp:313] Batch 23, accuracy/top5 = 1
I0801 14:22:24.957845  1577 caffe.cpp:313] Batch 23, loss = 0.55955
I0801 14:22:24.966325  1577 caffe.cpp:313] Batch 24, accuracy/top1 = 0.96
I0801 14:22:24.966332  1577 caffe.cpp:313] Batch 24, accuracy/top5 = 1
I0801 14:22:24.966336  1577 caffe.cpp:313] Batch 24, loss = 0.115718
I0801 14:22:24.974783  1577 caffe.cpp:313] Batch 25, accuracy/top1 = 0.9
I0801 14:22:24.974791  1577 caffe.cpp:313] Batch 25, accuracy/top5 = 1
I0801 14:22:24.974793  1577 caffe.cpp:313] Batch 25, loss = 0.342816
I0801 14:22:24.983264  1577 caffe.cpp:313] Batch 26, accuracy/top1 = 0.88
I0801 14:22:24.983273  1577 caffe.cpp:313] Batch 26, accuracy/top5 = 1
I0801 14:22:24.983274  1577 caffe.cpp:313] Batch 26, loss = 0.5979
I0801 14:22:24.991703  1577 caffe.cpp:313] Batch 27, accuracy/top1 = 0.9
I0801 14:22:24.991709  1577 caffe.cpp:313] Batch 27, accuracy/top5 = 0.96
I0801 14:22:24.991713  1577 caffe.cpp:313] Batch 27, loss = 0.348048
I0801 14:22:25.000191  1577 caffe.cpp:313] Batch 28, accuracy/top1 = 0.9
I0801 14:22:25.000200  1577 caffe.cpp:313] Batch 28, accuracy/top5 = 1
I0801 14:22:25.000201  1577 caffe.cpp:313] Batch 28, loss = 0.284451
I0801 14:22:25.008779  1577 caffe.cpp:313] Batch 29, accuracy/top1 = 0.8
I0801 14:22:25.008800  1577 caffe.cpp:313] Batch 29, accuracy/top5 = 1
I0801 14:22:25.008802  1577 caffe.cpp:313] Batch 29, loss = 0.744966
I0801 14:22:25.017494  1577 caffe.cpp:313] Batch 30, accuracy/top1 = 0.86
I0801 14:22:25.017524  1577 caffe.cpp:313] Batch 30, accuracy/top5 = 0.98
I0801 14:22:25.017527  1577 caffe.cpp:313] Batch 30, loss = 0.488817
I0801 14:22:25.026068  1577 caffe.cpp:313] Batch 31, accuracy/top1 = 0.88
I0801 14:22:25.026079  1577 caffe.cpp:313] Batch 31, accuracy/top5 = 0.98
I0801 14:22:25.026082  1577 caffe.cpp:313] Batch 31, loss = 0.378043
I0801 14:22:25.034651  1577 caffe.cpp:313] Batch 32, accuracy/top1 = 0.92
I0801 14:22:25.034659  1577 caffe.cpp:313] Batch 32, accuracy/top5 = 1
I0801 14:22:25.034662  1577 caffe.cpp:313] Batch 32, loss = 0.136081
I0801 14:22:25.043161  1577 caffe.cpp:313] Batch 33, accuracy/top1 = 0.94
I0801 14:22:25.043169  1577 caffe.cpp:313] Batch 33, accuracy/top5 = 1
I0801 14:22:25.043172  1577 caffe.cpp:313] Batch 33, loss = 0.401892
I0801 14:22:25.051682  1577 caffe.cpp:313] Batch 34, accuracy/top1 = 0.92
I0801 14:22:25.051690  1577 caffe.cpp:313] Batch 34, accuracy/top5 = 1
I0801 14:22:25.051693  1577 caffe.cpp:313] Batch 34, loss = 0.405307
I0801 14:22:25.060174  1577 caffe.cpp:313] Batch 35, accuracy/top1 = 0.92
I0801 14:22:25.060184  1577 caffe.cpp:313] Batch 35, accuracy/top5 = 1
I0801 14:22:25.060186  1577 caffe.cpp:313] Batch 35, loss = 0.304145
I0801 14:22:25.068725  1577 caffe.cpp:313] Batch 36, accuracy/top1 = 0.88
I0801 14:22:25.068734  1577 caffe.cpp:313] Batch 36, accuracy/top5 = 1
I0801 14:22:25.068737  1577 caffe.cpp:313] Batch 36, loss = 0.281722
I0801 14:22:25.077200  1577 caffe.cpp:313] Batch 37, accuracy/top1 = 0.88
I0801 14:22:25.077209  1577 caffe.cpp:313] Batch 37, accuracy/top5 = 1
I0801 14:22:25.077211  1577 caffe.cpp:313] Batch 37, loss = 0.58732
I0801 14:22:25.085711  1577 caffe.cpp:313] Batch 38, accuracy/top1 = 0.84
I0801 14:22:25.085719  1577 caffe.cpp:313] Batch 38, accuracy/top5 = 0.96
I0801 14:22:25.085722  1577 caffe.cpp:313] Batch 38, loss = 0.856529
I0801 14:22:25.094177  1577 caffe.cpp:313] Batch 39, accuracy/top1 = 0.92
I0801 14:22:25.094187  1577 caffe.cpp:313] Batch 39, accuracy/top5 = 0.98
I0801 14:22:25.094189  1577 caffe.cpp:313] Batch 39, loss = 0.592637
I0801 14:22:25.102691  1577 caffe.cpp:313] Batch 40, accuracy/top1 = 0.88
I0801 14:22:25.102699  1577 caffe.cpp:313] Batch 40, accuracy/top5 = 1
I0801 14:22:25.102702  1577 caffe.cpp:313] Batch 40, loss = 0.455167
I0801 14:22:25.111158  1577 caffe.cpp:313] Batch 41, accuracy/top1 = 0.88
I0801 14:22:25.111166  1577 caffe.cpp:313] Batch 41, accuracy/top5 = 1
I0801 14:22:25.111169  1577 caffe.cpp:313] Batch 41, loss = 0.253717
I0801 14:22:25.119698  1577 caffe.cpp:313] Batch 42, accuracy/top1 = 0.94
I0801 14:22:25.119705  1577 caffe.cpp:313] Batch 42, accuracy/top5 = 1
I0801 14:22:25.119709  1577 caffe.cpp:313] Batch 42, loss = 0.140982
I0801 14:22:25.128209  1577 caffe.cpp:313] Batch 43, accuracy/top1 = 0.92
I0801 14:22:25.128217  1577 caffe.cpp:313] Batch 43, accuracy/top5 = 1
I0801 14:22:25.128221  1577 caffe.cpp:313] Batch 43, loss = 0.314564
I0801 14:22:25.136700  1577 caffe.cpp:313] Batch 44, accuracy/top1 = 0.92
I0801 14:22:25.136709  1577 caffe.cpp:313] Batch 44, accuracy/top5 = 0.98
I0801 14:22:25.136713  1577 caffe.cpp:313] Batch 44, loss = 0.487034
I0801 14:22:25.145190  1577 caffe.cpp:313] Batch 45, accuracy/top1 = 0.86
I0801 14:22:25.145200  1577 caffe.cpp:313] Batch 45, accuracy/top5 = 0.98
I0801 14:22:25.145201  1577 caffe.cpp:313] Batch 45, loss = 1.03355
I0801 14:22:25.153650  1577 caffe.cpp:313] Batch 46, accuracy/top1 = 0.94
I0801 14:22:25.153658  1577 caffe.cpp:313] Batch 46, accuracy/top5 = 1
I0801 14:22:25.153661  1577 caffe.cpp:313] Batch 46, loss = 0.183861
I0801 14:22:25.162089  1577 caffe.cpp:313] Batch 47, accuracy/top1 = 0.86
I0801 14:22:25.162096  1577 caffe.cpp:313] Batch 47, accuracy/top5 = 0.98
I0801 14:22:25.162099  1577 caffe.cpp:313] Batch 47, loss = 0.562122
I0801 14:22:25.170547  1577 caffe.cpp:313] Batch 48, accuracy/top1 = 0.9
I0801 14:22:25.170554  1577 caffe.cpp:313] Batch 48, accuracy/top5 = 1
I0801 14:22:25.170557  1577 caffe.cpp:313] Batch 48, loss = 0.862427
I0801 14:22:25.179021  1577 caffe.cpp:313] Batch 49, accuracy/top1 = 0.92
I0801 14:22:25.179044  1577 caffe.cpp:313] Batch 49, accuracy/top5 = 1
I0801 14:22:25.179046  1577 caffe.cpp:313] Batch 49, loss = 0.41145
I0801 14:22:25.187482  1577 caffe.cpp:313] Batch 50, accuracy/top1 = 0.88
I0801 14:22:25.187489  1577 caffe.cpp:313] Batch 50, accuracy/top5 = 0.98
I0801 14:22:25.187492  1577 caffe.cpp:313] Batch 50, loss = 0.905406
I0801 14:22:25.195978  1577 caffe.cpp:313] Batch 51, accuracy/top1 = 0.9
I0801 14:22:25.195986  1577 caffe.cpp:313] Batch 51, accuracy/top5 = 0.96
I0801 14:22:25.195988  1577 caffe.cpp:313] Batch 51, loss = 0.572369
I0801 14:22:25.204419  1577 caffe.cpp:313] Batch 52, accuracy/top1 = 0.9
I0801 14:22:25.204427  1577 caffe.cpp:313] Batch 52, accuracy/top5 = 1
I0801 14:22:25.204429  1577 caffe.cpp:313] Batch 52, loss = 0.385243
I0801 14:22:25.212906  1577 caffe.cpp:313] Batch 53, accuracy/top1 = 0.94
I0801 14:22:25.212914  1577 caffe.cpp:313] Batch 53, accuracy/top5 = 1
I0801 14:22:25.212916  1577 caffe.cpp:313] Batch 53, loss = 0.175892
I0801 14:22:25.221364  1577 caffe.cpp:313] Batch 54, accuracy/top1 = 0.9
I0801 14:22:25.221370  1577 caffe.cpp:313] Batch 54, accuracy/top5 = 1
I0801 14:22:25.221374  1577 caffe.cpp:313] Batch 54, loss = 0.560985
I0801 14:22:25.229876  1577 caffe.cpp:313] Batch 55, accuracy/top1 = 0.88
I0801 14:22:25.229883  1577 caffe.cpp:313] Batch 55, accuracy/top5 = 0.98
I0801 14:22:25.229887  1577 caffe.cpp:313] Batch 55, loss = 0.509652
I0801 14:22:25.238319  1577 caffe.cpp:313] Batch 56, accuracy/top1 = 0.9
I0801 14:22:25.238327  1577 caffe.cpp:313] Batch 56, accuracy/top5 = 0.98
I0801 14:22:25.238328  1577 caffe.cpp:313] Batch 56, loss = 0.569866
I0801 14:22:25.246821  1577 caffe.cpp:313] Batch 57, accuracy/top1 = 0.92
I0801 14:22:25.246829  1577 caffe.cpp:313] Batch 57, accuracy/top5 = 1
I0801 14:22:25.246831  1577 caffe.cpp:313] Batch 57, loss = 0.237857
I0801 14:22:25.255270  1577 caffe.cpp:313] Batch 58, accuracy/top1 = 0.92
I0801 14:22:25.255276  1577 caffe.cpp:313] Batch 58, accuracy/top5 = 1
I0801 14:22:25.255278  1577 caffe.cpp:313] Batch 58, loss = 0.389293
I0801 14:22:25.263777  1577 caffe.cpp:313] Batch 59, accuracy/top1 = 0.96
I0801 14:22:25.263784  1577 caffe.cpp:313] Batch 59, accuracy/top5 = 1
I0801 14:22:25.263787  1577 caffe.cpp:313] Batch 59, loss = 0.25388
I0801 14:22:25.272233  1577 caffe.cpp:313] Batch 60, accuracy/top1 = 0.9
I0801 14:22:25.272241  1577 caffe.cpp:313] Batch 60, accuracy/top5 = 0.98
I0801 14:22:25.272244  1577 caffe.cpp:313] Batch 60, loss = 0.360995
I0801 14:22:25.280731  1577 caffe.cpp:313] Batch 61, accuracy/top1 = 0.88
I0801 14:22:25.280738  1577 caffe.cpp:313] Batch 61, accuracy/top5 = 0.96
I0801 14:22:25.280741  1577 caffe.cpp:313] Batch 61, loss = 0.66677
I0801 14:22:25.289180  1577 caffe.cpp:313] Batch 62, accuracy/top1 = 0.96
I0801 14:22:25.289186  1577 caffe.cpp:313] Batch 62, accuracy/top5 = 1
I0801 14:22:25.289189  1577 caffe.cpp:313] Batch 62, loss = 0.234698
I0801 14:22:25.297631  1577 caffe.cpp:313] Batch 63, accuracy/top1 = 0.86
I0801 14:22:25.297637  1577 caffe.cpp:313] Batch 63, accuracy/top5 = 0.98
I0801 14:22:25.297641  1577 caffe.cpp:313] Batch 63, loss = 0.639523
I0801 14:22:25.306073  1577 caffe.cpp:313] Batch 64, accuracy/top1 = 0.9
I0801 14:22:25.306080  1577 caffe.cpp:313] Batch 64, accuracy/top5 = 0.98
I0801 14:22:25.306083  1577 caffe.cpp:313] Batch 64, loss = 0.37947
I0801 14:22:25.314556  1577 caffe.cpp:313] Batch 65, accuracy/top1 = 0.9
I0801 14:22:25.314563  1577 caffe.cpp:313] Batch 65, accuracy/top5 = 0.98
I0801 14:22:25.314566  1577 caffe.cpp:313] Batch 65, loss = 0.574146
I0801 14:22:25.322996  1577 caffe.cpp:313] Batch 66, accuracy/top1 = 0.86
I0801 14:22:25.323004  1577 caffe.cpp:313] Batch 66, accuracy/top5 = 1
I0801 14:22:25.323006  1577 caffe.cpp:313] Batch 66, loss = 0.398948
I0801 14:22:25.331519  1577 caffe.cpp:313] Batch 67, accuracy/top1 = 0.92
I0801 14:22:25.331526  1577 caffe.cpp:313] Batch 67, accuracy/top5 = 1
I0801 14:22:25.331529  1577 caffe.cpp:313] Batch 67, loss = 0.375773
I0801 14:22:25.339972  1577 caffe.cpp:313] Batch 68, accuracy/top1 = 0.88
I0801 14:22:25.339987  1577 caffe.cpp:313] Batch 68, accuracy/top5 = 1
I0801 14:22:25.339989  1577 caffe.cpp:313] Batch 68, loss = 0.811679
I0801 14:22:25.348443  1577 caffe.cpp:313] Batch 69, accuracy/top1 = 0.88
I0801 14:22:25.348450  1577 caffe.cpp:313] Batch 69, accuracy/top5 = 1
I0801 14:22:25.348453  1577 caffe.cpp:313] Batch 69, loss = 0.474067
I0801 14:22:25.356894  1577 caffe.cpp:313] Batch 70, accuracy/top1 = 0.96
I0801 14:22:25.356900  1577 caffe.cpp:313] Batch 70, accuracy/top5 = 0.98
I0801 14:22:25.356904  1577 caffe.cpp:313] Batch 70, loss = 0.456746
I0801 14:22:25.365350  1577 caffe.cpp:313] Batch 71, accuracy/top1 = 0.9
I0801 14:22:25.365358  1577 caffe.cpp:313] Batch 71, accuracy/top5 = 1
I0801 14:22:25.365360  1577 caffe.cpp:313] Batch 71, loss = 0.49832
I0801 14:22:25.373795  1577 caffe.cpp:313] Batch 72, accuracy/top1 = 0.88
I0801 14:22:25.373801  1577 caffe.cpp:313] Batch 72, accuracy/top5 = 0.98
I0801 14:22:25.373803  1577 caffe.cpp:313] Batch 72, loss = 0.665709
I0801 14:22:25.382272  1577 caffe.cpp:313] Batch 73, accuracy/top1 = 0.94
I0801 14:22:25.382280  1577 caffe.cpp:313] Batch 73, accuracy/top5 = 1
I0801 14:22:25.382282  1577 caffe.cpp:313] Batch 73, loss = 0.245877
I0801 14:22:25.390712  1577 caffe.cpp:313] Batch 74, accuracy/top1 = 0.88
I0801 14:22:25.390719  1577 caffe.cpp:313] Batch 74, accuracy/top5 = 1
I0801 14:22:25.390722  1577 caffe.cpp:313] Batch 74, loss = 0.413226
I0801 14:22:25.399207  1577 caffe.cpp:313] Batch 75, accuracy/top1 = 0.86
I0801 14:22:25.399214  1577 caffe.cpp:313] Batch 75, accuracy/top5 = 1
I0801 14:22:25.399217  1577 caffe.cpp:313] Batch 75, loss = 0.51521
I0801 14:22:25.407649  1577 caffe.cpp:313] Batch 76, accuracy/top1 = 0.94
I0801 14:22:25.407656  1577 caffe.cpp:313] Batch 76, accuracy/top5 = 1
I0801 14:22:25.407658  1577 caffe.cpp:313] Batch 76, loss = 0.208526
I0801 14:22:25.416143  1577 caffe.cpp:313] Batch 77, accuracy/top1 = 0.9
I0801 14:22:25.416151  1577 caffe.cpp:313] Batch 77, accuracy/top5 = 1
I0801 14:22:25.416152  1577 caffe.cpp:313] Batch 77, loss = 0.496416
I0801 14:22:25.424602  1577 caffe.cpp:313] Batch 78, accuracy/top1 = 0.96
I0801 14:22:25.424609  1577 caffe.cpp:313] Batch 78, accuracy/top5 = 1
I0801 14:22:25.424612  1577 caffe.cpp:313] Batch 78, loss = 0.119986
I0801 14:22:25.433116  1577 caffe.cpp:313] Batch 79, accuracy/top1 = 0.94
I0801 14:22:25.433123  1577 caffe.cpp:313] Batch 79, accuracy/top5 = 0.98
I0801 14:22:25.433126  1577 caffe.cpp:313] Batch 79, loss = 0.280822
I0801 14:22:25.441561  1577 caffe.cpp:313] Batch 80, accuracy/top1 = 0.9
I0801 14:22:25.441568  1577 caffe.cpp:313] Batch 80, accuracy/top5 = 1
I0801 14:22:25.441571  1577 caffe.cpp:313] Batch 80, loss = 0.51554
I0801 14:22:25.450034  1577 caffe.cpp:313] Batch 81, accuracy/top1 = 0.9
I0801 14:22:25.450042  1577 caffe.cpp:313] Batch 81, accuracy/top5 = 1
I0801 14:22:25.450043  1577 caffe.cpp:313] Batch 81, loss = 0.257762
I0801 14:22:25.458503  1577 caffe.cpp:313] Batch 82, accuracy/top1 = 0.9
I0801 14:22:25.458511  1577 caffe.cpp:313] Batch 82, accuracy/top5 = 0.98
I0801 14:22:25.458514  1577 caffe.cpp:313] Batch 82, loss = 0.649744
I0801 14:22:25.467010  1577 caffe.cpp:313] Batch 83, accuracy/top1 = 0.94
I0801 14:22:25.467017  1577 caffe.cpp:313] Batch 83, accuracy/top5 = 1
I0801 14:22:25.467020  1577 caffe.cpp:313] Batch 83, loss = 0.197182
I0801 14:22:25.475471  1577 caffe.cpp:313] Batch 84, accuracy/top1 = 0.94
I0801 14:22:25.475481  1577 caffe.cpp:313] Batch 84, accuracy/top5 = 1
I0801 14:22:25.475483  1577 caffe.cpp:313] Batch 84, loss = 0.146333
I0801 14:22:25.484030  1577 caffe.cpp:313] Batch 85, accuracy/top1 = 0.92
I0801 14:22:25.484045  1577 caffe.cpp:313] Batch 85, accuracy/top5 = 1
I0801 14:22:25.484046  1577 caffe.cpp:313] Batch 85, loss = 0.412278
I0801 14:22:25.492525  1577 caffe.cpp:313] Batch 86, accuracy/top1 = 0.86
I0801 14:22:25.492533  1577 caffe.cpp:313] Batch 86, accuracy/top5 = 1
I0801 14:22:25.492537  1577 caffe.cpp:313] Batch 86, loss = 0.724727
I0801 14:22:25.501010  1577 caffe.cpp:313] Batch 87, accuracy/top1 = 0.96
I0801 14:22:25.501019  1577 caffe.cpp:313] Batch 87, accuracy/top5 = 1
I0801 14:22:25.501029  1577 caffe.cpp:313] Batch 87, loss = 0.129772
I0801 14:22:25.509490  1577 caffe.cpp:313] Batch 88, accuracy/top1 = 0.92
I0801 14:22:25.509497  1577 caffe.cpp:313] Batch 88, accuracy/top5 = 1
I0801 14:22:25.509500  1577 caffe.cpp:313] Batch 88, loss = 0.287431
I0801 14:22:25.517985  1577 caffe.cpp:313] Batch 89, accuracy/top1 = 0.9
I0801 14:22:25.517992  1577 caffe.cpp:313] Batch 89, accuracy/top5 = 1
I0801 14:22:25.517994  1577 caffe.cpp:313] Batch 89, loss = 0.304062
I0801 14:22:25.526437  1577 caffe.cpp:313] Batch 90, accuracy/top1 = 0.92
I0801 14:22:25.526444  1577 caffe.cpp:313] Batch 90, accuracy/top5 = 1
I0801 14:22:25.526446  1577 caffe.cpp:313] Batch 90, loss = 0.270963
I0801 14:22:25.534925  1577 caffe.cpp:313] Batch 91, accuracy/top1 = 0.9
I0801 14:22:25.534932  1577 caffe.cpp:313] Batch 91, accuracy/top5 = 0.98
I0801 14:22:25.534935  1577 caffe.cpp:313] Batch 91, loss = 0.274124
I0801 14:22:25.543360  1577 caffe.cpp:313] Batch 92, accuracy/top1 = 0.8
I0801 14:22:25.543367  1577 caffe.cpp:313] Batch 92, accuracy/top5 = 1
I0801 14:22:25.543370  1577 caffe.cpp:313] Batch 92, loss = 0.704639
I0801 14:22:25.551856  1577 caffe.cpp:313] Batch 93, accuracy/top1 = 0.96
I0801 14:22:25.551862  1577 caffe.cpp:313] Batch 93, accuracy/top5 = 1
I0801 14:22:25.551865  1577 caffe.cpp:313] Batch 93, loss = 0.116413
I0801 14:22:25.560314  1577 caffe.cpp:313] Batch 94, accuracy/top1 = 0.82
I0801 14:22:25.560322  1577 caffe.cpp:313] Batch 94, accuracy/top5 = 1
I0801 14:22:25.560324  1577 caffe.cpp:313] Batch 94, loss = 0.522466
I0801 14:22:25.568827  1577 caffe.cpp:313] Batch 95, accuracy/top1 = 0.9
I0801 14:22:25.568835  1577 caffe.cpp:313] Batch 95, accuracy/top5 = 0.98
I0801 14:22:25.568837  1577 caffe.cpp:313] Batch 95, loss = 0.462427
I0801 14:22:25.577277  1577 caffe.cpp:313] Batch 96, accuracy/top1 = 1
I0801 14:22:25.577286  1577 caffe.cpp:313] Batch 96, accuracy/top5 = 1
I0801 14:22:25.577288  1577 caffe.cpp:313] Batch 96, loss = 0.0223844
I0801 14:22:25.585789  1577 caffe.cpp:313] Batch 97, accuracy/top1 = 0.96
I0801 14:22:25.585798  1577 caffe.cpp:313] Batch 97, accuracy/top5 = 1
I0801 14:22:25.585801  1577 caffe.cpp:313] Batch 97, loss = 0.0955476
I0801 14:22:25.594269  1577 caffe.cpp:313] Batch 98, accuracy/top1 = 0.88
I0801 14:22:25.594279  1577 caffe.cpp:313] Batch 98, accuracy/top5 = 1
I0801 14:22:25.594280  1577 caffe.cpp:313] Batch 98, loss = 0.446247
I0801 14:22:25.602787  1577 caffe.cpp:313] Batch 99, accuracy/top1 = 0.84
I0801 14:22:25.602794  1577 caffe.cpp:313] Batch 99, accuracy/top5 = 0.98
I0801 14:22:25.602797  1577 caffe.cpp:313] Batch 99, loss = 0.815701
I0801 14:22:25.611239  1577 caffe.cpp:313] Batch 100, accuracy/top1 = 0.94
I0801 14:22:25.611246  1577 caffe.cpp:313] Batch 100, accuracy/top5 = 1
I0801 14:22:25.611248  1577 caffe.cpp:313] Batch 100, loss = 0.223333
I0801 14:22:25.619709  1577 caffe.cpp:313] Batch 101, accuracy/top1 = 0.9
I0801 14:22:25.619717  1577 caffe.cpp:313] Batch 101, accuracy/top5 = 1
I0801 14:22:25.619719  1577 caffe.cpp:313] Batch 101, loss = 0.449036
I0801 14:22:25.628176  1577 caffe.cpp:313] Batch 102, accuracy/top1 = 0.92
I0801 14:22:25.628185  1577 caffe.cpp:313] Batch 102, accuracy/top5 = 1
I0801 14:22:25.628186  1577 caffe.cpp:313] Batch 102, loss = 0.212118
I0801 14:22:25.636616  1577 caffe.cpp:313] Batch 103, accuracy/top1 = 0.88
I0801 14:22:25.636623  1577 caffe.cpp:313] Batch 103, accuracy/top5 = 1
I0801 14:22:25.636626  1577 caffe.cpp:313] Batch 103, loss = 0.41664
I0801 14:22:25.645107  1577 caffe.cpp:313] Batch 104, accuracy/top1 = 0.86
I0801 14:22:25.645117  1577 caffe.cpp:313] Batch 104, accuracy/top5 = 1
I0801 14:22:25.645119  1577 caffe.cpp:313] Batch 104, loss = 0.605479
I0801 14:22:25.653651  1577 caffe.cpp:313] Batch 105, accuracy/top1 = 0.94
I0801 14:22:25.653671  1577 caffe.cpp:313] Batch 105, accuracy/top5 = 1
I0801 14:22:25.653672  1577 caffe.cpp:313] Batch 105, loss = 0.339937
I0801 14:22:25.662242  1577 caffe.cpp:313] Batch 106, accuracy/top1 = 0.96
I0801 14:22:25.662261  1577 caffe.cpp:313] Batch 106, accuracy/top5 = 1
I0801 14:22:25.662276  1577 caffe.cpp:313] Batch 106, loss = 0.0952518
I0801 14:22:25.670784  1577 caffe.cpp:313] Batch 107, accuracy/top1 = 0.88
I0801 14:22:25.670797  1577 caffe.cpp:313] Batch 107, accuracy/top5 = 1
I0801 14:22:25.670800  1577 caffe.cpp:313] Batch 107, loss = 0.398468
I0801 14:22:25.679385  1577 caffe.cpp:313] Batch 108, accuracy/top1 = 0.86
I0801 14:22:25.679409  1577 caffe.cpp:313] Batch 108, accuracy/top5 = 1
I0801 14:22:25.679411  1577 caffe.cpp:313] Batch 108, loss = 0.707825
I0801 14:22:25.687960  1577 caffe.cpp:313] Batch 109, accuracy/top1 = 0.96
I0801 14:22:25.687973  1577 caffe.cpp:313] Batch 109, accuracy/top5 = 1
I0801 14:22:25.687975  1577 caffe.cpp:313] Batch 109, loss = 0.190648
I0801 14:22:25.696530  1577 caffe.cpp:313] Batch 110, accuracy/top1 = 0.86
I0801 14:22:25.696537  1577 caffe.cpp:313] Batch 110, accuracy/top5 = 1
I0801 14:22:25.696540  1577 caffe.cpp:313] Batch 110, loss = 0.918479
I0801 14:22:25.704998  1577 caffe.cpp:313] Batch 111, accuracy/top1 = 0.98
I0801 14:22:25.705006  1577 caffe.cpp:313] Batch 111, accuracy/top5 = 1
I0801 14:22:25.705008  1577 caffe.cpp:313] Batch 111, loss = 0.122347
I0801 14:22:25.713531  1577 caffe.cpp:313] Batch 112, accuracy/top1 = 0.9
I0801 14:22:25.713539  1577 caffe.cpp:313] Batch 112, accuracy/top5 = 1
I0801 14:22:25.713542  1577 caffe.cpp:313] Batch 112, loss = 0.536039
I0801 14:22:25.722008  1577 caffe.cpp:313] Batch 113, accuracy/top1 = 0.9
I0801 14:22:25.722019  1577 caffe.cpp:313] Batch 113, accuracy/top5 = 1
I0801 14:22:25.722023  1577 caffe.cpp:313] Batch 113, loss = 0.287984
I0801 14:22:25.730525  1577 caffe.cpp:313] Batch 114, accuracy/top1 = 0.92
I0801 14:22:25.730532  1577 caffe.cpp:313] Batch 114, accuracy/top5 = 1
I0801 14:22:25.730535  1577 caffe.cpp:313] Batch 114, loss = 0.426622
I0801 14:22:25.738987  1577 caffe.cpp:313] Batch 115, accuracy/top1 = 1
I0801 14:22:25.738994  1577 caffe.cpp:313] Batch 115, accuracy/top5 = 1
I0801 14:22:25.738997  1577 caffe.cpp:313] Batch 115, loss = 0.0138095
I0801 14:22:25.747460  1577 caffe.cpp:313] Batch 116, accuracy/top1 = 0.86
I0801 14:22:25.747467  1577 caffe.cpp:313] Batch 116, accuracy/top5 = 1
I0801 14:22:25.747470  1577 caffe.cpp:313] Batch 116, loss = 0.692753
I0801 14:22:25.755919  1577 caffe.cpp:313] Batch 117, accuracy/top1 = 0.82
I0801 14:22:25.755926  1577 caffe.cpp:313] Batch 117, accuracy/top5 = 1
I0801 14:22:25.755929  1577 caffe.cpp:313] Batch 117, loss = 0.647026
I0801 14:22:25.764395  1577 caffe.cpp:313] Batch 118, accuracy/top1 = 0.84
I0801 14:22:25.764403  1577 caffe.cpp:313] Batch 118, accuracy/top5 = 1
I0801 14:22:25.764406  1577 caffe.cpp:313] Batch 118, loss = 0.583526
I0801 14:22:25.772868  1577 caffe.cpp:313] Batch 119, accuracy/top1 = 0.94
I0801 14:22:25.772876  1577 caffe.cpp:313] Batch 119, accuracy/top5 = 1
I0801 14:22:25.772878  1577 caffe.cpp:313] Batch 119, loss = 0.207519
I0801 14:22:25.781399  1577 caffe.cpp:313] Batch 120, accuracy/top1 = 0.86
I0801 14:22:25.781409  1577 caffe.cpp:313] Batch 120, accuracy/top5 = 1
I0801 14:22:25.781411  1577 caffe.cpp:313] Batch 120, loss = 0.705631
I0801 14:22:25.789876  1577 caffe.cpp:313] Batch 121, accuracy/top1 = 0.92
I0801 14:22:25.789885  1577 caffe.cpp:313] Batch 121, accuracy/top5 = 1
I0801 14:22:25.789888  1577 caffe.cpp:313] Batch 121, loss = 0.25864
I0801 14:22:25.798405  1577 caffe.cpp:313] Batch 122, accuracy/top1 = 0.88
I0801 14:22:25.798414  1577 caffe.cpp:313] Batch 122, accuracy/top5 = 0.98
I0801 14:22:25.798416  1577 caffe.cpp:313] Batch 122, loss = 0.509848
I0801 14:22:25.806881  1577 caffe.cpp:313] Batch 123, accuracy/top1 = 0.88
I0801 14:22:25.806892  1577 caffe.cpp:313] Batch 123, accuracy/top5 = 1
I0801 14:22:25.806895  1577 caffe.cpp:313] Batch 123, loss = 0.555059
I0801 14:22:25.815358  1577 caffe.cpp:313] Batch 124, accuracy/top1 = 0.98
I0801 14:22:25.815366  1577 caffe.cpp:313] Batch 124, accuracy/top5 = 1
I0801 14:22:25.815369  1577 caffe.cpp:313] Batch 124, loss = 0.157121
I0801 14:22:25.823832  1577 caffe.cpp:313] Batch 125, accuracy/top1 = 0.92
I0801 14:22:25.823848  1577 caffe.cpp:313] Batch 125, accuracy/top5 = 1
I0801 14:22:25.823851  1577 caffe.cpp:313] Batch 125, loss = 0.401237
I0801 14:22:25.832337  1577 caffe.cpp:313] Batch 126, accuracy/top1 = 0.98
I0801 14:22:25.832346  1577 caffe.cpp:313] Batch 126, accuracy/top5 = 1
I0801 14:22:25.832350  1577 caffe.cpp:313] Batch 126, loss = 0.0320846
I0801 14:22:25.840831  1577 caffe.cpp:313] Batch 127, accuracy/top1 = 0.92
I0801 14:22:25.840842  1577 caffe.cpp:313] Batch 127, accuracy/top5 = 1
I0801 14:22:25.840844  1577 caffe.cpp:313] Batch 127, loss = 0.267961
I0801 14:22:25.849388  1577 caffe.cpp:313] Batch 128, accuracy/top1 = 0.88
I0801 14:22:25.849406  1577 caffe.cpp:313] Batch 128, accuracy/top5 = 1
I0801 14:22:25.849408  1577 caffe.cpp:313] Batch 128, loss = 0.296094
I0801 14:22:25.857969  1577 caffe.cpp:313] Batch 129, accuracy/top1 = 0.94
I0801 14:22:25.857991  1577 caffe.cpp:313] Batch 129, accuracy/top5 = 1
I0801 14:22:25.857995  1577 caffe.cpp:313] Batch 129, loss = 0.246574
I0801 14:22:25.866559  1577 caffe.cpp:313] Batch 130, accuracy/top1 = 0.92
I0801 14:22:25.866574  1577 caffe.cpp:313] Batch 130, accuracy/top5 = 1
I0801 14:22:25.866576  1577 caffe.cpp:313] Batch 130, loss = 0.342205
I0801 14:22:25.875061  1577 caffe.cpp:313] Batch 131, accuracy/top1 = 0.88
I0801 14:22:25.875069  1577 caffe.cpp:313] Batch 131, accuracy/top5 = 1
I0801 14:22:25.875072  1577 caffe.cpp:313] Batch 131, loss = 0.444162
I0801 14:22:25.883575  1577 caffe.cpp:313] Batch 132, accuracy/top1 = 0.94
I0801 14:22:25.883584  1577 caffe.cpp:313] Batch 132, accuracy/top5 = 1
I0801 14:22:25.883586  1577 caffe.cpp:313] Batch 132, loss = 0.134834
I0801 14:22:25.892045  1577 caffe.cpp:313] Batch 133, accuracy/top1 = 0.94
I0801 14:22:25.892053  1577 caffe.cpp:313] Batch 133, accuracy/top5 = 1
I0801 14:22:25.892056  1577 caffe.cpp:313] Batch 133, loss = 0.246676
I0801 14:22:25.900565  1577 caffe.cpp:313] Batch 134, accuracy/top1 = 0.88
I0801 14:22:25.900575  1577 caffe.cpp:313] Batch 134, accuracy/top5 = 1
I0801 14:22:25.900578  1577 caffe.cpp:313] Batch 134, loss = 0.627018
I0801 14:22:25.909039  1577 caffe.cpp:313] Batch 135, accuracy/top1 = 0.78
I0801 14:22:25.909046  1577 caffe.cpp:313] Batch 135, accuracy/top5 = 1
I0801 14:22:25.909049  1577 caffe.cpp:313] Batch 135, loss = 0.41648
I0801 14:22:25.917526  1577 caffe.cpp:313] Batch 136, accuracy/top1 = 0.96
I0801 14:22:25.917534  1577 caffe.cpp:313] Batch 136, accuracy/top5 = 1
I0801 14:22:25.917536  1577 caffe.cpp:313] Batch 136, loss = 0.179598
I0801 14:22:25.925977  1577 caffe.cpp:313] Batch 137, accuracy/top1 = 0.8
I0801 14:22:25.925985  1577 caffe.cpp:313] Batch 137, accuracy/top5 = 1
I0801 14:22:25.925988  1577 caffe.cpp:313] Batch 137, loss = 0.706326
I0801 14:22:25.934454  1577 caffe.cpp:313] Batch 138, accuracy/top1 = 0.9
I0801 14:22:25.934461  1577 caffe.cpp:313] Batch 138, accuracy/top5 = 1
I0801 14:22:25.934464  1577 caffe.cpp:313] Batch 138, loss = 0.394024
I0801 14:22:25.942904  1577 caffe.cpp:313] Batch 139, accuracy/top1 = 0.92
I0801 14:22:25.942914  1577 caffe.cpp:313] Batch 139, accuracy/top5 = 1
I0801 14:22:25.942915  1577 caffe.cpp:313] Batch 139, loss = 0.430421
I0801 14:22:25.951432  1577 caffe.cpp:313] Batch 140, accuracy/top1 = 0.86
I0801 14:22:25.951441  1577 caffe.cpp:313] Batch 140, accuracy/top5 = 1
I0801 14:22:25.951443  1577 caffe.cpp:313] Batch 140, loss = 0.674657
I0801 14:22:25.959897  1577 caffe.cpp:313] Batch 141, accuracy/top1 = 0.96
I0801 14:22:25.959904  1577 caffe.cpp:313] Batch 141, accuracy/top5 = 1
I0801 14:22:25.959908  1577 caffe.cpp:313] Batch 141, loss = 0.31229
I0801 14:22:25.968390  1577 caffe.cpp:313] Batch 142, accuracy/top1 = 0.92
I0801 14:22:25.968399  1577 caffe.cpp:313] Batch 142, accuracy/top5 = 1
I0801 14:22:25.968400  1577 caffe.cpp:313] Batch 142, loss = 0.219965
I0801 14:22:25.976846  1577 caffe.cpp:313] Batch 143, accuracy/top1 = 0.92
I0801 14:22:25.976853  1577 caffe.cpp:313] Batch 143, accuracy/top5 = 1
I0801 14:22:25.976855  1577 caffe.cpp:313] Batch 143, loss = 0.354975
I0801 14:22:25.985328  1577 caffe.cpp:313] Batch 144, accuracy/top1 = 0.98
I0801 14:22:25.985342  1577 caffe.cpp:313] Batch 144, accuracy/top5 = 1
I0801 14:22:25.985345  1577 caffe.cpp:313] Batch 144, loss = 0.0624649
I0801 14:22:25.993803  1577 caffe.cpp:313] Batch 145, accuracy/top1 = 0.96
I0801 14:22:25.993810  1577 caffe.cpp:313] Batch 145, accuracy/top5 = 1
I0801 14:22:25.993813  1577 caffe.cpp:313] Batch 145, loss = 0.307891
I0801 14:22:26.002313  1577 caffe.cpp:313] Batch 146, accuracy/top1 = 0.94
I0801 14:22:26.002321  1577 caffe.cpp:313] Batch 146, accuracy/top5 = 1
I0801 14:22:26.002322  1577 caffe.cpp:313] Batch 146, loss = 0.242185
I0801 14:22:26.010942  1577 caffe.cpp:313] Batch 147, accuracy/top1 = 0.94
I0801 14:22:26.010962  1577 caffe.cpp:313] Batch 147, accuracy/top5 = 1
I0801 14:22:26.010963  1577 caffe.cpp:313] Batch 147, loss = 0.24384
I0801 14:22:26.019601  1577 caffe.cpp:313] Batch 148, accuracy/top1 = 0.92
I0801 14:22:26.019618  1577 caffe.cpp:313] Batch 148, accuracy/top5 = 1
I0801 14:22:26.019620  1577 caffe.cpp:313] Batch 148, loss = 0.314471
I0801 14:22:26.028146  1577 caffe.cpp:313] Batch 149, accuracy/top1 = 0.94
I0801 14:22:26.028156  1577 caffe.cpp:313] Batch 149, accuracy/top5 = 1
I0801 14:22:26.028159  1577 caffe.cpp:313] Batch 149, loss = 0.399393
I0801 14:22:26.036638  1577 caffe.cpp:313] Batch 150, accuracy/top1 = 0.94
I0801 14:22:26.036646  1577 caffe.cpp:313] Batch 150, accuracy/top5 = 0.98
I0801 14:22:26.036649  1577 caffe.cpp:313] Batch 150, loss = 0.227165
I0801 14:22:26.045145  1577 caffe.cpp:313] Batch 151, accuracy/top1 = 0.88
I0801 14:22:26.045152  1577 caffe.cpp:313] Batch 151, accuracy/top5 = 0.98
I0801 14:22:26.045155  1577 caffe.cpp:313] Batch 151, loss = 0.475866
I0801 14:22:26.053602  1577 caffe.cpp:313] Batch 152, accuracy/top1 = 0.88
I0801 14:22:26.053609  1577 caffe.cpp:313] Batch 152, accuracy/top5 = 1
I0801 14:22:26.053612  1577 caffe.cpp:313] Batch 152, loss = 0.31968
I0801 14:22:26.062089  1577 caffe.cpp:313] Batch 153, accuracy/top1 = 0.92
I0801 14:22:26.062096  1577 caffe.cpp:313] Batch 153, accuracy/top5 = 0.98
I0801 14:22:26.062099  1577 caffe.cpp:313] Batch 153, loss = 0.713667
I0801 14:22:26.070569  1577 caffe.cpp:313] Batch 154, accuracy/top1 = 0.96
I0801 14:22:26.070576  1577 caffe.cpp:313] Batch 154, accuracy/top5 = 1
I0801 14:22:26.070580  1577 caffe.cpp:313] Batch 154, loss = 0.195953
I0801 14:22:26.079041  1577 caffe.cpp:313] Batch 155, accuracy/top1 = 0.88
I0801 14:22:26.079047  1577 caffe.cpp:313] Batch 155, accuracy/top5 = 0.98
I0801 14:22:26.079049  1577 caffe.cpp:313] Batch 155, loss = 0.760813
I0801 14:22:26.087499  1577 caffe.cpp:313] Batch 156, accuracy/top1 = 0.92
I0801 14:22:26.087507  1577 caffe.cpp:313] Batch 156, accuracy/top5 = 0.98
I0801 14:22:26.087508  1577 caffe.cpp:313] Batch 156, loss = 0.222407
I0801 14:22:26.095985  1577 caffe.cpp:313] Batch 157, accuracy/top1 = 0.92
I0801 14:22:26.095993  1577 caffe.cpp:313] Batch 157, accuracy/top5 = 1
I0801 14:22:26.095995  1577 caffe.cpp:313] Batch 157, loss = 0.60225
I0801 14:22:26.104430  1577 caffe.cpp:313] Batch 158, accuracy/top1 = 0.92
I0801 14:22:26.104437  1577 caffe.cpp:313] Batch 158, accuracy/top5 = 1
I0801 14:22:26.104440  1577 caffe.cpp:313] Batch 158, loss = 0.206864
I0801 14:22:26.112910  1577 caffe.cpp:313] Batch 159, accuracy/top1 = 0.9
I0801 14:22:26.112917  1577 caffe.cpp:313] Batch 159, accuracy/top5 = 0.98
I0801 14:22:26.112920  1577 caffe.cpp:313] Batch 159, loss = 0.351562
I0801 14:22:26.121377  1577 caffe.cpp:313] Batch 160, accuracy/top1 = 0.92
I0801 14:22:26.121384  1577 caffe.cpp:313] Batch 160, accuracy/top5 = 1
I0801 14:22:26.121387  1577 caffe.cpp:313] Batch 160, loss = 0.253498
I0801 14:22:26.129871  1577 caffe.cpp:313] Batch 161, accuracy/top1 = 0.94
I0801 14:22:26.129878  1577 caffe.cpp:313] Batch 161, accuracy/top5 = 1
I0801 14:22:26.129881  1577 caffe.cpp:313] Batch 161, loss = 0.348168
I0801 14:22:26.138341  1577 caffe.cpp:313] Batch 162, accuracy/top1 = 0.88
I0801 14:22:26.138348  1577 caffe.cpp:313] Batch 162, accuracy/top5 = 1
I0801 14:22:26.138350  1577 caffe.cpp:313] Batch 162, loss = 0.389248
I0801 14:22:26.146848  1577 caffe.cpp:313] Batch 163, accuracy/top1 = 0.9
I0801 14:22:26.146864  1577 caffe.cpp:313] Batch 163, accuracy/top5 = 0.98
I0801 14:22:26.146867  1577 caffe.cpp:313] Batch 163, loss = 0.641134
I0801 14:22:26.155324  1577 caffe.cpp:313] Batch 164, accuracy/top1 = 0.92
I0801 14:22:26.155331  1577 caffe.cpp:313] Batch 164, accuracy/top5 = 1
I0801 14:22:26.155334  1577 caffe.cpp:313] Batch 164, loss = 0.218051
I0801 14:22:26.163827  1577 caffe.cpp:313] Batch 165, accuracy/top1 = 0.86
I0801 14:22:26.163836  1577 caffe.cpp:313] Batch 165, accuracy/top5 = 1
I0801 14:22:26.163837  1577 caffe.cpp:313] Batch 165, loss = 0.704406
I0801 14:22:26.172266  1577 caffe.cpp:313] Batch 166, accuracy/top1 = 0.86
I0801 14:22:26.172273  1577 caffe.cpp:313] Batch 166, accuracy/top5 = 1
I0801 14:22:26.172276  1577 caffe.cpp:313] Batch 166, loss = 0.378903
I0801 14:22:26.180763  1577 caffe.cpp:313] Batch 167, accuracy/top1 = 0.92
I0801 14:22:26.180769  1577 caffe.cpp:313] Batch 167, accuracy/top5 = 1
I0801 14:22:26.180773  1577 caffe.cpp:313] Batch 167, loss = 0.396927
I0801 14:22:26.189230  1577 caffe.cpp:313] Batch 168, accuracy/top1 = 0.86
I0801 14:22:26.189237  1577 caffe.cpp:313] Batch 168, accuracy/top5 = 1
I0801 14:22:26.189239  1577 caffe.cpp:313] Batch 168, loss = 0.613174
I0801 14:22:26.197679  1577 caffe.cpp:313] Batch 169, accuracy/top1 = 0.82
I0801 14:22:26.197685  1577 caffe.cpp:313] Batch 169, accuracy/top5 = 1
I0801 14:22:26.197688  1577 caffe.cpp:313] Batch 169, loss = 0.502842
I0801 14:22:26.206110  1577 caffe.cpp:313] Batch 170, accuracy/top1 = 0.82
I0801 14:22:26.206117  1577 caffe.cpp:313] Batch 170, accuracy/top5 = 1
I0801 14:22:26.206120  1577 caffe.cpp:313] Batch 170, loss = 0.512811
I0801 14:22:26.214612  1577 caffe.cpp:313] Batch 171, accuracy/top1 = 0.88
I0801 14:22:26.214619  1577 caffe.cpp:313] Batch 171, accuracy/top5 = 1
I0801 14:22:26.214622  1577 caffe.cpp:313] Batch 171, loss = 0.685794
I0801 14:22:26.223063  1577 caffe.cpp:313] Batch 172, accuracy/top1 = 0.94
I0801 14:22:26.223071  1577 caffe.cpp:313] Batch 172, accuracy/top5 = 1
I0801 14:22:26.223073  1577 caffe.cpp:313] Batch 172, loss = 0.173031
I0801 14:22:26.231554  1577 caffe.cpp:313] Batch 173, accuracy/top1 = 0.94
I0801 14:22:26.231560  1577 caffe.cpp:313] Batch 173, accuracy/top5 = 1
I0801 14:22:26.231562  1577 caffe.cpp:313] Batch 173, loss = 0.220795
I0801 14:22:26.240012  1577 caffe.cpp:313] Batch 174, accuracy/top1 = 0.88
I0801 14:22:26.240020  1577 caffe.cpp:313] Batch 174, accuracy/top5 = 1
I0801 14:22:26.240022  1577 caffe.cpp:313] Batch 174, loss = 0.704571
I0801 14:22:26.248482  1577 caffe.cpp:313] Batch 175, accuracy/top1 = 0.9
I0801 14:22:26.248489  1577 caffe.cpp:313] Batch 175, accuracy/top5 = 0.98
I0801 14:22:26.248492  1577 caffe.cpp:313] Batch 175, loss = 0.409615
I0801 14:22:26.256923  1577 caffe.cpp:313] Batch 176, accuracy/top1 = 0.86
I0801 14:22:26.256930  1577 caffe.cpp:313] Batch 176, accuracy/top5 = 1
I0801 14:22:26.256932  1577 caffe.cpp:313] Batch 176, loss = 0.474941
I0801 14:22:26.265375  1577 caffe.cpp:313] Batch 177, accuracy/top1 = 0.9
I0801 14:22:26.265383  1577 caffe.cpp:313] Batch 177, accuracy/top5 = 1
I0801 14:22:26.265385  1577 caffe.cpp:313] Batch 177, loss = 0.278935
I0801 14:22:26.273818  1577 caffe.cpp:313] Batch 178, accuracy/top1 = 0.84
I0801 14:22:26.273824  1577 caffe.cpp:313] Batch 178, accuracy/top5 = 1
I0801 14:22:26.273828  1577 caffe.cpp:313] Batch 178, loss = 0.64587
I0801 14:22:26.282302  1577 caffe.cpp:313] Batch 179, accuracy/top1 = 0.88
I0801 14:22:26.282311  1577 caffe.cpp:313] Batch 179, accuracy/top5 = 1
I0801 14:22:26.282315  1577 caffe.cpp:313] Batch 179, loss = 0.470233
I0801 14:22:26.290768  1577 caffe.cpp:313] Batch 180, accuracy/top1 = 0.92
I0801 14:22:26.290776  1577 caffe.cpp:313] Batch 180, accuracy/top5 = 1
I0801 14:22:26.290778  1577 caffe.cpp:313] Batch 180, loss = 0.221331
I0801 14:22:26.299262  1577 caffe.cpp:313] Batch 181, accuracy/top1 = 0.88
I0801 14:22:26.299268  1577 caffe.cpp:313] Batch 181, accuracy/top5 = 0.98
I0801 14:22:26.299270  1577 caffe.cpp:313] Batch 181, loss = 0.471364
I0801 14:22:26.307718  1577 caffe.cpp:313] Batch 182, accuracy/top1 = 0.82
I0801 14:22:26.307725  1577 caffe.cpp:313] Batch 182, accuracy/top5 = 1
I0801 14:22:26.307729  1577 caffe.cpp:313] Batch 182, loss = 0.402982
I0801 14:22:26.316197  1577 caffe.cpp:313] Batch 183, accuracy/top1 = 0.94
I0801 14:22:26.316206  1577 caffe.cpp:313] Batch 183, accuracy/top5 = 1
I0801 14:22:26.316208  1577 caffe.cpp:313] Batch 183, loss = 0.144772
I0801 14:22:26.324651  1577 caffe.cpp:313] Batch 184, accuracy/top1 = 0.82
I0801 14:22:26.324661  1577 caffe.cpp:313] Batch 184, accuracy/top5 = 1
I0801 14:22:26.324662  1577 caffe.cpp:313] Batch 184, loss = 0.685406
I0801 14:22:26.333135  1577 caffe.cpp:313] Batch 185, accuracy/top1 = 0.88
I0801 14:22:26.333143  1577 caffe.cpp:313] Batch 185, accuracy/top5 = 1
I0801 14:22:26.333145  1577 caffe.cpp:313] Batch 185, loss = 0.341016
I0801 14:22:26.341583  1577 caffe.cpp:313] Batch 186, accuracy/top1 = 0.92
I0801 14:22:26.341590  1577 caffe.cpp:313] Batch 186, accuracy/top5 = 1
I0801 14:22:26.341593  1577 caffe.cpp:313] Batch 186, loss = 0.292856
I0801 14:22:26.350066  1577 caffe.cpp:313] Batch 187, accuracy/top1 = 0.88
I0801 14:22:26.350075  1577 caffe.cpp:313] Batch 187, accuracy/top5 = 1
I0801 14:22:26.350078  1577 caffe.cpp:313] Batch 187, loss = 0.500002
I0801 14:22:26.358552  1577 caffe.cpp:313] Batch 188, accuracy/top1 = 0.9
I0801 14:22:26.358559  1577 caffe.cpp:313] Batch 188, accuracy/top5 = 1
I0801 14:22:26.358562  1577 caffe.cpp:313] Batch 188, loss = 0.396199
I0801 14:22:26.367048  1577 caffe.cpp:313] Batch 189, accuracy/top1 = 0.9
I0801 14:22:26.367055  1577 caffe.cpp:313] Batch 189, accuracy/top5 = 1
I0801 14:22:26.367058  1577 caffe.cpp:313] Batch 189, loss = 0.137807
I0801 14:22:26.375501  1577 caffe.cpp:313] Batch 190, accuracy/top1 = 0.84
I0801 14:22:26.375509  1577 caffe.cpp:313] Batch 190, accuracy/top5 = 1
I0801 14:22:26.375510  1577 caffe.cpp:313] Batch 190, loss = 0.436137
I0801 14:22:26.384017  1577 caffe.cpp:313] Batch 191, accuracy/top1 = 0.92
I0801 14:22:26.384024  1577 caffe.cpp:313] Batch 191, accuracy/top5 = 1
I0801 14:22:26.384027  1577 caffe.cpp:313] Batch 191, loss = 0.337901
I0801 14:22:26.392458  1577 caffe.cpp:313] Batch 192, accuracy/top1 = 0.92
I0801 14:22:26.392467  1577 caffe.cpp:313] Batch 192, accuracy/top5 = 1
I0801 14:22:26.392468  1577 caffe.cpp:313] Batch 192, loss = 0.390136
I0801 14:22:26.400940  1577 caffe.cpp:313] Batch 193, accuracy/top1 = 0.96
I0801 14:22:26.400949  1577 caffe.cpp:313] Batch 193, accuracy/top5 = 1
I0801 14:22:26.400952  1577 caffe.cpp:313] Batch 193, loss = 0.243811
I0801 14:22:26.409449  1577 caffe.cpp:313] Batch 194, accuracy/top1 = 0.86
I0801 14:22:26.409462  1577 caffe.cpp:313] Batch 194, accuracy/top5 = 0.98
I0801 14:22:26.409466  1577 caffe.cpp:313] Batch 194, loss = 1.03322
I0801 14:22:26.418025  1577 caffe.cpp:313] Batch 195, accuracy/top1 = 0.92
I0801 14:22:26.418035  1577 caffe.cpp:313] Batch 195, accuracy/top5 = 1
I0801 14:22:26.418037  1577 caffe.cpp:313] Batch 195, loss = 0.294765
I0801 14:22:26.426539  1577 caffe.cpp:313] Batch 196, accuracy/top1 = 0.86
I0801 14:22:26.426553  1577 caffe.cpp:313] Batch 196, accuracy/top5 = 1
I0801 14:22:26.426556  1577 caffe.cpp:313] Batch 196, loss = 0.875881
I0801 14:22:26.426968  1605 data_reader.cpp:264] Starting prefetch of epoch 1
I0801 14:22:26.435135  1577 caffe.cpp:313] Batch 197, accuracy/top1 = 0.94
I0801 14:22:26.435150  1577 caffe.cpp:313] Batch 197, accuracy/top5 = 1
I0801 14:22:26.435153  1577 caffe.cpp:313] Batch 197, loss = 0.150467
I0801 14:22:26.443701  1577 caffe.cpp:313] Batch 198, accuracy/top1 = 0.94
I0801 14:22:26.443714  1577 caffe.cpp:313] Batch 198, accuracy/top5 = 1
I0801 14:22:26.443717  1577 caffe.cpp:313] Batch 198, loss = 0.146874
I0801 14:22:26.452262  1577 caffe.cpp:313] Batch 199, accuracy/top1 = 0.92
I0801 14:22:26.452273  1577 caffe.cpp:313] Batch 199, accuracy/top5 = 1
I0801 14:22:26.452276  1577 caffe.cpp:313] Batch 199, loss = 0.411661
I0801 14:22:26.452278  1577 caffe.cpp:318] Loss: 0.406073
I0801 14:22:26.452286  1577 caffe.cpp:330] accuracy/top1 = 0.9042
I0801 14:22:26.452298  1577 caffe.cpp:330] accuracy/top5 = 0.9958
I0801 14:22:26.452304  1577 caffe.cpp:330] loss = 0.406073 (* 1 = 0.406073 loss)
./train_cifar10_classification.sh: line 120: training/cifar10_jacintonet11v2_2017-08-01_13-11-28/train-log_2017-08-01_13-11-28.txt/run.sh: Not a directory
