Logging output to training/cifar10_jacintonet11v2_2017-06-30_01-13-02/train-log_2017-06-30_01-13-02.txt
WARNING: gnome-keyring:: couldn't connect to: /run/user/30409/keyring-KJvviu/pkcs11: Connection refused
p11-kit: skipping module 'gnome-keyring' whose initialization failed: An error occurred on the device
I0630 01:13:03.212628 21943 caffe.cpp:209] Using GPUs 0, 1, 2
I0630 01:13:03.213093 21943 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0630 01:13:03.213426 21943 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0630 01:13:03.213764 21943 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0630 01:13:03.599185 21943 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.1
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: true
iter_size: 1
type: "SGD"
I0630 01:13:03.599280 21943 solver.cpp:82] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/train.prototxt
I0630 01:13:03.599740 21943 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0630 01:13:03.599746 21943 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0630 01:13:03.599903 21943 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 21
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0630 01:13:03.599998 21943 layer_factory.hpp:77] Creating layer data
I0630 01:13:03.600078 21943 net.cpp:98] Creating Layer data
I0630 01:13:03.600085 21943 net.cpp:413] data -> data
I0630 01:13:03.600100 21943 net.cpp:413] data -> label
I0630 01:13:03.674168 21975 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0630 01:13:03.692245 21943 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0630 01:13:03.692515 21943 data_layer.cpp:83] output data size: 21,3,32,32
I0630 01:13:03.698848 21943 net.cpp:148] Setting up data
I0630 01:13:03.698901 21943 net.cpp:155] Top shape: 21 3 32 32 (64512)
I0630 01:13:03.698917 21943 net.cpp:155] Top shape: 21 (21)
I0630 01:13:03.698927 21943 net.cpp:163] Memory required for data: 258132
I0630 01:13:03.698949 21943 layer_factory.hpp:77] Creating layer data/bias
I0630 01:13:03.698987 21943 net.cpp:98] Creating Layer data/bias
I0630 01:13:03.699004 21943 net.cpp:439] data/bias <- data
I0630 01:13:03.699033 21943 net.cpp:413] data/bias -> data/bias
I0630 01:13:03.703682 21943 net.cpp:148] Setting up data/bias
I0630 01:13:03.703744 21943 net.cpp:155] Top shape: 21 3 32 32 (64512)
I0630 01:13:03.703758 21943 net.cpp:163] Memory required for data: 516180
I0630 01:13:03.703793 21943 layer_factory.hpp:77] Creating layer conv1a
I0630 01:13:03.703835 21943 net.cpp:98] Creating Layer conv1a
I0630 01:13:03.703850 21943 net.cpp:439] conv1a <- data/bias
I0630 01:13:03.703867 21943 net.cpp:413] conv1a -> conv1a
I0630 01:13:03.716235 21977 blocking_queue.cpp:50] Waiting for data
I0630 01:13:03.716251 21943 net.cpp:148] Setting up conv1a
I0630 01:13:03.716261 21943 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:13:03.716264 21943 net.cpp:163] Memory required for data: 3268692
I0630 01:13:03.716270 21943 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 01:13:03.716277 21943 net.cpp:98] Creating Layer conv1a/bn
I0630 01:13:03.716280 21943 net.cpp:439] conv1a/bn <- conv1a
I0630 01:13:03.716284 21943 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 01:13:03.717001 21943 net.cpp:148] Setting up conv1a/bn
I0630 01:13:03.717010 21943 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:13:03.717013 21943 net.cpp:163] Memory required for data: 6021204
I0630 01:13:03.717020 21943 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 01:13:03.717025 21943 net.cpp:98] Creating Layer conv1a/relu
I0630 01:13:03.717026 21943 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 01:13:03.717030 21943 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 01:13:03.717038 21943 net.cpp:148] Setting up conv1a/relu
I0630 01:13:03.717043 21943 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:13:03.717046 21943 net.cpp:163] Memory required for data: 8773716
I0630 01:13:03.717047 21943 layer_factory.hpp:77] Creating layer conv1b
I0630 01:13:03.717061 21943 net.cpp:98] Creating Layer conv1b
I0630 01:13:03.717064 21943 net.cpp:439] conv1b <- conv1a/bn
I0630 01:13:03.717067 21943 net.cpp:413] conv1b -> conv1b
I0630 01:13:03.717510 21943 net.cpp:148] Setting up conv1b
I0630 01:13:03.717517 21943 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:13:03.717519 21943 net.cpp:163] Memory required for data: 11526228
I0630 01:13:03.717525 21943 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 01:13:03.717528 21943 net.cpp:98] Creating Layer conv1b/bn
I0630 01:13:03.717530 21943 net.cpp:439] conv1b/bn <- conv1b
I0630 01:13:03.717533 21943 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 01:13:03.718238 21943 net.cpp:148] Setting up conv1b/bn
I0630 01:13:03.718243 21943 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:13:03.718246 21943 net.cpp:163] Memory required for data: 14278740
I0630 01:13:03.718251 21943 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 01:13:03.718255 21943 net.cpp:98] Creating Layer conv1b/relu
I0630 01:13:03.718256 21943 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 01:13:03.718260 21943 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 01:13:03.718262 21943 net.cpp:148] Setting up conv1b/relu
I0630 01:13:03.718266 21943 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:13:03.718267 21943 net.cpp:163] Memory required for data: 17031252
I0630 01:13:03.718268 21943 layer_factory.hpp:77] Creating layer pool1
I0630 01:13:03.718274 21943 net.cpp:98] Creating Layer pool1
I0630 01:13:03.718276 21943 net.cpp:439] pool1 <- conv1b/bn
I0630 01:13:03.718281 21943 net.cpp:413] pool1 -> pool1
I0630 01:13:03.718328 21943 net.cpp:148] Setting up pool1
I0630 01:13:03.718333 21943 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:13:03.718334 21943 net.cpp:163] Memory required for data: 19783764
I0630 01:13:03.718336 21943 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 01:13:03.718340 21943 net.cpp:98] Creating Layer res2a_branch2a
I0630 01:13:03.718343 21943 net.cpp:439] res2a_branch2a <- pool1
I0630 01:13:03.718345 21943 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 01:13:03.719028 21943 net.cpp:148] Setting up res2a_branch2a
I0630 01:13:03.719035 21943 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:13:03.719038 21943 net.cpp:163] Memory required for data: 25288788
I0630 01:13:03.719043 21943 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 01:13:03.719045 21943 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 01:13:03.719048 21943 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 01:13:03.719050 21943 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 01:13:03.719749 21943 net.cpp:148] Setting up res2a_branch2a/bn
I0630 01:13:03.719755 21943 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:13:03.719758 21943 net.cpp:163] Memory required for data: 30793812
I0630 01:13:03.719763 21943 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 01:13:03.719765 21943 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 01:13:03.719768 21943 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 01:13:03.719770 21943 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 01:13:03.719774 21943 net.cpp:148] Setting up res2a_branch2a/relu
I0630 01:13:03.719777 21943 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:13:03.719779 21943 net.cpp:163] Memory required for data: 36298836
I0630 01:13:03.719780 21943 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 01:13:03.719784 21943 net.cpp:98] Creating Layer res2a_branch2b
I0630 01:13:03.719786 21943 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 01:13:03.719789 21943 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 01:13:03.721204 21943 net.cpp:148] Setting up res2a_branch2b
I0630 01:13:03.721212 21943 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:13:03.721215 21943 net.cpp:163] Memory required for data: 41803860
I0630 01:13:03.721218 21943 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 01:13:03.721222 21943 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 01:13:03.721225 21943 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 01:13:03.721235 21943 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 01:13:03.721947 21943 net.cpp:148] Setting up res2a_branch2b/bn
I0630 01:13:03.721953 21943 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:13:03.721956 21943 net.cpp:163] Memory required for data: 47308884
I0630 01:13:03.721961 21943 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 01:13:03.721964 21943 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 01:13:03.721966 21943 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 01:13:03.721969 21943 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 01:13:03.721976 21943 net.cpp:148] Setting up res2a_branch2b/relu
I0630 01:13:03.721977 21943 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:13:03.721979 21943 net.cpp:163] Memory required for data: 52813908
I0630 01:13:03.721982 21943 layer_factory.hpp:77] Creating layer pool2
I0630 01:13:03.721984 21943 net.cpp:98] Creating Layer pool2
I0630 01:13:03.721987 21943 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 01:13:03.721990 21943 net.cpp:413] pool2 -> pool2
I0630 01:13:03.722033 21943 net.cpp:148] Setting up pool2
I0630 01:13:03.722038 21943 net.cpp:155] Top shape: 21 64 16 16 (344064)
I0630 01:13:03.722039 21943 net.cpp:163] Memory required for data: 54190164
I0630 01:13:03.722041 21943 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 01:13:03.722051 21943 net.cpp:98] Creating Layer res3a_branch2a
I0630 01:13:03.722054 21943 net.cpp:439] res3a_branch2a <- pool2
I0630 01:13:03.722056 21943 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 01:13:03.724781 21943 net.cpp:148] Setting up res3a_branch2a
I0630 01:13:03.724789 21943 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:13:03.724792 21943 net.cpp:163] Memory required for data: 56942676
I0630 01:13:03.724797 21943 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 01:13:03.724800 21943 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 01:13:03.724802 21943 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 01:13:03.724807 21943 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 01:13:03.725437 21943 net.cpp:148] Setting up res3a_branch2a/bn
I0630 01:13:03.725443 21943 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:13:03.725445 21943 net.cpp:163] Memory required for data: 59695188
I0630 01:13:03.725458 21943 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 01:13:03.725461 21943 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 01:13:03.725464 21943 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 01:13:03.725466 21943 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 01:13:03.725471 21943 net.cpp:148] Setting up res3a_branch2a/relu
I0630 01:13:03.725472 21943 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:13:03.725474 21943 net.cpp:163] Memory required for data: 62447700
I0630 01:13:03.725476 21943 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 01:13:03.725482 21943 net.cpp:98] Creating Layer res3a_branch2b
I0630 01:13:03.725486 21943 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 01:13:03.725488 21943 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 01:13:03.726562 21943 net.cpp:148] Setting up res3a_branch2b
I0630 01:13:03.726567 21943 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:13:03.726569 21943 net.cpp:163] Memory required for data: 65200212
I0630 01:13:03.726572 21943 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 01:13:03.726577 21943 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 01:13:03.726579 21943 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 01:13:03.726583 21943 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 01:13:03.727228 21943 net.cpp:148] Setting up res3a_branch2b/bn
I0630 01:13:03.727234 21943 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:13:03.727237 21943 net.cpp:163] Memory required for data: 67952724
I0630 01:13:03.727242 21943 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 01:13:03.727252 21943 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 01:13:03.727254 21943 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 01:13:03.727257 21943 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 01:13:03.727260 21943 net.cpp:148] Setting up res3a_branch2b/relu
I0630 01:13:03.727263 21943 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:13:03.727265 21943 net.cpp:163] Memory required for data: 70705236
I0630 01:13:03.727267 21943 layer_factory.hpp:77] Creating layer pool3
I0630 01:13:03.727270 21943 net.cpp:98] Creating Layer pool3
I0630 01:13:03.727274 21943 net.cpp:439] pool3 <- res3a_branch2b/bn
I0630 01:13:03.727277 21943 net.cpp:413] pool3 -> pool3
I0630 01:13:03.727313 21943 net.cpp:148] Setting up pool3
I0630 01:13:03.727318 21943 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:13:03.727319 21943 net.cpp:163] Memory required for data: 73457748
I0630 01:13:03.727320 21943 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 01:13:03.727325 21943 net.cpp:98] Creating Layer res4a_branch2a
I0630 01:13:03.727327 21943 net.cpp:439] res4a_branch2a <- pool3
I0630 01:13:03.727330 21943 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 01:13:03.733817 21943 net.cpp:148] Setting up res4a_branch2a
I0630 01:13:03.733824 21943 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:13:03.733825 21943 net.cpp:163] Memory required for data: 78962772
I0630 01:13:03.733829 21943 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 01:13:03.733832 21943 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 01:13:03.733836 21943 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 01:13:03.733839 21943 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 01:13:03.734557 21943 net.cpp:148] Setting up res4a_branch2a/bn
I0630 01:13:03.734565 21943 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:13:03.734566 21943 net.cpp:163] Memory required for data: 84467796
I0630 01:13:03.734572 21943 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 01:13:03.734576 21943 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 01:13:03.734578 21943 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 01:13:03.734581 21943 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 01:13:03.734586 21943 net.cpp:148] Setting up res4a_branch2a/relu
I0630 01:13:03.734588 21943 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:13:03.734589 21943 net.cpp:163] Memory required for data: 89972820
I0630 01:13:03.734591 21943 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 01:13:03.734597 21943 net.cpp:98] Creating Layer res4a_branch2b
I0630 01:13:03.734601 21943 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 01:13:03.734603 21943 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 01:13:03.738062 21943 net.cpp:148] Setting up res4a_branch2b
I0630 01:13:03.738071 21943 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:13:03.738073 21943 net.cpp:163] Memory required for data: 95477844
I0630 01:13:03.738077 21943 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 01:13:03.738083 21943 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 01:13:03.738085 21943 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 01:13:03.738090 21943 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 01:13:03.738742 21943 net.cpp:148] Setting up res4a_branch2b/bn
I0630 01:13:03.738749 21943 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:13:03.738751 21943 net.cpp:163] Memory required for data: 100982868
I0630 01:13:03.738756 21943 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 01:13:03.738760 21943 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 01:13:03.738764 21943 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 01:13:03.738766 21943 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 01:13:03.738770 21943 net.cpp:148] Setting up res4a_branch2b/relu
I0630 01:13:03.738773 21943 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:13:03.738783 21943 net.cpp:163] Memory required for data: 106487892
I0630 01:13:03.738785 21943 layer_factory.hpp:77] Creating layer pool4
I0630 01:13:03.738790 21943 net.cpp:98] Creating Layer pool4
I0630 01:13:03.738793 21943 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 01:13:03.738796 21943 net.cpp:413] pool4 -> pool4
I0630 01:13:03.738837 21943 net.cpp:148] Setting up pool4
I0630 01:13:03.738842 21943 net.cpp:155] Top shape: 21 256 8 8 (344064)
I0630 01:13:03.738844 21943 net.cpp:163] Memory required for data: 107864148
I0630 01:13:03.738847 21943 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 01:13:03.738852 21943 net.cpp:98] Creating Layer res5a_branch2a
I0630 01:13:03.738855 21943 net.cpp:439] res5a_branch2a <- pool4
I0630 01:13:03.738858 21943 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 01:13:03.764006 21943 net.cpp:148] Setting up res5a_branch2a
I0630 01:13:03.764034 21943 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:13:03.764037 21943 net.cpp:163] Memory required for data: 110616660
I0630 01:13:03.764045 21943 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 01:13:03.764053 21943 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 01:13:03.764057 21943 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 01:13:03.764062 21943 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 01:13:03.764752 21943 net.cpp:148] Setting up res5a_branch2a/bn
I0630 01:13:03.764760 21943 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:13:03.764761 21943 net.cpp:163] Memory required for data: 113369172
I0630 01:13:03.764766 21943 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 01:13:03.764770 21943 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 01:13:03.764772 21943 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 01:13:03.764775 21943 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 01:13:03.764780 21943 net.cpp:148] Setting up res5a_branch2a/relu
I0630 01:13:03.764781 21943 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:13:03.764783 21943 net.cpp:163] Memory required for data: 116121684
I0630 01:13:03.764786 21943 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 01:13:03.764796 21943 net.cpp:98] Creating Layer res5a_branch2b
I0630 01:13:03.764799 21943 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 01:13:03.764803 21943 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 01:13:03.777714 21943 net.cpp:148] Setting up res5a_branch2b
I0630 01:13:03.777727 21943 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:13:03.777729 21943 net.cpp:163] Memory required for data: 118874196
I0630 01:13:03.777740 21943 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 01:13:03.777746 21943 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 01:13:03.777750 21943 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 01:13:03.777752 21943 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 01:13:03.778417 21943 net.cpp:148] Setting up res5a_branch2b/bn
I0630 01:13:03.778424 21943 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:13:03.778425 21943 net.cpp:163] Memory required for data: 121626708
I0630 01:13:03.778430 21943 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 01:13:03.778434 21943 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 01:13:03.778436 21943 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 01:13:03.778439 21943 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 01:13:03.778441 21943 net.cpp:148] Setting up res5a_branch2b/relu
I0630 01:13:03.778445 21943 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:13:03.778446 21943 net.cpp:163] Memory required for data: 124379220
I0630 01:13:03.778447 21943 layer_factory.hpp:77] Creating layer pool5
I0630 01:13:03.778453 21943 net.cpp:98] Creating Layer pool5
I0630 01:13:03.778455 21943 net.cpp:439] pool5 <- res5a_branch2b/bn
I0630 01:13:03.778457 21943 net.cpp:413] pool5 -> pool5
I0630 01:13:03.778484 21943 net.cpp:148] Setting up pool5
I0630 01:13:03.778488 21943 net.cpp:155] Top shape: 21 512 1 1 (10752)
I0630 01:13:03.778498 21943 net.cpp:163] Memory required for data: 124422228
I0630 01:13:03.778501 21943 layer_factory.hpp:77] Creating layer fc10
I0630 01:13:03.778509 21943 net.cpp:98] Creating Layer fc10
I0630 01:13:03.778512 21943 net.cpp:439] fc10 <- pool5
I0630 01:13:03.778514 21943 net.cpp:413] fc10 -> fc10
I0630 01:13:03.778743 21943 net.cpp:148] Setting up fc10
I0630 01:13:03.778748 21943 net.cpp:155] Top shape: 21 10 (210)
I0630 01:13:03.778750 21943 net.cpp:163] Memory required for data: 124423068
I0630 01:13:03.778753 21943 layer_factory.hpp:77] Creating layer loss
I0630 01:13:03.778759 21943 net.cpp:98] Creating Layer loss
I0630 01:13:03.778761 21943 net.cpp:439] loss <- fc10
I0630 01:13:03.778764 21943 net.cpp:439] loss <- label
I0630 01:13:03.778767 21943 net.cpp:413] loss -> loss
I0630 01:13:03.778775 21943 layer_factory.hpp:77] Creating layer loss
I0630 01:13:03.778894 21943 net.cpp:148] Setting up loss
I0630 01:13:03.778899 21943 net.cpp:155] Top shape: (1)
I0630 01:13:03.778901 21943 net.cpp:158]     with loss weight 1
I0630 01:13:03.778913 21943 net.cpp:163] Memory required for data: 124423072
I0630 01:13:03.778914 21943 net.cpp:224] loss needs backward computation.
I0630 01:13:03.778916 21943 net.cpp:224] fc10 needs backward computation.
I0630 01:13:03.778918 21943 net.cpp:224] pool5 needs backward computation.
I0630 01:13:03.778920 21943 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 01:13:03.778923 21943 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 01:13:03.778924 21943 net.cpp:224] res5a_branch2b needs backward computation.
I0630 01:13:03.778926 21943 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 01:13:03.778928 21943 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 01:13:03.778929 21943 net.cpp:224] res5a_branch2a needs backward computation.
I0630 01:13:03.778933 21943 net.cpp:224] pool4 needs backward computation.
I0630 01:13:03.778934 21943 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 01:13:03.778936 21943 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 01:13:03.778939 21943 net.cpp:224] res4a_branch2b needs backward computation.
I0630 01:13:03.778941 21943 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 01:13:03.778944 21943 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 01:13:03.778945 21943 net.cpp:224] res4a_branch2a needs backward computation.
I0630 01:13:03.778949 21943 net.cpp:224] pool3 needs backward computation.
I0630 01:13:03.778950 21943 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 01:13:03.778952 21943 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 01:13:03.778955 21943 net.cpp:224] res3a_branch2b needs backward computation.
I0630 01:13:03.778957 21943 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 01:13:03.778959 21943 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 01:13:03.778962 21943 net.cpp:224] res3a_branch2a needs backward computation.
I0630 01:13:03.778964 21943 net.cpp:224] pool2 needs backward computation.
I0630 01:13:03.778966 21943 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 01:13:03.778969 21943 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 01:13:03.778970 21943 net.cpp:224] res2a_branch2b needs backward computation.
I0630 01:13:03.778973 21943 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 01:13:03.778975 21943 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 01:13:03.778977 21943 net.cpp:224] res2a_branch2a needs backward computation.
I0630 01:13:03.778978 21943 net.cpp:224] pool1 needs backward computation.
I0630 01:13:03.778981 21943 net.cpp:224] conv1b/relu needs backward computation.
I0630 01:13:03.778983 21943 net.cpp:224] conv1b/bn needs backward computation.
I0630 01:13:03.778985 21943 net.cpp:224] conv1b needs backward computation.
I0630 01:13:03.778987 21943 net.cpp:224] conv1a/relu needs backward computation.
I0630 01:13:03.778990 21943 net.cpp:224] conv1a/bn needs backward computation.
I0630 01:13:03.778997 21943 net.cpp:224] conv1a needs backward computation.
I0630 01:13:03.779000 21943 net.cpp:226] data/bias does not need backward computation.
I0630 01:13:03.779002 21943 net.cpp:226] data does not need backward computation.
I0630 01:13:03.779005 21943 net.cpp:268] This network produces output loss
I0630 01:13:03.779021 21943 net.cpp:288] Network initialization done.
I0630 01:13:03.779448 21943 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/test.prototxt
I0630 01:13:03.779629 21943 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0630 01:13:03.779721 21943 layer_factory.hpp:77] Creating layer data
I0630 01:13:03.779780 21943 net.cpp:98] Creating Layer data
I0630 01:13:03.779784 21943 net.cpp:413] data -> data
I0630 01:13:03.779789 21943 net.cpp:413] data -> label
I0630 01:13:03.792845 21978 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0630 01:13:03.796630 21943 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0630 01:13:03.796696 21943 data_layer.cpp:83] output data size: 50,3,32,32
I0630 01:13:03.799067 21943 net.cpp:148] Setting up data
I0630 01:13:03.799078 21943 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0630 01:13:03.799082 21943 net.cpp:155] Top shape: 50 (50)
I0630 01:13:03.799083 21943 net.cpp:163] Memory required for data: 614600
I0630 01:13:03.799088 21943 layer_factory.hpp:77] Creating layer label_data_1_split
I0630 01:13:03.799094 21943 net.cpp:98] Creating Layer label_data_1_split
I0630 01:13:03.799098 21943 net.cpp:439] label_data_1_split <- label
I0630 01:13:03.799100 21943 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0630 01:13:03.799106 21943 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0630 01:13:03.799109 21943 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0630 01:13:03.799233 21943 net.cpp:148] Setting up label_data_1_split
I0630 01:13:03.799243 21943 net.cpp:155] Top shape: 50 (50)
I0630 01:13:03.799247 21943 net.cpp:155] Top shape: 50 (50)
I0630 01:13:03.799248 21943 net.cpp:155] Top shape: 50 (50)
I0630 01:13:03.799250 21943 net.cpp:163] Memory required for data: 615200
I0630 01:13:03.799252 21943 layer_factory.hpp:77] Creating layer data/bias
I0630 01:13:03.799258 21943 net.cpp:98] Creating Layer data/bias
I0630 01:13:03.799262 21943 net.cpp:439] data/bias <- data
I0630 01:13:03.799264 21943 net.cpp:413] data/bias -> data/bias
I0630 01:13:03.799367 21943 net.cpp:148] Setting up data/bias
I0630 01:13:03.799372 21943 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0630 01:13:03.799374 21943 net.cpp:163] Memory required for data: 1229600
I0630 01:13:03.799378 21943 layer_factory.hpp:77] Creating layer conv1a
I0630 01:13:03.799384 21943 net.cpp:98] Creating Layer conv1a
I0630 01:13:03.799386 21943 net.cpp:439] conv1a <- data/bias
I0630 01:13:03.799389 21943 net.cpp:413] conv1a -> conv1a
I0630 01:13:03.799756 21943 net.cpp:148] Setting up conv1a
I0630 01:13:03.799760 21943 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:13:03.799762 21943 net.cpp:163] Memory required for data: 7783200
I0630 01:13:03.799767 21943 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 01:13:03.799772 21943 net.cpp:98] Creating Layer conv1a/bn
I0630 01:13:03.799773 21943 net.cpp:439] conv1a/bn <- conv1a
I0630 01:13:03.799777 21943 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 01:13:03.800767 21943 net.cpp:148] Setting up conv1a/bn
I0630 01:13:03.800776 21943 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:13:03.800779 21943 net.cpp:163] Memory required for data: 14336800
I0630 01:13:03.800786 21943 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 01:13:03.800789 21943 net.cpp:98] Creating Layer conv1a/relu
I0630 01:13:03.800791 21943 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 01:13:03.800804 21943 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 01:13:03.800809 21943 net.cpp:148] Setting up conv1a/relu
I0630 01:13:03.800812 21943 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:13:03.800814 21943 net.cpp:163] Memory required for data: 20890400
I0630 01:13:03.800817 21943 layer_factory.hpp:77] Creating layer conv1b
I0630 01:13:03.800822 21943 net.cpp:98] Creating Layer conv1b
I0630 01:13:03.800823 21943 net.cpp:439] conv1b <- conv1a/bn
I0630 01:13:03.800827 21943 net.cpp:413] conv1b -> conv1b
I0630 01:13:03.801229 21943 net.cpp:148] Setting up conv1b
I0630 01:13:03.801235 21943 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:13:03.801237 21943 net.cpp:163] Memory required for data: 27444000
I0630 01:13:03.801241 21943 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 01:13:03.801250 21943 net.cpp:98] Creating Layer conv1b/bn
I0630 01:13:03.801252 21943 net.cpp:439] conv1b/bn <- conv1b
I0630 01:13:03.801255 21943 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 01:13:03.802085 21943 net.cpp:148] Setting up conv1b/bn
I0630 01:13:03.802091 21943 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:13:03.802093 21943 net.cpp:163] Memory required for data: 33997600
I0630 01:13:03.802098 21943 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 01:13:03.802103 21943 net.cpp:98] Creating Layer conv1b/relu
I0630 01:13:03.802104 21943 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 01:13:03.802106 21943 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 01:13:03.802109 21943 net.cpp:148] Setting up conv1b/relu
I0630 01:13:03.802112 21943 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:13:03.802114 21943 net.cpp:163] Memory required for data: 40551200
I0630 01:13:03.802116 21943 layer_factory.hpp:77] Creating layer pool1
I0630 01:13:03.802119 21943 net.cpp:98] Creating Layer pool1
I0630 01:13:03.802121 21943 net.cpp:439] pool1 <- conv1b/bn
I0630 01:13:03.802125 21943 net.cpp:413] pool1 -> pool1
I0630 01:13:03.802160 21943 net.cpp:148] Setting up pool1
I0630 01:13:03.802165 21943 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:13:03.802166 21943 net.cpp:163] Memory required for data: 47104800
I0630 01:13:03.802168 21943 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 01:13:03.802177 21943 net.cpp:98] Creating Layer res2a_branch2a
I0630 01:13:03.802181 21943 net.cpp:439] res2a_branch2a <- pool1
I0630 01:13:03.802183 21943 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 01:13:03.802865 21943 net.cpp:148] Setting up res2a_branch2a
I0630 01:13:03.802870 21943 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:13:03.802873 21943 net.cpp:163] Memory required for data: 60212000
I0630 01:13:03.802877 21943 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 01:13:03.802881 21943 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 01:13:03.802883 21943 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 01:13:03.802886 21943 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 01:13:03.803598 21943 net.cpp:148] Setting up res2a_branch2a/bn
I0630 01:13:03.803604 21943 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:13:03.803606 21943 net.cpp:163] Memory required for data: 73319200
I0630 01:13:03.803611 21943 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 01:13:03.803613 21943 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 01:13:03.803616 21943 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 01:13:03.803618 21943 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 01:13:03.803622 21943 net.cpp:148] Setting up res2a_branch2a/relu
I0630 01:13:03.803623 21943 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:13:03.803625 21943 net.cpp:163] Memory required for data: 86426400
I0630 01:13:03.803627 21943 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 01:13:03.803632 21943 net.cpp:98] Creating Layer res2a_branch2b
I0630 01:13:03.803633 21943 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 01:13:03.803635 21943 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 01:13:03.804118 21943 net.cpp:148] Setting up res2a_branch2b
I0630 01:13:03.804124 21943 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:13:03.804126 21943 net.cpp:163] Memory required for data: 99533600
I0630 01:13:03.804129 21943 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 01:13:03.804132 21943 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 01:13:03.804134 21943 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 01:13:03.804137 21943 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 01:13:03.804857 21943 net.cpp:148] Setting up res2a_branch2b/bn
I0630 01:13:03.804862 21943 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:13:03.804863 21943 net.cpp:163] Memory required for data: 112640800
I0630 01:13:03.804868 21943 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 01:13:03.804870 21943 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 01:13:03.804872 21943 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 01:13:03.804875 21943 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 01:13:03.804878 21943 net.cpp:148] Setting up res2a_branch2b/relu
I0630 01:13:03.804880 21943 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:13:03.804883 21943 net.cpp:163] Memory required for data: 125748000
I0630 01:13:03.804884 21943 layer_factory.hpp:77] Creating layer pool2
I0630 01:13:03.804888 21943 net.cpp:98] Creating Layer pool2
I0630 01:13:03.804889 21943 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 01:13:03.804891 21943 net.cpp:413] pool2 -> pool2
I0630 01:13:03.804927 21943 net.cpp:148] Setting up pool2
I0630 01:13:03.804930 21943 net.cpp:155] Top shape: 50 64 16 16 (819200)
I0630 01:13:03.804932 21943 net.cpp:163] Memory required for data: 129024800
I0630 01:13:03.804934 21943 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 01:13:03.804939 21943 net.cpp:98] Creating Layer res3a_branch2a
I0630 01:13:03.804940 21943 net.cpp:439] res3a_branch2a <- pool2
I0630 01:13:03.804944 21943 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 01:13:03.807574 21943 net.cpp:148] Setting up res3a_branch2a
I0630 01:13:03.807584 21943 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:13:03.807585 21943 net.cpp:163] Memory required for data: 135578400
I0630 01:13:03.807590 21943 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 01:13:03.807593 21943 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 01:13:03.807595 21943 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 01:13:03.807600 21943 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 01:13:03.808230 21943 net.cpp:148] Setting up res3a_branch2a/bn
I0630 01:13:03.808235 21943 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:13:03.808238 21943 net.cpp:163] Memory required for data: 142132000
I0630 01:13:03.808244 21943 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 01:13:03.808251 21943 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 01:13:03.808254 21943 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 01:13:03.808256 21943 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 01:13:03.808260 21943 net.cpp:148] Setting up res3a_branch2a/relu
I0630 01:13:03.808262 21943 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:13:03.808264 21943 net.cpp:163] Memory required for data: 148685600
I0630 01:13:03.808266 21943 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 01:13:03.808270 21943 net.cpp:98] Creating Layer res3a_branch2b
I0630 01:13:03.808272 21943 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 01:13:03.808275 21943 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 01:13:03.809301 21943 net.cpp:148] Setting up res3a_branch2b
I0630 01:13:03.809307 21943 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:13:03.809309 21943 net.cpp:163] Memory required for data: 155239200
I0630 01:13:03.809312 21943 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 01:13:03.809315 21943 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 01:13:03.809317 21943 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 01:13:03.809326 21943 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 01:13:03.809963 21943 net.cpp:148] Setting up res3a_branch2b/bn
I0630 01:13:03.809969 21943 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:13:03.809972 21943 net.cpp:163] Memory required for data: 161792800
I0630 01:13:03.809976 21943 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 01:13:03.809978 21943 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 01:13:03.809981 21943 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 01:13:03.809983 21943 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 01:13:03.809986 21943 net.cpp:148] Setting up res3a_branch2b/relu
I0630 01:13:03.809988 21943 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:13:03.809990 21943 net.cpp:163] Memory required for data: 168346400
I0630 01:13:03.809993 21943 layer_factory.hpp:77] Creating layer pool3
I0630 01:13:03.809995 21943 net.cpp:98] Creating Layer pool3
I0630 01:13:03.809998 21943 net.cpp:439] pool3 <- res3a_branch2b/bn
I0630 01:13:03.809999 21943 net.cpp:413] pool3 -> pool3
I0630 01:13:03.810039 21943 net.cpp:148] Setting up pool3
I0630 01:13:03.810042 21943 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:13:03.810045 21943 net.cpp:163] Memory required for data: 174900000
I0630 01:13:03.810046 21943 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 01:13:03.810051 21943 net.cpp:98] Creating Layer res4a_branch2a
I0630 01:13:03.810053 21943 net.cpp:439] res4a_branch2a <- pool3
I0630 01:13:03.810056 21943 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 01:13:03.816105 21943 net.cpp:148] Setting up res4a_branch2a
I0630 01:13:03.816112 21943 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:13:03.816113 21943 net.cpp:163] Memory required for data: 188007200
I0630 01:13:03.816117 21943 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 01:13:03.816120 21943 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 01:13:03.816123 21943 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 01:13:03.816124 21943 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 01:13:03.816779 21943 net.cpp:148] Setting up res4a_branch2a/bn
I0630 01:13:03.816786 21943 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:13:03.816787 21943 net.cpp:163] Memory required for data: 201114400
I0630 01:13:03.816792 21943 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 01:13:03.816795 21943 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 01:13:03.816797 21943 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 01:13:03.816799 21943 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 01:13:03.816802 21943 net.cpp:148] Setting up res4a_branch2a/relu
I0630 01:13:03.816805 21943 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:13:03.816807 21943 net.cpp:163] Memory required for data: 214221600
I0630 01:13:03.816808 21943 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 01:13:03.816812 21943 net.cpp:98] Creating Layer res4a_branch2b
I0630 01:13:03.816814 21943 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 01:13:03.816817 21943 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 01:13:03.819984 21943 net.cpp:148] Setting up res4a_branch2b
I0630 01:13:03.819990 21943 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:13:03.819993 21943 net.cpp:163] Memory required for data: 227328800
I0630 01:13:03.819995 21943 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 01:13:03.819999 21943 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 01:13:03.820001 21943 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 01:13:03.820008 21943 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 01:13:03.820643 21943 net.cpp:148] Setting up res4a_branch2b/bn
I0630 01:13:03.820648 21943 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:13:03.820650 21943 net.cpp:163] Memory required for data: 240436000
I0630 01:13:03.820655 21943 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 01:13:03.820663 21943 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 01:13:03.820667 21943 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 01:13:03.820668 21943 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 01:13:03.820672 21943 net.cpp:148] Setting up res4a_branch2b/relu
I0630 01:13:03.820674 21943 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:13:03.820677 21943 net.cpp:163] Memory required for data: 253543200
I0630 01:13:03.820677 21943 layer_factory.hpp:77] Creating layer pool4
I0630 01:13:03.820680 21943 net.cpp:98] Creating Layer pool4
I0630 01:13:03.820683 21943 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 01:13:03.820684 21943 net.cpp:413] pool4 -> pool4
I0630 01:13:03.820725 21943 net.cpp:148] Setting up pool4
I0630 01:13:03.820729 21943 net.cpp:155] Top shape: 50 256 8 8 (819200)
I0630 01:13:03.820731 21943 net.cpp:163] Memory required for data: 256820000
I0630 01:13:03.820734 21943 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 01:13:03.820740 21943 net.cpp:98] Creating Layer res5a_branch2a
I0630 01:13:03.820742 21943 net.cpp:439] res5a_branch2a <- pool4
I0630 01:13:03.820745 21943 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 01:13:03.845532 21943 net.cpp:148] Setting up res5a_branch2a
I0630 01:13:03.845554 21943 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:13:03.845556 21943 net.cpp:163] Memory required for data: 263373600
I0630 01:13:03.845562 21943 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 01:13:03.845571 21943 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 01:13:03.845576 21943 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 01:13:03.845579 21943 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 01:13:03.846284 21943 net.cpp:148] Setting up res5a_branch2a/bn
I0630 01:13:03.846289 21943 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:13:03.846292 21943 net.cpp:163] Memory required for data: 269927200
I0630 01:13:03.846297 21943 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 01:13:03.846300 21943 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 01:13:03.846302 21943 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 01:13:03.846305 21943 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 01:13:03.846309 21943 net.cpp:148] Setting up res5a_branch2a/relu
I0630 01:13:03.846312 21943 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:13:03.846313 21943 net.cpp:163] Memory required for data: 276480800
I0630 01:13:03.846315 21943 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 01:13:03.846324 21943 net.cpp:98] Creating Layer res5a_branch2b
I0630 01:13:03.846326 21943 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 01:13:03.846329 21943 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 01:13:03.859257 21943 net.cpp:148] Setting up res5a_branch2b
I0630 01:13:03.859277 21943 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:13:03.859280 21943 net.cpp:163] Memory required for data: 283034400
I0630 01:13:03.859293 21943 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 01:13:03.859302 21943 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 01:13:03.859305 21943 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 01:13:03.859309 21943 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 01:13:03.860020 21943 net.cpp:148] Setting up res5a_branch2b/bn
I0630 01:13:03.860026 21943 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:13:03.860028 21943 net.cpp:163] Memory required for data: 289588000
I0630 01:13:03.860033 21943 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 01:13:03.860036 21943 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 01:13:03.860038 21943 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 01:13:03.860041 21943 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 01:13:03.860045 21943 net.cpp:148] Setting up res5a_branch2b/relu
I0630 01:13:03.860047 21943 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:13:03.860049 21943 net.cpp:163] Memory required for data: 296141600
I0630 01:13:03.860061 21943 layer_factory.hpp:77] Creating layer pool5
I0630 01:13:03.860065 21943 net.cpp:98] Creating Layer pool5
I0630 01:13:03.860069 21943 net.cpp:439] pool5 <- res5a_branch2b/bn
I0630 01:13:03.860071 21943 net.cpp:413] pool5 -> pool5
I0630 01:13:03.860095 21943 net.cpp:148] Setting up pool5
I0630 01:13:03.860100 21943 net.cpp:155] Top shape: 50 512 1 1 (25600)
I0630 01:13:03.860101 21943 net.cpp:163] Memory required for data: 296244000
I0630 01:13:03.860103 21943 layer_factory.hpp:77] Creating layer fc10
I0630 01:13:03.860111 21943 net.cpp:98] Creating Layer fc10
I0630 01:13:03.860115 21943 net.cpp:439] fc10 <- pool5
I0630 01:13:03.860117 21943 net.cpp:413] fc10 -> fc10
I0630 01:13:03.860378 21943 net.cpp:148] Setting up fc10
I0630 01:13:03.860383 21943 net.cpp:155] Top shape: 50 10 (500)
I0630 01:13:03.860384 21943 net.cpp:163] Memory required for data: 296246000
I0630 01:13:03.860388 21943 layer_factory.hpp:77] Creating layer fc10_fc10_0_split
I0630 01:13:03.860391 21943 net.cpp:98] Creating Layer fc10_fc10_0_split
I0630 01:13:03.860394 21943 net.cpp:439] fc10_fc10_0_split <- fc10
I0630 01:13:03.860396 21943 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0630 01:13:03.860400 21943 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0630 01:13:03.860404 21943 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0630 01:13:03.860468 21943 net.cpp:148] Setting up fc10_fc10_0_split
I0630 01:13:03.860472 21943 net.cpp:155] Top shape: 50 10 (500)
I0630 01:13:03.860474 21943 net.cpp:155] Top shape: 50 10 (500)
I0630 01:13:03.860476 21943 net.cpp:155] Top shape: 50 10 (500)
I0630 01:13:03.860478 21943 net.cpp:163] Memory required for data: 296252000
I0630 01:13:03.860481 21943 layer_factory.hpp:77] Creating layer loss
I0630 01:13:03.860484 21943 net.cpp:98] Creating Layer loss
I0630 01:13:03.860486 21943 net.cpp:439] loss <- fc10_fc10_0_split_0
I0630 01:13:03.860488 21943 net.cpp:439] loss <- label_data_1_split_0
I0630 01:13:03.860491 21943 net.cpp:413] loss -> loss
I0630 01:13:03.860496 21943 layer_factory.hpp:77] Creating layer loss
I0630 01:13:03.860602 21943 net.cpp:148] Setting up loss
I0630 01:13:03.860606 21943 net.cpp:155] Top shape: (1)
I0630 01:13:03.860607 21943 net.cpp:158]     with loss weight 1
I0630 01:13:03.860615 21943 net.cpp:163] Memory required for data: 296252004
I0630 01:13:03.860617 21943 layer_factory.hpp:77] Creating layer accuracy/top1
I0630 01:13:03.860621 21943 net.cpp:98] Creating Layer accuracy/top1
I0630 01:13:03.860623 21943 net.cpp:439] accuracy/top1 <- fc10_fc10_0_split_1
I0630 01:13:03.860626 21943 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0630 01:13:03.860628 21943 net.cpp:413] accuracy/top1 -> accuracy/top1
I0630 01:13:03.860632 21943 net.cpp:148] Setting up accuracy/top1
I0630 01:13:03.860635 21943 net.cpp:155] Top shape: (1)
I0630 01:13:03.860636 21943 net.cpp:163] Memory required for data: 296252008
I0630 01:13:03.860638 21943 layer_factory.hpp:77] Creating layer accuracy/top5
I0630 01:13:03.860646 21943 net.cpp:98] Creating Layer accuracy/top5
I0630 01:13:03.860649 21943 net.cpp:439] accuracy/top5 <- fc10_fc10_0_split_2
I0630 01:13:03.860651 21943 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0630 01:13:03.860654 21943 net.cpp:413] accuracy/top5 -> accuracy/top5
I0630 01:13:03.860658 21943 net.cpp:148] Setting up accuracy/top5
I0630 01:13:03.860661 21943 net.cpp:155] Top shape: (1)
I0630 01:13:03.860663 21943 net.cpp:163] Memory required for data: 296252012
I0630 01:13:03.860666 21943 net.cpp:226] accuracy/top5 does not need backward computation.
I0630 01:13:03.860668 21943 net.cpp:226] accuracy/top1 does not need backward computation.
I0630 01:13:03.860671 21943 net.cpp:224] loss needs backward computation.
I0630 01:13:03.860673 21943 net.cpp:224] fc10_fc10_0_split needs backward computation.
I0630 01:13:03.860676 21943 net.cpp:224] fc10 needs backward computation.
I0630 01:13:03.860677 21943 net.cpp:224] pool5 needs backward computation.
I0630 01:13:03.860679 21943 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 01:13:03.860687 21943 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 01:13:03.860689 21943 net.cpp:224] res5a_branch2b needs backward computation.
I0630 01:13:03.860692 21943 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 01:13:03.860693 21943 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 01:13:03.860695 21943 net.cpp:224] res5a_branch2a needs backward computation.
I0630 01:13:03.860697 21943 net.cpp:224] pool4 needs backward computation.
I0630 01:13:03.860700 21943 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 01:13:03.860702 21943 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 01:13:03.860704 21943 net.cpp:224] res4a_branch2b needs backward computation.
I0630 01:13:03.860707 21943 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 01:13:03.860709 21943 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 01:13:03.860711 21943 net.cpp:224] res4a_branch2a needs backward computation.
I0630 01:13:03.860714 21943 net.cpp:224] pool3 needs backward computation.
I0630 01:13:03.860716 21943 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 01:13:03.860719 21943 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 01:13:03.860721 21943 net.cpp:224] res3a_branch2b needs backward computation.
I0630 01:13:03.860723 21943 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 01:13:03.860725 21943 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 01:13:03.860728 21943 net.cpp:224] res3a_branch2a needs backward computation.
I0630 01:13:03.860729 21943 net.cpp:224] pool2 needs backward computation.
I0630 01:13:03.860733 21943 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 01:13:03.860734 21943 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 01:13:03.860736 21943 net.cpp:224] res2a_branch2b needs backward computation.
I0630 01:13:03.860738 21943 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 01:13:03.860740 21943 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 01:13:03.860743 21943 net.cpp:224] res2a_branch2a needs backward computation.
I0630 01:13:03.860745 21943 net.cpp:224] pool1 needs backward computation.
I0630 01:13:03.860747 21943 net.cpp:224] conv1b/relu needs backward computation.
I0630 01:13:03.860750 21943 net.cpp:224] conv1b/bn needs backward computation.
I0630 01:13:03.860752 21943 net.cpp:224] conv1b needs backward computation.
I0630 01:13:03.860755 21943 net.cpp:224] conv1a/relu needs backward computation.
I0630 01:13:03.860757 21943 net.cpp:224] conv1a/bn needs backward computation.
I0630 01:13:03.860760 21943 net.cpp:224] conv1a needs backward computation.
I0630 01:13:03.860762 21943 net.cpp:226] data/bias does not need backward computation.
I0630 01:13:03.860765 21943 net.cpp:226] label_data_1_split does not need backward computation.
I0630 01:13:03.860769 21943 net.cpp:226] data does not need backward computation.
I0630 01:13:03.860770 21943 net.cpp:268] This network produces output accuracy/top1
I0630 01:13:03.860772 21943 net.cpp:268] This network produces output accuracy/top5
I0630 01:13:03.860774 21943 net.cpp:268] This network produces output loss
I0630 01:13:03.860796 21943 net.cpp:288] Network initialization done.
I0630 01:13:03.860867 21943 solver.cpp:60] Solver scaffolding done.
I0630 01:13:03.872201 21943 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0630 01:13:03.872261 21943 data_layer.cpp:83] output data size: 21,3,32,32
I0630 01:13:04.322703 21943 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0630 01:13:04.322778 21943 data_layer.cpp:83] output data size: 21,3,32,32
I0630 01:13:04.803813 21943 parallel.cpp:334] Starting Optimization
I0630 01:13:04.803874 21943 solver.cpp:406] Solving jacintonet11v2_train
I0630 01:13:04.803880 21943 solver.cpp:407] Learning Rate Policy: poly
I0630 01:13:04.809265 21943 solver.cpp:464] Iteration 0, Testing net (#0)
I0630 01:13:06.477227 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.1
I0630 01:13:06.477262 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.504
I0630 01:13:06.477270 21943 solver.cpp:537]     Test net output #2: loss = 78.2999 (* 1 = 78.2999 loss)
I0630 01:13:06.586685 21943 solver.cpp:290] Iteration 0 (0 iter/s, 1.78273s/100 iter), loss = 1.57143
I0630 01:13:06.586709 21943 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0630 01:13:06.586719 21943 sgd_solver.cpp:106] Iteration 0, lr = 0.1
I0630 01:13:08.675415 21943 solver.cpp:290] Iteration 100 (47.8782 iter/s, 2.08863s/100 iter), loss = 1.19048
I0630 01:13:08.675441 21943 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0630 01:13:08.675449 21943 sgd_solver.cpp:106] Iteration 100, lr = 0.0998438
I0630 01:13:10.729986 21943 solver.cpp:290] Iteration 200 (48.6741 iter/s, 2.05448s/100 iter), loss = 1.04762
I0630 01:13:10.730008 21943 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 01:13:10.730015 21943 sgd_solver.cpp:106] Iteration 200, lr = 0.0996875
I0630 01:13:12.784934 21943 solver.cpp:290] Iteration 300 (48.6652 iter/s, 2.05486s/100 iter), loss = 0.761905
I0630 01:13:12.784957 21943 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 01:13:12.784965 21943 sgd_solver.cpp:106] Iteration 300, lr = 0.0995313
I0630 01:13:14.836782 21943 solver.cpp:290] Iteration 400 (48.7387 iter/s, 2.05176s/100 iter), loss = 0.809524
I0630 01:13:14.836805 21943 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 01:13:14.836812 21943 sgd_solver.cpp:106] Iteration 400, lr = 0.099375
I0630 01:13:16.891265 21943 solver.cpp:290] Iteration 500 (48.6762 iter/s, 2.05439s/100 iter), loss = 1.04762
I0630 01:13:16.891288 21943 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0630 01:13:16.891295 21943 sgd_solver.cpp:106] Iteration 500, lr = 0.0992187
I0630 01:13:18.950470 21943 solver.cpp:290] Iteration 600 (48.5645 iter/s, 2.05912s/100 iter), loss = 0.666667
I0630 01:13:18.950495 21943 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 01:13:18.950500 21943 sgd_solver.cpp:106] Iteration 600, lr = 0.0990625
I0630 01:13:21.007866 21943 solver.cpp:290] Iteration 700 (48.6073 iter/s, 2.05731s/100 iter), loss = 0.809524
I0630 01:13:21.007889 21943 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0630 01:13:21.007896 21943 sgd_solver.cpp:106] Iteration 700, lr = 0.0989062
I0630 01:13:23.063457 21943 solver.cpp:290] Iteration 800 (48.65 iter/s, 2.0555s/100 iter), loss = 0.238095
I0630 01:13:23.063478 21943 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0630 01:13:23.063484 21943 sgd_solver.cpp:106] Iteration 800, lr = 0.09875
I0630 01:13:25.121084 21943 solver.cpp:290] Iteration 900 (48.6017 iter/s, 2.05754s/100 iter), loss = 0.571428
I0630 01:13:25.121106 21943 solver.cpp:309]     Train net output #0: loss = 0.571429 (* 1 = 0.571429 loss)
I0630 01:13:25.121114 21943 sgd_solver.cpp:106] Iteration 900, lr = 0.0985937
I0630 01:13:27.156118 21943 solver.cpp:464] Iteration 1000, Testing net (#0)
I0630 01:13:28.792589 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.4962
I0630 01:13:28.792609 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.947001
I0630 01:13:28.792615 21943 solver.cpp:537]     Test net output #2: loss = 1.2498 (* 1 = 1.2498 loss)
I0630 01:13:28.813490 21943 solver.cpp:290] Iteration 1000 (27.0836 iter/s, 3.69227s/100 iter), loss = 0.238095
I0630 01:13:28.813510 21943 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0630 01:13:28.813519 21943 sgd_solver.cpp:106] Iteration 1000, lr = 0.0984375
I0630 01:13:30.870007 21943 solver.cpp:290] Iteration 1100 (48.628 iter/s, 2.05643s/100 iter), loss = 0.0952379
I0630 01:13:30.870028 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:13:30.870035 21943 sgd_solver.cpp:106] Iteration 1100, lr = 0.0982813
I0630 01:13:32.926277 21943 solver.cpp:290] Iteration 1200 (48.6338 iter/s, 2.05618s/100 iter), loss = 0.428571
I0630 01:13:32.926300 21943 solver.cpp:309]     Train net output #0: loss = 0.428571 (* 1 = 0.428571 loss)
I0630 01:13:32.926306 21943 sgd_solver.cpp:106] Iteration 1200, lr = 0.098125
I0630 01:13:34.990106 21943 solver.cpp:290] Iteration 1300 (48.4557 iter/s, 2.06374s/100 iter), loss = 0.666666
I0630 01:13:34.990197 21943 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 01:13:34.990206 21943 sgd_solver.cpp:106] Iteration 1300, lr = 0.0979687
I0630 01:13:37.046712 21943 solver.cpp:290] Iteration 1400 (48.6275 iter/s, 2.05645s/100 iter), loss = 0.619047
I0630 01:13:37.046733 21943 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0630 01:13:37.046741 21943 sgd_solver.cpp:106] Iteration 1400, lr = 0.0978125
I0630 01:13:39.101771 21943 solver.cpp:290] Iteration 1500 (48.6625 iter/s, 2.05497s/100 iter), loss = 0.428571
I0630 01:13:39.101794 21943 solver.cpp:309]     Train net output #0: loss = 0.428571 (* 1 = 0.428571 loss)
I0630 01:13:39.101800 21943 sgd_solver.cpp:106] Iteration 1500, lr = 0.0976562
I0630 01:13:41.159577 21943 solver.cpp:290] Iteration 1600 (48.5976 iter/s, 2.05771s/100 iter), loss = 0.190475
I0630 01:13:41.159605 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:13:41.159612 21943 sgd_solver.cpp:106] Iteration 1600, lr = 0.0975
I0630 01:13:43.228200 21943 solver.cpp:290] Iteration 1700 (48.3435 iter/s, 2.06853s/100 iter), loss = 0.523809
I0630 01:13:43.228222 21943 solver.cpp:309]     Train net output #0: loss = 0.52381 (* 1 = 0.52381 loss)
I0630 01:13:43.228229 21943 sgd_solver.cpp:106] Iteration 1700, lr = 0.0973438
I0630 01:13:45.281924 21943 solver.cpp:290] Iteration 1800 (48.6942 iter/s, 2.05363s/100 iter), loss = 0.666666
I0630 01:13:45.281950 21943 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0630 01:13:45.281960 21943 sgd_solver.cpp:106] Iteration 1800, lr = 0.0971875
I0630 01:13:47.338517 21943 solver.cpp:290] Iteration 1900 (48.6263 iter/s, 2.0565s/100 iter), loss = 0.380952
I0630 01:13:47.338551 21943 solver.cpp:309]     Train net output #0: loss = 0.380952 (* 1 = 0.380952 loss)
I0630 01:13:47.338562 21943 sgd_solver.cpp:106] Iteration 1900, lr = 0.0970313
I0630 01:13:49.374569 21943 solver.cpp:464] Iteration 2000, Testing net (#0)
I0630 01:13:51.014519 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.6411
I0630 01:13:51.014539 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.965501
I0630 01:13:51.014544 21943 solver.cpp:537]     Test net output #2: loss = 0.6985 (* 1 = 0.6985 loss)
I0630 01:13:51.040899 21943 solver.cpp:290] Iteration 2000 (27.0107 iter/s, 3.70223s/100 iter), loss = 0.190476
I0630 01:13:51.040931 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:13:51.040940 21943 sgd_solver.cpp:106] Iteration 2000, lr = 0.096875
I0630 01:13:53.091150 21943 solver.cpp:290] Iteration 2100 (48.7768 iter/s, 2.05015s/100 iter), loss = 0.47619
I0630 01:13:53.091172 21943 solver.cpp:309]     Train net output #0: loss = 0.47619 (* 1 = 0.47619 loss)
I0630 01:13:53.091179 21943 sgd_solver.cpp:106] Iteration 2100, lr = 0.0967188
I0630 01:13:55.141748 21943 solver.cpp:290] Iteration 2200 (48.7684 iter/s, 2.05051s/100 iter), loss = 0.380951
I0630 01:13:55.141770 21943 solver.cpp:309]     Train net output #0: loss = 0.380952 (* 1 = 0.380952 loss)
I0630 01:13:55.141777 21943 sgd_solver.cpp:106] Iteration 2200, lr = 0.0965625
I0630 01:13:57.193569 21943 solver.cpp:290] Iteration 2300 (48.7393 iter/s, 2.05173s/100 iter), loss = 0.190475
I0630 01:13:57.193594 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:13:57.193603 21943 sgd_solver.cpp:106] Iteration 2300, lr = 0.0964063
I0630 01:13:59.243680 21943 solver.cpp:290] Iteration 2400 (48.78 iter/s, 2.05002s/100 iter), loss = 0.380951
I0630 01:13:59.243701 21943 solver.cpp:309]     Train net output #0: loss = 0.380952 (* 1 = 0.380952 loss)
I0630 01:13:59.243707 21943 sgd_solver.cpp:106] Iteration 2400, lr = 0.09625
I0630 01:14:01.296561 21943 solver.cpp:290] Iteration 2500 (48.7141 iter/s, 2.05279s/100 iter), loss = 0.714285
I0630 01:14:01.296583 21943 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0630 01:14:01.296591 21943 sgd_solver.cpp:106] Iteration 2500, lr = 0.0960938
I0630 01:14:03.350191 21943 solver.cpp:290] Iteration 2600 (48.6964 iter/s, 2.05354s/100 iter), loss = 0.190475
I0630 01:14:03.350216 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:14:03.350224 21943 sgd_solver.cpp:106] Iteration 2600, lr = 0.0959375
I0630 01:14:05.401377 21943 solver.cpp:290] Iteration 2700 (48.7544 iter/s, 2.0511s/100 iter), loss = 0.761904
I0630 01:14:05.401484 21943 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0630 01:14:05.401494 21943 sgd_solver.cpp:106] Iteration 2700, lr = 0.0957813
I0630 01:14:07.464676 21943 solver.cpp:290] Iteration 2800 (48.4701 iter/s, 2.06313s/100 iter), loss = 0.333332
I0630 01:14:07.464704 21943 solver.cpp:309]     Train net output #0: loss = 0.333333 (* 1 = 0.333333 loss)
I0630 01:14:07.464714 21943 sgd_solver.cpp:106] Iteration 2800, lr = 0.095625
I0630 01:14:09.520499 21943 solver.cpp:290] Iteration 2900 (48.6445 iter/s, 2.05573s/100 iter), loss = 0.333332
I0630 01:14:09.520521 21943 solver.cpp:309]     Train net output #0: loss = 0.333333 (* 1 = 0.333333 loss)
I0630 01:14:09.520529 21943 sgd_solver.cpp:106] Iteration 2900, lr = 0.0954688
I0630 01:14:11.548200 21943 solver.cpp:464] Iteration 3000, Testing net (#0)
I0630 01:14:13.200207 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.7264
I0630 01:14:13.200227 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9854
I0630 01:14:13.200232 21943 solver.cpp:537]     Test net output #2: loss = 0.5192 (* 1 = 0.5192 loss)
I0630 01:14:13.220034 21943 solver.cpp:290] Iteration 3000 (27.0314 iter/s, 3.6994s/100 iter), loss = 0.285713
I0630 01:14:13.220062 21943 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0630 01:14:13.220068 21943 sgd_solver.cpp:106] Iteration 3000, lr = 0.0953125
I0630 01:14:15.271906 21943 solver.cpp:290] Iteration 3100 (48.7382 iter/s, 2.05178s/100 iter), loss = 0.42857
I0630 01:14:15.271929 21943 solver.cpp:309]     Train net output #0: loss = 0.428571 (* 1 = 0.428571 loss)
I0630 01:14:15.271935 21943 sgd_solver.cpp:106] Iteration 3100, lr = 0.0951563
I0630 01:14:17.320114 21943 solver.cpp:290] Iteration 3200 (48.8253 iter/s, 2.04812s/100 iter), loss = 0.571428
I0630 01:14:17.320137 21943 solver.cpp:309]     Train net output #0: loss = 0.571429 (* 1 = 0.571429 loss)
I0630 01:14:17.320143 21943 sgd_solver.cpp:106] Iteration 3200, lr = 0.095
I0630 01:14:19.367733 21943 solver.cpp:290] Iteration 3300 (48.8394 iter/s, 2.04753s/100 iter), loss = 0.095237
I0630 01:14:19.367763 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:14:19.367771 21943 sgd_solver.cpp:106] Iteration 3300, lr = 0.0948438
I0630 01:14:21.418843 21943 solver.cpp:290] Iteration 3400 (48.7566 iter/s, 2.05101s/100 iter), loss = 0.238094
I0630 01:14:21.418874 21943 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0630 01:14:21.418882 21943 sgd_solver.cpp:106] Iteration 3400, lr = 0.0946875
I0630 01:14:23.468662 21943 solver.cpp:290] Iteration 3500 (48.7871 iter/s, 2.04972s/100 iter), loss = 0.0476179
I0630 01:14:23.468684 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:14:23.468691 21943 sgd_solver.cpp:106] Iteration 3500, lr = 0.0945313
I0630 01:14:25.515344 21943 solver.cpp:290] Iteration 3600 (48.8617 iter/s, 2.04659s/100 iter), loss = 0.285713
I0630 01:14:25.515367 21943 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0630 01:14:25.515372 21943 sgd_solver.cpp:106] Iteration 3600, lr = 0.094375
I0630 01:14:27.561763 21943 solver.cpp:290] Iteration 3700 (48.868 iter/s, 2.04633s/100 iter), loss = -1.19209e-06
I0630 01:14:27.561784 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:14:27.561791 21943 sgd_solver.cpp:106] Iteration 3700, lr = 0.0942188
I0630 01:14:29.616595 21943 solver.cpp:290] Iteration 3800 (48.6679 iter/s, 2.05474s/100 iter), loss = 0.380951
I0630 01:14:29.616617 21943 solver.cpp:309]     Train net output #0: loss = 0.380952 (* 1 = 0.380952 loss)
I0630 01:14:29.616624 21943 sgd_solver.cpp:106] Iteration 3800, lr = 0.0940625
I0630 01:14:31.665362 21943 solver.cpp:290] Iteration 3900 (48.812 iter/s, 2.04868s/100 iter), loss = 0.285713
I0630 01:14:31.665385 21943 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0630 01:14:31.665392 21943 sgd_solver.cpp:106] Iteration 3900, lr = 0.0939062
I0630 01:14:33.696187 21943 solver.cpp:464] Iteration 4000, Testing net (#0)
I0630 01:14:35.334905 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.7707
I0630 01:14:35.334923 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9865
I0630 01:14:35.334929 21943 solver.cpp:537]     Test net output #2: loss = 0.4097 (* 1 = 0.4097 loss)
I0630 01:14:35.354562 21943 solver.cpp:290] Iteration 4000 (27.1071 iter/s, 3.68906s/100 iter), loss = 0.190475
I0630 01:14:35.354578 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:14:35.354591 21943 sgd_solver.cpp:106] Iteration 4000, lr = 0.09375
I0630 01:14:37.409919 21943 solver.cpp:290] Iteration 4100 (48.6553 iter/s, 2.05527s/100 iter), loss = 0.0476176
I0630 01:14:37.409971 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:14:37.409981 21943 sgd_solver.cpp:106] Iteration 4100, lr = 0.0935938
I0630 01:14:39.460674 21943 solver.cpp:290] Iteration 4200 (48.7653 iter/s, 2.05064s/100 iter), loss = 0.190475
I0630 01:14:39.460695 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:14:39.460702 21943 sgd_solver.cpp:106] Iteration 4200, lr = 0.0934375
I0630 01:14:41.514209 21943 solver.cpp:290] Iteration 4300 (48.6987 iter/s, 2.05344s/100 iter), loss = -1.40071e-06
I0630 01:14:41.514232 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:14:41.514238 21943 sgd_solver.cpp:106] Iteration 4300, lr = 0.0932813
I0630 01:14:43.564474 21943 solver.cpp:290] Iteration 4400 (48.7763 iter/s, 2.05018s/100 iter), loss = 0.142856
I0630 01:14:43.564496 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:14:43.564503 21943 sgd_solver.cpp:106] Iteration 4400, lr = 0.093125
I0630 01:14:45.616130 21943 solver.cpp:290] Iteration 4500 (48.7432 iter/s, 2.05157s/100 iter), loss = 0.238094
I0630 01:14:45.616153 21943 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0630 01:14:45.616159 21943 sgd_solver.cpp:106] Iteration 4500, lr = 0.0929688
I0630 01:14:47.667122 21943 solver.cpp:290] Iteration 4600 (48.759 iter/s, 2.0509s/100 iter), loss = 0.42857
I0630 01:14:47.667145 21943 solver.cpp:309]     Train net output #0: loss = 0.428571 (* 1 = 0.428571 loss)
I0630 01:14:47.667151 21943 sgd_solver.cpp:106] Iteration 4600, lr = 0.0928125
I0630 01:14:49.717648 21943 solver.cpp:290] Iteration 4700 (48.7701 iter/s, 2.05044s/100 iter), loss = 0.285713
I0630 01:14:49.717671 21943 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0630 01:14:49.717680 21943 sgd_solver.cpp:106] Iteration 4700, lr = 0.0926562
I0630 01:14:51.766410 21943 solver.cpp:290] Iteration 4800 (48.8121 iter/s, 2.04867s/100 iter), loss = 0.0952365
I0630 01:14:51.766433 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:14:51.766440 21943 sgd_solver.cpp:106] Iteration 4800, lr = 0.0925
I0630 01:14:53.815423 21943 solver.cpp:290] Iteration 4900 (48.8061 iter/s, 2.04892s/100 iter), loss = 0.333332
I0630 01:14:53.815446 21943 solver.cpp:309]     Train net output #0: loss = 0.333333 (* 1 = 0.333333 loss)
I0630 01:14:53.815452 21943 sgd_solver.cpp:106] Iteration 4900, lr = 0.0923437
I0630 01:14:55.845837 21943 solver.cpp:464] Iteration 5000, Testing net (#0)
I0630 01:14:57.484678 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.7852
I0630 01:14:57.484696 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9853
I0630 01:14:57.484701 21943 solver.cpp:537]     Test net output #2: loss = 0.4558 (* 1 = 0.4558 loss)
I0630 01:14:57.504287 21943 solver.cpp:290] Iteration 5000 (27.1096 iter/s, 3.68873s/100 iter), loss = 0.238094
I0630 01:14:57.504307 21943 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0630 01:14:57.504317 21943 sgd_solver.cpp:106] Iteration 5000, lr = 0.0921875
I0630 01:14:59.558249 21943 solver.cpp:290] Iteration 5100 (48.6884 iter/s, 2.05388s/100 iter), loss = 0.0952365
I0630 01:14:59.558272 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:14:59.558279 21943 sgd_solver.cpp:106] Iteration 5100, lr = 0.0920313
I0630 01:15:01.612354 21943 solver.cpp:290] Iteration 5200 (48.6852 iter/s, 2.05401s/100 iter), loss = 0.285713
I0630 01:15:01.612380 21943 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0630 01:15:01.612388 21943 sgd_solver.cpp:106] Iteration 5200, lr = 0.091875
I0630 01:15:03.665659 21943 solver.cpp:290] Iteration 5300 (48.7041 iter/s, 2.05321s/100 iter), loss = 0.0476174
I0630 01:15:03.665683 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:15:03.665688 21943 sgd_solver.cpp:106] Iteration 5300, lr = 0.0917188
I0630 01:15:05.714684 21943 solver.cpp:290] Iteration 5400 (48.8059 iter/s, 2.04893s/100 iter), loss = 0.285713
I0630 01:15:05.714707 21943 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0630 01:15:05.714716 21943 sgd_solver.cpp:106] Iteration 5400, lr = 0.0915625
I0630 01:15:07.773471 21943 solver.cpp:290] Iteration 5500 (48.5744 iter/s, 2.0587s/100 iter), loss = 0.238093
I0630 01:15:07.773520 21943 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0630 01:15:07.773527 21943 sgd_solver.cpp:106] Iteration 5500, lr = 0.0914062
I0630 01:15:09.823981 21943 solver.cpp:290] Iteration 5600 (48.7711 iter/s, 2.0504s/100 iter), loss = 0.190474
I0630 01:15:09.824003 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:15:09.824010 21943 sgd_solver.cpp:106] Iteration 5600, lr = 0.09125
I0630 01:15:11.875771 21943 solver.cpp:290] Iteration 5700 (48.7401 iter/s, 2.0517s/100 iter), loss = 0.285712
I0630 01:15:11.875793 21943 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0630 01:15:11.875799 21943 sgd_solver.cpp:106] Iteration 5700, lr = 0.0910937
I0630 01:15:13.929306 21943 solver.cpp:290] Iteration 5800 (48.6986 iter/s, 2.05345s/100 iter), loss = 0.285712
I0630 01:15:13.929329 21943 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0630 01:15:13.929337 21943 sgd_solver.cpp:106] Iteration 5800, lr = 0.0909375
I0630 01:15:15.987143 21943 solver.cpp:290] Iteration 5900 (48.5969 iter/s, 2.05774s/100 iter), loss = 0.523808
I0630 01:15:15.987169 21943 solver.cpp:309]     Train net output #0: loss = 0.52381 (* 1 = 0.52381 loss)
I0630 01:15:15.987177 21943 sgd_solver.cpp:106] Iteration 5900, lr = 0.0907812
I0630 01:15:18.021529 21943 solver.cpp:464] Iteration 6000, Testing net (#0)
I0630 01:15:19.661967 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.7851
I0630 01:15:19.661985 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9876
I0630 01:15:19.661990 21943 solver.cpp:537]     Test net output #2: loss = 0.457 (* 1 = 0.457 loss)
I0630 01:15:19.681789 21943 solver.cpp:290] Iteration 6000 (27.0672 iter/s, 3.69451s/100 iter), loss = 0.0952361
I0630 01:15:19.681808 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:15:19.681819 21943 sgd_solver.cpp:106] Iteration 6000, lr = 0.090625
I0630 01:15:21.736598 21943 solver.cpp:290] Iteration 6100 (48.6684 iter/s, 2.05472s/100 iter), loss = 0.095236
I0630 01:15:21.736620 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:15:21.736627 21943 sgd_solver.cpp:106] Iteration 6100, lr = 0.0904688
I0630 01:15:23.786866 21943 solver.cpp:290] Iteration 6200 (48.7762 iter/s, 2.05018s/100 iter), loss = 0.142855
I0630 01:15:23.786891 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:15:23.786900 21943 sgd_solver.cpp:106] Iteration 6200, lr = 0.0903125
I0630 01:15:25.842322 21943 solver.cpp:290] Iteration 6300 (48.6531 iter/s, 2.05537s/100 iter), loss = 0.428569
I0630 01:15:25.842345 21943 solver.cpp:309]     Train net output #0: loss = 0.428571 (* 1 = 0.428571 loss)
I0630 01:15:25.842352 21943 sgd_solver.cpp:106] Iteration 6300, lr = 0.0901562
I0630 01:15:27.895829 21943 solver.cpp:290] Iteration 6400 (48.6993 iter/s, 2.05342s/100 iter), loss = 0.0952359
I0630 01:15:27.895851 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:15:27.895859 21943 sgd_solver.cpp:106] Iteration 6400, lr = 0.09
I0630 01:15:29.944782 21943 solver.cpp:290] Iteration 6500 (48.8076 iter/s, 2.04886s/100 iter), loss = 0.0476168
I0630 01:15:29.944805 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:15:29.944813 21943 sgd_solver.cpp:106] Iteration 6500, lr = 0.0898438
I0630 01:15:31.996129 21943 solver.cpp:290] Iteration 6600 (48.7506 iter/s, 2.05126s/100 iter), loss = 0.0952359
I0630 01:15:31.996151 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:15:31.996158 21943 sgd_solver.cpp:106] Iteration 6600, lr = 0.0896875
I0630 01:15:34.046525 21943 solver.cpp:290] Iteration 6700 (48.7733 iter/s, 2.0503s/100 iter), loss = 0.428569
I0630 01:15:34.046550 21943 solver.cpp:309]     Train net output #0: loss = 0.428571 (* 1 = 0.428571 loss)
I0630 01:15:34.046556 21943 sgd_solver.cpp:106] Iteration 6700, lr = 0.0895313
I0630 01:15:36.100375 21943 solver.cpp:290] Iteration 6800 (48.6912 iter/s, 2.05376s/100 iter), loss = 0.285712
I0630 01:15:36.100401 21943 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0630 01:15:36.100409 21943 sgd_solver.cpp:106] Iteration 6800, lr = 0.089375
I0630 01:15:38.152465 21943 solver.cpp:290] Iteration 6900 (48.733 iter/s, 2.052s/100 iter), loss = -2.26498e-06
I0630 01:15:38.152525 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:15:38.152534 21943 sgd_solver.cpp:106] Iteration 6900, lr = 0.0892188
I0630 01:15:40.184358 21943 solver.cpp:464] Iteration 7000, Testing net (#0)
I0630 01:15:41.829852 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.798
I0630 01:15:41.829872 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9846
I0630 01:15:41.829877 21943 solver.cpp:537]     Test net output #2: loss = 0.449 (* 1 = 0.449 loss)
I0630 01:15:41.849567 21943 solver.cpp:290] Iteration 7000 (27.0495 iter/s, 3.69693s/100 iter), loss = 0.0952358
I0630 01:15:41.849591 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:15:41.849597 21943 sgd_solver.cpp:106] Iteration 7000, lr = 0.0890625
I0630 01:15:43.902058 21943 solver.cpp:290] Iteration 7100 (48.7234 iter/s, 2.0524s/100 iter), loss = 0.0476168
I0630 01:15:43.902081 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:15:43.902087 21943 sgd_solver.cpp:106] Iteration 7100, lr = 0.0889063
I0630 01:15:45.953395 21943 solver.cpp:290] Iteration 7200 (48.7508 iter/s, 2.05125s/100 iter), loss = 0.0952358
I0630 01:15:45.953419 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:15:45.953428 21943 sgd_solver.cpp:106] Iteration 7200, lr = 0.08875
I0630 01:15:48.002712 21943 solver.cpp:290] Iteration 7300 (48.7989 iter/s, 2.04923s/100 iter), loss = 0.142855
I0630 01:15:48.002737 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:15:48.002746 21943 sgd_solver.cpp:106] Iteration 7300, lr = 0.0885938
I0630 01:15:50.055354 21943 solver.cpp:290] Iteration 7400 (48.7198 iter/s, 2.05255s/100 iter), loss = 0.142855
I0630 01:15:50.055375 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:15:50.055383 21943 sgd_solver.cpp:106] Iteration 7400, lr = 0.0884375
I0630 01:15:52.106309 21943 solver.cpp:290] Iteration 7500 (48.7598 iter/s, 2.05087s/100 iter), loss = -2.41399e-06
I0630 01:15:52.106333 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:15:52.106339 21943 sgd_solver.cpp:106] Iteration 7500, lr = 0.0882813
I0630 01:15:54.161303 21943 solver.cpp:290] Iteration 7600 (48.6641 iter/s, 2.0549s/100 iter), loss = 0.190474
I0630 01:15:54.161325 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:15:54.161332 21943 sgd_solver.cpp:106] Iteration 7600, lr = 0.088125
I0630 01:15:56.210134 21943 solver.cpp:290] Iteration 7700 (48.8104 iter/s, 2.04874s/100 iter), loss = 0.142855
I0630 01:15:56.210155 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:15:56.210161 21943 sgd_solver.cpp:106] Iteration 7700, lr = 0.0879688
I0630 01:15:58.260845 21943 solver.cpp:290] Iteration 7800 (48.7656 iter/s, 2.05062s/100 iter), loss = 0.0952357
I0630 01:15:58.260869 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:15:58.260874 21943 sgd_solver.cpp:106] Iteration 7800, lr = 0.0878125
I0630 01:16:00.315937 21943 solver.cpp:290] Iteration 7900 (48.6618 iter/s, 2.055s/100 iter), loss = 0.0476167
I0630 01:16:00.315959 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:16:00.315966 21943 sgd_solver.cpp:106] Iteration 7900, lr = 0.0876563
I0630 01:16:02.347311 21943 solver.cpp:464] Iteration 8000, Testing net (#0)
I0630 01:16:03.986959 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.7681
I0630 01:16:03.986979 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9778
I0630 01:16:03.986984 21943 solver.cpp:537]     Test net output #2: loss = 0.5832 (* 1 = 0.5832 loss)
I0630 01:16:04.006827 21943 solver.cpp:290] Iteration 8000 (27.0947 iter/s, 3.69076s/100 iter), loss = 0.0476167
I0630 01:16:04.006847 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:16:04.006858 21943 sgd_solver.cpp:106] Iteration 8000, lr = 0.0875
I0630 01:16:06.063233 21943 solver.cpp:290] Iteration 8100 (48.6306 iter/s, 2.05632s/100 iter), loss = 0.238093
I0630 01:16:06.063256 21943 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0630 01:16:06.063262 21943 sgd_solver.cpp:106] Iteration 8100, lr = 0.0873438
I0630 01:16:08.116020 21943 solver.cpp:290] Iteration 8200 (48.7164 iter/s, 2.0527s/100 iter), loss = -2.38419e-06
I0630 01:16:08.116050 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:16:08.116058 21943 sgd_solver.cpp:106] Iteration 8200, lr = 0.0871875
I0630 01:16:10.166646 21943 solver.cpp:290] Iteration 8300 (48.7679 iter/s, 2.05053s/100 iter), loss = 0.142855
I0630 01:16:10.166728 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:16:10.166738 21943 sgd_solver.cpp:106] Iteration 8300, lr = 0.0870313
I0630 01:16:12.219529 21943 solver.cpp:290] Iteration 8400 (48.7154 iter/s, 2.05274s/100 iter), loss = 0.0952357
I0630 01:16:12.219552 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:16:12.219559 21943 sgd_solver.cpp:106] Iteration 8400, lr = 0.086875
I0630 01:16:14.271950 21943 solver.cpp:290] Iteration 8500 (48.7251 iter/s, 2.05233s/100 iter), loss = 0.142855
I0630 01:16:14.271975 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:16:14.271983 21943 sgd_solver.cpp:106] Iteration 8500, lr = 0.0867188
I0630 01:16:16.324761 21943 solver.cpp:290] Iteration 8600 (48.7159 iter/s, 2.05272s/100 iter), loss = 0.285712
I0630 01:16:16.324787 21943 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0630 01:16:16.324796 21943 sgd_solver.cpp:106] Iteration 8600, lr = 0.0865625
I0630 01:16:18.378056 21943 solver.cpp:290] Iteration 8700 (48.7044 iter/s, 2.0532s/100 iter), loss = 0.142855
I0630 01:16:18.378077 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:16:18.378084 21943 sgd_solver.cpp:106] Iteration 8700, lr = 0.0864063
I0630 01:16:20.429769 21943 solver.cpp:290] Iteration 8800 (48.7419 iter/s, 2.05162s/100 iter), loss = 0.0952355
I0630 01:16:20.429790 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:16:20.429797 21943 sgd_solver.cpp:106] Iteration 8800, lr = 0.08625
I0630 01:16:22.481655 21943 solver.cpp:290] Iteration 8900 (48.7377 iter/s, 2.0518s/100 iter), loss = 0.190474
I0630 01:16:22.481676 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:16:22.481683 21943 sgd_solver.cpp:106] Iteration 8900, lr = 0.0860937
I0630 01:16:24.511209 21943 solver.cpp:464] Iteration 9000, Testing net (#0)
I0630 01:16:26.149556 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.5532
I0630 01:16:26.149576 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.952601
I0630 01:16:26.149582 21943 solver.cpp:537]     Test net output #2: loss = 1.7537 (* 1 = 1.7537 loss)
I0630 01:16:26.169071 21943 solver.cpp:290] Iteration 9000 (27.1202 iter/s, 3.68728s/100 iter), loss = 0.142855
I0630 01:16:26.169090 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:16:26.169101 21943 sgd_solver.cpp:106] Iteration 9000, lr = 0.0859375
I0630 01:16:28.223114 21943 solver.cpp:290] Iteration 9100 (48.6865 iter/s, 2.05396s/100 iter), loss = 0.0952355
I0630 01:16:28.223136 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:16:28.223143 21943 sgd_solver.cpp:106] Iteration 9100, lr = 0.0857813
I0630 01:16:30.275079 21943 solver.cpp:290] Iteration 9200 (48.7359 iter/s, 2.05188s/100 iter), loss = 0.0476165
I0630 01:16:30.275102 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:16:30.275108 21943 sgd_solver.cpp:106] Iteration 9200, lr = 0.085625
I0630 01:16:32.326568 21943 solver.cpp:290] Iteration 9300 (48.7472 iter/s, 2.0514s/100 iter), loss = -2.65986e-06
I0630 01:16:32.326593 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:16:32.326602 21943 sgd_solver.cpp:106] Iteration 9300, lr = 0.0854688
I0630 01:16:34.376694 21943 solver.cpp:290] Iteration 9400 (48.7796 iter/s, 2.05004s/100 iter), loss = -2.71201e-06
I0630 01:16:34.376716 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:16:34.376724 21943 sgd_solver.cpp:106] Iteration 9400, lr = 0.0853125
I0630 01:16:36.429318 21943 solver.cpp:290] Iteration 9500 (48.7202 iter/s, 2.05254s/100 iter), loss = 0.0952353
I0630 01:16:36.429342 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:16:36.429348 21943 sgd_solver.cpp:106] Iteration 9500, lr = 0.0851563
I0630 01:16:38.478729 21943 solver.cpp:290] Iteration 9600 (48.7966 iter/s, 2.04932s/100 iter), loss = 0.0476163
I0630 01:16:38.478752 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:16:38.478759 21943 sgd_solver.cpp:106] Iteration 9600, lr = 0.085
I0630 01:16:40.529894 21943 solver.cpp:290] Iteration 9700 (48.7549 iter/s, 2.05108s/100 iter), loss = 0.0476163
I0630 01:16:40.529954 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:16:40.529963 21943 sgd_solver.cpp:106] Iteration 9700, lr = 0.0848437
I0630 01:16:42.581105 21943 solver.cpp:290] Iteration 9800 (48.7547 iter/s, 2.05109s/100 iter), loss = -2.74181e-06
I0630 01:16:42.581127 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:16:42.581135 21943 sgd_solver.cpp:106] Iteration 9800, lr = 0.0846875
I0630 01:16:44.639417 21943 solver.cpp:290] Iteration 9900 (48.5856 iter/s, 2.05822s/100 iter), loss = 0.0476163
I0630 01:16:44.639441 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:16:44.639447 21943 sgd_solver.cpp:106] Iteration 9900, lr = 0.0845312
I0630 01:16:46.668831 21943 solver.cpp:591] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2_iter_10000.caffemodel
I0630 01:16:46.692525 21943 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2_iter_10000.solverstate
I0630 01:16:46.699898 21943 solver.cpp:464] Iteration 10000, Testing net (#0)
I0630 01:16:48.345657 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8068
I0630 01:16:48.345677 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9923
I0630 01:16:48.345682 21943 solver.cpp:537]     Test net output #2: loss = 0.4304 (* 1 = 0.4304 loss)
I0630 01:16:48.365600 21943 solver.cpp:290] Iteration 10000 (26.8381 iter/s, 3.72604s/100 iter), loss = -2.77162e-06
I0630 01:16:48.365625 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:16:48.365633 21943 sgd_solver.cpp:106] Iteration 10000, lr = 0.084375
I0630 01:16:50.423287 21943 solver.cpp:290] Iteration 10100 (48.6004 iter/s, 2.0576s/100 iter), loss = 0.142854
I0630 01:16:50.423312 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:16:50.423321 21943 sgd_solver.cpp:106] Iteration 10100, lr = 0.0842188
I0630 01:16:52.477499 21943 solver.cpp:290] Iteration 10200 (48.6827 iter/s, 2.05412s/100 iter), loss = 0.0476162
I0630 01:16:52.477524 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:16:52.477532 21943 sgd_solver.cpp:106] Iteration 10200, lr = 0.0840625
I0630 01:16:54.527721 21943 solver.cpp:290] Iteration 10300 (48.7773 iter/s, 2.05013s/100 iter), loss = 0.0476162
I0630 01:16:54.527743 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:16:54.527750 21943 sgd_solver.cpp:106] Iteration 10300, lr = 0.0839063
I0630 01:16:56.590679 21943 solver.cpp:290] Iteration 10400 (48.4762 iter/s, 2.06287s/100 iter), loss = 0.428569
I0630 01:16:56.590703 21943 solver.cpp:309]     Train net output #0: loss = 0.428571 (* 1 = 0.428571 loss)
I0630 01:16:56.590713 21943 sgd_solver.cpp:106] Iteration 10400, lr = 0.08375
I0630 01:16:58.646298 21943 solver.cpp:290] Iteration 10500 (48.6493 iter/s, 2.05553s/100 iter), loss = 0.142854
I0630 01:16:58.646322 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:16:58.646328 21943 sgd_solver.cpp:106] Iteration 10500, lr = 0.0835937
I0630 01:17:00.697163 21943 solver.cpp:290] Iteration 10600 (48.762 iter/s, 2.05078s/100 iter), loss = 0.0476161
I0630 01:17:00.697185 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:17:00.697193 21943 sgd_solver.cpp:106] Iteration 10600, lr = 0.0834375
I0630 01:17:02.753464 21943 solver.cpp:290] Iteration 10700 (48.6331 iter/s, 2.05621s/100 iter), loss = 0.142854
I0630 01:17:02.753487 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:17:02.753494 21943 sgd_solver.cpp:106] Iteration 10700, lr = 0.0832812
I0630 01:17:04.805038 21943 solver.cpp:290] Iteration 10800 (48.7452 iter/s, 2.05148s/100 iter), loss = -3.01749e-06
I0630 01:17:04.805061 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:17:04.805083 21943 sgd_solver.cpp:106] Iteration 10800, lr = 0.083125
I0630 01:17:06.852421 21943 solver.cpp:290] Iteration 10900 (48.845 iter/s, 2.04729s/100 iter), loss = -3.02494e-06
I0630 01:17:06.852442 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:17:06.852448 21943 sgd_solver.cpp:106] Iteration 10900, lr = 0.0829687
I0630 01:17:08.900642 21943 solver.cpp:464] Iteration 11000, Testing net (#0)
I0630 01:17:10.542145 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.7851
I0630 01:17:10.542204 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9859
I0630 01:17:10.542212 21943 solver.cpp:537]     Test net output #2: loss = 0.5706 (* 1 = 0.5706 loss)
I0630 01:17:10.562152 21943 solver.cpp:290] Iteration 11000 (26.9571 iter/s, 3.7096s/100 iter), loss = 0.047616
I0630 01:17:10.562172 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:17:10.562180 21943 sgd_solver.cpp:106] Iteration 11000, lr = 0.0828125
I0630 01:17:12.617103 21943 solver.cpp:290] Iteration 11100 (48.665 iter/s, 2.05486s/100 iter), loss = 0.238092
I0630 01:17:12.617126 21943 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0630 01:17:12.617133 21943 sgd_solver.cpp:106] Iteration 11100, lr = 0.0826563
I0630 01:17:14.672242 21943 solver.cpp:290] Iteration 11200 (48.6606 iter/s, 2.05505s/100 iter), loss = 0.33333
I0630 01:17:14.672264 21943 solver.cpp:309]     Train net output #0: loss = 0.333333 (* 1 = 0.333333 loss)
I0630 01:17:14.672271 21943 sgd_solver.cpp:106] Iteration 11200, lr = 0.0825
I0630 01:17:16.726878 21943 solver.cpp:290] Iteration 11300 (48.6725 iter/s, 2.05455s/100 iter), loss = 0.047616
I0630 01:17:16.726900 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:17:16.726907 21943 sgd_solver.cpp:106] Iteration 11300, lr = 0.0823437
I0630 01:17:18.781455 21943 solver.cpp:290] Iteration 11400 (48.6739 iter/s, 2.05449s/100 iter), loss = 0.095235
I0630 01:17:18.781476 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:17:18.781483 21943 sgd_solver.cpp:106] Iteration 11400, lr = 0.0821875
I0630 01:17:20.837533 21943 solver.cpp:290] Iteration 11500 (48.6384 iter/s, 2.05599s/100 iter), loss = 0.095235
I0630 01:17:20.837558 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:17:20.837563 21943 sgd_solver.cpp:106] Iteration 11500, lr = 0.0820312
I0630 01:17:22.895895 21943 solver.cpp:290] Iteration 11600 (48.5845 iter/s, 2.05827s/100 iter), loss = 0.0476159
I0630 01:17:22.895920 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:17:22.895926 21943 sgd_solver.cpp:106] Iteration 11600, lr = 0.081875
I0630 01:17:24.951128 21943 solver.cpp:290] Iteration 11700 (48.6585 iter/s, 2.05514s/100 iter), loss = 0.0476159
I0630 01:17:24.951154 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:17:24.951161 21943 sgd_solver.cpp:106] Iteration 11700, lr = 0.0817188
I0630 01:17:27.006361 21943 solver.cpp:290] Iteration 11800 (48.6585 iter/s, 2.05514s/100 iter), loss = 0.0476159
I0630 01:17:27.006384 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:17:27.006393 21943 sgd_solver.cpp:106] Iteration 11800, lr = 0.0815625
I0630 01:17:29.062778 21943 solver.cpp:290] Iteration 11900 (48.6304 iter/s, 2.05633s/100 iter), loss = 0.142854
I0630 01:17:29.062803 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:17:29.062811 21943 sgd_solver.cpp:106] Iteration 11900, lr = 0.0814063
I0630 01:17:31.095845 21943 solver.cpp:464] Iteration 12000, Testing net (#0)
I0630 01:17:32.743042 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.7398
I0630 01:17:32.743062 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9619
I0630 01:17:32.743068 21943 solver.cpp:537]     Test net output #2: loss = 0.7685 (* 1 = 0.7685 loss)
I0630 01:17:32.762758 21943 solver.cpp:290] Iteration 12000 (27.0282 iter/s, 3.69984s/100 iter), loss = 0.142854
I0630 01:17:32.762776 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:17:32.762787 21943 sgd_solver.cpp:106] Iteration 12000, lr = 0.08125
I0630 01:17:34.817679 21943 solver.cpp:290] Iteration 12100 (48.6657 iter/s, 2.05484s/100 iter), loss = 0.238092
I0630 01:17:34.817703 21943 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0630 01:17:34.817709 21943 sgd_solver.cpp:106] Iteration 12100, lr = 0.0810938
I0630 01:17:36.868952 21943 solver.cpp:290] Iteration 12200 (48.7523 iter/s, 2.05118s/100 iter), loss = -3.18885e-06
I0630 01:17:36.868976 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:17:36.868983 21943 sgd_solver.cpp:106] Iteration 12200, lr = 0.0809375
I0630 01:17:38.920893 21943 solver.cpp:290] Iteration 12300 (48.7365 iter/s, 2.05185s/100 iter), loss = -3.15905e-06
I0630 01:17:38.920917 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:17:38.920923 21943 sgd_solver.cpp:106] Iteration 12300, lr = 0.0807813
I0630 01:17:40.973886 21943 solver.cpp:290] Iteration 12400 (48.7115 iter/s, 2.0529s/100 iter), loss = 0.476187
I0630 01:17:40.973974 21943 solver.cpp:309]     Train net output #0: loss = 0.47619 (* 1 = 0.47619 loss)
I0630 01:17:40.973983 21943 sgd_solver.cpp:106] Iteration 12400, lr = 0.080625
I0630 01:17:43.026212 21943 solver.cpp:290] Iteration 12500 (48.7288 iter/s, 2.05217s/100 iter), loss = 0.0476159
I0630 01:17:43.026234 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:17:43.026242 21943 sgd_solver.cpp:106] Iteration 12500, lr = 0.0804688
I0630 01:17:45.086058 21943 solver.cpp:290] Iteration 12600 (48.5495 iter/s, 2.05975s/100 iter), loss = 0.190473
I0630 01:17:45.086083 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:17:45.086092 21943 sgd_solver.cpp:106] Iteration 12600, lr = 0.0803125
I0630 01:17:47.139968 21943 solver.cpp:290] Iteration 12700 (48.6898 iter/s, 2.05382s/100 iter), loss = 0.142854
I0630 01:17:47.139991 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:17:47.139997 21943 sgd_solver.cpp:106] Iteration 12700, lr = 0.0801563
I0630 01:17:49.194557 21943 solver.cpp:290] Iteration 12800 (48.6736 iter/s, 2.0545s/100 iter), loss = -3.18885e-06
I0630 01:17:49.194579 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:17:49.194586 21943 sgd_solver.cpp:106] Iteration 12800, lr = 0.08
I0630 01:17:51.247160 21943 solver.cpp:290] Iteration 12900 (48.7207 iter/s, 2.05251s/100 iter), loss = 0.285711
I0630 01:17:51.247185 21943 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0630 01:17:51.247193 21943 sgd_solver.cpp:106] Iteration 12900, lr = 0.0798438
I0630 01:17:53.282796 21943 solver.cpp:464] Iteration 13000, Testing net (#0)
I0630 01:17:54.922178 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.7142
I0630 01:17:54.922197 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9782
I0630 01:17:54.922202 21943 solver.cpp:537]     Test net output #2: loss = 0.9118 (* 1 = 0.9118 loss)
I0630 01:17:54.942526 21943 solver.cpp:290] Iteration 13000 (27.0619 iter/s, 3.69523s/100 iter), loss = 0.0952349
I0630 01:17:54.942544 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:17:54.942556 21943 sgd_solver.cpp:106] Iteration 13000, lr = 0.0796875
I0630 01:17:56.998826 21943 solver.cpp:290] Iteration 13100 (48.6331 iter/s, 2.05621s/100 iter), loss = 0.0952349
I0630 01:17:56.998859 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:17:56.998868 21943 sgd_solver.cpp:106] Iteration 13100, lr = 0.0795313
I0630 01:17:59.053045 21943 solver.cpp:290] Iteration 13200 (48.6827 iter/s, 2.05412s/100 iter), loss = 0.0476158
I0630 01:17:59.053072 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:17:59.053081 21943 sgd_solver.cpp:106] Iteration 13200, lr = 0.079375
I0630 01:18:01.109611 21943 solver.cpp:290] Iteration 13300 (48.6269 iter/s, 2.05647s/100 iter), loss = 0.0952349
I0630 01:18:01.109633 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:18:01.109639 21943 sgd_solver.cpp:106] Iteration 13300, lr = 0.0792188
I0630 01:18:03.164479 21943 solver.cpp:290] Iteration 13400 (48.667 iter/s, 2.05478s/100 iter), loss = 0.190473
I0630 01:18:03.164501 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:18:03.164508 21943 sgd_solver.cpp:106] Iteration 13400, lr = 0.0790625
I0630 01:18:05.218842 21943 solver.cpp:290] Iteration 13500 (48.679 iter/s, 2.05427s/100 iter), loss = -3.21865e-06
I0630 01:18:05.218865 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:18:05.218871 21943 sgd_solver.cpp:106] Iteration 13500, lr = 0.0789063
I0630 01:18:07.276069 21943 solver.cpp:290] Iteration 13600 (48.6112 iter/s, 2.05714s/100 iter), loss = -3.23355e-06
I0630 01:18:07.276094 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:18:07.276103 21943 sgd_solver.cpp:106] Iteration 13600, lr = 0.07875
I0630 01:18:09.331840 21943 solver.cpp:290] Iteration 13700 (48.6457 iter/s, 2.05568s/100 iter), loss = -3.22238e-06
I0630 01:18:09.331861 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:18:09.331868 21943 sgd_solver.cpp:106] Iteration 13700, lr = 0.0785938
I0630 01:18:11.383746 21943 solver.cpp:290] Iteration 13800 (48.7373 iter/s, 2.05182s/100 iter), loss = 0.0952349
I0630 01:18:11.383810 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:18:11.383821 21943 sgd_solver.cpp:106] Iteration 13800, lr = 0.0784375
I0630 01:18:13.441570 21943 solver.cpp:290] Iteration 13900 (48.5981 iter/s, 2.05769s/100 iter), loss = -3.17395e-06
I0630 01:18:13.441604 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:18:13.441614 21943 sgd_solver.cpp:106] Iteration 13900, lr = 0.0782812
I0630 01:18:15.483846 21943 solver.cpp:464] Iteration 14000, Testing net (#0)
I0630 01:18:17.125083 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.7197
I0630 01:18:17.125102 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9706
I0630 01:18:17.125108 21943 solver.cpp:537]     Test net output #2: loss = 0.996 (* 1 = 0.996 loss)
I0630 01:18:17.144989 21943 solver.cpp:290] Iteration 14000 (27.0031 iter/s, 3.70327s/100 iter), loss = -3.18885e-06
I0630 01:18:17.145007 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:18:17.145018 21943 sgd_solver.cpp:106] Iteration 14000, lr = 0.078125
I0630 01:18:19.201264 21943 solver.cpp:290] Iteration 14100 (48.6336 iter/s, 2.05619s/100 iter), loss = 0.476187
I0630 01:18:19.201287 21943 solver.cpp:309]     Train net output #0: loss = 0.47619 (* 1 = 0.47619 loss)
I0630 01:18:19.201293 21943 sgd_solver.cpp:106] Iteration 14100, lr = 0.0779688
I0630 01:18:21.253289 21943 solver.cpp:290] Iteration 14200 (48.7345 iter/s, 2.05194s/100 iter), loss = 0.0476159
I0630 01:18:21.253312 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:18:21.253320 21943 sgd_solver.cpp:106] Iteration 14200, lr = 0.0778125
I0630 01:18:23.306342 21943 solver.cpp:290] Iteration 14300 (48.71 iter/s, 2.05296s/100 iter), loss = 0.0952349
I0630 01:18:23.306365 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:18:23.306372 21943 sgd_solver.cpp:106] Iteration 14300, lr = 0.0776563
I0630 01:18:25.358398 21943 solver.cpp:290] Iteration 14400 (48.7338 iter/s, 2.05197s/100 iter), loss = -3.26335e-06
I0630 01:18:25.358419 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:18:25.358425 21943 sgd_solver.cpp:106] Iteration 14400, lr = 0.0775
I0630 01:18:27.421828 21943 solver.cpp:290] Iteration 14500 (48.4651 iter/s, 2.06334s/100 iter), loss = 0.380949
I0630 01:18:27.421851 21943 solver.cpp:309]     Train net output #0: loss = 0.380952 (* 1 = 0.380952 loss)
I0630 01:18:27.421857 21943 sgd_solver.cpp:106] Iteration 14500, lr = 0.0773438
I0630 01:18:29.477083 21943 solver.cpp:290] Iteration 14600 (48.6579 iter/s, 2.05517s/100 iter), loss = -3.24845e-06
I0630 01:18:29.477107 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:18:29.477113 21943 sgd_solver.cpp:106] Iteration 14600, lr = 0.0771875
I0630 01:18:31.531816 21943 solver.cpp:290] Iteration 14700 (48.6702 iter/s, 2.05464s/100 iter), loss = 0.142854
I0630 01:18:31.531837 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:18:31.531846 21943 sgd_solver.cpp:106] Iteration 14700, lr = 0.0770312
I0630 01:18:33.585554 21943 solver.cpp:290] Iteration 14800 (48.6938 iter/s, 2.05365s/100 iter), loss = -3.24845e-06
I0630 01:18:33.585577 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:18:33.585582 21943 sgd_solver.cpp:106] Iteration 14800, lr = 0.076875
I0630 01:18:35.637014 21943 solver.cpp:290] Iteration 14900 (48.7479 iter/s, 2.05137s/100 iter), loss = -3.24845e-06
I0630 01:18:35.637038 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:18:35.637044 21943 sgd_solver.cpp:106] Iteration 14900, lr = 0.0767187
I0630 01:18:37.672020 21943 solver.cpp:464] Iteration 15000, Testing net (#0)
I0630 01:18:39.320135 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.6644
I0630 01:18:39.320155 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.967
I0630 01:18:39.320163 21943 solver.cpp:537]     Test net output #2: loss = 1.2482 (* 1 = 1.2482 loss)
I0630 01:18:39.340472 21943 solver.cpp:290] Iteration 15000 (27.0028 iter/s, 3.70332s/100 iter), loss = -3.23355e-06
I0630 01:18:39.340493 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:18:39.340502 21943 sgd_solver.cpp:106] Iteration 15000, lr = 0.0765625
I0630 01:18:41.396857 21943 solver.cpp:290] Iteration 15100 (48.6311 iter/s, 2.0563s/100 iter), loss = 0.142854
I0630 01:18:41.396914 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:18:41.396921 21943 sgd_solver.cpp:106] Iteration 15100, lr = 0.0764063
I0630 01:18:43.451578 21943 solver.cpp:290] Iteration 15200 (48.6713 iter/s, 2.0546s/100 iter), loss = 0.142854
I0630 01:18:43.451601 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:18:43.451609 21943 sgd_solver.cpp:106] Iteration 15200, lr = 0.07625
I0630 01:18:45.507128 21943 solver.cpp:290] Iteration 15300 (48.6509 iter/s, 2.05546s/100 iter), loss = 0.142854
I0630 01:18:45.507149 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:18:45.507156 21943 sgd_solver.cpp:106] Iteration 15300, lr = 0.0760938
I0630 01:18:47.562417 21943 solver.cpp:290] Iteration 15400 (48.657 iter/s, 2.0552s/100 iter), loss = -3.30806e-06
I0630 01:18:47.562438 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:18:47.562445 21943 sgd_solver.cpp:106] Iteration 15400, lr = 0.0759375
I0630 01:18:49.619946 21943 solver.cpp:290] Iteration 15500 (48.6041 iter/s, 2.05744s/100 iter), loss = 0.0476157
I0630 01:18:49.619971 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:18:49.619976 21943 sgd_solver.cpp:106] Iteration 15500, lr = 0.0757812
I0630 01:18:51.684916 21943 solver.cpp:290] Iteration 15600 (48.429 iter/s, 2.06488s/100 iter), loss = 0.0476158
I0630 01:18:51.684939 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:18:51.684945 21943 sgd_solver.cpp:106] Iteration 15600, lr = 0.075625
I0630 01:18:53.740051 21943 solver.cpp:290] Iteration 15700 (48.6607 iter/s, 2.05505s/100 iter), loss = -3.31551e-06
I0630 01:18:53.740072 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:18:53.740079 21943 sgd_solver.cpp:106] Iteration 15700, lr = 0.0754687
I0630 01:18:55.793768 21943 solver.cpp:290] Iteration 15800 (48.6943 iter/s, 2.05363s/100 iter), loss = 0.142854
I0630 01:18:55.793792 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:18:55.793800 21943 sgd_solver.cpp:106] Iteration 15800, lr = 0.0753125
I0630 01:18:57.850492 21943 solver.cpp:290] Iteration 15900 (48.6231 iter/s, 2.05663s/100 iter), loss = 0.238092
I0630 01:18:57.850512 21943 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0630 01:18:57.850520 21943 sgd_solver.cpp:106] Iteration 15900, lr = 0.0751562
I0630 01:18:59.883080 21943 solver.cpp:464] Iteration 16000, Testing net (#0)
I0630 01:19:01.521838 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8027
I0630 01:19:01.521857 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9901
I0630 01:19:01.521862 21943 solver.cpp:537]     Test net output #2: loss = 0.5056 (* 1 = 0.5056 loss)
I0630 01:19:01.541549 21943 solver.cpp:290] Iteration 16000 (27.0935 iter/s, 3.69093s/100 iter), loss = -3.36766e-06
I0630 01:19:01.541566 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:19:01.541579 21943 sgd_solver.cpp:106] Iteration 16000, lr = 0.075
I0630 01:19:03.594848 21943 solver.cpp:290] Iteration 16100 (48.7041 iter/s, 2.05322s/100 iter), loss = -3.38256e-06
I0630 01:19:03.594869 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:19:03.594877 21943 sgd_solver.cpp:106] Iteration 16100, lr = 0.0748438
I0630 01:19:05.645861 21943 solver.cpp:290] Iteration 16200 (48.7585 iter/s, 2.05093s/100 iter), loss = 0.33333
I0630 01:19:05.645884 21943 solver.cpp:309]     Train net output #0: loss = 0.333333 (* 1 = 0.333333 loss)
I0630 01:19:05.645890 21943 sgd_solver.cpp:106] Iteration 16200, lr = 0.0746875
I0630 01:19:07.706898 21943 solver.cpp:290] Iteration 16300 (48.5213 iter/s, 2.06095s/100 iter), loss = 0.190473
I0630 01:19:07.706921 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:19:07.706928 21943 sgd_solver.cpp:106] Iteration 16300, lr = 0.0745312
I0630 01:19:09.762691 21943 solver.cpp:290] Iteration 16400 (48.6451 iter/s, 2.0557s/100 iter), loss = 0.0476157
I0630 01:19:09.762714 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:19:09.762720 21943 sgd_solver.cpp:106] Iteration 16400, lr = 0.074375
I0630 01:19:11.814836 21943 solver.cpp:290] Iteration 16500 (48.7317 iter/s, 2.05205s/100 iter), loss = 0.0476157
I0630 01:19:11.814899 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:19:11.814908 21943 sgd_solver.cpp:106] Iteration 16500, lr = 0.0742188
I0630 01:19:13.871634 21943 solver.cpp:290] Iteration 16600 (48.6223 iter/s, 2.05667s/100 iter), loss = 0.0952347
I0630 01:19:13.871655 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:19:13.871662 21943 sgd_solver.cpp:106] Iteration 16600, lr = 0.0740625
I0630 01:19:15.923578 21943 solver.cpp:290] Iteration 16700 (48.7364 iter/s, 2.05186s/100 iter), loss = 0.142854
I0630 01:19:15.923611 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:19:15.923624 21943 sgd_solver.cpp:106] Iteration 16700, lr = 0.0739063
I0630 01:19:17.975011 21943 solver.cpp:290] Iteration 16800 (48.7487 iter/s, 2.05134s/100 iter), loss = -3.44589e-06
I0630 01:19:17.975040 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:19:17.975049 21943 sgd_solver.cpp:106] Iteration 16800, lr = 0.07375
I0630 01:19:20.031059 21943 solver.cpp:290] Iteration 16900 (48.6392 iter/s, 2.05596s/100 iter), loss = 0.285711
I0630 01:19:20.031081 21943 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0630 01:19:20.031088 21943 sgd_solver.cpp:106] Iteration 16900, lr = 0.0735938
I0630 01:19:22.066095 21943 solver.cpp:464] Iteration 17000, Testing net (#0)
I0630 01:19:23.705654 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.7954
I0630 01:19:23.705673 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9886
I0630 01:19:23.705679 21943 solver.cpp:537]     Test net output #2: loss = 0.5607 (* 1 = 0.5607 loss)
I0630 01:19:23.725380 21943 solver.cpp:290] Iteration 17000 (27.0696 iter/s, 3.69419s/100 iter), loss = -3.50177e-06
I0630 01:19:23.725399 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:19:23.725409 21943 sgd_solver.cpp:106] Iteration 17000, lr = 0.0734375
I0630 01:19:25.790513 21943 solver.cpp:290] Iteration 17100 (48.425 iter/s, 2.06505s/100 iter), loss = -3.48687e-06
I0630 01:19:25.790537 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:19:25.790544 21943 sgd_solver.cpp:106] Iteration 17100, lr = 0.0732813
I0630 01:19:27.840662 21943 solver.cpp:290] Iteration 17200 (48.7791 iter/s, 2.05006s/100 iter), loss = 0.380949
I0630 01:19:27.840684 21943 solver.cpp:309]     Train net output #0: loss = 0.380952 (* 1 = 0.380952 loss)
I0630 01:19:27.840692 21943 sgd_solver.cpp:106] Iteration 17200, lr = 0.073125
I0630 01:19:29.894572 21943 solver.cpp:290] Iteration 17300 (48.6898 iter/s, 2.05382s/100 iter), loss = -3.51667e-06
I0630 01:19:29.894594 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:19:29.894601 21943 sgd_solver.cpp:106] Iteration 17300, lr = 0.0729688
I0630 01:19:31.950856 21943 solver.cpp:290] Iteration 17400 (48.6335 iter/s, 2.05619s/100 iter), loss = -3.5204e-06
I0630 01:19:31.950880 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:19:31.950886 21943 sgd_solver.cpp:106] Iteration 17400, lr = 0.0728125
I0630 01:19:34.005882 21943 solver.cpp:290] Iteration 17500 (48.6633 iter/s, 2.05494s/100 iter), loss = -3.51667e-06
I0630 01:19:34.005906 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:19:34.005913 21943 sgd_solver.cpp:106] Iteration 17500, lr = 0.0726563
I0630 01:19:36.061173 21943 solver.cpp:290] Iteration 17600 (48.657 iter/s, 2.0552s/100 iter), loss = -3.56138e-06
I0630 01:19:36.061197 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:19:36.061203 21943 sgd_solver.cpp:106] Iteration 17600, lr = 0.0725
I0630 01:19:38.116346 21943 solver.cpp:290] Iteration 17700 (48.6598 iter/s, 2.05508s/100 iter), loss = 0.0476155
I0630 01:19:38.116369 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:19:38.116376 21943 sgd_solver.cpp:106] Iteration 17700, lr = 0.0723438
I0630 01:19:40.170312 21943 solver.cpp:290] Iteration 17800 (48.6884 iter/s, 2.05388s/100 iter), loss = 0.0952346
I0630 01:19:40.170351 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:19:40.170357 21943 sgd_solver.cpp:106] Iteration 17800, lr = 0.0721875
I0630 01:19:42.222614 21943 solver.cpp:290] Iteration 17900 (48.7282 iter/s, 2.0522s/100 iter), loss = -3.56138e-06
I0630 01:19:42.222682 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:19:42.222692 21943 sgd_solver.cpp:106] Iteration 17900, lr = 0.0720313
I0630 01:19:44.256491 21943 solver.cpp:464] Iteration 18000, Testing net (#0)
I0630 01:19:45.895061 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.7827
I0630 01:19:45.895081 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9892
I0630 01:19:45.895086 21943 solver.cpp:537]     Test net output #2: loss = 0.7233 (* 1 = 0.7233 loss)
I0630 01:19:45.915305 21943 solver.cpp:290] Iteration 18000 (27.0818 iter/s, 3.69252s/100 iter), loss = 0.285711
I0630 01:19:45.915323 21943 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0630 01:19:45.915334 21943 sgd_solver.cpp:106] Iteration 18000, lr = 0.071875
I0630 01:19:47.970594 21943 solver.cpp:290] Iteration 18100 (48.6569 iter/s, 2.0552s/100 iter), loss = -3.53158e-06
I0630 01:19:47.970616 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:19:47.970623 21943 sgd_solver.cpp:106] Iteration 18100, lr = 0.0717188
I0630 01:19:50.024829 21943 solver.cpp:290] Iteration 18200 (48.6821 iter/s, 2.05414s/100 iter), loss = 0.0476155
I0630 01:19:50.024854 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:19:50.024863 21943 sgd_solver.cpp:106] Iteration 18200, lr = 0.0715625
I0630 01:19:52.077311 21943 solver.cpp:290] Iteration 18300 (48.7236 iter/s, 2.05239s/100 iter), loss = 0.0952345
I0630 01:19:52.077337 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:19:52.077345 21943 sgd_solver.cpp:106] Iteration 18300, lr = 0.0714063
I0630 01:19:54.131547 21943 solver.cpp:290] Iteration 18400 (48.6821 iter/s, 2.05414s/100 iter), loss = 0.0476155
I0630 01:19:54.131572 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:19:54.131582 21943 sgd_solver.cpp:106] Iteration 18400, lr = 0.07125
I0630 01:19:56.185421 21943 solver.cpp:290] Iteration 18500 (48.6906 iter/s, 2.05378s/100 iter), loss = -3.59863e-06
I0630 01:19:56.185444 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:19:56.185451 21943 sgd_solver.cpp:106] Iteration 18500, lr = 0.0710938
I0630 01:19:58.238538 21943 solver.cpp:290] Iteration 18600 (48.7085 iter/s, 2.05303s/100 iter), loss = 0.0952345
I0630 01:19:58.238561 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:19:58.238569 21943 sgd_solver.cpp:106] Iteration 18600, lr = 0.0709375
I0630 01:20:00.291978 21943 solver.cpp:290] Iteration 18700 (48.7009 iter/s, 2.05335s/100 iter), loss = -3.60608e-06
I0630 01:20:00.292001 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:20:00.292007 21943 sgd_solver.cpp:106] Iteration 18700, lr = 0.0707813
I0630 01:20:02.345439 21943 solver.cpp:290] Iteration 18800 (48.7004 iter/s, 2.05337s/100 iter), loss = 0.0476154
I0630 01:20:02.345463 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:20:02.345469 21943 sgd_solver.cpp:106] Iteration 18800, lr = 0.070625
I0630 01:20:04.399801 21943 solver.cpp:290] Iteration 18900 (48.6791 iter/s, 2.05427s/100 iter), loss = -3.60608e-06
I0630 01:20:04.399823 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:20:04.399830 21943 sgd_solver.cpp:106] Iteration 18900, lr = 0.0704687
I0630 01:20:06.431807 21943 solver.cpp:464] Iteration 19000, Testing net (#0)
I0630 01:20:08.073235 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8114
I0630 01:20:08.073256 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9895
I0630 01:20:08.073261 21943 solver.cpp:537]     Test net output #2: loss = 0.5442 (* 1 = 0.5442 loss)
I0630 01:20:08.092962 21943 solver.cpp:290] Iteration 19000 (27.078 iter/s, 3.69303s/100 iter), loss = -3.62098e-06
I0630 01:20:08.092980 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:20:08.092993 21943 sgd_solver.cpp:106] Iteration 19000, lr = 0.0703125
I0630 01:20:10.145634 21943 solver.cpp:290] Iteration 19100 (48.719 iter/s, 2.05259s/100 iter), loss = 0.0476154
I0630 01:20:10.145658 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:20:10.145664 21943 sgd_solver.cpp:106] Iteration 19100, lr = 0.0701563
I0630 01:20:12.197434 21943 solver.cpp:290] Iteration 19200 (48.7398 iter/s, 2.05171s/100 iter), loss = 0.0476154
I0630 01:20:12.197456 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:20:12.197463 21943 sgd_solver.cpp:106] Iteration 19200, lr = 0.07
I0630 01:20:14.249186 21943 solver.cpp:290] Iteration 19300 (48.741 iter/s, 2.05166s/100 iter), loss = -3.60608e-06
I0630 01:20:14.249238 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:20:14.249246 21943 sgd_solver.cpp:106] Iteration 19300, lr = 0.0698438
I0630 01:20:16.305081 21943 solver.cpp:290] Iteration 19400 (48.6434 iter/s, 2.05578s/100 iter), loss = 0.0476154
I0630 01:20:16.305105 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:20:16.305112 21943 sgd_solver.cpp:106] Iteration 19400, lr = 0.0696875
I0630 01:20:18.360811 21943 solver.cpp:290] Iteration 19500 (48.6466 iter/s, 2.05564s/100 iter), loss = 0.190473
I0630 01:20:18.360833 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:20:18.360839 21943 sgd_solver.cpp:106] Iteration 19500, lr = 0.0695313
I0630 01:20:20.413972 21943 solver.cpp:290] Iteration 19600 (48.7075 iter/s, 2.05307s/100 iter), loss = -3.68059e-06
I0630 01:20:20.413996 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:20:20.414002 21943 sgd_solver.cpp:106] Iteration 19600, lr = 0.069375
I0630 01:20:22.474704 21943 solver.cpp:290] Iteration 19700 (48.5286 iter/s, 2.06064s/100 iter), loss = 0.0476154
I0630 01:20:22.474726 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:20:22.474732 21943 sgd_solver.cpp:106] Iteration 19700, lr = 0.0692187
I0630 01:20:24.528297 21943 solver.cpp:290] Iteration 19800 (48.6973 iter/s, 2.0535s/100 iter), loss = -3.66569e-06
I0630 01:20:24.528319 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:20:24.528326 21943 sgd_solver.cpp:106] Iteration 19800, lr = 0.0690625
I0630 01:20:26.580420 21943 solver.cpp:290] Iteration 19900 (48.7321 iter/s, 2.05203s/100 iter), loss = -3.71039e-06
I0630 01:20:26.580447 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:20:26.580456 21943 sgd_solver.cpp:106] Iteration 19900, lr = 0.0689062
I0630 01:20:28.613729 21943 solver.cpp:591] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2_iter_20000.caffemodel
I0630 01:20:28.630277 21943 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2_iter_20000.solverstate
I0630 01:20:28.637688 21943 solver.cpp:464] Iteration 20000, Testing net (#0)
I0630 01:20:30.274785 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8185
I0630 01:20:30.274803 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9914
I0630 01:20:30.274811 21943 solver.cpp:537]     Test net output #2: loss = 0.4478 (* 1 = 0.4478 loss)
I0630 01:20:30.294839 21943 solver.cpp:290] Iteration 20000 (26.9231 iter/s, 3.71428s/100 iter), loss = 0.0952344
I0630 01:20:30.294862 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:20:30.294870 21943 sgd_solver.cpp:106] Iteration 20000, lr = 0.06875
I0630 01:20:32.355232 21943 solver.cpp:290] Iteration 20100 (48.5365 iter/s, 2.06031s/100 iter), loss = 0.0476153
I0630 01:20:32.355255 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:20:32.355262 21943 sgd_solver.cpp:106] Iteration 20100, lr = 0.0685938
I0630 01:20:34.407800 21943 solver.cpp:290] Iteration 20200 (48.7215 iter/s, 2.05248s/100 iter), loss = 0.142853
I0630 01:20:34.407824 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:20:34.407830 21943 sgd_solver.cpp:106] Iteration 20200, lr = 0.0684375
I0630 01:20:36.462425 21943 solver.cpp:290] Iteration 20300 (48.6728 iter/s, 2.05454s/100 iter), loss = 0.190472
I0630 01:20:36.462447 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:20:36.462455 21943 sgd_solver.cpp:106] Iteration 20300, lr = 0.0682813
I0630 01:20:38.518064 21943 solver.cpp:290] Iteration 20400 (48.6488 iter/s, 2.05555s/100 iter), loss = -3.7998e-06
I0630 01:20:38.518087 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:20:38.518113 21943 sgd_solver.cpp:106] Iteration 20400, lr = 0.068125
I0630 01:20:40.570740 21943 solver.cpp:290] Iteration 20500 (48.719 iter/s, 2.05259s/100 iter), loss = -3.7998e-06
I0630 01:20:40.570762 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:20:40.570770 21943 sgd_solver.cpp:106] Iteration 20500, lr = 0.0679687
I0630 01:20:42.622828 21943 solver.cpp:290] Iteration 20600 (48.7331 iter/s, 2.05199s/100 iter), loss = -3.78489e-06
I0630 01:20:42.622859 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:20:42.622869 21943 sgd_solver.cpp:106] Iteration 20600, lr = 0.0678125
I0630 01:20:44.675307 21943 solver.cpp:290] Iteration 20700 (48.7238 iter/s, 2.05239s/100 iter), loss = -3.78489e-06
I0630 01:20:44.675371 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:20:44.675379 21943 sgd_solver.cpp:106] Iteration 20700, lr = 0.0676562
I0630 01:20:46.730406 21943 solver.cpp:290] Iteration 20800 (48.6625 iter/s, 2.05497s/100 iter), loss = 0.0476153
I0630 01:20:46.730428 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:20:46.730435 21943 sgd_solver.cpp:106] Iteration 20800, lr = 0.0675
I0630 01:20:48.783857 21943 solver.cpp:290] Iteration 20900 (48.7006 iter/s, 2.05336s/100 iter), loss = 0.142853
I0630 01:20:48.783879 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:20:48.783887 21943 sgd_solver.cpp:106] Iteration 20900, lr = 0.0673437
I0630 01:20:50.815443 21943 solver.cpp:464] Iteration 21000, Testing net (#0)
I0630 01:20:52.454563 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.783
I0630 01:20:52.454582 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9862
I0630 01:20:52.454587 21943 solver.cpp:537]     Test net output #2: loss = 0.6548 (* 1 = 0.6548 loss)
I0630 01:20:52.474687 21943 solver.cpp:290] Iteration 21000 (27.0952 iter/s, 3.6907s/100 iter), loss = -3.78489e-06
I0630 01:20:52.474705 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:20:52.474717 21943 sgd_solver.cpp:106] Iteration 21000, lr = 0.0671875
I0630 01:20:54.526770 21943 solver.cpp:290] Iteration 21100 (48.7329 iter/s, 2.052s/100 iter), loss = 0.190472
I0630 01:20:54.526793 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:20:54.526800 21943 sgd_solver.cpp:106] Iteration 21100, lr = 0.0670313
I0630 01:20:56.582120 21943 solver.cpp:290] Iteration 21200 (48.6556 iter/s, 2.05526s/100 iter), loss = 0.142853
I0630 01:20:56.582142 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:20:56.582150 21943 sgd_solver.cpp:106] Iteration 21200, lr = 0.066875
I0630 01:20:58.635516 21943 solver.cpp:290] Iteration 21300 (48.7019 iter/s, 2.05331s/100 iter), loss = 0.238091
I0630 01:20:58.635542 21943 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0630 01:20:58.635548 21943 sgd_solver.cpp:106] Iteration 21300, lr = 0.0667187
I0630 01:21:00.688657 21943 solver.cpp:290] Iteration 21400 (48.708 iter/s, 2.05305s/100 iter), loss = -3.78489e-06
I0630 01:21:00.688678 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:21:00.688686 21943 sgd_solver.cpp:106] Iteration 21400, lr = 0.0665625
I0630 01:21:02.743125 21943 solver.cpp:290] Iteration 21500 (48.6765 iter/s, 2.05438s/100 iter), loss = 0.190472
I0630 01:21:02.743151 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:21:02.743158 21943 sgd_solver.cpp:106] Iteration 21500, lr = 0.0664062
I0630 01:21:04.794808 21943 solver.cpp:290] Iteration 21600 (48.7427 iter/s, 2.05159s/100 iter), loss = 0.0476153
I0630 01:21:04.794831 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:21:04.794853 21943 sgd_solver.cpp:106] Iteration 21600, lr = 0.06625
I0630 01:21:06.848247 21943 solver.cpp:290] Iteration 21700 (48.7009 iter/s, 2.05335s/100 iter), loss = -3.80725e-06
I0630 01:21:06.848270 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:21:06.848278 21943 sgd_solver.cpp:106] Iteration 21700, lr = 0.0660938
I0630 01:21:08.904925 21943 solver.cpp:290] Iteration 21800 (48.6242 iter/s, 2.05659s/100 iter), loss = 0.190472
I0630 01:21:08.904949 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:21:08.904958 21943 sgd_solver.cpp:106] Iteration 21800, lr = 0.0659375
I0630 01:21:10.958390 21943 solver.cpp:290] Iteration 21900 (48.7003 iter/s, 2.05338s/100 iter), loss = 0.190472
I0630 01:21:10.958415 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:21:10.958422 21943 sgd_solver.cpp:106] Iteration 21900, lr = 0.0657813
I0630 01:21:12.996809 21943 solver.cpp:464] Iteration 22000, Testing net (#0)
I0630 01:21:14.634331 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.7118
I0630 01:21:14.634351 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.942201
I0630 01:21:14.634357 21943 solver.cpp:537]     Test net output #2: loss = 1.4105 (* 1 = 1.4105 loss)
I0630 01:21:14.655365 21943 solver.cpp:290] Iteration 22000 (27.0501 iter/s, 3.69684s/100 iter), loss = 0.190472
I0630 01:21:14.655391 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:21:14.655400 21943 sgd_solver.cpp:106] Iteration 22000, lr = 0.065625
I0630 01:21:16.716519 21943 solver.cpp:290] Iteration 22100 (48.5187 iter/s, 2.06106s/100 iter), loss = -3.78489e-06
I0630 01:21:16.716593 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:21:16.716605 21943 sgd_solver.cpp:106] Iteration 22100, lr = 0.0654688
I0630 01:21:18.779906 21943 solver.cpp:290] Iteration 22200 (48.4672 iter/s, 2.06325s/100 iter), loss = 0.428568
I0630 01:21:18.779927 21943 solver.cpp:309]     Train net output #0: loss = 0.428571 (* 1 = 0.428571 loss)
I0630 01:21:18.779934 21943 sgd_solver.cpp:106] Iteration 22200, lr = 0.0653125
I0630 01:21:20.834281 21943 solver.cpp:290] Iteration 22300 (48.6787 iter/s, 2.05429s/100 iter), loss = 0.0952343
I0630 01:21:20.834306 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:21:20.834316 21943 sgd_solver.cpp:106] Iteration 22300, lr = 0.0651563
I0630 01:21:22.888847 21943 solver.cpp:290] Iteration 22400 (48.6742 iter/s, 2.05448s/100 iter), loss = 0.0476152
I0630 01:21:22.888870 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:21:22.888876 21943 sgd_solver.cpp:106] Iteration 22400, lr = 0.065
I0630 01:21:24.943153 21943 solver.cpp:290] Iteration 22500 (48.6803 iter/s, 2.05422s/100 iter), loss = -3.7998e-06
I0630 01:21:24.943176 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:21:24.943184 21943 sgd_solver.cpp:106] Iteration 22500, lr = 0.0648438
I0630 01:21:26.999176 21943 solver.cpp:290] Iteration 22600 (48.6396 iter/s, 2.05594s/100 iter), loss = -3.78862e-06
I0630 01:21:26.999199 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:21:26.999207 21943 sgd_solver.cpp:106] Iteration 22600, lr = 0.0646875
I0630 01:21:29.052646 21943 solver.cpp:290] Iteration 22700 (48.7002 iter/s, 2.05338s/100 iter), loss = 0.142853
I0630 01:21:29.052671 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:21:29.052680 21943 sgd_solver.cpp:106] Iteration 22700, lr = 0.0645313
I0630 01:21:31.104648 21943 solver.cpp:290] Iteration 22800 (48.735 iter/s, 2.05191s/100 iter), loss = 0.0476153
I0630 01:21:31.104671 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:21:31.104678 21943 sgd_solver.cpp:106] Iteration 22800, lr = 0.064375
I0630 01:21:33.161659 21943 solver.cpp:290] Iteration 22900 (48.6163 iter/s, 2.05692s/100 iter), loss = -3.80725e-06
I0630 01:21:33.161680 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:21:33.161689 21943 sgd_solver.cpp:106] Iteration 22900, lr = 0.0642188
I0630 01:21:35.194438 21943 solver.cpp:464] Iteration 23000, Testing net (#0)
I0630 01:21:36.840004 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8253
I0630 01:21:36.840023 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9917
I0630 01:21:36.840029 21943 solver.cpp:537]     Test net output #2: loss = 0.489 (* 1 = 0.489 loss)
I0630 01:21:36.860394 21943 solver.cpp:290] Iteration 23000 (27.0373 iter/s, 3.6986s/100 iter), loss = -3.8445e-06
I0630 01:21:36.860419 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:21:36.860425 21943 sgd_solver.cpp:106] Iteration 23000, lr = 0.0640625
I0630 01:21:38.913743 21943 solver.cpp:290] Iteration 23100 (48.7031 iter/s, 2.05326s/100 iter), loss = 0.0476152
I0630 01:21:38.913769 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:21:38.913775 21943 sgd_solver.cpp:106] Iteration 23100, lr = 0.0639063
I0630 01:21:40.971575 21943 solver.cpp:290] Iteration 23200 (48.597 iter/s, 2.05774s/100 iter), loss = -3.8594e-06
I0630 01:21:40.971599 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:21:40.971608 21943 sgd_solver.cpp:106] Iteration 23200, lr = 0.06375
I0630 01:21:43.025454 21943 solver.cpp:290] Iteration 23300 (48.6905 iter/s, 2.05379s/100 iter), loss = -3.8594e-06
I0630 01:21:43.025475 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:21:43.025483 21943 sgd_solver.cpp:106] Iteration 23300, lr = 0.0635938
I0630 01:21:45.084867 21943 solver.cpp:290] Iteration 23400 (48.5597 iter/s, 2.05932s/100 iter), loss = -3.8445e-06
I0630 01:21:45.084910 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:21:45.084920 21943 sgd_solver.cpp:106] Iteration 23400, lr = 0.0634375
I0630 01:21:47.139299 21943 solver.cpp:290] Iteration 23500 (48.6778 iter/s, 2.05433s/100 iter), loss = -3.8445e-06
I0630 01:21:47.139356 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:21:47.139364 21943 sgd_solver.cpp:106] Iteration 23500, lr = 0.0632813
I0630 01:21:49.194262 21943 solver.cpp:290] Iteration 23600 (48.6656 iter/s, 2.05484s/100 iter), loss = 0.0476152
I0630 01:21:49.194285 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:21:49.194293 21943 sgd_solver.cpp:106] Iteration 23600, lr = 0.063125
I0630 01:21:51.248984 21943 solver.cpp:290] Iteration 23700 (48.6705 iter/s, 2.05463s/100 iter), loss = -3.8445e-06
I0630 01:21:51.249007 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:21:51.249014 21943 sgd_solver.cpp:106] Iteration 23700, lr = 0.0629688
I0630 01:21:53.303539 21943 solver.cpp:290] Iteration 23800 (48.6745 iter/s, 2.05446s/100 iter), loss = -3.86685e-06
I0630 01:21:53.303562 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:21:53.303568 21943 sgd_solver.cpp:106] Iteration 23800, lr = 0.0628125
I0630 01:21:55.353334 21943 solver.cpp:290] Iteration 23900 (48.7874 iter/s, 2.04971s/100 iter), loss = -3.8743e-06
I0630 01:21:55.353360 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:21:55.353369 21943 sgd_solver.cpp:106] Iteration 23900, lr = 0.0626562
I0630 01:21:57.383117 21943 solver.cpp:464] Iteration 24000, Testing net (#0)
I0630 01:21:59.020140 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.7536
I0630 01:21:59.020160 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9741
I0630 01:21:59.020165 21943 solver.cpp:537]     Test net output #2: loss = 0.9309 (* 1 = 0.9309 loss)
I0630 01:21:59.040235 21943 solver.cpp:290] Iteration 24000 (27.1241 iter/s, 3.68676s/100 iter), loss = 0.142853
I0630 01:21:59.040258 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:21:59.040266 21943 sgd_solver.cpp:106] Iteration 24000, lr = 0.0625
I0630 01:22:01.101472 21943 solver.cpp:290] Iteration 24100 (48.5166 iter/s, 2.06115s/100 iter), loss = -3.92273e-06
I0630 01:22:01.101495 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:01.101502 21943 sgd_solver.cpp:106] Iteration 24100, lr = 0.0623438
I0630 01:22:03.157666 21943 solver.cpp:290] Iteration 24200 (48.6357 iter/s, 2.0561s/100 iter), loss = -3.9041e-06
I0630 01:22:03.157688 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:03.157696 21943 sgd_solver.cpp:106] Iteration 24200, lr = 0.0621875
I0630 01:22:05.213966 21943 solver.cpp:290] Iteration 24300 (48.6331 iter/s, 2.05621s/100 iter), loss = -3.9041e-06
I0630 01:22:05.213989 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:05.213995 21943 sgd_solver.cpp:106] Iteration 24300, lr = 0.0620313
I0630 01:22:07.268970 21943 solver.cpp:290] Iteration 24400 (48.6638 iter/s, 2.05491s/100 iter), loss = -3.9041e-06
I0630 01:22:07.268993 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:07.269002 21943 sgd_solver.cpp:106] Iteration 24400, lr = 0.061875
I0630 01:22:09.319310 21943 solver.cpp:290] Iteration 24500 (48.7745 iter/s, 2.05025s/100 iter), loss = -3.91901e-06
I0630 01:22:09.319334 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:09.319340 21943 sgd_solver.cpp:106] Iteration 24500, lr = 0.0617188
I0630 01:22:11.371186 21943 solver.cpp:290] Iteration 24600 (48.738 iter/s, 2.05179s/100 iter), loss = 0.0952342
I0630 01:22:11.371208 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:22:11.371215 21943 sgd_solver.cpp:106] Iteration 24600, lr = 0.0615625
I0630 01:22:13.431843 21943 solver.cpp:290] Iteration 24700 (48.5303 iter/s, 2.06057s/100 iter), loss = -3.96371e-06
I0630 01:22:13.431865 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:13.431872 21943 sgd_solver.cpp:106] Iteration 24700, lr = 0.0614063
I0630 01:22:15.484467 21943 solver.cpp:290] Iteration 24800 (48.7202 iter/s, 2.05254s/100 iter), loss = -3.96371e-06
I0630 01:22:15.484505 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:15.484513 21943 sgd_solver.cpp:106] Iteration 24800, lr = 0.06125
I0630 01:22:17.539342 21943 solver.cpp:290] Iteration 24900 (48.6672 iter/s, 2.05477s/100 iter), loss = 0.0476151
I0630 01:22:17.539427 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:22:17.539433 21943 sgd_solver.cpp:106] Iteration 24900, lr = 0.0610937
I0630 01:22:19.572284 21943 solver.cpp:464] Iteration 25000, Testing net (#0)
I0630 01:22:21.214864 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8403
I0630 01:22:21.214882 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9939
I0630 01:22:21.214889 21943 solver.cpp:537]     Test net output #2: loss = 0.385 (* 1 = 0.385 loss)
I0630 01:22:21.234706 21943 solver.cpp:290] Iteration 25000 (27.0623 iter/s, 3.69517s/100 iter), loss = -3.96743e-06
I0630 01:22:21.234725 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:21.234735 21943 sgd_solver.cpp:106] Iteration 25000, lr = 0.0609375
I0630 01:22:23.294327 21943 solver.cpp:290] Iteration 25100 (48.5546 iter/s, 2.05954s/100 iter), loss = -3.96371e-06
I0630 01:22:23.294350 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:23.294356 21943 sgd_solver.cpp:106] Iteration 25100, lr = 0.0607813
I0630 01:22:25.347295 21943 solver.cpp:290] Iteration 25200 (48.7121 iter/s, 2.05288s/100 iter), loss = -3.96371e-06
I0630 01:22:25.347317 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:25.347324 21943 sgd_solver.cpp:106] Iteration 25200, lr = 0.060625
I0630 01:22:27.407538 21943 solver.cpp:290] Iteration 25300 (48.54 iter/s, 2.06015s/100 iter), loss = 0.0476151
I0630 01:22:27.407562 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:22:27.407569 21943 sgd_solver.cpp:106] Iteration 25300, lr = 0.0604688
I0630 01:22:29.472709 21943 solver.cpp:290] Iteration 25400 (48.4242 iter/s, 2.06508s/100 iter), loss = 0.0952341
I0630 01:22:29.472733 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:22:29.472739 21943 sgd_solver.cpp:106] Iteration 25400, lr = 0.0603125
I0630 01:22:31.527794 21943 solver.cpp:290] Iteration 25500 (48.6619 iter/s, 2.055s/100 iter), loss = -3.97116e-06
I0630 01:22:31.527817 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:31.527823 21943 sgd_solver.cpp:106] Iteration 25500, lr = 0.0601563
I0630 01:22:33.582671 21943 solver.cpp:290] Iteration 25600 (48.6668 iter/s, 2.05479s/100 iter), loss = 0.0476151
I0630 01:22:33.582693 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:22:33.582701 21943 sgd_solver.cpp:106] Iteration 25600, lr = 0.06
I0630 01:22:35.632165 21943 solver.cpp:290] Iteration 25700 (48.7946 iter/s, 2.04941s/100 iter), loss = 0.0476151
I0630 01:22:35.632189 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:22:35.632194 21943 sgd_solver.cpp:106] Iteration 25700, lr = 0.0598437
I0630 01:22:37.684051 21943 solver.cpp:290] Iteration 25800 (48.7378 iter/s, 2.0518s/100 iter), loss = -3.96371e-06
I0630 01:22:37.684073 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:37.684080 21943 sgd_solver.cpp:106] Iteration 25800, lr = 0.0596875
I0630 01:22:39.741247 21943 solver.cpp:290] Iteration 25900 (48.612 iter/s, 2.0571s/100 iter), loss = -3.98234e-06
I0630 01:22:39.741317 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:39.741341 21943 sgd_solver.cpp:106] Iteration 25900, lr = 0.0595312
I0630 01:22:41.774744 21943 solver.cpp:464] Iteration 26000, Testing net (#0)
I0630 01:22:43.418223 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8558
I0630 01:22:43.418241 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9932
I0630 01:22:43.418246 21943 solver.cpp:537]     Test net output #2: loss = 0.4122 (* 1 = 0.4122 loss)
I0630 01:22:43.438216 21943 solver.cpp:290] Iteration 26000 (27.0505 iter/s, 3.69679s/100 iter), loss = -3.97861e-06
I0630 01:22:43.438233 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:43.438247 21943 sgd_solver.cpp:106] Iteration 26000, lr = 0.059375
I0630 01:22:45.494840 21943 solver.cpp:290] Iteration 26100 (48.6254 iter/s, 2.05654s/100 iter), loss = 0.142853
I0630 01:22:45.494864 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:22:45.494871 21943 sgd_solver.cpp:106] Iteration 26100, lr = 0.0592188
I0630 01:22:47.552086 21943 solver.cpp:290] Iteration 26200 (48.6108 iter/s, 2.05716s/100 iter), loss = 0.0476151
I0630 01:22:47.552150 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:22:47.552160 21943 sgd_solver.cpp:106] Iteration 26200, lr = 0.0590625
I0630 01:22:49.607193 21943 solver.cpp:290] Iteration 26300 (48.6622 iter/s, 2.05498s/100 iter), loss = -3.97861e-06
I0630 01:22:49.607218 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:49.607228 21943 sgd_solver.cpp:106] Iteration 26300, lr = 0.0589063
I0630 01:22:51.663616 21943 solver.cpp:290] Iteration 26400 (48.6303 iter/s, 2.05633s/100 iter), loss = -3.97861e-06
I0630 01:22:51.663640 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:51.663645 21943 sgd_solver.cpp:106] Iteration 26400, lr = 0.05875
I0630 01:22:53.716228 21943 solver.cpp:290] Iteration 26500 (48.7206 iter/s, 2.05252s/100 iter), loss = 0.0952341
I0630 01:22:53.716250 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:22:53.716259 21943 sgd_solver.cpp:106] Iteration 26500, lr = 0.0585938
I0630 01:22:55.768218 21943 solver.cpp:290] Iteration 26600 (48.7354 iter/s, 2.0519s/100 iter), loss = -3.97861e-06
I0630 01:22:55.768245 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:55.768254 21943 sgd_solver.cpp:106] Iteration 26600, lr = 0.0584375
I0630 01:22:57.817797 21943 solver.cpp:290] Iteration 26700 (48.7927 iter/s, 2.04948s/100 iter), loss = 0.190472
I0630 01:22:57.817821 21943 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0630 01:22:57.817828 21943 sgd_solver.cpp:106] Iteration 26700, lr = 0.0582813
I0630 01:22:59.871915 21943 solver.cpp:290] Iteration 26800 (48.6848 iter/s, 2.05403s/100 iter), loss = -4.00841e-06
I0630 01:22:59.871937 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:22:59.871944 21943 sgd_solver.cpp:106] Iteration 26800, lr = 0.058125
I0630 01:23:01.927211 21943 solver.cpp:290] Iteration 26900 (48.6569 iter/s, 2.05521s/100 iter), loss = -4.02331e-06
I0630 01:23:01.927237 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:01.927245 21943 sgd_solver.cpp:106] Iteration 26900, lr = 0.0579687
I0630 01:23:03.959694 21943 solver.cpp:464] Iteration 27000, Testing net (#0)
I0630 01:23:05.598603 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8284
I0630 01:23:05.598626 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9905
I0630 01:23:05.598634 21943 solver.cpp:537]     Test net output #2: loss = 0.4912 (* 1 = 0.4912 loss)
I0630 01:23:05.618304 21943 solver.cpp:290] Iteration 27000 (27.0932 iter/s, 3.69096s/100 iter), loss = -4.04567e-06
I0630 01:23:05.618322 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:05.618335 21943 sgd_solver.cpp:106] Iteration 27000, lr = 0.0578125
I0630 01:23:07.696801 21943 solver.cpp:290] Iteration 27100 (48.1136 iter/s, 2.07841s/100 iter), loss = -4.05312e-06
I0630 01:23:07.696825 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:07.696831 21943 sgd_solver.cpp:106] Iteration 27100, lr = 0.0576563
I0630 01:23:09.748641 21943 solver.cpp:290] Iteration 27200 (48.7389 iter/s, 2.05175s/100 iter), loss = 0.142853
I0630 01:23:09.748666 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:23:09.748672 21943 sgd_solver.cpp:106] Iteration 27200, lr = 0.0575
I0630 01:23:11.810034 21943 solver.cpp:290] Iteration 27300 (48.513 iter/s, 2.0613s/100 iter), loss = -4.06802e-06
I0630 01:23:11.810056 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:11.810063 21943 sgd_solver.cpp:106] Iteration 27300, lr = 0.0573438
I0630 01:23:13.867985 21943 solver.cpp:290] Iteration 27400 (48.5941 iter/s, 2.05786s/100 iter), loss = -4.09782e-06
I0630 01:23:13.868008 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:13.868016 21943 sgd_solver.cpp:106] Iteration 27400, lr = 0.0571875
I0630 01:23:15.920213 21943 solver.cpp:290] Iteration 27500 (48.7297 iter/s, 2.05214s/100 iter), loss = -4.11272e-06
I0630 01:23:15.920249 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:15.920258 21943 sgd_solver.cpp:106] Iteration 27500, lr = 0.0570313
I0630 01:23:17.974792 21943 solver.cpp:290] Iteration 27600 (48.6742 iter/s, 2.05448s/100 iter), loss = 0.142853
I0630 01:23:17.974843 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:23:17.974850 21943 sgd_solver.cpp:106] Iteration 27600, lr = 0.056875
I0630 01:23:20.037688 21943 solver.cpp:290] Iteration 27700 (48.4783 iter/s, 2.06278s/100 iter), loss = -4.09037e-06
I0630 01:23:20.037710 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:20.037716 21943 sgd_solver.cpp:106] Iteration 27700, lr = 0.0567187
I0630 01:23:22.096837 21943 solver.cpp:290] Iteration 27800 (48.5659 iter/s, 2.05906s/100 iter), loss = -4.09782e-06
I0630 01:23:22.096858 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:22.096864 21943 sgd_solver.cpp:106] Iteration 27800, lr = 0.0565625
I0630 01:23:24.152428 21943 solver.cpp:290] Iteration 27900 (48.6499 iter/s, 2.0555s/100 iter), loss = -4.10154e-06
I0630 01:23:24.152451 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:24.152459 21943 sgd_solver.cpp:106] Iteration 27900, lr = 0.0564062
I0630 01:23:26.183892 21943 solver.cpp:464] Iteration 28000, Testing net (#0)
I0630 01:23:27.820679 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8411
I0630 01:23:27.820699 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9919
I0630 01:23:27.820704 21943 solver.cpp:537]     Test net output #2: loss = 0.4894 (* 1 = 0.4894 loss)
I0630 01:23:27.840301 21943 solver.cpp:290] Iteration 28000 (27.1169 iter/s, 3.68773s/100 iter), loss = -4.08664e-06
I0630 01:23:27.840339 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:27.840348 21943 sgd_solver.cpp:106] Iteration 28000, lr = 0.05625
I0630 01:23:29.892909 21943 solver.cpp:290] Iteration 28100 (48.721 iter/s, 2.0525s/100 iter), loss = -4.08292e-06
I0630 01:23:29.892935 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:29.892946 21943 sgd_solver.cpp:106] Iteration 28100, lr = 0.0560938
I0630 01:23:31.944000 21943 solver.cpp:290] Iteration 28200 (48.7567 iter/s, 2.051s/100 iter), loss = -4.12762e-06
I0630 01:23:31.944028 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:31.944037 21943 sgd_solver.cpp:106] Iteration 28200, lr = 0.0559375
I0630 01:23:33.995728 21943 solver.cpp:290] Iteration 28300 (48.7416 iter/s, 2.05164s/100 iter), loss = 0.0476149
I0630 01:23:33.995750 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:23:33.995757 21943 sgd_solver.cpp:106] Iteration 28300, lr = 0.0557813
I0630 01:23:36.048053 21943 solver.cpp:290] Iteration 28400 (48.7273 iter/s, 2.05224s/100 iter), loss = -4.14252e-06
I0630 01:23:36.048076 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:36.048085 21943 sgd_solver.cpp:106] Iteration 28400, lr = 0.055625
I0630 01:23:38.099293 21943 solver.cpp:290] Iteration 28500 (48.7531 iter/s, 2.05115s/100 iter), loss = -4.15742e-06
I0630 01:23:38.099315 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:38.099323 21943 sgd_solver.cpp:106] Iteration 28500, lr = 0.0554687
I0630 01:23:40.150342 21943 solver.cpp:290] Iteration 28600 (48.7576 iter/s, 2.05096s/100 iter), loss = -4.15742e-06
I0630 01:23:40.150367 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:40.150374 21943 sgd_solver.cpp:106] Iteration 28600, lr = 0.0553125
I0630 01:23:42.204216 21943 solver.cpp:290] Iteration 28700 (48.6906 iter/s, 2.05378s/100 iter), loss = 0.0952339
I0630 01:23:42.204241 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:23:42.204246 21943 sgd_solver.cpp:106] Iteration 28700, lr = 0.0551562
I0630 01:23:44.256577 21943 solver.cpp:290] Iteration 28800 (48.7265 iter/s, 2.05227s/100 iter), loss = -4.15742e-06
I0630 01:23:44.256604 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:44.256613 21943 sgd_solver.cpp:106] Iteration 28800, lr = 0.055
I0630 01:23:46.311815 21943 solver.cpp:290] Iteration 28900 (48.6583 iter/s, 2.05515s/100 iter), loss = -4.16115e-06
I0630 01:23:46.311862 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:46.311872 21943 sgd_solver.cpp:106] Iteration 28900, lr = 0.0548437
I0630 01:23:48.348026 21943 solver.cpp:464] Iteration 29000, Testing net (#0)
I0630 01:23:49.986980 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.78
I0630 01:23:49.987000 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9801
I0630 01:23:49.987005 21943 solver.cpp:537]     Test net output #2: loss = 0.7215 (* 1 = 0.7215 loss)
I0630 01:23:50.007241 21943 solver.cpp:290] Iteration 29000 (27.0616 iter/s, 3.69527s/100 iter), loss = -4.14252e-06
I0630 01:23:50.007261 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:50.007272 21943 sgd_solver.cpp:106] Iteration 29000, lr = 0.0546875
I0630 01:23:52.057723 21943 solver.cpp:290] Iteration 29100 (48.771 iter/s, 2.0504s/100 iter), loss = 0.0952339
I0630 01:23:52.057745 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:23:52.057751 21943 sgd_solver.cpp:106] Iteration 29100, lr = 0.0545313
I0630 01:23:54.109413 21943 solver.cpp:290] Iteration 29200 (48.7424 iter/s, 2.0516s/100 iter), loss = -4.14252e-06
I0630 01:23:54.109436 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:54.109441 21943 sgd_solver.cpp:106] Iteration 29200, lr = 0.054375
I0630 01:23:56.162500 21943 solver.cpp:290] Iteration 29300 (48.7092 iter/s, 2.053s/100 iter), loss = 0.28571
I0630 01:23:56.162523 21943 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0630 01:23:56.162529 21943 sgd_solver.cpp:106] Iteration 29300, lr = 0.0542188
I0630 01:23:58.216243 21943 solver.cpp:290] Iteration 29400 (48.6937 iter/s, 2.05365s/100 iter), loss = -4.14252e-06
I0630 01:23:58.216266 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:23:58.216272 21943 sgd_solver.cpp:106] Iteration 29400, lr = 0.0540625
I0630 01:24:00.269089 21943 solver.cpp:290] Iteration 29500 (48.715 iter/s, 2.05276s/100 iter), loss = -4.14252e-06
I0630 01:24:00.269112 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:00.269119 21943 sgd_solver.cpp:106] Iteration 29500, lr = 0.0539063
I0630 01:24:02.324023 21943 solver.cpp:290] Iteration 29600 (48.6655 iter/s, 2.05484s/100 iter), loss = 0.0476149
I0630 01:24:02.324048 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:24:02.324055 21943 sgd_solver.cpp:106] Iteration 29600, lr = 0.05375
I0630 01:24:04.379283 21943 solver.cpp:290] Iteration 29700 (48.6577 iter/s, 2.05517s/100 iter), loss = -4.14252e-06
I0630 01:24:04.379308 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:04.379317 21943 sgd_solver.cpp:106] Iteration 29700, lr = 0.0535938
I0630 01:24:06.435662 21943 solver.cpp:290] Iteration 29800 (48.6313 iter/s, 2.05629s/100 iter), loss = 0.0476149
I0630 01:24:06.435685 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:24:06.435694 21943 sgd_solver.cpp:106] Iteration 29800, lr = 0.0534375
I0630 01:24:08.520179 21943 solver.cpp:290] Iteration 29900 (47.9748 iter/s, 2.08443s/100 iter), loss = 0.142853
I0630 01:24:08.520201 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:24:08.520208 21943 sgd_solver.cpp:106] Iteration 29900, lr = 0.0532812
I0630 01:24:10.556771 21943 solver.cpp:591] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2_iter_30000.caffemodel
I0630 01:24:10.573276 21943 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2_iter_30000.solverstate
I0630 01:24:10.580672 21943 solver.cpp:464] Iteration 30000, Testing net (#0)
I0630 01:24:12.217538 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8059
I0630 01:24:12.217556 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9914
I0630 01:24:12.217562 21943 solver.cpp:537]     Test net output #2: loss = 0.6526 (* 1 = 0.6526 loss)
I0630 01:24:12.237244 21943 solver.cpp:290] Iteration 30000 (26.9039 iter/s, 3.71693s/100 iter), loss = -4.14997e-06
I0630 01:24:12.237264 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:12.237287 21943 sgd_solver.cpp:106] Iteration 30000, lr = 0.053125
I0630 01:24:14.289999 21943 solver.cpp:290] Iteration 30100 (48.717 iter/s, 2.05267s/100 iter), loss = -4.15742e-06
I0630 01:24:14.290022 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:14.290030 21943 sgd_solver.cpp:106] Iteration 30100, lr = 0.0529688
I0630 01:24:16.342196 21943 solver.cpp:290] Iteration 30200 (48.7304 iter/s, 2.05211s/100 iter), loss = -4.14252e-06
I0630 01:24:16.342218 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:16.342226 21943 sgd_solver.cpp:106] Iteration 30200, lr = 0.0528125
I0630 01:24:18.398221 21943 solver.cpp:290] Iteration 30300 (48.6396 iter/s, 2.05594s/100 iter), loss = -4.13135e-06
I0630 01:24:18.398282 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:18.398289 21943 sgd_solver.cpp:106] Iteration 30300, lr = 0.0526563
I0630 01:24:20.453958 21943 solver.cpp:290] Iteration 30400 (48.6473 iter/s, 2.05561s/100 iter), loss = -4.13135e-06
I0630 01:24:20.453980 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:20.453987 21943 sgd_solver.cpp:106] Iteration 30400, lr = 0.0525
I0630 01:24:22.506093 21943 solver.cpp:290] Iteration 30500 (48.7318 iter/s, 2.05205s/100 iter), loss = -4.12762e-06
I0630 01:24:22.506114 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:22.506121 21943 sgd_solver.cpp:106] Iteration 30500, lr = 0.0523438
I0630 01:24:24.572057 21943 solver.cpp:290] Iteration 30600 (48.4057 iter/s, 2.06587s/100 iter), loss = -4.15742e-06
I0630 01:24:24.572083 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:24.572093 21943 sgd_solver.cpp:106] Iteration 30600, lr = 0.0521875
I0630 01:24:26.633867 21943 solver.cpp:290] Iteration 30700 (48.5033 iter/s, 2.06172s/100 iter), loss = -4.15742e-06
I0630 01:24:26.633893 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:26.633903 21943 sgd_solver.cpp:106] Iteration 30700, lr = 0.0520312
I0630 01:24:28.687196 21943 solver.cpp:290] Iteration 30800 (48.7035 iter/s, 2.05324s/100 iter), loss = -4.15742e-06
I0630 01:24:28.687218 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:28.687227 21943 sgd_solver.cpp:106] Iteration 30800, lr = 0.051875
I0630 01:24:30.749415 21943 solver.cpp:290] Iteration 30900 (48.4935 iter/s, 2.06213s/100 iter), loss = -4.14997e-06
I0630 01:24:30.749439 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:30.749444 21943 sgd_solver.cpp:106] Iteration 30900, lr = 0.0517187
I0630 01:24:32.782708 21943 solver.cpp:464] Iteration 31000, Testing net (#0)
I0630 01:24:34.422734 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8404
I0630 01:24:34.422754 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9917
I0630 01:24:34.422758 21943 solver.cpp:537]     Test net output #2: loss = 0.4787 (* 1 = 0.4787 loss)
I0630 01:24:34.442548 21943 solver.cpp:290] Iteration 31000 (27.0783 iter/s, 3.693s/100 iter), loss = -4.14252e-06
I0630 01:24:34.442565 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:34.442577 21943 sgd_solver.cpp:106] Iteration 31000, lr = 0.0515625
I0630 01:24:36.497854 21943 solver.cpp:290] Iteration 31100 (48.6566 iter/s, 2.05522s/100 iter), loss = 0.0476149
I0630 01:24:36.497880 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:24:36.497889 21943 sgd_solver.cpp:106] Iteration 31100, lr = 0.0514063
I0630 01:24:38.554709 21943 solver.cpp:290] Iteration 31200 (48.6201 iter/s, 2.05676s/100 iter), loss = 0.0476149
I0630 01:24:38.554731 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:24:38.554738 21943 sgd_solver.cpp:106] Iteration 31200, lr = 0.05125
I0630 01:24:40.614856 21943 solver.cpp:290] Iteration 31300 (48.5423 iter/s, 2.06006s/100 iter), loss = -4.12762e-06
I0630 01:24:40.614877 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:40.614884 21943 sgd_solver.cpp:106] Iteration 31300, lr = 0.0510938
I0630 01:24:42.671959 21943 solver.cpp:290] Iteration 31400 (48.6141 iter/s, 2.05702s/100 iter), loss = 0.095234
I0630 01:24:42.671983 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:24:42.671989 21943 sgd_solver.cpp:106] Iteration 31400, lr = 0.0509375
I0630 01:24:44.733129 21943 solver.cpp:290] Iteration 31500 (48.5183 iter/s, 2.06108s/100 iter), loss = -4.14997e-06
I0630 01:24:44.733158 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:44.733167 21943 sgd_solver.cpp:106] Iteration 31500, lr = 0.0507812
I0630 01:24:46.786466 21943 solver.cpp:290] Iteration 31600 (48.7035 iter/s, 2.05324s/100 iter), loss = 0.0476149
I0630 01:24:46.786504 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:24:46.786511 21943 sgd_solver.cpp:106] Iteration 31600, lr = 0.050625
I0630 01:24:48.838996 21943 solver.cpp:290] Iteration 31700 (48.7228 iter/s, 2.05243s/100 iter), loss = -4.18723e-06
I0630 01:24:48.839049 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:48.839058 21943 sgd_solver.cpp:106] Iteration 31700, lr = 0.0504688
I0630 01:24:50.900830 21943 solver.cpp:290] Iteration 31800 (48.5032 iter/s, 2.06172s/100 iter), loss = -4.20213e-06
I0630 01:24:50.900852 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:50.900858 21943 sgd_solver.cpp:106] Iteration 31800, lr = 0.0503125
I0630 01:24:52.952013 21943 solver.cpp:290] Iteration 31900 (48.7545 iter/s, 2.05109s/100 iter), loss = -4.20213e-06
I0630 01:24:52.952036 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:52.952042 21943 sgd_solver.cpp:106] Iteration 31900, lr = 0.0501562
I0630 01:24:54.987098 21943 solver.cpp:464] Iteration 32000, Testing net (#0)
I0630 01:24:56.625366 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8006
I0630 01:24:56.625386 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9895
I0630 01:24:56.625393 21943 solver.cpp:537]     Test net output #2: loss = 0.7039 (* 1 = 0.7039 loss)
I0630 01:24:56.647107 21943 solver.cpp:290] Iteration 32000 (27.0639 iter/s, 3.69496s/100 iter), loss = 0.0952339
I0630 01:24:56.647127 21943 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:24:56.647138 21943 sgd_solver.cpp:106] Iteration 32000, lr = 0.05
I0630 01:24:58.704253 21943 solver.cpp:290] Iteration 32100 (48.6131 iter/s, 2.05706s/100 iter), loss = -4.20585e-06
I0630 01:24:58.704277 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:24:58.704285 21943 sgd_solver.cpp:106] Iteration 32100, lr = 0.0498438
I0630 01:25:00.757213 21943 solver.cpp:290] Iteration 32200 (48.7123 iter/s, 2.05287s/100 iter), loss = -4.21703e-06
I0630 01:25:00.757236 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:00.757244 21943 sgd_solver.cpp:106] Iteration 32200, lr = 0.0496875
I0630 01:25:02.809526 21943 solver.cpp:290] Iteration 32300 (48.7276 iter/s, 2.05222s/100 iter), loss = -4.21703e-06
I0630 01:25:02.809548 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:02.809556 21943 sgd_solver.cpp:106] Iteration 32300, lr = 0.0495313
I0630 01:25:04.860671 21943 solver.cpp:290] Iteration 32400 (48.7553 iter/s, 2.05106s/100 iter), loss = -4.22075e-06
I0630 01:25:04.860692 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:04.860699 21943 sgd_solver.cpp:106] Iteration 32400, lr = 0.049375
I0630 01:25:06.914535 21943 solver.cpp:290] Iteration 32500 (48.6909 iter/s, 2.05377s/100 iter), loss = -4.20958e-06
I0630 01:25:06.914557 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:06.914564 21943 sgd_solver.cpp:106] Iteration 32500, lr = 0.0492188
I0630 01:25:08.997836 21943 solver.cpp:290] Iteration 32600 (48.0028 iter/s, 2.08321s/100 iter), loss = -4.24683e-06
I0630 01:25:08.997861 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:08.997870 21943 sgd_solver.cpp:106] Iteration 32600, lr = 0.0490625
I0630 01:25:11.056545 21943 solver.cpp:290] Iteration 32700 (48.5763 iter/s, 2.05862s/100 iter), loss = -4.25056e-06
I0630 01:25:11.056567 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:11.056574 21943 sgd_solver.cpp:106] Iteration 32700, lr = 0.0489062
I0630 01:25:13.106622 21943 solver.cpp:290] Iteration 32800 (48.7807 iter/s, 2.04999s/100 iter), loss = -4.24683e-06
I0630 01:25:13.106647 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:13.106655 21943 sgd_solver.cpp:106] Iteration 32800, lr = 0.04875
I0630 01:25:15.162401 21943 solver.cpp:290] Iteration 32900 (48.6455 iter/s, 2.05569s/100 iter), loss = -4.24683e-06
I0630 01:25:15.162425 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:15.162431 21943 sgd_solver.cpp:106] Iteration 32900, lr = 0.0485937
I0630 01:25:17.199028 21943 solver.cpp:464] Iteration 33000, Testing net (#0)
I0630 01:25:18.838537 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8555
I0630 01:25:18.838556 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9925
I0630 01:25:18.838562 21943 solver.cpp:537]     Test net output #2: loss = 0.4413 (* 1 = 0.4413 loss)
I0630 01:25:18.858772 21943 solver.cpp:290] Iteration 33000 (27.0545 iter/s, 3.69624s/100 iter), loss = -4.26918e-06
I0630 01:25:18.858845 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:18.858855 21943 sgd_solver.cpp:106] Iteration 33000, lr = 0.0484375
I0630 01:25:20.921689 21943 solver.cpp:290] Iteration 33100 (48.4783 iter/s, 2.06278s/100 iter), loss = 0.0476148
I0630 01:25:20.921715 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:25:20.921725 21943 sgd_solver.cpp:106] Iteration 33100, lr = 0.0482813
I0630 01:25:22.972154 21943 solver.cpp:290] Iteration 33200 (48.7716 iter/s, 2.05037s/100 iter), loss = -4.28408e-06
I0630 01:25:22.972178 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:22.972185 21943 sgd_solver.cpp:106] Iteration 33200, lr = 0.048125
I0630 01:25:25.025287 21943 solver.cpp:290] Iteration 33300 (48.7082 iter/s, 2.05304s/100 iter), loss = -4.26173e-06
I0630 01:25:25.025310 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:25.025316 21943 sgd_solver.cpp:106] Iteration 33300, lr = 0.0479688
I0630 01:25:27.081246 21943 solver.cpp:290] Iteration 33400 (48.6412 iter/s, 2.05587s/100 iter), loss = -4.26173e-06
I0630 01:25:27.081271 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:27.081279 21943 sgd_solver.cpp:106] Iteration 33400, lr = 0.0478125
I0630 01:25:29.135711 21943 solver.cpp:290] Iteration 33500 (48.6766 iter/s, 2.05438s/100 iter), loss = 0.0476148
I0630 01:25:29.135733 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:25:29.135740 21943 sgd_solver.cpp:106] Iteration 33500, lr = 0.0476562
I0630 01:25:31.191753 21943 solver.cpp:290] Iteration 33600 (48.6392 iter/s, 2.05595s/100 iter), loss = -4.26173e-06
I0630 01:25:31.191776 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:31.191783 21943 sgd_solver.cpp:106] Iteration 33600, lr = 0.0475
I0630 01:25:33.247086 21943 solver.cpp:290] Iteration 33700 (48.656 iter/s, 2.05524s/100 iter), loss = -4.26173e-06
I0630 01:25:33.247108 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:33.247115 21943 sgd_solver.cpp:106] Iteration 33700, lr = 0.0473437
I0630 01:25:35.298475 21943 solver.cpp:290] Iteration 33800 (48.7496 iter/s, 2.0513s/100 iter), loss = -4.26173e-06
I0630 01:25:35.298498 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:35.298504 21943 sgd_solver.cpp:106] Iteration 33800, lr = 0.0471875
I0630 01:25:37.350014 21943 solver.cpp:290] Iteration 33900 (48.746 iter/s, 2.05145s/100 iter), loss = -4.26173e-06
I0630 01:25:37.350037 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:37.350044 21943 sgd_solver.cpp:106] Iteration 33900, lr = 0.0470312
I0630 01:25:39.385226 21943 solver.cpp:464] Iteration 34000, Testing net (#0)
I0630 01:25:41.022624 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8182
I0630 01:25:41.022644 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9797
I0630 01:25:41.022650 21943 solver.cpp:537]     Test net output #2: loss = 0.7002 (* 1 = 0.7002 loss)
I0630 01:25:41.042367 21943 solver.cpp:290] Iteration 34000 (27.084 iter/s, 3.69222s/100 iter), loss = -4.26173e-06
I0630 01:25:41.042387 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:41.042398 21943 sgd_solver.cpp:106] Iteration 34000, lr = 0.046875
I0630 01:25:43.106047 21943 solver.cpp:290] Iteration 34100 (48.4591 iter/s, 2.06359s/100 iter), loss = -4.26173e-06
I0630 01:25:43.106070 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:43.106077 21943 sgd_solver.cpp:106] Iteration 34100, lr = 0.0467188
I0630 01:25:45.158958 21943 solver.cpp:290] Iteration 34200 (48.7134 iter/s, 2.05282s/100 iter), loss = 0.0476148
I0630 01:25:45.158980 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:25:45.158987 21943 sgd_solver.cpp:106] Iteration 34200, lr = 0.0465625
I0630 01:25:47.211154 21943 solver.cpp:290] Iteration 34300 (48.7304 iter/s, 2.05211s/100 iter), loss = -4.27663e-06
I0630 01:25:47.211191 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:47.211199 21943 sgd_solver.cpp:106] Iteration 34300, lr = 0.0464063
I0630 01:25:49.263077 21943 solver.cpp:290] Iteration 34400 (48.7372 iter/s, 2.05182s/100 iter), loss = 0.0476148
I0630 01:25:49.263144 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:25:49.263150 21943 sgd_solver.cpp:106] Iteration 34400, lr = 0.04625
I0630 01:25:51.314085 21943 solver.cpp:290] Iteration 34500 (48.7596 iter/s, 2.05088s/100 iter), loss = 0.142853
I0630 01:25:51.314107 21943 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0630 01:25:51.314115 21943 sgd_solver.cpp:106] Iteration 34500, lr = 0.0460938
I0630 01:25:53.370312 21943 solver.cpp:290] Iteration 34600 (48.6349 iter/s, 2.05614s/100 iter), loss = -4.28408e-06
I0630 01:25:53.370335 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:53.370344 21943 sgd_solver.cpp:106] Iteration 34600, lr = 0.0459375
I0630 01:25:55.420308 21943 solver.cpp:290] Iteration 34700 (48.7827 iter/s, 2.04991s/100 iter), loss = -4.27663e-06
I0630 01:25:55.420331 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:55.420337 21943 sgd_solver.cpp:106] Iteration 34700, lr = 0.0457813
I0630 01:25:57.474128 21943 solver.cpp:290] Iteration 34800 (48.6919 iter/s, 2.05373s/100 iter), loss = -4.26173e-06
I0630 01:25:57.474151 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:25:57.474159 21943 sgd_solver.cpp:106] Iteration 34800, lr = 0.045625
I0630 01:25:59.527730 21943 solver.cpp:290] Iteration 34900 (48.697 iter/s, 2.05351s/100 iter), loss = 0.0476148
I0630 01:25:59.527755 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:25:59.527763 21943 sgd_solver.cpp:106] Iteration 34900, lr = 0.0454687
I0630 01:26:01.565143 21943 solver.cpp:464] Iteration 35000, Testing net (#0)
I0630 01:26:03.202869 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8464
I0630 01:26:03.202888 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9919
I0630 01:26:03.202894 21943 solver.cpp:537]     Test net output #2: loss = 0.5015 (* 1 = 0.5015 loss)
I0630 01:26:03.222486 21943 solver.cpp:290] Iteration 35000 (27.0664 iter/s, 3.69462s/100 iter), loss = -4.28408e-06
I0630 01:26:03.222503 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:03.222515 21943 sgd_solver.cpp:106] Iteration 35000, lr = 0.0453125
I0630 01:26:05.275512 21943 solver.cpp:290] Iteration 35100 (48.7106 iter/s, 2.05294s/100 iter), loss = -4.27663e-06
I0630 01:26:05.275535 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:05.275544 21943 sgd_solver.cpp:106] Iteration 35100, lr = 0.0451563
I0630 01:26:07.332286 21943 solver.cpp:290] Iteration 35200 (48.6219 iter/s, 2.05668s/100 iter), loss = -4.27663e-06
I0630 01:26:07.332312 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:07.332321 21943 sgd_solver.cpp:106] Iteration 35200, lr = 0.045
I0630 01:26:09.387966 21943 solver.cpp:290] Iteration 35300 (48.6478 iter/s, 2.05559s/100 iter), loss = -4.26173e-06
I0630 01:26:09.387989 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:09.387996 21943 sgd_solver.cpp:106] Iteration 35300, lr = 0.0448438
I0630 01:26:11.442422 21943 solver.cpp:290] Iteration 35400 (48.6769 iter/s, 2.05436s/100 iter), loss = -4.26173e-06
I0630 01:26:11.442452 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:11.442459 21943 sgd_solver.cpp:106] Iteration 35400, lr = 0.0446875
I0630 01:26:13.497952 21943 solver.cpp:290] Iteration 35500 (48.6515 iter/s, 2.05543s/100 iter), loss = -4.27663e-06
I0630 01:26:13.497975 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:13.497982 21943 sgd_solver.cpp:106] Iteration 35500, lr = 0.0445313
I0630 01:26:15.548357 21943 solver.cpp:290] Iteration 35600 (48.773 iter/s, 2.05032s/100 iter), loss = 0.238091
I0630 01:26:15.548379 21943 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0630 01:26:15.548388 21943 sgd_solver.cpp:106] Iteration 35600, lr = 0.044375
I0630 01:26:17.601436 21943 solver.cpp:290] Iteration 35700 (48.7094 iter/s, 2.05299s/100 iter), loss = -4.27663e-06
I0630 01:26:17.601474 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:17.601480 21943 sgd_solver.cpp:106] Iteration 35700, lr = 0.0442187
I0630 01:26:19.654462 21943 solver.cpp:290] Iteration 35800 (48.711 iter/s, 2.05292s/100 iter), loss = -4.28036e-06
I0630 01:26:19.654546 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:19.654553 21943 sgd_solver.cpp:106] Iteration 35800, lr = 0.0440625
I0630 01:26:21.709575 21943 solver.cpp:290] Iteration 35900 (48.6626 iter/s, 2.05497s/100 iter), loss = -4.27663e-06
I0630 01:26:21.709599 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:21.709606 21943 sgd_solver.cpp:106] Iteration 35900, lr = 0.0439062
I0630 01:26:23.740298 21943 solver.cpp:464] Iteration 36000, Testing net (#0)
I0630 01:26:25.385888 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8559
I0630 01:26:25.385908 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9946
I0630 01:26:25.385915 21943 solver.cpp:537]     Test net output #2: loss = 0.419 (* 1 = 0.419 loss)
I0630 01:26:25.405781 21943 solver.cpp:290] Iteration 36000 (27.0558 iter/s, 3.69607s/100 iter), loss = -4.27663e-06
I0630 01:26:25.405800 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:25.405812 21943 sgd_solver.cpp:106] Iteration 36000, lr = 0.04375
I0630 01:26:27.461510 21943 solver.cpp:290] Iteration 36100 (48.6466 iter/s, 2.05564s/100 iter), loss = -4.27663e-06
I0630 01:26:27.461532 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:27.461540 21943 sgd_solver.cpp:106] Iteration 36100, lr = 0.0435938
I0630 01:26:29.512526 21943 solver.cpp:290] Iteration 36200 (48.7584 iter/s, 2.05093s/100 iter), loss = -4.26173e-06
I0630 01:26:29.512549 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:29.512555 21943 sgd_solver.cpp:106] Iteration 36200, lr = 0.0434375
I0630 01:26:31.565810 21943 solver.cpp:290] Iteration 36300 (48.7046 iter/s, 2.0532s/100 iter), loss = 0.0476148
I0630 01:26:31.565832 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:26:31.565840 21943 sgd_solver.cpp:106] Iteration 36300, lr = 0.0432813
I0630 01:26:33.618942 21943 solver.cpp:290] Iteration 36400 (48.7082 iter/s, 2.05304s/100 iter), loss = -4.26546e-06
I0630 01:26:33.618968 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:33.618976 21943 sgd_solver.cpp:106] Iteration 36400, lr = 0.043125
I0630 01:26:35.675529 21943 solver.cpp:290] Iteration 36500 (48.6264 iter/s, 2.0565s/100 iter), loss = -4.26918e-06
I0630 01:26:35.675552 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:35.675559 21943 sgd_solver.cpp:106] Iteration 36500, lr = 0.0429688
I0630 01:26:37.728528 21943 solver.cpp:290] Iteration 36600 (48.7114 iter/s, 2.05291s/100 iter), loss = -4.26173e-06
I0630 01:26:37.728552 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:37.728561 21943 sgd_solver.cpp:106] Iteration 36600, lr = 0.0428125
I0630 01:26:39.782766 21943 solver.cpp:290] Iteration 36700 (48.682 iter/s, 2.05415s/100 iter), loss = -4.26918e-06
I0630 01:26:39.782791 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:39.782799 21943 sgd_solver.cpp:106] Iteration 36700, lr = 0.0426563
I0630 01:26:41.837258 21943 solver.cpp:290] Iteration 36800 (48.676 iter/s, 2.0544s/100 iter), loss = -4.26173e-06
I0630 01:26:41.837285 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:41.837293 21943 sgd_solver.cpp:106] Iteration 36800, lr = 0.0425
I0630 01:26:43.890947 21943 solver.cpp:290] Iteration 36900 (48.6951 iter/s, 2.0536s/100 iter), loss = -4.26173e-06
I0630 01:26:43.890971 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:43.890980 21943 sgd_solver.cpp:106] Iteration 36900, lr = 0.0423437
I0630 01:26:45.922472 21943 solver.cpp:464] Iteration 37000, Testing net (#0)
I0630 01:26:47.565013 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8594
I0630 01:26:47.565032 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9929
I0630 01:26:47.565037 21943 solver.cpp:537]     Test net output #2: loss = 0.42 (* 1 = 0.42 loss)
I0630 01:26:47.586832 21943 solver.cpp:290] Iteration 37000 (27.0582 iter/s, 3.69574s/100 iter), loss = -4.26546e-06
I0630 01:26:47.586902 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:47.586910 21943 sgd_solver.cpp:106] Iteration 37000, lr = 0.0421875
I0630 01:26:49.638301 21943 solver.cpp:290] Iteration 37100 (48.7487 iter/s, 2.05133s/100 iter), loss = -4.26918e-06
I0630 01:26:49.638324 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:49.638330 21943 sgd_solver.cpp:106] Iteration 37100, lr = 0.0420313
I0630 01:26:51.690881 21943 solver.cpp:290] Iteration 37200 (48.7213 iter/s, 2.05249s/100 iter), loss = -4.27663e-06
I0630 01:26:51.690945 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:51.690955 21943 sgd_solver.cpp:106] Iteration 37200, lr = 0.041875
I0630 01:26:53.743854 21943 solver.cpp:290] Iteration 37300 (48.7129 iter/s, 2.05284s/100 iter), loss = -4.26173e-06
I0630 01:26:53.743877 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:53.743883 21943 sgd_solver.cpp:106] Iteration 37300, lr = 0.0417188
I0630 01:26:55.797348 21943 solver.cpp:290] Iteration 37400 (48.6996 iter/s, 2.05341s/100 iter), loss = 0.0476148
I0630 01:26:55.797371 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:26:55.797379 21943 sgd_solver.cpp:106] Iteration 37400, lr = 0.0415625
I0630 01:26:57.849418 21943 solver.cpp:290] Iteration 37500 (48.7334 iter/s, 2.05198s/100 iter), loss = -4.27663e-06
I0630 01:26:57.849440 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:57.849447 21943 sgd_solver.cpp:106] Iteration 37500, lr = 0.0414063
I0630 01:26:59.902706 21943 solver.cpp:290] Iteration 37600 (48.7045 iter/s, 2.0532s/100 iter), loss = -4.27663e-06
I0630 01:26:59.902729 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:26:59.902736 21943 sgd_solver.cpp:106] Iteration 37600, lr = 0.04125
I0630 01:27:01.955775 21943 solver.cpp:290] Iteration 37700 (48.7097 iter/s, 2.05298s/100 iter), loss = -4.27663e-06
I0630 01:27:01.955798 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:01.955804 21943 sgd_solver.cpp:106] Iteration 37700, lr = 0.0410937
I0630 01:27:04.007791 21943 solver.cpp:290] Iteration 37800 (48.7347 iter/s, 2.05193s/100 iter), loss = -4.26173e-06
I0630 01:27:04.007813 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:04.007820 21943 sgd_solver.cpp:106] Iteration 37800, lr = 0.0409375
I0630 01:27:06.059351 21943 solver.cpp:290] Iteration 37900 (48.7455 iter/s, 2.05147s/100 iter), loss = -4.26173e-06
I0630 01:27:06.059372 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:06.059379 21943 sgd_solver.cpp:106] Iteration 37900, lr = 0.0407812
I0630 01:27:08.126080 21943 solver.cpp:464] Iteration 38000, Testing net (#0)
I0630 01:27:09.770681 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8534
I0630 01:27:09.770700 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9906
I0630 01:27:09.770705 21943 solver.cpp:537]     Test net output #2: loss = 0.4886 (* 1 = 0.4886 loss)
I0630 01:27:09.791044 21943 solver.cpp:290] Iteration 38000 (26.7985 iter/s, 3.73156s/100 iter), loss = -4.26173e-06
I0630 01:27:09.791074 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:09.791084 21943 sgd_solver.cpp:106] Iteration 38000, lr = 0.040625
I0630 01:27:11.847537 21943 solver.cpp:290] Iteration 38100 (48.6288 iter/s, 2.0564s/100 iter), loss = 0.0476148
I0630 01:27:11.847636 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:27:11.847666 21943 sgd_solver.cpp:106] Iteration 38100, lr = 0.0404688
I0630 01:27:13.897641 21943 solver.cpp:290] Iteration 38200 (48.7819 iter/s, 2.04994s/100 iter), loss = -4.26173e-06
I0630 01:27:13.897670 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:13.897678 21943 sgd_solver.cpp:106] Iteration 38200, lr = 0.0403125
I0630 01:27:15.953663 21943 solver.cpp:290] Iteration 38300 (48.6398 iter/s, 2.05593s/100 iter), loss = 0.0476148
I0630 01:27:15.953687 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:27:15.953696 21943 sgd_solver.cpp:106] Iteration 38300, lr = 0.0401563
I0630 01:27:18.010546 21943 solver.cpp:290] Iteration 38400 (48.6193 iter/s, 2.0568s/100 iter), loss = -4.26173e-06
I0630 01:27:18.010570 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:18.010577 21943 sgd_solver.cpp:106] Iteration 38400, lr = 0.04
I0630 01:27:20.063089 21943 solver.cpp:290] Iteration 38500 (48.7222 iter/s, 2.05245s/100 iter), loss = -4.26173e-06
I0630 01:27:20.063127 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:20.063133 21943 sgd_solver.cpp:106] Iteration 38500, lr = 0.0398437
I0630 01:27:22.116091 21943 solver.cpp:290] Iteration 38600 (48.7116 iter/s, 2.0529s/100 iter), loss = -4.26173e-06
I0630 01:27:22.116164 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:22.116173 21943 sgd_solver.cpp:106] Iteration 38600, lr = 0.0396875
I0630 01:27:24.167001 21943 solver.cpp:290] Iteration 38700 (48.7621 iter/s, 2.05077s/100 iter), loss = -4.26173e-06
I0630 01:27:24.167022 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:24.167031 21943 sgd_solver.cpp:106] Iteration 38700, lr = 0.0395312
I0630 01:27:26.218240 21943 solver.cpp:290] Iteration 38800 (48.7531 iter/s, 2.05115s/100 iter), loss = -4.26173e-06
I0630 01:27:26.218261 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:26.218268 21943 sgd_solver.cpp:106] Iteration 38800, lr = 0.039375
I0630 01:27:28.275305 21943 solver.cpp:290] Iteration 38900 (48.615 iter/s, 2.05698s/100 iter), loss = -4.26173e-06
I0630 01:27:28.275326 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:28.275332 21943 sgd_solver.cpp:106] Iteration 38900, lr = 0.0392187
I0630 01:27:30.307162 21943 solver.cpp:464] Iteration 39000, Testing net (#0)
I0630 01:27:31.945183 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.855
I0630 01:27:31.945201 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9892
I0630 01:27:31.945207 21943 solver.cpp:537]     Test net output #2: loss = 0.441 (* 1 = 0.441 loss)
I0630 01:27:31.964874 21943 solver.cpp:290] Iteration 39000 (27.1044 iter/s, 3.68944s/100 iter), loss = -4.26546e-06
I0630 01:27:31.964892 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:31.964902 21943 sgd_solver.cpp:106] Iteration 39000, lr = 0.0390625
I0630 01:27:34.018430 21943 solver.cpp:290] Iteration 39100 (48.698 iter/s, 2.05347s/100 iter), loss = -4.26918e-06
I0630 01:27:34.018452 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:34.018460 21943 sgd_solver.cpp:106] Iteration 39100, lr = 0.0389063
I0630 01:27:36.071876 21943 solver.cpp:290] Iteration 39200 (48.7008 iter/s, 2.05336s/100 iter), loss = -4.27663e-06
I0630 01:27:36.071898 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:36.071904 21943 sgd_solver.cpp:106] Iteration 39200, lr = 0.03875
I0630 01:27:38.127499 21943 solver.cpp:290] Iteration 39300 (48.6491 iter/s, 2.05554s/100 iter), loss = -4.27663e-06
I0630 01:27:38.127521 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:38.127528 21943 sgd_solver.cpp:106] Iteration 39300, lr = 0.0385938
I0630 01:27:40.178815 21943 solver.cpp:290] Iteration 39400 (48.7512 iter/s, 2.05123s/100 iter), loss = -4.27663e-06
I0630 01:27:40.178843 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:40.178853 21943 sgd_solver.cpp:106] Iteration 39400, lr = 0.0384375
I0630 01:27:42.232120 21943 solver.cpp:290] Iteration 39500 (48.7041 iter/s, 2.05321s/100 iter), loss = -4.28036e-06
I0630 01:27:42.232144 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:42.232152 21943 sgd_solver.cpp:106] Iteration 39500, lr = 0.0382813
I0630 01:27:44.288424 21943 solver.cpp:290] Iteration 39600 (48.633 iter/s, 2.05622s/100 iter), loss = 0.0476148
I0630 01:27:44.288449 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:27:44.288455 21943 sgd_solver.cpp:106] Iteration 39600, lr = 0.038125
I0630 01:27:46.342511 21943 solver.cpp:290] Iteration 39700 (48.6855 iter/s, 2.054s/100 iter), loss = -4.27663e-06
I0630 01:27:46.342535 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:46.342541 21943 sgd_solver.cpp:106] Iteration 39700, lr = 0.0379688
I0630 01:27:48.396636 21943 solver.cpp:290] Iteration 39800 (48.6846 iter/s, 2.05404s/100 iter), loss = -4.27663e-06
I0630 01:27:48.396659 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:48.396667 21943 sgd_solver.cpp:106] Iteration 39800, lr = 0.0378125
I0630 01:27:50.449648 21943 solver.cpp:290] Iteration 39900 (48.711 iter/s, 2.05292s/100 iter), loss = -4.27663e-06
I0630 01:27:50.449684 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:50.449692 21943 sgd_solver.cpp:106] Iteration 39900, lr = 0.0376562
I0630 01:27:52.485105 21943 solver.cpp:591] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2_iter_40000.caffemodel
I0630 01:27:52.501462 21943 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2_iter_40000.solverstate
I0630 01:27:52.508872 21943 solver.cpp:464] Iteration 40000, Testing net (#0)
I0630 01:27:54.155083 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8271
I0630 01:27:54.155102 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9881
I0630 01:27:54.155107 21943 solver.cpp:537]     Test net output #2: loss = 0.5876 (* 1 = 0.5876 loss)
I0630 01:27:54.176240 21943 solver.cpp:290] Iteration 40000 (26.8352 iter/s, 3.72645s/100 iter), loss = -4.27663e-06
I0630 01:27:54.176259 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:54.176270 21943 sgd_solver.cpp:106] Iteration 40000, lr = 0.0375
I0630 01:27:56.237707 21943 solver.cpp:290] Iteration 40100 (48.5111 iter/s, 2.06138s/100 iter), loss = -4.27663e-06
I0630 01:27:56.237730 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:56.237737 21943 sgd_solver.cpp:106] Iteration 40100, lr = 0.0373438
I0630 01:27:58.287025 21943 solver.cpp:290] Iteration 40200 (48.7988 iter/s, 2.04923s/100 iter), loss = -4.27663e-06
I0630 01:27:58.287047 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:27:58.287055 21943 sgd_solver.cpp:106] Iteration 40200, lr = 0.0371875
I0630 01:28:00.343338 21943 solver.cpp:290] Iteration 40300 (48.6329 iter/s, 2.05622s/100 iter), loss = -4.27663e-06
I0630 01:28:00.343359 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:00.343366 21943 sgd_solver.cpp:106] Iteration 40300, lr = 0.0370313
I0630 01:28:02.394876 21943 solver.cpp:290] Iteration 40400 (48.746 iter/s, 2.05145s/100 iter), loss = 0.0476148
I0630 01:28:02.394898 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:28:02.394906 21943 sgd_solver.cpp:106] Iteration 40400, lr = 0.036875
I0630 01:28:04.446318 21943 solver.cpp:290] Iteration 40500 (48.7483 iter/s, 2.05135s/100 iter), loss = -4.27663e-06
I0630 01:28:04.446343 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:04.446352 21943 sgd_solver.cpp:106] Iteration 40500, lr = 0.0367188
I0630 01:28:06.508668 21943 solver.cpp:290] Iteration 40600 (48.4905 iter/s, 2.06226s/100 iter), loss = -4.27663e-06
I0630 01:28:06.508690 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:06.508698 21943 sgd_solver.cpp:106] Iteration 40600, lr = 0.0365625
I0630 01:28:08.576058 21943 solver.cpp:290] Iteration 40700 (48.3722 iter/s, 2.0673s/100 iter), loss = -4.26173e-06
I0630 01:28:08.576081 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:08.576086 21943 sgd_solver.cpp:106] Iteration 40700, lr = 0.0364062
I0630 01:28:10.627276 21943 solver.cpp:290] Iteration 40800 (48.7536 iter/s, 2.05113s/100 iter), loss = -4.26173e-06
I0630 01:28:10.627300 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:10.627305 21943 sgd_solver.cpp:106] Iteration 40800, lr = 0.03625
I0630 01:28:12.678377 21943 solver.cpp:290] Iteration 40900 (48.7564 iter/s, 2.05101s/100 iter), loss = -4.26546e-06
I0630 01:28:12.678400 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:12.678408 21943 sgd_solver.cpp:106] Iteration 40900, lr = 0.0360937
I0630 01:28:14.707294 21943 solver.cpp:464] Iteration 41000, Testing net (#0)
I0630 01:28:16.344637 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.867299
I0630 01:28:16.344657 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9932
I0630 01:28:16.344662 21943 solver.cpp:537]     Test net output #2: loss = 0.4182 (* 1 = 0.4182 loss)
I0630 01:28:16.366766 21943 solver.cpp:290] Iteration 41000 (27.1131 iter/s, 3.68826s/100 iter), loss = 0.0476148
I0630 01:28:16.366787 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:28:16.366797 21943 sgd_solver.cpp:106] Iteration 41000, lr = 0.0359375
I0630 01:28:18.421538 21943 solver.cpp:290] Iteration 41100 (48.6692 iter/s, 2.05469s/100 iter), loss = -4.26173e-06
I0630 01:28:18.421562 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:18.421568 21943 sgd_solver.cpp:106] Iteration 41100, lr = 0.0357813
I0630 01:28:20.475837 21943 solver.cpp:290] Iteration 41200 (48.6805 iter/s, 2.05421s/100 iter), loss = -4.26173e-06
I0630 01:28:20.475859 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:20.475865 21943 sgd_solver.cpp:106] Iteration 41200, lr = 0.035625
I0630 01:28:22.527570 21943 solver.cpp:290] Iteration 41300 (48.7414 iter/s, 2.05165s/100 iter), loss = -4.26173e-06
I0630 01:28:22.527643 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:22.527652 21943 sgd_solver.cpp:106] Iteration 41300, lr = 0.0354688
I0630 01:28:24.578320 21943 solver.cpp:290] Iteration 41400 (48.7659 iter/s, 2.05061s/100 iter), loss = -4.26173e-06
I0630 01:28:24.578343 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:24.578349 21943 sgd_solver.cpp:106] Iteration 41400, lr = 0.0353125
I0630 01:28:26.630409 21943 solver.cpp:290] Iteration 41500 (48.7329 iter/s, 2.052s/100 iter), loss = -4.26173e-06
I0630 01:28:26.630434 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:26.630440 21943 sgd_solver.cpp:106] Iteration 41500, lr = 0.0351562
I0630 01:28:28.683459 21943 solver.cpp:290] Iteration 41600 (48.7103 iter/s, 2.05296s/100 iter), loss = -4.26173e-06
I0630 01:28:28.683485 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:28.683495 21943 sgd_solver.cpp:106] Iteration 41600, lr = 0.035
I0630 01:28:30.739064 21943 solver.cpp:290] Iteration 41700 (48.6496 iter/s, 2.05552s/100 iter), loss = -4.26173e-06
I0630 01:28:30.739087 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:30.739094 21943 sgd_solver.cpp:106] Iteration 41700, lr = 0.0348438
I0630 01:28:32.791477 21943 solver.cpp:290] Iteration 41800 (48.7253 iter/s, 2.05232s/100 iter), loss = -4.26173e-06
I0630 01:28:32.791504 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:32.791513 21943 sgd_solver.cpp:106] Iteration 41800, lr = 0.0346875
I0630 01:28:34.845191 21943 solver.cpp:290] Iteration 41900 (48.6944 iter/s, 2.05362s/100 iter), loss = -4.26173e-06
I0630 01:28:34.845214 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:34.845221 21943 sgd_solver.cpp:106] Iteration 41900, lr = 0.0345312
I0630 01:28:36.879477 21943 solver.cpp:464] Iteration 42000, Testing net (#0)
I0630 01:28:38.528650 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8309
I0630 01:28:38.528669 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9883
I0630 01:28:38.528676 21943 solver.cpp:537]     Test net output #2: loss = 0.5865 (* 1 = 0.5865 loss)
I0630 01:28:38.550367 21943 solver.cpp:290] Iteration 42000 (26.9902 iter/s, 3.70504s/100 iter), loss = -4.26173e-06
I0630 01:28:38.550386 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:38.550396 21943 sgd_solver.cpp:106] Iteration 42000, lr = 0.034375
I0630 01:28:40.604349 21943 solver.cpp:290] Iteration 42100 (48.688 iter/s, 2.0539s/100 iter), loss = -4.26173e-06
I0630 01:28:40.604372 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:40.604379 21943 sgd_solver.cpp:106] Iteration 42100, lr = 0.0342188
I0630 01:28:42.658145 21943 solver.cpp:290] Iteration 42200 (48.6924 iter/s, 2.05371s/100 iter), loss = -4.26173e-06
I0630 01:28:42.658169 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:42.658176 21943 sgd_solver.cpp:106] Iteration 42200, lr = 0.0340625
I0630 01:28:44.709566 21943 solver.cpp:290] Iteration 42300 (48.7488 iter/s, 2.05133s/100 iter), loss = -4.26173e-06
I0630 01:28:44.709589 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:44.709596 21943 sgd_solver.cpp:106] Iteration 42300, lr = 0.0339063
I0630 01:28:46.767472 21943 solver.cpp:290] Iteration 42400 (48.5953 iter/s, 2.05781s/100 iter), loss = -4.26173e-06
I0630 01:28:46.767498 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:46.767506 21943 sgd_solver.cpp:106] Iteration 42400, lr = 0.03375
I0630 01:28:48.821218 21943 solver.cpp:290] Iteration 42500 (48.6936 iter/s, 2.05366s/100 iter), loss = -4.26173e-06
I0630 01:28:48.821244 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:48.821252 21943 sgd_solver.cpp:106] Iteration 42500, lr = 0.0335938
I0630 01:28:50.871330 21943 solver.cpp:290] Iteration 42600 (48.78 iter/s, 2.05002s/100 iter), loss = 0.0476148
I0630 01:28:50.871368 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:28:50.871376 21943 sgd_solver.cpp:106] Iteration 42600, lr = 0.0334375
I0630 01:28:52.924681 21943 solver.cpp:290] Iteration 42700 (48.7033 iter/s, 2.05325s/100 iter), loss = 0.0476148
I0630 01:28:52.924746 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:28:52.924753 21943 sgd_solver.cpp:106] Iteration 42700, lr = 0.0332812
I0630 01:28:54.975661 21943 solver.cpp:290] Iteration 42800 (48.7602 iter/s, 2.05085s/100 iter), loss = -4.26173e-06
I0630 01:28:54.975683 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:54.975689 21943 sgd_solver.cpp:106] Iteration 42800, lr = 0.033125
I0630 01:28:57.029201 21943 solver.cpp:290] Iteration 42900 (48.6985 iter/s, 2.05345s/100 iter), loss = -4.26173e-06
I0630 01:28:57.029227 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:28:57.029235 21943 sgd_solver.cpp:106] Iteration 42900, lr = 0.0329687
I0630 01:28:59.062393 21943 solver.cpp:464] Iteration 43000, Testing net (#0)
I0630 01:29:00.706599 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8961
I0630 01:29:00.706619 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9943
I0630 01:29:00.706624 21943 solver.cpp:537]     Test net output #2: loss = 0.3258 (* 1 = 0.3258 loss)
I0630 01:29:00.727263 21943 solver.cpp:290] Iteration 43000 (27.0422 iter/s, 3.69792s/100 iter), loss = -4.26173e-06
I0630 01:29:00.727289 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:00.727296 21943 sgd_solver.cpp:106] Iteration 43000, lr = 0.0328125
I0630 01:29:02.778466 21943 solver.cpp:290] Iteration 43100 (48.7541 iter/s, 2.05111s/100 iter), loss = -4.26173e-06
I0630 01:29:02.778491 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:02.778496 21943 sgd_solver.cpp:106] Iteration 43100, lr = 0.0326563
I0630 01:29:04.832963 21943 solver.cpp:290] Iteration 43200 (48.6758 iter/s, 2.05441s/100 iter), loss = -4.26173e-06
I0630 01:29:04.832988 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:04.832993 21943 sgd_solver.cpp:106] Iteration 43200, lr = 0.0325
I0630 01:29:06.883393 21943 solver.cpp:290] Iteration 43300 (48.7724 iter/s, 2.05034s/100 iter), loss = 0.0476148
I0630 01:29:06.883415 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:29:06.883421 21943 sgd_solver.cpp:106] Iteration 43300, lr = 0.0323438
I0630 01:29:08.961318 21943 solver.cpp:290] Iteration 43400 (48.127 iter/s, 2.07784s/100 iter), loss = 0.0476148
I0630 01:29:08.961341 21943 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:29:08.961347 21943 sgd_solver.cpp:106] Iteration 43400, lr = 0.0321875
I0630 01:29:11.015233 21943 solver.cpp:290] Iteration 43500 (48.6896 iter/s, 2.05383s/100 iter), loss = -4.26173e-06
I0630 01:29:11.015257 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:11.015264 21943 sgd_solver.cpp:106] Iteration 43500, lr = 0.0320312
I0630 01:29:13.065632 21943 solver.cpp:290] Iteration 43600 (48.7731 iter/s, 2.05031s/100 iter), loss = -4.26173e-06
I0630 01:29:13.065655 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:13.065662 21943 sgd_solver.cpp:106] Iteration 43600, lr = 0.031875
I0630 01:29:15.117429 21943 solver.cpp:290] Iteration 43700 (48.7399 iter/s, 2.05171s/100 iter), loss = -4.26173e-06
I0630 01:29:15.117450 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:15.117457 21943 sgd_solver.cpp:106] Iteration 43700, lr = 0.0317187
I0630 01:29:17.172041 21943 solver.cpp:290] Iteration 43800 (48.6731 iter/s, 2.05452s/100 iter), loss = -4.26173e-06
I0630 01:29:17.172068 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:17.172077 21943 sgd_solver.cpp:106] Iteration 43800, lr = 0.0315625
I0630 01:29:19.224287 21943 solver.cpp:290] Iteration 43900 (48.7293 iter/s, 2.05216s/100 iter), loss = -4.26173e-06
I0630 01:29:19.224311 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:19.224319 21943 sgd_solver.cpp:106] Iteration 43900, lr = 0.0314062
I0630 01:29:21.262619 21943 solver.cpp:464] Iteration 44000, Testing net (#0)
I0630 01:29:22.904180 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8771
I0630 01:29:22.904201 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9944
I0630 01:29:22.904206 21943 solver.cpp:537]     Test net output #2: loss = 0.4064 (* 1 = 0.4064 loss)
I0630 01:29:22.923792 21943 solver.cpp:290] Iteration 44000 (27.0316 iter/s, 3.69937s/100 iter), loss = -4.26173e-06
I0630 01:29:22.923810 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:22.923821 21943 sgd_solver.cpp:106] Iteration 44000, lr = 0.03125
I0630 01:29:24.981920 21943 solver.cpp:290] Iteration 44100 (48.5898 iter/s, 2.05804s/100 iter), loss = -4.26173e-06
I0630 01:29:24.981994 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:24.982002 21943 sgd_solver.cpp:106] Iteration 44100, lr = 0.0310938
I0630 01:29:27.031167 21943 solver.cpp:290] Iteration 44200 (48.8017 iter/s, 2.04911s/100 iter), loss = -4.26173e-06
I0630 01:29:27.031189 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:27.031195 21943 sgd_solver.cpp:106] Iteration 44200, lr = 0.0309375
I0630 01:29:29.085896 21943 solver.cpp:290] Iteration 44300 (48.6703 iter/s, 2.05464s/100 iter), loss = -4.26173e-06
I0630 01:29:29.085918 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:29.085925 21943 sgd_solver.cpp:106] Iteration 44300, lr = 0.0307813
I0630 01:29:31.141440 21943 solver.cpp:290] Iteration 44400 (48.651 iter/s, 2.05546s/100 iter), loss = -4.26173e-06
I0630 01:29:31.141466 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:31.141474 21943 sgd_solver.cpp:106] Iteration 44400, lr = 0.030625
I0630 01:29:33.194684 21943 solver.cpp:290] Iteration 44500 (48.7056 iter/s, 2.05315s/100 iter), loss = -4.26173e-06
I0630 01:29:33.194711 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:33.194720 21943 sgd_solver.cpp:106] Iteration 44500, lr = 0.0304688
I0630 01:29:35.248055 21943 solver.cpp:290] Iteration 44600 (48.7026 iter/s, 2.05328s/100 iter), loss = -4.26173e-06
I0630 01:29:35.248078 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:35.248085 21943 sgd_solver.cpp:106] Iteration 44600, lr = 0.0303125
I0630 01:29:37.299849 21943 solver.cpp:290] Iteration 44700 (48.74 iter/s, 2.0517s/100 iter), loss = -4.26173e-06
I0630 01:29:37.299871 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:37.299877 21943 sgd_solver.cpp:106] Iteration 44700, lr = 0.0301562
I0630 01:29:39.354521 21943 solver.cpp:290] Iteration 44800 (48.6717 iter/s, 2.05458s/100 iter), loss = -4.26173e-06
I0630 01:29:39.354543 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:39.354550 21943 sgd_solver.cpp:106] Iteration 44800, lr = 0.03
I0630 01:29:41.407850 21943 solver.cpp:290] Iteration 44900 (48.7035 iter/s, 2.05324s/100 iter), loss = -4.26173e-06
I0630 01:29:41.407872 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:41.407878 21943 sgd_solver.cpp:106] Iteration 44900, lr = 0.0298437
I0630 01:29:43.442806 21943 solver.cpp:464] Iteration 45000, Testing net (#0)
I0630 01:29:45.078666 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.8964
I0630 01:29:45.078685 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9968
I0630 01:29:45.078691 21943 solver.cpp:537]     Test net output #2: loss = 0.3096 (* 1 = 0.3096 loss)
I0630 01:29:45.099014 21943 solver.cpp:290] Iteration 45000 (27.0927 iter/s, 3.69103s/100 iter), loss = -4.26173e-06
I0630 01:29:45.099032 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:45.099045 21943 sgd_solver.cpp:106] Iteration 45000, lr = 0.0296875
I0630 01:29:47.154001 21943 solver.cpp:290] Iteration 45100 (48.6641 iter/s, 2.0549s/100 iter), loss = -4.26173e-06
I0630 01:29:47.154023 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:47.154031 21943 sgd_solver.cpp:106] Iteration 45100, lr = 0.0295313
I0630 01:29:49.208755 21943 solver.cpp:290] Iteration 45200 (48.6698 iter/s, 2.05466s/100 iter), loss = -4.26173e-06
I0630 01:29:49.208781 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:49.208787 21943 sgd_solver.cpp:106] Iteration 45200, lr = 0.029375
I0630 01:29:51.260407 21943 solver.cpp:290] Iteration 45300 (48.7434 iter/s, 2.05156s/100 iter), loss = -4.26173e-06
I0630 01:29:51.260429 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:51.260435 21943 sgd_solver.cpp:106] Iteration 45300, lr = 0.0292188
I0630 01:29:53.314496 21943 solver.cpp:290] Iteration 45400 (48.6855 iter/s, 2.054s/100 iter), loss = -4.26173e-06
I0630 01:29:53.314533 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:53.314541 21943 sgd_solver.cpp:106] Iteration 45400, lr = 0.0290625
I0630 01:29:55.370805 21943 solver.cpp:290] Iteration 45500 (48.6332 iter/s, 2.05621s/100 iter), loss = -4.26173e-06
I0630 01:29:55.370877 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:55.370887 21943 sgd_solver.cpp:106] Iteration 45500, lr = 0.0289063
I0630 01:29:57.422026 21943 solver.cpp:290] Iteration 45600 (48.7547 iter/s, 2.05109s/100 iter), loss = -4.26173e-06
I0630 01:29:57.422049 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:57.422058 21943 sgd_solver.cpp:106] Iteration 45600, lr = 0.02875
I0630 01:29:59.481034 21943 solver.cpp:290] Iteration 45700 (48.5692 iter/s, 2.05892s/100 iter), loss = -4.26173e-06
I0630 01:29:59.481056 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:29:59.481062 21943 sgd_solver.cpp:106] Iteration 45700, lr = 0.0285937
I0630 01:30:01.533629 21943 solver.cpp:290] Iteration 45800 (48.7209 iter/s, 2.05251s/100 iter), loss = -4.26173e-06
I0630 01:30:01.533651 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:01.533658 21943 sgd_solver.cpp:106] Iteration 45800, lr = 0.0284375
I0630 01:30:03.584252 21943 solver.cpp:290] Iteration 45900 (48.7678 iter/s, 2.05053s/100 iter), loss = -4.26173e-06
I0630 01:30:03.584280 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:03.584290 21943 sgd_solver.cpp:106] Iteration 45900, lr = 0.0282812
I0630 01:30:05.619021 21943 solver.cpp:464] Iteration 46000, Testing net (#0)
I0630 01:30:07.256717 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9055
I0630 01:30:07.256747 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9976
I0630 01:30:07.256753 21943 solver.cpp:537]     Test net output #2: loss = 0.2594 (* 1 = 0.2594 loss)
I0630 01:30:07.276980 21943 solver.cpp:290] Iteration 46000 (27.0813 iter/s, 3.69259s/100 iter), loss = -4.26173e-06
I0630 01:30:07.277034 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:07.277053 21943 sgd_solver.cpp:106] Iteration 46000, lr = 0.028125
I0630 01:30:09.339906 21943 solver.cpp:290] Iteration 46100 (48.4776 iter/s, 2.06281s/100 iter), loss = -4.26173e-06
I0630 01:30:09.339928 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:09.339936 21943 sgd_solver.cpp:106] Iteration 46100, lr = 0.0279688
I0630 01:30:11.391324 21943 solver.cpp:290] Iteration 46200 (48.7489 iter/s, 2.05133s/100 iter), loss = -4.26173e-06
I0630 01:30:11.391346 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:11.391353 21943 sgd_solver.cpp:106] Iteration 46200, lr = 0.0278125
I0630 01:30:13.446686 21943 solver.cpp:290] Iteration 46300 (48.6553 iter/s, 2.05527s/100 iter), loss = -4.26173e-06
I0630 01:30:13.446708 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:13.446715 21943 sgd_solver.cpp:106] Iteration 46300, lr = 0.0276563
I0630 01:30:15.499264 21943 solver.cpp:290] Iteration 46400 (48.7214 iter/s, 2.05249s/100 iter), loss = -4.26173e-06
I0630 01:30:15.499291 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:15.499300 21943 sgd_solver.cpp:106] Iteration 46400, lr = 0.0275
I0630 01:30:17.552530 21943 solver.cpp:290] Iteration 46500 (48.7051 iter/s, 2.05317s/100 iter), loss = -4.26173e-06
I0630 01:30:17.552556 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:17.552564 21943 sgd_solver.cpp:106] Iteration 46500, lr = 0.0273438
I0630 01:30:19.603632 21943 solver.cpp:290] Iteration 46600 (48.7564 iter/s, 2.05101s/100 iter), loss = -4.26173e-06
I0630 01:30:19.603654 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:19.603662 21943 sgd_solver.cpp:106] Iteration 46600, lr = 0.0271875
I0630 01:30:21.657285 21943 solver.cpp:290] Iteration 46700 (48.6958 iter/s, 2.05357s/100 iter), loss = -4.26173e-06
I0630 01:30:21.657308 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:21.657315 21943 sgd_solver.cpp:106] Iteration 46700, lr = 0.0270312
I0630 01:30:23.711032 21943 solver.cpp:290] Iteration 46800 (48.6936 iter/s, 2.05366s/100 iter), loss = -4.26173e-06
I0630 01:30:23.711081 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:23.711091 21943 sgd_solver.cpp:106] Iteration 46800, lr = 0.026875
I0630 01:30:25.765427 21943 solver.cpp:290] Iteration 46900 (48.6788 iter/s, 2.05428s/100 iter), loss = -4.26173e-06
I0630 01:30:25.765503 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:25.765511 21943 sgd_solver.cpp:106] Iteration 46900, lr = 0.0267187
I0630 01:30:27.794214 21943 solver.cpp:464] Iteration 47000, Testing net (#0)
I0630 01:30:29.432047 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9061
I0630 01:30:29.432066 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9962
I0630 01:30:29.432071 21943 solver.cpp:537]     Test net output #2: loss = 0.2588 (* 1 = 0.2588 loss)
I0630 01:30:29.452316 21943 solver.cpp:290] Iteration 47000 (27.1245 iter/s, 3.6867s/100 iter), loss = -4.26173e-06
I0630 01:30:29.452337 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:29.452349 21943 sgd_solver.cpp:106] Iteration 47000, lr = 0.0265625
I0630 01:30:31.510675 21943 solver.cpp:290] Iteration 47100 (48.5845 iter/s, 2.05827s/100 iter), loss = -4.26173e-06
I0630 01:30:31.510697 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:31.510705 21943 sgd_solver.cpp:106] Iteration 47100, lr = 0.0264063
I0630 01:30:33.565860 21943 solver.cpp:290] Iteration 47200 (48.6595 iter/s, 2.0551s/100 iter), loss = -4.26173e-06
I0630 01:30:33.565881 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:33.565887 21943 sgd_solver.cpp:106] Iteration 47200, lr = 0.02625
I0630 01:30:35.620120 21943 solver.cpp:290] Iteration 47300 (48.6814 iter/s, 2.05417s/100 iter), loss = -4.26173e-06
I0630 01:30:35.620142 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:35.620148 21943 sgd_solver.cpp:106] Iteration 47300, lr = 0.0260938
I0630 01:30:37.673514 21943 solver.cpp:290] Iteration 47400 (48.7019 iter/s, 2.05331s/100 iter), loss = -4.26173e-06
I0630 01:30:37.673537 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:37.673544 21943 sgd_solver.cpp:106] Iteration 47400, lr = 0.0259375
I0630 01:30:39.728787 21943 solver.cpp:290] Iteration 47500 (48.6574 iter/s, 2.05519s/100 iter), loss = -4.26173e-06
I0630 01:30:39.728811 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:39.728817 21943 sgd_solver.cpp:106] Iteration 47500, lr = 0.0257812
I0630 01:30:41.787009 21943 solver.cpp:290] Iteration 47600 (48.5877 iter/s, 2.05813s/100 iter), loss = -4.26173e-06
I0630 01:30:41.787034 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:41.787042 21943 sgd_solver.cpp:106] Iteration 47600, lr = 0.025625
I0630 01:30:43.837433 21943 solver.cpp:290] Iteration 47700 (48.7725 iter/s, 2.05033s/100 iter), loss = -4.26173e-06
I0630 01:30:43.837456 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:43.837463 21943 sgd_solver.cpp:106] Iteration 47700, lr = 0.0254687
I0630 01:30:45.887143 21943 solver.cpp:290] Iteration 47800 (48.7895 iter/s, 2.04962s/100 iter), loss = -4.26173e-06
I0630 01:30:45.887166 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:45.887176 21943 sgd_solver.cpp:106] Iteration 47800, lr = 0.0253125
I0630 01:30:47.945744 21943 solver.cpp:290] Iteration 47900 (48.5788 iter/s, 2.05851s/100 iter), loss = -4.26173e-06
I0630 01:30:47.945765 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:47.945773 21943 sgd_solver.cpp:106] Iteration 47900, lr = 0.0251562
I0630 01:30:49.976879 21943 solver.cpp:464] Iteration 48000, Testing net (#0)
I0630 01:30:51.613140 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9084
I0630 01:30:51.613160 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9967
I0630 01:30:51.613168 21943 solver.cpp:537]     Test net output #2: loss = 0.2529 (* 1 = 0.2529 loss)
I0630 01:30:51.632741 21943 solver.cpp:290] Iteration 48000 (27.1233 iter/s, 3.68687s/100 iter), loss = -4.26173e-06
I0630 01:30:51.632761 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:51.632772 21943 sgd_solver.cpp:106] Iteration 48000, lr = 0.025
I0630 01:30:53.687042 21943 solver.cpp:290] Iteration 48100 (48.6804 iter/s, 2.05422s/100 iter), loss = -4.26173e-06
I0630 01:30:53.687085 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:53.687093 21943 sgd_solver.cpp:106] Iteration 48100, lr = 0.0248438
I0630 01:30:55.737102 21943 solver.cpp:290] Iteration 48200 (48.7816 iter/s, 2.04995s/100 iter), loss = -4.26173e-06
I0630 01:30:55.737125 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:55.737131 21943 sgd_solver.cpp:106] Iteration 48200, lr = 0.0246875
I0630 01:30:57.790031 21943 solver.cpp:290] Iteration 48300 (48.713 iter/s, 2.05284s/100 iter), loss = -4.26173e-06
I0630 01:30:57.790101 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:57.790108 21943 sgd_solver.cpp:106] Iteration 48300, lr = 0.0245313
I0630 01:30:59.847306 21943 solver.cpp:290] Iteration 48400 (48.6111 iter/s, 2.05714s/100 iter), loss = -4.26173e-06
I0630 01:30:59.847329 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:30:59.847337 21943 sgd_solver.cpp:106] Iteration 48400, lr = 0.024375
I0630 01:31:01.902158 21943 solver.cpp:290] Iteration 48500 (48.6674 iter/s, 2.05476s/100 iter), loss = -4.26173e-06
I0630 01:31:01.902181 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:01.902189 21943 sgd_solver.cpp:106] Iteration 48500, lr = 0.0242188
I0630 01:31:03.955768 21943 solver.cpp:290] Iteration 48600 (48.6968 iter/s, 2.05352s/100 iter), loss = -4.26173e-06
I0630 01:31:03.955791 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:03.955797 21943 sgd_solver.cpp:106] Iteration 48600, lr = 0.0240625
I0630 01:31:06.008584 21943 solver.cpp:290] Iteration 48700 (48.7157 iter/s, 2.05273s/100 iter), loss = -4.26173e-06
I0630 01:31:06.008607 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:06.008616 21943 sgd_solver.cpp:106] Iteration 48700, lr = 0.0239062
I0630 01:31:08.064211 21943 solver.cpp:290] Iteration 48800 (48.649 iter/s, 2.05554s/100 iter), loss = -4.26173e-06
I0630 01:31:08.064235 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:08.064241 21943 sgd_solver.cpp:106] Iteration 48800, lr = 0.02375
I0630 01:31:10.113942 21943 solver.cpp:290] Iteration 48900 (48.789 iter/s, 2.04964s/100 iter), loss = -4.26173e-06
I0630 01:31:10.113965 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:10.113972 21943 sgd_solver.cpp:106] Iteration 48900, lr = 0.0235937
I0630 01:31:12.147403 21943 solver.cpp:464] Iteration 49000, Testing net (#0)
I0630 01:31:13.785253 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9147
I0630 01:31:13.785271 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9975
I0630 01:31:13.785277 21943 solver.cpp:537]     Test net output #2: loss = 0.2368 (* 1 = 0.2368 loss)
I0630 01:31:13.805263 21943 solver.cpp:290] Iteration 49000 (27.0915 iter/s, 3.69119s/100 iter), loss = -4.26173e-06
I0630 01:31:13.805284 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:13.805292 21943 sgd_solver.cpp:106] Iteration 49000, lr = 0.0234375
I0630 01:31:15.861508 21943 solver.cpp:290] Iteration 49100 (48.6343 iter/s, 2.05616s/100 iter), loss = -4.26173e-06
I0630 01:31:15.861531 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:15.861536 21943 sgd_solver.cpp:106] Iteration 49100, lr = 0.0232813
I0630 01:31:17.912359 21943 solver.cpp:290] Iteration 49200 (48.7623 iter/s, 2.05076s/100 iter), loss = -4.26173e-06
I0630 01:31:17.912382 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:17.912389 21943 sgd_solver.cpp:106] Iteration 49200, lr = 0.023125
I0630 01:31:19.962630 21943 solver.cpp:290] Iteration 49300 (48.7761 iter/s, 2.05018s/100 iter), loss = -4.26173e-06
I0630 01:31:19.962651 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:19.962659 21943 sgd_solver.cpp:106] Iteration 49300, lr = 0.0229688
I0630 01:31:22.022286 21943 solver.cpp:290] Iteration 49400 (48.5538 iter/s, 2.05957s/100 iter), loss = -4.26173e-06
I0630 01:31:22.022312 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:22.022322 21943 sgd_solver.cpp:106] Iteration 49400, lr = 0.0228125
I0630 01:31:24.077085 21943 solver.cpp:290] Iteration 49500 (48.6687 iter/s, 2.05471s/100 iter), loss = -4.26173e-06
I0630 01:31:24.077109 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:24.077118 21943 sgd_solver.cpp:106] Iteration 49500, lr = 0.0226563
I0630 01:31:26.132864 21943 solver.cpp:290] Iteration 49600 (48.6454 iter/s, 2.05569s/100 iter), loss = -4.26173e-06
I0630 01:31:26.132902 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:26.132908 21943 sgd_solver.cpp:106] Iteration 49600, lr = 0.0225
I0630 01:31:28.188843 21943 solver.cpp:290] Iteration 49700 (48.641 iter/s, 2.05588s/100 iter), loss = -4.26173e-06
I0630 01:31:28.188877 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:28.188885 21943 sgd_solver.cpp:106] Iteration 49700, lr = 0.0223437
I0630 01:31:30.241351 21943 solver.cpp:290] Iteration 49800 (48.7232 iter/s, 2.05241s/100 iter), loss = -4.26173e-06
I0630 01:31:30.241380 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:30.241389 21943 sgd_solver.cpp:106] Iteration 49800, lr = 0.0221875
I0630 01:31:32.291960 21943 solver.cpp:290] Iteration 49900 (48.7682 iter/s, 2.05052s/100 iter), loss = -4.26173e-06
I0630 01:31:32.291985 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:32.291991 21943 sgd_solver.cpp:106] Iteration 49900, lr = 0.0220312
I0630 01:31:34.323418 21943 solver.cpp:591] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2_iter_50000.caffemodel
I0630 01:31:34.340333 21943 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2_iter_50000.solverstate
I0630 01:31:34.347687 21943 solver.cpp:464] Iteration 50000, Testing net (#0)
I0630 01:31:35.983736 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9147
I0630 01:31:35.983754 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:31:35.983759 21943 solver.cpp:537]     Test net output #2: loss = 0.2317 (* 1 = 0.2317 loss)
I0630 01:31:36.003715 21943 solver.cpp:290] Iteration 50000 (26.9424 iter/s, 3.71162s/100 iter), loss = -4.26173e-06
I0630 01:31:36.003736 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:36.003744 21943 sgd_solver.cpp:106] Iteration 50000, lr = 0.021875
I0630 01:31:38.057801 21943 solver.cpp:290] Iteration 50100 (48.6855 iter/s, 2.054s/100 iter), loss = -4.26173e-06
I0630 01:31:38.057824 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:38.057832 21943 sgd_solver.cpp:106] Iteration 50100, lr = 0.0217188
I0630 01:31:40.111351 21943 solver.cpp:290] Iteration 50200 (48.6982 iter/s, 2.05346s/100 iter), loss = -4.26173e-06
I0630 01:31:40.111374 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:40.111382 21943 sgd_solver.cpp:106] Iteration 50200, lr = 0.0215625
I0630 01:31:42.164219 21943 solver.cpp:290] Iteration 50300 (48.7145 iter/s, 2.05278s/100 iter), loss = -4.26173e-06
I0630 01:31:42.164242 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:42.164248 21943 sgd_solver.cpp:106] Iteration 50300, lr = 0.0214063
I0630 01:31:44.216680 21943 solver.cpp:290] Iteration 50400 (48.7241 iter/s, 2.05237s/100 iter), loss = -4.26173e-06
I0630 01:31:44.216702 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:44.216708 21943 sgd_solver.cpp:106] Iteration 50400, lr = 0.02125
I0630 01:31:46.272986 21943 solver.cpp:290] Iteration 50500 (48.6329 iter/s, 2.05622s/100 iter), loss = -4.26173e-06
I0630 01:31:46.273010 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:46.273016 21943 sgd_solver.cpp:106] Iteration 50500, lr = 0.0210938
I0630 01:31:48.332276 21943 solver.cpp:290] Iteration 50600 (48.5626 iter/s, 2.0592s/100 iter), loss = -4.26173e-06
I0630 01:31:48.332301 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:48.332310 21943 sgd_solver.cpp:106] Iteration 50600, lr = 0.0209375
I0630 01:31:50.384018 21943 solver.cpp:290] Iteration 50700 (48.7412 iter/s, 2.05165s/100 iter), loss = -4.26173e-06
I0630 01:31:50.384040 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:50.384048 21943 sgd_solver.cpp:106] Iteration 50700, lr = 0.0207812
I0630 01:31:52.435431 21943 solver.cpp:290] Iteration 50800 (48.749 iter/s, 2.05132s/100 iter), loss = -4.26173e-06
I0630 01:31:52.435452 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:52.435459 21943 sgd_solver.cpp:106] Iteration 50800, lr = 0.020625
I0630 01:31:54.489876 21943 solver.cpp:290] Iteration 50900 (48.677 iter/s, 2.05436s/100 iter), loss = -4.26173e-06
I0630 01:31:54.489899 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:54.489905 21943 sgd_solver.cpp:106] Iteration 50900, lr = 0.0204687
I0630 01:31:56.518812 21943 solver.cpp:464] Iteration 51000, Testing net (#0)
I0630 01:31:58.162837 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9157
I0630 01:31:58.162858 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9975
I0630 01:31:58.162865 21943 solver.cpp:537]     Test net output #2: loss = 0.2279 (* 1 = 0.2279 loss)
I0630 01:31:58.182938 21943 solver.cpp:290] Iteration 51000 (27.0788 iter/s, 3.69293s/100 iter), loss = -4.26173e-06
I0630 01:31:58.182960 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:31:58.182970 21943 sgd_solver.cpp:106] Iteration 51000, lr = 0.0203125
I0630 01:32:00.241127 21943 solver.cpp:290] Iteration 51100 (48.5885 iter/s, 2.0581s/100 iter), loss = -4.26173e-06
I0630 01:32:00.241173 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:00.241179 21943 sgd_solver.cpp:106] Iteration 51100, lr = 0.0201563
I0630 01:32:02.291999 21943 solver.cpp:290] Iteration 51200 (48.7624 iter/s, 2.05076s/100 iter), loss = -4.26173e-06
I0630 01:32:02.292026 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:02.292032 21943 sgd_solver.cpp:106] Iteration 51200, lr = 0.02
I0630 01:32:04.344166 21943 solver.cpp:290] Iteration 51300 (48.7312 iter/s, 2.05207s/100 iter), loss = -4.26173e-06
I0630 01:32:04.344189 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:04.344197 21943 sgd_solver.cpp:106] Iteration 51300, lr = 0.0198438
I0630 01:32:06.399852 21943 solver.cpp:290] Iteration 51400 (48.6476 iter/s, 2.0556s/100 iter), loss = -4.26173e-06
I0630 01:32:06.399874 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:06.399880 21943 sgd_solver.cpp:106] Iteration 51400, lr = 0.0196875
I0630 01:32:08.469954 21943 solver.cpp:290] Iteration 51500 (48.3088 iter/s, 2.07001s/100 iter), loss = -4.26173e-06
I0630 01:32:08.469976 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:08.469985 21943 sgd_solver.cpp:106] Iteration 51500, lr = 0.0195312
I0630 01:32:10.519352 21943 solver.cpp:290] Iteration 51600 (48.7969 iter/s, 2.04931s/100 iter), loss = -4.26173e-06
I0630 01:32:10.519374 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:10.519381 21943 sgd_solver.cpp:106] Iteration 51600, lr = 0.019375
I0630 01:32:12.571151 21943 solver.cpp:290] Iteration 51700 (48.7398 iter/s, 2.05171s/100 iter), loss = -4.26173e-06
I0630 01:32:12.571173 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:12.571182 21943 sgd_solver.cpp:106] Iteration 51700, lr = 0.0192187
I0630 01:32:14.624414 21943 solver.cpp:290] Iteration 51800 (48.705 iter/s, 2.05318s/100 iter), loss = -4.26173e-06
I0630 01:32:14.624439 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:14.624445 21943 sgd_solver.cpp:106] Iteration 51800, lr = 0.0190625
I0630 01:32:16.679193 21943 solver.cpp:290] Iteration 51900 (48.6692 iter/s, 2.05469s/100 iter), loss = -4.26173e-06
I0630 01:32:16.679216 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:16.679224 21943 sgd_solver.cpp:106] Iteration 51900, lr = 0.0189062
I0630 01:32:18.709491 21943 solver.cpp:464] Iteration 52000, Testing net (#0)
I0630 01:32:20.348958 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9154
I0630 01:32:20.348984 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9976
I0630 01:32:20.348991 21943 solver.cpp:537]     Test net output #2: loss = 0.2163 (* 1 = 0.2163 loss)
I0630 01:32:20.369504 21943 solver.cpp:290] Iteration 52000 (27.099 iter/s, 3.69017s/100 iter), loss = -4.26173e-06
I0630 01:32:20.369527 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:20.369536 21943 sgd_solver.cpp:106] Iteration 52000, lr = 0.01875
I0630 01:32:22.429260 21943 solver.cpp:290] Iteration 52100 (48.5515 iter/s, 2.05967s/100 iter), loss = -4.26173e-06
I0630 01:32:22.429281 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:22.429288 21943 sgd_solver.cpp:106] Iteration 52100, lr = 0.0185938
I0630 01:32:24.484519 21943 solver.cpp:290] Iteration 52200 (48.6577 iter/s, 2.05517s/100 iter), loss = -4.26173e-06
I0630 01:32:24.484541 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:24.484549 21943 sgd_solver.cpp:106] Iteration 52200, lr = 0.0184375
I0630 01:32:26.534739 21943 solver.cpp:290] Iteration 52300 (48.7773 iter/s, 2.05013s/100 iter), loss = -4.26173e-06
I0630 01:32:26.534761 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:26.534767 21943 sgd_solver.cpp:106] Iteration 52300, lr = 0.0182813
I0630 01:32:28.587628 21943 solver.cpp:290] Iteration 52400 (48.7139 iter/s, 2.0528s/100 iter), loss = -4.26173e-06
I0630 01:32:28.587677 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:28.587687 21943 sgd_solver.cpp:106] Iteration 52400, lr = 0.018125
I0630 01:32:30.646215 21943 solver.cpp:290] Iteration 52500 (48.5796 iter/s, 2.05848s/100 iter), loss = -4.26173e-06
I0630 01:32:30.646278 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:30.646286 21943 sgd_solver.cpp:106] Iteration 52500, lr = 0.0179687
I0630 01:32:32.698853 21943 solver.cpp:290] Iteration 52600 (48.7208 iter/s, 2.05251s/100 iter), loss = -4.26173e-06
I0630 01:32:32.698878 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:32.698884 21943 sgd_solver.cpp:106] Iteration 52600, lr = 0.0178125
I0630 01:32:34.750497 21943 solver.cpp:290] Iteration 52700 (48.7435 iter/s, 2.05156s/100 iter), loss = -4.26173e-06
I0630 01:32:34.750524 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:34.750531 21943 sgd_solver.cpp:106] Iteration 52700, lr = 0.0176562
I0630 01:32:36.813443 21943 solver.cpp:290] Iteration 52800 (48.4765 iter/s, 2.06286s/100 iter), loss = -4.26173e-06
I0630 01:32:36.813467 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:36.813473 21943 sgd_solver.cpp:106] Iteration 52800, lr = 0.0175
I0630 01:32:38.865880 21943 solver.cpp:290] Iteration 52900 (48.7247 iter/s, 2.05235s/100 iter), loss = -4.26173e-06
I0630 01:32:38.865903 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:38.865911 21943 sgd_solver.cpp:106] Iteration 52900, lr = 0.0173437
I0630 01:32:40.899868 21943 solver.cpp:464] Iteration 53000, Testing net (#0)
I0630 01:32:42.544433 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.918
I0630 01:32:42.544451 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9972
I0630 01:32:42.544457 21943 solver.cpp:537]     Test net output #2: loss = 0.2204 (* 1 = 0.2204 loss)
I0630 01:32:42.564606 21943 solver.cpp:290] Iteration 53000 (27.0373 iter/s, 3.69859s/100 iter), loss = -4.26173e-06
I0630 01:32:42.564626 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:42.564635 21943 sgd_solver.cpp:106] Iteration 53000, lr = 0.0171875
I0630 01:32:44.619278 21943 solver.cpp:290] Iteration 53100 (48.6716 iter/s, 2.05459s/100 iter), loss = -4.26173e-06
I0630 01:32:44.619299 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:44.619307 21943 sgd_solver.cpp:106] Iteration 53100, lr = 0.0170313
I0630 01:32:46.672660 21943 solver.cpp:290] Iteration 53200 (48.7022 iter/s, 2.0533s/100 iter), loss = -4.26173e-06
I0630 01:32:46.672682 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:46.672689 21943 sgd_solver.cpp:106] Iteration 53200, lr = 0.016875
I0630 01:32:48.723552 21943 solver.cpp:290] Iteration 53300 (48.7613 iter/s, 2.05081s/100 iter), loss = -4.26173e-06
I0630 01:32:48.723574 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:48.723582 21943 sgd_solver.cpp:106] Iteration 53300, lr = 0.0167188
I0630 01:32:50.779541 21943 solver.cpp:290] Iteration 53400 (48.6405 iter/s, 2.0559s/100 iter), loss = -4.26173e-06
I0630 01:32:50.779563 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:50.779569 21943 sgd_solver.cpp:106] Iteration 53400, lr = 0.0165625
I0630 01:32:52.832829 21943 solver.cpp:290] Iteration 53500 (48.7044 iter/s, 2.0532s/100 iter), loss = -4.26173e-06
I0630 01:32:52.832851 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:52.832859 21943 sgd_solver.cpp:106] Iteration 53500, lr = 0.0164063
I0630 01:32:54.883074 21943 solver.cpp:290] Iteration 53600 (48.7768 iter/s, 2.05016s/100 iter), loss = -4.26173e-06
I0630 01:32:54.883096 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:54.883105 21943 sgd_solver.cpp:106] Iteration 53600, lr = 0.01625
I0630 01:32:56.937245 21943 solver.cpp:290] Iteration 53700 (48.6835 iter/s, 2.05408s/100 iter), loss = -4.26173e-06
I0630 01:32:56.937268 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:56.937273 21943 sgd_solver.cpp:106] Iteration 53700, lr = 0.0160937
I0630 01:32:58.989621 21943 solver.cpp:290] Iteration 53800 (48.7261 iter/s, 2.05229s/100 iter), loss = -4.26173e-06
I0630 01:32:58.989660 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:32:58.989666 21943 sgd_solver.cpp:106] Iteration 53800, lr = 0.0159375
I0630 01:33:01.043501 21943 solver.cpp:290] Iteration 53900 (48.6908 iter/s, 2.05378s/100 iter), loss = -4.26173e-06
I0630 01:33:01.043562 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:01.043573 21943 sgd_solver.cpp:106] Iteration 53900, lr = 0.0157812
I0630 01:33:03.087435 21943 solver.cpp:464] Iteration 54000, Testing net (#0)
I0630 01:33:04.723719 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9178
I0630 01:33:04.723738 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:33:04.723743 21943 solver.cpp:537]     Test net output #2: loss = 0.211 (* 1 = 0.211 loss)
I0630 01:33:04.743407 21943 solver.cpp:290] Iteration 54000 (27.0289 iter/s, 3.69974s/100 iter), loss = -4.26173e-06
I0630 01:33:04.743427 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:04.743440 21943 sgd_solver.cpp:106] Iteration 54000, lr = 0.015625
I0630 01:33:06.817088 21943 solver.cpp:290] Iteration 54100 (48.2255 iter/s, 2.07359s/100 iter), loss = -4.26173e-06
I0630 01:33:06.817116 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:06.817124 21943 sgd_solver.cpp:106] Iteration 54100, lr = 0.0154688
I0630 01:33:08.902128 21943 solver.cpp:290] Iteration 54200 (47.9628 iter/s, 2.08495s/100 iter), loss = -4.26173e-06
I0630 01:33:08.902151 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:08.902158 21943 sgd_solver.cpp:106] Iteration 54200, lr = 0.0153125
I0630 01:33:10.958719 21943 solver.cpp:290] Iteration 54300 (48.6263 iter/s, 2.0565s/100 iter), loss = -4.26173e-06
I0630 01:33:10.958742 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:10.958750 21943 sgd_solver.cpp:106] Iteration 54300, lr = 0.0151563
I0630 01:33:13.008390 21943 solver.cpp:290] Iteration 54400 (48.7904 iter/s, 2.04958s/100 iter), loss = -4.26173e-06
I0630 01:33:13.008414 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:13.008420 21943 sgd_solver.cpp:106] Iteration 54400, lr = 0.015
I0630 01:33:15.060799 21943 solver.cpp:290] Iteration 54500 (48.7253 iter/s, 2.05232s/100 iter), loss = -4.26173e-06
I0630 01:33:15.060822 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:15.060828 21943 sgd_solver.cpp:106] Iteration 54500, lr = 0.0148437
I0630 01:33:17.114012 21943 solver.cpp:290] Iteration 54600 (48.7062 iter/s, 2.05312s/100 iter), loss = -4.26173e-06
I0630 01:33:17.114037 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:17.114045 21943 sgd_solver.cpp:106] Iteration 54600, lr = 0.0146875
I0630 01:33:19.165621 21943 solver.cpp:290] Iteration 54700 (48.7444 iter/s, 2.05152s/100 iter), loss = -4.26173e-06
I0630 01:33:19.165645 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:19.165653 21943 sgd_solver.cpp:106] Iteration 54700, lr = 0.0145312
I0630 01:33:21.221261 21943 solver.cpp:290] Iteration 54800 (48.6488 iter/s, 2.05555s/100 iter), loss = -4.26173e-06
I0630 01:33:21.221285 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:21.221292 21943 sgd_solver.cpp:106] Iteration 54800, lr = 0.014375
I0630 01:33:23.271085 21943 solver.cpp:290] Iteration 54900 (48.7868 iter/s, 2.04973s/100 iter), loss = -4.26173e-06
I0630 01:33:23.271118 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:23.271131 21943 sgd_solver.cpp:106] Iteration 54900, lr = 0.0142187
I0630 01:33:25.311848 21943 solver.cpp:464] Iteration 55000, Testing net (#0)
I0630 01:33:26.948956 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9158
I0630 01:33:26.948976 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:33:26.948982 21943 solver.cpp:537]     Test net output #2: loss = 0.2116 (* 1 = 0.2116 loss)
I0630 01:33:26.968760 21943 solver.cpp:290] Iteration 55000 (27.0451 iter/s, 3.69753s/100 iter), loss = -4.26173e-06
I0630 01:33:26.968782 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:26.968789 21943 sgd_solver.cpp:106] Iteration 55000, lr = 0.0140625
I0630 01:33:29.024684 21943 solver.cpp:290] Iteration 55100 (48.642 iter/s, 2.05584s/100 iter), loss = -4.26173e-06
I0630 01:33:29.024724 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:29.024731 21943 sgd_solver.cpp:106] Iteration 55100, lr = 0.0139063
I0630 01:33:31.075837 21943 solver.cpp:290] Iteration 55200 (48.7555 iter/s, 2.05105s/100 iter), loss = -4.26173e-06
I0630 01:33:31.075917 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:31.075925 21943 sgd_solver.cpp:106] Iteration 55200, lr = 0.01375
I0630 01:33:33.130549 21943 solver.cpp:290] Iteration 55300 (48.672 iter/s, 2.05457s/100 iter), loss = -4.26173e-06
I0630 01:33:33.130573 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:33.130583 21943 sgd_solver.cpp:106] Iteration 55300, lr = 0.0135938
I0630 01:33:35.181834 21943 solver.cpp:290] Iteration 55400 (48.752 iter/s, 2.0512s/100 iter), loss = -4.26173e-06
I0630 01:33:35.181857 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:35.181864 21943 sgd_solver.cpp:106] Iteration 55400, lr = 0.0134375
I0630 01:33:37.236735 21943 solver.cpp:290] Iteration 55500 (48.6662 iter/s, 2.05481s/100 iter), loss = -4.26173e-06
I0630 01:33:37.236758 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:37.236764 21943 sgd_solver.cpp:106] Iteration 55500, lr = 0.0132813
I0630 01:33:39.288444 21943 solver.cpp:290] Iteration 55600 (48.742 iter/s, 2.05162s/100 iter), loss = -4.26173e-06
I0630 01:33:39.288466 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:39.288475 21943 sgd_solver.cpp:106] Iteration 55600, lr = 0.013125
I0630 01:33:41.340545 21943 solver.cpp:290] Iteration 55700 (48.7326 iter/s, 2.05201s/100 iter), loss = -4.26173e-06
I0630 01:33:41.340569 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:41.340575 21943 sgd_solver.cpp:106] Iteration 55700, lr = 0.0129687
I0630 01:33:43.399647 21943 solver.cpp:290] Iteration 55800 (48.567 iter/s, 2.05901s/100 iter), loss = -4.26173e-06
I0630 01:33:43.399673 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:43.399682 21943 sgd_solver.cpp:106] Iteration 55800, lr = 0.0128125
I0630 01:33:45.455348 21943 solver.cpp:290] Iteration 55900 (48.6474 iter/s, 2.05561s/100 iter), loss = -4.26173e-06
I0630 01:33:45.455371 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:45.455377 21943 sgd_solver.cpp:106] Iteration 55900, lr = 0.0126562
I0630 01:33:47.489157 21943 solver.cpp:464] Iteration 56000, Testing net (#0)
I0630 01:33:49.130538 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9177
I0630 01:33:49.130558 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:33:49.130563 21943 solver.cpp:537]     Test net output #2: loss = 0.2081 (* 1 = 0.2081 loss)
I0630 01:33:49.154475 21943 solver.cpp:290] Iteration 56000 (27.0344 iter/s, 3.699s/100 iter), loss = -4.26173e-06
I0630 01:33:49.154495 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:49.154507 21943 sgd_solver.cpp:106] Iteration 56000, lr = 0.0125
I0630 01:33:51.205724 21943 solver.cpp:290] Iteration 56100 (48.7528 iter/s, 2.05116s/100 iter), loss = -4.26173e-06
I0630 01:33:51.205747 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:51.205756 21943 sgd_solver.cpp:106] Iteration 56100, lr = 0.0123438
I0630 01:33:53.256434 21943 solver.cpp:290] Iteration 56200 (48.7657 iter/s, 2.05062s/100 iter), loss = -4.26173e-06
I0630 01:33:53.256458 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:53.256464 21943 sgd_solver.cpp:106] Iteration 56200, lr = 0.0121875
I0630 01:33:55.316069 21943 solver.cpp:290] Iteration 56300 (48.5545 iter/s, 2.05954s/100 iter), loss = -4.26173e-06
I0630 01:33:55.316097 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:55.316105 21943 sgd_solver.cpp:106] Iteration 56300, lr = 0.0120313
I0630 01:33:57.367066 21943 solver.cpp:290] Iteration 56400 (48.759 iter/s, 2.0509s/100 iter), loss = -4.26173e-06
I0630 01:33:57.367099 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:57.367108 21943 sgd_solver.cpp:106] Iteration 56400, lr = 0.011875
I0630 01:33:59.418572 21943 solver.cpp:290] Iteration 56500 (48.7469 iter/s, 2.05141s/100 iter), loss = -4.26173e-06
I0630 01:33:59.418612 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:33:59.418619 21943 sgd_solver.cpp:106] Iteration 56500, lr = 0.0117188
I0630 01:34:01.479246 21943 solver.cpp:290] Iteration 56600 (48.5303 iter/s, 2.06057s/100 iter), loss = -4.26173e-06
I0630 01:34:01.479344 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:01.479352 21943 sgd_solver.cpp:106] Iteration 56600, lr = 0.0115625
I0630 01:34:03.530936 21943 solver.cpp:290] Iteration 56700 (48.7441 iter/s, 2.05153s/100 iter), loss = -4.26173e-06
I0630 01:34:03.530958 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:03.530964 21943 sgd_solver.cpp:106] Iteration 56700, lr = 0.0114062
I0630 01:34:05.584100 21943 solver.cpp:290] Iteration 56800 (48.7075 iter/s, 2.05307s/100 iter), loss = -4.26173e-06
I0630 01:34:05.584125 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:05.584133 21943 sgd_solver.cpp:106] Iteration 56800, lr = 0.01125
I0630 01:34:07.647482 21943 solver.cpp:290] Iteration 56900 (48.4662 iter/s, 2.06329s/100 iter), loss = -4.26173e-06
I0630 01:34:07.647505 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:07.647511 21943 sgd_solver.cpp:106] Iteration 56900, lr = 0.0110937
I0630 01:34:09.685298 21943 solver.cpp:464] Iteration 57000, Testing net (#0)
I0630 01:34:11.330739 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9174
I0630 01:34:11.330759 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9972
I0630 01:34:11.330763 21943 solver.cpp:537]     Test net output #2: loss = 0.2106 (* 1 = 0.2106 loss)
I0630 01:34:11.351016 21943 solver.cpp:290] Iteration 57000 (27.0022 iter/s, 3.7034s/100 iter), loss = -4.26173e-06
I0630 01:34:11.351035 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:11.351045 21943 sgd_solver.cpp:106] Iteration 57000, lr = 0.0109375
I0630 01:34:13.407975 21943 solver.cpp:290] Iteration 57100 (48.6175 iter/s, 2.05687s/100 iter), loss = -4.26173e-06
I0630 01:34:13.407999 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:13.408004 21943 sgd_solver.cpp:106] Iteration 57100, lr = 0.0107813
I0630 01:34:15.465040 21943 solver.cpp:290] Iteration 57200 (48.615 iter/s, 2.05698s/100 iter), loss = -4.26173e-06
I0630 01:34:15.465064 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:15.465070 21943 sgd_solver.cpp:106] Iteration 57200, lr = 0.010625
I0630 01:34:17.522936 21943 solver.cpp:290] Iteration 57300 (48.5955 iter/s, 2.0578s/100 iter), loss = -4.26173e-06
I0630 01:34:17.522969 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:17.522977 21943 sgd_solver.cpp:106] Iteration 57300, lr = 0.0104688
I0630 01:34:19.589577 21943 solver.cpp:290] Iteration 57400 (48.3899 iter/s, 2.06654s/100 iter), loss = -4.26173e-06
I0630 01:34:19.589607 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:19.589612 21943 sgd_solver.cpp:106] Iteration 57400, lr = 0.0103125
I0630 01:34:21.654008 21943 solver.cpp:290] Iteration 57500 (48.4417 iter/s, 2.06434s/100 iter), loss = -4.26173e-06
I0630 01:34:21.654032 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:21.654038 21943 sgd_solver.cpp:106] Iteration 57500, lr = 0.0101563
I0630 01:34:23.703524 21943 solver.cpp:290] Iteration 57600 (48.7941 iter/s, 2.04943s/100 iter), loss = -4.26173e-06
I0630 01:34:23.703548 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:23.703554 21943 sgd_solver.cpp:106] Iteration 57600, lr = 0.01
I0630 01:34:25.758224 21943 solver.cpp:290] Iteration 57700 (48.671 iter/s, 2.05461s/100 iter), loss = -4.26173e-06
I0630 01:34:25.758247 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:25.758255 21943 sgd_solver.cpp:106] Iteration 57700, lr = 0.00984375
I0630 01:34:27.809115 21943 solver.cpp:290] Iteration 57800 (48.7614 iter/s, 2.0508s/100 iter), loss = -4.26173e-06
I0630 01:34:27.809139 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:27.809146 21943 sgd_solver.cpp:106] Iteration 57800, lr = 0.0096875
I0630 01:34:29.866794 21943 solver.cpp:290] Iteration 57900 (48.6005 iter/s, 2.05759s/100 iter), loss = -4.26173e-06
I0630 01:34:29.866837 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:29.866843 21943 sgd_solver.cpp:106] Iteration 57900, lr = 0.00953125
I0630 01:34:31.902083 21943 solver.cpp:464] Iteration 58000, Testing net (#0)
I0630 01:34:33.537364 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9183
I0630 01:34:33.537382 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9974
I0630 01:34:33.537389 21943 solver.cpp:537]     Test net output #2: loss = 0.2112 (* 1 = 0.2112 loss)
I0630 01:34:33.557552 21943 solver.cpp:290] Iteration 58000 (27.0958 iter/s, 3.69061s/100 iter), loss = -4.26173e-06
I0630 01:34:33.557569 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:33.557584 21943 sgd_solver.cpp:106] Iteration 58000, lr = 0.009375
I0630 01:34:35.611704 21943 solver.cpp:290] Iteration 58100 (48.6839 iter/s, 2.05407s/100 iter), loss = -4.26173e-06
I0630 01:34:35.611726 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:35.611734 21943 sgd_solver.cpp:106] Iteration 58100, lr = 0.00921875
I0630 01:34:37.665779 21943 solver.cpp:290] Iteration 58200 (48.6858 iter/s, 2.05399s/100 iter), loss = -4.26173e-06
I0630 01:34:37.665802 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:37.665807 21943 sgd_solver.cpp:106] Iteration 58200, lr = 0.0090625
I0630 01:34:39.719736 21943 solver.cpp:290] Iteration 58300 (48.6886 iter/s, 2.05387s/100 iter), loss = -4.26173e-06
I0630 01:34:39.719758 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:39.719766 21943 sgd_solver.cpp:106] Iteration 58300, lr = 0.00890625
I0630 01:34:41.773797 21943 solver.cpp:290] Iteration 58400 (48.6861 iter/s, 2.05397s/100 iter), loss = -4.26173e-06
I0630 01:34:41.773819 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:41.773825 21943 sgd_solver.cpp:106] Iteration 58400, lr = 0.00875
I0630 01:34:43.828018 21943 solver.cpp:290] Iteration 58500 (48.6823 iter/s, 2.05413s/100 iter), loss = -4.26173e-06
I0630 01:34:43.828040 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:43.828047 21943 sgd_solver.cpp:106] Iteration 58500, lr = 0.00859375
I0630 01:34:45.881337 21943 solver.cpp:290] Iteration 58600 (48.7037 iter/s, 2.05323s/100 iter), loss = -4.26173e-06
I0630 01:34:45.881361 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:45.881367 21943 sgd_solver.cpp:106] Iteration 58600, lr = 0.0084375
I0630 01:34:47.935912 21943 solver.cpp:290] Iteration 58700 (48.674 iter/s, 2.05449s/100 iter), loss = -4.26173e-06
I0630 01:34:47.935935 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:47.935942 21943 sgd_solver.cpp:106] Iteration 58700, lr = 0.00828125
I0630 01:34:49.990936 21943 solver.cpp:290] Iteration 58800 (48.6634 iter/s, 2.05493s/100 iter), loss = -4.26173e-06
I0630 01:34:49.990960 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:49.990969 21943 sgd_solver.cpp:106] Iteration 58800, lr = 0.008125
I0630 01:34:52.046563 21943 solver.cpp:290] Iteration 58900 (48.649 iter/s, 2.05554s/100 iter), loss = -4.26173e-06
I0630 01:34:52.046586 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:52.046591 21943 sgd_solver.cpp:106] Iteration 58900, lr = 0.00796875
I0630 01:34:54.077917 21943 solver.cpp:464] Iteration 59000, Testing net (#0)
I0630 01:34:55.714668 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.919
I0630 01:34:55.714686 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:34:55.714694 21943 solver.cpp:537]     Test net output #2: loss = 0.2075 (* 1 = 0.2075 loss)
I0630 01:34:55.735152 21943 solver.cpp:290] Iteration 59000 (27.1116 iter/s, 3.68846s/100 iter), loss = -4.26173e-06
I0630 01:34:55.735172 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:55.735184 21943 sgd_solver.cpp:106] Iteration 59000, lr = 0.0078125
I0630 01:34:57.796497 21943 solver.cpp:290] Iteration 59100 (48.5141 iter/s, 2.06126s/100 iter), loss = -4.26173e-06
I0630 01:34:57.796524 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:57.796532 21943 sgd_solver.cpp:106] Iteration 59100, lr = 0.00765625
I0630 01:34:59.855350 21943 solver.cpp:290] Iteration 59200 (48.5729 iter/s, 2.05876s/100 iter), loss = -4.26173e-06
I0630 01:34:59.855372 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:34:59.855378 21943 sgd_solver.cpp:106] Iteration 59200, lr = 0.0075
I0630 01:35:01.906664 21943 solver.cpp:290] Iteration 59300 (48.7513 iter/s, 2.05123s/100 iter), loss = -4.26173e-06
I0630 01:35:01.906718 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:01.906728 21943 sgd_solver.cpp:106] Iteration 59300, lr = 0.00734375
I0630 01:35:03.957137 21943 solver.cpp:290] Iteration 59400 (48.772 iter/s, 2.05036s/100 iter), loss = -4.26173e-06
I0630 01:35:03.957160 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:03.957167 21943 sgd_solver.cpp:106] Iteration 59400, lr = 0.0071875
I0630 01:35:06.005939 21943 solver.cpp:290] Iteration 59500 (48.8111 iter/s, 2.04871s/100 iter), loss = -4.26173e-06
I0630 01:35:06.005961 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:06.005967 21943 sgd_solver.cpp:106] Iteration 59500, lr = 0.00703125
I0630 01:35:08.062912 21943 solver.cpp:290] Iteration 59600 (48.6172 iter/s, 2.05689s/100 iter), loss = -4.26173e-06
I0630 01:35:08.062934 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:08.062942 21943 sgd_solver.cpp:106] Iteration 59600, lr = 0.006875
I0630 01:35:10.120506 21943 solver.cpp:290] Iteration 59700 (48.6025 iter/s, 2.05751s/100 iter), loss = -4.26173e-06
I0630 01:35:10.120528 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:10.120534 21943 sgd_solver.cpp:106] Iteration 59700, lr = 0.00671875
I0630 01:35:12.172945 21943 solver.cpp:290] Iteration 59800 (48.7246 iter/s, 2.05235s/100 iter), loss = -4.26173e-06
I0630 01:35:12.172966 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:12.172972 21943 sgd_solver.cpp:106] Iteration 59800, lr = 0.0065625
I0630 01:35:14.224275 21943 solver.cpp:290] Iteration 59900 (48.751 iter/s, 2.05124s/100 iter), loss = -4.26173e-06
I0630 01:35:14.224303 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:14.224308 21943 sgd_solver.cpp:106] Iteration 59900, lr = 0.00640625
I0630 01:35:16.254657 21943 solver.cpp:591] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2_iter_60000.caffemodel
I0630 01:35:16.270949 21943 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2_iter_60000.solverstate
I0630 01:35:16.278260 21943 solver.cpp:464] Iteration 60000, Testing net (#0)
I0630 01:35:17.915169 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9179
I0630 01:35:17.915189 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9974
I0630 01:35:17.915194 21943 solver.cpp:537]     Test net output #2: loss = 0.2045 (* 1 = 0.2045 loss)
I0630 01:35:17.936698 21943 solver.cpp:290] Iteration 60000 (26.9376 iter/s, 3.71229s/100 iter), loss = -4.26173e-06
I0630 01:35:17.936720 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:17.936743 21943 sgd_solver.cpp:106] Iteration 60000, lr = 0.00625
I0630 01:35:19.997462 21943 solver.cpp:290] Iteration 60100 (48.5277 iter/s, 2.06068s/100 iter), loss = -4.26173e-06
I0630 01:35:19.997484 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:19.997490 21943 sgd_solver.cpp:106] Iteration 60100, lr = 0.00609375
I0630 01:35:22.050889 21943 solver.cpp:290] Iteration 60200 (48.7011 iter/s, 2.05334s/100 iter), loss = -4.26173e-06
I0630 01:35:22.050911 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:22.050918 21943 sgd_solver.cpp:106] Iteration 60200, lr = 0.0059375
I0630 01:35:24.103189 21943 solver.cpp:290] Iteration 60300 (48.7279 iter/s, 2.05221s/100 iter), loss = -4.26173e-06
I0630 01:35:24.103211 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:24.103219 21943 sgd_solver.cpp:106] Iteration 60300, lr = 0.00578125
I0630 01:35:26.153650 21943 solver.cpp:290] Iteration 60400 (48.7716 iter/s, 2.05037s/100 iter), loss = -4.26173e-06
I0630 01:35:26.153673 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:26.153681 21943 sgd_solver.cpp:106] Iteration 60400, lr = 0.005625
I0630 01:35:28.210486 21943 solver.cpp:290] Iteration 60500 (48.6205 iter/s, 2.05674s/100 iter), loss = -4.26173e-06
I0630 01:35:28.210512 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:28.210521 21943 sgd_solver.cpp:106] Iteration 60500, lr = 0.00546875
I0630 01:35:30.262567 21943 solver.cpp:290] Iteration 60600 (48.7331 iter/s, 2.05199s/100 iter), loss = -4.26173e-06
I0630 01:35:30.262593 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:30.262601 21943 sgd_solver.cpp:106] Iteration 60600, lr = 0.0053125
I0630 01:35:32.312549 21943 solver.cpp:290] Iteration 60700 (48.783 iter/s, 2.04989s/100 iter), loss = -4.26173e-06
I0630 01:35:32.312628 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:32.312636 21943 sgd_solver.cpp:106] Iteration 60700, lr = 0.00515625
I0630 01:35:34.367456 21943 solver.cpp:290] Iteration 60800 (48.6674 iter/s, 2.05476s/100 iter), loss = -4.26173e-06
I0630 01:35:34.367480 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:34.367486 21943 sgd_solver.cpp:106] Iteration 60800, lr = 0.005
I0630 01:35:36.420188 21943 solver.cpp:290] Iteration 60900 (48.7176 iter/s, 2.05264s/100 iter), loss = -4.26173e-06
I0630 01:35:36.420212 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:36.420217 21943 sgd_solver.cpp:106] Iteration 60900, lr = 0.00484375
I0630 01:35:38.453255 21943 solver.cpp:464] Iteration 61000, Testing net (#0)
I0630 01:35:40.090849 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9179
I0630 01:35:40.090867 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9974
I0630 01:35:40.090873 21943 solver.cpp:537]     Test net output #2: loss = 0.2079 (* 1 = 0.2079 loss)
I0630 01:35:40.111110 21943 solver.cpp:290] Iteration 61000 (27.0945 iter/s, 3.69079s/100 iter), loss = -4.26173e-06
I0630 01:35:40.111129 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:40.111140 21943 sgd_solver.cpp:106] Iteration 61000, lr = 0.0046875
I0630 01:35:42.166803 21943 solver.cpp:290] Iteration 61100 (48.6474 iter/s, 2.05561s/100 iter), loss = -4.26173e-06
I0630 01:35:42.166826 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:42.166839 21943 sgd_solver.cpp:106] Iteration 61100, lr = 0.00453125
I0630 01:35:44.220352 21943 solver.cpp:290] Iteration 61200 (48.6983 iter/s, 2.05346s/100 iter), loss = -4.26173e-06
I0630 01:35:44.220376 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:44.220382 21943 sgd_solver.cpp:106] Iteration 61200, lr = 0.004375
I0630 01:35:46.269399 21943 solver.cpp:290] Iteration 61300 (48.8054 iter/s, 2.04896s/100 iter), loss = -4.26173e-06
I0630 01:35:46.269471 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:46.269495 21943 sgd_solver.cpp:106] Iteration 61300, lr = 0.00421875
I0630 01:35:48.323251 21943 solver.cpp:290] Iteration 61400 (48.6923 iter/s, 2.05371s/100 iter), loss = -4.26173e-06
I0630 01:35:48.323281 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:48.323290 21943 sgd_solver.cpp:106] Iteration 61400, lr = 0.0040625
I0630 01:35:50.377840 21943 solver.cpp:290] Iteration 61500 (48.6738 iter/s, 2.0545s/100 iter), loss = -4.26173e-06
I0630 01:35:50.377862 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:50.377868 21943 sgd_solver.cpp:106] Iteration 61500, lr = 0.00390625
I0630 01:35:52.426306 21943 solver.cpp:290] Iteration 61600 (48.8192 iter/s, 2.04838s/100 iter), loss = -4.26173e-06
I0630 01:35:52.426332 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:52.426340 21943 sgd_solver.cpp:106] Iteration 61600, lr = 0.00375
I0630 01:35:54.477375 21943 solver.cpp:290] Iteration 61700 (48.7573 iter/s, 2.05098s/100 iter), loss = -4.26173e-06
I0630 01:35:54.477399 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:54.477408 21943 sgd_solver.cpp:106] Iteration 61700, lr = 0.00359375
I0630 01:35:56.528918 21943 solver.cpp:290] Iteration 61800 (48.7459 iter/s, 2.05145s/100 iter), loss = -4.26173e-06
I0630 01:35:56.528940 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:56.528949 21943 sgd_solver.cpp:106] Iteration 61800, lr = 0.0034375
I0630 01:35:58.580590 21943 solver.cpp:290] Iteration 61900 (48.7428 iter/s, 2.05159s/100 iter), loss = -4.26173e-06
I0630 01:35:58.580613 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:35:58.580621 21943 sgd_solver.cpp:106] Iteration 61900, lr = 0.00328125
I0630 01:36:00.612603 21943 solver.cpp:464] Iteration 62000, Testing net (#0)
I0630 01:36:02.255901 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9181
I0630 01:36:02.255939 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9972
I0630 01:36:02.255944 21943 solver.cpp:537]     Test net output #2: loss = 0.2061 (* 1 = 0.2061 loss)
I0630 01:36:02.275612 21943 solver.cpp:290] Iteration 62000 (27.0644 iter/s, 3.69489s/100 iter), loss = -4.26173e-06
I0630 01:36:02.275631 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:02.275645 21943 sgd_solver.cpp:106] Iteration 62000, lr = 0.003125
I0630 01:36:04.333250 21943 solver.cpp:290] Iteration 62100 (48.6014 iter/s, 2.05755s/100 iter), loss = -4.26173e-06
I0630 01:36:04.333302 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:04.333312 21943 sgd_solver.cpp:106] Iteration 62100, lr = 0.00296875
I0630 01:36:06.389189 21943 solver.cpp:290] Iteration 62200 (48.6423 iter/s, 2.05582s/100 iter), loss = -4.26173e-06
I0630 01:36:06.389212 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:06.389219 21943 sgd_solver.cpp:106] Iteration 62200, lr = 0.0028125
I0630 01:36:08.442334 21943 solver.cpp:290] Iteration 62300 (48.7079 iter/s, 2.05306s/100 iter), loss = -4.26173e-06
I0630 01:36:08.442358 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:08.442363 21943 sgd_solver.cpp:106] Iteration 62300, lr = 0.00265625
I0630 01:36:10.496855 21943 solver.cpp:290] Iteration 62400 (48.6753 iter/s, 2.05443s/100 iter), loss = -4.26173e-06
I0630 01:36:10.496876 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:10.496883 21943 sgd_solver.cpp:106] Iteration 62400, lr = 0.0025
I0630 01:36:12.548190 21943 solver.cpp:290] Iteration 62500 (48.7508 iter/s, 2.05125s/100 iter), loss = -4.26173e-06
I0630 01:36:12.548213 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:12.548221 21943 sgd_solver.cpp:106] Iteration 62500, lr = 0.00234375
I0630 01:36:14.602748 21943 solver.cpp:290] Iteration 62600 (48.6744 iter/s, 2.05447s/100 iter), loss = -4.26173e-06
I0630 01:36:14.602774 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:14.602783 21943 sgd_solver.cpp:106] Iteration 62600, lr = 0.0021875
I0630 01:36:16.652550 21943 solver.cpp:290] Iteration 62700 (48.7873 iter/s, 2.04971s/100 iter), loss = -4.26173e-06
I0630 01:36:16.652571 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:16.652580 21943 sgd_solver.cpp:106] Iteration 62700, lr = 0.00203125
I0630 01:36:18.705863 21943 solver.cpp:290] Iteration 62800 (48.7038 iter/s, 2.05323s/100 iter), loss = -4.26173e-06
I0630 01:36:18.705886 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:18.705893 21943 sgd_solver.cpp:106] Iteration 62800, lr = 0.001875
I0630 01:36:20.762442 21943 solver.cpp:290] Iteration 62900 (48.6265 iter/s, 2.05649s/100 iter), loss = -4.26173e-06
I0630 01:36:20.762465 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:20.762472 21943 sgd_solver.cpp:106] Iteration 62900, lr = 0.00171875
I0630 01:36:22.796241 21943 solver.cpp:464] Iteration 63000, Testing net (#0)
I0630 01:36:24.433061 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9179
I0630 01:36:24.433080 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:36:24.433086 21943 solver.cpp:537]     Test net output #2: loss = 0.2064 (* 1 = 0.2064 loss)
I0630 01:36:24.452863 21943 solver.cpp:290] Iteration 63000 (27.0982 iter/s, 3.69029s/100 iter), loss = -4.26173e-06
I0630 01:36:24.452883 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:24.452896 21943 sgd_solver.cpp:106] Iteration 63000, lr = 0.0015625
I0630 01:36:26.505295 21943 solver.cpp:290] Iteration 63100 (48.7247 iter/s, 2.05235s/100 iter), loss = -4.26173e-06
I0630 01:36:26.505318 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:26.505326 21943 sgd_solver.cpp:106] Iteration 63100, lr = 0.00140625
I0630 01:36:28.559967 21943 solver.cpp:290] Iteration 63200 (48.6716 iter/s, 2.05458s/100 iter), loss = -4.26173e-06
I0630 01:36:28.559989 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:28.559996 21943 sgd_solver.cpp:106] Iteration 63200, lr = 0.00125
I0630 01:36:30.611389 21943 solver.cpp:290] Iteration 63300 (48.7487 iter/s, 2.05134s/100 iter), loss = -4.26173e-06
I0630 01:36:30.611413 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:30.611419 21943 sgd_solver.cpp:106] Iteration 63300, lr = 0.00109375
I0630 01:36:32.666685 21943 solver.cpp:290] Iteration 63400 (48.6569 iter/s, 2.05521s/100 iter), loss = -4.26173e-06
I0630 01:36:32.666724 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:32.666730 21943 sgd_solver.cpp:106] Iteration 63400, lr = 0.000937498
I0630 01:36:34.718902 21943 solver.cpp:290] Iteration 63500 (48.7302 iter/s, 2.05211s/100 iter), loss = -4.26173e-06
I0630 01:36:34.718961 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:34.718971 21943 sgd_solver.cpp:106] Iteration 63500, lr = 0.00078125
I0630 01:36:36.773066 21943 solver.cpp:290] Iteration 63600 (48.6845 iter/s, 2.05404s/100 iter), loss = -4.26173e-06
I0630 01:36:36.773087 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:36.773094 21943 sgd_solver.cpp:106] Iteration 63600, lr = 0.000625002
I0630 01:36:38.824342 21943 solver.cpp:290] Iteration 63700 (48.7522 iter/s, 2.05119s/100 iter), loss = -4.26173e-06
I0630 01:36:38.824364 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:38.824373 21943 sgd_solver.cpp:106] Iteration 63700, lr = 0.000468749
I0630 01:36:40.877444 21943 solver.cpp:290] Iteration 63800 (48.7089 iter/s, 2.05301s/100 iter), loss = -4.26173e-06
I0630 01:36:40.877470 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:40.877478 21943 sgd_solver.cpp:106] Iteration 63800, lr = 0.000312501
I0630 01:36:42.931140 21943 solver.cpp:290] Iteration 63900 (48.6948 iter/s, 2.05361s/100 iter), loss = -4.26173e-06
I0630 01:36:42.931164 21943 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:42.931170 21943 sgd_solver.cpp:106] Iteration 63900, lr = 0.000156248
I0630 01:36:44.961344 21943 solver.cpp:591] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2_iter_64000.caffemodel
I0630 01:36:44.977663 21943 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2_iter_64000.solverstate
I0630 01:36:44.989564 21943 solver.cpp:444] Iteration 64000, loss = -4.26173e-06
I0630 01:36:44.989584 21943 solver.cpp:464] Iteration 64000, Testing net (#0)
I0630 01:36:46.626811 21943 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.917
I0630 01:36:46.626829 21943 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9974
I0630 01:36:46.626837 21943 solver.cpp:537]     Test net output #2: loss = 0.2069 (* 1 = 0.2069 loss)
I0630 01:36:46.626840 21943 solver.cpp:449] Optimization Done.
I0630 01:36:46.673257 21943 caffe.cpp:246] Optimization Done.
training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse
WARNING: gnome-keyring:: couldn't connect to: /run/user/30409/keyring-KJvviu/pkcs11: Connection refused
p11-kit: skipping module 'gnome-keyring' whose initialization failed: An error occurred on the device
I0630 01:36:47.842895 19877 caffe.cpp:209] Using GPUs 0, 1, 2
I0630 01:36:47.843366 19877 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0630 01:36:47.843693 19877 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0630 01:36:47.844017 19877 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0630 01:36:48.229328 19877 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: true
iter_size: 1
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.02
sparsity_step_iter: 1000
sparsity_start_iter: 0
sparsity_start_factor: 0
I0630 01:36:48.229832 19877 solver.cpp:82] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/train.prototxt
I0630 01:36:48.230290 19877 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0630 01:36:48.230295 19877 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0630 01:36:48.230450 19877 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 21
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0630 01:36:48.230535 19877 layer_factory.hpp:77] Creating layer data
I0630 01:36:48.230614 19877 net.cpp:98] Creating Layer data
I0630 01:36:48.230619 19877 net.cpp:413] data -> data
I0630 01:36:48.230635 19877 net.cpp:413] data -> label
I0630 01:36:48.231442 19915 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0630 01:36:48.232427 19877 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0630 01:36:48.232481 19877 data_layer.cpp:83] output data size: 21,3,32,32
I0630 01:36:48.234097 19877 net.cpp:148] Setting up data
I0630 01:36:48.234110 19877 net.cpp:155] Top shape: 21 3 32 32 (64512)
I0630 01:36:48.234113 19877 net.cpp:155] Top shape: 21 (21)
I0630 01:36:48.234117 19877 net.cpp:163] Memory required for data: 258132
I0630 01:36:48.234123 19877 layer_factory.hpp:77] Creating layer data/bias
I0630 01:36:48.234133 19877 net.cpp:98] Creating Layer data/bias
I0630 01:36:48.234136 19877 net.cpp:439] data/bias <- data
I0630 01:36:48.234146 19877 net.cpp:413] data/bias -> data/bias
I0630 01:36:48.235149 19877 net.cpp:148] Setting up data/bias
I0630 01:36:48.235160 19877 net.cpp:155] Top shape: 21 3 32 32 (64512)
I0630 01:36:48.235164 19877 net.cpp:163] Memory required for data: 516180
I0630 01:36:48.235175 19877 layer_factory.hpp:77] Creating layer conv1a
I0630 01:36:48.235188 19877 net.cpp:98] Creating Layer conv1a
I0630 01:36:48.235191 19877 net.cpp:439] conv1a <- data/bias
I0630 01:36:48.235196 19877 net.cpp:413] conv1a -> conv1a
I0630 01:36:48.235447 19919 blocking_queue.cpp:50] Waiting for data
I0630 01:36:48.236551 19877 net.cpp:148] Setting up conv1a
I0630 01:36:48.236562 19877 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:36:48.236564 19877 net.cpp:163] Memory required for data: 3268692
I0630 01:36:48.236572 19877 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 01:36:48.236582 19877 net.cpp:98] Creating Layer conv1a/bn
I0630 01:36:48.236587 19877 net.cpp:439] conv1a/bn <- conv1a
I0630 01:36:48.236591 19877 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 01:36:48.237248 19877 net.cpp:148] Setting up conv1a/bn
I0630 01:36:48.237258 19877 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:36:48.237263 19877 net.cpp:163] Memory required for data: 6021204
I0630 01:36:48.237272 19877 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 01:36:48.237277 19877 net.cpp:98] Creating Layer conv1a/relu
I0630 01:36:48.237282 19877 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 01:36:48.237287 19877 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 01:36:48.237301 19877 net.cpp:148] Setting up conv1a/relu
I0630 01:36:48.237305 19877 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:36:48.237316 19877 net.cpp:163] Memory required for data: 8773716
I0630 01:36:48.237320 19877 layer_factory.hpp:77] Creating layer conv1b
I0630 01:36:48.237327 19877 net.cpp:98] Creating Layer conv1b
I0630 01:36:48.237331 19877 net.cpp:439] conv1b <- conv1a/bn
I0630 01:36:48.237335 19877 net.cpp:413] conv1b -> conv1b
I0630 01:36:48.237645 19877 net.cpp:148] Setting up conv1b
I0630 01:36:48.237651 19877 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:36:48.237655 19877 net.cpp:163] Memory required for data: 11526228
I0630 01:36:48.237663 19877 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 01:36:48.237668 19877 net.cpp:98] Creating Layer conv1b/bn
I0630 01:36:48.237671 19877 net.cpp:439] conv1b/bn <- conv1b
I0630 01:36:48.237678 19877 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 01:36:48.238310 19877 net.cpp:148] Setting up conv1b/bn
I0630 01:36:48.238317 19877 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:36:48.238320 19877 net.cpp:163] Memory required for data: 14278740
I0630 01:36:48.238329 19877 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 01:36:48.238333 19877 net.cpp:98] Creating Layer conv1b/relu
I0630 01:36:48.238337 19877 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 01:36:48.238343 19877 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 01:36:48.238348 19877 net.cpp:148] Setting up conv1b/relu
I0630 01:36:48.238353 19877 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:36:48.238356 19877 net.cpp:163] Memory required for data: 17031252
I0630 01:36:48.238360 19877 layer_factory.hpp:77] Creating layer pool1
I0630 01:36:48.238368 19877 net.cpp:98] Creating Layer pool1
I0630 01:36:48.238370 19877 net.cpp:439] pool1 <- conv1b/bn
I0630 01:36:48.238374 19877 net.cpp:413] pool1 -> pool1
I0630 01:36:48.238418 19877 net.cpp:148] Setting up pool1
I0630 01:36:48.238423 19877 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:36:48.238427 19877 net.cpp:163] Memory required for data: 19783764
I0630 01:36:48.238431 19877 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 01:36:48.238437 19877 net.cpp:98] Creating Layer res2a_branch2a
I0630 01:36:48.238441 19877 net.cpp:439] res2a_branch2a <- pool1
I0630 01:36:48.238446 19877 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 01:36:48.239084 19877 net.cpp:148] Setting up res2a_branch2a
I0630 01:36:48.239091 19877 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:36:48.239094 19877 net.cpp:163] Memory required for data: 25288788
I0630 01:36:48.239101 19877 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 01:36:48.239107 19877 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 01:36:48.239111 19877 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 01:36:48.239116 19877 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 01:36:48.239737 19877 net.cpp:148] Setting up res2a_branch2a/bn
I0630 01:36:48.239744 19877 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:36:48.239748 19877 net.cpp:163] Memory required for data: 30793812
I0630 01:36:48.239755 19877 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 01:36:48.239759 19877 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 01:36:48.239764 19877 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 01:36:48.239769 19877 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 01:36:48.239775 19877 net.cpp:148] Setting up res2a_branch2a/relu
I0630 01:36:48.239780 19877 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:36:48.239784 19877 net.cpp:163] Memory required for data: 36298836
I0630 01:36:48.239786 19877 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 01:36:48.239794 19877 net.cpp:98] Creating Layer res2a_branch2b
I0630 01:36:48.239796 19877 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 01:36:48.239801 19877 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 01:36:48.241127 19877 net.cpp:148] Setting up res2a_branch2b
I0630 01:36:48.241139 19877 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:36:48.241142 19877 net.cpp:163] Memory required for data: 41803860
I0630 01:36:48.241158 19877 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 01:36:48.241164 19877 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 01:36:48.241168 19877 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 01:36:48.241175 19877 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 01:36:48.241811 19877 net.cpp:148] Setting up res2a_branch2b/bn
I0630 01:36:48.241818 19877 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:36:48.241822 19877 net.cpp:163] Memory required for data: 47308884
I0630 01:36:48.241830 19877 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 01:36:48.241835 19877 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 01:36:48.241839 19877 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 01:36:48.241844 19877 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 01:36:48.241853 19877 net.cpp:148] Setting up res2a_branch2b/relu
I0630 01:36:48.241858 19877 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:36:48.241861 19877 net.cpp:163] Memory required for data: 52813908
I0630 01:36:48.241865 19877 layer_factory.hpp:77] Creating layer pool2
I0630 01:36:48.241870 19877 net.cpp:98] Creating Layer pool2
I0630 01:36:48.241874 19877 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 01:36:48.241878 19877 net.cpp:413] pool2 -> pool2
I0630 01:36:48.241916 19877 net.cpp:148] Setting up pool2
I0630 01:36:48.241921 19877 net.cpp:155] Top shape: 21 64 16 16 (344064)
I0630 01:36:48.241925 19877 net.cpp:163] Memory required for data: 54190164
I0630 01:36:48.241928 19877 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 01:36:48.241937 19877 net.cpp:98] Creating Layer res3a_branch2a
I0630 01:36:48.241940 19877 net.cpp:439] res3a_branch2a <- pool2
I0630 01:36:48.241945 19877 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 01:36:48.244577 19877 net.cpp:148] Setting up res3a_branch2a
I0630 01:36:48.244590 19877 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:36:48.244595 19877 net.cpp:163] Memory required for data: 56942676
I0630 01:36:48.244601 19877 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 01:36:48.244608 19877 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 01:36:48.244613 19877 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 01:36:48.244619 19877 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 01:36:48.245201 19877 net.cpp:148] Setting up res3a_branch2a/bn
I0630 01:36:48.245208 19877 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:36:48.245211 19877 net.cpp:163] Memory required for data: 59695188
I0630 01:36:48.245224 19877 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 01:36:48.245229 19877 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 01:36:48.245232 19877 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 01:36:48.245237 19877 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 01:36:48.245244 19877 net.cpp:148] Setting up res3a_branch2a/relu
I0630 01:36:48.245249 19877 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:36:48.245252 19877 net.cpp:163] Memory required for data: 62447700
I0630 01:36:48.245255 19877 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 01:36:48.245262 19877 net.cpp:98] Creating Layer res3a_branch2b
I0630 01:36:48.245266 19877 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 01:36:48.245270 19877 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 01:36:48.246269 19877 net.cpp:148] Setting up res3a_branch2b
I0630 01:36:48.246276 19877 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:36:48.246280 19877 net.cpp:163] Memory required for data: 65200212
I0630 01:36:48.246286 19877 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 01:36:48.246292 19877 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 01:36:48.246296 19877 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 01:36:48.246300 19877 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 01:36:48.246882 19877 net.cpp:148] Setting up res3a_branch2b/bn
I0630 01:36:48.246889 19877 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:36:48.246901 19877 net.cpp:163] Memory required for data: 67952724
I0630 01:36:48.246909 19877 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 01:36:48.246913 19877 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 01:36:48.246917 19877 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 01:36:48.246922 19877 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 01:36:48.246928 19877 net.cpp:148] Setting up res3a_branch2b/relu
I0630 01:36:48.246932 19877 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:36:48.246937 19877 net.cpp:163] Memory required for data: 70705236
I0630 01:36:48.246940 19877 layer_factory.hpp:77] Creating layer pool3
I0630 01:36:48.246947 19877 net.cpp:98] Creating Layer pool3
I0630 01:36:48.246949 19877 net.cpp:439] pool3 <- res3a_branch2b/bn
I0630 01:36:48.246953 19877 net.cpp:413] pool3 -> pool3
I0630 01:36:48.246990 19877 net.cpp:148] Setting up pool3
I0630 01:36:48.246995 19877 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:36:48.246999 19877 net.cpp:163] Memory required for data: 73457748
I0630 01:36:48.247002 19877 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 01:36:48.247009 19877 net.cpp:98] Creating Layer res4a_branch2a
I0630 01:36:48.247012 19877 net.cpp:439] res4a_branch2a <- pool3
I0630 01:36:48.247017 19877 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 01:36:48.253106 19877 net.cpp:148] Setting up res4a_branch2a
I0630 01:36:48.253115 19877 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:36:48.253118 19877 net.cpp:163] Memory required for data: 78962772
I0630 01:36:48.253125 19877 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 01:36:48.253131 19877 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 01:36:48.253135 19877 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 01:36:48.253139 19877 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 01:36:48.253715 19877 net.cpp:148] Setting up res4a_branch2a/bn
I0630 01:36:48.253722 19877 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:36:48.253726 19877 net.cpp:163] Memory required for data: 84467796
I0630 01:36:48.253736 19877 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 01:36:48.253739 19877 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 01:36:48.253742 19877 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 01:36:48.253747 19877 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 01:36:48.253752 19877 net.cpp:148] Setting up res4a_branch2a/relu
I0630 01:36:48.253757 19877 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:36:48.253760 19877 net.cpp:163] Memory required for data: 89972820
I0630 01:36:48.253764 19877 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 01:36:48.253770 19877 net.cpp:98] Creating Layer res4a_branch2b
I0630 01:36:48.253774 19877 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 01:36:48.253778 19877 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 01:36:48.257081 19877 net.cpp:148] Setting up res4a_branch2b
I0630 01:36:48.257093 19877 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:36:48.257097 19877 net.cpp:163] Memory required for data: 95477844
I0630 01:36:48.257103 19877 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 01:36:48.257112 19877 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 01:36:48.257117 19877 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 01:36:48.257123 19877 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 01:36:48.257753 19877 net.cpp:148] Setting up res4a_branch2b/bn
I0630 01:36:48.257762 19877 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:36:48.257766 19877 net.cpp:163] Memory required for data: 100982868
I0630 01:36:48.257774 19877 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 01:36:48.257781 19877 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 01:36:48.257784 19877 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 01:36:48.257788 19877 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 01:36:48.257804 19877 net.cpp:148] Setting up res4a_branch2b/relu
I0630 01:36:48.257809 19877 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:36:48.257812 19877 net.cpp:163] Memory required for data: 106487892
I0630 01:36:48.257817 19877 layer_factory.hpp:77] Creating layer pool4
I0630 01:36:48.257824 19877 net.cpp:98] Creating Layer pool4
I0630 01:36:48.257827 19877 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 01:36:48.257832 19877 net.cpp:413] pool4 -> pool4
I0630 01:36:48.257869 19877 net.cpp:148] Setting up pool4
I0630 01:36:48.257874 19877 net.cpp:155] Top shape: 21 256 8 8 (344064)
I0630 01:36:48.257879 19877 net.cpp:163] Memory required for data: 107864148
I0630 01:36:48.257882 19877 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 01:36:48.257889 19877 net.cpp:98] Creating Layer res5a_branch2a
I0630 01:36:48.257892 19877 net.cpp:439] res5a_branch2a <- pool4
I0630 01:36:48.257897 19877 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 01:36:48.282958 19877 net.cpp:148] Setting up res5a_branch2a
I0630 01:36:48.282979 19877 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:36:48.282982 19877 net.cpp:163] Memory required for data: 110616660
I0630 01:36:48.282989 19877 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 01:36:48.282999 19877 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 01:36:48.283004 19877 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 01:36:48.283010 19877 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 01:36:48.283664 19877 net.cpp:148] Setting up res5a_branch2a/bn
I0630 01:36:48.283673 19877 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:36:48.283675 19877 net.cpp:163] Memory required for data: 113369172
I0630 01:36:48.283684 19877 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 01:36:48.283690 19877 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 01:36:48.283694 19877 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 01:36:48.283699 19877 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 01:36:48.283706 19877 net.cpp:148] Setting up res5a_branch2a/relu
I0630 01:36:48.283711 19877 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:36:48.283715 19877 net.cpp:163] Memory required for data: 116121684
I0630 01:36:48.283718 19877 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 01:36:48.283732 19877 net.cpp:98] Creating Layer res5a_branch2b
I0630 01:36:48.283735 19877 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 01:36:48.283741 19877 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 01:36:48.296727 19877 net.cpp:148] Setting up res5a_branch2b
I0630 01:36:48.296749 19877 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:36:48.296752 19877 net.cpp:163] Memory required for data: 118874196
I0630 01:36:48.296771 19877 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 01:36:48.296780 19877 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 01:36:48.296785 19877 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 01:36:48.296792 19877 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 01:36:48.297459 19877 net.cpp:148] Setting up res5a_branch2b/bn
I0630 01:36:48.297466 19877 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:36:48.297471 19877 net.cpp:163] Memory required for data: 121626708
I0630 01:36:48.297478 19877 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 01:36:48.297483 19877 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 01:36:48.297487 19877 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 01:36:48.297492 19877 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 01:36:48.297498 19877 net.cpp:148] Setting up res5a_branch2b/relu
I0630 01:36:48.297503 19877 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:36:48.297507 19877 net.cpp:163] Memory required for data: 124379220
I0630 01:36:48.297510 19877 layer_factory.hpp:77] Creating layer pool5
I0630 01:36:48.297518 19877 net.cpp:98] Creating Layer pool5
I0630 01:36:48.297520 19877 net.cpp:439] pool5 <- res5a_branch2b/bn
I0630 01:36:48.297533 19877 net.cpp:413] pool5 -> pool5
I0630 01:36:48.297564 19877 net.cpp:148] Setting up pool5
I0630 01:36:48.297570 19877 net.cpp:155] Top shape: 21 512 1 1 (10752)
I0630 01:36:48.297574 19877 net.cpp:163] Memory required for data: 124422228
I0630 01:36:48.297577 19877 layer_factory.hpp:77] Creating layer fc10
I0630 01:36:48.297585 19877 net.cpp:98] Creating Layer fc10
I0630 01:36:48.297587 19877 net.cpp:439] fc10 <- pool5
I0630 01:36:48.297592 19877 net.cpp:413] fc10 -> fc10
I0630 01:36:48.297827 19877 net.cpp:148] Setting up fc10
I0630 01:36:48.297833 19877 net.cpp:155] Top shape: 21 10 (210)
I0630 01:36:48.297837 19877 net.cpp:163] Memory required for data: 124423068
I0630 01:36:48.297842 19877 layer_factory.hpp:77] Creating layer loss
I0630 01:36:48.297852 19877 net.cpp:98] Creating Layer loss
I0630 01:36:48.297855 19877 net.cpp:439] loss <- fc10
I0630 01:36:48.297859 19877 net.cpp:439] loss <- label
I0630 01:36:48.297865 19877 net.cpp:413] loss -> loss
I0630 01:36:48.297874 19877 layer_factory.hpp:77] Creating layer loss
I0630 01:36:48.297991 19877 net.cpp:148] Setting up loss
I0630 01:36:48.297996 19877 net.cpp:155] Top shape: (1)
I0630 01:36:48.297999 19877 net.cpp:158]     with loss weight 1
I0630 01:36:48.298012 19877 net.cpp:163] Memory required for data: 124423072
I0630 01:36:48.298017 19877 net.cpp:224] loss needs backward computation.
I0630 01:36:48.298020 19877 net.cpp:224] fc10 needs backward computation.
I0630 01:36:48.298024 19877 net.cpp:224] pool5 needs backward computation.
I0630 01:36:48.298028 19877 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 01:36:48.298032 19877 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 01:36:48.298036 19877 net.cpp:224] res5a_branch2b needs backward computation.
I0630 01:36:48.298040 19877 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 01:36:48.298043 19877 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 01:36:48.298048 19877 net.cpp:224] res5a_branch2a needs backward computation.
I0630 01:36:48.298053 19877 net.cpp:224] pool4 needs backward computation.
I0630 01:36:48.298055 19877 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 01:36:48.298059 19877 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 01:36:48.298063 19877 net.cpp:224] res4a_branch2b needs backward computation.
I0630 01:36:48.298068 19877 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 01:36:48.298071 19877 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 01:36:48.298075 19877 net.cpp:224] res4a_branch2a needs backward computation.
I0630 01:36:48.298079 19877 net.cpp:224] pool3 needs backward computation.
I0630 01:36:48.298084 19877 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 01:36:48.298087 19877 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 01:36:48.298091 19877 net.cpp:224] res3a_branch2b needs backward computation.
I0630 01:36:48.298095 19877 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 01:36:48.298099 19877 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 01:36:48.298104 19877 net.cpp:224] res3a_branch2a needs backward computation.
I0630 01:36:48.298107 19877 net.cpp:224] pool2 needs backward computation.
I0630 01:36:48.298111 19877 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 01:36:48.298115 19877 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 01:36:48.298118 19877 net.cpp:224] res2a_branch2b needs backward computation.
I0630 01:36:48.298122 19877 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 01:36:48.298126 19877 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 01:36:48.298130 19877 net.cpp:224] res2a_branch2a needs backward computation.
I0630 01:36:48.298135 19877 net.cpp:224] pool1 needs backward computation.
I0630 01:36:48.298138 19877 net.cpp:224] conv1b/relu needs backward computation.
I0630 01:36:48.298141 19877 net.cpp:224] conv1b/bn needs backward computation.
I0630 01:36:48.298146 19877 net.cpp:224] conv1b needs backward computation.
I0630 01:36:48.298153 19877 net.cpp:224] conv1a/relu needs backward computation.
I0630 01:36:48.298157 19877 net.cpp:224] conv1a/bn needs backward computation.
I0630 01:36:48.298161 19877 net.cpp:224] conv1a needs backward computation.
I0630 01:36:48.298166 19877 net.cpp:226] data/bias does not need backward computation.
I0630 01:36:48.298171 19877 net.cpp:226] data does not need backward computation.
I0630 01:36:48.298174 19877 net.cpp:268] This network produces output loss
I0630 01:36:48.298192 19877 net.cpp:288] Network initialization done.
I0630 01:36:48.298627 19877 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/test.prototxt
I0630 01:36:48.298815 19877 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0630 01:36:48.298921 19877 layer_factory.hpp:77] Creating layer data
I0630 01:36:48.298984 19877 net.cpp:98] Creating Layer data
I0630 01:36:48.298990 19877 net.cpp:413] data -> data
I0630 01:36:48.298995 19877 net.cpp:413] data -> label
I0630 01:36:48.318876 19925 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0630 01:36:48.319017 19877 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0630 01:36:48.319082 19877 data_layer.cpp:83] output data size: 50,3,32,32
I0630 01:36:48.321422 19877 net.cpp:148] Setting up data
I0630 01:36:48.321434 19877 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0630 01:36:48.321439 19877 net.cpp:155] Top shape: 50 (50)
I0630 01:36:48.321444 19877 net.cpp:163] Memory required for data: 614600
I0630 01:36:48.321449 19877 layer_factory.hpp:77] Creating layer label_data_1_split
I0630 01:36:48.321460 19877 net.cpp:98] Creating Layer label_data_1_split
I0630 01:36:48.321465 19877 net.cpp:439] label_data_1_split <- label
I0630 01:36:48.321470 19877 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0630 01:36:48.321477 19877 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0630 01:36:48.321483 19877 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0630 01:36:48.321612 19877 net.cpp:148] Setting up label_data_1_split
I0630 01:36:48.321624 19877 net.cpp:155] Top shape: 50 (50)
I0630 01:36:48.321629 19877 net.cpp:155] Top shape: 50 (50)
I0630 01:36:48.321632 19877 net.cpp:155] Top shape: 50 (50)
I0630 01:36:48.321642 19877 net.cpp:163] Memory required for data: 615200
I0630 01:36:48.321651 19877 layer_factory.hpp:77] Creating layer data/bias
I0630 01:36:48.321660 19877 net.cpp:98] Creating Layer data/bias
I0630 01:36:48.321665 19877 net.cpp:439] data/bias <- data
I0630 01:36:48.321674 19877 net.cpp:413] data/bias -> data/bias
I0630 01:36:48.321785 19877 net.cpp:148] Setting up data/bias
I0630 01:36:48.321790 19877 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0630 01:36:48.321794 19877 net.cpp:163] Memory required for data: 1229600
I0630 01:36:48.321799 19877 layer_factory.hpp:77] Creating layer conv1a
I0630 01:36:48.321807 19877 net.cpp:98] Creating Layer conv1a
I0630 01:36:48.321812 19877 net.cpp:439] conv1a <- data/bias
I0630 01:36:48.321817 19877 net.cpp:413] conv1a -> conv1a
I0630 01:36:48.322322 19877 net.cpp:148] Setting up conv1a
I0630 01:36:48.322329 19877 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:36:48.322332 19877 net.cpp:163] Memory required for data: 7783200
I0630 01:36:48.322340 19877 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 01:36:48.322347 19877 net.cpp:98] Creating Layer conv1a/bn
I0630 01:36:48.322350 19877 net.cpp:439] conv1a/bn <- conv1a
I0630 01:36:48.322353 19877 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 01:36:48.323163 19877 net.cpp:148] Setting up conv1a/bn
I0630 01:36:48.323171 19877 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:36:48.323177 19877 net.cpp:163] Memory required for data: 14336800
I0630 01:36:48.323194 19877 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 01:36:48.323206 19877 net.cpp:98] Creating Layer conv1a/relu
I0630 01:36:48.323211 19877 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 01:36:48.323216 19877 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 01:36:48.323222 19877 net.cpp:148] Setting up conv1a/relu
I0630 01:36:48.323228 19877 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:36:48.323231 19877 net.cpp:163] Memory required for data: 20890400
I0630 01:36:48.323236 19877 layer_factory.hpp:77] Creating layer conv1b
I0630 01:36:48.323243 19877 net.cpp:98] Creating Layer conv1b
I0630 01:36:48.323247 19877 net.cpp:439] conv1b <- conv1a/bn
I0630 01:36:48.323253 19877 net.cpp:413] conv1b -> conv1b
I0630 01:36:48.323601 19877 net.cpp:148] Setting up conv1b
I0630 01:36:48.323608 19877 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:36:48.323611 19877 net.cpp:163] Memory required for data: 27444000
I0630 01:36:48.323619 19877 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 01:36:48.323629 19877 net.cpp:98] Creating Layer conv1b/bn
I0630 01:36:48.323633 19877 net.cpp:439] conv1b/bn <- conv1b
I0630 01:36:48.323637 19877 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 01:36:48.324478 19877 net.cpp:148] Setting up conv1b/bn
I0630 01:36:48.324486 19877 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:36:48.324488 19877 net.cpp:163] Memory required for data: 33997600
I0630 01:36:48.324497 19877 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 01:36:48.324503 19877 net.cpp:98] Creating Layer conv1b/relu
I0630 01:36:48.324507 19877 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 01:36:48.324512 19877 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 01:36:48.324519 19877 net.cpp:148] Setting up conv1b/relu
I0630 01:36:48.324524 19877 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:36:48.324528 19877 net.cpp:163] Memory required for data: 40551200
I0630 01:36:48.324532 19877 layer_factory.hpp:77] Creating layer pool1
I0630 01:36:48.324538 19877 net.cpp:98] Creating Layer pool1
I0630 01:36:48.324543 19877 net.cpp:439] pool1 <- conv1b/bn
I0630 01:36:48.324548 19877 net.cpp:413] pool1 -> pool1
I0630 01:36:48.324589 19877 net.cpp:148] Setting up pool1
I0630 01:36:48.324594 19877 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:36:48.324597 19877 net.cpp:163] Memory required for data: 47104800
I0630 01:36:48.324601 19877 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 01:36:48.324612 19877 net.cpp:98] Creating Layer res2a_branch2a
I0630 01:36:48.324615 19877 net.cpp:439] res2a_branch2a <- pool1
I0630 01:36:48.324620 19877 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 01:36:48.325296 19877 net.cpp:148] Setting up res2a_branch2a
I0630 01:36:48.325302 19877 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:36:48.325306 19877 net.cpp:163] Memory required for data: 60212000
I0630 01:36:48.325314 19877 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 01:36:48.325320 19877 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 01:36:48.325325 19877 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 01:36:48.325330 19877 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 01:36:48.326037 19877 net.cpp:148] Setting up res2a_branch2a/bn
I0630 01:36:48.326043 19877 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:36:48.326047 19877 net.cpp:163] Memory required for data: 73319200
I0630 01:36:48.326056 19877 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 01:36:48.326061 19877 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 01:36:48.326066 19877 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 01:36:48.326071 19877 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 01:36:48.326076 19877 net.cpp:148] Setting up res2a_branch2a/relu
I0630 01:36:48.326081 19877 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:36:48.326084 19877 net.cpp:163] Memory required for data: 86426400
I0630 01:36:48.326088 19877 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 01:36:48.326095 19877 net.cpp:98] Creating Layer res2a_branch2b
I0630 01:36:48.326104 19877 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 01:36:48.326110 19877 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 01:36:48.326591 19877 net.cpp:148] Setting up res2a_branch2b
I0630 01:36:48.326597 19877 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:36:48.326601 19877 net.cpp:163] Memory required for data: 99533600
I0630 01:36:48.326607 19877 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 01:36:48.326613 19877 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 01:36:48.326618 19877 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 01:36:48.326623 19877 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 01:36:48.327342 19877 net.cpp:148] Setting up res2a_branch2b/bn
I0630 01:36:48.327347 19877 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:36:48.327352 19877 net.cpp:163] Memory required for data: 112640800
I0630 01:36:48.327360 19877 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 01:36:48.327365 19877 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 01:36:48.327369 19877 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 01:36:48.327374 19877 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 01:36:48.327380 19877 net.cpp:148] Setting up res2a_branch2b/relu
I0630 01:36:48.327385 19877 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:36:48.327389 19877 net.cpp:163] Memory required for data: 125748000
I0630 01:36:48.327394 19877 layer_factory.hpp:77] Creating layer pool2
I0630 01:36:48.327399 19877 net.cpp:98] Creating Layer pool2
I0630 01:36:48.327404 19877 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 01:36:48.327409 19877 net.cpp:413] pool2 -> pool2
I0630 01:36:48.327448 19877 net.cpp:148] Setting up pool2
I0630 01:36:48.327453 19877 net.cpp:155] Top shape: 50 64 16 16 (819200)
I0630 01:36:48.327456 19877 net.cpp:163] Memory required for data: 129024800
I0630 01:36:48.327461 19877 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 01:36:48.327467 19877 net.cpp:98] Creating Layer res3a_branch2a
I0630 01:36:48.327472 19877 net.cpp:439] res3a_branch2a <- pool2
I0630 01:36:48.327477 19877 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 01:36:48.330114 19877 net.cpp:148] Setting up res3a_branch2a
I0630 01:36:48.330122 19877 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:36:48.330127 19877 net.cpp:163] Memory required for data: 135578400
I0630 01:36:48.330133 19877 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 01:36:48.330142 19877 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 01:36:48.330147 19877 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 01:36:48.330152 19877 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 01:36:48.330780 19877 net.cpp:148] Setting up res3a_branch2a/bn
I0630 01:36:48.330787 19877 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:36:48.330791 19877 net.cpp:163] Memory required for data: 142132000
I0630 01:36:48.330802 19877 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 01:36:48.330812 19877 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 01:36:48.330816 19877 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 01:36:48.330821 19877 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 01:36:48.330827 19877 net.cpp:148] Setting up res3a_branch2a/relu
I0630 01:36:48.330837 19877 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:36:48.330839 19877 net.cpp:163] Memory required for data: 148685600
I0630 01:36:48.330843 19877 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 01:36:48.330849 19877 net.cpp:98] Creating Layer res3a_branch2b
I0630 01:36:48.330853 19877 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 01:36:48.330859 19877 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 01:36:48.331888 19877 net.cpp:148] Setting up res3a_branch2b
I0630 01:36:48.331895 19877 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:36:48.331898 19877 net.cpp:163] Memory required for data: 155239200
I0630 01:36:48.331905 19877 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 01:36:48.331918 19877 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 01:36:48.331923 19877 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 01:36:48.331928 19877 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 01:36:48.332564 19877 net.cpp:148] Setting up res3a_branch2b/bn
I0630 01:36:48.332571 19877 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:36:48.332576 19877 net.cpp:163] Memory required for data: 161792800
I0630 01:36:48.332583 19877 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 01:36:48.332588 19877 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 01:36:48.332592 19877 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 01:36:48.332597 19877 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 01:36:48.332603 19877 net.cpp:148] Setting up res3a_branch2b/relu
I0630 01:36:48.332609 19877 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:36:48.332613 19877 net.cpp:163] Memory required for data: 168346400
I0630 01:36:48.332617 19877 layer_factory.hpp:77] Creating layer pool3
I0630 01:36:48.332623 19877 net.cpp:98] Creating Layer pool3
I0630 01:36:48.332626 19877 net.cpp:439] pool3 <- res3a_branch2b/bn
I0630 01:36:48.332631 19877 net.cpp:413] pool3 -> pool3
I0630 01:36:48.332675 19877 net.cpp:148] Setting up pool3
I0630 01:36:48.332680 19877 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:36:48.332684 19877 net.cpp:163] Memory required for data: 174900000
I0630 01:36:48.332689 19877 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 01:36:48.332695 19877 net.cpp:98] Creating Layer res4a_branch2a
I0630 01:36:48.332700 19877 net.cpp:439] res4a_branch2a <- pool3
I0630 01:36:48.332705 19877 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 01:36:48.338790 19877 net.cpp:148] Setting up res4a_branch2a
I0630 01:36:48.338798 19877 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:36:48.338800 19877 net.cpp:163] Memory required for data: 188007200
I0630 01:36:48.338807 19877 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 01:36:48.338814 19877 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 01:36:48.338819 19877 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 01:36:48.338824 19877 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 01:36:48.339457 19877 net.cpp:148] Setting up res4a_branch2a/bn
I0630 01:36:48.339463 19877 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:36:48.339468 19877 net.cpp:163] Memory required for data: 201114400
I0630 01:36:48.339476 19877 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 01:36:48.339481 19877 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 01:36:48.339485 19877 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 01:36:48.339490 19877 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 01:36:48.339496 19877 net.cpp:148] Setting up res4a_branch2a/relu
I0630 01:36:48.339503 19877 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:36:48.339506 19877 net.cpp:163] Memory required for data: 214221600
I0630 01:36:48.339510 19877 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 01:36:48.339517 19877 net.cpp:98] Creating Layer res4a_branch2b
I0630 01:36:48.339521 19877 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 01:36:48.339527 19877 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 01:36:48.342763 19877 net.cpp:148] Setting up res4a_branch2b
I0630 01:36:48.342769 19877 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:36:48.342773 19877 net.cpp:163] Memory required for data: 227328800
I0630 01:36:48.342780 19877 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 01:36:48.342793 19877 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 01:36:48.342799 19877 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 01:36:48.342808 19877 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 01:36:48.343451 19877 net.cpp:148] Setting up res4a_branch2b/bn
I0630 01:36:48.343457 19877 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:36:48.343466 19877 net.cpp:163] Memory required for data: 240436000
I0630 01:36:48.343471 19877 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 01:36:48.343473 19877 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 01:36:48.343475 19877 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 01:36:48.343477 19877 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 01:36:48.343482 19877 net.cpp:148] Setting up res4a_branch2b/relu
I0630 01:36:48.343484 19877 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:36:48.343487 19877 net.cpp:163] Memory required for data: 253543200
I0630 01:36:48.343489 19877 layer_factory.hpp:77] Creating layer pool4
I0630 01:36:48.343494 19877 net.cpp:98] Creating Layer pool4
I0630 01:36:48.343498 19877 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 01:36:48.343502 19877 net.cpp:413] pool4 -> pool4
I0630 01:36:48.343547 19877 net.cpp:148] Setting up pool4
I0630 01:36:48.343552 19877 net.cpp:155] Top shape: 50 256 8 8 (819200)
I0630 01:36:48.343556 19877 net.cpp:163] Memory required for data: 256820000
I0630 01:36:48.343560 19877 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 01:36:48.343571 19877 net.cpp:98] Creating Layer res5a_branch2a
I0630 01:36:48.343575 19877 net.cpp:439] res5a_branch2a <- pool4
I0630 01:36:48.343580 19877 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 01:36:48.368466 19877 net.cpp:148] Setting up res5a_branch2a
I0630 01:36:48.368489 19877 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:36:48.368492 19877 net.cpp:163] Memory required for data: 263373600
I0630 01:36:48.368499 19877 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 01:36:48.368511 19877 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 01:36:48.368517 19877 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 01:36:48.368521 19877 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 01:36:48.369223 19877 net.cpp:148] Setting up res5a_branch2a/bn
I0630 01:36:48.369230 19877 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:36:48.369232 19877 net.cpp:163] Memory required for data: 269927200
I0630 01:36:48.369237 19877 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 01:36:48.369241 19877 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 01:36:48.369243 19877 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 01:36:48.369246 19877 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 01:36:48.369251 19877 net.cpp:148] Setting up res5a_branch2a/relu
I0630 01:36:48.369252 19877 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:36:48.369254 19877 net.cpp:163] Memory required for data: 276480800
I0630 01:36:48.369256 19877 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 01:36:48.369269 19877 net.cpp:98] Creating Layer res5a_branch2b
I0630 01:36:48.369273 19877 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 01:36:48.369278 19877 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 01:36:48.382241 19877 net.cpp:148] Setting up res5a_branch2b
I0630 01:36:48.382262 19877 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:36:48.382266 19877 net.cpp:163] Memory required for data: 283034400
I0630 01:36:48.382278 19877 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 01:36:48.382287 19877 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 01:36:48.382290 19877 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 01:36:48.382295 19877 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 01:36:48.383009 19877 net.cpp:148] Setting up res5a_branch2b/bn
I0630 01:36:48.383016 19877 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:36:48.383019 19877 net.cpp:163] Memory required for data: 289588000
I0630 01:36:48.383024 19877 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 01:36:48.383028 19877 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 01:36:48.383030 19877 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 01:36:48.383033 19877 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 01:36:48.383046 19877 net.cpp:148] Setting up res5a_branch2b/relu
I0630 01:36:48.383050 19877 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:36:48.383054 19877 net.cpp:163] Memory required for data: 296141600
I0630 01:36:48.383057 19877 layer_factory.hpp:77] Creating layer pool5
I0630 01:36:48.383064 19877 net.cpp:98] Creating Layer pool5
I0630 01:36:48.383067 19877 net.cpp:439] pool5 <- res5a_branch2b/bn
I0630 01:36:48.383071 19877 net.cpp:413] pool5 -> pool5
I0630 01:36:48.383098 19877 net.cpp:148] Setting up pool5
I0630 01:36:48.383103 19877 net.cpp:155] Top shape: 50 512 1 1 (25600)
I0630 01:36:48.383105 19877 net.cpp:163] Memory required for data: 296244000
I0630 01:36:48.383107 19877 layer_factory.hpp:77] Creating layer fc10
I0630 01:36:48.383116 19877 net.cpp:98] Creating Layer fc10
I0630 01:36:48.383118 19877 net.cpp:439] fc10 <- pool5
I0630 01:36:48.383121 19877 net.cpp:413] fc10 -> fc10
I0630 01:36:48.383366 19877 net.cpp:148] Setting up fc10
I0630 01:36:48.383373 19877 net.cpp:155] Top shape: 50 10 (500)
I0630 01:36:48.383374 19877 net.cpp:163] Memory required for data: 296246000
I0630 01:36:48.383378 19877 layer_factory.hpp:77] Creating layer fc10_fc10_0_split
I0630 01:36:48.383380 19877 net.cpp:98] Creating Layer fc10_fc10_0_split
I0630 01:36:48.383383 19877 net.cpp:439] fc10_fc10_0_split <- fc10
I0630 01:36:48.383385 19877 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0630 01:36:48.383389 19877 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0630 01:36:48.383393 19877 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0630 01:36:48.383456 19877 net.cpp:148] Setting up fc10_fc10_0_split
I0630 01:36:48.383461 19877 net.cpp:155] Top shape: 50 10 (500)
I0630 01:36:48.383463 19877 net.cpp:155] Top shape: 50 10 (500)
I0630 01:36:48.383466 19877 net.cpp:155] Top shape: 50 10 (500)
I0630 01:36:48.383467 19877 net.cpp:163] Memory required for data: 296252000
I0630 01:36:48.383469 19877 layer_factory.hpp:77] Creating layer loss
I0630 01:36:48.383472 19877 net.cpp:98] Creating Layer loss
I0630 01:36:48.383474 19877 net.cpp:439] loss <- fc10_fc10_0_split_0
I0630 01:36:48.383477 19877 net.cpp:439] loss <- label_data_1_split_0
I0630 01:36:48.383479 19877 net.cpp:413] loss -> loss
I0630 01:36:48.383484 19877 layer_factory.hpp:77] Creating layer loss
I0630 01:36:48.383591 19877 net.cpp:148] Setting up loss
I0630 01:36:48.383597 19877 net.cpp:155] Top shape: (1)
I0630 01:36:48.383599 19877 net.cpp:158]     with loss weight 1
I0630 01:36:48.383605 19877 net.cpp:163] Memory required for data: 296252004
I0630 01:36:48.383607 19877 layer_factory.hpp:77] Creating layer accuracy/top1
I0630 01:36:48.383615 19877 net.cpp:98] Creating Layer accuracy/top1
I0630 01:36:48.383617 19877 net.cpp:439] accuracy/top1 <- fc10_fc10_0_split_1
I0630 01:36:48.383620 19877 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0630 01:36:48.383622 19877 net.cpp:413] accuracy/top1 -> accuracy/top1
I0630 01:36:48.383626 19877 net.cpp:148] Setting up accuracy/top1
I0630 01:36:48.383630 19877 net.cpp:155] Top shape: (1)
I0630 01:36:48.383633 19877 net.cpp:163] Memory required for data: 296252008
I0630 01:36:48.383636 19877 layer_factory.hpp:77] Creating layer accuracy/top5
I0630 01:36:48.383642 19877 net.cpp:98] Creating Layer accuracy/top5
I0630 01:36:48.383646 19877 net.cpp:439] accuracy/top5 <- fc10_fc10_0_split_2
I0630 01:36:48.383651 19877 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0630 01:36:48.383656 19877 net.cpp:413] accuracy/top5 -> accuracy/top5
I0630 01:36:48.383662 19877 net.cpp:148] Setting up accuracy/top5
I0630 01:36:48.383675 19877 net.cpp:155] Top shape: (1)
I0630 01:36:48.383679 19877 net.cpp:163] Memory required for data: 296252012
I0630 01:36:48.383682 19877 net.cpp:226] accuracy/top5 does not need backward computation.
I0630 01:36:48.383687 19877 net.cpp:226] accuracy/top1 does not need backward computation.
I0630 01:36:48.383689 19877 net.cpp:224] loss needs backward computation.
I0630 01:36:48.383693 19877 net.cpp:224] fc10_fc10_0_split needs backward computation.
I0630 01:36:48.383697 19877 net.cpp:224] fc10 needs backward computation.
I0630 01:36:48.383708 19877 net.cpp:224] pool5 needs backward computation.
I0630 01:36:48.383713 19877 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 01:36:48.383716 19877 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 01:36:48.383720 19877 net.cpp:224] res5a_branch2b needs backward computation.
I0630 01:36:48.383725 19877 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 01:36:48.383729 19877 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 01:36:48.383734 19877 net.cpp:224] res5a_branch2a needs backward computation.
I0630 01:36:48.383739 19877 net.cpp:224] pool4 needs backward computation.
I0630 01:36:48.383744 19877 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 01:36:48.383749 19877 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 01:36:48.383752 19877 net.cpp:224] res4a_branch2b needs backward computation.
I0630 01:36:48.383757 19877 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 01:36:48.383761 19877 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 01:36:48.383766 19877 net.cpp:224] res4a_branch2a needs backward computation.
I0630 01:36:48.383770 19877 net.cpp:224] pool3 needs backward computation.
I0630 01:36:48.383775 19877 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 01:36:48.383780 19877 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 01:36:48.383783 19877 net.cpp:224] res3a_branch2b needs backward computation.
I0630 01:36:48.383788 19877 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 01:36:48.383792 19877 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 01:36:48.383796 19877 net.cpp:224] res3a_branch2a needs backward computation.
I0630 01:36:48.383801 19877 net.cpp:224] pool2 needs backward computation.
I0630 01:36:48.383805 19877 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 01:36:48.383810 19877 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 01:36:48.383815 19877 net.cpp:224] res2a_branch2b needs backward computation.
I0630 01:36:48.383818 19877 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 01:36:48.383822 19877 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 01:36:48.383827 19877 net.cpp:224] res2a_branch2a needs backward computation.
I0630 01:36:48.383832 19877 net.cpp:224] pool1 needs backward computation.
I0630 01:36:48.383836 19877 net.cpp:224] conv1b/relu needs backward computation.
I0630 01:36:48.383841 19877 net.cpp:224] conv1b/bn needs backward computation.
I0630 01:36:48.383846 19877 net.cpp:224] conv1b needs backward computation.
I0630 01:36:48.383851 19877 net.cpp:224] conv1a/relu needs backward computation.
I0630 01:36:48.383854 19877 net.cpp:224] conv1a/bn needs backward computation.
I0630 01:36:48.383858 19877 net.cpp:224] conv1a needs backward computation.
I0630 01:36:48.383862 19877 net.cpp:226] data/bias does not need backward computation.
I0630 01:36:48.383867 19877 net.cpp:226] label_data_1_split does not need backward computation.
I0630 01:36:48.383872 19877 net.cpp:226] data does not need backward computation.
I0630 01:36:48.383877 19877 net.cpp:268] This network produces output accuracy/top1
I0630 01:36:48.383882 19877 net.cpp:268] This network produces output accuracy/top5
I0630 01:36:48.383885 19877 net.cpp:268] This network produces output loss
I0630 01:36:48.383911 19877 net.cpp:288] Network initialization done.
I0630 01:36:48.383976 19877 solver.cpp:60] Solver scaffolding done.
I0630 01:36:48.387434 19877 caffe.cpp:145] Finetuning from training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2_iter_64000.caffemodel
I0630 01:36:48.415014 19877 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0630 01:36:48.415082 19877 data_layer.cpp:83] output data size: 21,3,32,32
I0630 01:36:48.862893 19877 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0630 01:36:48.862972 19877 data_layer.cpp:83] output data size: 21,3,32,32
I0630 01:36:49.343606 19877 parallel.cpp:334] Starting Optimization
I0630 01:36:49.343660 19877 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:36:49.352423 19877 solver.cpp:406] Solving jacintonet11v2_train
I0630 01:36:49.352439 19877 solver.cpp:407] Learning Rate Policy: poly
I0630 01:36:49.354305 19877 solver.cpp:464] Iteration 0, Testing net (#0)
I0630 01:36:51.031159 19877 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9171
I0630 01:36:51.031179 19877 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9974
I0630 01:36:51.031184 19877 solver.cpp:537]     Test net output #2: loss = 0.2066 (* 1 = 0.2066 loss)
I0630 01:36:51.136889 19877 solver.cpp:290] Iteration 0 (0 iter/s, 1.78438s/100 iter), loss = 0
I0630 01:36:51.136912 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:51.136919 19877 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0630 01:36:51.147413 19877 solver.cpp:369] Finding and applying thresholds. Target sparsity = 0.02
I0630 01:36:51.259326 19877 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:36:53.315438 19877 solver.cpp:290] Iteration 100 (45.9041 iter/s, 2.17846s/100 iter), loss = 0
I0630 01:36:53.315460 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:53.315466 19877 sgd_solver.cpp:106] Iteration 100, lr = 0.00998437
I0630 01:36:55.370718 19877 solver.cpp:290] Iteration 200 (48.6573 iter/s, 2.05519s/100 iter), loss = 0
I0630 01:36:55.370740 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:55.370746 19877 sgd_solver.cpp:106] Iteration 200, lr = 0.00996875
I0630 01:36:57.426564 19877 solver.cpp:290] Iteration 300 (48.6439 iter/s, 2.05576s/100 iter), loss = 0
I0630 01:36:57.426587 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:57.426594 19877 sgd_solver.cpp:106] Iteration 300, lr = 0.00995312
I0630 01:36:59.482527 19877 solver.cpp:290] Iteration 400 (48.6411 iter/s, 2.05587s/100 iter), loss = 0
I0630 01:36:59.482548 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:36:59.482555 19877 sgd_solver.cpp:106] Iteration 400, lr = 0.0099375
I0630 01:37:01.538954 19877 solver.cpp:290] Iteration 500 (48.6301 iter/s, 2.05634s/100 iter), loss = 0
I0630 01:37:01.538975 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:01.538981 19877 sgd_solver.cpp:106] Iteration 500, lr = 0.00992187
I0630 01:37:03.596727 19877 solver.cpp:290] Iteration 600 (48.5983 iter/s, 2.05769s/100 iter), loss = 0
I0630 01:37:03.596750 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:03.596756 19877 sgd_solver.cpp:106] Iteration 600, lr = 0.00990625
I0630 01:37:05.657263 19877 solver.cpp:290] Iteration 700 (48.5332 iter/s, 2.06044s/100 iter), loss = 0
I0630 01:37:05.657296 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:05.657305 19877 sgd_solver.cpp:106] Iteration 700, lr = 0.00989062
I0630 01:37:07.745072 19877 solver.cpp:290] Iteration 800 (47.8993 iter/s, 2.08771s/100 iter), loss = 0
I0630 01:37:07.745095 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:07.745101 19877 sgd_solver.cpp:106] Iteration 800, lr = 0.009875
I0630 01:37:09.797600 19877 solver.cpp:290] Iteration 900 (48.7225 iter/s, 2.05244s/100 iter), loss = 0
I0630 01:37:09.797623 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:09.797631 19877 sgd_solver.cpp:106] Iteration 900, lr = 0.00985937
I0630 01:37:11.833076 19877 solver.cpp:464] Iteration 1000, Testing net (#0)
I0630 01:37:13.471585 19877 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.918
I0630 01:37:13.471606 19877 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:37:13.471612 19877 solver.cpp:537]     Test net output #2: loss = 0.2049 (* 1 = 0.2049 loss)
I0630 01:37:13.492050 19877 solver.cpp:290] Iteration 1000 (27.0686 iter/s, 3.69432s/100 iter), loss = 0
I0630 01:37:13.492069 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:13.492089 19877 sgd_solver.cpp:106] Iteration 1000, lr = 0.00984375
I0630 01:37:13.492586 19877 solver.cpp:369] Finding and applying thresholds. Target sparsity = 0.04
I0630 01:37:13.622772 19877 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:37:15.677472 19877 solver.cpp:290] Iteration 1100 (45.7597 iter/s, 2.18533s/100 iter), loss = 0
I0630 01:37:15.677525 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:15.677546 19877 sgd_solver.cpp:106] Iteration 1100, lr = 0.00982813
I0630 01:37:17.731930 19877 solver.cpp:290] Iteration 1200 (48.6773 iter/s, 2.05434s/100 iter), loss = 0
I0630 01:37:17.731952 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:17.731959 19877 sgd_solver.cpp:106] Iteration 1200, lr = 0.0098125
I0630 01:37:19.785733 19877 solver.cpp:290] Iteration 1300 (48.6922 iter/s, 2.05372s/100 iter), loss = 0
I0630 01:37:19.785784 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:19.785791 19877 sgd_solver.cpp:106] Iteration 1300, lr = 0.00979687
I0630 01:37:21.847923 19877 solver.cpp:290] Iteration 1400 (48.4949 iter/s, 2.06207s/100 iter), loss = 0
I0630 01:37:21.847945 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:21.847954 19877 sgd_solver.cpp:106] Iteration 1400, lr = 0.00978125
I0630 01:37:23.903832 19877 solver.cpp:290] Iteration 1500 (48.6423 iter/s, 2.05582s/100 iter), loss = 0
I0630 01:37:23.903854 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:23.903861 19877 sgd_solver.cpp:106] Iteration 1500, lr = 0.00976562
I0630 01:37:25.961966 19877 solver.cpp:290] Iteration 1600 (48.5898 iter/s, 2.05805s/100 iter), loss = 0
I0630 01:37:25.961988 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:25.961995 19877 sgd_solver.cpp:106] Iteration 1600, lr = 0.00975
I0630 01:37:28.016490 19877 solver.cpp:290] Iteration 1700 (48.6752 iter/s, 2.05444s/100 iter), loss = 0
I0630 01:37:28.016512 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:28.016520 19877 sgd_solver.cpp:106] Iteration 1700, lr = 0.00973437
I0630 01:37:30.077034 19877 solver.cpp:290] Iteration 1800 (48.5329 iter/s, 2.06046s/100 iter), loss = 0
I0630 01:37:30.077056 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:30.077064 19877 sgd_solver.cpp:106] Iteration 1800, lr = 0.00971875
I0630 01:37:32.131949 19877 solver.cpp:290] Iteration 1900 (48.6659 iter/s, 2.05483s/100 iter), loss = 0
I0630 01:37:32.131971 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:32.131978 19877 sgd_solver.cpp:106] Iteration 1900, lr = 0.00970312
I0630 01:37:34.171661 19877 solver.cpp:464] Iteration 2000, Testing net (#0)
I0630 01:37:35.811064 19877 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9175
I0630 01:37:35.811082 19877 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:37:35.811087 19877 solver.cpp:537]     Test net output #2: loss = 0.2027 (* 1 = 0.2027 loss)
I0630 01:37:35.830839 19877 solver.cpp:290] Iteration 2000 (27.0361 iter/s, 3.69875s/100 iter), loss = 0
I0630 01:37:35.830859 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:35.830868 19877 sgd_solver.cpp:106] Iteration 2000, lr = 0.0096875
I0630 01:37:35.831612 19877 solver.cpp:369] Finding and applying thresholds. Target sparsity = 0.06
I0630 01:37:35.983880 19877 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:37:38.042840 19877 solver.cpp:290] Iteration 2100 (45.2099 iter/s, 2.21191s/100 iter), loss = 0
I0630 01:37:38.042862 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:38.042870 19877 sgd_solver.cpp:106] Iteration 2100, lr = 0.00967188
I0630 01:37:40.097782 19877 solver.cpp:290] Iteration 2200 (48.6652 iter/s, 2.05486s/100 iter), loss = 0
I0630 01:37:40.097805 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:40.097811 19877 sgd_solver.cpp:106] Iteration 2200, lr = 0.00965625
I0630 01:37:42.155652 19877 solver.cpp:290] Iteration 2300 (48.596 iter/s, 2.05778s/100 iter), loss = 0
I0630 01:37:42.155674 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:42.155681 19877 sgd_solver.cpp:106] Iteration 2300, lr = 0.00964062
I0630 01:37:44.225436 19877 solver.cpp:290] Iteration 2400 (48.3163 iter/s, 2.06969s/100 iter), loss = 0
I0630 01:37:44.225467 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:44.225474 19877 sgd_solver.cpp:106] Iteration 2400, lr = 0.009625
I0630 01:37:46.292860 19877 solver.cpp:290] Iteration 2500 (48.3716 iter/s, 2.06733s/100 iter), loss = 0
I0630 01:37:46.292882 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:46.292888 19877 sgd_solver.cpp:106] Iteration 2500, lr = 0.00960938
I0630 01:37:48.348461 19877 solver.cpp:290] Iteration 2600 (48.6497 iter/s, 2.05551s/100 iter), loss = 0
I0630 01:37:48.348498 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:48.348508 19877 sgd_solver.cpp:106] Iteration 2600, lr = 0.00959375
I0630 01:37:50.400192 19877 solver.cpp:290] Iteration 2700 (48.7418 iter/s, 2.05163s/100 iter), loss = 0
I0630 01:37:50.400257 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:50.400267 19877 sgd_solver.cpp:106] Iteration 2700, lr = 0.00957812
I0630 01:37:52.457042 19877 solver.cpp:290] Iteration 2800 (48.621 iter/s, 2.05672s/100 iter), loss = 0
I0630 01:37:52.457065 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:52.457074 19877 sgd_solver.cpp:106] Iteration 2800, lr = 0.0095625
I0630 01:37:54.513938 19877 solver.cpp:290] Iteration 2900 (48.619 iter/s, 2.05681s/100 iter), loss = 0
I0630 01:37:54.513960 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:54.513967 19877 sgd_solver.cpp:106] Iteration 2900, lr = 0.00954687
I0630 01:37:56.549072 19877 solver.cpp:464] Iteration 3000, Testing net (#0)
I0630 01:37:58.185874 19877 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9176
I0630 01:37:58.185894 19877 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9971
I0630 01:37:58.185899 19877 solver.cpp:537]     Test net output #2: loss = 0.2044 (* 1 = 0.2044 loss)
I0630 01:37:58.205858 19877 solver.cpp:290] Iteration 3000 (27.0872 iter/s, 3.69179s/100 iter), loss = 0
I0630 01:37:58.205879 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:37:58.205888 19877 sgd_solver.cpp:106] Iteration 3000, lr = 0.00953125
I0630 01:37:58.206636 19877 solver.cpp:369] Finding and applying thresholds. Target sparsity = 0.08
I0630 01:37:58.379254 19877 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:38:00.433578 19877 solver.cpp:290] Iteration 3100 (44.8908 iter/s, 2.22763s/100 iter), loss = 0
I0630 01:38:00.433599 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:00.433607 19877 sgd_solver.cpp:106] Iteration 3100, lr = 0.00951563
I0630 01:38:02.490417 19877 solver.cpp:290] Iteration 3200 (48.6204 iter/s, 2.05675s/100 iter), loss = 0
I0630 01:38:02.490438 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:02.490445 19877 sgd_solver.cpp:106] Iteration 3200, lr = 0.0095
I0630 01:38:04.545173 19877 solver.cpp:290] Iteration 3300 (48.6697 iter/s, 2.05467s/100 iter), loss = 0
I0630 01:38:04.545195 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:04.545202 19877 sgd_solver.cpp:106] Iteration 3300, lr = 0.00948437
I0630 01:38:06.603890 19877 solver.cpp:290] Iteration 3400 (48.576 iter/s, 2.05863s/100 iter), loss = 0
I0630 01:38:06.603914 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:06.603919 19877 sgd_solver.cpp:106] Iteration 3400, lr = 0.00946875
I0630 01:38:08.668720 19877 solver.cpp:290] Iteration 3500 (48.4322 iter/s, 2.06474s/100 iter), loss = 0
I0630 01:38:08.668741 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:08.668748 19877 sgd_solver.cpp:106] Iteration 3500, lr = 0.00945312
I0630 01:38:10.728335 19877 solver.cpp:290] Iteration 3600 (48.5549 iter/s, 2.05953s/100 iter), loss = 0
I0630 01:38:10.728365 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:10.728375 19877 sgd_solver.cpp:106] Iteration 3600, lr = 0.0094375
I0630 01:38:12.784766 19877 solver.cpp:290] Iteration 3700 (48.6301 iter/s, 2.05634s/100 iter), loss = 0
I0630 01:38:12.784787 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:12.784796 19877 sgd_solver.cpp:106] Iteration 3700, lr = 0.00942187
I0630 01:38:14.836446 19877 solver.cpp:290] Iteration 3800 (48.7426 iter/s, 2.05159s/100 iter), loss = 0
I0630 01:38:14.836467 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:14.836475 19877 sgd_solver.cpp:106] Iteration 3800, lr = 0.00940625
I0630 01:38:16.893494 19877 solver.cpp:290] Iteration 3900 (48.6154 iter/s, 2.05696s/100 iter), loss = 0
I0630 01:38:16.893517 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:16.893524 19877 sgd_solver.cpp:106] Iteration 3900, lr = 0.00939062
I0630 01:38:18.932373 19877 solver.cpp:464] Iteration 4000, Testing net (#0)
I0630 01:38:20.570446 19877 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9179
I0630 01:38:20.570528 19877 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:38:20.570535 19877 solver.cpp:537]     Test net output #2: loss = 0.1999 (* 1 = 0.1999 loss)
I0630 01:38:20.590287 19877 solver.cpp:290] Iteration 4000 (27.0514 iter/s, 3.69666s/100 iter), loss = 0
I0630 01:38:20.590308 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:20.590317 19877 sgd_solver.cpp:106] Iteration 4000, lr = 0.009375
I0630 01:38:20.590865 19877 solver.cpp:369] Finding and applying thresholds. Target sparsity = 0.1
I0630 01:38:20.778458 19877 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:38:22.834213 19877 solver.cpp:290] Iteration 4100 (44.5666 iter/s, 2.24383s/100 iter), loss = 0
I0630 01:38:22.834235 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:22.834242 19877 sgd_solver.cpp:106] Iteration 4100, lr = 0.00935937
I0630 01:38:24.888439 19877 solver.cpp:290] Iteration 4200 (48.6823 iter/s, 2.05413s/100 iter), loss = 0
I0630 01:38:24.888464 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:24.888473 19877 sgd_solver.cpp:106] Iteration 4200, lr = 0.00934375
I0630 01:38:26.946317 19877 solver.cpp:290] Iteration 4300 (48.5959 iter/s, 2.05779s/100 iter), loss = 0
I0630 01:38:26.946338 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:26.946346 19877 sgd_solver.cpp:106] Iteration 4300, lr = 0.00932813
I0630 01:38:29.001440 19877 solver.cpp:290] Iteration 4400 (48.661 iter/s, 2.05504s/100 iter), loss = 0
I0630 01:38:29.001462 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:29.001468 19877 sgd_solver.cpp:106] Iteration 4400, lr = 0.0093125
I0630 01:38:31.054234 19877 solver.cpp:290] Iteration 4500 (48.7162 iter/s, 2.05271s/100 iter), loss = 0
I0630 01:38:31.054256 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:31.054263 19877 sgd_solver.cpp:106] Iteration 4500, lr = 0.00929687
I0630 01:38:33.110064 19877 solver.cpp:290] Iteration 4600 (48.6442 iter/s, 2.05574s/100 iter), loss = 0
I0630 01:38:33.110085 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:33.110093 19877 sgd_solver.cpp:106] Iteration 4600, lr = 0.00928125
I0630 01:38:35.168346 19877 solver.cpp:290] Iteration 4700 (48.5863 iter/s, 2.05819s/100 iter), loss = 0
I0630 01:38:35.168370 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:35.168376 19877 sgd_solver.cpp:106] Iteration 4700, lr = 0.00926562
I0630 01:38:37.232589 19877 solver.cpp:290] Iteration 4800 (48.446 iter/s, 2.06415s/100 iter), loss = 0
I0630 01:38:37.232612 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:37.232620 19877 sgd_solver.cpp:106] Iteration 4800, lr = 0.00925
I0630 01:38:39.285079 19877 solver.cpp:290] Iteration 4900 (48.7234 iter/s, 2.0524s/100 iter), loss = 0
I0630 01:38:39.285101 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:39.285109 19877 sgd_solver.cpp:106] Iteration 4900, lr = 0.00923437
I0630 01:38:41.328344 19877 solver.cpp:464] Iteration 5000, Testing net (#0)
I0630 01:38:42.972107 19877 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9172
I0630 01:38:42.972126 19877 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:38:42.972132 19877 solver.cpp:537]     Test net output #2: loss = 0.2018 (* 1 = 0.2018 loss)
I0630 01:38:42.991780 19877 solver.cpp:290] Iteration 5000 (26.9791 iter/s, 3.70657s/100 iter), loss = 0
I0630 01:38:42.991797 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:42.991812 19877 sgd_solver.cpp:106] Iteration 5000, lr = 0.00921875
I0630 01:38:42.992947 19877 solver.cpp:369] Finding and applying thresholds. Target sparsity = 0.12
I0630 01:38:43.204921 19877 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:38:45.263597 19877 solver.cpp:290] Iteration 5100 (44.0194 iter/s, 2.27173s/100 iter), loss = 0
I0630 01:38:45.263619 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:45.263643 19877 sgd_solver.cpp:106] Iteration 5100, lr = 0.00920312
I0630 01:38:47.318727 19877 solver.cpp:290] Iteration 5200 (48.6608 iter/s, 2.05504s/100 iter), loss = 0
I0630 01:38:47.318752 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:47.318758 19877 sgd_solver.cpp:106] Iteration 5200, lr = 0.0091875
I0630 01:38:49.378654 19877 solver.cpp:290] Iteration 5300 (48.5475 iter/s, 2.05984s/100 iter), loss = 0
I0630 01:38:49.378676 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:49.378684 19877 sgd_solver.cpp:106] Iteration 5300, lr = 0.00917188
I0630 01:38:51.431026 19877 solver.cpp:290] Iteration 5400 (48.7262 iter/s, 2.05228s/100 iter), loss = 0
I0630 01:38:51.431092 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:51.431100 19877 sgd_solver.cpp:106] Iteration 5400, lr = 0.00915625
I0630 01:38:53.486795 19877 solver.cpp:290] Iteration 5500 (48.6467 iter/s, 2.05564s/100 iter), loss = 0
I0630 01:38:53.486817 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:53.486824 19877 sgd_solver.cpp:106] Iteration 5500, lr = 0.00914062
I0630 01:38:55.546851 19877 solver.cpp:290] Iteration 5600 (48.5444 iter/s, 2.05997s/100 iter), loss = 0
I0630 01:38:55.546874 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:55.546880 19877 sgd_solver.cpp:106] Iteration 5600, lr = 0.009125
I0630 01:38:57.600299 19877 solver.cpp:290] Iteration 5700 (48.7006 iter/s, 2.05336s/100 iter), loss = 0
I0630 01:38:57.600323 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:57.600332 19877 sgd_solver.cpp:106] Iteration 5700, lr = 0.00910938
I0630 01:38:59.655788 19877 solver.cpp:290] Iteration 5800 (48.6523 iter/s, 2.0554s/100 iter), loss = 0
I0630 01:38:59.655812 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:38:59.655819 19877 sgd_solver.cpp:106] Iteration 5800, lr = 0.00909375
I0630 01:39:01.715876 19877 solver.cpp:290] Iteration 5900 (48.5437 iter/s, 2.06s/100 iter), loss = 0
I0630 01:39:01.715900 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:01.715906 19877 sgd_solver.cpp:106] Iteration 5900, lr = 0.00907812
I0630 01:39:03.768396 19877 solver.cpp:464] Iteration 6000, Testing net (#0)
I0630 01:39:05.408213 19877 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9187
I0630 01:39:05.408233 19877 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9972
I0630 01:39:05.408238 19877 solver.cpp:537]     Test net output #2: loss = 0.2019 (* 1 = 0.2019 loss)
I0630 01:39:05.428061 19877 solver.cpp:290] Iteration 6000 (26.9393 iter/s, 3.71205s/100 iter), loss = 0
I0630 01:39:05.428083 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:05.428092 19877 sgd_solver.cpp:106] Iteration 6000, lr = 0.0090625
I0630 01:39:05.428830 19877 solver.cpp:369] Finding and applying thresholds. Target sparsity = 0.14
I0630 01:39:05.656930 19877 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:39:07.712714 19877 solver.cpp:290] Iteration 6100 (43.7721 iter/s, 2.28456s/100 iter), loss = 0
I0630 01:39:07.712738 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:07.712744 19877 sgd_solver.cpp:106] Iteration 6100, lr = 0.00904687
I0630 01:39:09.766726 19877 solver.cpp:290] Iteration 6200 (48.6873 iter/s, 2.05392s/100 iter), loss = 0
I0630 01:39:09.766748 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:09.766754 19877 sgd_solver.cpp:106] Iteration 6200, lr = 0.00903125
I0630 01:39:11.822443 19877 solver.cpp:290] Iteration 6300 (48.647 iter/s, 2.05563s/100 iter), loss = 0
I0630 01:39:11.822469 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:11.822479 19877 sgd_solver.cpp:106] Iteration 6300, lr = 0.00901563
I0630 01:39:13.890035 19877 solver.cpp:290] Iteration 6400 (48.3676 iter/s, 2.0675s/100 iter), loss = 0
I0630 01:39:13.890058 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:13.890064 19877 sgd_solver.cpp:106] Iteration 6400, lr = 0.009
I0630 01:39:15.944001 19877 solver.cpp:290] Iteration 6500 (48.6884 iter/s, 2.05388s/100 iter), loss = 0
I0630 01:39:15.944026 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:15.944036 19877 sgd_solver.cpp:106] Iteration 6500, lr = 0.00898437
I0630 01:39:18.013360 19877 solver.cpp:290] Iteration 6600 (48.3263 iter/s, 2.06927s/100 iter), loss = 0
I0630 01:39:18.013383 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:18.013389 19877 sgd_solver.cpp:106] Iteration 6600, lr = 0.00896875
I0630 01:39:20.071107 19877 solver.cpp:290] Iteration 6700 (48.5989 iter/s, 2.05766s/100 iter), loss = 0
I0630 01:39:20.071146 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:20.071153 19877 sgd_solver.cpp:106] Iteration 6700, lr = 0.00895312
I0630 01:39:22.125468 19877 solver.cpp:290] Iteration 6800 (48.6794 iter/s, 2.05426s/100 iter), loss = 0
I0630 01:39:22.125535 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:22.125541 19877 sgd_solver.cpp:106] Iteration 6800, lr = 0.0089375
I0630 01:39:24.190377 19877 solver.cpp:290] Iteration 6900 (48.4313 iter/s, 2.06478s/100 iter), loss = 0
I0630 01:39:24.190400 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:24.190408 19877 sgd_solver.cpp:106] Iteration 6900, lr = 0.00892187
I0630 01:39:26.224954 19877 solver.cpp:464] Iteration 7000, Testing net (#0)
I0630 01:39:27.867779 19877 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9181
I0630 01:39:27.867799 19877 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9971
I0630 01:39:27.867804 19877 solver.cpp:537]     Test net output #2: loss = 0.2022 (* 1 = 0.2022 loss)
I0630 01:39:27.887585 19877 solver.cpp:290] Iteration 7000 (27.0484 iter/s, 3.69708s/100 iter), loss = 0
I0630 01:39:27.887603 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:27.887615 19877 sgd_solver.cpp:106] Iteration 7000, lr = 0.00890625
I0630 01:39:27.888159 19877 solver.cpp:369] Finding and applying thresholds. Target sparsity = 0.16
I0630 01:39:28.133862 19877 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:39:30.186993 19877 solver.cpp:290] Iteration 7100 (43.4912 iter/s, 2.29932s/100 iter), loss = 0
I0630 01:39:30.187016 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:30.187021 19877 sgd_solver.cpp:106] Iteration 7100, lr = 0.00889063
I0630 01:39:32.238159 19877 solver.cpp:290] Iteration 7200 (48.7548 iter/s, 2.05108s/100 iter), loss = 0
I0630 01:39:32.238183 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:32.238188 19877 sgd_solver.cpp:106] Iteration 7200, lr = 0.008875
I0630 01:39:34.294277 19877 solver.cpp:290] Iteration 7300 (48.6374 iter/s, 2.05603s/100 iter), loss = 0
I0630 01:39:34.294301 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:34.294306 19877 sgd_solver.cpp:106] Iteration 7300, lr = 0.00885937
I0630 01:39:36.350945 19877 solver.cpp:290] Iteration 7400 (48.6244 iter/s, 2.05658s/100 iter), loss = 0
I0630 01:39:36.350967 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:36.350975 19877 sgd_solver.cpp:106] Iteration 7400, lr = 0.00884375
I0630 01:39:38.405136 19877 solver.cpp:290] Iteration 7500 (48.683 iter/s, 2.0541s/100 iter), loss = 0
I0630 01:39:38.405159 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:38.405166 19877 sgd_solver.cpp:106] Iteration 7500, lr = 0.00882812
I0630 01:39:40.456188 19877 solver.cpp:290] Iteration 7600 (48.7576 iter/s, 2.05096s/100 iter), loss = 0
I0630 01:39:40.456212 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:40.456218 19877 sgd_solver.cpp:106] Iteration 7600, lr = 0.0088125
I0630 01:39:42.513018 19877 solver.cpp:290] Iteration 7700 (48.6206 iter/s, 2.05674s/100 iter), loss = 0
I0630 01:39:42.513039 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:42.513048 19877 sgd_solver.cpp:106] Iteration 7700, lr = 0.00879687
I0630 01:39:44.568147 19877 solver.cpp:290] Iteration 7800 (48.6608 iter/s, 2.05504s/100 iter), loss = 0
I0630 01:39:44.568169 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:44.568176 19877 sgd_solver.cpp:106] Iteration 7800, lr = 0.00878125
I0630 01:39:46.617405 19877 solver.cpp:290] Iteration 7900 (48.8002 iter/s, 2.04917s/100 iter), loss = 0
I0630 01:39:46.617429 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:46.617435 19877 sgd_solver.cpp:106] Iteration 7900, lr = 0.00876562
I0630 01:39:48.655571 19877 solver.cpp:464] Iteration 8000, Testing net (#0)
I0630 01:39:50.302021 19877 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9183
I0630 01:39:50.302040 19877 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9972
I0630 01:39:50.302045 19877 solver.cpp:537]     Test net output #2: loss = 0.2034 (* 1 = 0.2034 loss)
I0630 01:39:50.321698 19877 solver.cpp:290] Iteration 8000 (26.9967 iter/s, 3.70416s/100 iter), loss = 0
I0630 01:39:50.321714 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:50.321729 19877 sgd_solver.cpp:106] Iteration 8000, lr = 0.00875
I0630 01:39:50.322257 19877 solver.cpp:369] Finding and applying thresholds. Target sparsity = 0.18
I0630 01:39:50.601949 19877 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:39:52.659061 19877 solver.cpp:290] Iteration 8100 (42.7849 iter/s, 2.33727s/100 iter), loss = 0
I0630 01:39:52.659165 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:52.659176 19877 sgd_solver.cpp:106] Iteration 8100, lr = 0.00873438
I0630 01:39:54.712606 19877 solver.cpp:290] Iteration 8200 (48.7002 iter/s, 2.05338s/100 iter), loss = 0
I0630 01:39:54.712630 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:54.712635 19877 sgd_solver.cpp:106] Iteration 8200, lr = 0.00871875
I0630 01:39:56.764219 19877 solver.cpp:290] Iteration 8300 (48.7442 iter/s, 2.05152s/100 iter), loss = 0
I0630 01:39:56.764241 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:56.764250 19877 sgd_solver.cpp:106] Iteration 8300, lr = 0.00870312
I0630 01:39:58.818835 19877 solver.cpp:290] Iteration 8400 (48.6731 iter/s, 2.05452s/100 iter), loss = 0
I0630 01:39:58.818858 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:39:58.818864 19877 sgd_solver.cpp:106] Iteration 8400, lr = 0.0086875
I0630 01:40:00.873062 19877 solver.cpp:290] Iteration 8500 (48.6822 iter/s, 2.05414s/100 iter), loss = 0
I0630 01:40:00.873083 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:00.873092 19877 sgd_solver.cpp:106] Iteration 8500, lr = 0.00867188
I0630 01:40:02.928620 19877 solver.cpp:290] Iteration 8600 (48.6506 iter/s, 2.05547s/100 iter), loss = 0
I0630 01:40:02.928642 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:02.928650 19877 sgd_solver.cpp:106] Iteration 8600, lr = 0.00865625
I0630 01:40:04.982102 19877 solver.cpp:290] Iteration 8700 (48.6998 iter/s, 2.05339s/100 iter), loss = 0
I0630 01:40:04.982125 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:04.982134 19877 sgd_solver.cpp:106] Iteration 8700, lr = 0.00864062
I0630 01:40:07.044148 19877 solver.cpp:290] Iteration 8800 (48.4976 iter/s, 2.06196s/100 iter), loss = 0
I0630 01:40:07.044170 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:07.044178 19877 sgd_solver.cpp:106] Iteration 8800, lr = 0.008625
I0630 01:40:09.108255 19877 solver.cpp:290] Iteration 8900 (48.4492 iter/s, 2.06402s/100 iter), loss = 0
I0630 01:40:09.108278 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:09.108284 19877 sgd_solver.cpp:106] Iteration 8900, lr = 0.00860937
I0630 01:40:11.142940 19877 solver.cpp:464] Iteration 9000, Testing net (#0)
I0630 01:40:12.784883 19877 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9185
I0630 01:40:12.784904 19877 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.997
I0630 01:40:12.784909 19877 solver.cpp:537]     Test net output #2: loss = 0.1984 (* 1 = 0.1984 loss)
I0630 01:40:12.806198 19877 solver.cpp:290] Iteration 9000 (27.043 iter/s, 3.69781s/100 iter), loss = 0
I0630 01:40:12.806216 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:12.806227 19877 sgd_solver.cpp:106] Iteration 9000, lr = 0.00859375
I0630 01:40:12.806784 19877 solver.cpp:369] Finding and applying thresholds. Target sparsity = 0.2
I0630 01:40:13.091150 19877 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:40:15.144016 19877 solver.cpp:290] Iteration 9100 (42.7766 iter/s, 2.33773s/100 iter), loss = 0
I0630 01:40:15.144039 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:15.144047 19877 sgd_solver.cpp:106] Iteration 9100, lr = 0.00857813
I0630 01:40:17.207808 19877 solver.cpp:290] Iteration 9200 (48.4566 iter/s, 2.0637s/100 iter), loss = 0
I0630 01:40:17.207831 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:17.207837 19877 sgd_solver.cpp:106] Iteration 9200, lr = 0.0085625
I0630 01:40:19.261971 19877 solver.cpp:290] Iteration 9300 (48.6837 iter/s, 2.05408s/100 iter), loss = 0
I0630 01:40:19.261994 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:19.262001 19877 sgd_solver.cpp:106] Iteration 9300, lr = 0.00854687
I0630 01:40:21.321151 19877 solver.cpp:290] Iteration 9400 (48.5651 iter/s, 2.05909s/100 iter), loss = 0
I0630 01:40:21.321188 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:21.321195 19877 sgd_solver.cpp:106] Iteration 9400, lr = 0.00853125
I0630 01:40:23.377281 19877 solver.cpp:290] Iteration 9500 (48.6375 iter/s, 2.05603s/100 iter), loss = 0
I0630 01:40:23.377336 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:23.377343 19877 sgd_solver.cpp:106] Iteration 9500, lr = 0.00851563
I0630 01:40:25.431639 19877 solver.cpp:290] Iteration 9600 (48.6798 iter/s, 2.05424s/100 iter), loss = 0
I0630 01:40:25.431661 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:25.431668 19877 sgd_solver.cpp:106] Iteration 9600, lr = 0.0085
I0630 01:40:27.494074 19877 solver.cpp:290] Iteration 9700 (48.4884 iter/s, 2.06235s/100 iter), loss = 0
I0630 01:40:27.494101 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:27.494109 19877 sgd_solver.cpp:106] Iteration 9700, lr = 0.00848437
I0630 01:40:29.551841 19877 solver.cpp:290] Iteration 9800 (48.5985 iter/s, 2.05768s/100 iter), loss = 0
I0630 01:40:29.551863 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:29.551869 19877 sgd_solver.cpp:106] Iteration 9800, lr = 0.00846875
I0630 01:40:31.607728 19877 solver.cpp:290] Iteration 9900 (48.6429 iter/s, 2.0558s/100 iter), loss = 0
I0630 01:40:31.607748 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:31.607755 19877 sgd_solver.cpp:106] Iteration 9900, lr = 0.00845312
I0630 01:40:33.641398 19877 solver.cpp:591] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_10000.caffemodel
I0630 01:40:33.664670 19877 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_10000.solverstate
I0630 01:40:33.671983 19877 solver.cpp:464] Iteration 10000, Testing net (#0)
I0630 01:40:35.310825 19877 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9175
I0630 01:40:35.310852 19877 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.997
I0630 01:40:35.310858 19877 solver.cpp:537]     Test net output #2: loss = 0.1996 (* 1 = 0.1996 loss)
I0630 01:40:35.330472 19877 solver.cpp:290] Iteration 10000 (26.8628 iter/s, 3.72261s/100 iter), loss = 0
I0630 01:40:35.330490 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:35.330498 19877 sgd_solver.cpp:106] Iteration 10000, lr = 0.0084375
I0630 01:40:35.331048 19877 solver.cpp:369] Finding and applying thresholds. Target sparsity = 0.22
I0630 01:40:35.634384 19877 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:40:37.688961 19877 solver.cpp:290] Iteration 10100 (42.4017 iter/s, 2.3584s/100 iter), loss = 0
I0630 01:40:37.688984 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:37.688990 19877 sgd_solver.cpp:106] Iteration 10100, lr = 0.00842187
I0630 01:40:39.747584 19877 solver.cpp:290] Iteration 10200 (48.5782 iter/s, 2.05853s/100 iter), loss = 0
I0630 01:40:39.747606 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:39.747613 19877 sgd_solver.cpp:106] Iteration 10200, lr = 0.00840625
I0630 01:40:41.804685 19877 solver.cpp:290] Iteration 10300 (48.6142 iter/s, 2.05701s/100 iter), loss = 0
I0630 01:40:41.804710 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:41.804719 19877 sgd_solver.cpp:106] Iteration 10300, lr = 0.00839063
I0630 01:40:43.869575 19877 solver.cpp:290] Iteration 10400 (48.4309 iter/s, 2.0648s/100 iter), loss = 0
I0630 01:40:43.869598 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:43.869604 19877 sgd_solver.cpp:106] Iteration 10400, lr = 0.008375
I0630 01:40:45.924561 19877 solver.cpp:290] Iteration 10500 (48.6642 iter/s, 2.0549s/100 iter), loss = 0
I0630 01:40:45.924583 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:45.924589 19877 sgd_solver.cpp:106] Iteration 10500, lr = 0.00835937
I0630 01:40:47.981160 19877 solver.cpp:290] Iteration 10600 (48.626 iter/s, 2.05651s/100 iter), loss = 0
I0630 01:40:47.981182 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:47.981189 19877 sgd_solver.cpp:106] Iteration 10600, lr = 0.00834375
I0630 01:40:50.034904 19877 solver.cpp:290] Iteration 10700 (48.6936 iter/s, 2.05366s/100 iter), loss = 0
I0630 01:40:50.034925 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:50.034934 19877 sgd_solver.cpp:106] Iteration 10700, lr = 0.00832812
I0630 01:40:52.093456 19877 solver.cpp:290] Iteration 10800 (48.5799 iter/s, 2.05846s/100 iter), loss = 0
I0630 01:40:52.093477 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:52.093485 19877 sgd_solver.cpp:106] Iteration 10800, lr = 0.0083125
I0630 01:40:54.150980 19877 solver.cpp:290] Iteration 10900 (48.6042 iter/s, 2.05744s/100 iter), loss = 0
I0630 01:40:54.151082 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:54.151093 19877 sgd_solver.cpp:106] Iteration 10900, lr = 0.00829687
I0630 01:40:56.191334 19877 solver.cpp:464] Iteration 11000, Testing net (#0)
I0630 01:40:57.829922 19877 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.918
I0630 01:40:57.829942 19877 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.9969
I0630 01:40:57.829947 19877 solver.cpp:537]     Test net output #2: loss = 0.2025 (* 1 = 0.2025 loss)
I0630 01:40:57.851042 19877 solver.cpp:290] Iteration 11000 (27.0281 iter/s, 3.69985s/100 iter), loss = 0
I0630 01:40:57.851060 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:40:57.851071 19877 sgd_solver.cpp:106] Iteration 11000, lr = 0.00828125
I0630 01:40:57.851605 19877 solver.cpp:369] Finding and applying thresholds. Target sparsity = 0.24
I0630 01:40:58.180907 19877 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:41:00.242563 19877 solver.cpp:290] Iteration 11100 (41.8161 iter/s, 2.39142s/100 iter), loss = 0
I0630 01:41:00.242590 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:00.242599 19877 sgd_solver.cpp:106] Iteration 11100, lr = 0.00826562
I0630 01:41:02.305640 19877 solver.cpp:290] Iteration 11200 (48.4735 iter/s, 2.06298s/100 iter), loss = 0
I0630 01:41:02.305670 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:02.305678 19877 sgd_solver.cpp:106] Iteration 11200, lr = 0.00825
I0630 01:41:04.362236 19877 solver.cpp:290] Iteration 11300 (48.6263 iter/s, 2.0565s/100 iter), loss = 0
I0630 01:41:04.362269 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:04.362280 19877 sgd_solver.cpp:106] Iteration 11300, lr = 0.00823438
I0630 01:41:06.421268 19877 solver.cpp:290] Iteration 11400 (48.5688 iter/s, 2.05894s/100 iter), loss = 0
I0630 01:41:06.421290 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:06.421298 19877 sgd_solver.cpp:106] Iteration 11400, lr = 0.00821875
I0630 01:41:08.495419 19877 solver.cpp:290] Iteration 11500 (48.2146 iter/s, 2.07406s/100 iter), loss = 0
I0630 01:41:08.495440 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:08.495447 19877 sgd_solver.cpp:106] Iteration 11500, lr = 0.00820312
I0630 01:41:10.556818 19877 solver.cpp:290] Iteration 11600 (48.5129 iter/s, 2.06131s/100 iter), loss = 0
I0630 01:41:10.556850 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:10.556860 19877 sgd_solver.cpp:106] Iteration 11600, lr = 0.0081875
I0630 01:41:12.617390 19877 solver.cpp:290] Iteration 11700 (48.5324 iter/s, 2.06048s/100 iter), loss = 0
I0630 01:41:12.617414 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:12.617420 19877 sgd_solver.cpp:106] Iteration 11700, lr = 0.00817188
I0630 01:41:14.678640 19877 solver.cpp:290] Iteration 11800 (48.5163 iter/s, 2.06116s/100 iter), loss = 0
I0630 01:41:14.678663 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:14.678669 19877 sgd_solver.cpp:106] Iteration 11800, lr = 0.00815625
I0630 01:41:16.737702 19877 solver.cpp:290] Iteration 11900 (48.5679 iter/s, 2.05897s/100 iter), loss = 0
I0630 01:41:16.737725 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:16.737732 19877 sgd_solver.cpp:106] Iteration 11900, lr = 0.00814062
I0630 01:41:18.771687 19877 solver.cpp:464] Iteration 12000, Testing net (#0)
I0630 01:41:20.411921 19877 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9181
I0630 01:41:20.411940 19877 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.997
I0630 01:41:20.411945 19877 solver.cpp:537]     Test net output #2: loss = 0.1985 (* 1 = 0.1985 loss)
I0630 01:41:20.431897 19877 solver.cpp:290] Iteration 12000 (27.0705 iter/s, 3.69406s/100 iter), loss = 0
I0630 01:41:20.431926 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:20.431932 19877 sgd_solver.cpp:106] Iteration 12000, lr = 0.008125
I0630 01:41:20.432473 19877 solver.cpp:369] Finding and applying thresholds. Target sparsity = 0.26
I0630 01:41:20.776975 19877 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:41:22.832460 19877 solver.cpp:290] Iteration 12100 (41.6587 iter/s, 2.40046s/100 iter), loss = 0
I0630 01:41:22.832484 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:22.832490 19877 sgd_solver.cpp:106] Iteration 12100, lr = 0.00810937
I0630 01:41:24.885694 19877 solver.cpp:290] Iteration 12200 (48.7058 iter/s, 2.05315s/100 iter), loss = 0
I0630 01:41:24.885789 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:24.885797 19877 sgd_solver.cpp:106] Iteration 12200, lr = 0.00809375
I0630 01:41:26.939501 19877 solver.cpp:290] Iteration 12300 (48.6938 iter/s, 2.05365s/100 iter), loss = 0
I0630 01:41:26.939530 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:26.939540 19877 sgd_solver.cpp:106] Iteration 12300, lr = 0.00807813
I0630 01:41:28.997340 19877 solver.cpp:290] Iteration 12400 (48.5968 iter/s, 2.05775s/100 iter), loss = 0
I0630 01:41:28.997361 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:28.997369 19877 sgd_solver.cpp:106] Iteration 12400, lr = 0.0080625
I0630 01:41:31.056733 19877 solver.cpp:290] Iteration 12500 (48.56 iter/s, 2.05931s/100 iter), loss = 0
I0630 01:41:31.056756 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:31.056762 19877 sgd_solver.cpp:106] Iteration 12500, lr = 0.00804687
I0630 01:41:33.115103 19877 solver.cpp:290] Iteration 12600 (48.5843 iter/s, 2.05828s/100 iter), loss = 0
I0630 01:41:33.115129 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:33.115139 19877 sgd_solver.cpp:106] Iteration 12600, lr = 0.00803125
I0630 01:41:35.181154 19877 solver.cpp:290] Iteration 12700 (48.4036 iter/s, 2.06596s/100 iter), loss = 0
I0630 01:41:35.181176 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:35.181182 19877 sgd_solver.cpp:106] Iteration 12700, lr = 0.00801562
I0630 01:41:37.248596 19877 solver.cpp:290] Iteration 12800 (48.3711 iter/s, 2.06735s/100 iter), loss = 0
I0630 01:41:37.248630 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:37.248638 19877 sgd_solver.cpp:106] Iteration 12800, lr = 0.008
I0630 01:41:39.315346 19877 solver.cpp:290] Iteration 12900 (48.3874 iter/s, 2.06665s/100 iter), loss = 0
I0630 01:41:39.315368 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:39.315376 19877 sgd_solver.cpp:106] Iteration 12900, lr = 0.00798437
I0630 01:41:41.351496 19877 solver.cpp:464] Iteration 13000, Testing net (#0)
I0630 01:41:42.991287 19877 solver.cpp:537]     Test net output #0: accuracy/top1 = 0.9183
I0630 01:41:42.991307 19877 solver.cpp:537]     Test net output #1: accuracy/top5 = 0.997
I0630 01:41:42.991312 19877 solver.cpp:537]     Test net output #2: loss = 0.1994 (* 1 = 0.1994 loss)
I0630 01:41:43.011363 19877 solver.cpp:290] Iteration 13000 (27.0571 iter/s, 3.69589s/100 iter), loss = 0
I0630 01:41:43.011382 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:43.011394 19877 sgd_solver.cpp:106] Iteration 13000, lr = 0.00796875
I0630 01:41:43.011900 19877 solver.cpp:369] Finding and applying thresholds. Target sparsity = 0.28
I0630 01:41:43.353585 19877 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:41:45.410735 19877 solver.cpp:290] Iteration 13100 (41.6792 iter/s, 2.39928s/100 iter), loss = 0
I0630 01:41:45.410758 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:45.410765 19877 sgd_solver.cpp:106] Iteration 13100, lr = 0.00795313
I0630 01:41:47.463649 19877 solver.cpp:290] Iteration 13200 (48.7134 iter/s, 2.05283s/100 iter), loss = 0
I0630 01:41:47.463671 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:47.463678 19877 sgd_solver.cpp:106] Iteration 13200, lr = 0.0079375
I0630 01:41:49.526056 19877 solver.cpp:290] Iteration 13300 (48.4891 iter/s, 2.06232s/100 iter), loss = 0
I0630 01:41:49.526079 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:49.526088 19877 sgd_solver.cpp:106] Iteration 13300, lr = 0.00792187
I0630 01:41:51.580821 19877 solver.cpp:290] Iteration 13400 (48.6695 iter/s, 2.05468s/100 iter), loss = 0
I0630 01:41:51.580840 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:51.580849 19877 sgd_solver.cpp:106] Iteration 13400, lr = 0.00790625
I0630 01:41:53.636371 19877 solver.cpp:290] Iteration 13500 (48.6508 iter/s, 2.05547s/100 iter), loss = 0
I0630 01:41:53.636405 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:53.636412 19877 sgd_solver.cpp:106] Iteration 13500, lr = 0.00789062
I0630 01:41:55.696847 19877 solver.cpp:290] Iteration 13600 (48.5349 iter/s, 2.06037s/100 iter), loss = 0
I0630 01:41:55.696912 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:55.696919 19877 sgd_solver.cpp:106] Iteration 13600, lr = 0.007875
I0630 01:41:57.753708 19877 solver.cpp:290] Iteration 13700 (48.6209 iter/s, 2.05673s/100 iter), loss = 0
I0630 01:41:57.753736 19877 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:41:57.753746 19877 sgd_solver.cpp:106] Iteration 13700, lr = 0.00785937
Logging output to training/cifar10_jacintonet11v2_2017-06-30_01-13-02/train-log_2017-06-30_01-13-02.txt
mkdir: cannot create directory ‘training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial’: File exists
training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse
WARNING: gnome-keyring:: couldn't connect to: /run/user/30409/keyring-KJvviu/pkcs11: Connection refused
p11-kit: skipping module 'gnome-keyring' whose initialization failed: An error occurred on the device
I0630 01:43:36.961720 29015 caffe.cpp:209] Using GPUs 0, 1, 2
I0630 01:43:36.962589 29015 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0630 01:43:36.962942 29015 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0630 01:43:36.963276 29015 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0630 01:43:37.352587 29015 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: true
iter_size: 1
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.02
sparsity_step_iter: 1000
sparsity_start_iter: 0
sparsity_start_factor: 0
I0630 01:43:37.352681 29015 solver.cpp:82] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/train.prototxt
I0630 01:43:37.353327 29015 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0630 01:43:37.353333 29015 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0630 01:43:37.353543 29015 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 21
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0630 01:43:37.353634 29015 layer_factory.hpp:77] Creating layer data
I0630 01:43:37.353730 29015 net.cpp:98] Creating Layer data
I0630 01:43:37.353739 29015 net.cpp:413] data -> data
I0630 01:43:37.353757 29015 net.cpp:413] data -> label
I0630 01:43:37.354579 29042 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0630 01:43:37.355602 29015 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0630 01:43:37.355654 29015 data_layer.cpp:83] output data size: 21,3,32,32
I0630 01:43:37.357301 29015 net.cpp:148] Setting up data
I0630 01:43:37.357312 29015 net.cpp:155] Top shape: 21 3 32 32 (64512)
I0630 01:43:37.357316 29015 net.cpp:155] Top shape: 21 (21)
I0630 01:43:37.357317 29015 net.cpp:163] Memory required for data: 258132
I0630 01:43:37.357323 29015 layer_factory.hpp:77] Creating layer data/bias
I0630 01:43:37.357331 29015 net.cpp:98] Creating Layer data/bias
I0630 01:43:37.357336 29015 net.cpp:439] data/bias <- data
I0630 01:43:37.357342 29015 net.cpp:413] data/bias -> data/bias
I0630 01:43:37.358391 29015 net.cpp:148] Setting up data/bias
I0630 01:43:37.358399 29015 net.cpp:155] Top shape: 21 3 32 32 (64512)
I0630 01:43:37.358402 29015 net.cpp:163] Memory required for data: 516180
I0630 01:43:37.358410 29015 layer_factory.hpp:77] Creating layer conv1a
I0630 01:43:37.358419 29015 net.cpp:98] Creating Layer conv1a
I0630 01:43:37.358422 29015 net.cpp:439] conv1a <- data/bias
I0630 01:43:37.358424 29015 net.cpp:413] conv1a -> conv1a
I0630 01:43:37.359338 29044 blocking_queue.cpp:50] Waiting for data
I0630 01:43:37.359760 29015 net.cpp:148] Setting up conv1a
I0630 01:43:37.359769 29015 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:43:37.359771 29015 net.cpp:163] Memory required for data: 3268692
I0630 01:43:37.359776 29015 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 01:43:37.359783 29015 net.cpp:98] Creating Layer conv1a/bn
I0630 01:43:37.359786 29015 net.cpp:439] conv1a/bn <- conv1a
I0630 01:43:37.359788 29015 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 01:43:37.360522 29015 net.cpp:148] Setting up conv1a/bn
I0630 01:43:37.360532 29015 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:43:37.360534 29015 net.cpp:163] Memory required for data: 6021204
I0630 01:43:37.360541 29015 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 01:43:37.360545 29015 net.cpp:98] Creating Layer conv1a/relu
I0630 01:43:37.360548 29015 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 01:43:37.360550 29015 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 01:43:37.360559 29015 net.cpp:148] Setting up conv1a/relu
I0630 01:43:37.360563 29015 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:43:37.360572 29015 net.cpp:163] Memory required for data: 8773716
I0630 01:43:37.360575 29015 layer_factory.hpp:77] Creating layer conv1b
I0630 01:43:37.360579 29015 net.cpp:98] Creating Layer conv1b
I0630 01:43:37.360582 29015 net.cpp:439] conv1b <- conv1a/bn
I0630 01:43:37.360585 29015 net.cpp:413] conv1b -> conv1b
I0630 01:43:37.361001 29015 net.cpp:148] Setting up conv1b
I0630 01:43:37.361007 29015 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:43:37.361009 29015 net.cpp:163] Memory required for data: 11526228
I0630 01:43:37.361013 29015 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 01:43:37.361017 29015 net.cpp:98] Creating Layer conv1b/bn
I0630 01:43:37.361019 29015 net.cpp:439] conv1b/bn <- conv1b
I0630 01:43:37.361022 29015 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 01:43:37.361739 29015 net.cpp:148] Setting up conv1b/bn
I0630 01:43:37.361747 29015 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:43:37.361749 29015 net.cpp:163] Memory required for data: 14278740
I0630 01:43:37.361754 29015 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 01:43:37.361757 29015 net.cpp:98] Creating Layer conv1b/relu
I0630 01:43:37.361759 29015 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 01:43:37.361762 29015 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 01:43:37.361765 29015 net.cpp:148] Setting up conv1b/relu
I0630 01:43:37.361768 29015 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:43:37.361769 29015 net.cpp:163] Memory required for data: 17031252
I0630 01:43:37.361771 29015 layer_factory.hpp:77] Creating layer pool1
I0630 01:43:37.361776 29015 net.cpp:98] Creating Layer pool1
I0630 01:43:37.361778 29015 net.cpp:439] pool1 <- conv1b/bn
I0630 01:43:37.361780 29015 net.cpp:413] pool1 -> pool1
I0630 01:43:37.361825 29015 net.cpp:148] Setting up pool1
I0630 01:43:37.361829 29015 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:43:37.361831 29015 net.cpp:163] Memory required for data: 19783764
I0630 01:43:37.361835 29015 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 01:43:37.361840 29015 net.cpp:98] Creating Layer res2a_branch2a
I0630 01:43:37.361845 29015 net.cpp:439] res2a_branch2a <- pool1
I0630 01:43:37.361850 29015 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 01:43:37.362607 29015 net.cpp:148] Setting up res2a_branch2a
I0630 01:43:37.362614 29015 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:43:37.362617 29015 net.cpp:163] Memory required for data: 25288788
I0630 01:43:37.362620 29015 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 01:43:37.362624 29015 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 01:43:37.362627 29015 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 01:43:37.362629 29015 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 01:43:37.363348 29015 net.cpp:148] Setting up res2a_branch2a/bn
I0630 01:43:37.363354 29015 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:43:37.363356 29015 net.cpp:163] Memory required for data: 30793812
I0630 01:43:37.363361 29015 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 01:43:37.363364 29015 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 01:43:37.363366 29015 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 01:43:37.363369 29015 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 01:43:37.363373 29015 net.cpp:148] Setting up res2a_branch2a/relu
I0630 01:43:37.363375 29015 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:43:37.363378 29015 net.cpp:163] Memory required for data: 36298836
I0630 01:43:37.363379 29015 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 01:43:37.363384 29015 net.cpp:98] Creating Layer res2a_branch2b
I0630 01:43:37.363385 29015 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 01:43:37.363387 29015 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 01:43:37.364852 29015 net.cpp:148] Setting up res2a_branch2b
I0630 01:43:37.364861 29015 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:43:37.364863 29015 net.cpp:163] Memory required for data: 41803860
I0630 01:43:37.364873 29015 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 01:43:37.364878 29015 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 01:43:37.364881 29015 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 01:43:37.364883 29015 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 01:43:37.365602 29015 net.cpp:148] Setting up res2a_branch2b/bn
I0630 01:43:37.365610 29015 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:43:37.365612 29015 net.cpp:163] Memory required for data: 47308884
I0630 01:43:37.365617 29015 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 01:43:37.365622 29015 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 01:43:37.365623 29015 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 01:43:37.365627 29015 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 01:43:37.365633 29015 net.cpp:148] Setting up res2a_branch2b/relu
I0630 01:43:37.365635 29015 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:43:37.365638 29015 net.cpp:163] Memory required for data: 52813908
I0630 01:43:37.365639 29015 layer_factory.hpp:77] Creating layer pool2
I0630 01:43:37.365643 29015 net.cpp:98] Creating Layer pool2
I0630 01:43:37.365644 29015 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 01:43:37.365646 29015 net.cpp:413] pool2 -> pool2
I0630 01:43:37.365687 29015 net.cpp:148] Setting up pool2
I0630 01:43:37.365694 29015 net.cpp:155] Top shape: 21 64 16 16 (344064)
I0630 01:43:37.365697 29015 net.cpp:163] Memory required for data: 54190164
I0630 01:43:37.365700 29015 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 01:43:37.365710 29015 net.cpp:98] Creating Layer res3a_branch2a
I0630 01:43:37.365713 29015 net.cpp:439] res3a_branch2a <- pool2
I0630 01:43:37.365718 29015 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 01:43:37.368469 29015 net.cpp:148] Setting up res3a_branch2a
I0630 01:43:37.368479 29015 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:43:37.368481 29015 net.cpp:163] Memory required for data: 56942676
I0630 01:43:37.368485 29015 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 01:43:37.368489 29015 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 01:43:37.368491 29015 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 01:43:37.368495 29015 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 01:43:37.369231 29015 net.cpp:148] Setting up res3a_branch2a/bn
I0630 01:43:37.369240 29015 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:43:37.369241 29015 net.cpp:163] Memory required for data: 59695188
I0630 01:43:37.369252 29015 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 01:43:37.369256 29015 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 01:43:37.369258 29015 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 01:43:37.369261 29015 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 01:43:37.369264 29015 net.cpp:148] Setting up res3a_branch2a/relu
I0630 01:43:37.369267 29015 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:43:37.369268 29015 net.cpp:163] Memory required for data: 62447700
I0630 01:43:37.369271 29015 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 01:43:37.369277 29015 net.cpp:98] Creating Layer res3a_branch2b
I0630 01:43:37.369278 29015 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 01:43:37.369282 29015 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 01:43:37.370342 29015 net.cpp:148] Setting up res3a_branch2b
I0630 01:43:37.370349 29015 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:43:37.370352 29015 net.cpp:163] Memory required for data: 65200212
I0630 01:43:37.370355 29015 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 01:43:37.370360 29015 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 01:43:37.370363 29015 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 01:43:37.370367 29015 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 01:43:37.371006 29015 net.cpp:148] Setting up res3a_branch2b/bn
I0630 01:43:37.371012 29015 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:43:37.371021 29015 net.cpp:163] Memory required for data: 67952724
I0630 01:43:37.371026 29015 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 01:43:37.371028 29015 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 01:43:37.371031 29015 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 01:43:37.371033 29015 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 01:43:37.371037 29015 net.cpp:148] Setting up res3a_branch2b/relu
I0630 01:43:37.371039 29015 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:43:37.371042 29015 net.cpp:163] Memory required for data: 70705236
I0630 01:43:37.371043 29015 layer_factory.hpp:77] Creating layer pool3
I0630 01:43:37.371047 29015 net.cpp:98] Creating Layer pool3
I0630 01:43:37.371048 29015 net.cpp:439] pool3 <- res3a_branch2b/bn
I0630 01:43:37.371052 29015 net.cpp:413] pool3 -> pool3
I0630 01:43:37.371085 29015 net.cpp:148] Setting up pool3
I0630 01:43:37.371089 29015 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:43:37.371090 29015 net.cpp:163] Memory required for data: 73457748
I0630 01:43:37.371093 29015 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 01:43:37.371096 29015 net.cpp:98] Creating Layer res4a_branch2a
I0630 01:43:37.371098 29015 net.cpp:439] res4a_branch2a <- pool3
I0630 01:43:37.371101 29015 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 01:43:37.377280 29015 net.cpp:148] Setting up res4a_branch2a
I0630 01:43:37.377288 29015 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:43:37.377290 29015 net.cpp:163] Memory required for data: 78962772
I0630 01:43:37.377293 29015 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 01:43:37.377297 29015 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 01:43:37.377300 29015 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 01:43:37.377302 29015 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 01:43:37.377969 29015 net.cpp:148] Setting up res4a_branch2a/bn
I0630 01:43:37.377976 29015 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:43:37.377979 29015 net.cpp:163] Memory required for data: 84467796
I0630 01:43:37.377984 29015 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 01:43:37.377986 29015 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 01:43:37.377990 29015 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 01:43:37.377991 29015 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 01:43:37.377995 29015 net.cpp:148] Setting up res4a_branch2a/relu
I0630 01:43:37.377997 29015 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:43:37.378000 29015 net.cpp:163] Memory required for data: 89972820
I0630 01:43:37.378001 29015 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 01:43:37.378005 29015 net.cpp:98] Creating Layer res4a_branch2b
I0630 01:43:37.378006 29015 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 01:43:37.378010 29015 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 01:43:37.381299 29015 net.cpp:148] Setting up res4a_branch2b
I0630 01:43:37.381307 29015 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:43:37.381309 29015 net.cpp:163] Memory required for data: 95477844
I0630 01:43:37.381312 29015 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 01:43:37.381316 29015 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 01:43:37.381319 29015 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 01:43:37.381321 29015 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 01:43:37.381984 29015 net.cpp:148] Setting up res4a_branch2b/bn
I0630 01:43:37.381991 29015 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:43:37.381994 29015 net.cpp:163] Memory required for data: 100982868
I0630 01:43:37.381999 29015 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 01:43:37.382000 29015 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 01:43:37.382004 29015 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 01:43:37.382005 29015 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 01:43:37.382015 29015 net.cpp:148] Setting up res4a_branch2b/relu
I0630 01:43:37.382019 29015 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:43:37.382020 29015 net.cpp:163] Memory required for data: 106487892
I0630 01:43:37.382021 29015 layer_factory.hpp:77] Creating layer pool4
I0630 01:43:37.382025 29015 net.cpp:98] Creating Layer pool4
I0630 01:43:37.382027 29015 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 01:43:37.382030 29015 net.cpp:413] pool4 -> pool4
I0630 01:43:37.382064 29015 net.cpp:148] Setting up pool4
I0630 01:43:37.382068 29015 net.cpp:155] Top shape: 21 256 8 8 (344064)
I0630 01:43:37.382071 29015 net.cpp:163] Memory required for data: 107864148
I0630 01:43:37.382072 29015 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 01:43:37.382076 29015 net.cpp:98] Creating Layer res5a_branch2a
I0630 01:43:37.382078 29015 net.cpp:439] res5a_branch2a <- pool4
I0630 01:43:37.382081 29015 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 01:43:37.407178 29015 net.cpp:148] Setting up res5a_branch2a
I0630 01:43:37.407196 29015 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:43:37.407199 29015 net.cpp:163] Memory required for data: 110616660
I0630 01:43:37.407205 29015 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 01:43:37.407212 29015 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 01:43:37.407215 29015 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 01:43:37.407219 29015 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 01:43:37.407946 29015 net.cpp:148] Setting up res5a_branch2a/bn
I0630 01:43:37.407953 29015 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:43:37.407955 29015 net.cpp:163] Memory required for data: 113369172
I0630 01:43:37.407961 29015 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 01:43:37.407965 29015 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 01:43:37.407968 29015 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 01:43:37.407970 29015 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 01:43:37.407974 29015 net.cpp:148] Setting up res5a_branch2a/relu
I0630 01:43:37.407977 29015 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:43:37.407979 29015 net.cpp:163] Memory required for data: 116121684
I0630 01:43:37.407980 29015 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 01:43:37.407992 29015 net.cpp:98] Creating Layer res5a_branch2b
I0630 01:43:37.407995 29015 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 01:43:37.407999 29015 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 01:43:37.421036 29015 net.cpp:148] Setting up res5a_branch2b
I0630 01:43:37.421057 29015 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:43:37.421059 29015 net.cpp:163] Memory required for data: 118874196
I0630 01:43:37.421072 29015 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 01:43:37.421079 29015 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 01:43:37.421083 29015 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 01:43:37.421087 29015 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 01:43:37.421905 29015 net.cpp:148] Setting up res5a_branch2b/bn
I0630 01:43:37.421913 29015 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:43:37.421916 29015 net.cpp:163] Memory required for data: 121626708
I0630 01:43:37.421921 29015 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 01:43:37.421926 29015 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 01:43:37.421927 29015 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 01:43:37.421931 29015 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 01:43:37.421934 29015 net.cpp:148] Setting up res5a_branch2b/relu
I0630 01:43:37.421937 29015 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:43:37.421938 29015 net.cpp:163] Memory required for data: 124379220
I0630 01:43:37.421941 29015 layer_factory.hpp:77] Creating layer pool5
I0630 01:43:37.421947 29015 net.cpp:98] Creating Layer pool5
I0630 01:43:37.421948 29015 net.cpp:439] pool5 <- res5a_branch2b/bn
I0630 01:43:37.421962 29015 net.cpp:413] pool5 -> pool5
I0630 01:43:37.421998 29015 net.cpp:148] Setting up pool5
I0630 01:43:37.422004 29015 net.cpp:155] Top shape: 21 512 1 1 (10752)
I0630 01:43:37.422008 29015 net.cpp:163] Memory required for data: 124422228
I0630 01:43:37.422011 29015 layer_factory.hpp:77] Creating layer fc10
I0630 01:43:37.422026 29015 net.cpp:98] Creating Layer fc10
I0630 01:43:37.422030 29015 net.cpp:439] fc10 <- pool5
I0630 01:43:37.422035 29015 net.cpp:413] fc10 -> fc10
I0630 01:43:37.422368 29015 net.cpp:148] Setting up fc10
I0630 01:43:37.422375 29015 net.cpp:155] Top shape: 21 10 (210)
I0630 01:43:37.422379 29015 net.cpp:163] Memory required for data: 124423068
I0630 01:43:37.422385 29015 layer_factory.hpp:77] Creating layer loss
I0630 01:43:37.422391 29015 net.cpp:98] Creating Layer loss
I0630 01:43:37.422395 29015 net.cpp:439] loss <- fc10
I0630 01:43:37.422399 29015 net.cpp:439] loss <- label
I0630 01:43:37.422405 29015 net.cpp:413] loss -> loss
I0630 01:43:37.422426 29015 layer_factory.hpp:77] Creating layer loss
I0630 01:43:37.422600 29015 net.cpp:148] Setting up loss
I0630 01:43:37.422606 29015 net.cpp:155] Top shape: (1)
I0630 01:43:37.422610 29015 net.cpp:158]     with loss weight 1
I0630 01:43:37.422624 29015 net.cpp:163] Memory required for data: 124423072
I0630 01:43:37.422629 29015 net.cpp:224] loss needs backward computation.
I0630 01:43:37.422631 29015 net.cpp:224] fc10 needs backward computation.
I0630 01:43:37.422633 29015 net.cpp:224] pool5 needs backward computation.
I0630 01:43:37.422636 29015 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 01:43:37.422641 29015 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 01:43:37.422646 29015 net.cpp:224] res5a_branch2b needs backward computation.
I0630 01:43:37.422649 29015 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 01:43:37.422653 29015 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 01:43:37.422657 29015 net.cpp:224] res5a_branch2a needs backward computation.
I0630 01:43:37.422662 29015 net.cpp:224] pool4 needs backward computation.
I0630 01:43:37.422667 29015 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 01:43:37.422670 29015 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 01:43:37.422674 29015 net.cpp:224] res4a_branch2b needs backward computation.
I0630 01:43:37.422678 29015 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 01:43:37.422683 29015 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 01:43:37.422686 29015 net.cpp:224] res4a_branch2a needs backward computation.
I0630 01:43:37.422690 29015 net.cpp:224] pool3 needs backward computation.
I0630 01:43:37.422694 29015 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 01:43:37.422698 29015 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 01:43:37.422703 29015 net.cpp:224] res3a_branch2b needs backward computation.
I0630 01:43:37.422706 29015 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 01:43:37.422710 29015 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 01:43:37.422714 29015 net.cpp:224] res3a_branch2a needs backward computation.
I0630 01:43:37.422719 29015 net.cpp:224] pool2 needs backward computation.
I0630 01:43:37.422722 29015 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 01:43:37.422726 29015 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 01:43:37.422730 29015 net.cpp:224] res2a_branch2b needs backward computation.
I0630 01:43:37.422734 29015 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 01:43:37.422739 29015 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 01:43:37.422742 29015 net.cpp:224] res2a_branch2a needs backward computation.
I0630 01:43:37.422746 29015 net.cpp:224] pool1 needs backward computation.
I0630 01:43:37.422749 29015 net.cpp:224] conv1b/relu needs backward computation.
I0630 01:43:37.422754 29015 net.cpp:224] conv1b/bn needs backward computation.
I0630 01:43:37.422757 29015 net.cpp:224] conv1b needs backward computation.
I0630 01:43:37.422766 29015 net.cpp:224] conv1a/relu needs backward computation.
I0630 01:43:37.422771 29015 net.cpp:224] conv1a/bn needs backward computation.
I0630 01:43:37.422775 29015 net.cpp:224] conv1a needs backward computation.
I0630 01:43:37.422780 29015 net.cpp:226] data/bias does not need backward computation.
I0630 01:43:37.422785 29015 net.cpp:226] data does not need backward computation.
I0630 01:43:37.422790 29015 net.cpp:268] This network produces output loss
I0630 01:43:37.422813 29015 net.cpp:288] Network initialization done.
I0630 01:43:37.423477 29015 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/test.prototxt
I0630 01:43:37.423676 29015 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0630 01:43:37.423799 29015 layer_factory.hpp:77] Creating layer data
I0630 01:43:37.423871 29015 net.cpp:98] Creating Layer data
I0630 01:43:37.423877 29015 net.cpp:413] data -> data
I0630 01:43:37.423883 29015 net.cpp:413] data -> label
I0630 01:43:37.438899 29045 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0630 01:43:37.439009 29015 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0630 01:43:37.439069 29015 data_layer.cpp:83] output data size: 50,3,32,32
I0630 01:43:37.441308 29015 net.cpp:148] Setting up data
I0630 01:43:37.441316 29015 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0630 01:43:37.441319 29015 net.cpp:155] Top shape: 50 (50)
I0630 01:43:37.441321 29015 net.cpp:163] Memory required for data: 614600
I0630 01:43:37.441323 29015 layer_factory.hpp:77] Creating layer label_data_1_split
I0630 01:43:37.441329 29015 net.cpp:98] Creating Layer label_data_1_split
I0630 01:43:37.441331 29015 net.cpp:439] label_data_1_split <- label
I0630 01:43:37.441334 29015 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0630 01:43:37.441339 29015 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0630 01:43:37.441341 29015 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0630 01:43:37.441479 29015 net.cpp:148] Setting up label_data_1_split
I0630 01:43:37.441491 29015 net.cpp:155] Top shape: 50 (50)
I0630 01:43:37.441496 29015 net.cpp:155] Top shape: 50 (50)
I0630 01:43:37.441500 29015 net.cpp:155] Top shape: 50 (50)
I0630 01:43:37.441504 29015 net.cpp:163] Memory required for data: 615200
I0630 01:43:37.441506 29015 layer_factory.hpp:77] Creating layer data/bias
I0630 01:43:37.441514 29015 net.cpp:98] Creating Layer data/bias
I0630 01:43:37.441517 29015 net.cpp:439] data/bias <- data
I0630 01:43:37.441522 29015 net.cpp:413] data/bias -> data/bias
I0630 01:43:37.441653 29015 net.cpp:148] Setting up data/bias
I0630 01:43:37.441659 29015 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0630 01:43:37.441663 29015 net.cpp:163] Memory required for data: 1229600
I0630 01:43:37.441669 29015 layer_factory.hpp:77] Creating layer conv1a
I0630 01:43:37.441675 29015 net.cpp:98] Creating Layer conv1a
I0630 01:43:37.441679 29015 net.cpp:439] conv1a <- data/bias
I0630 01:43:37.441684 29015 net.cpp:413] conv1a -> conv1a
I0630 01:43:37.442312 29015 net.cpp:148] Setting up conv1a
I0630 01:43:37.442320 29015 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:43:37.442322 29015 net.cpp:163] Memory required for data: 7783200
I0630 01:43:37.442327 29015 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 01:43:37.442330 29015 net.cpp:98] Creating Layer conv1a/bn
I0630 01:43:37.442333 29015 net.cpp:439] conv1a/bn <- conv1a
I0630 01:43:37.442337 29015 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 01:43:37.443209 29015 net.cpp:148] Setting up conv1a/bn
I0630 01:43:37.443217 29015 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:43:37.443219 29015 net.cpp:163] Memory required for data: 14336800
I0630 01:43:37.443225 29015 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 01:43:37.443236 29015 net.cpp:98] Creating Layer conv1a/relu
I0630 01:43:37.443238 29015 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 01:43:37.443241 29015 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 01:43:37.443245 29015 net.cpp:148] Setting up conv1a/relu
I0630 01:43:37.443248 29015 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:43:37.443251 29015 net.cpp:163] Memory required for data: 20890400
I0630 01:43:37.443255 29015 layer_factory.hpp:77] Creating layer conv1b
I0630 01:43:37.443262 29015 net.cpp:98] Creating Layer conv1b
I0630 01:43:37.443267 29015 net.cpp:439] conv1b <- conv1a/bn
I0630 01:43:37.443272 29015 net.cpp:413] conv1b -> conv1b
I0630 01:43:37.443768 29015 net.cpp:148] Setting up conv1b
I0630 01:43:37.443775 29015 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:43:37.443778 29015 net.cpp:163] Memory required for data: 27444000
I0630 01:43:37.443781 29015 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 01:43:37.443789 29015 net.cpp:98] Creating Layer conv1b/bn
I0630 01:43:37.443791 29015 net.cpp:439] conv1b/bn <- conv1b
I0630 01:43:37.443794 29015 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 01:43:37.444612 29015 net.cpp:148] Setting up conv1b/bn
I0630 01:43:37.444619 29015 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:43:37.444622 29015 net.cpp:163] Memory required for data: 33997600
I0630 01:43:37.444628 29015 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 01:43:37.444630 29015 net.cpp:98] Creating Layer conv1b/relu
I0630 01:43:37.444633 29015 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 01:43:37.444635 29015 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 01:43:37.444638 29015 net.cpp:148] Setting up conv1b/relu
I0630 01:43:37.444640 29015 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:43:37.444643 29015 net.cpp:163] Memory required for data: 40551200
I0630 01:43:37.444644 29015 layer_factory.hpp:77] Creating layer pool1
I0630 01:43:37.444648 29015 net.cpp:98] Creating Layer pool1
I0630 01:43:37.444649 29015 net.cpp:439] pool1 <- conv1b/bn
I0630 01:43:37.444651 29015 net.cpp:413] pool1 -> pool1
I0630 01:43:37.444701 29015 net.cpp:148] Setting up pool1
I0630 01:43:37.444708 29015 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:43:37.444711 29015 net.cpp:163] Memory required for data: 47104800
I0630 01:43:37.444715 29015 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 01:43:37.444725 29015 net.cpp:98] Creating Layer res2a_branch2a
I0630 01:43:37.444728 29015 net.cpp:439] res2a_branch2a <- pool1
I0630 01:43:37.444735 29015 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 01:43:37.445516 29015 net.cpp:148] Setting up res2a_branch2a
I0630 01:43:37.445523 29015 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:43:37.445525 29015 net.cpp:163] Memory required for data: 60212000
I0630 01:43:37.445529 29015 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 01:43:37.445533 29015 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 01:43:37.445535 29015 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 01:43:37.445538 29015 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 01:43:37.446315 29015 net.cpp:148] Setting up res2a_branch2a/bn
I0630 01:43:37.446321 29015 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:43:37.446323 29015 net.cpp:163] Memory required for data: 73319200
I0630 01:43:37.446328 29015 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 01:43:37.446331 29015 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 01:43:37.446333 29015 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 01:43:37.446336 29015 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 01:43:37.446338 29015 net.cpp:148] Setting up res2a_branch2a/relu
I0630 01:43:37.446341 29015 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:43:37.446342 29015 net.cpp:163] Memory required for data: 86426400
I0630 01:43:37.446344 29015 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 01:43:37.446348 29015 net.cpp:98] Creating Layer res2a_branch2b
I0630 01:43:37.446355 29015 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 01:43:37.446362 29015 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 01:43:37.446949 29015 net.cpp:148] Setting up res2a_branch2b
I0630 01:43:37.446955 29015 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:43:37.446957 29015 net.cpp:163] Memory required for data: 99533600
I0630 01:43:37.446960 29015 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 01:43:37.446964 29015 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 01:43:37.446966 29015 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 01:43:37.446969 29015 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 01:43:37.447753 29015 net.cpp:148] Setting up res2a_branch2b/bn
I0630 01:43:37.447760 29015 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:43:37.447762 29015 net.cpp:163] Memory required for data: 112640800
I0630 01:43:37.447767 29015 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 01:43:37.447770 29015 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 01:43:37.447772 29015 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 01:43:37.447774 29015 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 01:43:37.447778 29015 net.cpp:148] Setting up res2a_branch2b/relu
I0630 01:43:37.447780 29015 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:43:37.447782 29015 net.cpp:163] Memory required for data: 125748000
I0630 01:43:37.447784 29015 layer_factory.hpp:77] Creating layer pool2
I0630 01:43:37.447788 29015 net.cpp:98] Creating Layer pool2
I0630 01:43:37.447789 29015 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 01:43:37.447791 29015 net.cpp:413] pool2 -> pool2
I0630 01:43:37.447829 29015 net.cpp:148] Setting up pool2
I0630 01:43:37.447835 29015 net.cpp:155] Top shape: 50 64 16 16 (819200)
I0630 01:43:37.447839 29015 net.cpp:163] Memory required for data: 129024800
I0630 01:43:37.447841 29015 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 01:43:37.447847 29015 net.cpp:98] Creating Layer res3a_branch2a
I0630 01:43:37.447851 29015 net.cpp:439] res3a_branch2a <- pool2
I0630 01:43:37.447855 29015 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 01:43:37.450871 29015 net.cpp:148] Setting up res3a_branch2a
I0630 01:43:37.450891 29015 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:43:37.450894 29015 net.cpp:163] Memory required for data: 135578400
I0630 01:43:37.450899 29015 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 01:43:37.450906 29015 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 01:43:37.450908 29015 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 01:43:37.450912 29015 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 01:43:37.451638 29015 net.cpp:148] Setting up res3a_branch2a/bn
I0630 01:43:37.451645 29015 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:43:37.451647 29015 net.cpp:163] Memory required for data: 142132000
I0630 01:43:37.451655 29015 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 01:43:37.451663 29015 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 01:43:37.451665 29015 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 01:43:37.451668 29015 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 01:43:37.451673 29015 net.cpp:148] Setting up res3a_branch2a/relu
I0630 01:43:37.451674 29015 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:43:37.451676 29015 net.cpp:163] Memory required for data: 148685600
I0630 01:43:37.451678 29015 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 01:43:37.451684 29015 net.cpp:98] Creating Layer res3a_branch2b
I0630 01:43:37.451688 29015 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 01:43:37.451690 29015 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 01:43:37.452738 29015 net.cpp:148] Setting up res3a_branch2b
I0630 01:43:37.452744 29015 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:43:37.452745 29015 net.cpp:163] Memory required for data: 155239200
I0630 01:43:37.452749 29015 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 01:43:37.452764 29015 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 01:43:37.452766 29015 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 01:43:37.452769 29015 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 01:43:37.453426 29015 net.cpp:148] Setting up res3a_branch2b/bn
I0630 01:43:37.453433 29015 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:43:37.453435 29015 net.cpp:163] Memory required for data: 161792800
I0630 01:43:37.453440 29015 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 01:43:37.453444 29015 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 01:43:37.453447 29015 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 01:43:37.453449 29015 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 01:43:37.453454 29015 net.cpp:148] Setting up res3a_branch2b/relu
I0630 01:43:37.453457 29015 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:43:37.453459 29015 net.cpp:163] Memory required for data: 168346400
I0630 01:43:37.453461 29015 layer_factory.hpp:77] Creating layer pool3
I0630 01:43:37.453465 29015 net.cpp:98] Creating Layer pool3
I0630 01:43:37.453469 29015 net.cpp:439] pool3 <- res3a_branch2b/bn
I0630 01:43:37.453470 29015 net.cpp:413] pool3 -> pool3
I0630 01:43:37.453510 29015 net.cpp:148] Setting up pool3
I0630 01:43:37.453514 29015 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:43:37.453517 29015 net.cpp:163] Memory required for data: 174900000
I0630 01:43:37.453519 29015 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 01:43:37.453524 29015 net.cpp:98] Creating Layer res4a_branch2a
I0630 01:43:37.453526 29015 net.cpp:439] res4a_branch2a <- pool3
I0630 01:43:37.453529 29015 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 01:43:37.459630 29015 net.cpp:148] Setting up res4a_branch2a
I0630 01:43:37.459636 29015 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:43:37.459638 29015 net.cpp:163] Memory required for data: 188007200
I0630 01:43:37.459642 29015 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 01:43:37.459647 29015 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 01:43:37.459650 29015 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 01:43:37.459653 29015 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 01:43:37.460295 29015 net.cpp:148] Setting up res4a_branch2a/bn
I0630 01:43:37.460301 29015 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:43:37.460304 29015 net.cpp:163] Memory required for data: 201114400
I0630 01:43:37.460309 29015 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 01:43:37.460312 29015 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 01:43:37.460315 29015 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 01:43:37.460319 29015 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 01:43:37.460322 29015 net.cpp:148] Setting up res4a_branch2a/relu
I0630 01:43:37.460325 29015 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:43:37.460327 29015 net.cpp:163] Memory required for data: 214221600
I0630 01:43:37.460330 29015 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 01:43:37.460333 29015 net.cpp:98] Creating Layer res4a_branch2b
I0630 01:43:37.460336 29015 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 01:43:37.460338 29015 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 01:43:37.463541 29015 net.cpp:148] Setting up res4a_branch2b
I0630 01:43:37.463547 29015 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:43:37.463549 29015 net.cpp:163] Memory required for data: 227328800
I0630 01:43:37.463552 29015 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 01:43:37.463557 29015 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 01:43:37.463559 29015 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 01:43:37.463565 29015 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 01:43:37.464203 29015 net.cpp:148] Setting up res4a_branch2b/bn
I0630 01:43:37.464208 29015 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:43:37.464216 29015 net.cpp:163] Memory required for data: 240436000
I0630 01:43:37.464222 29015 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 01:43:37.464224 29015 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 01:43:37.464226 29015 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 01:43:37.464229 29015 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 01:43:37.464232 29015 net.cpp:148] Setting up res4a_branch2b/relu
I0630 01:43:37.464234 29015 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:43:37.464236 29015 net.cpp:163] Memory required for data: 253543200
I0630 01:43:37.464238 29015 layer_factory.hpp:77] Creating layer pool4
I0630 01:43:37.464243 29015 net.cpp:98] Creating Layer pool4
I0630 01:43:37.464246 29015 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 01:43:37.464248 29015 net.cpp:413] pool4 -> pool4
I0630 01:43:37.464288 29015 net.cpp:148] Setting up pool4
I0630 01:43:37.464293 29015 net.cpp:155] Top shape: 50 256 8 8 (819200)
I0630 01:43:37.464293 29015 net.cpp:163] Memory required for data: 256820000
I0630 01:43:37.464295 29015 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 01:43:37.464303 29015 net.cpp:98] Creating Layer res5a_branch2a
I0630 01:43:37.464305 29015 net.cpp:439] res5a_branch2a <- pool4
I0630 01:43:37.464308 29015 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 01:43:37.489150 29015 net.cpp:148] Setting up res5a_branch2a
I0630 01:43:37.489171 29015 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:43:37.489172 29015 net.cpp:163] Memory required for data: 263373600
I0630 01:43:37.489178 29015 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 01:43:37.489187 29015 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 01:43:37.489190 29015 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 01:43:37.489194 29015 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 01:43:37.489903 29015 net.cpp:148] Setting up res5a_branch2a/bn
I0630 01:43:37.489909 29015 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:43:37.489912 29015 net.cpp:163] Memory required for data: 269927200
I0630 01:43:37.489917 29015 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 01:43:37.489920 29015 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 01:43:37.489923 29015 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 01:43:37.489925 29015 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 01:43:37.489930 29015 net.cpp:148] Setting up res5a_branch2a/relu
I0630 01:43:37.489933 29015 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:43:37.489934 29015 net.cpp:163] Memory required for data: 276480800
I0630 01:43:37.489936 29015 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 01:43:37.489945 29015 net.cpp:98] Creating Layer res5a_branch2b
I0630 01:43:37.489948 29015 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 01:43:37.489951 29015 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 01:43:37.502794 29015 net.cpp:148] Setting up res5a_branch2b
I0630 01:43:37.502802 29015 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:43:37.502805 29015 net.cpp:163] Memory required for data: 283034400
I0630 01:43:37.502815 29015 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 01:43:37.502820 29015 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 01:43:37.502821 29015 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 01:43:37.502825 29015 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 01:43:37.503551 29015 net.cpp:148] Setting up res5a_branch2b/bn
I0630 01:43:37.503557 29015 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:43:37.503559 29015 net.cpp:163] Memory required for data: 289588000
I0630 01:43:37.503564 29015 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 01:43:37.503568 29015 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 01:43:37.503571 29015 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 01:43:37.503572 29015 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 01:43:37.503587 29015 net.cpp:148] Setting up res5a_branch2b/relu
I0630 01:43:37.503589 29015 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:43:37.503592 29015 net.cpp:163] Memory required for data: 296141600
I0630 01:43:37.503593 29015 layer_factory.hpp:77] Creating layer pool5
I0630 01:43:37.503597 29015 net.cpp:98] Creating Layer pool5
I0630 01:43:37.503599 29015 net.cpp:439] pool5 <- res5a_branch2b/bn
I0630 01:43:37.503602 29015 net.cpp:413] pool5 -> pool5
I0630 01:43:37.503626 29015 net.cpp:148] Setting up pool5
I0630 01:43:37.503629 29015 net.cpp:155] Top shape: 50 512 1 1 (25600)
I0630 01:43:37.503631 29015 net.cpp:163] Memory required for data: 296244000
I0630 01:43:37.503633 29015 layer_factory.hpp:77] Creating layer fc10
I0630 01:43:37.503643 29015 net.cpp:98] Creating Layer fc10
I0630 01:43:37.503644 29015 net.cpp:439] fc10 <- pool5
I0630 01:43:37.503648 29015 net.cpp:413] fc10 -> fc10
I0630 01:43:37.503885 29015 net.cpp:148] Setting up fc10
I0630 01:43:37.503890 29015 net.cpp:155] Top shape: 50 10 (500)
I0630 01:43:37.503891 29015 net.cpp:163] Memory required for data: 296246000
I0630 01:43:37.503895 29015 layer_factory.hpp:77] Creating layer fc10_fc10_0_split
I0630 01:43:37.503897 29015 net.cpp:98] Creating Layer fc10_fc10_0_split
I0630 01:43:37.503900 29015 net.cpp:439] fc10_fc10_0_split <- fc10
I0630 01:43:37.503901 29015 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0630 01:43:37.503906 29015 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0630 01:43:37.503908 29015 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0630 01:43:37.503969 29015 net.cpp:148] Setting up fc10_fc10_0_split
I0630 01:43:37.503973 29015 net.cpp:155] Top shape: 50 10 (500)
I0630 01:43:37.503976 29015 net.cpp:155] Top shape: 50 10 (500)
I0630 01:43:37.503978 29015 net.cpp:155] Top shape: 50 10 (500)
I0630 01:43:37.503979 29015 net.cpp:163] Memory required for data: 296252000
I0630 01:43:37.503981 29015 layer_factory.hpp:77] Creating layer loss
I0630 01:43:37.503984 29015 net.cpp:98] Creating Layer loss
I0630 01:43:37.503986 29015 net.cpp:439] loss <- fc10_fc10_0_split_0
I0630 01:43:37.503989 29015 net.cpp:439] loss <- label_data_1_split_0
I0630 01:43:37.503991 29015 net.cpp:413] loss -> loss
I0630 01:43:37.503995 29015 layer_factory.hpp:77] Creating layer loss
I0630 01:43:37.504101 29015 net.cpp:148] Setting up loss
I0630 01:43:37.504104 29015 net.cpp:155] Top shape: (1)
I0630 01:43:37.504106 29015 net.cpp:158]     with loss weight 1
I0630 01:43:37.504113 29015 net.cpp:163] Memory required for data: 296252004
I0630 01:43:37.504115 29015 layer_factory.hpp:77] Creating layer accuracy/top1
I0630 01:43:37.504118 29015 net.cpp:98] Creating Layer accuracy/top1
I0630 01:43:37.504120 29015 net.cpp:439] accuracy/top1 <- fc10_fc10_0_split_1
I0630 01:43:37.504123 29015 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0630 01:43:37.504125 29015 net.cpp:413] accuracy/top1 -> accuracy/top1
I0630 01:43:37.504129 29015 net.cpp:148] Setting up accuracy/top1
I0630 01:43:37.504132 29015 net.cpp:155] Top shape: (1)
I0630 01:43:37.504133 29015 net.cpp:163] Memory required for data: 296252008
I0630 01:43:37.504135 29015 layer_factory.hpp:77] Creating layer accuracy/top5
I0630 01:43:37.504142 29015 net.cpp:98] Creating Layer accuracy/top5
I0630 01:43:37.504143 29015 net.cpp:439] accuracy/top5 <- fc10_fc10_0_split_2
I0630 01:43:37.504146 29015 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0630 01:43:37.504150 29015 net.cpp:413] accuracy/top5 -> accuracy/top5
I0630 01:43:37.504154 29015 net.cpp:148] Setting up accuracy/top5
I0630 01:43:37.504158 29015 net.cpp:155] Top shape: (1)
I0630 01:43:37.504159 29015 net.cpp:163] Memory required for data: 296252012
I0630 01:43:37.504161 29015 net.cpp:226] accuracy/top5 does not need backward computation.
I0630 01:43:37.504164 29015 net.cpp:226] accuracy/top1 does not need backward computation.
I0630 01:43:37.504166 29015 net.cpp:224] loss needs backward computation.
I0630 01:43:37.504168 29015 net.cpp:224] fc10_fc10_0_split needs backward computation.
I0630 01:43:37.504170 29015 net.cpp:224] fc10 needs backward computation.
I0630 01:43:37.504179 29015 net.cpp:224] pool5 needs backward computation.
I0630 01:43:37.504180 29015 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 01:43:37.504182 29015 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 01:43:37.504184 29015 net.cpp:224] res5a_branch2b needs backward computation.
I0630 01:43:37.504186 29015 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 01:43:37.504189 29015 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 01:43:37.504190 29015 net.cpp:224] res5a_branch2a needs backward computation.
I0630 01:43:37.504192 29015 net.cpp:224] pool4 needs backward computation.
I0630 01:43:37.504194 29015 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 01:43:37.504196 29015 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 01:43:37.504199 29015 net.cpp:224] res4a_branch2b needs backward computation.
I0630 01:43:37.504200 29015 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 01:43:37.504201 29015 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 01:43:37.504204 29015 net.cpp:224] res4a_branch2a needs backward computation.
I0630 01:43:37.504206 29015 net.cpp:224] pool3 needs backward computation.
I0630 01:43:37.504209 29015 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 01:43:37.504209 29015 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 01:43:37.504211 29015 net.cpp:224] res3a_branch2b needs backward computation.
I0630 01:43:37.504214 29015 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 01:43:37.504215 29015 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 01:43:37.504217 29015 net.cpp:224] res3a_branch2a needs backward computation.
I0630 01:43:37.504220 29015 net.cpp:224] pool2 needs backward computation.
I0630 01:43:37.504221 29015 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 01:43:37.504223 29015 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 01:43:37.504226 29015 net.cpp:224] res2a_branch2b needs backward computation.
I0630 01:43:37.504228 29015 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 01:43:37.504231 29015 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 01:43:37.504233 29015 net.cpp:224] res2a_branch2a needs backward computation.
I0630 01:43:37.504235 29015 net.cpp:224] pool1 needs backward computation.
I0630 01:43:37.504238 29015 net.cpp:224] conv1b/relu needs backward computation.
I0630 01:43:37.504240 29015 net.cpp:224] conv1b/bn needs backward computation.
I0630 01:43:37.504243 29015 net.cpp:224] conv1b needs backward computation.
I0630 01:43:37.504246 29015 net.cpp:224] conv1a/relu needs backward computation.
I0630 01:43:37.504248 29015 net.cpp:224] conv1a/bn needs backward computation.
I0630 01:43:37.504251 29015 net.cpp:224] conv1a needs backward computation.
I0630 01:43:37.504253 29015 net.cpp:226] data/bias does not need backward computation.
I0630 01:43:37.504257 29015 net.cpp:226] label_data_1_split does not need backward computation.
I0630 01:43:37.504261 29015 net.cpp:226] data does not need backward computation.
I0630 01:43:37.504262 29015 net.cpp:268] This network produces output accuracy/top1
I0630 01:43:37.504264 29015 net.cpp:268] This network produces output accuracy/top5
I0630 01:43:37.504266 29015 net.cpp:268] This network produces output loss
I0630 01:43:37.504289 29015 net.cpp:288] Network initialization done.
I0630 01:43:37.504353 29015 solver.cpp:60] Solver scaffolding done.
I0630 01:43:37.507827 29015 caffe.cpp:145] Finetuning from training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2_iter_64000.caffemodel
I0630 01:43:37.534812 29015 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0630 01:43:37.534884 29015 data_layer.cpp:83] output data size: 21,3,32,32
I0630 01:43:37.993855 29015 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0630 01:43:37.993927 29015 data_layer.cpp:83] output data size: 21,3,32,32
I0630 01:43:38.481310 29015 parallel.cpp:334] Starting Optimization
I0630 01:43:38.481366 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:43:38.490753 29015 solver.cpp:413] Solving jacintonet11v2_train
I0630 01:43:38.490769 29015 solver.cpp:414] Learning Rate Policy: poly
I0630 01:43:38.492677 29015 solver.cpp:471] Iteration 0, Testing net (#0)
I0630 01:43:40.161898 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9171
I0630 01:43:40.161918 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9974
I0630 01:43:40.161926 29015 solver.cpp:544]     Test net output #2: loss = 0.2066 (* 1 = 0.2066 loss)
I0630 01:43:40.264863 29015 solver.cpp:290] Iteration 0 (0 iter/s, 1.77402s/100 iter), loss = 0
I0630 01:43:40.264889 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:40.264897 29015 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0630 01:43:40.275413 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.02
I0630 01:43:40.387543 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:43:42.448305 29015 solver.cpp:290] Iteration 100 (45.8012 iter/s, 2.18335s/100 iter), loss = 0
I0630 01:43:42.448330 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:42.448336 29015 sgd_solver.cpp:106] Iteration 100, lr = 0.00998437
I0630 01:43:44.505753 29015 solver.cpp:290] Iteration 200 (48.606 iter/s, 2.05736s/100 iter), loss = 0
I0630 01:43:44.505775 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:44.505781 29015 sgd_solver.cpp:106] Iteration 200, lr = 0.00996875
I0630 01:43:46.563999 29015 solver.cpp:290] Iteration 300 (48.5871 iter/s, 2.05816s/100 iter), loss = 0
I0630 01:43:46.564020 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:46.564028 29015 sgd_solver.cpp:106] Iteration 300, lr = 0.00995312
I0630 01:43:48.626799 29015 solver.cpp:290] Iteration 400 (48.4798 iter/s, 2.06271s/100 iter), loss = 0
I0630 01:43:48.626822 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:48.626829 29015 sgd_solver.cpp:106] Iteration 400, lr = 0.0099375
I0630 01:43:50.683352 29015 solver.cpp:290] Iteration 500 (48.6272 iter/s, 2.05646s/100 iter), loss = 0
I0630 01:43:50.683375 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:50.683384 29015 sgd_solver.cpp:106] Iteration 500, lr = 0.00992187
I0630 01:43:52.744160 29015 solver.cpp:290] Iteration 600 (48.5268 iter/s, 2.06072s/100 iter), loss = 0
I0630 01:43:52.744182 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:52.744189 29015 sgd_solver.cpp:106] Iteration 600, lr = 0.00990625
I0630 01:43:54.803282 29015 solver.cpp:290] Iteration 700 (48.5665 iter/s, 2.05903s/100 iter), loss = 0
I0630 01:43:54.803304 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:54.803311 29015 sgd_solver.cpp:106] Iteration 700, lr = 0.00989062
I0630 01:43:56.861779 29015 solver.cpp:290] Iteration 800 (48.5812 iter/s, 2.05841s/100 iter), loss = 0
I0630 01:43:56.861801 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:56.861809 29015 sgd_solver.cpp:106] Iteration 800, lr = 0.009875
I0630 01:43:58.926098 29015 solver.cpp:290] Iteration 900 (48.4442 iter/s, 2.06423s/100 iter), loss = 0
I0630 01:43:58.926121 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:58.926127 29015 sgd_solver.cpp:106] Iteration 900, lr = 0.00985937
I0630 01:44:00.965198 29015 solver.cpp:354] Sparsity after update:
I0630 01:44:00.966459 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:44:00.966466 29015 net.cpp:1851] conv1a_param_0(0.01) 
I0630 01:44:00.966480 29015 net.cpp:1851] conv1b_param_0(0.02) 
I0630 01:44:00.966485 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:44:00.966490 29015 net.cpp:1851] res2a_branch2a_param_0(0.02) 
I0630 01:44:00.966495 29015 net.cpp:1851] res2a_branch2b_param_0(0.02) 
I0630 01:44:00.966500 29015 net.cpp:1851] res3a_branch2a_param_0(0.02) 
I0630 01:44:00.966513 29015 net.cpp:1851] res3a_branch2b_param_0(0.02) 
I0630 01:44:00.966518 29015 net.cpp:1851] res4a_branch2a_param_0(0.02) 
I0630 01:44:00.966522 29015 net.cpp:1851] res4a_branch2b_param_0(0.02) 
I0630 01:44:00.966527 29015 net.cpp:1851] res5a_branch2a_param_0(0.02) 
I0630 01:44:00.966532 29015 net.cpp:1851] res5a_branch2b_param_0(0.02) 
I0630 01:44:00.966536 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (47057/2.3599e+06) 0.0199
I0630 01:44:00.966629 29015 solver.cpp:471] Iteration 1000, Testing net (#0)
I0630 01:44:02.608512 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.918
I0630 01:44:02.608531 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:44:02.608537 29015 solver.cpp:544]     Test net output #2: loss = 0.2049 (* 1 = 0.2049 loss)
I0630 01:44:02.628374 29015 solver.cpp:290] Iteration 1000 (27.0114 iter/s, 3.70214s/100 iter), loss = 0
I0630 01:44:02.628392 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:02.628404 29015 sgd_solver.cpp:106] Iteration 1000, lr = 0.00984375
I0630 01:44:02.628931 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.04
I0630 01:44:02.761804 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:44:04.821288 29015 solver.cpp:290] Iteration 1100 (45.6033 iter/s, 2.19283s/100 iter), loss = 0
I0630 01:44:04.821312 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:04.821318 29015 sgd_solver.cpp:106] Iteration 1100, lr = 0.00982813
I0630 01:44:06.881510 29015 solver.cpp:290] Iteration 1200 (48.5406 iter/s, 2.06013s/100 iter), loss = 0
I0630 01:44:06.881534 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:06.881543 29015 sgd_solver.cpp:106] Iteration 1200, lr = 0.0098125
I0630 01:44:08.952986 29015 solver.cpp:290] Iteration 1300 (48.2768 iter/s, 2.07139s/100 iter), loss = 0
I0630 01:44:08.953083 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:08.953094 29015 sgd_solver.cpp:106] Iteration 1300, lr = 0.00979687
I0630 01:44:11.011071 29015 solver.cpp:290] Iteration 1400 (48.5926 iter/s, 2.05793s/100 iter), loss = 0
I0630 01:44:11.011093 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:11.011101 29015 sgd_solver.cpp:106] Iteration 1400, lr = 0.00978125
I0630 01:44:13.066612 29015 solver.cpp:290] Iteration 1500 (48.6511 iter/s, 2.05545s/100 iter), loss = 0
I0630 01:44:13.066634 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:13.066642 29015 sgd_solver.cpp:106] Iteration 1500, lr = 0.00976562
I0630 01:44:15.122361 29015 solver.cpp:290] Iteration 1600 (48.6461 iter/s, 2.05566s/100 iter), loss = 0
I0630 01:44:15.122383 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:15.122390 29015 sgd_solver.cpp:106] Iteration 1600, lr = 0.00975
I0630 01:44:17.177491 29015 solver.cpp:290] Iteration 1700 (48.6608 iter/s, 2.05504s/100 iter), loss = 0
I0630 01:44:17.177515 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:17.177523 29015 sgd_solver.cpp:106] Iteration 1700, lr = 0.00973437
I0630 01:44:19.232775 29015 solver.cpp:290] Iteration 1800 (48.6571 iter/s, 2.0552s/100 iter), loss = 0
I0630 01:44:19.232796 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:19.232805 29015 sgd_solver.cpp:106] Iteration 1800, lr = 0.00971875
I0630 01:44:21.287411 29015 solver.cpp:290] Iteration 1900 (48.6725 iter/s, 2.05455s/100 iter), loss = 0
I0630 01:44:21.287436 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:21.287444 29015 sgd_solver.cpp:106] Iteration 1900, lr = 0.00970312
I0630 01:44:23.317715 29015 solver.cpp:354] Sparsity after update:
I0630 01:44:23.318977 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:44:23.318984 29015 net.cpp:1851] conv1a_param_0(0.02) 
I0630 01:44:23.318992 29015 net.cpp:1851] conv1b_param_0(0.0399) 
I0630 01:44:23.318995 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:44:23.318998 29015 net.cpp:1851] res2a_branch2a_param_0(0.04) 
I0630 01:44:23.319000 29015 net.cpp:1851] res2a_branch2b_param_0(0.0399) 
I0630 01:44:23.319002 29015 net.cpp:1851] res3a_branch2a_param_0(0.04) 
I0630 01:44:23.319005 29015 net.cpp:1851] res3a_branch2b_param_0(0.04) 
I0630 01:44:23.319007 29015 net.cpp:1851] res4a_branch2a_param_0(0.04) 
I0630 01:44:23.319010 29015 net.cpp:1851] res4a_branch2b_param_0(0.04) 
I0630 01:44:23.319012 29015 net.cpp:1851] res5a_branch2a_param_0(0.04) 
I0630 01:44:23.319015 29015 net.cpp:1851] res5a_branch2b_param_0(0.04) 
I0630 01:44:23.319016 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (94129/2.3599e+06) 0.0399
I0630 01:44:23.319104 29015 solver.cpp:471] Iteration 2000, Testing net (#0)
I0630 01:44:24.964457 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9175
I0630 01:44:24.964476 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:44:24.964481 29015 solver.cpp:544]     Test net output #2: loss = 0.2027 (* 1 = 0.2027 loss)
I0630 01:44:24.984683 29015 solver.cpp:290] Iteration 2000 (27.0479 iter/s, 3.69714s/100 iter), loss = 0
I0630 01:44:24.984699 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:24.984711 29015 sgd_solver.cpp:106] Iteration 2000, lr = 0.0096875
I0630 01:44:24.985227 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.06
I0630 01:44:25.141501 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:44:27.199268 29015 solver.cpp:290] Iteration 2100 (45.157 iter/s, 2.2145s/100 iter), loss = 0
I0630 01:44:27.199290 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:27.199296 29015 sgd_solver.cpp:106] Iteration 2100, lr = 0.00967188
I0630 01:44:29.249996 29015 solver.cpp:290] Iteration 2200 (48.7652 iter/s, 2.05064s/100 iter), loss = 0
I0630 01:44:29.250018 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:29.250044 29015 sgd_solver.cpp:106] Iteration 2200, lr = 0.00965625
I0630 01:44:31.302518 29015 solver.cpp:290] Iteration 2300 (48.7226 iter/s, 2.05243s/100 iter), loss = 0
I0630 01:44:31.302543 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:31.302552 29015 sgd_solver.cpp:106] Iteration 2300, lr = 0.00964062
I0630 01:44:33.366215 29015 solver.cpp:290] Iteration 2400 (48.4588 iter/s, 2.06361s/100 iter), loss = 0
I0630 01:44:33.366238 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:33.366245 29015 sgd_solver.cpp:106] Iteration 2400, lr = 0.009625
I0630 01:44:35.418342 29015 solver.cpp:290] Iteration 2500 (48.732 iter/s, 2.05204s/100 iter), loss = 0
I0630 01:44:35.418365 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:35.418371 29015 sgd_solver.cpp:106] Iteration 2500, lr = 0.00960938
I0630 01:44:37.475153 29015 solver.cpp:290] Iteration 2600 (48.621 iter/s, 2.05672s/100 iter), loss = 0
I0630 01:44:37.475177 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:37.475183 29015 sgd_solver.cpp:106] Iteration 2600, lr = 0.00959375
I0630 01:44:39.526470 29015 solver.cpp:290] Iteration 2700 (48.7513 iter/s, 2.05123s/100 iter), loss = 0
I0630 01:44:39.526537 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:39.526545 29015 sgd_solver.cpp:106] Iteration 2700, lr = 0.00957812
I0630 01:44:41.579648 29015 solver.cpp:290] Iteration 2800 (48.7081 iter/s, 2.05305s/100 iter), loss = 0
I0630 01:44:41.579670 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:41.579679 29015 sgd_solver.cpp:106] Iteration 2800, lr = 0.0095625
I0630 01:44:43.634135 29015 solver.cpp:290] Iteration 2900 (48.676 iter/s, 2.0544s/100 iter), loss = 0
I0630 01:44:43.634157 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:43.634165 29015 sgd_solver.cpp:106] Iteration 2900, lr = 0.00954687
I0630 01:44:45.672564 29015 solver.cpp:354] Sparsity after update:
I0630 01:44:45.673852 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:44:45.673858 29015 net.cpp:1851] conv1a_param_0(0.03) 
I0630 01:44:45.673867 29015 net.cpp:1851] conv1b_param_0(0.0595) 
I0630 01:44:45.673871 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:44:45.673873 29015 net.cpp:1851] res2a_branch2a_param_0(0.06) 
I0630 01:44:45.673877 29015 net.cpp:1851] res2a_branch2b_param_0(0.0599) 
I0630 01:44:45.673879 29015 net.cpp:1851] res3a_branch2a_param_0(0.06) 
I0630 01:44:45.673882 29015 net.cpp:1851] res3a_branch2b_param_0(0.06) 
I0630 01:44:45.673884 29015 net.cpp:1851] res4a_branch2a_param_0(0.06) 
I0630 01:44:45.673887 29015 net.cpp:1851] res4a_branch2b_param_0(0.06) 
I0630 01:44:45.673889 29015 net.cpp:1851] res5a_branch2a_param_0(0.06) 
I0630 01:44:45.673892 29015 net.cpp:1851] res5a_branch2b_param_0(0.06) 
I0630 01:44:45.673893 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (141205/2.3599e+06) 0.0598
I0630 01:44:45.673981 29015 solver.cpp:471] Iteration 3000, Testing net (#0)
I0630 01:44:47.314412 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9176
I0630 01:44:47.314432 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9971
I0630 01:44:47.314437 29015 solver.cpp:544]     Test net output #2: loss = 0.2044 (* 1 = 0.2044 loss)
I0630 01:44:47.334811 29015 solver.cpp:290] Iteration 3000 (27.0231 iter/s, 3.70054s/100 iter), loss = 0
I0630 01:44:47.334836 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:47.334843 29015 sgd_solver.cpp:106] Iteration 3000, lr = 0.00953125
I0630 01:44:47.335373 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.08
I0630 01:44:47.510507 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:44:49.567606 29015 solver.cpp:290] Iteration 3100 (44.7887 iter/s, 2.2327s/100 iter), loss = 0
I0630 01:44:49.567628 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:49.567634 29015 sgd_solver.cpp:106] Iteration 3100, lr = 0.00951563
I0630 01:44:51.622885 29015 solver.cpp:290] Iteration 3200 (48.6572 iter/s, 2.05519s/100 iter), loss = 0
I0630 01:44:51.622908 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:51.622915 29015 sgd_solver.cpp:106] Iteration 3200, lr = 0.0095
I0630 01:44:53.676576 29015 solver.cpp:290] Iteration 3300 (48.6949 iter/s, 2.0536s/100 iter), loss = 0
I0630 01:44:53.676599 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:53.676605 29015 sgd_solver.cpp:106] Iteration 3300, lr = 0.00948437
I0630 01:44:55.734683 29015 solver.cpp:290] Iteration 3400 (48.5904 iter/s, 2.05802s/100 iter), loss = 0
I0630 01:44:55.734705 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:55.734712 29015 sgd_solver.cpp:106] Iteration 3400, lr = 0.00946875
I0630 01:44:57.792510 29015 solver.cpp:290] Iteration 3500 (48.5971 iter/s, 2.05774s/100 iter), loss = 0
I0630 01:44:57.792531 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:57.792538 29015 sgd_solver.cpp:106] Iteration 3500, lr = 0.00945312
I0630 01:44:59.850078 29015 solver.cpp:290] Iteration 3600 (48.6031 iter/s, 2.05748s/100 iter), loss = 0
I0630 01:44:59.850100 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:59.850123 29015 sgd_solver.cpp:106] Iteration 3600, lr = 0.0094375
I0630 01:45:01.905992 29015 solver.cpp:290] Iteration 3700 (48.6423 iter/s, 2.05583s/100 iter), loss = 0
I0630 01:45:01.906013 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:01.906020 29015 sgd_solver.cpp:106] Iteration 3700, lr = 0.00942187
I0630 01:45:03.961921 29015 solver.cpp:290] Iteration 3800 (48.6419 iter/s, 2.05584s/100 iter), loss = 0
I0630 01:45:03.961942 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:03.961951 29015 sgd_solver.cpp:106] Iteration 3800, lr = 0.00940625
I0630 01:45:06.019665 29015 solver.cpp:290] Iteration 3900 (48.599 iter/s, 2.05766s/100 iter), loss = 0
I0630 01:45:06.019688 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:06.019695 29015 sgd_solver.cpp:106] Iteration 3900, lr = 0.00939062
I0630 01:45:08.064386 29015 solver.cpp:354] Sparsity after update:
I0630 01:45:08.065672 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:45:08.065680 29015 net.cpp:1851] conv1a_param_0(0.04) 
I0630 01:45:08.065687 29015 net.cpp:1851] conv1b_param_0(0.0799) 
I0630 01:45:08.065690 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:45:08.065692 29015 net.cpp:1851] res2a_branch2a_param_0(0.08) 
I0630 01:45:08.065696 29015 net.cpp:1851] res2a_branch2b_param_0(0.08) 
I0630 01:45:08.065697 29015 net.cpp:1851] res3a_branch2a_param_0(0.08) 
I0630 01:45:08.065701 29015 net.cpp:1851] res3a_branch2b_param_0(0.08) 
I0630 01:45:08.065702 29015 net.cpp:1851] res4a_branch2a_param_0(0.08) 
I0630 01:45:08.065706 29015 net.cpp:1851] res4a_branch2b_param_0(0.08) 
I0630 01:45:08.065707 29015 net.cpp:1851] res5a_branch2a_param_0(0.08) 
I0630 01:45:08.065709 29015 net.cpp:1851] res5a_branch2b_param_0(0.08) 
I0630 01:45:08.065711 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (188276/2.3599e+06) 0.0798
I0630 01:45:08.065798 29015 solver.cpp:471] Iteration 4000, Testing net (#0)
I0630 01:45:09.704046 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9179
I0630 01:45:09.704115 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:45:09.704123 29015 solver.cpp:544]     Test net output #2: loss = 0.1999 (* 1 = 0.1999 loss)
I0630 01:45:09.723783 29015 solver.cpp:290] Iteration 4000 (26.9979 iter/s, 3.70399s/100 iter), loss = 0
I0630 01:45:09.723799 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:09.723814 29015 sgd_solver.cpp:106] Iteration 4000, lr = 0.009375
I0630 01:45:09.724346 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.1
I0630 01:45:09.914379 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:45:11.974668 29015 solver.cpp:290] Iteration 4100 (44.4287 iter/s, 2.2508s/100 iter), loss = 0
I0630 01:45:11.974690 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:11.974696 29015 sgd_solver.cpp:106] Iteration 4100, lr = 0.00935937
I0630 01:45:14.030148 29015 solver.cpp:290] Iteration 4200 (48.6525 iter/s, 2.05539s/100 iter), loss = 0
I0630 01:45:14.030170 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:14.030177 29015 sgd_solver.cpp:106] Iteration 4200, lr = 0.00934375
I0630 01:45:16.089253 29015 solver.cpp:290] Iteration 4300 (48.5669 iter/s, 2.05902s/100 iter), loss = 0
I0630 01:45:16.089275 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:16.089282 29015 sgd_solver.cpp:106] Iteration 4300, lr = 0.00932813
I0630 01:45:18.145922 29015 solver.cpp:290] Iteration 4400 (48.6244 iter/s, 2.05658s/100 iter), loss = 0
I0630 01:45:18.145943 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:18.145951 29015 sgd_solver.cpp:106] Iteration 4400, lr = 0.0093125
I0630 01:45:20.201972 29015 solver.cpp:290] Iteration 4500 (48.639 iter/s, 2.05596s/100 iter), loss = 0
I0630 01:45:20.202003 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:20.202009 29015 sgd_solver.cpp:106] Iteration 4500, lr = 0.00929687
I0630 01:45:22.256750 29015 solver.cpp:290] Iteration 4600 (48.6693 iter/s, 2.05468s/100 iter), loss = 0
I0630 01:45:22.256772 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:22.256778 29015 sgd_solver.cpp:106] Iteration 4600, lr = 0.00928125
I0630 01:45:24.309942 29015 solver.cpp:290] Iteration 4700 (48.7067 iter/s, 2.05311s/100 iter), loss = 0
I0630 01:45:24.309964 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:24.309972 29015 sgd_solver.cpp:106] Iteration 4700, lr = 0.00926562
I0630 01:45:26.367472 29015 solver.cpp:290] Iteration 4800 (48.604 iter/s, 2.05744s/100 iter), loss = 0
I0630 01:45:26.367494 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:26.367503 29015 sgd_solver.cpp:106] Iteration 4800, lr = 0.00925
I0630 01:45:28.423151 29015 solver.cpp:290] Iteration 4900 (48.6478 iter/s, 2.05559s/100 iter), loss = 0
I0630 01:45:28.423173 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:28.423180 29015 sgd_solver.cpp:106] Iteration 4900, lr = 0.00923437
I0630 01:45:30.457130 29015 solver.cpp:354] Sparsity after update:
I0630 01:45:30.458395 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:45:30.458401 29015 net.cpp:1851] conv1a_param_0(0.0492) 
I0630 01:45:30.458411 29015 net.cpp:1851] conv1b_param_0(0.0998) 
I0630 01:45:30.458415 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:45:30.458420 29015 net.cpp:1851] res2a_branch2a_param_0(0.1) 
I0630 01:45:30.458422 29015 net.cpp:1851] res2a_branch2b_param_0(0.0999) 
I0630 01:45:30.458425 29015 net.cpp:1851] res3a_branch2a_param_0(0.1) 
I0630 01:45:30.458426 29015 net.cpp:1851] res3a_branch2b_param_0(0.1) 
I0630 01:45:30.458428 29015 net.cpp:1851] res4a_branch2a_param_0(0.1) 
I0630 01:45:30.458431 29015 net.cpp:1851] res4a_branch2b_param_0(0.1) 
I0630 01:45:30.458432 29015 net.cpp:1851] res5a_branch2a_param_0(0.1) 
I0630 01:45:30.458434 29015 net.cpp:1851] res5a_branch2b_param_0(0.1) 
I0630 01:45:30.458436 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (235348/2.3599e+06) 0.0997
I0630 01:45:30.458539 29015 solver.cpp:471] Iteration 5000, Testing net (#0)
I0630 01:45:32.095787 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9172
I0630 01:45:32.095804 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:45:32.095809 29015 solver.cpp:544]     Test net output #2: loss = 0.2018 (* 1 = 0.2018 loss)
I0630 01:45:32.115591 29015 solver.cpp:290] Iteration 5000 (27.0834 iter/s, 3.6923s/100 iter), loss = 0
I0630 01:45:32.115612 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:32.115618 29015 sgd_solver.cpp:106] Iteration 5000, lr = 0.00921875
I0630 01:45:32.116143 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.12
I0630 01:45:32.328747 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:45:34.396525 29015 solver.cpp:290] Iteration 5100 (43.8435 iter/s, 2.28084s/100 iter), loss = 0
I0630 01:45:34.396548 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:34.396555 29015 sgd_solver.cpp:106] Iteration 5100, lr = 0.00920312
I0630 01:45:36.452087 29015 solver.cpp:290] Iteration 5200 (48.6506 iter/s, 2.05547s/100 iter), loss = 0
I0630 01:45:36.452109 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:36.452116 29015 sgd_solver.cpp:106] Iteration 5200, lr = 0.0091875
I0630 01:45:38.509613 29015 solver.cpp:290] Iteration 5300 (48.6041 iter/s, 2.05744s/100 iter), loss = 0
I0630 01:45:38.509635 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:38.509644 29015 sgd_solver.cpp:106] Iteration 5300, lr = 0.00917188
I0630 01:45:40.563597 29015 solver.cpp:290] Iteration 5400 (48.6879 iter/s, 2.0539s/100 iter), loss = 0
I0630 01:45:40.563657 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:40.563670 29015 sgd_solver.cpp:106] Iteration 5400, lr = 0.00915625
I0630 01:45:42.619472 29015 solver.cpp:290] Iteration 5500 (48.644 iter/s, 2.05575s/100 iter), loss = 0
I0630 01:45:42.619494 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:42.619501 29015 sgd_solver.cpp:106] Iteration 5500, lr = 0.00914062
I0630 01:45:44.672056 29015 solver.cpp:290] Iteration 5600 (48.7211 iter/s, 2.0525s/100 iter), loss = 0
I0630 01:45:44.672080 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:44.672089 29015 sgd_solver.cpp:106] Iteration 5600, lr = 0.009125
I0630 01:45:46.728898 29015 solver.cpp:290] Iteration 5700 (48.6203 iter/s, 2.05675s/100 iter), loss = 0
I0630 01:45:46.728922 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:46.728931 29015 sgd_solver.cpp:106] Iteration 5700, lr = 0.00910938
I0630 01:45:48.786849 29015 solver.cpp:290] Iteration 5800 (48.5941 iter/s, 2.05786s/100 iter), loss = 0
I0630 01:45:48.786870 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:48.786876 29015 sgd_solver.cpp:106] Iteration 5800, lr = 0.00909375
I0630 01:45:50.843174 29015 solver.cpp:290] Iteration 5900 (48.6325 iter/s, 2.05624s/100 iter), loss = 0
I0630 01:45:50.843197 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:50.843204 29015 sgd_solver.cpp:106] Iteration 5900, lr = 0.00907812
I0630 01:45:52.880007 29015 solver.cpp:354] Sparsity after update:
I0630 01:45:52.881274 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:45:52.881281 29015 net.cpp:1851] conv1a_param_0(0.0596) 
I0630 01:45:52.881290 29015 net.cpp:1851] conv1b_param_0(0.12) 
I0630 01:45:52.881294 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:45:52.881295 29015 net.cpp:1851] res2a_branch2a_param_0(0.12) 
I0630 01:45:52.881297 29015 net.cpp:1851] res2a_branch2b_param_0(0.12) 
I0630 01:45:52.881299 29015 net.cpp:1851] res3a_branch2a_param_0(0.12) 
I0630 01:45:52.881302 29015 net.cpp:1851] res3a_branch2b_param_0(0.12) 
I0630 01:45:52.881304 29015 net.cpp:1851] res4a_branch2a_param_0(0.12) 
I0630 01:45:52.881307 29015 net.cpp:1851] res4a_branch2b_param_0(0.12) 
I0630 01:45:52.881309 29015 net.cpp:1851] res5a_branch2a_param_0(0.12) 
I0630 01:45:52.881312 29015 net.cpp:1851] res5a_branch2b_param_0(0.12) 
I0630 01:45:52.881314 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (282414/2.3599e+06) 0.12
I0630 01:45:52.881402 29015 solver.cpp:471] Iteration 6000, Testing net (#0)
I0630 01:45:54.518863 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9187
I0630 01:45:54.518885 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9972
I0630 01:45:54.518892 29015 solver.cpp:544]     Test net output #2: loss = 0.2019 (* 1 = 0.2019 loss)
I0630 01:45:54.538581 29015 solver.cpp:290] Iteration 6000 (27.0616 iter/s, 3.69527s/100 iter), loss = 0
I0630 01:45:54.538600 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:54.538609 29015 sgd_solver.cpp:106] Iteration 6000, lr = 0.0090625
I0630 01:45:54.539093 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.14
I0630 01:45:54.773087 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:45:56.829032 29015 solver.cpp:290] Iteration 6100 (43.6613 iter/s, 2.29036s/100 iter), loss = 0
I0630 01:45:56.829066 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:56.829078 29015 sgd_solver.cpp:106] Iteration 6100, lr = 0.00904687
I0630 01:45:58.881546 29015 solver.cpp:290] Iteration 6200 (48.7231 iter/s, 2.05242s/100 iter), loss = 0
I0630 01:45:58.881567 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:58.881575 29015 sgd_solver.cpp:106] Iteration 6200, lr = 0.00903125
I0630 01:46:00.940690 29015 solver.cpp:290] Iteration 6300 (48.5659 iter/s, 2.05906s/100 iter), loss = 0
I0630 01:46:00.940713 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:00.940739 29015 sgd_solver.cpp:106] Iteration 6300, lr = 0.00901563
I0630 01:46:02.999428 29015 solver.cpp:290] Iteration 6400 (48.5756 iter/s, 2.05865s/100 iter), loss = 0
I0630 01:46:02.999450 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:02.999456 29015 sgd_solver.cpp:106] Iteration 6400, lr = 0.009
I0630 01:46:05.057929 29015 solver.cpp:290] Iteration 6500 (48.5811 iter/s, 2.05841s/100 iter), loss = 0
I0630 01:46:05.057950 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:05.057956 29015 sgd_solver.cpp:106] Iteration 6500, lr = 0.00898437
I0630 01:46:07.116565 29015 solver.cpp:290] Iteration 6600 (48.5779 iter/s, 2.05855s/100 iter), loss = 0
I0630 01:46:07.116587 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:07.116593 29015 sgd_solver.cpp:106] Iteration 6600, lr = 0.00896875
I0630 01:46:09.197156 29015 solver.cpp:290] Iteration 6700 (48.0653 iter/s, 2.0805s/100 iter), loss = 0
I0630 01:46:09.197178 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:09.197185 29015 sgd_solver.cpp:106] Iteration 6700, lr = 0.00895312
I0630 01:46:11.250587 29015 solver.cpp:290] Iteration 6800 (48.7011 iter/s, 2.05334s/100 iter), loss = 0
I0630 01:46:11.250668 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:11.250676 29015 sgd_solver.cpp:106] Iteration 6800, lr = 0.0089375
I0630 01:46:13.307732 29015 solver.cpp:290] Iteration 6900 (48.6145 iter/s, 2.057s/100 iter), loss = 0
I0630 01:46:13.307757 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:13.307765 29015 sgd_solver.cpp:106] Iteration 6900, lr = 0.00892187
I0630 01:46:15.345660 29015 solver.cpp:354] Sparsity after update:
I0630 01:46:15.346945 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:46:15.346951 29015 net.cpp:1851] conv1a_param_0(0.0696) 
I0630 01:46:15.346958 29015 net.cpp:1851] conv1b_param_0(0.14) 
I0630 01:46:15.346961 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:46:15.346963 29015 net.cpp:1851] res2a_branch2a_param_0(0.14) 
I0630 01:46:15.346966 29015 net.cpp:1851] res2a_branch2b_param_0(0.14) 
I0630 01:46:15.346967 29015 net.cpp:1851] res3a_branch2a_param_0(0.14) 
I0630 01:46:15.346968 29015 net.cpp:1851] res3a_branch2b_param_0(0.14) 
I0630 01:46:15.346971 29015 net.cpp:1851] res4a_branch2a_param_0(0.14) 
I0630 01:46:15.346972 29015 net.cpp:1851] res4a_branch2b_param_0(0.14) 
I0630 01:46:15.346974 29015 net.cpp:1851] res5a_branch2a_param_0(0.14) 
I0630 01:46:15.346976 29015 net.cpp:1851] res5a_branch2b_param_0(0.14) 
I0630 01:46:15.346978 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (329481/2.3599e+06) 0.14
I0630 01:46:15.347069 29015 solver.cpp:471] Iteration 7000, Testing net (#0)
I0630 01:46:16.986615 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9181
I0630 01:46:16.986636 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9971
I0630 01:46:16.986641 29015 solver.cpp:544]     Test net output #2: loss = 0.2022 (* 1 = 0.2022 loss)
I0630 01:46:17.010870 29015 solver.cpp:290] Iteration 7000 (27.0051 iter/s, 3.703s/100 iter), loss = 0
I0630 01:46:17.010887 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:17.010900 29015 sgd_solver.cpp:106] Iteration 7000, lr = 0.00890625
I0630 01:46:17.011425 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.16
I0630 01:46:17.262126 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:46:19.316997 29015 solver.cpp:290] Iteration 7100 (43.3644 iter/s, 2.30604s/100 iter), loss = 0
I0630 01:46:19.317018 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:19.317025 29015 sgd_solver.cpp:106] Iteration 7100, lr = 0.00889063
I0630 01:46:21.374128 29015 solver.cpp:290] Iteration 7200 (48.6134 iter/s, 2.05704s/100 iter), loss = 0
I0630 01:46:21.374150 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:21.374157 29015 sgd_solver.cpp:106] Iteration 7200, lr = 0.008875
I0630 01:46:23.427958 29015 solver.cpp:290] Iteration 7300 (48.6916 iter/s, 2.05374s/100 iter), loss = 0
I0630 01:46:23.427980 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:23.427986 29015 sgd_solver.cpp:106] Iteration 7300, lr = 0.00885937
I0630 01:46:25.483090 29015 solver.cpp:290] Iteration 7400 (48.6608 iter/s, 2.05504s/100 iter), loss = 0
I0630 01:46:25.483111 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:25.483119 29015 sgd_solver.cpp:106] Iteration 7400, lr = 0.00884375
I0630 01:46:27.538563 29015 solver.cpp:290] Iteration 7500 (48.6527 iter/s, 2.05538s/100 iter), loss = 0
I0630 01:46:27.538583 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:27.538590 29015 sgd_solver.cpp:106] Iteration 7500, lr = 0.00882812
I0630 01:46:29.595995 29015 solver.cpp:290] Iteration 7600 (48.6063 iter/s, 2.05734s/100 iter), loss = 0
I0630 01:46:29.596021 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:29.596030 29015 sgd_solver.cpp:106] Iteration 7600, lr = 0.0088125
I0630 01:46:31.652079 29015 solver.cpp:290] Iteration 7700 (48.6383 iter/s, 2.05599s/100 iter), loss = 0
I0630 01:46:31.652102 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:31.652125 29015 sgd_solver.cpp:106] Iteration 7700, lr = 0.00879687
I0630 01:46:33.708396 29015 solver.cpp:290] Iteration 7800 (48.6327 iter/s, 2.05623s/100 iter), loss = 0
I0630 01:46:33.708418 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:33.708425 29015 sgd_solver.cpp:106] Iteration 7800, lr = 0.00878125
I0630 01:46:35.767139 29015 solver.cpp:290] Iteration 7900 (48.5754 iter/s, 2.05866s/100 iter), loss = 0
I0630 01:46:35.767163 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:35.767169 29015 sgd_solver.cpp:106] Iteration 7900, lr = 0.00876562
I0630 01:46:37.806466 29015 solver.cpp:354] Sparsity after update:
I0630 01:46:37.807727 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:46:37.807734 29015 net.cpp:1851] conv1a_param_0(0.0796) 
I0630 01:46:37.807742 29015 net.cpp:1851] conv1b_param_0(0.16) 
I0630 01:46:37.807745 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:46:37.807749 29015 net.cpp:1851] res2a_branch2a_param_0(0.16) 
I0630 01:46:37.807750 29015 net.cpp:1851] res2a_branch2b_param_0(0.16) 
I0630 01:46:37.807752 29015 net.cpp:1851] res3a_branch2a_param_0(0.16) 
I0630 01:46:37.807755 29015 net.cpp:1851] res3a_branch2b_param_0(0.16) 
I0630 01:46:37.807757 29015 net.cpp:1851] res4a_branch2a_param_0(0.16) 
I0630 01:46:37.807760 29015 net.cpp:1851] res4a_branch2b_param_0(0.16) 
I0630 01:46:37.807762 29015 net.cpp:1851] res5a_branch2a_param_0(0.16) 
I0630 01:46:37.807765 29015 net.cpp:1851] res5a_branch2b_param_0(0.16) 
I0630 01:46:37.807766 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (376559/2.3599e+06) 0.16
I0630 01:46:37.807899 29015 solver.cpp:471] Iteration 8000, Testing net (#0)
I0630 01:46:39.444798 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9183
I0630 01:46:39.444818 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9972
I0630 01:46:39.444823 29015 solver.cpp:544]     Test net output #2: loss = 0.2034 (* 1 = 0.2034 loss)
I0630 01:46:39.464555 29015 solver.cpp:290] Iteration 8000 (27.0469 iter/s, 3.69728s/100 iter), loss = 0
I0630 01:46:39.464572 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:39.464584 29015 sgd_solver.cpp:106] Iteration 8000, lr = 0.00875
I0630 01:46:39.465090 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.18
I0630 01:46:39.735254 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:46:41.795044 29015 solver.cpp:290] Iteration 8100 (42.9111 iter/s, 2.3304s/100 iter), loss = 0
I0630 01:46:41.795126 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:41.795135 29015 sgd_solver.cpp:106] Iteration 8100, lr = 0.00873438
I0630 01:46:43.849437 29015 solver.cpp:290] Iteration 8200 (48.6797 iter/s, 2.05425s/100 iter), loss = 0
I0630 01:46:43.849462 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:43.849468 29015 sgd_solver.cpp:106] Iteration 8200, lr = 0.00871875
I0630 01:46:45.909889 29015 solver.cpp:290] Iteration 8300 (48.5351 iter/s, 2.06036s/100 iter), loss = 0
I0630 01:46:45.909914 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:45.909922 29015 sgd_solver.cpp:106] Iteration 8300, lr = 0.00870312
I0630 01:46:47.967157 29015 solver.cpp:290] Iteration 8400 (48.6102 iter/s, 2.05718s/100 iter), loss = 0
I0630 01:46:47.967180 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:47.967187 29015 sgd_solver.cpp:106] Iteration 8400, lr = 0.0086875
I0630 01:46:50.023327 29015 solver.cpp:290] Iteration 8500 (48.6362 iter/s, 2.05608s/100 iter), loss = 0
I0630 01:46:50.023350 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:50.023355 29015 sgd_solver.cpp:106] Iteration 8500, lr = 0.00867188
I0630 01:46:52.080648 29015 solver.cpp:290] Iteration 8600 (48.609 iter/s, 2.05723s/100 iter), loss = 0
I0630 01:46:52.080672 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:52.080680 29015 sgd_solver.cpp:106] Iteration 8600, lr = 0.00865625
I0630 01:46:54.140341 29015 solver.cpp:290] Iteration 8700 (48.553 iter/s, 2.05961s/100 iter), loss = 0
I0630 01:46:54.140363 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:54.140369 29015 sgd_solver.cpp:106] Iteration 8700, lr = 0.00864062
I0630 01:46:56.192591 29015 solver.cpp:290] Iteration 8800 (48.7291 iter/s, 2.05216s/100 iter), loss = 0
I0630 01:46:56.192613 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:56.192620 29015 sgd_solver.cpp:106] Iteration 8800, lr = 0.008625
I0630 01:46:58.251145 29015 solver.cpp:290] Iteration 8900 (48.5798 iter/s, 2.05847s/100 iter), loss = 0
I0630 01:46:58.251168 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:58.251174 29015 sgd_solver.cpp:106] Iteration 8900, lr = 0.00860937
I0630 01:47:00.287670 29015 solver.cpp:354] Sparsity after update:
I0630 01:47:00.288939 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:47:00.288946 29015 net.cpp:1851] conv1a_param_0(0.0896) 
I0630 01:47:00.288956 29015 net.cpp:1851] conv1b_param_0(0.18) 
I0630 01:47:00.288961 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:47:00.288966 29015 net.cpp:1851] res2a_branch2a_param_0(0.18) 
I0630 01:47:00.288970 29015 net.cpp:1851] res2a_branch2b_param_0(0.18) 
I0630 01:47:00.288975 29015 net.cpp:1851] res3a_branch2a_param_0(0.18) 
I0630 01:47:00.288980 29015 net.cpp:1851] res3a_branch2b_param_0(0.18) 
I0630 01:47:00.288985 29015 net.cpp:1851] res4a_branch2a_param_0(0.18) 
I0630 01:47:00.288990 29015 net.cpp:1851] res4a_branch2b_param_0(0.18) 
I0630 01:47:00.288995 29015 net.cpp:1851] res5a_branch2a_param_0(0.18) 
I0630 01:47:00.289000 29015 net.cpp:1851] res5a_branch2b_param_0(0.18) 
I0630 01:47:00.289005 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (423631/2.3599e+06) 0.18
I0630 01:47:00.289098 29015 solver.cpp:471] Iteration 9000, Testing net (#0)
I0630 01:47:01.927242 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9185
I0630 01:47:01.927260 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:47:01.927266 29015 solver.cpp:544]     Test net output #2: loss = 0.1984 (* 1 = 0.1984 loss)
I0630 01:47:01.947773 29015 solver.cpp:290] Iteration 9000 (27.0527 iter/s, 3.69649s/100 iter), loss = 0
I0630 01:47:01.947796 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:01.947805 29015 sgd_solver.cpp:106] Iteration 9000, lr = 0.00859375
I0630 01:47:01.948376 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.2
I0630 01:47:02.234014 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:47:04.292466 29015 solver.cpp:290] Iteration 9100 (42.6512 iter/s, 2.3446s/100 iter), loss = 0
I0630 01:47:04.292490 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:04.292495 29015 sgd_solver.cpp:106] Iteration 9100, lr = 0.00857813
I0630 01:47:06.349726 29015 solver.cpp:290] Iteration 9200 (48.6104 iter/s, 2.05717s/100 iter), loss = 0
I0630 01:47:06.349750 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:06.349759 29015 sgd_solver.cpp:106] Iteration 9200, lr = 0.0085625
I0630 01:47:08.420156 29015 solver.cpp:290] Iteration 9300 (48.3012 iter/s, 2.07034s/100 iter), loss = 0
I0630 01:47:08.420176 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:08.420184 29015 sgd_solver.cpp:106] Iteration 9300, lr = 0.00854687
I0630 01:47:10.476302 29015 solver.cpp:290] Iteration 9400 (48.6367 iter/s, 2.05606s/100 iter), loss = 0
I0630 01:47:10.476325 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:10.476332 29015 sgd_solver.cpp:106] Iteration 9400, lr = 0.00853125
I0630 01:47:12.535446 29015 solver.cpp:290] Iteration 9500 (48.566 iter/s, 2.05906s/100 iter), loss = 0
I0630 01:47:12.535501 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:12.535509 29015 sgd_solver.cpp:106] Iteration 9500, lr = 0.00851563
I0630 01:47:14.594506 29015 solver.cpp:290] Iteration 9600 (48.5686 iter/s, 2.05894s/100 iter), loss = 0
I0630 01:47:14.594527 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:14.594534 29015 sgd_solver.cpp:106] Iteration 9600, lr = 0.0085
I0630 01:47:16.652477 29015 solver.cpp:290] Iteration 9700 (48.5936 iter/s, 2.05788s/100 iter), loss = 0
I0630 01:47:16.652503 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:16.652511 29015 sgd_solver.cpp:106] Iteration 9700, lr = 0.00848437
I0630 01:47:18.706369 29015 solver.cpp:290] Iteration 9800 (48.6902 iter/s, 2.0538s/100 iter), loss = 0
I0630 01:47:18.706393 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:18.706398 29015 sgd_solver.cpp:106] Iteration 9800, lr = 0.00846875
I0630 01:47:20.763650 29015 solver.cpp:290] Iteration 9900 (48.6099 iter/s, 2.05719s/100 iter), loss = 0
I0630 01:47:20.763672 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:20.763679 29015 sgd_solver.cpp:106] Iteration 9900, lr = 0.00845312
I0630 01:47:22.800977 29015 solver.cpp:598] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_10000.caffemodel
I0630 01:47:22.826220 29015 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_10000.solverstate
I0630 01:47:22.833421 29015 solver.cpp:354] Sparsity after update:
I0630 01:47:22.834362 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:47:22.834369 29015 net.cpp:1851] conv1a_param_0(0.0996) 
I0630 01:47:22.834378 29015 net.cpp:1851] conv1b_param_0(0.2) 
I0630 01:47:22.834380 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:47:22.834383 29015 net.cpp:1851] res2a_branch2a_param_0(0.2) 
I0630 01:47:22.834385 29015 net.cpp:1851] res2a_branch2b_param_0(0.2) 
I0630 01:47:22.834388 29015 net.cpp:1851] res3a_branch2a_param_0(0.2) 
I0630 01:47:22.834388 29015 net.cpp:1851] res3a_branch2b_param_0(0.2) 
I0630 01:47:22.834390 29015 net.cpp:1851] res4a_branch2a_param_0(0.2) 
I0630 01:47:22.834393 29015 net.cpp:1851] res4a_branch2b_param_0(0.2) 
I0630 01:47:22.834394 29015 net.cpp:1851] res5a_branch2a_param_0(0.2) 
I0630 01:47:22.834398 29015 net.cpp:1851] res5a_branch2b_param_0(0.2) 
I0630 01:47:22.834399 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (470704/2.3599e+06) 0.199
I0630 01:47:22.834496 29015 solver.cpp:471] Iteration 10000, Testing net (#0)
I0630 01:47:24.471724 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9175
I0630 01:47:24.471743 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:47:24.471750 29015 solver.cpp:544]     Test net output #2: loss = 0.1996 (* 1 = 0.1996 loss)
I0630 01:47:24.491643 29015 solver.cpp:290] Iteration 10000 (26.8251 iter/s, 3.72786s/100 iter), loss = 0
I0630 01:47:24.491667 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:24.491673 29015 sgd_solver.cpp:106] Iteration 10000, lr = 0.0084375
I0630 01:47:24.492228 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.22
I0630 01:47:24.798264 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:47:26.852598 29015 solver.cpp:290] Iteration 10100 (42.3575 iter/s, 2.36086s/100 iter), loss = 0
I0630 01:47:26.852622 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:26.852628 29015 sgd_solver.cpp:106] Iteration 10100, lr = 0.00842187
I0630 01:47:28.906924 29015 solver.cpp:290] Iteration 10200 (48.6799 iter/s, 2.05424s/100 iter), loss = 0
I0630 01:47:28.906946 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:28.906954 29015 sgd_solver.cpp:106] Iteration 10200, lr = 0.00840625
I0630 01:47:30.960723 29015 solver.cpp:290] Iteration 10300 (48.6923 iter/s, 2.05371s/100 iter), loss = 0
I0630 01:47:30.960758 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:30.960765 29015 sgd_solver.cpp:106] Iteration 10300, lr = 0.00839063
I0630 01:47:33.019588 29015 solver.cpp:290] Iteration 10400 (48.5728 iter/s, 2.05876s/100 iter), loss = 0
I0630 01:47:33.019616 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:33.019625 29015 sgd_solver.cpp:106] Iteration 10400, lr = 0.008375
I0630 01:47:35.075844 29015 solver.cpp:290] Iteration 10500 (48.6343 iter/s, 2.05616s/100 iter), loss = 0
I0630 01:47:35.075871 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:35.075881 29015 sgd_solver.cpp:106] Iteration 10500, lr = 0.00835937
I0630 01:47:37.136994 29015 solver.cpp:290] Iteration 10600 (48.5187 iter/s, 2.06106s/100 iter), loss = 0
I0630 01:47:37.137017 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:37.137023 29015 sgd_solver.cpp:106] Iteration 10600, lr = 0.00834375
I0630 01:47:39.193583 29015 solver.cpp:290] Iteration 10700 (48.6263 iter/s, 2.0565s/100 iter), loss = 0
I0630 01:47:39.193603 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:39.193611 29015 sgd_solver.cpp:106] Iteration 10700, lr = 0.00832812
I0630 01:47:41.248898 29015 solver.cpp:290] Iteration 10800 (48.6563 iter/s, 2.05523s/100 iter), loss = 0
I0630 01:47:41.248920 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:41.248929 29015 sgd_solver.cpp:106] Iteration 10800, lr = 0.0083125
I0630 01:47:43.301059 29015 solver.cpp:290] Iteration 10900 (48.7312 iter/s, 2.05207s/100 iter), loss = 0
I0630 01:47:43.301126 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:43.301132 29015 sgd_solver.cpp:106] Iteration 10900, lr = 0.00829687
I0630 01:47:45.343042 29015 solver.cpp:354] Sparsity after update:
I0630 01:47:45.344326 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:47:45.344334 29015 net.cpp:1851] conv1a_param_0(0.11) 
I0630 01:47:45.344341 29015 net.cpp:1851] conv1b_param_0(0.22) 
I0630 01:47:45.344344 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:47:45.344347 29015 net.cpp:1851] res2a_branch2a_param_0(0.22) 
I0630 01:47:45.344348 29015 net.cpp:1851] res2a_branch2b_param_0(0.22) 
I0630 01:47:45.344350 29015 net.cpp:1851] res3a_branch2a_param_0(0.22) 
I0630 01:47:45.344352 29015 net.cpp:1851] res3a_branch2b_param_0(0.22) 
I0630 01:47:45.344354 29015 net.cpp:1851] res4a_branch2a_param_0(0.22) 
I0630 01:47:45.344357 29015 net.cpp:1851] res4a_branch2b_param_0(0.22) 
I0630 01:47:45.344358 29015 net.cpp:1851] res5a_branch2a_param_0(0.22) 
I0630 01:47:45.344360 29015 net.cpp:1851] res5a_branch2b_param_0(0.22) 
I0630 01:47:45.344362 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (517782/2.3599e+06) 0.219
I0630 01:47:45.344491 29015 solver.cpp:471] Iteration 11000, Testing net (#0)
I0630 01:47:46.984827 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.918
I0630 01:47:46.984844 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9969
I0630 01:47:46.984850 29015 solver.cpp:544]     Test net output #2: loss = 0.2025 (* 1 = 0.2025 loss)
I0630 01:47:47.006310 29015 solver.cpp:290] Iteration 11000 (26.99 iter/s, 3.70508s/100 iter), loss = 0
I0630 01:47:47.006328 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:47.006355 29015 sgd_solver.cpp:106] Iteration 11000, lr = 0.00828125
I0630 01:47:47.006877 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.24
I0630 01:47:47.335942 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:47:49.392742 29015 solver.cpp:290] Iteration 11100 (41.9052 iter/s, 2.38634s/100 iter), loss = 0
I0630 01:47:49.392767 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:49.392776 29015 sgd_solver.cpp:106] Iteration 11100, lr = 0.00826562
I0630 01:47:51.449079 29015 solver.cpp:290] Iteration 11200 (48.6323 iter/s, 2.05625s/100 iter), loss = 0
I0630 01:47:51.449102 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:51.449111 29015 sgd_solver.cpp:106] Iteration 11200, lr = 0.00825
I0630 01:47:53.501806 29015 solver.cpp:290] Iteration 11300 (48.7178 iter/s, 2.05264s/100 iter), loss = 0
I0630 01:47:53.501827 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:53.501833 29015 sgd_solver.cpp:106] Iteration 11300, lr = 0.00823438
I0630 01:47:55.556226 29015 solver.cpp:290] Iteration 11400 (48.6776 iter/s, 2.05433s/100 iter), loss = 0
I0630 01:47:55.556248 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:55.556257 29015 sgd_solver.cpp:106] Iteration 11400, lr = 0.00821875
I0630 01:47:57.610509 29015 solver.cpp:290] Iteration 11500 (48.6808 iter/s, 2.0542s/100 iter), loss = 0
I0630 01:47:57.610533 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:57.610543 29015 sgd_solver.cpp:106] Iteration 11500, lr = 0.00820312
I0630 01:47:59.669257 29015 solver.cpp:290] Iteration 11600 (48.5753 iter/s, 2.05866s/100 iter), loss = 0
I0630 01:47:59.669279 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:59.669286 29015 sgd_solver.cpp:106] Iteration 11600, lr = 0.0081875
I0630 01:48:01.722590 29015 solver.cpp:290] Iteration 11700 (48.7034 iter/s, 2.05324s/100 iter), loss = 0
I0630 01:48:01.722612 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:01.722620 29015 sgd_solver.cpp:106] Iteration 11700, lr = 0.00817188
I0630 01:48:03.775027 29015 solver.cpp:290] Iteration 11800 (48.7247 iter/s, 2.05235s/100 iter), loss = 0
I0630 01:48:03.775049 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:03.775072 29015 sgd_solver.cpp:106] Iteration 11800, lr = 0.00815625
I0630 01:48:05.831758 29015 solver.cpp:290] Iteration 11900 (48.6229 iter/s, 2.05664s/100 iter), loss = 0
I0630 01:48:05.831781 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:05.831789 29015 sgd_solver.cpp:106] Iteration 11900, lr = 0.00814062
I0630 01:48:07.876662 29015 solver.cpp:354] Sparsity after update:
I0630 01:48:07.877951 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:48:07.877959 29015 net.cpp:1851] conv1a_param_0(0.12) 
I0630 01:48:07.877969 29015 net.cpp:1851] conv1b_param_0(0.24) 
I0630 01:48:07.877974 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:48:07.877979 29015 net.cpp:1851] res2a_branch2a_param_0(0.24) 
I0630 01:48:07.877984 29015 net.cpp:1851] res2a_branch2b_param_0(0.24) 
I0630 01:48:07.877988 29015 net.cpp:1851] res3a_branch2a_param_0(0.24) 
I0630 01:48:07.877991 29015 net.cpp:1851] res3a_branch2b_param_0(0.24) 
I0630 01:48:07.877996 29015 net.cpp:1851] res4a_branch2a_param_0(0.24) 
I0630 01:48:07.878000 29015 net.cpp:1851] res4a_branch2b_param_0(0.24) 
I0630 01:48:07.878005 29015 net.cpp:1851] res5a_branch2a_param_0(0.24) 
I0630 01:48:07.878008 29015 net.cpp:1851] res5a_branch2b_param_0(0.24) 
I0630 01:48:07.878013 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (564848/2.3599e+06) 0.239
I0630 01:48:07.878103 29015 solver.cpp:471] Iteration 12000, Testing net (#0)
I0630 01:48:09.527745 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9181
I0630 01:48:09.527765 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:48:09.527770 29015 solver.cpp:544]     Test net output #2: loss = 0.1985 (* 1 = 0.1985 loss)
I0630 01:48:09.547580 29015 solver.cpp:290] Iteration 12000 (26.9129 iter/s, 3.71569s/100 iter), loss = 0
I0630 01:48:09.547598 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:09.547608 29015 sgd_solver.cpp:106] Iteration 12000, lr = 0.008125
I0630 01:48:09.548771 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.26
I0630 01:48:09.891808 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:48:11.955811 29015 solver.cpp:290] Iteration 12100 (41.5258 iter/s, 2.40814s/100 iter), loss = 0
I0630 01:48:11.955834 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:11.955842 29015 sgd_solver.cpp:106] Iteration 12100, lr = 0.00810937
I0630 01:48:14.009627 29015 solver.cpp:290] Iteration 12200 (48.692 iter/s, 2.05373s/100 iter), loss = 0
I0630 01:48:14.009732 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:14.009740 29015 sgd_solver.cpp:106] Iteration 12200, lr = 0.00809375
I0630 01:48:16.065171 29015 solver.cpp:290] Iteration 12300 (48.6529 iter/s, 2.05537s/100 iter), loss = 0
I0630 01:48:16.065192 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:16.065199 29015 sgd_solver.cpp:106] Iteration 12300, lr = 0.00807813
I0630 01:48:18.122608 29015 solver.cpp:290] Iteration 12400 (48.6062 iter/s, 2.05735s/100 iter), loss = 0
I0630 01:48:18.122630 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:18.122637 29015 sgd_solver.cpp:106] Iteration 12400, lr = 0.0080625
I0630 01:48:20.176175 29015 solver.cpp:290] Iteration 12500 (48.6978 iter/s, 2.05348s/100 iter), loss = 0
I0630 01:48:20.176198 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:20.176204 29015 sgd_solver.cpp:106] Iteration 12500, lr = 0.00804687
I0630 01:48:22.231140 29015 solver.cpp:290] Iteration 12600 (48.6647 iter/s, 2.05488s/100 iter), loss = 0
I0630 01:48:22.231163 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:22.231169 29015 sgd_solver.cpp:106] Iteration 12600, lr = 0.00803125
I0630 01:48:24.288444 29015 solver.cpp:290] Iteration 12700 (48.6094 iter/s, 2.05722s/100 iter), loss = 0
I0630 01:48:24.288466 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:24.288473 29015 sgd_solver.cpp:106] Iteration 12700, lr = 0.00801562
I0630 01:48:26.342401 29015 solver.cpp:290] Iteration 12800 (48.6887 iter/s, 2.05387s/100 iter), loss = 0
I0630 01:48:26.342427 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:26.342437 29015 sgd_solver.cpp:106] Iteration 12800, lr = 0.008
I0630 01:48:28.397506 29015 solver.cpp:290] Iteration 12900 (48.6614 iter/s, 2.05502s/100 iter), loss = 0
I0630 01:48:28.397534 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:28.397543 29015 sgd_solver.cpp:106] Iteration 12900, lr = 0.00798437
I0630 01:48:30.431874 29015 solver.cpp:354] Sparsity after update:
I0630 01:48:30.433161 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:48:30.433169 29015 net.cpp:1851] conv1a_param_0(0.13) 
I0630 01:48:30.433176 29015 net.cpp:1851] conv1b_param_0(0.26) 
I0630 01:48:30.433178 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:48:30.433181 29015 net.cpp:1851] res2a_branch2a_param_0(0.26) 
I0630 01:48:30.433182 29015 net.cpp:1851] res2a_branch2b_param_0(0.26) 
I0630 01:48:30.433184 29015 net.cpp:1851] res3a_branch2a_param_0(0.26) 
I0630 01:48:30.433187 29015 net.cpp:1851] res3a_branch2b_param_0(0.26) 
I0630 01:48:30.433188 29015 net.cpp:1851] res4a_branch2a_param_0(0.26) 
I0630 01:48:30.433190 29015 net.cpp:1851] res4a_branch2b_param_0(0.26) 
I0630 01:48:30.433192 29015 net.cpp:1851] res5a_branch2a_param_0(0.26) 
I0630 01:48:30.433194 29015 net.cpp:1851] res5a_branch2b_param_0(0.26) 
I0630 01:48:30.433197 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (611915/2.3599e+06) 0.259
I0630 01:48:30.433292 29015 solver.cpp:471] Iteration 13000, Testing net (#0)
I0630 01:48:32.072629 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9183
I0630 01:48:32.072649 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:48:32.072654 29015 solver.cpp:544]     Test net output #2: loss = 0.1994 (* 1 = 0.1994 loss)
I0630 01:48:32.093062 29015 solver.cpp:290] Iteration 13000 (27.0605 iter/s, 3.69542s/100 iter), loss = 0
I0630 01:48:32.093081 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:32.093091 29015 sgd_solver.cpp:106] Iteration 13000, lr = 0.00796875
I0630 01:48:32.093626 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.28
I0630 01:48:32.443626 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:48:34.501075 29015 solver.cpp:290] Iteration 13100 (41.5296 iter/s, 2.40792s/100 iter), loss = 0
I0630 01:48:34.501097 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:34.501121 29015 sgd_solver.cpp:106] Iteration 13100, lr = 0.00795313
I0630 01:48:36.560833 29015 solver.cpp:290] Iteration 13200 (48.5515 iter/s, 2.05967s/100 iter), loss = 0
I0630 01:48:36.560858 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:36.560866 29015 sgd_solver.cpp:106] Iteration 13200, lr = 0.0079375
I0630 01:48:38.621027 29015 solver.cpp:290] Iteration 13300 (48.5412 iter/s, 2.0601s/100 iter), loss = 0
I0630 01:48:38.621049 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:38.621057 29015 sgd_solver.cpp:106] Iteration 13300, lr = 0.00792187
I0630 01:48:40.675652 29015 solver.cpp:290] Iteration 13400 (48.6728 iter/s, 2.05454s/100 iter), loss = 0
I0630 01:48:40.675674 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:40.675683 29015 sgd_solver.cpp:106] Iteration 13400, lr = 0.00790625
I0630 01:48:42.729413 29015 solver.cpp:290] Iteration 13500 (48.6932 iter/s, 2.05367s/100 iter), loss = 0
I0630 01:48:42.729434 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:42.729440 29015 sgd_solver.cpp:106] Iteration 13500, lr = 0.00789062
I0630 01:48:44.785673 29015 solver.cpp:290] Iteration 13600 (48.634 iter/s, 2.05617s/100 iter), loss = 0
I0630 01:48:44.785758 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:44.785769 29015 sgd_solver.cpp:106] Iteration 13600, lr = 0.007875
I0630 01:48:46.843475 29015 solver.cpp:290] Iteration 13700 (48.599 iter/s, 2.05766s/100 iter), loss = 0
I0630 01:48:46.843498 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:46.843504 29015 sgd_solver.cpp:106] Iteration 13700, lr = 0.00785937
I0630 01:48:48.902039 29015 solver.cpp:290] Iteration 13800 (48.5797 iter/s, 2.05847s/100 iter), loss = 0
I0630 01:48:48.902060 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:48.902067 29015 sgd_solver.cpp:106] Iteration 13800, lr = 0.00784375
I0630 01:48:50.957772 29015 solver.cpp:290] Iteration 13900 (48.6465 iter/s, 2.05565s/100 iter), loss = 0
I0630 01:48:50.957798 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:50.957806 29015 sgd_solver.cpp:106] Iteration 13900, lr = 0.00782812
I0630 01:48:52.992187 29015 solver.cpp:354] Sparsity after update:
I0630 01:48:52.993450 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:48:52.993458 29015 net.cpp:1851] conv1a_param_0(0.139) 
I0630 01:48:52.993464 29015 net.cpp:1851] conv1b_param_0(0.28) 
I0630 01:48:52.993468 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:48:52.993470 29015 net.cpp:1851] res2a_branch2a_param_0(0.28) 
I0630 01:48:52.993474 29015 net.cpp:1851] res2a_branch2b_param_0(0.28) 
I0630 01:48:52.993475 29015 net.cpp:1851] res3a_branch2a_param_0(0.28) 
I0630 01:48:52.993479 29015 net.cpp:1851] res3a_branch2b_param_0(0.28) 
I0630 01:48:52.993480 29015 net.cpp:1851] res4a_branch2a_param_0(0.28) 
I0630 01:48:52.993482 29015 net.cpp:1851] res4a_branch2b_param_0(0.28) 
I0630 01:48:52.993484 29015 net.cpp:1851] res5a_branch2a_param_0(0.28) 
I0630 01:48:52.993486 29015 net.cpp:1851] res5a_branch2b_param_0(0.28) 
I0630 01:48:52.993489 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (658987/2.3599e+06) 0.279
I0630 01:48:52.993578 29015 solver.cpp:471] Iteration 14000, Testing net (#0)
I0630 01:48:54.631808 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9186
I0630 01:48:54.631826 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:48:54.631831 29015 solver.cpp:544]     Test net output #2: loss = 0.1984 (* 1 = 0.1984 loss)
I0630 01:48:54.652035 29015 solver.cpp:290] Iteration 14000 (27.07 iter/s, 3.69413s/100 iter), loss = 0
I0630 01:48:54.652053 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:54.652063 29015 sgd_solver.cpp:106] Iteration 14000, lr = 0.0078125
I0630 01:48:54.652616 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.3
I0630 01:48:55.030233 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:48:57.088340 29015 solver.cpp:290] Iteration 14100 (41.0474 iter/s, 2.43621s/100 iter), loss = 0
I0630 01:48:57.088362 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:57.088371 29015 sgd_solver.cpp:106] Iteration 14100, lr = 0.00779688
I0630 01:48:59.142227 29015 solver.cpp:290] Iteration 14200 (48.6902 iter/s, 2.0538s/100 iter), loss = 0
I0630 01:48:59.142251 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:59.142259 29015 sgd_solver.cpp:106] Iteration 14200, lr = 0.00778125
I0630 01:49:01.196597 29015 solver.cpp:290] Iteration 14300 (48.6788 iter/s, 2.05428s/100 iter), loss = 0
I0630 01:49:01.196620 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:01.196630 29015 sgd_solver.cpp:106] Iteration 14300, lr = 0.00776563
I0630 01:49:03.257863 29015 solver.cpp:290] Iteration 14400 (48.516 iter/s, 2.06118s/100 iter), loss = 0
I0630 01:49:03.257886 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:03.257894 29015 sgd_solver.cpp:106] Iteration 14400, lr = 0.00775
I0630 01:49:05.316495 29015 solver.cpp:290] Iteration 14500 (48.5781 iter/s, 2.05854s/100 iter), loss = 0
I0630 01:49:05.316517 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:05.316545 29015 sgd_solver.cpp:106] Iteration 14500, lr = 0.00773437
I0630 01:49:07.383358 29015 solver.cpp:290] Iteration 14600 (48.3846 iter/s, 2.06678s/100 iter), loss = 0
I0630 01:49:07.383379 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:07.383386 29015 sgd_solver.cpp:106] Iteration 14600, lr = 0.00771875
I0630 01:49:09.439498 29015 solver.cpp:290] Iteration 14700 (48.6369 iter/s, 2.05605s/100 iter), loss = 0
I0630 01:49:09.439520 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:09.439528 29015 sgd_solver.cpp:106] Iteration 14700, lr = 0.00770312
I0630 01:49:11.494148 29015 solver.cpp:290] Iteration 14800 (48.6722 iter/s, 2.05456s/100 iter), loss = 0
I0630 01:49:11.494169 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:11.494176 29015 sgd_solver.cpp:106] Iteration 14800, lr = 0.0076875
I0630 01:49:13.548177 29015 solver.cpp:290] Iteration 14900 (48.6869 iter/s, 2.05394s/100 iter), loss = 0
I0630 01:49:13.548198 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:13.548207 29015 sgd_solver.cpp:106] Iteration 14900, lr = 0.00767187
I0630 01:49:15.584455 29015 solver.cpp:354] Sparsity after update:
I0630 01:49:15.585728 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:49:15.585736 29015 net.cpp:1851] conv1a_param_0(0.15) 
I0630 01:49:15.585746 29015 net.cpp:1851] conv1b_param_0(0.3) 
I0630 01:49:15.585749 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:49:15.585753 29015 net.cpp:1851] res2a_branch2a_param_0(0.3) 
I0630 01:49:15.585757 29015 net.cpp:1851] res2a_branch2b_param_0(0.3) 
I0630 01:49:15.585762 29015 net.cpp:1851] res3a_branch2a_param_0(0.3) 
I0630 01:49:15.585767 29015 net.cpp:1851] res3a_branch2b_param_0(0.3) 
I0630 01:49:15.585772 29015 net.cpp:1851] res4a_branch2a_param_0(0.3) 
I0630 01:49:15.585777 29015 net.cpp:1851] res4a_branch2b_param_0(0.3) 
I0630 01:49:15.585782 29015 net.cpp:1851] res5a_branch2a_param_0(0.3) 
I0630 01:49:15.585786 29015 net.cpp:1851] res5a_branch2b_param_0(0.3) 
I0630 01:49:15.585791 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (706065/2.3599e+06) 0.299
I0630 01:49:15.585885 29015 solver.cpp:471] Iteration 15000, Testing net (#0)
I0630 01:49:17.224720 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9185
I0630 01:49:17.224740 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:49:17.224745 29015 solver.cpp:544]     Test net output #2: loss = 0.2003 (* 1 = 0.2003 loss)
I0630 01:49:17.247357 29015 solver.cpp:290] Iteration 15000 (27.034 iter/s, 3.69905s/100 iter), loss = 0
I0630 01:49:17.247377 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:17.247386 29015 sgd_solver.cpp:106] Iteration 15000, lr = 0.00765625
I0630 01:49:17.247922 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.32
I0630 01:49:17.648810 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:49:19.712558 29015 solver.cpp:290] Iteration 15100 (40.5662 iter/s, 2.4651s/100 iter), loss = 0
I0630 01:49:19.712580 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:19.712589 29015 sgd_solver.cpp:106] Iteration 15100, lr = 0.00764062
I0630 01:49:21.772344 29015 solver.cpp:290] Iteration 15200 (48.5508 iter/s, 2.0597s/100 iter), loss = 0
I0630 01:49:21.772366 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:21.772373 29015 sgd_solver.cpp:106] Iteration 15200, lr = 0.007625
I0630 01:49:23.826511 29015 solver.cpp:290] Iteration 15300 (48.6836 iter/s, 2.05408s/100 iter), loss = 0
I0630 01:49:23.826537 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:23.826546 29015 sgd_solver.cpp:106] Iteration 15300, lr = 0.00760937
I0630 01:49:25.880478 29015 solver.cpp:290] Iteration 15400 (48.6884 iter/s, 2.05388s/100 iter), loss = 0
I0630 01:49:25.880502 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:25.880509 29015 sgd_solver.cpp:106] Iteration 15400, lr = 0.00759375
I0630 01:49:27.935227 29015 solver.cpp:290] Iteration 15500 (48.6699 iter/s, 2.05466s/100 iter), loss = 0
I0630 01:49:27.935250 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:27.935256 29015 sgd_solver.cpp:106] Iteration 15500, lr = 0.00757812
I0630 01:49:29.988183 29015 solver.cpp:290] Iteration 15600 (48.7123 iter/s, 2.05287s/100 iter), loss = 0
I0630 01:49:29.988205 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:29.988212 29015 sgd_solver.cpp:106] Iteration 15600, lr = 0.0075625
I0630 01:49:32.049327 29015 solver.cpp:290] Iteration 15700 (48.5188 iter/s, 2.06106s/100 iter), loss = 0
I0630 01:49:32.049350 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:32.049356 29015 sgd_solver.cpp:106] Iteration 15700, lr = 0.00754687
I0630 01:49:34.102576 29015 solver.cpp:290] Iteration 15800 (48.7053 iter/s, 2.05316s/100 iter), loss = 0
I0630 01:49:34.102598 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:34.102605 29015 sgd_solver.cpp:106] Iteration 15800, lr = 0.00753125
I0630 01:49:36.162233 29015 solver.cpp:290] Iteration 15900 (48.5538 iter/s, 2.05957s/100 iter), loss = 0
I0630 01:49:36.162271 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:36.162278 29015 sgd_solver.cpp:106] Iteration 15900, lr = 0.00751562
I0630 01:49:38.195665 29015 solver.cpp:354] Sparsity after update:
I0630 01:49:38.196930 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:49:38.196938 29015 net.cpp:1851] conv1a_param_0(0.16) 
I0630 01:49:38.196945 29015 net.cpp:1851] conv1b_param_0(0.32) 
I0630 01:49:38.196947 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:49:38.196949 29015 net.cpp:1851] res2a_branch2a_param_0(0.32) 
I0630 01:49:38.196951 29015 net.cpp:1851] res2a_branch2b_param_0(0.32) 
I0630 01:49:38.196954 29015 net.cpp:1851] res3a_branch2a_param_0(0.32) 
I0630 01:49:38.196955 29015 net.cpp:1851] res3a_branch2b_param_0(0.32) 
I0630 01:49:38.196957 29015 net.cpp:1851] res4a_branch2a_param_0(0.32) 
I0630 01:49:38.196959 29015 net.cpp:1851] res4a_branch2b_param_0(0.32) 
I0630 01:49:38.196961 29015 net.cpp:1851] res5a_branch2a_param_0(0.32) 
I0630 01:49:38.196964 29015 net.cpp:1851] res5a_branch2b_param_0(0.32) 
I0630 01:49:38.196965 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (753133/2.3599e+06) 0.319
I0630 01:49:38.197063 29015 solver.cpp:471] Iteration 16000, Testing net (#0)
I0630 01:49:39.837420 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9191
I0630 01:49:39.837438 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:49:39.837443 29015 solver.cpp:544]     Test net output #2: loss = 0.1977 (* 1 = 0.1977 loss)
I0630 01:49:39.858006 29015 solver.cpp:290] Iteration 16000 (27.059 iter/s, 3.69563s/100 iter), loss = 0
I0630 01:49:39.858024 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:39.858037 29015 sgd_solver.cpp:106] Iteration 16000, lr = 0.0075
I0630 01:49:39.858592 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.34
I0630 01:49:40.291604 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:49:42.349653 29015 solver.cpp:290] Iteration 16100 (40.1356 iter/s, 2.49155s/100 iter), loss = 0
I0630 01:49:42.349675 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:42.349681 29015 sgd_solver.cpp:106] Iteration 16100, lr = 0.00748438
I0630 01:49:44.406536 29015 solver.cpp:290] Iteration 16200 (48.6193 iter/s, 2.0568s/100 iter), loss = 0
I0630 01:49:44.406558 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:44.406565 29015 sgd_solver.cpp:106] Iteration 16200, lr = 0.00746875
I0630 01:49:46.467320 29015 solver.cpp:290] Iteration 16300 (48.5273 iter/s, 2.0607s/100 iter), loss = 0
I0630 01:49:46.467401 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:46.467408 29015 sgd_solver.cpp:106] Iteration 16300, lr = 0.00745312
I0630 01:49:48.526259 29015 solver.cpp:290] Iteration 16400 (48.5721 iter/s, 2.05879s/100 iter), loss = 0
I0630 01:49:48.526281 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:48.526289 29015 sgd_solver.cpp:106] Iteration 16400, lr = 0.0074375
I0630 01:49:50.583183 29015 solver.cpp:290] Iteration 16500 (48.6184 iter/s, 2.05683s/100 iter), loss = 0
I0630 01:49:50.583209 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:50.583216 29015 sgd_solver.cpp:106] Iteration 16500, lr = 0.00742187
I0630 01:49:52.644371 29015 solver.cpp:290] Iteration 16600 (48.5178 iter/s, 2.0611s/100 iter), loss = 0
I0630 01:49:52.644393 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:52.644399 29015 sgd_solver.cpp:106] Iteration 16600, lr = 0.00740625
I0630 01:49:54.697944 29015 solver.cpp:290] Iteration 16700 (48.6977 iter/s, 2.05349s/100 iter), loss = 0
I0630 01:49:54.697968 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:54.697973 29015 sgd_solver.cpp:106] Iteration 16700, lr = 0.00739062
I0630 01:49:56.753500 29015 solver.cpp:290] Iteration 16800 (48.6507 iter/s, 2.05547s/100 iter), loss = 0
I0630 01:49:56.753523 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:56.753530 29015 sgd_solver.cpp:106] Iteration 16800, lr = 0.007375
I0630 01:49:58.808023 29015 solver.cpp:290] Iteration 16900 (48.6752 iter/s, 2.05444s/100 iter), loss = 0
I0630 01:49:58.808046 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:58.808053 29015 sgd_solver.cpp:106] Iteration 16900, lr = 0.00735937
I0630 01:50:00.843952 29015 solver.cpp:354] Sparsity after update:
I0630 01:50:00.845247 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:50:00.845254 29015 net.cpp:1851] conv1a_param_0(0.17) 
I0630 01:50:00.845263 29015 net.cpp:1851] conv1b_param_0(0.34) 
I0630 01:50:00.845268 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:50:00.845273 29015 net.cpp:1851] res2a_branch2a_param_0(0.34) 
I0630 01:50:00.845278 29015 net.cpp:1851] res2a_branch2b_param_0(0.34) 
I0630 01:50:00.845281 29015 net.cpp:1851] res3a_branch2a_param_0(0.34) 
I0630 01:50:00.845285 29015 net.cpp:1851] res3a_branch2b_param_0(0.34) 
I0630 01:50:00.845289 29015 net.cpp:1851] res4a_branch2a_param_0(0.34) 
I0630 01:50:00.845293 29015 net.cpp:1851] res4a_branch2b_param_0(0.34) 
I0630 01:50:00.845297 29015 net.cpp:1851] res5a_branch2a_param_0(0.34) 
I0630 01:50:00.845301 29015 net.cpp:1851] res5a_branch2b_param_0(0.34) 
I0630 01:50:00.845305 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (800205/2.3599e+06) 0.339
I0630 01:50:00.845443 29015 solver.cpp:471] Iteration 17000, Testing net (#0)
I0630 01:50:02.483361 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9192
I0630 01:50:02.483381 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:50:02.483387 29015 solver.cpp:544]     Test net output #2: loss = 0.1984 (* 1 = 0.1984 loss)
I0630 01:50:02.504524 29015 solver.cpp:290] Iteration 17000 (27.0536 iter/s, 3.69637s/100 iter), loss = 0
I0630 01:50:02.504555 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:02.504565 29015 sgd_solver.cpp:106] Iteration 17000, lr = 0.00734375
I0630 01:50:02.505172 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.36
I0630 01:50:02.945701 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:50:05.007841 29015 solver.cpp:290] Iteration 17100 (39.9487 iter/s, 2.50321s/100 iter), loss = 0
I0630 01:50:05.007863 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:05.007872 29015 sgd_solver.cpp:106] Iteration 17100, lr = 0.00732813
I0630 01:50:07.063791 29015 solver.cpp:290] Iteration 17200 (48.6414 iter/s, 2.05586s/100 iter), loss = 0
I0630 01:50:07.063813 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:07.063838 29015 sgd_solver.cpp:106] Iteration 17200, lr = 0.0073125
I0630 01:50:09.140094 29015 solver.cpp:290] Iteration 17300 (48.1645 iter/s, 2.07622s/100 iter), loss = 0
I0630 01:50:09.140115 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:09.140123 29015 sgd_solver.cpp:106] Iteration 17300, lr = 0.00729688
I0630 01:50:11.197922 29015 solver.cpp:290] Iteration 17400 (48.597 iter/s, 2.05774s/100 iter), loss = 0
I0630 01:50:11.197944 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:11.197952 29015 sgd_solver.cpp:106] Iteration 17400, lr = 0.00728125
I0630 01:50:13.259915 29015 solver.cpp:290] Iteration 17500 (48.4988 iter/s, 2.06191s/100 iter), loss = 0
I0630 01:50:13.259938 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:13.259945 29015 sgd_solver.cpp:106] Iteration 17500, lr = 0.00726563
I0630 01:50:15.315577 29015 solver.cpp:290] Iteration 17600 (48.6482 iter/s, 2.05558s/100 iter), loss = 0
I0630 01:50:15.315599 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:15.315608 29015 sgd_solver.cpp:106] Iteration 17600, lr = 0.00725
I0630 01:50:17.370419 29015 solver.cpp:290] Iteration 17700 (48.6677 iter/s, 2.05475s/100 iter), loss = 0
I0630 01:50:17.370504 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:17.370517 29015 sgd_solver.cpp:106] Iteration 17700, lr = 0.00723437
I0630 01:50:19.430191 29015 solver.cpp:290] Iteration 17800 (48.5526 iter/s, 2.05962s/100 iter), loss = 0
I0630 01:50:19.430212 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:19.430219 29015 sgd_solver.cpp:106] Iteration 17800, lr = 0.00721875
I0630 01:50:21.488754 29015 solver.cpp:290] Iteration 17900 (48.5797 iter/s, 2.05847s/100 iter), loss = 0
I0630 01:50:21.488777 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:21.488785 29015 sgd_solver.cpp:106] Iteration 17900, lr = 0.00720312
I0630 01:50:23.523341 29015 solver.cpp:354] Sparsity after update:
I0630 01:50:23.524507 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:50:23.524520 29015 net.cpp:1851] conv1a_param_0(0.18) 
I0630 01:50:23.524530 29015 net.cpp:1851] conv1b_param_0(0.36) 
I0630 01:50:23.524533 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:50:23.524535 29015 net.cpp:1851] res2a_branch2a_param_0(0.36) 
I0630 01:50:23.524538 29015 net.cpp:1851] res2a_branch2b_param_0(0.36) 
I0630 01:50:23.524541 29015 net.cpp:1851] res3a_branch2a_param_0(0.36) 
I0630 01:50:23.524544 29015 net.cpp:1851] res3a_branch2b_param_0(0.36) 
I0630 01:50:23.524545 29015 net.cpp:1851] res4a_branch2a_param_0(0.36) 
I0630 01:50:23.524549 29015 net.cpp:1851] res4a_branch2b_param_0(0.36) 
I0630 01:50:23.524550 29015 net.cpp:1851] res5a_branch2a_param_0(0.36) 
I0630 01:50:23.524554 29015 net.cpp:1851] res5a_branch2b_param_0(0.36) 
I0630 01:50:23.524555 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (847278/2.3599e+06) 0.359
I0630 01:50:23.524670 29015 solver.cpp:471] Iteration 18000, Testing net (#0)
I0630 01:50:25.164630 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9189
I0630 01:50:25.164649 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9972
I0630 01:50:25.164655 29015 solver.cpp:544]     Test net output #2: loss = 0.1968 (* 1 = 0.1968 loss)
I0630 01:50:25.184950 29015 solver.cpp:290] Iteration 18000 (27.0558 iter/s, 3.69607s/100 iter), loss = 0
I0630 01:50:25.184969 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:25.184978 29015 sgd_solver.cpp:106] Iteration 18000, lr = 0.0071875
I0630 01:50:25.185508 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.38
I0630 01:50:25.647279 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:50:27.706604 29015 solver.cpp:290] Iteration 18100 (39.6581 iter/s, 2.52156s/100 iter), loss = 0
I0630 01:50:27.706626 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:27.706632 29015 sgd_solver.cpp:106] Iteration 18100, lr = 0.00717187
I0630 01:50:29.761991 29015 solver.cpp:290] Iteration 18200 (48.6547 iter/s, 2.0553s/100 iter), loss = 0
I0630 01:50:29.762013 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:29.762020 29015 sgd_solver.cpp:106] Iteration 18200, lr = 0.00715625
I0630 01:50:31.819017 29015 solver.cpp:290] Iteration 18300 (48.6159 iter/s, 2.05694s/100 iter), loss = 0
I0630 01:50:31.819041 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:31.819047 29015 sgd_solver.cpp:106] Iteration 18300, lr = 0.00714062
I0630 01:50:33.872910 29015 solver.cpp:290] Iteration 18400 (48.6901 iter/s, 2.05381s/100 iter), loss = 0
I0630 01:50:33.872931 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:33.872938 29015 sgd_solver.cpp:106] Iteration 18400, lr = 0.007125
I0630 01:50:35.927224 29015 solver.cpp:290] Iteration 18500 (48.6801 iter/s, 2.05423s/100 iter), loss = 0
I0630 01:50:35.927248 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:35.927254 29015 sgd_solver.cpp:106] Iteration 18500, lr = 0.00710937
I0630 01:50:37.989910 29015 solver.cpp:290] Iteration 18600 (48.4825 iter/s, 2.0626s/100 iter), loss = 0
I0630 01:50:37.989933 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:37.989959 29015 sgd_solver.cpp:106] Iteration 18600, lr = 0.00709375
I0630 01:50:40.047602 29015 solver.cpp:290] Iteration 18700 (48.6002 iter/s, 2.05761s/100 iter), loss = 0
I0630 01:50:40.047623 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:40.047631 29015 sgd_solver.cpp:106] Iteration 18700, lr = 0.00707812
I0630 01:50:42.102632 29015 solver.cpp:290] Iteration 18800 (48.6632 iter/s, 2.05494s/100 iter), loss = 0
I0630 01:50:42.102654 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:42.102660 29015 sgd_solver.cpp:106] Iteration 18800, lr = 0.0070625
I0630 01:50:44.156960 29015 solver.cpp:290] Iteration 18900 (48.6798 iter/s, 2.05424s/100 iter), loss = 0
I0630 01:50:44.156983 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:44.156991 29015 sgd_solver.cpp:106] Iteration 18900, lr = 0.00704687
I0630 01:50:46.191015 29015 solver.cpp:354] Sparsity after update:
I0630 01:50:46.192294 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:50:46.192302 29015 net.cpp:1851] conv1a_param_0(0.19) 
I0630 01:50:46.192311 29015 net.cpp:1851] conv1b_param_0(0.38) 
I0630 01:50:46.192312 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:50:46.192315 29015 net.cpp:1851] res2a_branch2a_param_0(0.38) 
I0630 01:50:46.192318 29015 net.cpp:1851] res2a_branch2b_param_0(0.38) 
I0630 01:50:46.192320 29015 net.cpp:1851] res3a_branch2a_param_0(0.38) 
I0630 01:50:46.192323 29015 net.cpp:1851] res3a_branch2b_param_0(0.38) 
I0630 01:50:46.192325 29015 net.cpp:1851] res4a_branch2a_param_0(0.38) 
I0630 01:50:46.192328 29015 net.cpp:1851] res4a_branch2b_param_0(0.38) 
I0630 01:50:46.192330 29015 net.cpp:1851] res5a_branch2a_param_0(0.38) 
I0630 01:50:46.192332 29015 net.cpp:1851] res5a_branch2b_param_0(0.38) 
I0630 01:50:46.192334 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (894356/2.3599e+06) 0.379
I0630 01:50:46.192421 29015 solver.cpp:471] Iteration 19000, Testing net (#0)
I0630 01:50:47.830130 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9174
I0630 01:50:47.830240 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9971
I0630 01:50:47.830248 29015 solver.cpp:544]     Test net output #2: loss = 0.2007 (* 1 = 0.2007 loss)
I0630 01:50:47.850164 29015 solver.cpp:290] Iteration 19000 (27.0777 iter/s, 3.69307s/100 iter), loss = 0
I0630 01:50:47.850183 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:47.850194 29015 sgd_solver.cpp:106] Iteration 19000, lr = 0.00703125
I0630 01:50:47.850739 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.4
I0630 01:50:48.334815 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:50:50.396459 29015 solver.cpp:290] Iteration 19100 (39.2743 iter/s, 2.5462s/100 iter), loss = 0
I0630 01:50:50.396486 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:50.396495 29015 sgd_solver.cpp:106] Iteration 19100, lr = 0.00701563
I0630 01:50:52.450054 29015 solver.cpp:290] Iteration 19200 (48.6972 iter/s, 2.0535s/100 iter), loss = 0
I0630 01:50:52.450076 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:52.450083 29015 sgd_solver.cpp:106] Iteration 19200, lr = 0.007
I0630 01:50:54.504504 29015 solver.cpp:290] Iteration 19300 (48.6769 iter/s, 2.05436s/100 iter), loss = 0
I0630 01:50:54.504528 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:54.504534 29015 sgd_solver.cpp:106] Iteration 19300, lr = 0.00698437
I0630 01:50:56.559964 29015 solver.cpp:290] Iteration 19400 (48.6531 iter/s, 2.05537s/100 iter), loss = 0
I0630 01:50:56.559994 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:56.560004 29015 sgd_solver.cpp:106] Iteration 19400, lr = 0.00696875
I0630 01:50:58.620131 29015 solver.cpp:290] Iteration 19500 (48.5419 iter/s, 2.06007s/100 iter), loss = 0
I0630 01:50:58.620153 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:58.620160 29015 sgd_solver.cpp:106] Iteration 19500, lr = 0.00695312
I0630 01:51:00.675395 29015 solver.cpp:290] Iteration 19600 (48.6576 iter/s, 2.05518s/100 iter), loss = 0
I0630 01:51:00.675418 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:00.675424 29015 sgd_solver.cpp:106] Iteration 19600, lr = 0.0069375
I0630 01:51:02.731236 29015 solver.cpp:290] Iteration 19700 (48.644 iter/s, 2.05575s/100 iter), loss = 0
I0630 01:51:02.731258 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:02.731264 29015 sgd_solver.cpp:106] Iteration 19700, lr = 0.00692187
I0630 01:51:04.789537 29015 solver.cpp:290] Iteration 19800 (48.5858 iter/s, 2.05822s/100 iter), loss = 0
I0630 01:51:04.789559 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:04.789566 29015 sgd_solver.cpp:106] Iteration 19800, lr = 0.00690625
I0630 01:51:06.848969 29015 solver.cpp:290] Iteration 19900 (48.5591 iter/s, 2.05935s/100 iter), loss = 0
I0630 01:51:06.848992 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:06.848999 29015 sgd_solver.cpp:106] Iteration 19900, lr = 0.00689062
I0630 01:51:08.904639 29015 solver.cpp:598] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_20000.caffemodel
I0630 01:51:08.921322 29015 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_20000.solverstate
I0630 01:51:08.928608 29015 solver.cpp:354] Sparsity after update:
I0630 01:51:08.929569 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:51:08.929577 29015 net.cpp:1851] conv1a_param_0(0.2) 
I0630 01:51:08.929584 29015 net.cpp:1851] conv1b_param_0(0.4) 
I0630 01:51:08.929587 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:51:08.929589 29015 net.cpp:1851] res2a_branch2a_param_0(0.4) 
I0630 01:51:08.929591 29015 net.cpp:1851] res2a_branch2b_param_0(0.4) 
I0630 01:51:08.929594 29015 net.cpp:1851] res3a_branch2a_param_0(0.4) 
I0630 01:51:08.929605 29015 net.cpp:1851] res3a_branch2b_param_0(0.4) 
I0630 01:51:08.929607 29015 net.cpp:1851] res4a_branch2a_param_0(0.4) 
I0630 01:51:08.929610 29015 net.cpp:1851] res4a_branch2b_param_0(0.4) 
I0630 01:51:08.929612 29015 net.cpp:1851] res5a_branch2a_param_0(0.4) 
I0630 01:51:08.929615 29015 net.cpp:1851] res5a_branch2b_param_0(0.4) 
I0630 01:51:08.929616 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (941419/2.3599e+06) 0.399
I0630 01:51:08.929716 29015 solver.cpp:471] Iteration 20000, Testing net (#0)
I0630 01:51:10.571337 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9177
I0630 01:51:10.571358 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9968
I0630 01:51:10.571364 29015 solver.cpp:544]     Test net output #2: loss = 0.2009 (* 1 = 0.2009 loss)
I0630 01:51:10.591573 29015 solver.cpp:290] Iteration 20000 (26.7204 iter/s, 3.74247s/100 iter), loss = 0
I0630 01:51:10.591601 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:10.591609 29015 sgd_solver.cpp:106] Iteration 20000, lr = 0.006875
I0630 01:51:10.592325 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.42
I0630 01:51:11.095958 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:51:13.152346 29015 solver.cpp:290] Iteration 20100 (39.0523 iter/s, 2.56067s/100 iter), loss = 0
I0630 01:51:13.152370 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:13.152379 29015 sgd_solver.cpp:106] Iteration 20100, lr = 0.00685938
I0630 01:51:15.210052 29015 solver.cpp:290] Iteration 20200 (48.5999 iter/s, 2.05762s/100 iter), loss = 0
I0630 01:51:15.210072 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:15.210081 29015 sgd_solver.cpp:106] Iteration 20200, lr = 0.00684375
I0630 01:51:17.263579 29015 solver.cpp:290] Iteration 20300 (48.6987 iter/s, 2.05344s/100 iter), loss = 0
I0630 01:51:17.263602 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:17.263610 29015 sgd_solver.cpp:106] Iteration 20300, lr = 0.00682813
I0630 01:51:19.322036 29015 solver.cpp:290] Iteration 20400 (48.5822 iter/s, 2.05837s/100 iter), loss = 0
I0630 01:51:19.322123 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:19.322131 29015 sgd_solver.cpp:106] Iteration 20400, lr = 0.0068125
I0630 01:51:21.383972 29015 solver.cpp:290] Iteration 20500 (48.5016 iter/s, 2.06179s/100 iter), loss = 0
I0630 01:51:21.383994 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:21.384001 29015 sgd_solver.cpp:106] Iteration 20500, lr = 0.00679688
I0630 01:51:23.442646 29015 solver.cpp:290] Iteration 20600 (48.5771 iter/s, 2.05859s/100 iter), loss = 0
I0630 01:51:23.442672 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:23.442679 29015 sgd_solver.cpp:106] Iteration 20600, lr = 0.00678125
I0630 01:51:25.504473 29015 solver.cpp:290] Iteration 20700 (48.5029 iter/s, 2.06173s/100 iter), loss = 0
I0630 01:51:25.504508 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:25.504519 29015 sgd_solver.cpp:106] Iteration 20700, lr = 0.00676562
I0630 01:51:27.566524 29015 solver.cpp:290] Iteration 20800 (48.4977 iter/s, 2.06195s/100 iter), loss = 0
I0630 01:51:27.566546 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:27.566553 29015 sgd_solver.cpp:106] Iteration 20800, lr = 0.00675
I0630 01:51:29.633307 29015 solver.cpp:290] Iteration 20900 (48.3865 iter/s, 2.06669s/100 iter), loss = 0
I0630 01:51:29.633339 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:29.633348 29015 sgd_solver.cpp:106] Iteration 20900, lr = 0.00673437
I0630 01:51:31.679366 29015 solver.cpp:354] Sparsity after update:
I0630 01:51:31.680635 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:51:31.680642 29015 net.cpp:1851] conv1a_param_0(0.21) 
I0630 01:51:31.680650 29015 net.cpp:1851] conv1b_param_0(0.42) 
I0630 01:51:31.680652 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:51:31.680655 29015 net.cpp:1851] res2a_branch2a_param_0(0.42) 
I0630 01:51:31.680656 29015 net.cpp:1851] res2a_branch2b_param_0(0.42) 
I0630 01:51:31.680658 29015 net.cpp:1851] res3a_branch2a_param_0(0.42) 
I0630 01:51:31.680660 29015 net.cpp:1851] res3a_branch2b_param_0(0.42) 
I0630 01:51:31.680662 29015 net.cpp:1851] res4a_branch2a_param_0(0.42) 
I0630 01:51:31.680665 29015 net.cpp:1851] res4a_branch2b_param_0(0.42) 
I0630 01:51:31.680666 29015 net.cpp:1851] res5a_branch2a_param_0(0.42) 
I0630 01:51:31.680668 29015 net.cpp:1851] res5a_branch2b_param_0(0.42) 
I0630 01:51:31.680670 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (988499/2.3599e+06) 0.419
I0630 01:51:31.680800 29015 solver.cpp:471] Iteration 21000, Testing net (#0)
I0630 01:51:33.330025 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.917
I0630 01:51:33.330046 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9969
I0630 01:51:33.330052 29015 solver.cpp:544]     Test net output #2: loss = 0.1994 (* 1 = 0.1994 loss)
I0630 01:51:33.349984 29015 solver.cpp:290] Iteration 21000 (26.9068 iter/s, 3.71653s/100 iter), loss = 0
I0630 01:51:33.350010 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:33.350018 29015 sgd_solver.cpp:106] Iteration 21000, lr = 0.00671875
I0630 01:51:33.350580 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.44
I0630 01:51:33.865296 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:51:35.925601 29015 solver.cpp:290] Iteration 21100 (38.8272 iter/s, 2.57551s/100 iter), loss = 0
I0630 01:51:35.925622 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:35.925631 29015 sgd_solver.cpp:106] Iteration 21100, lr = 0.00670313
I0630 01:51:37.986672 29015 solver.cpp:290] Iteration 21200 (48.5205 iter/s, 2.06098s/100 iter), loss = 0
I0630 01:51:37.986696 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:37.986703 29015 sgd_solver.cpp:106] Iteration 21200, lr = 0.0066875
I0630 01:51:40.046135 29015 solver.cpp:290] Iteration 21300 (48.5585 iter/s, 2.05937s/100 iter), loss = 0
I0630 01:51:40.046156 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:40.046177 29015 sgd_solver.cpp:106] Iteration 21300, lr = 0.00667187
I0630 01:51:42.099647 29015 solver.cpp:290] Iteration 21400 (48.6991 iter/s, 2.05342s/100 iter), loss = 0
I0630 01:51:42.099668 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:42.099674 29015 sgd_solver.cpp:106] Iteration 21400, lr = 0.00665625
I0630 01:51:44.156528 29015 solver.cpp:290] Iteration 21500 (48.6193 iter/s, 2.0568s/100 iter), loss = 0
I0630 01:51:44.156550 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:44.156559 29015 sgd_solver.cpp:106] Iteration 21500, lr = 0.00664062
I0630 01:51:46.215790 29015 solver.cpp:290] Iteration 21600 (48.5632 iter/s, 2.05917s/100 iter), loss = 0
I0630 01:51:46.215817 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:46.215836 29015 sgd_solver.cpp:106] Iteration 21600, lr = 0.006625
I0630 01:51:48.274821 29015 solver.cpp:290] Iteration 21700 (48.5687 iter/s, 2.05894s/100 iter), loss = 0
I0630 01:51:48.274847 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:48.274857 29015 sgd_solver.cpp:106] Iteration 21700, lr = 0.00660937
I0630 01:51:50.331146 29015 solver.cpp:290] Iteration 21800 (48.6326 iter/s, 2.05624s/100 iter), loss = 0
I0630 01:51:50.331212 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:50.331219 29015 sgd_solver.cpp:106] Iteration 21800, lr = 0.00659375
I0630 01:51:52.392341 29015 solver.cpp:290] Iteration 21900 (48.5186 iter/s, 2.06107s/100 iter), loss = 0
I0630 01:51:52.392362 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:52.392370 29015 sgd_solver.cpp:106] Iteration 21900, lr = 0.00657812
I0630 01:51:54.425671 29015 solver.cpp:354] Sparsity after update:
I0630 01:51:54.426946 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:51:54.426954 29015 net.cpp:1851] conv1a_param_0(0.22) 
I0630 01:51:54.426962 29015 net.cpp:1851] conv1b_param_0(0.44) 
I0630 01:51:54.426966 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:51:54.426970 29015 net.cpp:1851] res2a_branch2a_param_0(0.44) 
I0630 01:51:54.426975 29015 net.cpp:1851] res2a_branch2b_param_0(0.44) 
I0630 01:51:54.426980 29015 net.cpp:1851] res3a_branch2a_param_0(0.44) 
I0630 01:51:54.426985 29015 net.cpp:1851] res3a_branch2b_param_0(0.44) 
I0630 01:51:54.426988 29015 net.cpp:1851] res4a_branch2a_param_0(0.44) 
I0630 01:51:54.426993 29015 net.cpp:1851] res4a_branch2b_param_0(0.44) 
I0630 01:51:54.426997 29015 net.cpp:1851] res5a_branch2a_param_0(0.44) 
I0630 01:51:54.427001 29015 net.cpp:1851] res5a_branch2b_param_0(0.44) 
I0630 01:51:54.427006 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.03556e+06/2.3599e+06) 0.439
I0630 01:51:54.427144 29015 solver.cpp:471] Iteration 22000, Testing net (#0)
I0630 01:51:56.066071 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9173
I0630 01:51:56.066089 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9971
I0630 01:51:56.066094 29015 solver.cpp:544]     Test net output #2: loss = 0.2 (* 1 = 0.2 loss)
I0630 01:51:56.086165 29015 solver.cpp:290] Iteration 22000 (27.0732 iter/s, 3.69369s/100 iter), loss = 0
I0630 01:51:56.086182 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:56.086194 29015 sgd_solver.cpp:106] Iteration 22000, lr = 0.0065625
I0630 01:51:56.086731 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.46
I0630 01:51:56.633684 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:51:58.690937 29015 solver.cpp:290] Iteration 22100 (38.3925 iter/s, 2.60467s/100 iter), loss = 0
I0630 01:51:58.690958 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:58.690964 29015 sgd_solver.cpp:106] Iteration 22100, lr = 0.00654687
I0630 01:52:00.746775 29015 solver.cpp:290] Iteration 22200 (48.644 iter/s, 2.05575s/100 iter), loss = 0
I0630 01:52:00.746798 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:00.746806 29015 sgd_solver.cpp:106] Iteration 22200, lr = 0.00653125
I0630 01:52:02.808218 29015 solver.cpp:290] Iteration 22300 (48.5118 iter/s, 2.06135s/100 iter), loss = 0
I0630 01:52:02.808243 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:02.808250 29015 sgd_solver.cpp:106] Iteration 22300, lr = 0.00651562
I0630 01:52:04.868268 29015 solver.cpp:290] Iteration 22400 (48.5446 iter/s, 2.05996s/100 iter), loss = 0
I0630 01:52:04.868288 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:04.868297 29015 sgd_solver.cpp:106] Iteration 22400, lr = 0.0065
I0630 01:52:06.927482 29015 solver.cpp:290] Iteration 22500 (48.5642 iter/s, 2.05913s/100 iter), loss = 0
I0630 01:52:06.927506 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:06.927513 29015 sgd_solver.cpp:106] Iteration 22500, lr = 0.00648437
I0630 01:52:09.000349 29015 solver.cpp:290] Iteration 22600 (48.2444 iter/s, 2.07278s/100 iter), loss = 0
I0630 01:52:09.000370 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:09.000378 29015 sgd_solver.cpp:106] Iteration 22600, lr = 0.00646875
I0630 01:52:11.064291 29015 solver.cpp:290] Iteration 22700 (48.4531 iter/s, 2.06385s/100 iter), loss = 0
I0630 01:52:11.064321 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:11.064352 29015 sgd_solver.cpp:106] Iteration 22700, lr = 0.00645312
I0630 01:52:13.127725 29015 solver.cpp:290] Iteration 22800 (48.4651 iter/s, 2.06334s/100 iter), loss = 0
I0630 01:52:13.127748 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:13.127754 29015 sgd_solver.cpp:106] Iteration 22800, lr = 0.0064375
I0630 01:52:15.186064 29015 solver.cpp:290] Iteration 22900 (48.585 iter/s, 2.05825s/100 iter), loss = 0
I0630 01:52:15.186086 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:15.186092 29015 sgd_solver.cpp:106] Iteration 22900, lr = 0.00642187
I0630 01:52:17.231580 29015 solver.cpp:354] Sparsity after update:
I0630 01:52:17.232861 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:52:17.232867 29015 net.cpp:1851] conv1a_param_0(0.23) 
I0630 01:52:17.232875 29015 net.cpp:1851] conv1b_param_0(0.46) 
I0630 01:52:17.232877 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:52:17.232880 29015 net.cpp:1851] res2a_branch2a_param_0(0.46) 
I0630 01:52:17.232882 29015 net.cpp:1851] res2a_branch2b_param_0(0.46) 
I0630 01:52:17.232885 29015 net.cpp:1851] res3a_branch2a_param_0(0.46) 
I0630 01:52:17.232887 29015 net.cpp:1851] res3a_branch2b_param_0(0.46) 
I0630 01:52:17.232889 29015 net.cpp:1851] res4a_branch2a_param_0(0.46) 
I0630 01:52:17.232892 29015 net.cpp:1851] res4a_branch2b_param_0(0.46) 
I0630 01:52:17.232894 29015 net.cpp:1851] res5a_branch2a_param_0(0.46) 
I0630 01:52:17.232897 29015 net.cpp:1851] res5a_branch2b_param_0(0.46) 
I0630 01:52:17.232898 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.08264e+06/2.3599e+06) 0.459
I0630 01:52:17.232986 29015 solver.cpp:471] Iteration 23000, Testing net (#0)
I0630 01:52:18.869118 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9168
I0630 01:52:18.869138 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9966
I0630 01:52:18.869143 29015 solver.cpp:544]     Test net output #2: loss = 0.1984 (* 1 = 0.1984 loss)
I0630 01:52:18.890843 29015 solver.cpp:290] Iteration 23000 (26.9932 iter/s, 3.70464s/100 iter), loss = 0
I0630 01:52:18.890869 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:18.890878 29015 sgd_solver.cpp:106] Iteration 23000, lr = 0.00640625
I0630 01:52:18.891463 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.48
I0630 01:52:19.460754 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:52:21.515347 29015 solver.cpp:290] Iteration 23100 (38.104 iter/s, 2.6244s/100 iter), loss = 0
I0630 01:52:21.515417 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:21.515425 29015 sgd_solver.cpp:106] Iteration 23100, lr = 0.00639063
I0630 01:52:23.578290 29015 solver.cpp:290] Iteration 23200 (48.4776 iter/s, 2.06281s/100 iter), loss = 0
I0630 01:52:23.578312 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:23.578318 29015 sgd_solver.cpp:106] Iteration 23200, lr = 0.006375
I0630 01:52:25.633970 29015 solver.cpp:290] Iteration 23300 (48.6478 iter/s, 2.05559s/100 iter), loss = 0
I0630 01:52:25.633992 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:25.633999 29015 sgd_solver.cpp:106] Iteration 23300, lr = 0.00635938
I0630 01:52:27.687383 29015 solver.cpp:290] Iteration 23400 (48.7015 iter/s, 2.05333s/100 iter), loss = 0
I0630 01:52:27.687404 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:27.687412 29015 sgd_solver.cpp:106] Iteration 23400, lr = 0.00634375
I0630 01:52:29.746770 29015 solver.cpp:290] Iteration 23500 (48.5602 iter/s, 2.0593s/100 iter), loss = 0
I0630 01:52:29.746791 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:29.746799 29015 sgd_solver.cpp:106] Iteration 23500, lr = 0.00632813
I0630 01:52:31.802999 29015 solver.cpp:290] Iteration 23600 (48.6348 iter/s, 2.05614s/100 iter), loss = 0
I0630 01:52:31.803025 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:31.803031 29015 sgd_solver.cpp:106] Iteration 23600, lr = 0.0063125
I0630 01:52:33.859071 29015 solver.cpp:290] Iteration 23700 (48.6386 iter/s, 2.05598s/100 iter), loss = 0
I0630 01:52:33.859097 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:33.859107 29015 sgd_solver.cpp:106] Iteration 23700, lr = 0.00629687
I0630 01:52:35.915735 29015 solver.cpp:290] Iteration 23800 (48.6245 iter/s, 2.05658s/100 iter), loss = 0
I0630 01:52:35.915758 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:35.915765 29015 sgd_solver.cpp:106] Iteration 23800, lr = 0.00628125
I0630 01:52:37.976523 29015 solver.cpp:290] Iteration 23900 (48.5272 iter/s, 2.0607s/100 iter), loss = 0
I0630 01:52:37.976544 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:37.976552 29015 sgd_solver.cpp:106] Iteration 23900, lr = 0.00626562
I0630 01:52:40.016388 29015 solver.cpp:354] Sparsity after update:
I0630 01:52:40.017664 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:52:40.017670 29015 net.cpp:1851] conv1a_param_0(0.24) 
I0630 01:52:40.017678 29015 net.cpp:1851] conv1b_param_0(0.48) 
I0630 01:52:40.017679 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:52:40.017683 29015 net.cpp:1851] res2a_branch2a_param_0(0.48) 
I0630 01:52:40.017684 29015 net.cpp:1851] res2a_branch2b_param_0(0.48) 
I0630 01:52:40.017686 29015 net.cpp:1851] res3a_branch2a_param_0(0.48) 
I0630 01:52:40.017688 29015 net.cpp:1851] res3a_branch2b_param_0(0.48) 
I0630 01:52:40.017690 29015 net.cpp:1851] res4a_branch2a_param_0(0.48) 
I0630 01:52:40.017693 29015 net.cpp:1851] res4a_branch2b_param_0(0.48) 
I0630 01:52:40.017694 29015 net.cpp:1851] res5a_branch2a_param_0(0.48) 
I0630 01:52:40.017696 29015 net.cpp:1851] res5a_branch2b_param_0(0.48) 
I0630 01:52:40.017699 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.12971e+06/2.3599e+06) 0.479
I0630 01:52:40.017786 29015 solver.cpp:471] Iteration 24000, Testing net (#0)
I0630 01:52:41.664010 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9165
I0630 01:52:41.664029 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9966
I0630 01:52:41.664034 29015 solver.cpp:544]     Test net output #2: loss = 0.198 (* 1 = 0.198 loss)
I0630 01:52:41.684386 29015 solver.cpp:290] Iteration 24000 (26.9707 iter/s, 3.70773s/100 iter), loss = 0
I0630 01:52:41.684414 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:41.684422 29015 sgd_solver.cpp:106] Iteration 24000, lr = 0.00625
I0630 01:52:41.684973 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.5
I0630 01:52:42.292356 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:52:44.353202 29015 solver.cpp:290] Iteration 24100 (37.4713 iter/s, 2.66871s/100 iter), loss = 0
I0630 01:52:44.353224 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:44.353231 29015 sgd_solver.cpp:106] Iteration 24100, lr = 0.00623438
I0630 01:52:46.413815 29015 solver.cpp:290] Iteration 24200 (48.5313 iter/s, 2.06053s/100 iter), loss = 0
I0630 01:52:46.413838 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:46.413846 29015 sgd_solver.cpp:106] Iteration 24200, lr = 0.00621875
I0630 01:52:48.475193 29015 solver.cpp:290] Iteration 24300 (48.5133 iter/s, 2.06129s/100 iter), loss = 0
I0630 01:52:48.475215 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:48.475222 29015 sgd_solver.cpp:106] Iteration 24300, lr = 0.00620312
I0630 01:52:50.529932 29015 solver.cpp:290] Iteration 24400 (48.67 iter/s, 2.05465s/100 iter), loss = 0
I0630 01:52:50.529953 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:50.529960 29015 sgd_solver.cpp:106] Iteration 24400, lr = 0.0061875
I0630 01:52:52.588497 29015 solver.cpp:290] Iteration 24500 (48.5796 iter/s, 2.05848s/100 iter), loss = 0
I0630 01:52:52.588562 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:52.588577 29015 sgd_solver.cpp:106] Iteration 24500, lr = 0.00617187
I0630 01:52:54.646412 29015 solver.cpp:290] Iteration 24600 (48.5958 iter/s, 2.05779s/100 iter), loss = 0
I0630 01:52:54.646436 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:54.646445 29015 sgd_solver.cpp:106] Iteration 24600, lr = 0.00615625
I0630 01:52:56.700996 29015 solver.cpp:290] Iteration 24700 (48.6738 iter/s, 2.0545s/100 iter), loss = 0
I0630 01:52:56.701025 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:56.701035 29015 sgd_solver.cpp:106] Iteration 24700, lr = 0.00614062
I0630 01:52:58.756503 29015 solver.cpp:290] Iteration 24800 (48.6519 iter/s, 2.05542s/100 iter), loss = 0
I0630 01:52:58.756525 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:58.756532 29015 sgd_solver.cpp:106] Iteration 24800, lr = 0.006125
I0630 01:53:00.815582 29015 solver.cpp:290] Iteration 24900 (48.5674 iter/s, 2.05899s/100 iter), loss = 0
I0630 01:53:00.815604 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:00.815611 29015 sgd_solver.cpp:106] Iteration 24900, lr = 0.00610937
I0630 01:53:02.852100 29015 solver.cpp:354] Sparsity after update:
I0630 01:53:02.853375 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:53:02.853382 29015 net.cpp:1851] conv1a_param_0(0.25) 
I0630 01:53:02.853389 29015 net.cpp:1851] conv1b_param_0(0.5) 
I0630 01:53:02.853392 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:53:02.853395 29015 net.cpp:1851] res2a_branch2a_param_0(0.5) 
I0630 01:53:02.853399 29015 net.cpp:1851] res2a_branch2b_param_0(0.5) 
I0630 01:53:02.853400 29015 net.cpp:1851] res3a_branch2a_param_0(0.5) 
I0630 01:53:02.853404 29015 net.cpp:1851] res3a_branch2b_param_0(0.5) 
I0630 01:53:02.853405 29015 net.cpp:1851] res4a_branch2a_param_0(0.5) 
I0630 01:53:02.853408 29015 net.cpp:1851] res4a_branch2b_param_0(0.5) 
I0630 01:53:02.853410 29015 net.cpp:1851] res5a_branch2a_param_0(0.5) 
I0630 01:53:02.853413 29015 net.cpp:1851] res5a_branch2b_param_0(0.5) 
I0630 01:53:02.853415 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.17679e+06/2.3599e+06) 0.499
I0630 01:53:02.853509 29015 solver.cpp:471] Iteration 25000, Testing net (#0)
I0630 01:53:04.491672 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.917
I0630 01:53:04.491690 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9965
I0630 01:53:04.491696 29015 solver.cpp:544]     Test net output #2: loss = 0.2016 (* 1 = 0.2016 loss)
I0630 01:53:04.512564 29015 solver.cpp:290] Iteration 25000 (27.0501 iter/s, 3.69685s/100 iter), loss = 0
I0630 01:53:04.512584 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:04.512593 29015 sgd_solver.cpp:106] Iteration 25000, lr = 0.00609375
I0630 01:53:04.513134 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.52
I0630 01:53:05.133620 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:53:07.189388 29015 solver.cpp:290] Iteration 25100 (37.3591 iter/s, 2.67672s/100 iter), loss = 0
I0630 01:53:07.189411 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:07.189419 29015 sgd_solver.cpp:106] Iteration 25100, lr = 0.00607812
I0630 01:53:09.264545 29015 solver.cpp:290] Iteration 25200 (48.1912 iter/s, 2.07507s/100 iter), loss = 0
I0630 01:53:09.264569 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:09.264575 29015 sgd_solver.cpp:106] Iteration 25200, lr = 0.0060625
I0630 01:53:11.316860 29015 solver.cpp:290] Iteration 25300 (48.7276 iter/s, 2.05223s/100 iter), loss = 0
I0630 01:53:11.316884 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:11.316892 29015 sgd_solver.cpp:106] Iteration 25300, lr = 0.00604687
I0630 01:53:13.372911 29015 solver.cpp:290] Iteration 25400 (48.639 iter/s, 2.05596s/100 iter), loss = 0
I0630 01:53:13.372934 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:13.372959 29015 sgd_solver.cpp:106] Iteration 25400, lr = 0.00603125
I0630 01:53:15.426880 29015 solver.cpp:290] Iteration 25500 (48.6883 iter/s, 2.05388s/100 iter), loss = 0
I0630 01:53:15.426903 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:15.426909 29015 sgd_solver.cpp:106] Iteration 25500, lr = 0.00601562
I0630 01:53:17.486565 29015 solver.cpp:290] Iteration 25600 (48.5532 iter/s, 2.0596s/100 iter), loss = 0
I0630 01:53:17.486588 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:17.486595 29015 sgd_solver.cpp:106] Iteration 25600, lr = 0.006
I0630 01:53:19.542414 29015 solver.cpp:290] Iteration 25700 (48.6438 iter/s, 2.05576s/100 iter), loss = 0
I0630 01:53:19.542435 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:19.542443 29015 sgd_solver.cpp:106] Iteration 25700, lr = 0.00598437
I0630 01:53:21.599553 29015 solver.cpp:290] Iteration 25800 (48.6133 iter/s, 2.05705s/100 iter), loss = 0
I0630 01:53:21.599576 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:21.599586 29015 sgd_solver.cpp:106] Iteration 25800, lr = 0.00596875
I0630 01:53:23.658293 29015 solver.cpp:290] Iteration 25900 (48.5755 iter/s, 2.05865s/100 iter), loss = 0
I0630 01:53:23.658345 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:23.658354 29015 sgd_solver.cpp:106] Iteration 25900, lr = 0.00595312
I0630 01:53:25.692698 29015 solver.cpp:354] Sparsity after update:
I0630 01:53:25.693979 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:53:25.693984 29015 net.cpp:1851] conv1a_param_0(0.26) 
I0630 01:53:25.693992 29015 net.cpp:1851] conv1b_param_0(0.52) 
I0630 01:53:25.693994 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:53:25.693997 29015 net.cpp:1851] res2a_branch2a_param_0(0.52) 
I0630 01:53:25.694000 29015 net.cpp:1851] res2a_branch2b_param_0(0.52) 
I0630 01:53:25.694002 29015 net.cpp:1851] res3a_branch2a_param_0(0.52) 
I0630 01:53:25.694005 29015 net.cpp:1851] res3a_branch2b_param_0(0.52) 
I0630 01:53:25.694007 29015 net.cpp:1851] res4a_branch2a_param_0(0.52) 
I0630 01:53:25.694010 29015 net.cpp:1851] res4a_branch2b_param_0(0.52) 
I0630 01:53:25.694012 29015 net.cpp:1851] res5a_branch2a_param_0(0.52) 
I0630 01:53:25.694015 29015 net.cpp:1851] res5a_branch2b_param_0(0.52) 
I0630 01:53:25.694016 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.22386e+06/2.3599e+06) 0.519
I0630 01:53:25.694104 29015 solver.cpp:471] Iteration 26000, Testing net (#0)
I0630 01:53:27.332832 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9162
I0630 01:53:27.332854 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:53:27.332859 29015 solver.cpp:544]     Test net output #2: loss = 0.2028 (* 1 = 0.2028 loss)
I0630 01:53:27.354873 29015 solver.cpp:290] Iteration 26000 (27.0532 iter/s, 3.69642s/100 iter), loss = 0
I0630 01:53:27.354892 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:27.354902 29015 sgd_solver.cpp:106] Iteration 26000, lr = 0.0059375
I0630 01:53:27.355448 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.54
I0630 01:53:27.993993 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:53:30.051065 29015 solver.cpp:290] Iteration 26100 (37.0907 iter/s, 2.69609s/100 iter), loss = 0
I0630 01:53:30.051087 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:30.051093 29015 sgd_solver.cpp:106] Iteration 26100, lr = 0.00592188
I0630 01:53:32.106640 29015 solver.cpp:290] Iteration 26200 (48.6502 iter/s, 2.05549s/100 iter), loss = 0
I0630 01:53:32.106662 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:32.106668 29015 sgd_solver.cpp:106] Iteration 26200, lr = 0.00590625
I0630 01:53:34.159575 29015 solver.cpp:290] Iteration 26300 (48.7128 iter/s, 2.05285s/100 iter), loss = 0
I0630 01:53:34.159596 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:34.159605 29015 sgd_solver.cpp:106] Iteration 26300, lr = 0.00589063
I0630 01:53:36.215476 29015 solver.cpp:290] Iteration 26400 (48.6425 iter/s, 2.05582s/100 iter), loss = 0
I0630 01:53:36.215499 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:36.215508 29015 sgd_solver.cpp:106] Iteration 26400, lr = 0.005875
I0630 01:53:38.268193 29015 solver.cpp:290] Iteration 26500 (48.718 iter/s, 2.05263s/100 iter), loss = 0
I0630 01:53:38.268214 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:38.268221 29015 sgd_solver.cpp:106] Iteration 26500, lr = 0.00585938
I0630 01:53:40.323005 29015 solver.cpp:290] Iteration 26600 (48.6683 iter/s, 2.05473s/100 iter), loss = 0
I0630 01:53:40.323029 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:40.323035 29015 sgd_solver.cpp:106] Iteration 26600, lr = 0.00584375
I0630 01:53:42.377677 29015 solver.cpp:290] Iteration 26700 (48.6716 iter/s, 2.05458s/100 iter), loss = 0
I0630 01:53:42.377699 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:42.377706 29015 sgd_solver.cpp:106] Iteration 26700, lr = 0.00582812
I0630 01:53:44.434330 29015 solver.cpp:290] Iteration 26800 (48.6248 iter/s, 2.05657s/100 iter), loss = 0
I0630 01:53:44.434361 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:44.434368 29015 sgd_solver.cpp:106] Iteration 26800, lr = 0.0058125
I0630 01:53:46.487848 29015 solver.cpp:290] Iteration 26900 (48.6992 iter/s, 2.05342s/100 iter), loss = 0
I0630 01:53:46.487874 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:46.487884 29015 sgd_solver.cpp:106] Iteration 26900, lr = 0.00579687
I0630 01:53:48.520813 29015 solver.cpp:354] Sparsity after update:
I0630 01:53:48.522075 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:53:48.522081 29015 net.cpp:1851] conv1a_param_0(0.27) 
I0630 01:53:48.522089 29015 net.cpp:1851] conv1b_param_0(0.54) 
I0630 01:53:48.522091 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:53:48.522094 29015 net.cpp:1851] res2a_branch2a_param_0(0.54) 
I0630 01:53:48.522095 29015 net.cpp:1851] res2a_branch2b_param_0(0.54) 
I0630 01:53:48.522097 29015 net.cpp:1851] res3a_branch2a_param_0(0.54) 
I0630 01:53:48.522099 29015 net.cpp:1851] res3a_branch2b_param_0(0.54) 
I0630 01:53:48.522101 29015 net.cpp:1851] res4a_branch2a_param_0(0.54) 
I0630 01:53:48.522104 29015 net.cpp:1851] res4a_branch2b_param_0(0.54) 
I0630 01:53:48.522105 29015 net.cpp:1851] res5a_branch2a_param_0(0.54) 
I0630 01:53:48.522107 29015 net.cpp:1851] res5a_branch2b_param_0(0.54) 
I0630 01:53:48.522109 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.27093e+06/2.3599e+06) 0.539
I0630 01:53:48.522197 29015 solver.cpp:471] Iteration 27000, Testing net (#0)
I0630 01:53:50.161535 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.915
I0630 01:53:50.161554 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9963
I0630 01:53:50.161561 29015 solver.cpp:544]     Test net output #2: loss = 0.2041 (* 1 = 0.2041 loss)
I0630 01:53:50.181360 29015 solver.cpp:290] Iteration 27000 (27.0755 iter/s, 3.69338s/100 iter), loss = 0
I0630 01:53:50.181376 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:50.181390 29015 sgd_solver.cpp:106] Iteration 27000, lr = 0.00578125
I0630 01:53:50.181923 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.56
I0630 01:53:50.855242 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:53:52.914207 29015 solver.cpp:290] Iteration 27100 (36.5932 iter/s, 2.73275s/100 iter), loss = 0
I0630 01:53:52.914239 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:52.914252 29015 sgd_solver.cpp:106] Iteration 27100, lr = 0.00576563
I0630 01:53:54.969884 29015 solver.cpp:290] Iteration 27200 (48.648 iter/s, 2.05558s/100 iter), loss = 0
I0630 01:53:54.969964 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:54.969975 29015 sgd_solver.cpp:106] Iteration 27200, lr = 0.00575
I0630 01:53:57.024755 29015 solver.cpp:290] Iteration 27300 (48.6682 iter/s, 2.05473s/100 iter), loss = 0
I0630 01:53:57.024777 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:57.024785 29015 sgd_solver.cpp:106] Iteration 27300, lr = 0.00573438
I0630 01:53:59.080796 29015 solver.cpp:290] Iteration 27400 (48.6393 iter/s, 2.05595s/100 iter), loss = 0
I0630 01:53:59.080816 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:59.080823 29015 sgd_solver.cpp:106] Iteration 27400, lr = 0.00571875
I0630 01:54:01.140151 29015 solver.cpp:290] Iteration 27500 (48.5609 iter/s, 2.05927s/100 iter), loss = 0
I0630 01:54:01.140172 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:01.140180 29015 sgd_solver.cpp:106] Iteration 27500, lr = 0.00570312
I0630 01:54:03.194702 29015 solver.cpp:290] Iteration 27600 (48.6745 iter/s, 2.05447s/100 iter), loss = 0
I0630 01:54:03.194726 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:03.194736 29015 sgd_solver.cpp:106] Iteration 27600, lr = 0.0056875
I0630 01:54:05.249008 29015 solver.cpp:290] Iteration 27700 (48.6804 iter/s, 2.05422s/100 iter), loss = 0
I0630 01:54:05.249033 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:05.249042 29015 sgd_solver.cpp:106] Iteration 27700, lr = 0.00567187
I0630 01:54:07.306790 29015 solver.cpp:290] Iteration 27800 (48.5982 iter/s, 2.05769s/100 iter), loss = 0
I0630 01:54:07.306849 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:07.306864 29015 sgd_solver.cpp:106] Iteration 27800, lr = 0.00565625
I0630 01:54:09.360980 29015 solver.cpp:290] Iteration 27900 (48.6839 iter/s, 2.05407s/100 iter), loss = 0
I0630 01:54:09.361001 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:09.361008 29015 sgd_solver.cpp:106] Iteration 27900, lr = 0.00564062
I0630 01:54:11.395774 29015 solver.cpp:354] Sparsity after update:
I0630 01:54:11.397043 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:54:11.397050 29015 net.cpp:1851] conv1a_param_0(0.28) 
I0630 01:54:11.397058 29015 net.cpp:1851] conv1b_param_0(0.56) 
I0630 01:54:11.397060 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:54:11.397063 29015 net.cpp:1851] res2a_branch2a_param_0(0.56) 
I0630 01:54:11.397066 29015 net.cpp:1851] res2a_branch2b_param_0(0.56) 
I0630 01:54:11.397068 29015 net.cpp:1851] res3a_branch2a_param_0(0.56) 
I0630 01:54:11.397070 29015 net.cpp:1851] res3a_branch2b_param_0(0.56) 
I0630 01:54:11.397073 29015 net.cpp:1851] res4a_branch2a_param_0(0.56) 
I0630 01:54:11.397074 29015 net.cpp:1851] res4a_branch2b_param_0(0.56) 
I0630 01:54:11.397076 29015 net.cpp:1851] res5a_branch2a_param_0(0.56) 
I0630 01:54:11.397078 29015 net.cpp:1851] res5a_branch2b_param_0(0.56) 
I0630 01:54:11.397081 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.318e+06/2.3599e+06) 0.558
I0630 01:54:11.397168 29015 solver.cpp:471] Iteration 28000, Testing net (#0)
I0630 01:54:13.040478 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9148
I0630 01:54:13.040498 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9963
I0630 01:54:13.040503 29015 solver.cpp:544]     Test net output #2: loss = 0.2025 (* 1 = 0.2025 loss)
I0630 01:54:13.060571 29015 solver.cpp:290] Iteration 28000 (27.031 iter/s, 3.69946s/100 iter), loss = 0
I0630 01:54:13.060590 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:13.060601 29015 sgd_solver.cpp:106] Iteration 28000, lr = 0.005625
I0630 01:54:13.061127 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.58
I0630 01:54:13.753742 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:54:15.808017 29015 solver.cpp:290] Iteration 28100 (36.3988 iter/s, 2.74734s/100 iter), loss = 0
I0630 01:54:15.808055 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:15.808066 29015 sgd_solver.cpp:106] Iteration 28100, lr = 0.00560937
I0630 01:54:17.872401 29015 solver.cpp:290] Iteration 28200 (48.443 iter/s, 2.06428s/100 iter), loss = 0
I0630 01:54:17.872426 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:17.872434 29015 sgd_solver.cpp:106] Iteration 28200, lr = 0.00559375
I0630 01:54:19.926692 29015 solver.cpp:290] Iteration 28300 (48.6807 iter/s, 2.0542s/100 iter), loss = 0
I0630 01:54:19.926715 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:19.926724 29015 sgd_solver.cpp:106] Iteration 28300, lr = 0.00557812
I0630 01:54:21.980610 29015 solver.cpp:290] Iteration 28400 (48.6895 iter/s, 2.05383s/100 iter), loss = 0
I0630 01:54:21.980635 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:21.980644 29015 sgd_solver.cpp:106] Iteration 28400, lr = 0.0055625
I0630 01:54:24.046368 29015 solver.cpp:290] Iteration 28500 (48.4104 iter/s, 2.06567s/100 iter), loss = 0
I0630 01:54:24.046391 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:24.046398 29015 sgd_solver.cpp:106] Iteration 28500, lr = 0.00554687
I0630 01:54:26.104881 29015 solver.cpp:290] Iteration 28600 (48.5808 iter/s, 2.05843s/100 iter), loss = 0
I0630 01:54:26.104943 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:26.104950 29015 sgd_solver.cpp:106] Iteration 28600, lr = 0.00553125
I0630 01:54:28.167084 29015 solver.cpp:290] Iteration 28700 (48.4947 iter/s, 2.06208s/100 iter), loss = 0
I0630 01:54:28.167106 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:28.167114 29015 sgd_solver.cpp:106] Iteration 28700, lr = 0.00551562
I0630 01:54:30.223207 29015 solver.cpp:290] Iteration 28800 (48.6373 iter/s, 2.05604s/100 iter), loss = 0
I0630 01:54:30.223230 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:30.223237 29015 sgd_solver.cpp:106] Iteration 28800, lr = 0.0055
I0630 01:54:32.281286 29015 solver.cpp:290] Iteration 28900 (48.5911 iter/s, 2.05799s/100 iter), loss = 0
I0630 01:54:32.281309 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:32.281316 29015 sgd_solver.cpp:106] Iteration 28900, lr = 0.00548437
I0630 01:54:34.316206 29015 solver.cpp:354] Sparsity after update:
I0630 01:54:34.317490 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:54:34.317498 29015 net.cpp:1851] conv1a_param_0(0.29) 
I0630 01:54:34.317505 29015 net.cpp:1851] conv1b_param_0(0.579) 
I0630 01:54:34.317508 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:54:34.317510 29015 net.cpp:1851] res2a_branch2a_param_0(0.58) 
I0630 01:54:34.317513 29015 net.cpp:1851] res2a_branch2b_param_0(0.58) 
I0630 01:54:34.317515 29015 net.cpp:1851] res3a_branch2a_param_0(0.58) 
I0630 01:54:34.317517 29015 net.cpp:1851] res3a_branch2b_param_0(0.58) 
I0630 01:54:34.317520 29015 net.cpp:1851] res4a_branch2a_param_0(0.58) 
I0630 01:54:34.317523 29015 net.cpp:1851] res4a_branch2b_param_0(0.58) 
I0630 01:54:34.317524 29015 net.cpp:1851] res5a_branch2a_param_0(0.58) 
I0630 01:54:34.317528 29015 net.cpp:1851] res5a_branch2b_param_0(0.58) 
I0630 01:54:34.317530 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.36506e+06/2.3599e+06) 0.578
I0630 01:54:34.317664 29015 solver.cpp:471] Iteration 29000, Testing net (#0)
I0630 01:54:35.960777 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9128
I0630 01:54:35.960798 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9963
I0630 01:54:35.960803 29015 solver.cpp:544]     Test net output #2: loss = 0.2048 (* 1 = 0.2048 loss)
I0630 01:54:35.980444 29015 solver.cpp:290] Iteration 29000 (27.0342 iter/s, 3.69902s/100 iter), loss = 0
I0630 01:54:35.980463 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:35.980473 29015 sgd_solver.cpp:106] Iteration 29000, lr = 0.00546875
I0630 01:54:35.980954 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.6
I0630 01:54:36.694331 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:54:38.753998 29015 solver.cpp:290] Iteration 29100 (36.0562 iter/s, 2.77345s/100 iter), loss = 0
I0630 01:54:38.754020 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:38.754027 29015 sgd_solver.cpp:106] Iteration 29100, lr = 0.00545313
I0630 01:54:40.813805 29015 solver.cpp:290] Iteration 29200 (48.5504 iter/s, 2.05972s/100 iter), loss = 0
I0630 01:54:40.813836 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:40.813843 29015 sgd_solver.cpp:106] Iteration 29200, lr = 0.0054375
I0630 01:54:42.871289 29015 solver.cpp:290] Iteration 29300 (48.6053 iter/s, 2.05739s/100 iter), loss = 0
I0630 01:54:42.871311 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:42.871317 29015 sgd_solver.cpp:106] Iteration 29300, lr = 0.00542188
I0630 01:54:44.925870 29015 solver.cpp:290] Iteration 29400 (48.6738 iter/s, 2.05449s/100 iter), loss = 0
I0630 01:54:44.925895 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:44.925905 29015 sgd_solver.cpp:106] Iteration 29400, lr = 0.00540625
I0630 01:54:46.981989 29015 solver.cpp:290] Iteration 29500 (48.6375 iter/s, 2.05603s/100 iter), loss = 0
I0630 01:54:46.982033 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:46.982039 29015 sgd_solver.cpp:106] Iteration 29500, lr = 0.00539062
I0630 01:54:49.038652 29015 solver.cpp:290] Iteration 29600 (48.625 iter/s, 2.05656s/100 iter), loss = 0
I0630 01:54:49.038674 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:49.038683 29015 sgd_solver.cpp:106] Iteration 29600, lr = 0.005375
I0630 01:54:51.096977 29015 solver.cpp:290] Iteration 29700 (48.5852 iter/s, 2.05824s/100 iter), loss = 0
I0630 01:54:51.096999 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:51.097007 29015 sgd_solver.cpp:106] Iteration 29700, lr = 0.00535937
I0630 01:54:53.151444 29015 solver.cpp:290] Iteration 29800 (48.6765 iter/s, 2.05438s/100 iter), loss = 0
I0630 01:54:53.151466 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:53.151473 29015 sgd_solver.cpp:106] Iteration 29800, lr = 0.00534375
I0630 01:54:55.207819 29015 solver.cpp:290] Iteration 29900 (48.6313 iter/s, 2.05629s/100 iter), loss = 0
I0630 01:54:55.207842 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:55.207849 29015 sgd_solver.cpp:106] Iteration 29900, lr = 0.00532812
I0630 01:54:57.241358 29015 solver.cpp:598] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_30000.caffemodel
I0630 01:54:57.257815 29015 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_30000.solverstate
I0630 01:54:57.265044 29015 solver.cpp:354] Sparsity after update:
I0630 01:54:57.265991 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:54:57.266000 29015 net.cpp:1851] conv1a_param_0(0.3) 
I0630 01:54:57.266007 29015 net.cpp:1851] conv1b_param_0(0.6) 
I0630 01:54:57.266010 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:54:57.266012 29015 net.cpp:1851] res2a_branch2a_param_0(0.6) 
I0630 01:54:57.266014 29015 net.cpp:1851] res2a_branch2b_param_0(0.6) 
I0630 01:54:57.266016 29015 net.cpp:1851] res3a_branch2a_param_0(0.6) 
I0630 01:54:57.266018 29015 net.cpp:1851] res3a_branch2b_param_0(0.6) 
I0630 01:54:57.266021 29015 net.cpp:1851] res4a_branch2a_param_0(0.6) 
I0630 01:54:57.266022 29015 net.cpp:1851] res4a_branch2b_param_0(0.6) 
I0630 01:54:57.266024 29015 net.cpp:1851] res5a_branch2a_param_0(0.6) 
I0630 01:54:57.266026 29015 net.cpp:1851] res5a_branch2b_param_0(0.6) 
I0630 01:54:57.266027 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.41214e+06/2.3599e+06) 0.598
I0630 01:54:57.266125 29015 solver.cpp:471] Iteration 30000, Testing net (#0)
I0630 01:54:58.902070 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9123
I0630 01:54:58.902088 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9962
I0630 01:54:58.902093 29015 solver.cpp:544]     Test net output #2: loss = 0.2109 (* 1 = 0.2109 loss)
I0630 01:54:58.922245 29015 solver.cpp:290] Iteration 30000 (26.923 iter/s, 3.71429s/100 iter), loss = 0
I0630 01:54:58.922263 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:58.922276 29015 sgd_solver.cpp:106] Iteration 30000, lr = 0.0053125
I0630 01:54:58.922852 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.62
I0630 01:54:59.694198 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:55:01.749969 29015 solver.cpp:290] Iteration 30100 (35.3655 iter/s, 2.82762s/100 iter), loss = 0
I0630 01:55:01.749991 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:01.749999 29015 sgd_solver.cpp:106] Iteration 30100, lr = 0.00529688
I0630 01:55:03.809131 29015 solver.cpp:290] Iteration 30200 (48.5655 iter/s, 2.05908s/100 iter), loss = 0
I0630 01:55:03.809154 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:03.809160 29015 sgd_solver.cpp:106] Iteration 30200, lr = 0.00528125
I0630 01:55:05.869904 29015 solver.cpp:290] Iteration 30300 (48.5276 iter/s, 2.06068s/100 iter), loss = 0
I0630 01:55:05.869928 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:05.869937 29015 sgd_solver.cpp:106] Iteration 30300, lr = 0.00526563
I0630 01:55:07.925861 29015 solver.cpp:290] Iteration 30400 (48.6413 iter/s, 2.05587s/100 iter), loss = 0
I0630 01:55:07.925884 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:07.925889 29015 sgd_solver.cpp:106] Iteration 30400, lr = 0.00525
I0630 01:55:09.983191 29015 solver.cpp:290] Iteration 30500 (48.6088 iter/s, 2.05724s/100 iter), loss = 0
I0630 01:55:09.983216 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:09.983223 29015 sgd_solver.cpp:106] Iteration 30500, lr = 0.00523437
I0630 01:55:12.039346 29015 solver.cpp:290] Iteration 30600 (48.6365 iter/s, 2.05607s/100 iter), loss = 0
I0630 01:55:12.039369 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:12.039376 29015 sgd_solver.cpp:106] Iteration 30600, lr = 0.00521875
I0630 01:55:14.093045 29015 solver.cpp:290] Iteration 30700 (48.6947 iter/s, 2.05361s/100 iter), loss = 0
I0630 01:55:14.093070 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:14.093076 29015 sgd_solver.cpp:106] Iteration 30700, lr = 0.00520312
I0630 01:55:16.147573 29015 solver.cpp:290] Iteration 30800 (48.6751 iter/s, 2.05444s/100 iter), loss = 0
I0630 01:55:16.147624 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:16.147635 29015 sgd_solver.cpp:106] Iteration 30800, lr = 0.0051875
I0630 01:55:18.199636 29015 solver.cpp:290] Iteration 30900 (48.7341 iter/s, 2.05195s/100 iter), loss = 0
I0630 01:55:18.199659 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:18.199666 29015 sgd_solver.cpp:106] Iteration 30900, lr = 0.00517187
I0630 01:55:20.233336 29015 solver.cpp:354] Sparsity after update:
I0630 01:55:20.234606 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:55:20.234612 29015 net.cpp:1851] conv1a_param_0(0.31) 
I0630 01:55:20.234621 29015 net.cpp:1851] conv1b_param_0(0.62) 
I0630 01:55:20.234622 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:55:20.234624 29015 net.cpp:1851] res2a_branch2a_param_0(0.62) 
I0630 01:55:20.234627 29015 net.cpp:1851] res2a_branch2b_param_0(0.62) 
I0630 01:55:20.234629 29015 net.cpp:1851] res3a_branch2a_param_0(0.62) 
I0630 01:55:20.234630 29015 net.cpp:1851] res3a_branch2b_param_0(0.62) 
I0630 01:55:20.234632 29015 net.cpp:1851] res4a_branch2a_param_0(0.62) 
I0630 01:55:20.234634 29015 net.cpp:1851] res4a_branch2b_param_0(0.62) 
I0630 01:55:20.234637 29015 net.cpp:1851] res5a_branch2a_param_0(0.62) 
I0630 01:55:20.234638 29015 net.cpp:1851] res5a_branch2b_param_0(0.62) 
I0630 01:55:20.234640 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.45921e+06/2.3599e+06) 0.618
I0630 01:55:20.234727 29015 solver.cpp:471] Iteration 31000, Testing net (#0)
I0630 01:55:21.872704 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.914
I0630 01:55:21.872722 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9963
I0630 01:55:21.872728 29015 solver.cpp:544]     Test net output #2: loss = 0.2083 (* 1 = 0.2083 loss)
I0630 01:55:21.892493 29015 solver.cpp:290] Iteration 31000 (27.0803 iter/s, 3.69272s/100 iter), loss = 0
I0630 01:55:21.892520 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:21.892527 29015 sgd_solver.cpp:106] Iteration 31000, lr = 0.00515625
I0630 01:55:21.893142 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.64
I0630 01:55:22.674687 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:55:24.747509 29015 solver.cpp:290] Iteration 31100 (35.0275 iter/s, 2.8549s/100 iter), loss = 0
I0630 01:55:24.747539 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:24.747548 29015 sgd_solver.cpp:106] Iteration 31100, lr = 0.00514062
I0630 01:55:26.803755 29015 solver.cpp:290] Iteration 31200 (48.6345 iter/s, 2.05615s/100 iter), loss = 0
I0630 01:55:26.803781 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:26.803787 29015 sgd_solver.cpp:106] Iteration 31200, lr = 0.005125
I0630 01:55:28.859252 29015 solver.cpp:290] Iteration 31300 (48.6522 iter/s, 2.0554s/100 iter), loss = 0
I0630 01:55:28.859360 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:28.859380 29015 sgd_solver.cpp:106] Iteration 31300, lr = 0.00510937
I0630 01:55:30.917551 29015 solver.cpp:290] Iteration 31400 (48.5878 iter/s, 2.05813s/100 iter), loss = 0
I0630 01:55:30.917574 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:30.917583 29015 sgd_solver.cpp:106] Iteration 31400, lr = 0.00509375
I0630 01:55:32.972857 29015 solver.cpp:290] Iteration 31500 (48.6566 iter/s, 2.05522s/100 iter), loss = 0
I0630 01:55:32.972879 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:32.972887 29015 sgd_solver.cpp:106] Iteration 31500, lr = 0.00507812
I0630 01:55:35.027335 29015 solver.cpp:290] Iteration 31600 (48.6762 iter/s, 2.05439s/100 iter), loss = 0
I0630 01:55:35.027360 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:35.027369 29015 sgd_solver.cpp:106] Iteration 31600, lr = 0.0050625
I0630 01:55:37.085808 29015 solver.cpp:290] Iteration 31700 (48.5819 iter/s, 2.05838s/100 iter), loss = 0
I0630 01:55:37.085832 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:37.085842 29015 sgd_solver.cpp:106] Iteration 31700, lr = 0.00504687
I0630 01:55:39.147521 29015 solver.cpp:290] Iteration 31800 (48.5054 iter/s, 2.06163s/100 iter), loss = 0
I0630 01:55:39.147543 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:39.147552 29015 sgd_solver.cpp:106] Iteration 31800, lr = 0.00503125
I0630 01:55:41.201611 29015 solver.cpp:290] Iteration 31900 (48.6854 iter/s, 2.054s/100 iter), loss = 0
I0630 01:55:41.201632 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:41.201639 29015 sgd_solver.cpp:106] Iteration 31900, lr = 0.00501562
I0630 01:55:43.238438 29015 solver.cpp:354] Sparsity after update:
I0630 01:55:43.239715 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:55:43.239722 29015 net.cpp:1851] conv1a_param_0(0.32) 
I0630 01:55:43.239733 29015 net.cpp:1851] conv1b_param_0(0.64) 
I0630 01:55:43.239739 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:55:43.239743 29015 net.cpp:1851] res2a_branch2a_param_0(0.64) 
I0630 01:55:43.239748 29015 net.cpp:1851] res2a_branch2b_param_0(0.64) 
I0630 01:55:43.239751 29015 net.cpp:1851] res3a_branch2a_param_0(0.64) 
I0630 01:55:43.239756 29015 net.cpp:1851] res3a_branch2b_param_0(0.64) 
I0630 01:55:43.239759 29015 net.cpp:1851] res4a_branch2a_param_0(0.64) 
I0630 01:55:43.239764 29015 net.cpp:1851] res4a_branch2b_param_0(0.64) 
I0630 01:55:43.239768 29015 net.cpp:1851] res5a_branch2a_param_0(0.64) 
I0630 01:55:43.239773 29015 net.cpp:1851] res5a_branch2b_param_0(0.64) 
I0630 01:55:43.239776 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.50628e+06/2.3599e+06) 0.638
I0630 01:55:43.239867 29015 solver.cpp:471] Iteration 32000, Testing net (#0)
I0630 01:55:44.875756 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9094
I0630 01:55:44.875782 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9963
I0630 01:55:44.875788 29015 solver.cpp:544]     Test net output #2: loss = 0.2107 (* 1 = 0.2107 loss)
I0630 01:55:44.896142 29015 solver.cpp:290] Iteration 32000 (27.068 iter/s, 3.6944s/100 iter), loss = 0
I0630 01:55:44.896167 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:44.896176 29015 sgd_solver.cpp:106] Iteration 32000, lr = 0.005
I0630 01:55:44.896739 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.66
I0630 01:55:45.705920 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:55:47.762543 29015 solver.cpp:290] Iteration 32100 (34.8883 iter/s, 2.86629s/100 iter), loss = 0
I0630 01:55:47.762565 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:47.762573 29015 sgd_solver.cpp:106] Iteration 32100, lr = 0.00498438
I0630 01:55:49.818377 29015 solver.cpp:290] Iteration 32200 (48.6441 iter/s, 2.05575s/100 iter), loss = 0
I0630 01:55:49.818400 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:49.818423 29015 sgd_solver.cpp:106] Iteration 32200, lr = 0.00496875
I0630 01:55:51.876652 29015 solver.cpp:290] Iteration 32300 (48.5865 iter/s, 2.05819s/100 iter), loss = 0
I0630 01:55:51.876673 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:51.876680 29015 sgd_solver.cpp:106] Iteration 32300, lr = 0.00495313
I0630 01:55:53.933140 29015 solver.cpp:290] Iteration 32400 (48.6287 iter/s, 2.0564s/100 iter), loss = 0
I0630 01:55:53.933161 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:53.933167 29015 sgd_solver.cpp:106] Iteration 32400, lr = 0.0049375
I0630 01:55:55.988200 29015 solver.cpp:290] Iteration 32500 (48.6624 iter/s, 2.05498s/100 iter), loss = 0
I0630 01:55:55.988224 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:55.988232 29015 sgd_solver.cpp:106] Iteration 32500, lr = 0.00492187
I0630 01:55:58.042055 29015 solver.cpp:290] Iteration 32600 (48.691 iter/s, 2.05377s/100 iter), loss = 0
I0630 01:55:58.042079 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:58.042088 29015 sgd_solver.cpp:106] Iteration 32600, lr = 0.00490625
I0630 01:56:00.095729 29015 solver.cpp:290] Iteration 32700 (48.6954 iter/s, 2.05358s/100 iter), loss = 0
I0630 01:56:00.095805 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:00.095814 29015 sgd_solver.cpp:106] Iteration 32700, lr = 0.00489062
I0630 01:56:02.150095 29015 solver.cpp:290] Iteration 32800 (48.6801 iter/s, 2.05423s/100 iter), loss = 0
I0630 01:56:02.150117 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:02.150125 29015 sgd_solver.cpp:106] Iteration 32800, lr = 0.004875
I0630 01:56:04.205725 29015 solver.cpp:290] Iteration 32900 (48.6489 iter/s, 2.05554s/100 iter), loss = 0
I0630 01:56:04.205747 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:04.205756 29015 sgd_solver.cpp:106] Iteration 32900, lr = 0.00485937
I0630 01:56:06.240263 29015 solver.cpp:354] Sparsity after update:
I0630 01:56:06.241513 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:56:06.241520 29015 net.cpp:1851] conv1a_param_0(0.33) 
I0630 01:56:06.241529 29015 net.cpp:1851] conv1b_param_0(0.66) 
I0630 01:56:06.241534 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:56:06.241539 29015 net.cpp:1851] res2a_branch2a_param_0(0.66) 
I0630 01:56:06.241544 29015 net.cpp:1851] res2a_branch2b_param_0(0.66) 
I0630 01:56:06.241549 29015 net.cpp:1851] res3a_branch2a_param_0(0.66) 
I0630 01:56:06.241552 29015 net.cpp:1851] res3a_branch2b_param_0(0.66) 
I0630 01:56:06.241557 29015 net.cpp:1851] res4a_branch2a_param_0(0.66) 
I0630 01:56:06.241561 29015 net.cpp:1851] res4a_branch2b_param_0(0.66) 
I0630 01:56:06.241566 29015 net.cpp:1851] res5a_branch2a_param_0(0.66) 
I0630 01:56:06.241571 29015 net.cpp:1851] res5a_branch2b_param_0(0.66) 
I0630 01:56:06.241575 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.55336e+06/2.3599e+06) 0.658
I0630 01:56:06.241669 29015 solver.cpp:471] Iteration 33000, Testing net (#0)
I0630 01:56:07.878237 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9117
I0630 01:56:07.878257 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9964
I0630 01:56:07.878262 29015 solver.cpp:544]     Test net output #2: loss = 0.2074 (* 1 = 0.2074 loss)
I0630 01:56:07.899209 29015 solver.cpp:290] Iteration 33000 (27.0757 iter/s, 3.69335s/100 iter), loss = 0
I0630 01:56:07.899240 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:07.899250 29015 sgd_solver.cpp:106] Iteration 33000, lr = 0.00484375
I0630 01:56:07.900014 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.68
I0630 01:56:08.738631 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:56:10.800022 29015 solver.cpp:290] Iteration 33100 (34.4745 iter/s, 2.9007s/100 iter), loss = 0
I0630 01:56:10.800045 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:10.800051 29015 sgd_solver.cpp:106] Iteration 33100, lr = 0.00482813
I0630 01:56:12.853410 29015 solver.cpp:290] Iteration 33200 (48.7021 iter/s, 2.0533s/100 iter), loss = 0
I0630 01:56:12.853432 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:12.853440 29015 sgd_solver.cpp:106] Iteration 33200, lr = 0.0048125
I0630 01:56:14.908212 29015 solver.cpp:290] Iteration 33300 (48.6686 iter/s, 2.05471s/100 iter), loss = 0
I0630 01:56:14.908241 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:14.908251 29015 sgd_solver.cpp:106] Iteration 33300, lr = 0.00479688
I0630 01:56:16.964588 29015 solver.cpp:290] Iteration 33400 (48.6314 iter/s, 2.05629s/100 iter), loss = 0
I0630 01:56:16.964612 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:16.964622 29015 sgd_solver.cpp:106] Iteration 33400, lr = 0.00478125
I0630 01:56:19.024181 29015 solver.cpp:290] Iteration 33500 (48.5553 iter/s, 2.05951s/100 iter), loss = 0
I0630 01:56:19.024204 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:19.024210 29015 sgd_solver.cpp:106] Iteration 33500, lr = 0.00476563
I0630 01:56:21.081120 29015 solver.cpp:290] Iteration 33600 (48.618 iter/s, 2.05685s/100 iter), loss = 0
I0630 01:56:21.081158 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:21.081168 29015 sgd_solver.cpp:106] Iteration 33600, lr = 0.00475
I0630 01:56:23.132580 29015 solver.cpp:290] Iteration 33700 (48.7482 iter/s, 2.05136s/100 iter), loss = 0
I0630 01:56:23.132602 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:23.132611 29015 sgd_solver.cpp:106] Iteration 33700, lr = 0.00473437
I0630 01:56:25.187274 29015 solver.cpp:290] Iteration 33800 (48.6711 iter/s, 2.05461s/100 iter), loss = 0
I0630 01:56:25.187297 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:25.187304 29015 sgd_solver.cpp:106] Iteration 33800, lr = 0.00471875
I0630 01:56:27.241703 29015 solver.cpp:290] Iteration 33900 (48.6774 iter/s, 2.05434s/100 iter), loss = 0
I0630 01:56:27.241725 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:27.241732 29015 sgd_solver.cpp:106] Iteration 33900, lr = 0.00470312
I0630 01:56:29.281149 29015 solver.cpp:354] Sparsity after update:
I0630 01:56:29.282420 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:56:29.282428 29015 net.cpp:1851] conv1a_param_0(0.34) 
I0630 01:56:29.282434 29015 net.cpp:1851] conv1b_param_0(0.68) 
I0630 01:56:29.282436 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:56:29.282438 29015 net.cpp:1851] res2a_branch2a_param_0(0.68) 
I0630 01:56:29.282441 29015 net.cpp:1851] res2a_branch2b_param_0(0.68) 
I0630 01:56:29.282444 29015 net.cpp:1851] res3a_branch2a_param_0(0.68) 
I0630 01:56:29.282445 29015 net.cpp:1851] res3a_branch2b_param_0(0.68) 
I0630 01:56:29.282446 29015 net.cpp:1851] res4a_branch2a_param_0(0.68) 
I0630 01:56:29.282449 29015 net.cpp:1851] res4a_branch2b_param_0(0.68) 
I0630 01:56:29.282450 29015 net.cpp:1851] res5a_branch2a_param_0(0.68) 
I0630 01:56:29.282452 29015 net.cpp:1851] res5a_branch2b_param_0(0.68) 
I0630 01:56:29.282454 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.60043e+06/2.3599e+06) 0.678
I0630 01:56:29.282549 29015 solver.cpp:471] Iteration 34000, Testing net (#0)
I0630 01:56:30.918226 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9101
I0630 01:56:30.918309 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9959
I0630 01:56:30.918318 29015 solver.cpp:544]     Test net output #2: loss = 0.2124 (* 1 = 0.2124 loss)
I0630 01:56:30.938485 29015 solver.cpp:290] Iteration 34000 (27.0515 iter/s, 3.69665s/100 iter), loss = 0
I0630 01:56:30.938540 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:30.938560 29015 sgd_solver.cpp:106] Iteration 34000, lr = 0.0046875
I0630 01:56:30.939313 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.7
I0630 01:56:31.809401 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:56:33.865023 29015 solver.cpp:290] Iteration 34100 (34.1717 iter/s, 2.9264s/100 iter), loss = 0
I0630 01:56:33.865046 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:33.865052 29015 sgd_solver.cpp:106] Iteration 34100, lr = 0.00467187
I0630 01:56:35.924119 29015 solver.cpp:290] Iteration 34200 (48.5671 iter/s, 2.05901s/100 iter), loss = 0
I0630 01:56:35.924141 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:35.924147 29015 sgd_solver.cpp:106] Iteration 34200, lr = 0.00465625
I0630 01:56:37.987541 29015 solver.cpp:290] Iteration 34300 (48.4652 iter/s, 2.06334s/100 iter), loss = 0
I0630 01:56:37.987563 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:37.987571 29015 sgd_solver.cpp:106] Iteration 34300, lr = 0.00464062
I0630 01:56:40.040853 29015 solver.cpp:290] Iteration 34400 (48.7039 iter/s, 2.05322s/100 iter), loss = 0
I0630 01:56:40.040879 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:40.040886 29015 sgd_solver.cpp:106] Iteration 34400, lr = 0.004625
I0630 01:56:42.098229 29015 solver.cpp:290] Iteration 34500 (48.6077 iter/s, 2.05729s/100 iter), loss = 0
I0630 01:56:42.098253 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:42.098263 29015 sgd_solver.cpp:106] Iteration 34500, lr = 0.00460937
I0630 01:56:44.152662 29015 solver.cpp:290] Iteration 34600 (48.6773 iter/s, 2.05434s/100 iter), loss = 0
I0630 01:56:44.152683 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:44.152689 29015 sgd_solver.cpp:106] Iteration 34600, lr = 0.00459375
I0630 01:56:46.207557 29015 solver.cpp:290] Iteration 34700 (48.6663 iter/s, 2.05481s/100 iter), loss = 0
I0630 01:56:46.207579 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:46.207587 29015 sgd_solver.cpp:106] Iteration 34700, lr = 0.00457812
I0630 01:56:48.265427 29015 solver.cpp:290] Iteration 34800 (48.596 iter/s, 2.05778s/100 iter), loss = 0
I0630 01:56:48.265451 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:48.265457 29015 sgd_solver.cpp:106] Iteration 34800, lr = 0.0045625
I0630 01:56:50.326642 29015 solver.cpp:290] Iteration 34900 (48.5172 iter/s, 2.06113s/100 iter), loss = 0
I0630 01:56:50.326664 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:50.326671 29015 sgd_solver.cpp:106] Iteration 34900, lr = 0.00454687
I0630 01:56:52.364058 29015 solver.cpp:354] Sparsity after update:
I0630 01:56:52.365321 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:56:52.365327 29015 net.cpp:1851] conv1a_param_0(0.35) 
I0630 01:56:52.365335 29015 net.cpp:1851] conv1b_param_0(0.7) 
I0630 01:56:52.365339 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:56:52.365341 29015 net.cpp:1851] res2a_branch2a_param_0(0.7) 
I0630 01:56:52.365345 29015 net.cpp:1851] res2a_branch2b_param_0(0.7) 
I0630 01:56:52.365347 29015 net.cpp:1851] res3a_branch2a_param_0(0.7) 
I0630 01:56:52.365350 29015 net.cpp:1851] res3a_branch2b_param_0(0.7) 
I0630 01:56:52.365351 29015 net.cpp:1851] res4a_branch2a_param_0(0.7) 
I0630 01:56:52.365355 29015 net.cpp:1851] res4a_branch2b_param_0(0.7) 
I0630 01:56:52.365356 29015 net.cpp:1851] res5a_branch2a_param_0(0.7) 
I0630 01:56:52.365360 29015 net.cpp:1851] res5a_branch2b_param_0(0.7) 
I0630 01:56:52.365362 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.6475e+06/2.3599e+06) 0.698
I0630 01:56:52.365461 29015 solver.cpp:471] Iteration 35000, Testing net (#0)
I0630 01:56:54.000535 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9089
I0630 01:56:54.000555 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9959
I0630 01:56:54.000560 29015 solver.cpp:544]     Test net output #2: loss = 0.214 (* 1 = 0.214 loss)
I0630 01:56:54.020362 29015 solver.cpp:290] Iteration 35000 (27.074 iter/s, 3.69359s/100 iter), loss = 0
I0630 01:56:54.020390 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:54.020395 29015 sgd_solver.cpp:106] Iteration 35000, lr = 0.00453125
I0630 01:56:54.020926 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.72
I0630 01:56:54.926093 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:56:56.985404 29015 solver.cpp:290] Iteration 35100 (33.7277 iter/s, 2.96493s/100 iter), loss = 0
I0630 01:56:56.985426 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:56.985433 29015 sgd_solver.cpp:106] Iteration 35100, lr = 0.00451563
I0630 01:56:59.039710 29015 solver.cpp:290] Iteration 35200 (48.6803 iter/s, 2.05422s/100 iter), loss = 0
I0630 01:56:59.039732 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:59.039739 29015 sgd_solver.cpp:106] Iteration 35200, lr = 0.0045
I0630 01:57:01.100965 29015 solver.cpp:290] Iteration 35300 (48.5162 iter/s, 2.06117s/100 iter), loss = 0
I0630 01:57:01.101035 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:01.101043 29015 sgd_solver.cpp:106] Iteration 35300, lr = 0.00448438
I0630 01:57:03.158298 29015 solver.cpp:290] Iteration 35400 (48.6097 iter/s, 2.0572s/100 iter), loss = 0
I0630 01:57:03.158319 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:03.158327 29015 sgd_solver.cpp:106] Iteration 35400, lr = 0.00446875
I0630 01:57:05.213938 29015 solver.cpp:290] Iteration 35500 (48.6487 iter/s, 2.05555s/100 iter), loss = 0
I0630 01:57:05.213961 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:05.213968 29015 sgd_solver.cpp:106] Iteration 35500, lr = 0.00445312
I0630 01:57:07.267876 29015 solver.cpp:290] Iteration 35600 (48.689 iter/s, 2.05385s/100 iter), loss = 0
I0630 01:57:07.267899 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:07.267905 29015 sgd_solver.cpp:106] Iteration 35600, lr = 0.0044375
I0630 01:57:09.330889 29015 solver.cpp:290] Iteration 35700 (48.4749 iter/s, 2.06292s/100 iter), loss = 0
I0630 01:57:09.330914 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:09.330921 29015 sgd_solver.cpp:106] Iteration 35700, lr = 0.00442187
I0630 01:57:11.385553 29015 solver.cpp:290] Iteration 35800 (48.6719 iter/s, 2.05457s/100 iter), loss = 0
I0630 01:57:11.385579 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:11.385588 29015 sgd_solver.cpp:106] Iteration 35800, lr = 0.00440625
I0630 01:57:13.441962 29015 solver.cpp:290] Iteration 35900 (48.6306 iter/s, 2.05632s/100 iter), loss = 0
I0630 01:57:13.441985 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:13.441992 29015 sgd_solver.cpp:106] Iteration 35900, lr = 0.00439062
I0630 01:57:15.477509 29015 solver.cpp:354] Sparsity after update:
I0630 01:57:15.478770 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:57:15.478775 29015 net.cpp:1851] conv1a_param_0(0.36) 
I0630 01:57:15.478782 29015 net.cpp:1851] conv1b_param_0(0.72) 
I0630 01:57:15.478785 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:57:15.478787 29015 net.cpp:1851] res2a_branch2a_param_0(0.72) 
I0630 01:57:15.478790 29015 net.cpp:1851] res2a_branch2b_param_0(0.72) 
I0630 01:57:15.478791 29015 net.cpp:1851] res3a_branch2a_param_0(0.72) 
I0630 01:57:15.478793 29015 net.cpp:1851] res3a_branch2b_param_0(0.72) 
I0630 01:57:15.478796 29015 net.cpp:1851] res4a_branch2a_param_0(0.72) 
I0630 01:57:15.478797 29015 net.cpp:1851] res4a_branch2b_param_0(0.72) 
I0630 01:57:15.478799 29015 net.cpp:1851] res5a_branch2a_param_0(0.72) 
I0630 01:57:15.478801 29015 net.cpp:1851] res5a_branch2b_param_0(0.72) 
I0630 01:57:15.478803 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.69457e+06/2.3599e+06) 0.718
I0630 01:57:15.478894 29015 solver.cpp:471] Iteration 36000, Testing net (#0)
I0630 01:57:17.115919 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.908
I0630 01:57:17.115938 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9955
I0630 01:57:17.115943 29015 solver.cpp:544]     Test net output #2: loss = 0.2171 (* 1 = 0.2171 loss)
I0630 01:57:17.135876 29015 solver.cpp:290] Iteration 36000 (27.0725 iter/s, 3.69378s/100 iter), loss = 0
I0630 01:57:17.135892 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:17.135905 29015 sgd_solver.cpp:106] Iteration 36000, lr = 0.004375
I0630 01:57:17.136431 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.74
I0630 01:57:18.088382 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:57:20.148715 29015 solver.cpp:290] Iteration 36100 (33.1925 iter/s, 3.01273s/100 iter), loss = 0
I0630 01:57:20.148736 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:20.148743 29015 sgd_solver.cpp:106] Iteration 36100, lr = 0.00435938
I0630 01:57:22.205132 29015 solver.cpp:290] Iteration 36200 (48.6303 iter/s, 2.05633s/100 iter), loss = 0
I0630 01:57:22.205168 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:22.205174 29015 sgd_solver.cpp:106] Iteration 36200, lr = 0.00434375
I0630 01:57:24.261343 29015 solver.cpp:290] Iteration 36300 (48.6355 iter/s, 2.05611s/100 iter), loss = 0
I0630 01:57:24.261375 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:24.261386 29015 sgd_solver.cpp:106] Iteration 36300, lr = 0.00432813
I0630 01:57:26.317399 29015 solver.cpp:290] Iteration 36400 (48.6391 iter/s, 2.05596s/100 iter), loss = 0
I0630 01:57:26.317427 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:26.317436 29015 sgd_solver.cpp:106] Iteration 36400, lr = 0.0043125
I0630 01:57:28.374474 29015 solver.cpp:290] Iteration 36500 (48.6149 iter/s, 2.05698s/100 iter), loss = 0
I0630 01:57:28.374495 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:28.374502 29015 sgd_solver.cpp:106] Iteration 36500, lr = 0.00429688
I0630 01:57:30.427381 29015 solver.cpp:290] Iteration 36600 (48.7135 iter/s, 2.05282s/100 iter), loss = 0
I0630 01:57:30.427402 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:30.427408 29015 sgd_solver.cpp:106] Iteration 36600, lr = 0.00428125
I0630 01:57:32.479621 29015 solver.cpp:290] Iteration 36700 (48.7293 iter/s, 2.05215s/100 iter), loss = 0
I0630 01:57:32.479693 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:32.479701 29015 sgd_solver.cpp:106] Iteration 36700, lr = 0.00426562
I0630 01:57:34.534464 29015 solver.cpp:290] Iteration 36800 (48.6687 iter/s, 2.05471s/100 iter), loss = 0
I0630 01:57:34.534488 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:34.534497 29015 sgd_solver.cpp:106] Iteration 36800, lr = 0.00425
I0630 01:57:36.589046 29015 solver.cpp:290] Iteration 36900 (48.6738 iter/s, 2.0545s/100 iter), loss = 0
I0630 01:57:36.589071 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:36.589079 29015 sgd_solver.cpp:106] Iteration 36900, lr = 0.00423437
I0630 01:57:38.628594 29015 solver.cpp:354] Sparsity after update:
I0630 01:57:38.629873 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:57:38.629880 29015 net.cpp:1851] conv1a_param_0(0.37) 
I0630 01:57:38.629889 29015 net.cpp:1851] conv1b_param_0(0.74) 
I0630 01:57:38.629891 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:57:38.629894 29015 net.cpp:1851] res2a_branch2a_param_0(0.74) 
I0630 01:57:38.629897 29015 net.cpp:1851] res2a_branch2b_param_0(0.74) 
I0630 01:57:38.629899 29015 net.cpp:1851] res3a_branch2a_param_0(0.74) 
I0630 01:57:38.629901 29015 net.cpp:1851] res3a_branch2b_param_0(0.74) 
I0630 01:57:38.629904 29015 net.cpp:1851] res4a_branch2a_param_0(0.74) 
I0630 01:57:38.629906 29015 net.cpp:1851] res4a_branch2b_param_0(0.74) 
I0630 01:57:38.629909 29015 net.cpp:1851] res5a_branch2a_param_0(0.74) 
I0630 01:57:38.629911 29015 net.cpp:1851] res5a_branch2b_param_0(0.74) 
I0630 01:57:38.629914 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.74164e+06/2.3599e+06) 0.738
I0630 01:57:38.630000 29015 solver.cpp:471] Iteration 37000, Testing net (#0)
I0630 01:57:40.264883 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9056
I0630 01:57:40.264902 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9956
I0630 01:57:40.264907 29015 solver.cpp:544]     Test net output #2: loss = 0.2166 (* 1 = 0.2166 loss)
I0630 01:57:40.284719 29015 solver.cpp:290] Iteration 37000 (27.0596 iter/s, 3.69554s/100 iter), loss = 0
I0630 01:57:40.284736 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:40.284749 29015 sgd_solver.cpp:106] Iteration 37000, lr = 0.00421875
I0630 01:57:40.285287 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.76
I0630 01:57:41.251469 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:57:43.309186 29015 solver.cpp:290] Iteration 37100 (33.0649 iter/s, 3.02436s/100 iter), loss = 0
I0630 01:57:43.309211 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:43.309217 29015 sgd_solver.cpp:106] Iteration 37100, lr = 0.00420313
I0630 01:57:45.365622 29015 solver.cpp:290] Iteration 37200 (48.6299 iter/s, 2.05635s/100 iter), loss = 0
I0630 01:57:45.365643 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:45.365649 29015 sgd_solver.cpp:106] Iteration 37200, lr = 0.0041875
I0630 01:57:47.420632 29015 solver.cpp:290] Iteration 37300 (48.6636 iter/s, 2.05492s/100 iter), loss = 0
I0630 01:57:47.420655 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:47.420662 29015 sgd_solver.cpp:106] Iteration 37300, lr = 0.00417187
I0630 01:57:49.475750 29015 solver.cpp:290] Iteration 37400 (48.6611 iter/s, 2.05503s/100 iter), loss = 0
I0630 01:57:49.475772 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:49.475780 29015 sgd_solver.cpp:106] Iteration 37400, lr = 0.00415625
I0630 01:57:51.528851 29015 solver.cpp:290] Iteration 37500 (48.7089 iter/s, 2.05301s/100 iter), loss = 0
I0630 01:57:51.528872 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:51.528879 29015 sgd_solver.cpp:106] Iteration 37500, lr = 0.00414062
I0630 01:57:53.587291 29015 solver.cpp:290] Iteration 37600 (48.5825 iter/s, 2.05835s/100 iter), loss = 0
I0630 01:57:53.587327 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:53.587334 29015 sgd_solver.cpp:106] Iteration 37600, lr = 0.004125
I0630 01:57:55.643421 29015 solver.cpp:290] Iteration 37700 (48.6374 iter/s, 2.05603s/100 iter), loss = 0
I0630 01:57:55.643445 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:55.643453 29015 sgd_solver.cpp:106] Iteration 37700, lr = 0.00410937
I0630 01:57:57.698813 29015 solver.cpp:290] Iteration 37800 (48.6546 iter/s, 2.0553s/100 iter), loss = 0
I0630 01:57:57.698843 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:57.698853 29015 sgd_solver.cpp:106] Iteration 37800, lr = 0.00409375
I0630 01:57:59.757151 29015 solver.cpp:290] Iteration 37900 (48.5851 iter/s, 2.05825s/100 iter), loss = 0
I0630 01:57:59.757174 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:59.757182 29015 sgd_solver.cpp:106] Iteration 37900, lr = 0.00407812
I0630 01:58:01.793303 29015 solver.cpp:354] Sparsity after update:
I0630 01:58:01.794575 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:58:01.794582 29015 net.cpp:1851] conv1a_param_0(0.38) 
I0630 01:58:01.794589 29015 net.cpp:1851] conv1b_param_0(0.76) 
I0630 01:58:01.794595 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:58:01.794598 29015 net.cpp:1851] res2a_branch2a_param_0(0.76) 
I0630 01:58:01.794603 29015 net.cpp:1851] res2a_branch2b_param_0(0.76) 
I0630 01:58:01.794607 29015 net.cpp:1851] res3a_branch2a_param_0(0.76) 
I0630 01:58:01.794611 29015 net.cpp:1851] res3a_branch2b_param_0(0.76) 
I0630 01:58:01.794615 29015 net.cpp:1851] res4a_branch2a_param_0(0.749) 
I0630 01:58:01.794620 29015 net.cpp:1851] res4a_branch2b_param_0(0.76) 
I0630 01:58:01.794622 29015 net.cpp:1851] res5a_branch2a_param_0(0.76) 
I0630 01:58:01.794626 29015 net.cpp:1851] res5a_branch2b_param_0(0.76) 
I0630 01:58:01.794631 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.78541e+06/2.3599e+06) 0.757
I0630 01:58:01.794721 29015 solver.cpp:471] Iteration 38000, Testing net (#0)
I0630 01:58:03.430794 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9043
I0630 01:58:03.430845 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9954
I0630 01:58:03.430852 29015 solver.cpp:544]     Test net output #2: loss = 0.2219 (* 1 = 0.2219 loss)
I0630 01:58:03.450937 29015 solver.cpp:290] Iteration 38000 (27.0735 iter/s, 3.69365s/100 iter), loss = 0
I0630 01:58:03.450958 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:03.450965 29015 sgd_solver.cpp:106] Iteration 38000, lr = 0.0040625
I0630 01:58:03.451719 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.78
I0630 01:58:04.457129 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:58:06.513922 29015 solver.cpp:290] Iteration 38100 (32.6491 iter/s, 3.06287s/100 iter), loss = 0
I0630 01:58:06.513943 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:06.513952 29015 sgd_solver.cpp:106] Iteration 38100, lr = 0.00404688
I0630 01:58:08.574615 29015 solver.cpp:290] Iteration 38200 (48.5294 iter/s, 2.06061s/100 iter), loss = 0
I0630 01:58:08.574638 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:08.574645 29015 sgd_solver.cpp:106] Iteration 38200, lr = 0.00403125
I0630 01:58:10.630375 29015 solver.cpp:290] Iteration 38300 (48.6459 iter/s, 2.05567s/100 iter), loss = 0
I0630 01:58:10.630398 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:10.630403 29015 sgd_solver.cpp:106] Iteration 38300, lr = 0.00401562
I0630 01:58:12.688112 29015 solver.cpp:290] Iteration 38400 (48.5991 iter/s, 2.05765s/100 iter), loss = 0
I0630 01:58:12.688135 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:12.688143 29015 sgd_solver.cpp:106] Iteration 38400, lr = 0.004
I0630 01:58:14.747319 29015 solver.cpp:290] Iteration 38500 (48.5645 iter/s, 2.05912s/100 iter), loss = 0
I0630 01:58:14.747341 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:14.747347 29015 sgd_solver.cpp:106] Iteration 38500, lr = 0.00398437
I0630 01:58:16.802923 29015 solver.cpp:290] Iteration 38600 (48.6496 iter/s, 2.05552s/100 iter), loss = 0
I0630 01:58:16.802947 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:16.802953 29015 sgd_solver.cpp:106] Iteration 38600, lr = 0.00396875
I0630 01:58:18.861778 29015 solver.cpp:290] Iteration 38700 (48.5728 iter/s, 2.05876s/100 iter), loss = 0
I0630 01:58:18.861799 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:18.861806 29015 sgd_solver.cpp:106] Iteration 38700, lr = 0.00395312
I0630 01:58:20.916635 29015 solver.cpp:290] Iteration 38800 (48.6672 iter/s, 2.05477s/100 iter), loss = 0
I0630 01:58:20.916657 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:20.916664 29015 sgd_solver.cpp:106] Iteration 38800, lr = 0.0039375
I0630 01:58:22.979776 29015 solver.cpp:290] Iteration 38900 (48.4718 iter/s, 2.06305s/100 iter), loss = 0
I0630 01:58:22.979799 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:22.979805 29015 sgd_solver.cpp:106] Iteration 38900, lr = 0.00392187
I0630 01:58:25.013273 29015 solver.cpp:354] Sparsity after update:
I0630 01:58:25.014554 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:58:25.014559 29015 net.cpp:1851] conv1a_param_0(0.39) 
I0630 01:58:25.014566 29015 net.cpp:1851] conv1b_param_0(0.78) 
I0630 01:58:25.014569 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:58:25.014571 29015 net.cpp:1851] res2a_branch2a_param_0(0.78) 
I0630 01:58:25.014574 29015 net.cpp:1851] res2a_branch2b_param_0(0.773) 
I0630 01:58:25.014575 29015 net.cpp:1851] res3a_branch2a_param_0(0.78) 
I0630 01:58:25.014577 29015 net.cpp:1851] res3a_branch2b_param_0(0.78) 
I0630 01:58:25.014580 29015 net.cpp:1851] res4a_branch2a_param_0(0.758) 
I0630 01:58:25.014581 29015 net.cpp:1851] res4a_branch2b_param_0(0.78) 
I0630 01:58:25.014583 29015 net.cpp:1851] res5a_branch2a_param_0(0.78) 
I0630 01:58:25.014585 29015 net.cpp:1851] res5a_branch2b_param_0(0.78) 
I0630 01:58:25.014605 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.82918e+06/2.3599e+06) 0.775
I0630 01:58:25.014693 29015 solver.cpp:471] Iteration 39000, Testing net (#0)
I0630 01:58:26.651511 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.8996
I0630 01:58:26.651531 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9947
I0630 01:58:26.651536 29015 solver.cpp:544]     Test net output #2: loss = 0.2316 (* 1 = 0.2316 loss)
I0630 01:58:26.671046 29015 solver.cpp:290] Iteration 39000 (27.0919 iter/s, 3.69114s/100 iter), loss = 0
I0630 01:58:26.671061 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:26.671073 29015 sgd_solver.cpp:106] Iteration 39000, lr = 0.00390625
I0630 01:58:26.671614 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.8
I0630 01:58:27.700516 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:58:29.755210 29015 solver.cpp:290] Iteration 39100 (32.4248 iter/s, 3.08406s/100 iter), loss = 0
I0630 01:58:29.755234 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:29.755239 29015 sgd_solver.cpp:106] Iteration 39100, lr = 0.00389063
I0630 01:58:31.810211 29015 solver.cpp:290] Iteration 39200 (48.6638 iter/s, 2.05491s/100 iter), loss = 0
I0630 01:58:31.810232 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:31.810240 29015 sgd_solver.cpp:106] Iteration 39200, lr = 0.003875
I0630 01:58:33.868337 29015 solver.cpp:290] Iteration 39300 (48.5899 iter/s, 2.05804s/100 iter), loss = 0
I0630 01:58:33.868407 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:33.868414 29015 sgd_solver.cpp:106] Iteration 39300, lr = 0.00385938
I0630 01:58:35.921938 29015 solver.cpp:290] Iteration 39400 (48.6981 iter/s, 2.05347s/100 iter), loss = 0
I0630 01:58:35.921962 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:35.921970 29015 sgd_solver.cpp:106] Iteration 39400, lr = 0.00384375
I0630 01:58:37.976224 29015 solver.cpp:290] Iteration 39500 (48.6808 iter/s, 2.0542s/100 iter), loss = 0
I0630 01:58:37.976248 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:37.976253 29015 sgd_solver.cpp:106] Iteration 39500, lr = 0.00382812
I0630 01:58:40.035140 29015 solver.cpp:290] Iteration 39600 (48.5714 iter/s, 2.05883s/100 iter), loss = 0
I0630 01:58:40.035161 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:40.035168 29015 sgd_solver.cpp:106] Iteration 39600, lr = 0.0038125
I0630 01:58:42.091280 29015 solver.cpp:290] Iteration 39700 (48.6369 iter/s, 2.05605s/100 iter), loss = 0
I0630 01:58:42.091305 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:42.091315 29015 sgd_solver.cpp:106] Iteration 39700, lr = 0.00379687
I0630 01:58:44.147649 29015 solver.cpp:290] Iteration 39800 (48.6315 iter/s, 2.05628s/100 iter), loss = 0
I0630 01:58:44.147670 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:44.147676 29015 sgd_solver.cpp:106] Iteration 39800, lr = 0.00378125
I0630 01:58:46.201027 29015 solver.cpp:290] Iteration 39900 (48.7023 iter/s, 2.05329s/100 iter), loss = 0
I0630 01:58:46.201050 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:46.201056 29015 sgd_solver.cpp:106] Iteration 39900, lr = 0.00376562
I0630 01:58:48.235278 29015 solver.cpp:598] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_40000.caffemodel
I0630 01:58:48.252578 29015 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_40000.solverstate
I0630 01:58:48.259923 29015 solver.cpp:354] Sparsity after update:
I0630 01:58:48.260892 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:58:48.260900 29015 net.cpp:1851] conv1a_param_0(0.4) 
I0630 01:58:48.260908 29015 net.cpp:1851] conv1b_param_0(0.8) 
I0630 01:58:48.260910 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:58:48.260912 29015 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 01:58:48.260915 29015 net.cpp:1851] res2a_branch2b_param_0(0.777) 
I0630 01:58:48.260916 29015 net.cpp:1851] res3a_branch2a_param_0(0.795) 
I0630 01:58:48.260918 29015 net.cpp:1851] res3a_branch2b_param_0(0.792) 
I0630 01:58:48.260921 29015 net.cpp:1851] res4a_branch2a_param_0(0.769) 
I0630 01:58:48.260922 29015 net.cpp:1851] res4a_branch2b_param_0(0.797) 
I0630 01:58:48.260924 29015 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 01:58:48.260926 29015 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 01:58:48.260928 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.87241e+06/2.3599e+06) 0.793
I0630 01:58:48.261028 29015 solver.cpp:471] Iteration 40000, Testing net (#0)
I0630 01:58:49.896380 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9006
I0630 01:58:49.896399 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9953
I0630 01:58:49.896404 29015 solver.cpp:544]     Test net output #2: loss = 0.2178 (* 1 = 0.2178 loss)
I0630 01:58:49.916481 29015 solver.cpp:290] Iteration 40000 (26.9156 iter/s, 3.71532s/100 iter), loss = 0
I0630 01:58:49.916499 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:49.916509 29015 sgd_solver.cpp:106] Iteration 40000, lr = 0.00375
I0630 01:58:49.917026 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.82
I0630 01:58:51.013628 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:58:53.070605 29015 solver.cpp:290] Iteration 40100 (31.7057 iter/s, 3.15401s/100 iter), loss = 0
I0630 01:58:53.070627 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:53.070633 29015 sgd_solver.cpp:106] Iteration 40100, lr = 0.00373438
I0630 01:58:55.128373 29015 solver.cpp:290] Iteration 40200 (48.5984 iter/s, 2.05768s/100 iter), loss = 0
I0630 01:58:55.128398 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:55.128407 29015 sgd_solver.cpp:106] Iteration 40200, lr = 0.00371875
I0630 01:58:57.183152 29015 solver.cpp:290] Iteration 40300 (48.6691 iter/s, 2.05469s/100 iter), loss = 0
I0630 01:58:57.183174 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:57.183181 29015 sgd_solver.cpp:106] Iteration 40300, lr = 0.00370313
I0630 01:58:59.235651 29015 solver.cpp:290] Iteration 40400 (48.7231 iter/s, 2.05241s/100 iter), loss = 0.0952381
I0630 01:58:59.235674 29015 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:58:59.235680 29015 sgd_solver.cpp:106] Iteration 40400, lr = 0.0036875
I0630 01:59:01.291451 29015 solver.cpp:290] Iteration 40500 (48.6449 iter/s, 2.05571s/100 iter), loss = 0
I0630 01:59:01.291472 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:01.291481 29015 sgd_solver.cpp:106] Iteration 40500, lr = 0.00367187
I0630 01:59:03.346729 29015 solver.cpp:290] Iteration 40600 (48.6572 iter/s, 2.05519s/100 iter), loss = 0.047619
I0630 01:59:03.346752 29015 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:59:03.346760 29015 sgd_solver.cpp:106] Iteration 40600, lr = 0.00365625
I0630 01:59:05.399518 29015 solver.cpp:290] Iteration 40700 (48.7163 iter/s, 2.0527s/100 iter), loss = 0
I0630 01:59:05.399590 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:05.399597 29015 sgd_solver.cpp:106] Iteration 40700, lr = 0.00364062
I0630 01:59:07.456954 29015 solver.cpp:290] Iteration 40800 (48.6074 iter/s, 2.0573s/100 iter), loss = 0
I0630 01:59:07.456975 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:07.456982 29015 sgd_solver.cpp:106] Iteration 40800, lr = 0.003625
I0630 01:59:09.531613 29015 solver.cpp:290] Iteration 40900 (48.2027 iter/s, 2.07457s/100 iter), loss = 0
I0630 01:59:09.531635 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:09.531642 29015 sgd_solver.cpp:106] Iteration 40900, lr = 0.00360937
I0630 01:59:11.576593 29015 solver.cpp:354] Sparsity after update:
I0630 01:59:11.577878 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:59:11.577884 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 01:59:11.577891 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 01:59:11.577894 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:59:11.577896 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 01:59:11.577898 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 01:59:11.577900 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 01:59:11.577903 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 01:59:11.577904 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 01:59:11.577906 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 01:59:11.577908 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 01:59:11.577910 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 01:59:11.577913 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 01:59:11.578001 29015 solver.cpp:471] Iteration 41000, Testing net (#0)
I0630 01:59:13.216274 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9001
I0630 01:59:13.216295 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9954
I0630 01:59:13.216300 29015 solver.cpp:544]     Test net output #2: loss = 0.223 (* 1 = 0.223 loss)
I0630 01:59:13.235980 29015 solver.cpp:290] Iteration 41000 (26.9961 iter/s, 3.70424s/100 iter), loss = 0
I0630 01:59:13.235996 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:13.236008 29015 sgd_solver.cpp:106] Iteration 41000, lr = 0.00359375
I0630 01:59:15.293154 29015 solver.cpp:290] Iteration 41100 (48.6123 iter/s, 2.05709s/100 iter), loss = 0
I0630 01:59:15.293175 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:15.293182 29015 sgd_solver.cpp:106] Iteration 41100, lr = 0.00357813
I0630 01:59:17.353003 29015 solver.cpp:290] Iteration 41200 (48.5493 iter/s, 2.05976s/100 iter), loss = 0
I0630 01:59:17.353024 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:17.353031 29015 sgd_solver.cpp:106] Iteration 41200, lr = 0.0035625
I0630 01:59:19.412956 29015 solver.cpp:290] Iteration 41300 (48.5468 iter/s, 2.05987s/100 iter), loss = 0
I0630 01:59:19.412978 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:19.412986 29015 sgd_solver.cpp:106] Iteration 41300, lr = 0.00354687
I0630 01:59:21.467670 29015 solver.cpp:290] Iteration 41400 (48.6707 iter/s, 2.05463s/100 iter), loss = 0
I0630 01:59:21.467697 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:21.467706 29015 sgd_solver.cpp:106] Iteration 41400, lr = 0.00353125
I0630 01:59:23.528538 29015 solver.cpp:290] Iteration 41500 (48.5254 iter/s, 2.06078s/100 iter), loss = 0
I0630 01:59:23.528570 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:23.528581 29015 sgd_solver.cpp:106] Iteration 41500, lr = 0.00351562
I0630 01:59:25.585319 29015 solver.cpp:290] Iteration 41600 (48.6219 iter/s, 2.05669s/100 iter), loss = 0
I0630 01:59:25.585342 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:25.585348 29015 sgd_solver.cpp:106] Iteration 41600, lr = 0.0035
I0630 01:59:27.643427 29015 solver.cpp:290] Iteration 41700 (48.5904 iter/s, 2.05802s/100 iter), loss = 0
I0630 01:59:27.643466 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:27.643473 29015 sgd_solver.cpp:106] Iteration 41700, lr = 0.00348437
I0630 01:59:29.698385 29015 solver.cpp:290] Iteration 41800 (48.6652 iter/s, 2.05486s/100 iter), loss = 0
I0630 01:59:29.698407 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:29.698415 29015 sgd_solver.cpp:106] Iteration 41800, lr = 0.00346875
I0630 01:59:31.751552 29015 solver.cpp:290] Iteration 41900 (48.7073 iter/s, 2.05308s/100 iter), loss = 0
I0630 01:59:31.751575 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:31.751581 29015 sgd_solver.cpp:106] Iteration 41900, lr = 0.00345312
I0630 01:59:33.791646 29015 solver.cpp:354] Sparsity after update:
I0630 01:59:33.792919 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:59:33.792927 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 01:59:33.792934 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 01:59:33.792937 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:59:33.792939 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 01:59:33.792943 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 01:59:33.792944 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 01:59:33.792946 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 01:59:33.792948 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 01:59:33.792951 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 01:59:33.792953 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 01:59:33.792955 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 01:59:33.792958 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 01:59:33.793045 29015 solver.cpp:471] Iteration 42000, Testing net (#0)
I0630 01:59:35.432191 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9018
I0630 01:59:35.433840 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9951
I0630 01:59:35.433848 29015 solver.cpp:544]     Test net output #2: loss = 0.2235 (* 1 = 0.2235 loss)
I0630 01:59:35.453702 29015 solver.cpp:290] Iteration 42000 (27.0123 iter/s, 3.70202s/100 iter), loss = 0
I0630 01:59:35.453722 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:35.453729 29015 sgd_solver.cpp:106] Iteration 42000, lr = 0.0034375
I0630 01:59:37.506883 29015 solver.cpp:290] Iteration 42100 (48.7069 iter/s, 2.0531s/100 iter), loss = 0
I0630 01:59:37.506906 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:37.506911 29015 sgd_solver.cpp:106] Iteration 42100, lr = 0.00342188
I0630 01:59:39.559950 29015 solver.cpp:290] Iteration 42200 (48.7097 iter/s, 2.05298s/100 iter), loss = 0
I0630 01:59:39.559972 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:39.559979 29015 sgd_solver.cpp:106] Iteration 42200, lr = 0.00340625
I0630 01:59:41.618245 29015 solver.cpp:290] Iteration 42300 (48.5859 iter/s, 2.05821s/100 iter), loss = 0
I0630 01:59:41.618268 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:41.618274 29015 sgd_solver.cpp:106] Iteration 42300, lr = 0.00339063
I0630 01:59:43.677448 29015 solver.cpp:290] Iteration 42400 (48.5645 iter/s, 2.05912s/100 iter), loss = 0
I0630 01:59:43.677469 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:43.677476 29015 sgd_solver.cpp:106] Iteration 42400, lr = 0.003375
I0630 01:59:45.734961 29015 solver.cpp:290] Iteration 42500 (48.6044 iter/s, 2.05743s/100 iter), loss = 0
I0630 01:59:45.734983 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:45.734992 29015 sgd_solver.cpp:106] Iteration 42500, lr = 0.00335937
I0630 01:59:47.789335 29015 solver.cpp:290] Iteration 42600 (48.6787 iter/s, 2.05429s/100 iter), loss = 0
I0630 01:59:47.789357 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:47.789363 29015 sgd_solver.cpp:106] Iteration 42600, lr = 0.00334375
I0630 01:59:49.849511 29015 solver.cpp:290] Iteration 42700 (48.5416 iter/s, 2.06009s/100 iter), loss = 0
I0630 01:59:49.849532 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:49.849540 29015 sgd_solver.cpp:106] Iteration 42700, lr = 0.00332812
I0630 01:59:51.907040 29015 solver.cpp:290] Iteration 42800 (48.604 iter/s, 2.05744s/100 iter), loss = 0
I0630 01:59:51.907063 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:51.907069 29015 sgd_solver.cpp:106] Iteration 42800, lr = 0.0033125
I0630 01:59:53.971001 29015 solver.cpp:290] Iteration 42900 (48.4526 iter/s, 2.06387s/100 iter), loss = 0
I0630 01:59:53.971024 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:53.971031 29015 sgd_solver.cpp:106] Iteration 42900, lr = 0.00329687
I0630 01:59:56.008731 29015 solver.cpp:354] Sparsity after update:
I0630 01:59:56.010001 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:59:56.010009 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 01:59:56.010018 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 01:59:56.010025 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:59:56.010028 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 01:59:56.010032 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 01:59:56.010037 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 01:59:56.010040 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 01:59:56.010044 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 01:59:56.010048 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 01:59:56.010052 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 01:59:56.010056 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 01:59:56.010061 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 01:59:56.010150 29015 solver.cpp:471] Iteration 43000, Testing net (#0)
I0630 01:59:57.646554 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9035
I0630 01:59:57.646574 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9954
I0630 01:59:57.646579 29015 solver.cpp:544]     Test net output #2: loss = 0.2197 (* 1 = 0.2197 loss)
I0630 01:59:57.666486 29015 solver.cpp:290] Iteration 43000 (27.061 iter/s, 3.69535s/100 iter), loss = 0
I0630 01:59:57.666509 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:57.666515 29015 sgd_solver.cpp:106] Iteration 43000, lr = 0.00328125
I0630 01:59:59.725824 29015 solver.cpp:290] Iteration 43100 (48.5614 iter/s, 2.05925s/100 iter), loss = 0
I0630 01:59:59.725849 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:59.725857 29015 sgd_solver.cpp:106] Iteration 43100, lr = 0.00326563
I0630 02:00:01.781293 29015 solver.cpp:290] Iteration 43200 (48.6528 iter/s, 2.05538s/100 iter), loss = 0.047619
I0630 02:00:01.781316 29015 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 02:00:01.781322 29015 sgd_solver.cpp:106] Iteration 43200, lr = 0.00325
I0630 02:00:03.834702 29015 solver.cpp:290] Iteration 43300 (48.7016 iter/s, 2.05332s/100 iter), loss = 0
I0630 02:00:03.834724 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:03.834731 29015 sgd_solver.cpp:106] Iteration 43300, lr = 0.00323438
I0630 02:00:05.894106 29015 solver.cpp:290] Iteration 43400 (48.5598 iter/s, 2.05932s/100 iter), loss = 0
I0630 02:00:05.894156 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:05.894165 29015 sgd_solver.cpp:106] Iteration 43400, lr = 0.00321875
I0630 02:00:07.950390 29015 solver.cpp:290] Iteration 43500 (48.6341 iter/s, 2.05617s/100 iter), loss = 0
I0630 02:00:07.950413 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:07.950420 29015 sgd_solver.cpp:106] Iteration 43500, lr = 0.00320312
I0630 02:00:10.054548 29015 solver.cpp:290] Iteration 43600 (47.527 iter/s, 2.10407s/100 iter), loss = 0
I0630 02:00:10.054571 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:10.054577 29015 sgd_solver.cpp:106] Iteration 43600, lr = 0.0031875
I0630 02:00:12.110186 29015 solver.cpp:290] Iteration 43700 (48.6488 iter/s, 2.05555s/100 iter), loss = 0
I0630 02:00:12.110208 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:12.110218 29015 sgd_solver.cpp:106] Iteration 43700, lr = 0.00317187
I0630 02:00:14.167547 29015 solver.cpp:290] Iteration 43800 (48.608 iter/s, 2.05727s/100 iter), loss = 0
I0630 02:00:14.167570 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:14.167577 29015 sgd_solver.cpp:106] Iteration 43800, lr = 0.00315625
I0630 02:00:16.226222 29015 solver.cpp:290] Iteration 43900 (48.577 iter/s, 2.05859s/100 iter), loss = 0
I0630 02:00:16.226243 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:16.226250 29015 sgd_solver.cpp:106] Iteration 43900, lr = 0.00314062
I0630 02:00:18.263720 29015 solver.cpp:354] Sparsity after update:
I0630 02:00:18.264983 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:00:18.264991 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:00:18.264998 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:00:18.265002 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:00:18.265004 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:00:18.265007 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:00:18.265010 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:00:18.265012 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:00:18.265015 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:00:18.265018 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:00:18.265020 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:00:18.265023 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:00:18.265027 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:00:18.265117 29015 solver.cpp:471] Iteration 44000, Testing net (#0)
I0630 02:00:19.900705 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9064
I0630 02:00:19.900723 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.996
I0630 02:00:19.900729 29015 solver.cpp:544]     Test net output #2: loss = 0.2205 (* 1 = 0.2205 loss)
I0630 02:00:19.921542 29015 solver.cpp:290] Iteration 44000 (27.0622 iter/s, 3.69519s/100 iter), loss = 0
I0630 02:00:19.921560 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:19.921571 29015 sgd_solver.cpp:106] Iteration 44000, lr = 0.003125
I0630 02:00:21.975893 29015 solver.cpp:290] Iteration 44100 (48.6791 iter/s, 2.05427s/100 iter), loss = 0.047619
I0630 02:00:21.975914 29015 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 02:00:21.975922 29015 sgd_solver.cpp:106] Iteration 44100, lr = 0.00310938
I0630 02:00:24.036231 29015 solver.cpp:290] Iteration 44200 (48.5377 iter/s, 2.06025s/100 iter), loss = 0
I0630 02:00:24.036257 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:24.036265 29015 sgd_solver.cpp:106] Iteration 44200, lr = 0.00309375
I0630 02:00:26.095580 29015 solver.cpp:290] Iteration 44300 (48.5612 iter/s, 2.05926s/100 iter), loss = 0
I0630 02:00:26.095603 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:26.095613 29015 sgd_solver.cpp:106] Iteration 44300, lr = 0.00307812
I0630 02:00:28.154667 29015 solver.cpp:290] Iteration 44400 (48.5673 iter/s, 2.059s/100 iter), loss = 0
I0630 02:00:28.154690 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:28.154697 29015 sgd_solver.cpp:106] Iteration 44400, lr = 0.0030625
I0630 02:00:30.211853 29015 solver.cpp:290] Iteration 44500 (48.6122 iter/s, 2.0571s/100 iter), loss = 0
I0630 02:00:30.211876 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:30.211885 29015 sgd_solver.cpp:106] Iteration 44500, lr = 0.00304687
I0630 02:00:32.268239 29015 solver.cpp:290] Iteration 44600 (48.6311 iter/s, 2.0563s/100 iter), loss = 0
I0630 02:00:32.268261 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:32.268267 29015 sgd_solver.cpp:106] Iteration 44600, lr = 0.00303125
I0630 02:00:34.327764 29015 solver.cpp:290] Iteration 44700 (48.557 iter/s, 2.05944s/100 iter), loss = 0
I0630 02:00:34.327785 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:34.327791 29015 sgd_solver.cpp:106] Iteration 44700, lr = 0.00301562
I0630 02:00:36.384623 29015 solver.cpp:290] Iteration 44800 (48.6199 iter/s, 2.05677s/100 iter), loss = 0
I0630 02:00:36.384678 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:36.384688 29015 sgd_solver.cpp:106] Iteration 44800, lr = 0.003
I0630 02:00:38.441824 29015 solver.cpp:290] Iteration 44900 (48.6125 iter/s, 2.05708s/100 iter), loss = 0
I0630 02:00:38.441848 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:38.441854 29015 sgd_solver.cpp:106] Iteration 44900, lr = 0.00298437
I0630 02:00:40.476889 29015 solver.cpp:354] Sparsity after update:
I0630 02:00:40.478153 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:00:40.478160 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:00:40.478170 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:00:40.478174 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:00:40.478179 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:00:40.478184 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:00:40.478189 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:00:40.478194 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:00:40.478199 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:00:40.478204 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:00:40.478209 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:00:40.478214 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:00:40.478219 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:00:40.478312 29015 solver.cpp:471] Iteration 45000, Testing net (#0)
I0630 02:00:42.113857 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9044
I0630 02:00:42.113875 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.996
I0630 02:00:42.113880 29015 solver.cpp:544]     Test net output #2: loss = 0.22 (* 1 = 0.22 loss)
I0630 02:00:42.134026 29015 solver.cpp:290] Iteration 45000 (27.0851 iter/s, 3.69207s/100 iter), loss = 0
I0630 02:00:42.134043 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:42.134054 29015 sgd_solver.cpp:106] Iteration 45000, lr = 0.00296875
I0630 02:00:44.194556 29015 solver.cpp:290] Iteration 45100 (48.5331 iter/s, 2.06045s/100 iter), loss = 0
I0630 02:00:44.194579 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:44.194586 29015 sgd_solver.cpp:106] Iteration 45100, lr = 0.00295313
I0630 02:00:46.250414 29015 solver.cpp:290] Iteration 45200 (48.6435 iter/s, 2.05577s/100 iter), loss = 0
I0630 02:00:46.250437 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:46.250443 29015 sgd_solver.cpp:106] Iteration 45200, lr = 0.0029375
I0630 02:00:48.305409 29015 solver.cpp:290] Iteration 45300 (48.664 iter/s, 2.05491s/100 iter), loss = 0
I0630 02:00:48.305430 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:48.305438 29015 sgd_solver.cpp:106] Iteration 45300, lr = 0.00292188
I0630 02:00:50.361637 29015 solver.cpp:290] Iteration 45400 (48.6348 iter/s, 2.05614s/100 iter), loss = 0
I0630 02:00:50.361660 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:50.361668 29015 sgd_solver.cpp:106] Iteration 45400, lr = 0.00290625
I0630 02:00:52.415776 29015 solver.cpp:290] Iteration 45500 (48.6843 iter/s, 2.05405s/100 iter), loss = 0
I0630 02:00:52.415798 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:52.415805 29015 sgd_solver.cpp:106] Iteration 45500, lr = 0.00289063
I0630 02:00:54.474714 29015 solver.cpp:290] Iteration 45600 (48.5708 iter/s, 2.05885s/100 iter), loss = 0
I0630 02:00:54.474740 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:54.474750 29015 sgd_solver.cpp:106] Iteration 45600, lr = 0.002875
I0630 02:00:56.530061 29015 solver.cpp:290] Iteration 45700 (48.6558 iter/s, 2.05526s/100 iter), loss = 0
I0630 02:00:56.530082 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:56.530088 29015 sgd_solver.cpp:106] Iteration 45700, lr = 0.00285937
I0630 02:00:58.581972 29015 solver.cpp:290] Iteration 45800 (48.7371 iter/s, 2.05182s/100 iter), loss = 0
I0630 02:00:58.582010 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:58.582017 29015 sgd_solver.cpp:106] Iteration 45800, lr = 0.00284375
I0630 02:01:00.638586 29015 solver.cpp:290] Iteration 45900 (48.626 iter/s, 2.05651s/100 iter), loss = 0
I0630 02:01:00.638609 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:00.638617 29015 sgd_solver.cpp:106] Iteration 45900, lr = 0.00282812
I0630 02:01:02.679064 29015 solver.cpp:354] Sparsity after update:
I0630 02:01:02.680521 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:01:02.680529 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:01:02.680538 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:01:02.680541 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:01:02.680546 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:01:02.680550 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:01:02.680555 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:01:02.680559 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:01:02.680564 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:01:02.680567 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:01:02.680572 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:01:02.680575 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:01:02.680579 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:01:02.680718 29015 solver.cpp:471] Iteration 46000, Testing net (#0)
I0630 02:01:04.316561 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9042
I0630 02:01:04.316581 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.996
I0630 02:01:04.316586 29015 solver.cpp:544]     Test net output #2: loss = 0.2223 (* 1 = 0.2223 loss)
I0630 02:01:04.336379 29015 solver.cpp:290] Iteration 46000 (27.0441 iter/s, 3.69766s/100 iter), loss = 0
I0630 02:01:04.336396 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:04.336408 29015 sgd_solver.cpp:106] Iteration 46000, lr = 0.0028125
I0630 02:01:06.394995 29015 solver.cpp:290] Iteration 46100 (48.5783 iter/s, 2.05853s/100 iter), loss = 0
I0630 02:01:06.395077 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:06.395089 29015 sgd_solver.cpp:106] Iteration 46100, lr = 0.00279688
I0630 02:01:08.456874 29015 solver.cpp:290] Iteration 46200 (48.5028 iter/s, 2.06174s/100 iter), loss = 0
I0630 02:01:08.456897 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:08.456903 29015 sgd_solver.cpp:106] Iteration 46200, lr = 0.00278125
I0630 02:01:10.510331 29015 solver.cpp:290] Iteration 46300 (48.7005 iter/s, 2.05337s/100 iter), loss = 0
I0630 02:01:10.510354 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:10.510360 29015 sgd_solver.cpp:106] Iteration 46300, lr = 0.00276563
I0630 02:01:12.574115 29015 solver.cpp:290] Iteration 46400 (48.4567 iter/s, 2.0637s/100 iter), loss = 0
I0630 02:01:12.574137 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:12.574143 29015 sgd_solver.cpp:106] Iteration 46400, lr = 0.00275
I0630 02:01:14.628897 29015 solver.cpp:290] Iteration 46500 (48.669 iter/s, 2.05469s/100 iter), loss = 0
I0630 02:01:14.628927 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:14.628937 29015 sgd_solver.cpp:106] Iteration 46500, lr = 0.00273437
I0630 02:01:16.685899 29015 solver.cpp:290] Iteration 46600 (48.6167 iter/s, 2.05691s/100 iter), loss = 0
I0630 02:01:16.685925 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:16.685931 29015 sgd_solver.cpp:106] Iteration 46600, lr = 0.00271875
I0630 02:01:18.746517 29015 solver.cpp:290] Iteration 46700 (48.5312 iter/s, 2.06053s/100 iter), loss = 0
I0630 02:01:18.746539 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:18.746546 29015 sgd_solver.cpp:106] Iteration 46700, lr = 0.00270312
I0630 02:01:20.803338 29015 solver.cpp:290] Iteration 46800 (48.6208 iter/s, 2.05673s/100 iter), loss = 0
I0630 02:01:20.803359 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:20.803365 29015 sgd_solver.cpp:106] Iteration 46800, lr = 0.0026875
I0630 02:01:22.859953 29015 solver.cpp:290] Iteration 46900 (48.6256 iter/s, 2.05653s/100 iter), loss = 0
I0630 02:01:22.859975 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:22.859982 29015 sgd_solver.cpp:106] Iteration 46900, lr = 0.00267187
I0630 02:01:24.895453 29015 solver.cpp:354] Sparsity after update:
I0630 02:01:24.896728 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:01:24.896735 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:01:24.896745 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:01:24.896750 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:01:24.896755 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:01:24.896760 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:01:24.896764 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:01:24.896770 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:01:24.896775 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:01:24.896780 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:01:24.896785 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:01:24.896790 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:01:24.896795 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:01:24.896886 29015 solver.cpp:471] Iteration 47000, Testing net (#0)
I0630 02:01:26.533449 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9044
I0630 02:01:26.533468 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9961
I0630 02:01:26.533473 29015 solver.cpp:544]     Test net output #2: loss = 0.2204 (* 1 = 0.2204 loss)
I0630 02:01:26.554072 29015 solver.cpp:290] Iteration 47000 (27.071 iter/s, 3.69399s/100 iter), loss = 0
I0630 02:01:26.554090 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:26.554101 29015 sgd_solver.cpp:106] Iteration 47000, lr = 0.00265625
I0630 02:01:28.611484 29015 solver.cpp:290] Iteration 47100 (48.6067 iter/s, 2.05733s/100 iter), loss = 0
I0630 02:01:28.611521 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:28.611529 29015 sgd_solver.cpp:106] Iteration 47100, lr = 0.00264063
I0630 02:01:30.668738 29015 solver.cpp:290] Iteration 47200 (48.6109 iter/s, 2.05715s/100 iter), loss = 0
I0630 02:01:30.668761 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:30.668769 29015 sgd_solver.cpp:106] Iteration 47200, lr = 0.002625
I0630 02:01:32.729290 29015 solver.cpp:290] Iteration 47300 (48.5328 iter/s, 2.06046s/100 iter), loss = 0
I0630 02:01:32.729312 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:32.729322 29015 sgd_solver.cpp:106] Iteration 47300, lr = 0.00260938
I0630 02:01:34.786485 29015 solver.cpp:290] Iteration 47400 (48.6119 iter/s, 2.05711s/100 iter), loss = 0
I0630 02:01:34.786507 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:34.786514 29015 sgd_solver.cpp:106] Iteration 47400, lr = 0.00259375
I0630 02:01:36.843921 29015 solver.cpp:290] Iteration 47500 (48.6063 iter/s, 2.05735s/100 iter), loss = 0
I0630 02:01:36.843978 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:36.843986 29015 sgd_solver.cpp:106] Iteration 47500, lr = 0.00257812
I0630 02:01:38.902601 29015 solver.cpp:290] Iteration 47600 (48.5777 iter/s, 2.05856s/100 iter), loss = 0
I0630 02:01:38.902626 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:38.902636 29015 sgd_solver.cpp:106] Iteration 47600, lr = 0.0025625
I0630 02:01:40.955061 29015 solver.cpp:290] Iteration 47700 (48.7241 iter/s, 2.05237s/100 iter), loss = 0
I0630 02:01:40.955083 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:40.955090 29015 sgd_solver.cpp:106] Iteration 47700, lr = 0.00254687
I0630 02:01:43.008847 29015 solver.cpp:290] Iteration 47800 (48.6926 iter/s, 2.0537s/100 iter), loss = 0
I0630 02:01:43.008870 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:43.008877 29015 sgd_solver.cpp:106] Iteration 47800, lr = 0.00253125
I0630 02:01:45.064370 29015 solver.cpp:290] Iteration 47900 (48.6515 iter/s, 2.05543s/100 iter), loss = 0
I0630 02:01:45.064393 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:45.064399 29015 sgd_solver.cpp:106] Iteration 47900, lr = 0.00251562
I0630 02:01:47.103688 29015 solver.cpp:354] Sparsity after update:
I0630 02:01:47.104965 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:01:47.104974 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:01:47.104980 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:01:47.104984 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:01:47.104985 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:01:47.104987 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:01:47.104990 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:01:47.104991 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:01:47.104993 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:01:47.104995 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:01:47.104997 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:01:47.105000 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:01:47.105001 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:01:47.105093 29015 solver.cpp:471] Iteration 48000, Testing net (#0)
I0630 02:01:48.742105 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9056
I0630 02:01:48.742123 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9956
I0630 02:01:48.742130 29015 solver.cpp:544]     Test net output #2: loss = 0.224 (* 1 = 0.224 loss)
I0630 02:01:48.761814 29015 solver.cpp:290] Iteration 48000 (27.0467 iter/s, 3.69731s/100 iter), loss = 0
I0630 02:01:48.761833 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:48.761842 29015 sgd_solver.cpp:106] Iteration 48000, lr = 0.0025
I0630 02:01:50.816542 29015 solver.cpp:290] Iteration 48100 (48.6703 iter/s, 2.05464s/100 iter), loss = 0
I0630 02:01:50.816568 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:50.816577 29015 sgd_solver.cpp:106] Iteration 48100, lr = 0.00248438
I0630 02:01:52.872143 29015 solver.cpp:290] Iteration 48200 (48.6498 iter/s, 2.05551s/100 iter), loss = 0
I0630 02:01:52.872169 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:52.872177 29015 sgd_solver.cpp:106] Iteration 48200, lr = 0.00246875
I0630 02:01:54.928942 29015 solver.cpp:290] Iteration 48300 (48.6213 iter/s, 2.05671s/100 iter), loss = 0
I0630 02:01:54.928964 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:54.928972 29015 sgd_solver.cpp:106] Iteration 48300, lr = 0.00245313
I0630 02:01:56.989125 29015 solver.cpp:290] Iteration 48400 (48.5414 iter/s, 2.0601s/100 iter), loss = 0
I0630 02:01:56.989146 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:56.989153 29015 sgd_solver.cpp:106] Iteration 48400, lr = 0.0024375
I0630 02:01:59.042384 29015 solver.cpp:290] Iteration 48500 (48.7051 iter/s, 2.05317s/100 iter), loss = 0
I0630 02:01:59.042421 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:59.042428 29015 sgd_solver.cpp:106] Iteration 48500, lr = 0.00242188
I0630 02:02:01.098153 29015 solver.cpp:290] Iteration 48600 (48.646 iter/s, 2.05567s/100 iter), loss = 0
I0630 02:02:01.098175 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:01.098181 29015 sgd_solver.cpp:106] Iteration 48600, lr = 0.00240625
I0630 02:02:03.152230 29015 solver.cpp:290] Iteration 48700 (48.6857 iter/s, 2.05399s/100 iter), loss = 0
I0630 02:02:03.152251 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:03.152258 29015 sgd_solver.cpp:106] Iteration 48700, lr = 0.00239062
I0630 02:02:05.211640 29015 solver.cpp:290] Iteration 48800 (48.5596 iter/s, 2.05932s/100 iter), loss = 0
I0630 02:02:05.211663 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:05.211669 29015 sgd_solver.cpp:106] Iteration 48800, lr = 0.002375
I0630 02:02:07.268162 29015 solver.cpp:290] Iteration 48900 (48.6279 iter/s, 2.05643s/100 iter), loss = 0
I0630 02:02:07.268230 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:07.268239 29015 sgd_solver.cpp:106] Iteration 48900, lr = 0.00235937
I0630 02:02:09.320024 29015 solver.cpp:354] Sparsity after update:
I0630 02:02:09.321174 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:02:09.321182 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:02:09.321189 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:02:09.321192 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:02:09.321194 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:02:09.321197 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:02:09.321198 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:02:09.321200 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:02:09.321202 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:02:09.321204 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:02:09.321207 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:02:09.321208 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:02:09.321210 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:02:09.321297 29015 solver.cpp:471] Iteration 49000, Testing net (#0)
I0630 02:02:10.956575 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9037
I0630 02:02:10.956594 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9954
I0630 02:02:10.956600 29015 solver.cpp:544]     Test net output #2: loss = 0.2248 (* 1 = 0.2248 loss)
I0630 02:02:10.976692 29015 solver.cpp:290] Iteration 49000 (26.9661 iter/s, 3.70835s/100 iter), loss = 0
I0630 02:02:10.976712 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:10.976722 29015 sgd_solver.cpp:106] Iteration 49000, lr = 0.00234375
I0630 02:02:13.036352 29015 solver.cpp:290] Iteration 49100 (48.5537 iter/s, 2.05958s/100 iter), loss = 0
I0630 02:02:13.036375 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:13.036381 29015 sgd_solver.cpp:106] Iteration 49100, lr = 0.00232813
I0630 02:02:15.090219 29015 solver.cpp:290] Iteration 49200 (48.6907 iter/s, 2.05378s/100 iter), loss = 0
I0630 02:02:15.090240 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:15.090247 29015 sgd_solver.cpp:106] Iteration 49200, lr = 0.0023125
I0630 02:02:17.147071 29015 solver.cpp:290] Iteration 49300 (48.62 iter/s, 2.05677s/100 iter), loss = 0
I0630 02:02:17.147094 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:17.147100 29015 sgd_solver.cpp:106] Iteration 49300, lr = 0.00229687
I0630 02:02:19.206521 29015 solver.cpp:290] Iteration 49400 (48.5587 iter/s, 2.05936s/100 iter), loss = 0
I0630 02:02:19.206542 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:19.206548 29015 sgd_solver.cpp:106] Iteration 49400, lr = 0.00228125
I0630 02:02:21.261109 29015 solver.cpp:290] Iteration 49500 (48.6736 iter/s, 2.0545s/100 iter), loss = 0
I0630 02:02:21.261132 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:21.261138 29015 sgd_solver.cpp:106] Iteration 49500, lr = 0.00226562
I0630 02:02:23.317569 29015 solver.cpp:290] Iteration 49600 (48.6295 iter/s, 2.05636s/100 iter), loss = 0
I0630 02:02:23.317612 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:23.317620 29015 sgd_solver.cpp:106] Iteration 49600, lr = 0.00225
I0630 02:02:25.374791 29015 solver.cpp:290] Iteration 49700 (48.6117 iter/s, 2.05712s/100 iter), loss = 0
I0630 02:02:25.374814 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:25.374820 29015 sgd_solver.cpp:106] Iteration 49700, lr = 0.00223437
I0630 02:02:27.434957 29015 solver.cpp:290] Iteration 49800 (48.5418 iter/s, 2.06008s/100 iter), loss = 0
I0630 02:02:27.434979 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:27.434986 29015 sgd_solver.cpp:106] Iteration 49800, lr = 0.00221875
I0630 02:02:29.487274 29015 solver.cpp:290] Iteration 49900 (48.7275 iter/s, 2.05223s/100 iter), loss = 0
I0630 02:02:29.487313 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:29.487319 29015 sgd_solver.cpp:106] Iteration 49900, lr = 0.00220312
I0630 02:02:31.523243 29015 solver.cpp:598] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_50000.caffemodel
I0630 02:02:31.539969 29015 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_50000.solverstate
I0630 02:02:31.547328 29015 solver.cpp:354] Sparsity after update:
I0630 02:02:31.548307 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:02:31.548316 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:02:31.548324 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:02:31.548327 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:02:31.548331 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:02:31.548333 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:02:31.548336 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:02:31.548338 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:02:31.548341 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:02:31.548343 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:02:31.548346 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:02:31.548348 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:02:31.548351 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:02:31.548449 29015 solver.cpp:471] Iteration 50000, Testing net (#0)
I0630 02:02:33.185673 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9056
I0630 02:02:33.185691 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9955
I0630 02:02:33.185696 29015 solver.cpp:544]     Test net output #2: loss = 0.2243 (* 1 = 0.2243 loss)
I0630 02:02:33.205238 29015 solver.cpp:290] Iteration 50000 (26.8975 iter/s, 3.71782s/100 iter), loss = 0
I0630 02:02:33.205256 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:33.205268 29015 sgd_solver.cpp:106] Iteration 50000, lr = 0.0021875
I0630 02:02:35.260809 29015 solver.cpp:290] Iteration 50100 (48.6502 iter/s, 2.05549s/100 iter), loss = 0
I0630 02:02:35.260833 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:35.260838 29015 sgd_solver.cpp:106] Iteration 50100, lr = 0.00217188
I0630 02:02:37.319917 29015 solver.cpp:290] Iteration 50200 (48.5668 iter/s, 2.05902s/100 iter), loss = 0
I0630 02:02:37.319989 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:37.319998 29015 sgd_solver.cpp:106] Iteration 50200, lr = 0.00215625
I0630 02:02:39.381103 29015 solver.cpp:290] Iteration 50300 (48.5189 iter/s, 2.06105s/100 iter), loss = 0
I0630 02:02:39.381124 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:39.381131 29015 sgd_solver.cpp:106] Iteration 50300, lr = 0.00214063
I0630 02:02:41.438710 29015 solver.cpp:290] Iteration 50400 (48.6021 iter/s, 2.05752s/100 iter), loss = 0
I0630 02:02:41.438732 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:41.438740 29015 sgd_solver.cpp:106] Iteration 50400, lr = 0.002125
I0630 02:02:43.493790 29015 solver.cpp:290] Iteration 50500 (48.6619 iter/s, 2.05499s/100 iter), loss = 0
I0630 02:02:43.493811 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:43.493819 29015 sgd_solver.cpp:106] Iteration 50500, lr = 0.00210937
I0630 02:02:45.547883 29015 solver.cpp:290] Iteration 50600 (48.6854 iter/s, 2.05401s/100 iter), loss = 0
I0630 02:02:45.547904 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:45.547912 29015 sgd_solver.cpp:106] Iteration 50600, lr = 0.00209375
I0630 02:02:47.608523 29015 solver.cpp:290] Iteration 50700 (48.5307 iter/s, 2.06055s/100 iter), loss = 0
I0630 02:02:47.608544 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:47.608552 29015 sgd_solver.cpp:106] Iteration 50700, lr = 0.00207812
I0630 02:02:49.665616 29015 solver.cpp:290] Iteration 50800 (48.6144 iter/s, 2.05701s/100 iter), loss = 0
I0630 02:02:49.665640 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:49.665649 29015 sgd_solver.cpp:106] Iteration 50800, lr = 0.0020625
I0630 02:02:51.719471 29015 solver.cpp:290] Iteration 50900 (48.691 iter/s, 2.05377s/100 iter), loss = 0
I0630 02:02:51.719493 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:51.719501 29015 sgd_solver.cpp:106] Iteration 50900, lr = 0.00204687
I0630 02:02:53.755233 29015 solver.cpp:354] Sparsity after update:
I0630 02:02:53.756497 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:02:53.756505 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:02:53.756513 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:02:53.756516 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:02:53.756518 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:02:53.756521 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:02:53.756525 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:02:53.756526 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:02:53.756530 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:02:53.756531 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:02:53.756533 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:02:53.756536 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:02:53.756538 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:02:53.756628 29015 solver.cpp:471] Iteration 51000, Testing net (#0)
I0630 02:02:55.393712 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9042
I0630 02:02:55.393729 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995
I0630 02:02:55.393734 29015 solver.cpp:544]     Test net output #2: loss = 0.223 (* 1 = 0.223 loss)
I0630 02:02:55.413451 29015 solver.cpp:290] Iteration 51000 (27.0721 iter/s, 3.69385s/100 iter), loss = 0
I0630 02:02:55.413477 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:55.413485 29015 sgd_solver.cpp:106] Iteration 51000, lr = 0.00203125
I0630 02:02:57.469255 29015 solver.cpp:290] Iteration 51100 (48.6449 iter/s, 2.05571s/100 iter), loss = 0
I0630 02:02:57.469277 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:57.469285 29015 sgd_solver.cpp:106] Iteration 51100, lr = 0.00201563
I0630 02:02:59.524360 29015 solver.cpp:290] Iteration 51200 (48.6614 iter/s, 2.05502s/100 iter), loss = 0
I0630 02:02:59.524402 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:59.524411 29015 sgd_solver.cpp:106] Iteration 51200, lr = 0.002
I0630 02:03:01.580196 29015 solver.cpp:290] Iteration 51300 (48.6445 iter/s, 2.05573s/100 iter), loss = 0
I0630 02:03:01.580219 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:01.580227 29015 sgd_solver.cpp:106] Iteration 51300, lr = 0.00198438
I0630 02:03:03.643579 29015 solver.cpp:290] Iteration 51400 (48.4662 iter/s, 2.06329s/100 iter), loss = 0
I0630 02:03:03.643600 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:03.643607 29015 sgd_solver.cpp:106] Iteration 51400, lr = 0.00196875
I0630 02:03:05.698901 29015 solver.cpp:290] Iteration 51500 (48.6562 iter/s, 2.05523s/100 iter), loss = 0
I0630 02:03:05.698923 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:05.698931 29015 sgd_solver.cpp:106] Iteration 51500, lr = 0.00195312
I0630 02:03:07.761953 29015 solver.cpp:290] Iteration 51600 (48.4739 iter/s, 2.06296s/100 iter), loss = 0
I0630 02:03:07.762030 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:07.762038 29015 sgd_solver.cpp:106] Iteration 51600, lr = 0.0019375
I0630 02:03:09.831660 29015 solver.cpp:290] Iteration 51700 (48.3194 iter/s, 2.06956s/100 iter), loss = 0
I0630 02:03:09.831688 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:09.831697 29015 sgd_solver.cpp:106] Iteration 51700, lr = 0.00192187
I0630 02:03:11.893529 29015 solver.cpp:290] Iteration 51800 (48.5019 iter/s, 2.06178s/100 iter), loss = 0
I0630 02:03:11.893563 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:11.893573 29015 sgd_solver.cpp:106] Iteration 51800, lr = 0.00190625
I0630 02:03:13.954514 29015 solver.cpp:290] Iteration 51900 (48.5228 iter/s, 2.06089s/100 iter), loss = 0
I0630 02:03:13.954537 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:13.954547 29015 sgd_solver.cpp:106] Iteration 51900, lr = 0.00189062
I0630 02:03:15.989691 29015 solver.cpp:354] Sparsity after update:
I0630 02:03:15.990849 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:03:15.990857 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:03:15.990865 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:03:15.990869 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:03:15.990870 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:03:15.990873 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:03:15.990875 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:03:15.990878 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:03:15.990880 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:03:15.990890 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:03:15.990893 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:03:15.990895 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:03:15.990897 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:03:15.990986 29015 solver.cpp:471] Iteration 52000, Testing net (#0)
I0630 02:03:17.628886 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9043
I0630 02:03:17.628906 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9956
I0630 02:03:17.628911 29015 solver.cpp:544]     Test net output #2: loss = 0.2266 (* 1 = 0.2266 loss)
I0630 02:03:17.648641 29015 solver.cpp:290] Iteration 52000 (27.071 iter/s, 3.69399s/100 iter), loss = 0
I0630 02:03:17.648663 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:17.648671 29015 sgd_solver.cpp:106] Iteration 52000, lr = 0.001875
I0630 02:03:19.705670 29015 solver.cpp:290] Iteration 52100 (48.6159 iter/s, 2.05694s/100 iter), loss = 0
I0630 02:03:19.705696 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:19.705705 29015 sgd_solver.cpp:106] Iteration 52100, lr = 0.00185938
I0630 02:03:21.760448 29015 solver.cpp:290] Iteration 52200 (48.6692 iter/s, 2.05469s/100 iter), loss = 0
I0630 02:03:21.760470 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:21.760476 29015 sgd_solver.cpp:106] Iteration 52200, lr = 0.00184375
I0630 02:03:23.815347 29015 solver.cpp:290] Iteration 52300 (48.6662 iter/s, 2.05481s/100 iter), loss = 0
I0630 02:03:23.815371 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:23.815376 29015 sgd_solver.cpp:106] Iteration 52300, lr = 0.00182813
I0630 02:03:25.872169 29015 solver.cpp:290] Iteration 52400 (48.6208 iter/s, 2.05673s/100 iter), loss = 0
I0630 02:03:25.872193 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:25.872202 29015 sgd_solver.cpp:106] Iteration 52400, lr = 0.0018125
I0630 02:03:27.931958 29015 solver.cpp:290] Iteration 52500 (48.5508 iter/s, 2.0597s/100 iter), loss = 0
I0630 02:03:27.931982 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:27.931991 29015 sgd_solver.cpp:106] Iteration 52500, lr = 0.00179687
I0630 02:03:29.988956 29015 solver.cpp:290] Iteration 52600 (48.6166 iter/s, 2.05691s/100 iter), loss = 0
I0630 02:03:29.988996 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:29.989004 29015 sgd_solver.cpp:106] Iteration 52600, lr = 0.00178125
I0630 02:03:32.043334 29015 solver.cpp:290] Iteration 52700 (48.679 iter/s, 2.05427s/100 iter), loss = 0
I0630 02:03:32.043357 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:32.043366 29015 sgd_solver.cpp:106] Iteration 52700, lr = 0.00176562
I0630 02:03:34.102903 29015 solver.cpp:290] Iteration 52800 (48.5559 iter/s, 2.05948s/100 iter), loss = 0
I0630 02:03:34.102926 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:34.102934 29015 sgd_solver.cpp:106] Iteration 52800, lr = 0.00175
I0630 02:03:36.157917 29015 solver.cpp:290] Iteration 52900 (48.6635 iter/s, 2.05493s/100 iter), loss = 0
I0630 02:03:36.157938 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:36.157945 29015 sgd_solver.cpp:106] Iteration 52900, lr = 0.00173437
I0630 02:03:38.192317 29015 solver.cpp:354] Sparsity after update:
I0630 02:03:38.193588 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:03:38.193595 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:03:38.193603 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:03:38.193606 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:03:38.193608 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:03:38.193611 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:03:38.193614 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:03:38.193616 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:03:38.193619 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:03:38.193621 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:03:38.193624 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:03:38.193626 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:03:38.193629 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:03:38.193716 29015 solver.cpp:471] Iteration 53000, Testing net (#0)
I0630 02:03:39.829602 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9054
I0630 02:03:39.829622 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9958
I0630 02:03:39.829627 29015 solver.cpp:544]     Test net output #2: loss = 0.2281 (* 1 = 0.2281 loss)
I0630 02:03:39.849933 29015 solver.cpp:290] Iteration 53000 (27.0864 iter/s, 3.69188s/100 iter), loss = 0
I0630 02:03:39.849952 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:39.849961 29015 sgd_solver.cpp:106] Iteration 53000, lr = 0.00171875
I0630 02:03:41.907095 29015 solver.cpp:290] Iteration 53100 (48.6126 iter/s, 2.05708s/100 iter), loss = 0
I0630 02:03:41.907116 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:41.907124 29015 sgd_solver.cpp:106] Iteration 53100, lr = 0.00170313
I0630 02:03:43.966562 29015 solver.cpp:290] Iteration 53200 (48.5584 iter/s, 2.05938s/100 iter), loss = 0
I0630 02:03:43.966586 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:43.966593 29015 sgd_solver.cpp:106] Iteration 53200, lr = 0.0016875
I0630 02:03:46.019554 29015 solver.cpp:290] Iteration 53300 (48.7115 iter/s, 2.05291s/100 iter), loss = 0
I0630 02:03:46.019577 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:46.019583 29015 sgd_solver.cpp:106] Iteration 53300, lr = 0.00167188
I0630 02:03:48.074405 29015 solver.cpp:290] Iteration 53400 (48.6674 iter/s, 2.05477s/100 iter), loss = 0
I0630 02:03:48.074429 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:48.074435 29015 sgd_solver.cpp:106] Iteration 53400, lr = 0.00165625
I0630 02:03:50.132513 29015 solver.cpp:290] Iteration 53500 (48.5904 iter/s, 2.05802s/100 iter), loss = 0
I0630 02:03:50.132535 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:50.132542 29015 sgd_solver.cpp:106] Iteration 53500, lr = 0.00164062
I0630 02:03:52.186378 29015 solver.cpp:290] Iteration 53600 (48.6907 iter/s, 2.05378s/100 iter), loss = 0
I0630 02:03:52.186400 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:52.186406 29015 sgd_solver.cpp:106] Iteration 53600, lr = 0.001625
I0630 02:03:54.240020 29015 solver.cpp:290] Iteration 53700 (48.6961 iter/s, 2.05355s/100 iter), loss = 0
I0630 02:03:54.240042 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:54.240049 29015 sgd_solver.cpp:106] Iteration 53700, lr = 0.00160937
I0630 02:03:56.321756 29015 solver.cpp:290] Iteration 53800 (48.0388 iter/s, 2.08165s/100 iter), loss = 0
I0630 02:03:56.321779 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:56.321785 29015 sgd_solver.cpp:106] Iteration 53800, lr = 0.00159375
I0630 02:03:58.384086 29015 solver.cpp:290] Iteration 53900 (48.4909 iter/s, 2.06224s/100 iter), loss = 0
I0630 02:03:58.384112 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:58.384121 29015 sgd_solver.cpp:106] Iteration 53900, lr = 0.00157812
I0630 02:04:00.420658 29015 solver.cpp:354] Sparsity after update:
I0630 02:04:00.422067 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:04:00.422080 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:04:00.422088 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:04:00.422091 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:04:00.422096 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:04:00.422098 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:04:00.422101 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:04:00.422102 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:04:00.422107 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:04:00.422109 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:04:00.422111 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:04:00.422113 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:04:00.422116 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:04:00.422212 29015 solver.cpp:471] Iteration 54000, Testing net (#0)
I0630 02:04:02.058346 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9043
I0630 02:04:02.058364 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9955
I0630 02:04:02.058370 29015 solver.cpp:544]     Test net output #2: loss = 0.2268 (* 1 = 0.2268 loss)
I0630 02:04:02.078349 29015 solver.cpp:290] Iteration 54000 (27.07 iter/s, 3.69413s/100 iter), loss = 0
I0630 02:04:02.078366 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:02.078377 29015 sgd_solver.cpp:106] Iteration 54000, lr = 0.0015625
I0630 02:04:04.137825 29015 solver.cpp:290] Iteration 54100 (48.558 iter/s, 2.05939s/100 iter), loss = 0
I0630 02:04:04.137847 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:04.137854 29015 sgd_solver.cpp:106] Iteration 54100, lr = 0.00154688
I0630 02:04:06.200726 29015 solver.cpp:290] Iteration 54200 (48.4775 iter/s, 2.06281s/100 iter), loss = 0
I0630 02:04:06.200747 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:06.200754 29015 sgd_solver.cpp:106] Iteration 54200, lr = 0.00153125
I0630 02:04:08.258731 29015 solver.cpp:290] Iteration 54300 (48.5928 iter/s, 2.05792s/100 iter), loss = 0
I0630 02:04:08.258821 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:08.258831 29015 sgd_solver.cpp:106] Iteration 54300, lr = 0.00151563
I0630 02:04:10.317973 29015 solver.cpp:290] Iteration 54400 (48.5651 iter/s, 2.05909s/100 iter), loss = 0
I0630 02:04:10.317996 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:10.318002 29015 sgd_solver.cpp:106] Iteration 54400, lr = 0.0015
I0630 02:04:12.378685 29015 solver.cpp:290] Iteration 54500 (48.5289 iter/s, 2.06063s/100 iter), loss = 0
I0630 02:04:12.378707 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:12.378715 29015 sgd_solver.cpp:106] Iteration 54500, lr = 0.00148437
I0630 02:04:14.441810 29015 solver.cpp:290] Iteration 54600 (48.4725 iter/s, 2.06302s/100 iter), loss = 0
I0630 02:04:14.441861 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:14.441870 29015 sgd_solver.cpp:106] Iteration 54600, lr = 0.00146875
I0630 02:04:16.497994 29015 solver.cpp:290] Iteration 54700 (48.6365 iter/s, 2.05607s/100 iter), loss = 0
I0630 02:04:16.498020 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:16.498028 29015 sgd_solver.cpp:106] Iteration 54700, lr = 0.00145312
I0630 02:04:18.555886 29015 solver.cpp:290] Iteration 54800 (48.5955 iter/s, 2.0578s/100 iter), loss = 0
I0630 02:04:18.555908 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:18.555915 29015 sgd_solver.cpp:106] Iteration 54800, lr = 0.0014375
I0630 02:04:20.613112 29015 solver.cpp:290] Iteration 54900 (48.6112 iter/s, 2.05714s/100 iter), loss = 0
I0630 02:04:20.613137 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:20.613145 29015 sgd_solver.cpp:106] Iteration 54900, lr = 0.00142187
I0630 02:04:22.648934 29015 solver.cpp:354] Sparsity after update:
I0630 02:04:22.650183 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:04:22.650189 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:04:22.650197 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:04:22.650198 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:04:22.650202 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:04:22.650203 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:04:22.650205 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:04:22.650207 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:04:22.650209 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:04:22.650212 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:04:22.650213 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:04:22.650215 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:04:22.650218 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:04:22.650301 29015 solver.cpp:471] Iteration 55000, Testing net (#0)
I0630 02:04:24.287328 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9058
I0630 02:04:24.287348 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9956
I0630 02:04:24.287353 29015 solver.cpp:544]     Test net output #2: loss = 0.2271 (* 1 = 0.2271 loss)
I0630 02:04:24.311683 29015 solver.cpp:290] Iteration 55000 (27.0384 iter/s, 3.69844s/100 iter), loss = 0
I0630 02:04:24.311700 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:24.311712 29015 sgd_solver.cpp:106] Iteration 55000, lr = 0.00140625
I0630 02:04:26.374333 29015 solver.cpp:290] Iteration 55100 (48.4833 iter/s, 2.06257s/100 iter), loss = 0
I0630 02:04:26.374356 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:26.374363 29015 sgd_solver.cpp:106] Iteration 55100, lr = 0.00139063
I0630 02:04:28.434306 29015 solver.cpp:290] Iteration 55200 (48.5464 iter/s, 2.05989s/100 iter), loss = 0
I0630 02:04:28.434329 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:28.434334 29015 sgd_solver.cpp:106] Iteration 55200, lr = 0.001375
I0630 02:04:30.495827 29015 solver.cpp:290] Iteration 55300 (48.5099 iter/s, 2.06143s/100 iter), loss = 0
I0630 02:04:30.495864 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:30.495872 29015 sgd_solver.cpp:106] Iteration 55300, lr = 0.00135938
I0630 02:04:32.561600 29015 solver.cpp:290] Iteration 55400 (48.4105 iter/s, 2.06567s/100 iter), loss = 0
I0630 02:04:32.561625 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:32.561635 29015 sgd_solver.cpp:106] Iteration 55400, lr = 0.00134375
I0630 02:04:34.618835 29015 solver.cpp:290] Iteration 55500 (48.6111 iter/s, 2.05715s/100 iter), loss = 0
I0630 02:04:34.618858 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:34.618867 29015 sgd_solver.cpp:106] Iteration 55500, lr = 0.00132813
I0630 02:04:36.675850 29015 solver.cpp:290] Iteration 55600 (48.6162 iter/s, 2.05693s/100 iter), loss = 0
I0630 02:04:36.675873 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:36.675879 29015 sgd_solver.cpp:106] Iteration 55600, lr = 0.0013125
I0630 02:04:38.730872 29015 solver.cpp:290] Iteration 55700 (48.6634 iter/s, 2.05493s/100 iter), loss = 0
I0630 02:04:38.730978 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:38.730986 29015 sgd_solver.cpp:106] Iteration 55700, lr = 0.00129687
I0630 02:04:40.788645 29015 solver.cpp:290] Iteration 55800 (48.6002 iter/s, 2.05761s/100 iter), loss = 0
I0630 02:04:40.788667 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:40.788674 29015 sgd_solver.cpp:106] Iteration 55800, lr = 0.00128125
I0630 02:04:42.844759 29015 solver.cpp:290] Iteration 55900 (48.6375 iter/s, 2.05603s/100 iter), loss = 0
I0630 02:04:42.844781 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:42.844789 29015 sgd_solver.cpp:106] Iteration 55900, lr = 0.00126562
I0630 02:04:44.883028 29015 solver.cpp:354] Sparsity after update:
I0630 02:04:44.884281 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:04:44.884289 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:04:44.884295 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:04:44.884297 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:04:44.884299 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:04:44.884301 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:04:44.884304 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:04:44.884305 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:04:44.884307 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:04:44.884310 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:04:44.884311 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:04:44.884313 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:04:44.884315 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:04:44.884405 29015 solver.cpp:471] Iteration 56000, Testing net (#0)
I0630 02:04:46.523270 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9059
I0630 02:04:46.523288 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995
I0630 02:04:46.523294 29015 solver.cpp:544]     Test net output #2: loss = 0.2256 (* 1 = 0.2256 loss)
I0630 02:04:46.543218 29015 solver.cpp:290] Iteration 56000 (27.0393 iter/s, 3.69833s/100 iter), loss = 0
I0630 02:04:46.543243 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:46.543249 29015 sgd_solver.cpp:106] Iteration 56000, lr = 0.00125
I0630 02:04:48.611007 29015 solver.cpp:290] Iteration 56100 (48.3629 iter/s, 2.0677s/100 iter), loss = 0
I0630 02:04:48.611030 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:48.611037 29015 sgd_solver.cpp:106] Iteration 56100, lr = 0.00123438
I0630 02:04:50.672852 29015 solver.cpp:290] Iteration 56200 (48.5023 iter/s, 2.06176s/100 iter), loss = 0
I0630 02:04:50.672876 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:50.672883 29015 sgd_solver.cpp:106] Iteration 56200, lr = 0.00121875
I0630 02:04:52.728236 29015 solver.cpp:290] Iteration 56300 (48.6548 iter/s, 2.0553s/100 iter), loss = 0
I0630 02:04:52.728257 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:52.728266 29015 sgd_solver.cpp:106] Iteration 56300, lr = 0.00120313
I0630 02:04:54.787145 29015 solver.cpp:290] Iteration 56400 (48.5714 iter/s, 2.05882s/100 iter), loss = 0
I0630 02:04:54.787168 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:54.787174 29015 sgd_solver.cpp:106] Iteration 56400, lr = 0.0011875
I0630 02:04:56.846226 29015 solver.cpp:290] Iteration 56500 (48.5674 iter/s, 2.05899s/100 iter), loss = 0
I0630 02:04:56.846248 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:56.846256 29015 sgd_solver.cpp:106] Iteration 56500, lr = 0.00117187
I0630 02:04:58.899850 29015 solver.cpp:290] Iteration 56600 (48.6965 iter/s, 2.05354s/100 iter), loss = 0
I0630 02:04:58.899871 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:58.899878 29015 sgd_solver.cpp:106] Iteration 56600, lr = 0.00115625
I0630 02:05:00.953819 29015 solver.cpp:290] Iteration 56700 (48.6883 iter/s, 2.05388s/100 iter), loss = 0
I0630 02:05:00.953860 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:00.953871 29015 sgd_solver.cpp:106] Iteration 56700, lr = 0.00114062
I0630 02:05:03.005146 29015 solver.cpp:290] Iteration 56800 (48.7514 iter/s, 2.05122s/100 iter), loss = 0
I0630 02:05:03.005172 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:03.005178 29015 sgd_solver.cpp:106] Iteration 56800, lr = 0.001125
I0630 02:05:05.059659 29015 solver.cpp:290] Iteration 56900 (48.6755 iter/s, 2.05442s/100 iter), loss = 0
I0630 02:05:05.059679 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:05.059686 29015 sgd_solver.cpp:106] Iteration 56900, lr = 0.00110937
I0630 02:05:07.093394 29015 solver.cpp:354] Sparsity after update:
I0630 02:05:07.094655 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:05:07.094662 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:05:07.094671 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:05:07.094674 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:05:07.094678 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:05:07.094683 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:05:07.094688 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:05:07.094692 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:05:07.094696 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:05:07.094700 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:05:07.094704 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:05:07.094708 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:05:07.094712 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:05:07.094856 29015 solver.cpp:471] Iteration 57000, Testing net (#0)
I0630 02:05:08.733505 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9061
I0630 02:05:08.733551 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9956
I0630 02:05:08.733557 29015 solver.cpp:544]     Test net output #2: loss = 0.2275 (* 1 = 0.2275 loss)
I0630 02:05:08.753720 29015 solver.cpp:290] Iteration 57000 (27.0714 iter/s, 3.69393s/100 iter), loss = 0
I0630 02:05:08.753737 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:08.753749 29015 sgd_solver.cpp:106] Iteration 57000, lr = 0.00109375
I0630 02:05:10.808569 29015 solver.cpp:290] Iteration 57100 (48.6673 iter/s, 2.05477s/100 iter), loss = 0
I0630 02:05:10.808591 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:10.808599 29015 sgd_solver.cpp:106] Iteration 57100, lr = 0.00107813
I0630 02:05:12.863256 29015 solver.cpp:290] Iteration 57200 (48.6713 iter/s, 2.0546s/100 iter), loss = 0
I0630 02:05:12.863278 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:12.863284 29015 sgd_solver.cpp:106] Iteration 57200, lr = 0.0010625
I0630 02:05:14.916241 29015 solver.cpp:290] Iteration 57300 (48.7116 iter/s, 2.0529s/100 iter), loss = 0
I0630 02:05:14.916265 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:14.916270 29015 sgd_solver.cpp:106] Iteration 57300, lr = 0.00104688
I0630 02:05:16.969579 29015 solver.cpp:290] Iteration 57400 (48.7033 iter/s, 2.05325s/100 iter), loss = 0
I0630 02:05:16.969601 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:16.969609 29015 sgd_solver.cpp:106] Iteration 57400, lr = 0.00103125
I0630 02:05:19.024271 29015 solver.cpp:290] Iteration 57500 (48.6712 iter/s, 2.0546s/100 iter), loss = 0
I0630 02:05:19.024294 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:19.024303 29015 sgd_solver.cpp:106] Iteration 57500, lr = 0.00101562
I0630 02:05:21.080379 29015 solver.cpp:290] Iteration 57600 (48.6376 iter/s, 2.05602s/100 iter), loss = 0
I0630 02:05:21.080401 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:21.080409 29015 sgd_solver.cpp:106] Iteration 57600, lr = 0.001
I0630 02:05:23.135583 29015 solver.cpp:290] Iteration 57700 (48.659 iter/s, 2.05512s/100 iter), loss = 0
I0630 02:05:23.135606 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:23.135612 29015 sgd_solver.cpp:106] Iteration 57700, lr = 0.000984375
I0630 02:05:25.192625 29015 solver.cpp:290] Iteration 57800 (48.6155 iter/s, 2.05696s/100 iter), loss = 0
I0630 02:05:25.192647 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:25.192656 29015 sgd_solver.cpp:106] Iteration 57800, lr = 0.00096875
I0630 02:05:27.249508 29015 solver.cpp:290] Iteration 57900 (48.6193 iter/s, 2.0568s/100 iter), loss = 0
I0630 02:05:27.249531 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:27.249537 29015 sgd_solver.cpp:106] Iteration 57900, lr = 0.000953125
I0630 02:05:29.288687 29015 solver.cpp:354] Sparsity after update:
I0630 02:05:29.289952 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:05:29.289959 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:05:29.289968 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:05:29.289969 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:05:29.289971 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:05:29.289973 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:05:29.289975 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:05:29.289978 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:05:29.289979 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:05:29.289981 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:05:29.289983 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:05:29.289985 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:05:29.289988 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:05:29.290073 29015 solver.cpp:471] Iteration 58000, Testing net (#0)
I0630 02:05:30.927268 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9047
I0630 02:05:30.927286 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9957
I0630 02:05:30.927292 29015 solver.cpp:544]     Test net output #2: loss = 0.2289 (* 1 = 0.2289 loss)
I0630 02:05:30.948078 29015 solver.cpp:290] Iteration 58000 (27.0384 iter/s, 3.69844s/100 iter), loss = 0
I0630 02:05:30.948096 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:30.948109 29015 sgd_solver.cpp:106] Iteration 58000, lr = 0.0009375
I0630 02:05:33.005215 29015 solver.cpp:290] Iteration 58100 (48.6133 iter/s, 2.05705s/100 iter), loss = 0
I0630 02:05:33.005236 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:33.005244 29015 sgd_solver.cpp:106] Iteration 58100, lr = 0.000921875
I0630 02:05:35.069025 29015 solver.cpp:290] Iteration 58200 (48.4561 iter/s, 2.06372s/100 iter), loss = 0
I0630 02:05:35.069051 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:35.069056 29015 sgd_solver.cpp:106] Iteration 58200, lr = 0.00090625
I0630 02:05:37.127912 29015 solver.cpp:290] Iteration 58300 (48.5721 iter/s, 2.05879s/100 iter), loss = 0
I0630 02:05:37.127935 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:37.127943 29015 sgd_solver.cpp:106] Iteration 58300, lr = 0.000890625
I0630 02:05:39.181345 29015 solver.cpp:290] Iteration 58400 (48.701 iter/s, 2.05335s/100 iter), loss = 0
I0630 02:05:39.181429 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:39.181437 29015 sgd_solver.cpp:106] Iteration 58400, lr = 0.000875
I0630 02:05:41.239459 29015 solver.cpp:290] Iteration 58500 (48.5917 iter/s, 2.05797s/100 iter), loss = 0
I0630 02:05:41.239480 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:41.239487 29015 sgd_solver.cpp:106] Iteration 58500, lr = 0.000859375
I0630 02:05:43.294471 29015 solver.cpp:290] Iteration 58600 (48.6636 iter/s, 2.05493s/100 iter), loss = 0
I0630 02:05:43.294493 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:43.294500 29015 sgd_solver.cpp:106] Iteration 58600, lr = 0.00084375
I0630 02:05:45.346027 29015 solver.cpp:290] Iteration 58700 (48.7455 iter/s, 2.05147s/100 iter), loss = 0
I0630 02:05:45.346050 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:45.346056 29015 sgd_solver.cpp:106] Iteration 58700, lr = 0.000828125
I0630 02:05:47.399729 29015 solver.cpp:290] Iteration 58800 (48.6947 iter/s, 2.05361s/100 iter), loss = 0
I0630 02:05:47.399750 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:47.399757 29015 sgd_solver.cpp:106] Iteration 58800, lr = 0.0008125
I0630 02:05:49.452879 29015 solver.cpp:290] Iteration 58900 (48.7077 iter/s, 2.05306s/100 iter), loss = 0
I0630 02:05:49.452903 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:49.452909 29015 sgd_solver.cpp:106] Iteration 58900, lr = 0.000796875
I0630 02:05:51.485743 29015 solver.cpp:354] Sparsity after update:
I0630 02:05:51.486997 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:05:51.487004 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:05:51.487011 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:05:51.487013 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:05:51.487016 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:05:51.487018 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:05:51.487020 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:05:51.487022 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:05:51.487025 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:05:51.487026 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:05:51.487028 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:05:51.487030 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:05:51.487032 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:05:51.487116 29015 solver.cpp:471] Iteration 59000, Testing net (#0)
I0630 02:05:53.123535 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9057
I0630 02:05:53.123554 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9956
I0630 02:05:53.123559 29015 solver.cpp:544]     Test net output #2: loss = 0.226 (* 1 = 0.226 loss)
I0630 02:05:53.143133 29015 solver.cpp:290] Iteration 59000 (27.0994 iter/s, 3.69012s/100 iter), loss = 0
I0630 02:05:53.143151 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:53.143162 29015 sgd_solver.cpp:106] Iteration 59000, lr = 0.00078125
I0630 02:05:55.199924 29015 solver.cpp:290] Iteration 59100 (48.6214 iter/s, 2.05671s/100 iter), loss = 0
I0630 02:05:55.199947 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:55.199954 29015 sgd_solver.cpp:106] Iteration 59100, lr = 0.000765625
I0630 02:05:57.253805 29015 solver.cpp:290] Iteration 59200 (48.6904 iter/s, 2.05379s/100 iter), loss = 0
I0630 02:05:57.253829 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:57.253836 29015 sgd_solver.cpp:106] Iteration 59200, lr = 0.00075
I0630 02:05:59.306532 29015 solver.cpp:290] Iteration 59300 (48.7178 iter/s, 2.05264s/100 iter), loss = 0
I0630 02:05:59.306557 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:59.306566 29015 sgd_solver.cpp:106] Iteration 59300, lr = 0.000734375
I0630 02:06:01.360487 29015 solver.cpp:290] Iteration 59400 (48.6887 iter/s, 2.05387s/100 iter), loss = 0
I0630 02:06:01.360527 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:01.360536 29015 sgd_solver.cpp:106] Iteration 59400, lr = 0.00071875
I0630 02:06:03.419324 29015 solver.cpp:290] Iteration 59500 (48.5735 iter/s, 2.05873s/100 iter), loss = 0
I0630 02:06:03.419345 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:03.419353 29015 sgd_solver.cpp:106] Iteration 59500, lr = 0.000703125
I0630 02:06:05.476352 29015 solver.cpp:290] Iteration 59600 (48.6158 iter/s, 2.05694s/100 iter), loss = 0
I0630 02:06:05.476373 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:05.476382 29015 sgd_solver.cpp:106] Iteration 59600, lr = 0.0006875
I0630 02:06:07.530237 29015 solver.cpp:290] Iteration 59700 (48.6903 iter/s, 2.0538s/100 iter), loss = 0
I0630 02:06:07.530258 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:07.530266 29015 sgd_solver.cpp:106] Iteration 59700, lr = 0.000671875
I0630 02:06:09.596856 29015 solver.cpp:290] Iteration 59800 (48.3902 iter/s, 2.06653s/100 iter), loss = 0
I0630 02:06:09.596920 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:09.596928 29015 sgd_solver.cpp:106] Iteration 59800, lr = 0.00065625
I0630 02:06:11.651645 29015 solver.cpp:290] Iteration 59900 (48.6698 iter/s, 2.05466s/100 iter), loss = 0
I0630 02:06:11.651667 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:11.651674 29015 sgd_solver.cpp:106] Iteration 59900, lr = 0.000640625
I0630 02:06:13.686931 29015 solver.cpp:598] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_60000.caffemodel
I0630 02:06:13.703379 29015 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_60000.solverstate
I0630 02:06:13.710661 29015 solver.cpp:354] Sparsity after update:
I0630 02:06:13.711603 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:06:13.711611 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:06:13.711619 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:06:13.711621 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:06:13.711624 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:06:13.711627 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:06:13.711628 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:06:13.711630 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:06:13.711632 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:06:13.711634 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:06:13.711637 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:06:13.711639 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:06:13.711642 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:06:13.711737 29015 solver.cpp:471] Iteration 60000, Testing net (#0)
I0630 02:06:15.345600 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.906
I0630 02:06:15.345620 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9957
I0630 02:06:15.345625 29015 solver.cpp:544]     Test net output #2: loss = 0.2259 (* 1 = 0.2259 loss)
I0630 02:06:15.365943 29015 solver.cpp:290] Iteration 60000 (26.9239 iter/s, 3.71417s/100 iter), loss = 0
I0630 02:06:15.365960 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:15.365972 29015 sgd_solver.cpp:106] Iteration 60000, lr = 0.000625
I0630 02:06:17.427134 29015 solver.cpp:290] Iteration 60100 (48.5176 iter/s, 2.06111s/100 iter), loss = 0
I0630 02:06:17.427156 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:17.427163 29015 sgd_solver.cpp:106] Iteration 60100, lr = 0.000609375
I0630 02:06:19.480485 29015 solver.cpp:290] Iteration 60200 (48.7029 iter/s, 2.05326s/100 iter), loss = 0
I0630 02:06:19.480507 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:19.480515 29015 sgd_solver.cpp:106] Iteration 60200, lr = 0.00059375
I0630 02:06:21.536072 29015 solver.cpp:290] Iteration 60300 (48.65 iter/s, 2.0555s/100 iter), loss = 0
I0630 02:06:21.536093 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:21.536100 29015 sgd_solver.cpp:106] Iteration 60300, lr = 0.000578125
I0630 02:06:23.590320 29015 solver.cpp:290] Iteration 60400 (48.6817 iter/s, 2.05416s/100 iter), loss = 0
I0630 02:06:23.590343 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:23.590350 29015 sgd_solver.cpp:106] Iteration 60400, lr = 0.0005625
I0630 02:06:25.645503 29015 solver.cpp:290] Iteration 60500 (48.6595 iter/s, 2.0551s/100 iter), loss = 0
I0630 02:06:25.645524 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:25.645531 29015 sgd_solver.cpp:106] Iteration 60500, lr = 0.000546875
I0630 02:06:27.699985 29015 solver.cpp:290] Iteration 60600 (48.6761 iter/s, 2.0544s/100 iter), loss = 0
I0630 02:06:27.700006 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:27.700034 29015 sgd_solver.cpp:106] Iteration 60600, lr = 0.00053125
I0630 02:06:29.754212 29015 solver.cpp:290] Iteration 60700 (48.6822 iter/s, 2.05414s/100 iter), loss = 0
I0630 02:06:29.754235 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:29.754241 29015 sgd_solver.cpp:106] Iteration 60700, lr = 0.000515625
I0630 02:06:31.809944 29015 solver.cpp:290] Iteration 60800 (48.6465 iter/s, 2.05565s/100 iter), loss = 0
I0630 02:06:31.809967 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:31.809974 29015 sgd_solver.cpp:106] Iteration 60800, lr = 0.0005
I0630 02:06:33.862788 29015 solver.cpp:290] Iteration 60900 (48.715 iter/s, 2.05276s/100 iter), loss = 0
I0630 02:06:33.862810 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:33.862817 29015 sgd_solver.cpp:106] Iteration 60900, lr = 0.000484375
I0630 02:06:35.901886 29015 solver.cpp:354] Sparsity after update:
I0630 02:06:35.903144 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:06:35.903151 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:06:35.903159 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:06:35.903162 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:06:35.903164 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:06:35.903167 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:06:35.903169 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:06:35.903172 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:06:35.903174 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:06:35.903177 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:06:35.903179 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:06:35.903182 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:06:35.903183 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:06:35.903268 29015 solver.cpp:471] Iteration 61000, Testing net (#0)
I0630 02:06:37.538398 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9056
I0630 02:06:37.538417 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9955
I0630 02:06:37.538422 29015 solver.cpp:544]     Test net output #2: loss = 0.2262 (* 1 = 0.2262 loss)
I0630 02:06:37.558166 29015 solver.cpp:290] Iteration 61000 (27.0618 iter/s, 3.69525s/100 iter), loss = 0
I0630 02:06:37.558183 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:37.558195 29015 sgd_solver.cpp:106] Iteration 61000, lr = 0.00046875
I0630 02:06:39.611891 29015 solver.cpp:290] Iteration 61100 (48.694 iter/s, 2.05364s/100 iter), loss = 0
I0630 02:06:39.611956 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:39.611964 29015 sgd_solver.cpp:106] Iteration 61100, lr = 0.000453125
I0630 02:06:41.667369 29015 solver.cpp:290] Iteration 61200 (48.6535 iter/s, 2.05535s/100 iter), loss = 0.047619
I0630 02:06:41.667392 29015 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 02:06:41.667398 29015 sgd_solver.cpp:106] Iteration 61200, lr = 0.0004375
I0630 02:06:43.728478 29015 solver.cpp:290] Iteration 61300 (48.5196 iter/s, 2.06102s/100 iter), loss = 0
I0630 02:06:43.728500 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:43.728508 29015 sgd_solver.cpp:106] Iteration 61300, lr = 0.000421875
I0630 02:06:45.795836 29015 solver.cpp:290] Iteration 61400 (48.373 iter/s, 2.06727s/100 iter), loss = 0
I0630 02:06:45.795859 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:45.795866 29015 sgd_solver.cpp:106] Iteration 61400, lr = 0.00040625
I0630 02:06:47.851689 29015 solver.cpp:290] Iteration 61500 (48.6437 iter/s, 2.05577s/100 iter), loss = 0
I0630 02:06:47.851712 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:47.851719 29015 sgd_solver.cpp:106] Iteration 61500, lr = 0.000390625
I0630 02:06:49.905979 29015 solver.cpp:290] Iteration 61600 (48.6807 iter/s, 2.0542s/100 iter), loss = 0
I0630 02:06:49.906000 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:49.906008 29015 sgd_solver.cpp:106] Iteration 61600, lr = 0.000375
I0630 02:06:51.961374 29015 solver.cpp:290] Iteration 61700 (48.6545 iter/s, 2.05531s/100 iter), loss = 0
I0630 02:06:51.961396 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:51.961403 29015 sgd_solver.cpp:106] Iteration 61700, lr = 0.000359375
I0630 02:06:54.022766 29015 solver.cpp:290] Iteration 61800 (48.513 iter/s, 2.0613s/100 iter), loss = 0
I0630 02:06:54.022791 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:54.022799 29015 sgd_solver.cpp:106] Iteration 61800, lr = 0.00034375
I0630 02:06:56.079090 29015 solver.cpp:290] Iteration 61900 (48.6326 iter/s, 2.05623s/100 iter), loss = 0
I0630 02:06:56.079118 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:56.079128 29015 sgd_solver.cpp:106] Iteration 61900, lr = 0.000328125
I0630 02:06:58.112349 29015 solver.cpp:354] Sparsity after update:
I0630 02:06:58.113610 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:06:58.113617 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:06:58.113626 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:06:58.113628 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:06:58.113631 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:06:58.113632 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:06:58.113636 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:06:58.113637 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:06:58.113641 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:06:58.113643 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:06:58.113646 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:06:58.113648 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:06:58.113651 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:06:58.113735 29015 solver.cpp:471] Iteration 62000, Testing net (#0)
I0630 02:06:59.750670 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9058
I0630 02:06:59.750689 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9955
I0630 02:06:59.750694 29015 solver.cpp:544]     Test net output #2: loss = 0.2274 (* 1 = 0.2274 loss)
I0630 02:06:59.770423 29015 solver.cpp:290] Iteration 62000 (27.0915 iter/s, 3.6912s/100 iter), loss = 0
I0630 02:06:59.770442 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:59.770454 29015 sgd_solver.cpp:106] Iteration 62000, lr = 0.0003125
I0630 02:07:01.827659 29015 solver.cpp:290] Iteration 62100 (48.6109 iter/s, 2.05715s/100 iter), loss = 0
I0630 02:07:01.827682 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:01.827688 29015 sgd_solver.cpp:106] Iteration 62100, lr = 0.000296875
I0630 02:07:03.887364 29015 solver.cpp:290] Iteration 62200 (48.5527 iter/s, 2.05962s/100 iter), loss = 0
I0630 02:07:03.887385 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:03.887393 29015 sgd_solver.cpp:106] Iteration 62200, lr = 0.00028125
I0630 02:07:05.944180 29015 solver.cpp:290] Iteration 62300 (48.6208 iter/s, 2.05673s/100 iter), loss = 0
I0630 02:07:05.944202 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:05.944209 29015 sgd_solver.cpp:106] Iteration 62300, lr = 0.000265625
I0630 02:07:07.999330 29015 solver.cpp:290] Iteration 62400 (48.6603 iter/s, 2.05506s/100 iter), loss = 0
I0630 02:07:07.999352 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:07.999359 29015 sgd_solver.cpp:106] Iteration 62400, lr = 0.00025
I0630 02:07:10.055614 29015 solver.cpp:290] Iteration 62500 (48.6335 iter/s, 2.0562s/100 iter), loss = 0
I0630 02:07:10.055685 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:10.055692 29015 sgd_solver.cpp:106] Iteration 62500, lr = 0.000234375
I0630 02:07:12.111199 29015 solver.cpp:290] Iteration 62600 (48.6511 iter/s, 2.05545s/100 iter), loss = 0
I0630 02:07:12.111222 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:12.111228 29015 sgd_solver.cpp:106] Iteration 62600, lr = 0.00021875
I0630 02:07:14.165974 29015 solver.cpp:290] Iteration 62700 (48.6692 iter/s, 2.05469s/100 iter), loss = 0
I0630 02:07:14.165997 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:14.166003 29015 sgd_solver.cpp:106] Iteration 62700, lr = 0.000203125
I0630 02:07:16.220568 29015 solver.cpp:290] Iteration 62800 (48.6735 iter/s, 2.05451s/100 iter), loss = 0
I0630 02:07:16.220590 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:16.220597 29015 sgd_solver.cpp:106] Iteration 62800, lr = 0.0001875
I0630 02:07:18.275588 29015 solver.cpp:290] Iteration 62900 (48.6634 iter/s, 2.05493s/100 iter), loss = 0
I0630 02:07:18.275611 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:18.275617 29015 sgd_solver.cpp:106] Iteration 62900, lr = 0.000171875
I0630 02:07:20.309784 29015 solver.cpp:354] Sparsity after update:
I0630 02:07:20.311051 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:07:20.311058 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:07:20.311065 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:07:20.311067 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:07:20.311070 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:07:20.311072 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:07:20.311074 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:07:20.311076 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:07:20.311079 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:07:20.311080 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:07:20.311082 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:07:20.311084 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:07:20.311086 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:07:20.311170 29015 solver.cpp:471] Iteration 63000, Testing net (#0)
I0630 02:07:21.948072 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9056
I0630 02:07:21.948091 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9953
I0630 02:07:21.948097 29015 solver.cpp:544]     Test net output #2: loss = 0.2272 (* 1 = 0.2272 loss)
I0630 02:07:21.967617 29015 solver.cpp:290] Iteration 63000 (27.0863 iter/s, 3.6919s/100 iter), loss = 0
I0630 02:07:21.967636 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:21.967648 29015 sgd_solver.cpp:106] Iteration 63000, lr = 0.00015625
I0630 02:07:24.024210 29015 solver.cpp:290] Iteration 63100 (48.6261 iter/s, 2.05651s/100 iter), loss = 0
I0630 02:07:24.024231 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:24.024238 29015 sgd_solver.cpp:106] Iteration 63100, lr = 0.000140625
I0630 02:07:26.081771 29015 solver.cpp:290] Iteration 63200 (48.6033 iter/s, 2.05747s/100 iter), loss = 0
I0630 02:07:26.081794 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:26.081800 29015 sgd_solver.cpp:106] Iteration 63200, lr = 0.000125
I0630 02:07:28.139916 29015 solver.cpp:290] Iteration 63300 (48.5896 iter/s, 2.05806s/100 iter), loss = 0
I0630 02:07:28.139940 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:28.139947 29015 sgd_solver.cpp:106] Iteration 63300, lr = 0.000109375
I0630 02:07:30.194211 29015 solver.cpp:290] Iteration 63400 (48.6806 iter/s, 2.05421s/100 iter), loss = 0
I0630 02:07:30.194233 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:30.194242 29015 sgd_solver.cpp:106] Iteration 63400, lr = 9.37498e-05
I0630 02:07:32.252765 29015 solver.cpp:290] Iteration 63500 (48.5799 iter/s, 2.05847s/100 iter), loss = 0
I0630 02:07:32.252804 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:32.252812 29015 sgd_solver.cpp:106] Iteration 63500, lr = 7.8125e-05
I0630 02:07:34.308624 29015 solver.cpp:290] Iteration 63600 (48.6439 iter/s, 2.05575s/100 iter), loss = 0
I0630 02:07:34.308646 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:34.308652 29015 sgd_solver.cpp:106] Iteration 63600, lr = 6.25002e-05
I0630 02:07:36.365772 29015 solver.cpp:290] Iteration 63700 (48.6131 iter/s, 2.05706s/100 iter), loss = 0
I0630 02:07:36.365793 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:36.365803 29015 sgd_solver.cpp:106] Iteration 63700, lr = 4.68749e-05
I0630 02:07:38.419731 29015 solver.cpp:290] Iteration 63800 (48.6885 iter/s, 2.05387s/100 iter), loss = 0
I0630 02:07:38.419754 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:38.419760 29015 sgd_solver.cpp:106] Iteration 63800, lr = 3.12501e-05
I0630 02:07:40.475477 29015 solver.cpp:290] Iteration 63900 (48.6462 iter/s, 2.05566s/100 iter), loss = 0
I0630 02:07:40.475548 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:40.475555 29015 sgd_solver.cpp:106] Iteration 63900, lr = 1.56248e-05
I0630 02:07:42.513162 29015 solver.cpp:354] Sparsity after update:
I0630 02:07:42.514575 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:07:42.514581 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:07:42.514590 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:07:42.514595 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:07:42.514600 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:07:42.514605 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:07:42.514611 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:07:42.514616 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:07:42.514621 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:07:42.514626 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:07:42.514629 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:07:42.514636 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:07:42.514639 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:07:42.514652 29015 solver.cpp:598] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_64000.caffemodel
I0630 02:07:42.530710 29015 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_64000.solverstate
I0630 02:07:42.542845 29015 solver.cpp:451] Iteration 64000, loss = 0
I0630 02:07:42.542870 29015 solver.cpp:471] Iteration 64000, Testing net (#0)
I0630 02:07:44.179949 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9062
I0630 02:07:44.179967 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9959
I0630 02:07:44.179973 29015 solver.cpp:544]     Test net output #2: loss = 0.2254 (* 1 = 0.2254 loss)
I0630 02:07:44.179976 29015 solver.cpp:456] Optimization Done.
I0630 02:07:44.225229 29015 caffe.cpp:246] Optimization Done.
training/cifar10_jacintonet11v2_2017-06-30_01-13-02/test
WARNING: gnome-keyring:: couldn't connect to: /run/user/30409/keyring-KJvviu/pkcs11: Connection refused
p11-kit: skipping module 'gnome-keyring' whose initialization failed: An error occurred on the device
I0630 02:07:44.975304 27297 caffe.cpp:264] Not using GPU #2 for single-GPU function
I0630 02:07:44.975421 27297 caffe.cpp:264] Not using GPU #1 for single-GPU function
I0630 02:07:45.161372 27297 caffe.cpp:273] Use GPU with device ID 0
I0630 02:07:45.161731 27297 caffe.cpp:277] GPU device name: GeForce GTX 1080
I0630 02:07:45.560605 27297 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0630 02:07:45.560739 27297 layer_factory.hpp:77] Creating layer data
I0630 02:07:45.561100 27297 net.cpp:98] Creating Layer data
I0630 02:07:45.561110 27297 net.cpp:413] data -> data
I0630 02:07:45.561131 27297 net.cpp:413] data -> label
I0630 02:07:45.561998 27327 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0630 02:07:45.562799 27297 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0630 02:07:45.562844 27297 data_layer.cpp:83] output data size: 50,3,32,32
I0630 02:07:45.564374 27297 net.cpp:148] Setting up data
I0630 02:07:45.564386 27297 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0630 02:07:45.564393 27297 net.cpp:155] Top shape: 50 (50)
I0630 02:07:45.564396 27297 net.cpp:163] Memory required for data: 614600
I0630 02:07:45.564404 27297 layer_factory.hpp:77] Creating layer label_data_1_split
I0630 02:07:45.564415 27297 net.cpp:98] Creating Layer label_data_1_split
I0630 02:07:45.564420 27297 net.cpp:439] label_data_1_split <- label
I0630 02:07:45.564430 27297 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0630 02:07:45.564437 27297 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0630 02:07:45.564443 27297 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0630 02:07:45.564505 27297 net.cpp:148] Setting up label_data_1_split
I0630 02:07:45.564510 27297 net.cpp:155] Top shape: 50 (50)
I0630 02:07:45.564515 27297 net.cpp:155] Top shape: 50 (50)
I0630 02:07:45.564520 27297 net.cpp:155] Top shape: 50 (50)
I0630 02:07:45.564523 27297 net.cpp:163] Memory required for data: 615200
I0630 02:07:45.564527 27297 layer_factory.hpp:77] Creating layer data/bias
I0630 02:07:45.564538 27297 net.cpp:98] Creating Layer data/bias
I0630 02:07:45.564541 27297 net.cpp:439] data/bias <- data
I0630 02:07:45.564546 27297 net.cpp:413] data/bias -> data/bias
I0630 02:07:45.565104 27297 net.cpp:148] Setting up data/bias
I0630 02:07:45.565112 27297 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0630 02:07:45.565115 27297 net.cpp:163] Memory required for data: 1229600
I0630 02:07:45.565127 27297 layer_factory.hpp:77] Creating layer conv1a
I0630 02:07:45.565137 27297 net.cpp:98] Creating Layer conv1a
I0630 02:07:45.565141 27297 net.cpp:439] conv1a <- data/bias
I0630 02:07:45.565147 27297 net.cpp:413] conv1a -> conv1a
I0630 02:07:45.565878 27297 net.cpp:148] Setting up conv1a
I0630 02:07:45.565886 27297 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 02:07:45.565891 27297 net.cpp:163] Memory required for data: 7783200
I0630 02:07:45.565897 27297 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 02:07:45.565906 27297 net.cpp:98] Creating Layer conv1a/bn
I0630 02:07:45.565908 27297 net.cpp:439] conv1a/bn <- conv1a
I0630 02:07:45.565913 27297 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 02:07:45.566277 27297 net.cpp:148] Setting up conv1a/bn
I0630 02:07:45.566283 27297 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 02:07:45.566287 27297 net.cpp:163] Memory required for data: 14336800
I0630 02:07:45.566296 27297 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 02:07:45.566303 27297 net.cpp:98] Creating Layer conv1a/relu
I0630 02:07:45.566306 27297 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 02:07:45.566310 27297 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 02:07:45.566321 27297 net.cpp:148] Setting up conv1a/relu
I0630 02:07:45.566326 27297 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 02:07:45.566330 27297 net.cpp:163] Memory required for data: 20890400
I0630 02:07:45.566334 27297 layer_factory.hpp:77] Creating layer conv1b
I0630 02:07:45.566342 27297 net.cpp:98] Creating Layer conv1b
I0630 02:07:45.566352 27297 net.cpp:439] conv1b <- conv1a/bn
I0630 02:07:45.566359 27297 net.cpp:413] conv1b -> conv1b
I0630 02:07:45.566659 27297 net.cpp:148] Setting up conv1b
I0630 02:07:45.566666 27297 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 02:07:45.566670 27297 net.cpp:163] Memory required for data: 27444000
I0630 02:07:45.566678 27297 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 02:07:45.566684 27297 net.cpp:98] Creating Layer conv1b/bn
I0630 02:07:45.566687 27297 net.cpp:439] conv1b/bn <- conv1b
I0630 02:07:45.566692 27297 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 02:07:45.567000 27297 net.cpp:148] Setting up conv1b/bn
I0630 02:07:45.567008 27297 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 02:07:45.567010 27297 net.cpp:163] Memory required for data: 33997600
I0630 02:07:45.567019 27297 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 02:07:45.567024 27297 net.cpp:98] Creating Layer conv1b/relu
I0630 02:07:45.567028 27297 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 02:07:45.567034 27297 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 02:07:45.567039 27297 net.cpp:148] Setting up conv1b/relu
I0630 02:07:45.567044 27297 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 02:07:45.567049 27297 net.cpp:163] Memory required for data: 40551200
I0630 02:07:45.567051 27297 layer_factory.hpp:77] Creating layer pool1
I0630 02:07:45.567059 27297 net.cpp:98] Creating Layer pool1
I0630 02:07:45.567061 27297 net.cpp:439] pool1 <- conv1b/bn
I0630 02:07:45.567066 27297 net.cpp:413] pool1 -> pool1
I0630 02:07:45.567092 27297 net.cpp:148] Setting up pool1
I0630 02:07:45.567097 27297 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 02:07:45.567101 27297 net.cpp:163] Memory required for data: 47104800
I0630 02:07:45.567104 27297 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 02:07:45.567112 27297 net.cpp:98] Creating Layer res2a_branch2a
I0630 02:07:45.567116 27297 net.cpp:439] res2a_branch2a <- pool1
I0630 02:07:45.567121 27297 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 02:07:45.568189 27297 net.cpp:148] Setting up res2a_branch2a
I0630 02:07:45.568198 27297 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 02:07:45.568202 27297 net.cpp:163] Memory required for data: 60212000
I0630 02:07:45.568209 27297 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 02:07:45.568215 27297 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 02:07:45.568220 27297 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 02:07:45.568225 27297 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 02:07:45.568526 27297 net.cpp:148] Setting up res2a_branch2a/bn
I0630 02:07:45.568532 27297 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 02:07:45.568536 27297 net.cpp:163] Memory required for data: 73319200
I0630 02:07:45.568544 27297 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 02:07:45.568549 27297 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 02:07:45.568553 27297 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 02:07:45.568557 27297 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 02:07:45.568564 27297 net.cpp:148] Setting up res2a_branch2a/relu
I0630 02:07:45.568568 27297 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 02:07:45.568572 27297 net.cpp:163] Memory required for data: 86426400
I0630 02:07:45.568575 27297 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 02:07:45.568583 27297 net.cpp:98] Creating Layer res2a_branch2b
I0630 02:07:45.568585 27297 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 02:07:45.568590 27297 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 02:07:45.569409 27297 net.cpp:148] Setting up res2a_branch2b
I0630 02:07:45.569418 27297 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 02:07:45.569422 27297 net.cpp:163] Memory required for data: 99533600
I0630 02:07:45.569427 27297 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 02:07:45.569434 27297 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 02:07:45.569439 27297 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 02:07:45.569452 27297 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 02:07:45.569748 27297 net.cpp:148] Setting up res2a_branch2b/bn
I0630 02:07:45.569756 27297 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 02:07:45.569758 27297 net.cpp:163] Memory required for data: 112640800
I0630 02:07:45.569767 27297 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 02:07:45.569772 27297 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 02:07:45.569777 27297 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 02:07:45.569782 27297 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 02:07:45.569787 27297 net.cpp:148] Setting up res2a_branch2b/relu
I0630 02:07:45.569792 27297 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 02:07:45.569795 27297 net.cpp:163] Memory required for data: 125748000
I0630 02:07:45.569798 27297 layer_factory.hpp:77] Creating layer pool2
I0630 02:07:45.569804 27297 net.cpp:98] Creating Layer pool2
I0630 02:07:45.569808 27297 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 02:07:45.569813 27297 net.cpp:413] pool2 -> pool2
I0630 02:07:45.569833 27297 net.cpp:148] Setting up pool2
I0630 02:07:45.569836 27297 net.cpp:155] Top shape: 50 64 16 16 (819200)
I0630 02:07:45.569840 27297 net.cpp:163] Memory required for data: 129024800
I0630 02:07:45.569844 27297 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 02:07:45.569854 27297 net.cpp:98] Creating Layer res3a_branch2a
I0630 02:07:45.569859 27297 net.cpp:439] res3a_branch2a <- pool2
I0630 02:07:45.569864 27297 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 02:07:45.571950 27297 net.cpp:148] Setting up res3a_branch2a
I0630 02:07:45.571960 27297 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 02:07:45.571964 27297 net.cpp:163] Memory required for data: 135578400
I0630 02:07:45.571970 27297 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 02:07:45.571980 27297 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 02:07:45.571985 27297 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 02:07:45.571990 27297 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 02:07:45.572249 27297 net.cpp:148] Setting up res3a_branch2a/bn
I0630 02:07:45.572255 27297 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 02:07:45.572259 27297 net.cpp:163] Memory required for data: 142132000
I0630 02:07:45.572268 27297 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 02:07:45.572274 27297 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 02:07:45.572278 27297 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 02:07:45.572283 27297 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 02:07:45.572289 27297 net.cpp:148] Setting up res3a_branch2a/relu
I0630 02:07:45.572293 27297 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 02:07:45.572298 27297 net.cpp:163] Memory required for data: 148685600
I0630 02:07:45.572300 27297 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 02:07:45.572307 27297 net.cpp:98] Creating Layer res3a_branch2b
I0630 02:07:45.572311 27297 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 02:07:45.572315 27297 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 02:07:45.573201 27297 net.cpp:148] Setting up res3a_branch2b
I0630 02:07:45.573210 27297 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 02:07:45.573212 27297 net.cpp:163] Memory required for data: 155239200
I0630 02:07:45.573220 27297 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 02:07:45.573225 27297 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 02:07:45.573230 27297 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 02:07:45.573235 27297 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 02:07:45.573500 27297 net.cpp:148] Setting up res3a_branch2b/bn
I0630 02:07:45.573508 27297 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 02:07:45.573510 27297 net.cpp:163] Memory required for data: 161792800
I0630 02:07:45.573518 27297 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 02:07:45.573529 27297 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 02:07:45.573532 27297 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 02:07:45.573537 27297 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 02:07:45.573544 27297 net.cpp:148] Setting up res3a_branch2b/relu
I0630 02:07:45.573547 27297 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 02:07:45.573551 27297 net.cpp:163] Memory required for data: 168346400
I0630 02:07:45.573555 27297 layer_factory.hpp:77] Creating layer pool3
I0630 02:07:45.573560 27297 net.cpp:98] Creating Layer pool3
I0630 02:07:45.573565 27297 net.cpp:439] pool3 <- res3a_branch2b/bn
I0630 02:07:45.573568 27297 net.cpp:413] pool3 -> pool3
I0630 02:07:45.573590 27297 net.cpp:148] Setting up pool3
I0630 02:07:45.573593 27297 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 02:07:45.573597 27297 net.cpp:163] Memory required for data: 174900000
I0630 02:07:45.573601 27297 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 02:07:45.573608 27297 net.cpp:98] Creating Layer res4a_branch2a
I0630 02:07:45.573611 27297 net.cpp:439] res4a_branch2a <- pool3
I0630 02:07:45.573616 27297 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 02:07:45.579524 27297 net.cpp:148] Setting up res4a_branch2a
I0630 02:07:45.579531 27297 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 02:07:45.579535 27297 net.cpp:163] Memory required for data: 188007200
I0630 02:07:45.579540 27297 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 02:07:45.579546 27297 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 02:07:45.579550 27297 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 02:07:45.579555 27297 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 02:07:45.579812 27297 net.cpp:148] Setting up res4a_branch2a/bn
I0630 02:07:45.579818 27297 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 02:07:45.579823 27297 net.cpp:163] Memory required for data: 201114400
I0630 02:07:45.579830 27297 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 02:07:45.579834 27297 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 02:07:45.579838 27297 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 02:07:45.579843 27297 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 02:07:45.579849 27297 net.cpp:148] Setting up res4a_branch2a/relu
I0630 02:07:45.579854 27297 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 02:07:45.579856 27297 net.cpp:163] Memory required for data: 214221600
I0630 02:07:45.579860 27297 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 02:07:45.579866 27297 net.cpp:98] Creating Layer res4a_branch2b
I0630 02:07:45.579870 27297 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 02:07:45.579875 27297 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 02:07:45.582893 27297 net.cpp:148] Setting up res4a_branch2b
I0630 02:07:45.582900 27297 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 02:07:45.582902 27297 net.cpp:163] Memory required for data: 227328800
I0630 02:07:45.582908 27297 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 02:07:45.582914 27297 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 02:07:45.582918 27297 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 02:07:45.582924 27297 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 02:07:45.583183 27297 net.cpp:148] Setting up res4a_branch2b/bn
I0630 02:07:45.583189 27297 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 02:07:45.583191 27297 net.cpp:163] Memory required for data: 240436000
I0630 02:07:45.583199 27297 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 02:07:45.583204 27297 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 02:07:45.583209 27297 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 02:07:45.583212 27297 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 02:07:45.583219 27297 net.cpp:148] Setting up res4a_branch2b/relu
I0630 02:07:45.583222 27297 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 02:07:45.583231 27297 net.cpp:163] Memory required for data: 253543200
I0630 02:07:45.583235 27297 layer_factory.hpp:77] Creating layer pool4
I0630 02:07:45.583240 27297 net.cpp:98] Creating Layer pool4
I0630 02:07:45.583245 27297 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 02:07:45.583250 27297 net.cpp:413] pool4 -> pool4
I0630 02:07:45.583271 27297 net.cpp:148] Setting up pool4
I0630 02:07:45.583276 27297 net.cpp:155] Top shape: 50 256 8 8 (819200)
I0630 02:07:45.583279 27297 net.cpp:163] Memory required for data: 256820000
I0630 02:07:45.583282 27297 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 02:07:45.583289 27297 net.cpp:98] Creating Layer res5a_branch2a
I0630 02:07:45.583293 27297 net.cpp:439] res5a_branch2a <- pool4
I0630 02:07:45.583298 27297 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 02:07:45.607550 27297 net.cpp:148] Setting up res5a_branch2a
I0630 02:07:45.607573 27297 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 02:07:45.607576 27297 net.cpp:163] Memory required for data: 263373600
I0630 02:07:45.607583 27297 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 02:07:45.607594 27297 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 02:07:45.607599 27297 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 02:07:45.607605 27297 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 02:07:45.607903 27297 net.cpp:148] Setting up res5a_branch2a/bn
I0630 02:07:45.607909 27297 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 02:07:45.607914 27297 net.cpp:163] Memory required for data: 269927200
I0630 02:07:45.607923 27297 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 02:07:45.607928 27297 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 02:07:45.607931 27297 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 02:07:45.607936 27297 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 02:07:45.607944 27297 net.cpp:148] Setting up res5a_branch2a/relu
I0630 02:07:45.607947 27297 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 02:07:45.607951 27297 net.cpp:163] Memory required for data: 276480800
I0630 02:07:45.607954 27297 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 02:07:45.607962 27297 net.cpp:98] Creating Layer res5a_branch2b
I0630 02:07:45.607965 27297 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 02:07:45.607970 27297 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 02:07:45.620391 27297 net.cpp:148] Setting up res5a_branch2b
I0630 02:07:45.620415 27297 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 02:07:45.620419 27297 net.cpp:163] Memory required for data: 283034400
I0630 02:07:45.620434 27297 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 02:07:45.620445 27297 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 02:07:45.620451 27297 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 02:07:45.620460 27297 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 02:07:45.620774 27297 net.cpp:148] Setting up res5a_branch2b/bn
I0630 02:07:45.620780 27297 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 02:07:45.620784 27297 net.cpp:163] Memory required for data: 289588000
I0630 02:07:45.620793 27297 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 02:07:45.620797 27297 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 02:07:45.620802 27297 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 02:07:45.620806 27297 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 02:07:45.620813 27297 net.cpp:148] Setting up res5a_branch2b/relu
I0630 02:07:45.620817 27297 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 02:07:45.620820 27297 net.cpp:163] Memory required for data: 296141600
I0630 02:07:45.620824 27297 layer_factory.hpp:77] Creating layer pool5
I0630 02:07:45.620832 27297 net.cpp:98] Creating Layer pool5
I0630 02:07:45.620837 27297 net.cpp:439] pool5 <- res5a_branch2b/bn
I0630 02:07:45.620841 27297 net.cpp:413] pool5 -> pool5
I0630 02:07:45.620864 27297 net.cpp:148] Setting up pool5
I0630 02:07:45.620869 27297 net.cpp:155] Top shape: 50 512 1 1 (25600)
I0630 02:07:45.620880 27297 net.cpp:163] Memory required for data: 296244000
I0630 02:07:45.620884 27297 layer_factory.hpp:77] Creating layer fc10
I0630 02:07:45.620892 27297 net.cpp:98] Creating Layer fc10
I0630 02:07:45.620894 27297 net.cpp:439] fc10 <- pool5
I0630 02:07:45.620899 27297 net.cpp:413] fc10 -> fc10
I0630 02:07:45.621070 27297 net.cpp:148] Setting up fc10
I0630 02:07:45.621075 27297 net.cpp:155] Top shape: 50 10 (500)
I0630 02:07:45.621079 27297 net.cpp:163] Memory required for data: 296246000
I0630 02:07:45.621084 27297 layer_factory.hpp:77] Creating layer fc10_fc10_0_split
I0630 02:07:45.621090 27297 net.cpp:98] Creating Layer fc10_fc10_0_split
I0630 02:07:45.621094 27297 net.cpp:439] fc10_fc10_0_split <- fc10
I0630 02:07:45.621100 27297 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0630 02:07:45.621106 27297 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0630 02:07:45.621111 27297 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0630 02:07:45.621139 27297 net.cpp:148] Setting up fc10_fc10_0_split
I0630 02:07:45.621143 27297 net.cpp:155] Top shape: 50 10 (500)
I0630 02:07:45.621148 27297 net.cpp:155] Top shape: 50 10 (500)
I0630 02:07:45.621152 27297 net.cpp:155] Top shape: 50 10 (500)
I0630 02:07:45.621156 27297 net.cpp:163] Memory required for data: 296252000
I0630 02:07:45.621160 27297 layer_factory.hpp:77] Creating layer loss
I0630 02:07:45.621165 27297 net.cpp:98] Creating Layer loss
I0630 02:07:45.621168 27297 net.cpp:439] loss <- fc10_fc10_0_split_0
I0630 02:07:45.621173 27297 net.cpp:439] loss <- label_data_1_split_0
I0630 02:07:45.621178 27297 net.cpp:413] loss -> loss
I0630 02:07:45.621186 27297 layer_factory.hpp:77] Creating layer loss
I0630 02:07:45.621245 27297 net.cpp:148] Setting up loss
I0630 02:07:45.621250 27297 net.cpp:155] Top shape: (1)
I0630 02:07:45.621254 27297 net.cpp:158]     with loss weight 1
I0630 02:07:45.621269 27297 net.cpp:163] Memory required for data: 296252004
I0630 02:07:45.621273 27297 layer_factory.hpp:77] Creating layer accuracy/top1
I0630 02:07:45.621279 27297 net.cpp:98] Creating Layer accuracy/top1
I0630 02:07:45.621282 27297 net.cpp:439] accuracy/top1 <- fc10_fc10_0_split_1
I0630 02:07:45.621286 27297 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0630 02:07:45.621291 27297 net.cpp:413] accuracy/top1 -> accuracy/top1
I0630 02:07:45.621302 27297 net.cpp:148] Setting up accuracy/top1
I0630 02:07:45.621306 27297 net.cpp:155] Top shape: (1)
I0630 02:07:45.621310 27297 net.cpp:163] Memory required for data: 296252008
I0630 02:07:45.621314 27297 layer_factory.hpp:77] Creating layer accuracy/top5
I0630 02:07:45.621318 27297 net.cpp:98] Creating Layer accuracy/top5
I0630 02:07:45.621321 27297 net.cpp:439] accuracy/top5 <- fc10_fc10_0_split_2
I0630 02:07:45.621325 27297 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0630 02:07:45.621330 27297 net.cpp:413] accuracy/top5 -> accuracy/top5
I0630 02:07:45.621338 27297 net.cpp:148] Setting up accuracy/top5
I0630 02:07:45.621342 27297 net.cpp:155] Top shape: (1)
I0630 02:07:45.621345 27297 net.cpp:163] Memory required for data: 296252012
I0630 02:07:45.621350 27297 net.cpp:226] accuracy/top5 does not need backward computation.
I0630 02:07:45.621353 27297 net.cpp:226] accuracy/top1 does not need backward computation.
I0630 02:07:45.621357 27297 net.cpp:224] loss needs backward computation.
I0630 02:07:45.621361 27297 net.cpp:224] fc10_fc10_0_split needs backward computation.
I0630 02:07:45.621366 27297 net.cpp:224] fc10 needs backward computation.
I0630 02:07:45.621369 27297 net.cpp:224] pool5 needs backward computation.
I0630 02:07:45.621373 27297 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 02:07:45.621376 27297 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 02:07:45.621381 27297 net.cpp:224] res5a_branch2b needs backward computation.
I0630 02:07:45.621383 27297 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 02:07:45.621387 27297 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 02:07:45.621397 27297 net.cpp:224] res5a_branch2a needs backward computation.
I0630 02:07:45.621400 27297 net.cpp:224] pool4 needs backward computation.
I0630 02:07:45.621404 27297 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 02:07:45.621409 27297 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 02:07:45.621413 27297 net.cpp:224] res4a_branch2b needs backward computation.
I0630 02:07:45.621417 27297 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 02:07:45.621421 27297 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 02:07:45.621425 27297 net.cpp:224] res4a_branch2a needs backward computation.
I0630 02:07:45.621429 27297 net.cpp:224] pool3 needs backward computation.
I0630 02:07:45.621433 27297 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 02:07:45.621438 27297 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 02:07:45.621440 27297 net.cpp:224] res3a_branch2b needs backward computation.
I0630 02:07:45.621444 27297 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 02:07:45.621448 27297 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 02:07:45.621451 27297 net.cpp:224] res3a_branch2a needs backward computation.
I0630 02:07:45.621455 27297 net.cpp:224] pool2 needs backward computation.
I0630 02:07:45.621459 27297 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 02:07:45.621464 27297 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 02:07:45.621467 27297 net.cpp:224] res2a_branch2b needs backward computation.
I0630 02:07:45.621471 27297 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 02:07:45.621475 27297 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 02:07:45.621479 27297 net.cpp:224] res2a_branch2a needs backward computation.
I0630 02:07:45.621484 27297 net.cpp:224] pool1 needs backward computation.
I0630 02:07:45.621486 27297 net.cpp:224] conv1b/relu needs backward computation.
I0630 02:07:45.621490 27297 net.cpp:224] conv1b/bn needs backward computation.
I0630 02:07:45.621495 27297 net.cpp:224] conv1b needs backward computation.
I0630 02:07:45.621500 27297 net.cpp:224] conv1a/relu needs backward computation.
I0630 02:07:45.621502 27297 net.cpp:224] conv1a/bn needs backward computation.
I0630 02:07:45.621506 27297 net.cpp:224] conv1a needs backward computation.
I0630 02:07:45.621511 27297 net.cpp:226] data/bias does not need backward computation.
I0630 02:07:45.621515 27297 net.cpp:226] label_data_1_split does not need backward computation.
I0630 02:07:45.621520 27297 net.cpp:226] data does not need backward computation.
I0630 02:07:45.621525 27297 net.cpp:268] This network produces output accuracy/top1
I0630 02:07:45.621527 27297 net.cpp:268] This network produces output accuracy/top5
I0630 02:07:45.621531 27297 net.cpp:268] This network produces output loss
I0630 02:07:45.621552 27297 net.cpp:288] Network initialization done.
I0630 02:07:45.631685 27297 caffe.cpp:289] Running for 200 iterations.
I0630 02:07:45.654253 27297 caffe.cpp:312] Batch 0, accuracy/top1 = 0.88
I0630 02:07:45.654276 27297 caffe.cpp:312] Batch 0, accuracy/top5 = 1
I0630 02:07:45.654281 27297 caffe.cpp:312] Batch 0, loss = 0.14
I0630 02:07:45.662513 27297 caffe.cpp:312] Batch 1, accuracy/top1 = 0.9
I0630 02:07:45.662551 27297 caffe.cpp:312] Batch 1, accuracy/top5 = 1
I0630 02:07:45.662556 27297 caffe.cpp:312] Batch 1, loss = 0.18
I0630 02:07:45.670703 27297 caffe.cpp:312] Batch 2, accuracy/top1 = 0.94
I0630 02:07:45.670722 27297 caffe.cpp:312] Batch 2, accuracy/top5 = 1
I0630 02:07:45.670727 27297 caffe.cpp:312] Batch 2, loss = 0.18
I0630 02:07:45.678936 27297 caffe.cpp:312] Batch 3, accuracy/top1 = 0.92
I0630 02:07:45.678951 27297 caffe.cpp:312] Batch 3, accuracy/top5 = 1
I0630 02:07:45.678954 27297 caffe.cpp:312] Batch 3, loss = 0.1
I0630 02:07:45.687098 27297 caffe.cpp:312] Batch 4, accuracy/top1 = 0.9
I0630 02:07:45.687116 27297 caffe.cpp:312] Batch 4, accuracy/top5 = 1
I0630 02:07:45.687120 27297 caffe.cpp:312] Batch 4, loss = 0.12
I0630 02:07:45.695289 27297 caffe.cpp:312] Batch 5, accuracy/top1 = 0.92
I0630 02:07:45.695297 27297 caffe.cpp:312] Batch 5, accuracy/top5 = 0.98
I0630 02:07:45.695302 27297 caffe.cpp:312] Batch 5, loss = 0.14
I0630 02:07:45.703456 27297 caffe.cpp:312] Batch 6, accuracy/top1 = 0.96
I0630 02:07:45.703464 27297 caffe.cpp:312] Batch 6, accuracy/top5 = 1
I0630 02:07:45.703469 27297 caffe.cpp:312] Batch 6, loss = 0.1
I0630 02:07:45.711649 27297 caffe.cpp:312] Batch 7, accuracy/top1 = 0.9
I0630 02:07:45.711658 27297 caffe.cpp:312] Batch 7, accuracy/top5 = 1
I0630 02:07:45.711661 27297 caffe.cpp:312] Batch 7, loss = 0.36
I0630 02:07:45.719823 27297 caffe.cpp:312] Batch 8, accuracy/top1 = 0.9
I0630 02:07:45.719837 27297 caffe.cpp:312] Batch 8, accuracy/top5 = 1
I0630 02:07:45.719841 27297 caffe.cpp:312] Batch 8, loss = 0.16
I0630 02:07:45.728086 27297 caffe.cpp:312] Batch 9, accuracy/top1 = 0.94
I0630 02:07:45.728094 27297 caffe.cpp:312] Batch 9, accuracy/top5 = 1
I0630 02:07:45.728098 27297 caffe.cpp:312] Batch 9, loss = 0.08
I0630 02:07:45.736114 27297 caffe.cpp:312] Batch 10, accuracy/top1 = 0.94
I0630 02:07:45.736122 27297 caffe.cpp:312] Batch 10, accuracy/top5 = 1
I0630 02:07:45.736126 27297 caffe.cpp:312] Batch 10, loss = 0.08
I0630 02:07:45.744282 27297 caffe.cpp:312] Batch 11, accuracy/top1 = 0.98
I0630 02:07:45.744293 27297 caffe.cpp:312] Batch 11, accuracy/top5 = 1
I0630 02:07:45.744297 27297 caffe.cpp:312] Batch 11, loss = 0.1
I0630 02:07:45.752499 27297 caffe.cpp:312] Batch 12, accuracy/top1 = 0.92
I0630 02:07:45.752508 27297 caffe.cpp:312] Batch 12, accuracy/top5 = 1
I0630 02:07:45.752512 27297 caffe.cpp:312] Batch 12, loss = 0.06
I0630 02:07:45.760668 27297 caffe.cpp:312] Batch 13, accuracy/top1 = 0.88
I0630 02:07:45.760676 27297 caffe.cpp:312] Batch 13, accuracy/top5 = 1
I0630 02:07:45.760680 27297 caffe.cpp:312] Batch 13, loss = 0.48
I0630 02:07:45.768815 27297 caffe.cpp:312] Batch 14, accuracy/top1 = 0.88
I0630 02:07:45.768823 27297 caffe.cpp:312] Batch 14, accuracy/top5 = 1
I0630 02:07:45.768826 27297 caffe.cpp:312] Batch 14, loss = 0.32
I0630 02:07:45.777077 27297 caffe.cpp:312] Batch 15, accuracy/top1 = 0.82
I0630 02:07:45.777094 27297 caffe.cpp:312] Batch 15, accuracy/top5 = 1
I0630 02:07:45.777097 27297 caffe.cpp:312] Batch 15, loss = 0.42
I0630 02:07:45.785259 27297 caffe.cpp:312] Batch 16, accuracy/top1 = 0.94
I0630 02:07:45.785266 27297 caffe.cpp:312] Batch 16, accuracy/top5 = 1
I0630 02:07:45.785270 27297 caffe.cpp:312] Batch 16, loss = 0.26
I0630 02:07:45.793349 27297 caffe.cpp:312] Batch 17, accuracy/top1 = 0.88
I0630 02:07:45.793357 27297 caffe.cpp:312] Batch 17, accuracy/top5 = 0.98
I0630 02:07:45.793361 27297 caffe.cpp:312] Batch 17, loss = 0.56
I0630 02:07:45.801525 27297 caffe.cpp:312] Batch 18, accuracy/top1 = 0.88
I0630 02:07:45.801533 27297 caffe.cpp:312] Batch 18, accuracy/top5 = 1
I0630 02:07:45.801538 27297 caffe.cpp:312] Batch 18, loss = 0.16
I0630 02:07:45.809681 27297 caffe.cpp:312] Batch 19, accuracy/top1 = 0.92
I0630 02:07:45.809700 27297 caffe.cpp:312] Batch 19, accuracy/top5 = 1
I0630 02:07:45.809703 27297 caffe.cpp:312] Batch 19, loss = 0.14
I0630 02:07:45.817883 27297 caffe.cpp:312] Batch 20, accuracy/top1 = 0.88
I0630 02:07:45.817891 27297 caffe.cpp:312] Batch 20, accuracy/top5 = 1
I0630 02:07:45.817895 27297 caffe.cpp:312] Batch 20, loss = 0.2
I0630 02:07:45.826041 27297 caffe.cpp:312] Batch 21, accuracy/top1 = 0.94
I0630 02:07:45.826050 27297 caffe.cpp:312] Batch 21, accuracy/top5 = 1
I0630 02:07:45.826053 27297 caffe.cpp:312] Batch 21, loss = 0.2
I0630 02:07:45.834205 27297 caffe.cpp:312] Batch 22, accuracy/top1 = 0.84
I0630 02:07:45.834215 27297 caffe.cpp:312] Batch 22, accuracy/top5 = 1
I0630 02:07:45.834218 27297 caffe.cpp:312] Batch 22, loss = 0.3
I0630 02:07:45.842427 27297 caffe.cpp:312] Batch 23, accuracy/top1 = 0.88
I0630 02:07:45.842437 27297 caffe.cpp:312] Batch 23, accuracy/top5 = 0.98
I0630 02:07:45.842440 27297 caffe.cpp:312] Batch 23, loss = 0.42
I0630 02:07:45.850589 27297 caffe.cpp:312] Batch 24, accuracy/top1 = 0.92
I0630 02:07:45.850597 27297 caffe.cpp:312] Batch 24, accuracy/top5 = 0.98
I0630 02:07:45.850610 27297 caffe.cpp:312] Batch 24, loss = 0.28
I0630 02:07:45.858597 27297 caffe.cpp:312] Batch 25, accuracy/top1 = 0.94
I0630 02:07:45.858605 27297 caffe.cpp:312] Batch 25, accuracy/top5 = 1
I0630 02:07:45.858609 27297 caffe.cpp:312] Batch 25, loss = 0.1
I0630 02:07:45.866783 27297 caffe.cpp:312] Batch 26, accuracy/top1 = 0.92
I0630 02:07:45.866796 27297 caffe.cpp:312] Batch 26, accuracy/top5 = 1
I0630 02:07:45.866799 27297 caffe.cpp:312] Batch 26, loss = 0.28
I0630 02:07:45.874938 27297 caffe.cpp:312] Batch 27, accuracy/top1 = 0.9
I0630 02:07:45.874946 27297 caffe.cpp:312] Batch 27, accuracy/top5 = 1
I0630 02:07:45.874950 27297 caffe.cpp:312] Batch 27, loss = 0.24
I0630 02:07:45.883059 27297 caffe.cpp:312] Batch 28, accuracy/top1 = 0.92
I0630 02:07:45.883067 27297 caffe.cpp:312] Batch 28, accuracy/top5 = 1
I0630 02:07:45.883071 27297 caffe.cpp:312] Batch 28, loss = 0.18
I0630 02:07:45.891263 27297 caffe.cpp:312] Batch 29, accuracy/top1 = 0.88
I0630 02:07:45.891270 27297 caffe.cpp:312] Batch 29, accuracy/top5 = 1
I0630 02:07:45.891274 27297 caffe.cpp:312] Batch 29, loss = 0.36
I0630 02:07:45.899479 27297 caffe.cpp:312] Batch 30, accuracy/top1 = 0.9
I0630 02:07:45.899497 27297 caffe.cpp:312] Batch 30, accuracy/top5 = 1
I0630 02:07:45.899500 27297 caffe.cpp:312] Batch 30, loss = 0.28
I0630 02:07:45.907656 27297 caffe.cpp:312] Batch 31, accuracy/top1 = 0.9
I0630 02:07:45.907665 27297 caffe.cpp:312] Batch 31, accuracy/top5 = 1
I0630 02:07:45.907668 27297 caffe.cpp:312] Batch 31, loss = 0.14
I0630 02:07:45.915861 27297 caffe.cpp:312] Batch 32, accuracy/top1 = 0.88
I0630 02:07:45.915868 27297 caffe.cpp:312] Batch 32, accuracy/top5 = 1
I0630 02:07:45.915873 27297 caffe.cpp:312] Batch 32, loss = 0.2
I0630 02:07:45.923897 27297 caffe.cpp:312] Batch 33, accuracy/top1 = 0.96
I0630 02:07:45.923905 27297 caffe.cpp:312] Batch 33, accuracy/top5 = 0.98
I0630 02:07:45.923909 27297 caffe.cpp:312] Batch 33, loss = 0.12
I0630 02:07:45.932098 27297 caffe.cpp:312] Batch 34, accuracy/top1 = 0.88
I0630 02:07:45.932116 27297 caffe.cpp:312] Batch 34, accuracy/top5 = 1
I0630 02:07:45.932121 27297 caffe.cpp:312] Batch 34, loss = 0.22
I0630 02:07:45.940312 27297 caffe.cpp:312] Batch 35, accuracy/top1 = 0.92
I0630 02:07:45.940321 27297 caffe.cpp:312] Batch 35, accuracy/top5 = 1
I0630 02:07:45.940325 27297 caffe.cpp:312] Batch 35, loss = 0.1
I0630 02:07:45.948504 27297 caffe.cpp:312] Batch 36, accuracy/top1 = 0.92
I0630 02:07:45.948513 27297 caffe.cpp:312] Batch 36, accuracy/top5 = 0.98
I0630 02:07:45.948516 27297 caffe.cpp:312] Batch 36, loss = 0.18
I0630 02:07:45.956715 27297 caffe.cpp:312] Batch 37, accuracy/top1 = 0.9
I0630 02:07:45.956724 27297 caffe.cpp:312] Batch 37, accuracy/top5 = 1
I0630 02:07:45.956728 27297 caffe.cpp:312] Batch 37, loss = 0.2
I0630 02:07:45.964915 27297 caffe.cpp:312] Batch 38, accuracy/top1 = 0.86
I0630 02:07:45.964925 27297 caffe.cpp:312] Batch 38, accuracy/top5 = 0.96
I0630 02:07:45.964927 27297 caffe.cpp:312] Batch 38, loss = 0.62
I0630 02:07:45.973088 27297 caffe.cpp:312] Batch 39, accuracy/top1 = 0.94
I0630 02:07:45.973095 27297 caffe.cpp:312] Batch 39, accuracy/top5 = 0.98
I0630 02:07:45.973099 27297 caffe.cpp:312] Batch 39, loss = 0.34
I0630 02:07:45.981307 27297 caffe.cpp:312] Batch 40, accuracy/top1 = 0.88
I0630 02:07:45.981315 27297 caffe.cpp:312] Batch 40, accuracy/top5 = 1
I0630 02:07:45.981319 27297 caffe.cpp:312] Batch 40, loss = 0.18
I0630 02:07:45.989517 27297 caffe.cpp:312] Batch 41, accuracy/top1 = 0.9
I0630 02:07:45.989528 27297 caffe.cpp:312] Batch 41, accuracy/top5 = 1
I0630 02:07:45.989532 27297 caffe.cpp:312] Batch 41, loss = 0.2
I0630 02:07:45.997722 27297 caffe.cpp:312] Batch 42, accuracy/top1 = 0.94
I0630 02:07:45.997731 27297 caffe.cpp:312] Batch 42, accuracy/top5 = 1
I0630 02:07:45.997735 27297 caffe.cpp:312] Batch 42, loss = 0.2
I0630 02:07:46.005900 27297 caffe.cpp:312] Batch 43, accuracy/top1 = 0.88
I0630 02:07:46.005908 27297 caffe.cpp:312] Batch 43, accuracy/top5 = 1
I0630 02:07:46.005913 27297 caffe.cpp:312] Batch 43, loss = 0.18
I0630 02:07:46.014091 27297 caffe.cpp:312] Batch 44, accuracy/top1 = 0.86
I0630 02:07:46.014098 27297 caffe.cpp:312] Batch 44, accuracy/top5 = 0.98
I0630 02:07:46.014102 27297 caffe.cpp:312] Batch 44, loss = 0.38
I0630 02:07:46.022328 27297 caffe.cpp:312] Batch 45, accuracy/top1 = 0.84
I0630 02:07:46.022348 27297 caffe.cpp:312] Batch 45, accuracy/top5 = 1
I0630 02:07:46.022352 27297 caffe.cpp:312] Batch 45, loss = 0.42
I0630 02:07:46.030549 27297 caffe.cpp:312] Batch 46, accuracy/top1 = 0.92
I0630 02:07:46.030558 27297 caffe.cpp:312] Batch 46, accuracy/top5 = 1
I0630 02:07:46.030561 27297 caffe.cpp:312] Batch 46, loss = 0.1
I0630 02:07:46.038686 27297 caffe.cpp:312] Batch 47, accuracy/top1 = 0.82
I0630 02:07:46.038693 27297 caffe.cpp:312] Batch 47, accuracy/top5 = 1
I0630 02:07:46.038697 27297 caffe.cpp:312] Batch 47, loss = 0.4
I0630 02:07:46.046886 27297 caffe.cpp:312] Batch 48, accuracy/top1 = 0.88
I0630 02:07:46.046893 27297 caffe.cpp:312] Batch 48, accuracy/top5 = 0.98
I0630 02:07:46.046897 27297 caffe.cpp:312] Batch 48, loss = 0.46
I0630 02:07:46.055061 27297 caffe.cpp:312] Batch 49, accuracy/top1 = 0.84
I0630 02:07:46.055079 27297 caffe.cpp:312] Batch 49, accuracy/top5 = 1
I0630 02:07:46.055083 27297 caffe.cpp:312] Batch 49, loss = 0.26
I0630 02:07:46.063300 27297 caffe.cpp:312] Batch 50, accuracy/top1 = 0.86
I0630 02:07:46.063308 27297 caffe.cpp:312] Batch 50, accuracy/top5 = 1
I0630 02:07:46.063311 27297 caffe.cpp:312] Batch 50, loss = 0.46
I0630 02:07:46.071497 27297 caffe.cpp:312] Batch 51, accuracy/top1 = 0.92
I0630 02:07:46.071506 27297 caffe.cpp:312] Batch 51, accuracy/top5 = 0.98
I0630 02:07:46.071509 27297 caffe.cpp:312] Batch 51, loss = 0.26
I0630 02:07:46.079725 27297 caffe.cpp:312] Batch 52, accuracy/top1 = 0.94
I0630 02:07:46.079735 27297 caffe.cpp:312] Batch 52, accuracy/top5 = 1
I0630 02:07:46.079737 27297 caffe.cpp:312] Batch 52, loss = 0.02
I0630 02:07:46.087896 27297 caffe.cpp:312] Batch 53, accuracy/top1 = 0.96
I0630 02:07:46.087908 27297 caffe.cpp:312] Batch 53, accuracy/top5 = 1
I0630 02:07:46.087911 27297 caffe.cpp:312] Batch 53, loss = 0.04
I0630 02:07:46.096112 27297 caffe.cpp:312] Batch 54, accuracy/top1 = 0.9
I0630 02:07:46.096119 27297 caffe.cpp:312] Batch 54, accuracy/top5 = 1
I0630 02:07:46.096123 27297 caffe.cpp:312] Batch 54, loss = 0.24
I0630 02:07:46.104315 27297 caffe.cpp:312] Batch 55, accuracy/top1 = 0.9
I0630 02:07:46.104322 27297 caffe.cpp:312] Batch 55, accuracy/top5 = 0.98
I0630 02:07:46.104326 27297 caffe.cpp:312] Batch 55, loss = 0.38
I0630 02:07:46.112478 27297 caffe.cpp:312] Batch 56, accuracy/top1 = 0.86
I0630 02:07:46.112490 27297 caffe.cpp:312] Batch 56, accuracy/top5 = 0.98
I0630 02:07:46.112494 27297 caffe.cpp:312] Batch 56, loss = 0.52
I0630 02:07:46.120659 27297 caffe.cpp:312] Batch 57, accuracy/top1 = 0.94
I0630 02:07:46.120667 27297 caffe.cpp:312] Batch 57, accuracy/top5 = 1
I0630 02:07:46.120671 27297 caffe.cpp:312] Batch 57, loss = 0.1
I0630 02:07:46.128850 27297 caffe.cpp:312] Batch 58, accuracy/top1 = 0.92
I0630 02:07:46.128859 27297 caffe.cpp:312] Batch 58, accuracy/top5 = 1
I0630 02:07:46.128862 27297 caffe.cpp:312] Batch 58, loss = 0.16
I0630 02:07:46.136986 27297 caffe.cpp:312] Batch 59, accuracy/top1 = 0.92
I0630 02:07:46.136993 27297 caffe.cpp:312] Batch 59, accuracy/top5 = 1
I0630 02:07:46.136997 27297 caffe.cpp:312] Batch 59, loss = 0.22
I0630 02:07:46.145151 27297 caffe.cpp:312] Batch 60, accuracy/top1 = 0.88
I0630 02:07:46.145167 27297 caffe.cpp:312] Batch 60, accuracy/top5 = 1
I0630 02:07:46.145171 27297 caffe.cpp:312] Batch 60, loss = 0.36
I0630 02:07:46.153347 27297 caffe.cpp:312] Batch 61, accuracy/top1 = 0.92
I0630 02:07:46.153354 27297 caffe.cpp:312] Batch 61, accuracy/top5 = 0.98
I0630 02:07:46.153358 27297 caffe.cpp:312] Batch 61, loss = 0.24
I0630 02:07:46.161582 27297 caffe.cpp:312] Batch 62, accuracy/top1 = 0.98
I0630 02:07:46.161589 27297 caffe.cpp:312] Batch 62, accuracy/top5 = 1
I0630 02:07:46.161593 27297 caffe.cpp:312] Batch 62, loss = 0.02
I0630 02:07:46.169607 27297 caffe.cpp:312] Batch 63, accuracy/top1 = 0.88
I0630 02:07:46.169615 27297 caffe.cpp:312] Batch 63, accuracy/top5 = 1
I0630 02:07:46.169627 27297 caffe.cpp:312] Batch 63, loss = 0.24
I0630 02:07:46.177821 27297 caffe.cpp:312] Batch 64, accuracy/top1 = 0.86
I0630 02:07:46.177839 27297 caffe.cpp:312] Batch 64, accuracy/top5 = 1
I0630 02:07:46.177844 27297 caffe.cpp:312] Batch 64, loss = 0.22
I0630 02:07:46.186022 27297 caffe.cpp:312] Batch 65, accuracy/top1 = 0.94
I0630 02:07:46.186029 27297 caffe.cpp:312] Batch 65, accuracy/top5 = 1
I0630 02:07:46.186033 27297 caffe.cpp:312] Batch 65, loss = 0.12
I0630 02:07:46.194183 27297 caffe.cpp:312] Batch 66, accuracy/top1 = 0.92
I0630 02:07:46.194191 27297 caffe.cpp:312] Batch 66, accuracy/top5 = 1
I0630 02:07:46.194195 27297 caffe.cpp:312] Batch 66, loss = 0.16
I0630 02:07:46.202353 27297 caffe.cpp:312] Batch 67, accuracy/top1 = 0.92
I0630 02:07:46.202363 27297 caffe.cpp:312] Batch 67, accuracy/top5 = 1
I0630 02:07:46.202366 27297 caffe.cpp:312] Batch 67, loss = 0.12
I0630 02:07:46.210562 27297 caffe.cpp:312] Batch 68, accuracy/top1 = 0.92
I0630 02:07:46.210572 27297 caffe.cpp:312] Batch 68, accuracy/top5 = 1
I0630 02:07:46.210575 27297 caffe.cpp:312] Batch 68, loss = 0.28
I0630 02:07:46.218742 27297 caffe.cpp:312] Batch 69, accuracy/top1 = 0.94
I0630 02:07:46.218750 27297 caffe.cpp:312] Batch 69, accuracy/top5 = 1
I0630 02:07:46.218755 27297 caffe.cpp:312] Batch 69, loss = 0.24
I0630 02:07:46.226951 27297 caffe.cpp:312] Batch 70, accuracy/top1 = 0.96
I0630 02:07:46.226958 27297 caffe.cpp:312] Batch 70, accuracy/top5 = 0.98
I0630 02:07:46.226963 27297 caffe.cpp:312] Batch 70, loss = 0.32
I0630 02:07:46.235163 27297 caffe.cpp:312] Batch 71, accuracy/top1 = 0.86
I0630 02:07:46.235175 27297 caffe.cpp:312] Batch 71, accuracy/top5 = 1
I0630 02:07:46.235179 27297 caffe.cpp:312] Batch 71, loss = 0.24
I0630 02:07:46.243381 27297 caffe.cpp:312] Batch 72, accuracy/top1 = 0.84
I0630 02:07:46.243388 27297 caffe.cpp:312] Batch 72, accuracy/top5 = 0.98
I0630 02:07:46.243392 27297 caffe.cpp:312] Batch 72, loss = 0.46
I0630 02:07:46.251556 27297 caffe.cpp:312] Batch 73, accuracy/top1 = 0.9
I0630 02:07:46.251564 27297 caffe.cpp:312] Batch 73, accuracy/top5 = 1
I0630 02:07:46.251569 27297 caffe.cpp:312] Batch 73, loss = 0.24
I0630 02:07:46.259759 27297 caffe.cpp:312] Batch 74, accuracy/top1 = 0.88
I0630 02:07:46.259766 27297 caffe.cpp:312] Batch 74, accuracy/top5 = 1
I0630 02:07:46.259770 27297 caffe.cpp:312] Batch 74, loss = 0.18
I0630 02:07:46.267994 27297 caffe.cpp:312] Batch 75, accuracy/top1 = 0.9
I0630 02:07:46.268010 27297 caffe.cpp:312] Batch 75, accuracy/top5 = 1
I0630 02:07:46.268014 27297 caffe.cpp:312] Batch 75, loss = 0.28
I0630 02:07:46.276198 27297 caffe.cpp:312] Batch 76, accuracy/top1 = 0.9
I0630 02:07:46.276206 27297 caffe.cpp:312] Batch 76, accuracy/top5 = 1
I0630 02:07:46.276211 27297 caffe.cpp:312] Batch 76, loss = 0.14
I0630 02:07:46.284261 27297 caffe.cpp:312] Batch 77, accuracy/top1 = 0.9
I0630 02:07:46.284268 27297 caffe.cpp:312] Batch 77, accuracy/top5 = 1
I0630 02:07:46.284272 27297 caffe.cpp:312] Batch 77, loss = 0.18
I0630 02:07:46.292464 27297 caffe.cpp:312] Batch 78, accuracy/top1 = 0.92
I0630 02:07:46.292471 27297 caffe.cpp:312] Batch 78, accuracy/top5 = 1
I0630 02:07:46.292475 27297 caffe.cpp:312] Batch 78, loss = 0.18
I0630 02:07:46.300704 27297 caffe.cpp:312] Batch 79, accuracy/top1 = 0.94
I0630 02:07:46.300720 27297 caffe.cpp:312] Batch 79, accuracy/top5 = 1
I0630 02:07:46.300724 27297 caffe.cpp:312] Batch 79, loss = 0.24
I0630 02:07:46.308949 27297 caffe.cpp:312] Batch 80, accuracy/top1 = 0.96
I0630 02:07:46.308957 27297 caffe.cpp:312] Batch 80, accuracy/top5 = 0.98
I0630 02:07:46.308961 27297 caffe.cpp:312] Batch 80, loss = 0.12
I0630 02:07:46.317067 27297 caffe.cpp:312] Batch 81, accuracy/top1 = 0.92
I0630 02:07:46.317075 27297 caffe.cpp:312] Batch 81, accuracy/top5 = 1
I0630 02:07:46.317080 27297 caffe.cpp:312] Batch 81, loss = 0.12
I0630 02:07:46.325270 27297 caffe.cpp:312] Batch 82, accuracy/top1 = 0.88
I0630 02:07:46.325280 27297 caffe.cpp:312] Batch 82, accuracy/top5 = 1
I0630 02:07:46.325284 27297 caffe.cpp:312] Batch 82, loss = 0.48
I0630 02:07:46.333367 27297 caffe.cpp:312] Batch 83, accuracy/top1 = 0.94
I0630 02:07:46.333375 27297 caffe.cpp:312] Batch 83, accuracy/top5 = 1
I0630 02:07:46.333379 27297 caffe.cpp:312] Batch 83, loss = 0.14
I0630 02:07:46.341547 27297 caffe.cpp:312] Batch 84, accuracy/top1 = 0.98
I0630 02:07:46.341555 27297 caffe.cpp:312] Batch 84, accuracy/top5 = 1
I0630 02:07:46.341559 27297 caffe.cpp:312] Batch 84, loss = 0.12
I0630 02:07:46.349701 27297 caffe.cpp:312] Batch 85, accuracy/top1 = 0.92
I0630 02:07:46.349709 27297 caffe.cpp:312] Batch 85, accuracy/top5 = 1
I0630 02:07:46.349714 27297 caffe.cpp:312] Batch 85, loss = 0.08
I0630 02:07:46.357916 27297 caffe.cpp:312] Batch 86, accuracy/top1 = 0.92
I0630 02:07:46.357928 27297 caffe.cpp:312] Batch 86, accuracy/top5 = 1
I0630 02:07:46.357931 27297 caffe.cpp:312] Batch 86, loss = 0.2
I0630 02:07:46.366111 27297 caffe.cpp:312] Batch 87, accuracy/top1 = 0.98
I0630 02:07:46.366119 27297 caffe.cpp:312] Batch 87, accuracy/top5 = 1
I0630 02:07:46.366123 27297 caffe.cpp:312] Batch 87, loss = 0.1
I0630 02:07:46.374229 27297 caffe.cpp:312] Batch 88, accuracy/top1 = 0.92
I0630 02:07:46.374238 27297 caffe.cpp:312] Batch 88, accuracy/top5 = 1
I0630 02:07:46.374241 27297 caffe.cpp:312] Batch 88, loss = 0.16
I0630 02:07:46.382446 27297 caffe.cpp:312] Batch 89, accuracy/top1 = 0.92
I0630 02:07:46.382453 27297 caffe.cpp:312] Batch 89, accuracy/top5 = 1
I0630 02:07:46.382457 27297 caffe.cpp:312] Batch 89, loss = 0.14
I0630 02:07:46.390636 27297 caffe.cpp:312] Batch 90, accuracy/top1 = 0.88
I0630 02:07:46.390653 27297 caffe.cpp:312] Batch 90, accuracy/top5 = 1
I0630 02:07:46.390657 27297 caffe.cpp:312] Batch 90, loss = 0.22
I0630 02:07:46.398646 27297 caffe.cpp:312] Batch 91, accuracy/top1 = 0.92
I0630 02:07:46.398654 27297 caffe.cpp:312] Batch 91, accuracy/top5 = 1
I0630 02:07:46.398658 27297 caffe.cpp:312] Batch 91, loss = 0.24
I0630 02:07:46.406771 27297 caffe.cpp:312] Batch 92, accuracy/top1 = 0.88
I0630 02:07:46.406779 27297 caffe.cpp:312] Batch 92, accuracy/top5 = 1
I0630 02:07:46.406782 27297 caffe.cpp:312] Batch 92, loss = 0.16
I0630 02:07:46.414925 27297 caffe.cpp:312] Batch 93, accuracy/top1 = 0.92
I0630 02:07:46.414932 27297 caffe.cpp:312] Batch 93, accuracy/top5 = 1
I0630 02:07:46.414937 27297 caffe.cpp:312] Batch 93, loss = 0.18
I0630 02:07:46.423072 27297 caffe.cpp:312] Batch 94, accuracy/top1 = 0.92
I0630 02:07:46.423087 27297 caffe.cpp:312] Batch 94, accuracy/top5 = 0.98
I0630 02:07:46.423091 27297 caffe.cpp:312] Batch 94, loss = 0.32
I0630 02:07:46.431278 27297 caffe.cpp:312] Batch 95, accuracy/top1 = 0.92
I0630 02:07:46.431287 27297 caffe.cpp:312] Batch 95, accuracy/top5 = 0.98
I0630 02:07:46.431289 27297 caffe.cpp:312] Batch 95, loss = 0.36
I0630 02:07:46.439409 27297 caffe.cpp:312] Batch 96, accuracy/top1 = 1
I0630 02:07:46.439416 27297 caffe.cpp:312] Batch 96, accuracy/top5 = 1
I0630 02:07:46.439420 27297 caffe.cpp:312] Batch 96, loss = 0
I0630 02:07:46.447546 27297 caffe.cpp:312] Batch 97, accuracy/top1 = 0.94
I0630 02:07:46.447556 27297 caffe.cpp:312] Batch 97, accuracy/top5 = 1
I0630 02:07:46.447561 27297 caffe.cpp:312] Batch 97, loss = 0.14
I0630 02:07:46.455755 27297 caffe.cpp:312] Batch 98, accuracy/top1 = 0.92
I0630 02:07:46.455765 27297 caffe.cpp:312] Batch 98, accuracy/top5 = 1
I0630 02:07:46.455768 27297 caffe.cpp:312] Batch 98, loss = 0.14
I0630 02:07:46.463953 27297 caffe.cpp:312] Batch 99, accuracy/top1 = 0.88
I0630 02:07:46.463961 27297 caffe.cpp:312] Batch 99, accuracy/top5 = 1
I0630 02:07:46.463965 27297 caffe.cpp:312] Batch 99, loss = 0.34
I0630 02:07:46.472072 27297 caffe.cpp:312] Batch 100, accuracy/top1 = 0.96
I0630 02:07:46.472080 27297 caffe.cpp:312] Batch 100, accuracy/top5 = 1
I0630 02:07:46.472084 27297 caffe.cpp:312] Batch 100, loss = 0.1
I0630 02:07:46.480275 27297 caffe.cpp:312] Batch 101, accuracy/top1 = 0.88
I0630 02:07:46.480288 27297 caffe.cpp:312] Batch 101, accuracy/top5 = 1
I0630 02:07:46.480291 27297 caffe.cpp:312] Batch 101, loss = 0.1
I0630 02:07:46.488435 27297 caffe.cpp:312] Batch 102, accuracy/top1 = 0.94
I0630 02:07:46.488445 27297 caffe.cpp:312] Batch 102, accuracy/top5 = 1
I0630 02:07:46.488457 27297 caffe.cpp:312] Batch 102, loss = 0.06
I0630 02:07:46.496610 27297 caffe.cpp:312] Batch 103, accuracy/top1 = 0.88
I0630 02:07:46.496618 27297 caffe.cpp:312] Batch 103, accuracy/top5 = 1
I0630 02:07:46.496623 27297 caffe.cpp:312] Batch 103, loss = 0.3
I0630 02:07:46.504784 27297 caffe.cpp:312] Batch 104, accuracy/top1 = 0.88
I0630 02:07:46.504792 27297 caffe.cpp:312] Batch 104, accuracy/top5 = 1
I0630 02:07:46.504796 27297 caffe.cpp:312] Batch 104, loss = 0.26
I0630 02:07:46.513046 27297 caffe.cpp:312] Batch 105, accuracy/top1 = 0.94
I0630 02:07:46.513065 27297 caffe.cpp:312] Batch 105, accuracy/top5 = 0.98
I0630 02:07:46.513068 27297 caffe.cpp:312] Batch 105, loss = 0.14
I0630 02:07:46.521219 27297 caffe.cpp:312] Batch 106, accuracy/top1 = 0.94
I0630 02:07:46.521229 27297 caffe.cpp:312] Batch 106, accuracy/top5 = 1
I0630 02:07:46.521232 27297 caffe.cpp:312] Batch 106, loss = 0.12
I0630 02:07:46.529398 27297 caffe.cpp:312] Batch 107, accuracy/top1 = 0.9
I0630 02:07:46.529407 27297 caffe.cpp:312] Batch 107, accuracy/top5 = 1
I0630 02:07:46.529410 27297 caffe.cpp:312] Batch 107, loss = 0.16
I0630 02:07:46.537549 27297 caffe.cpp:312] Batch 108, accuracy/top1 = 0.86
I0630 02:07:46.537556 27297 caffe.cpp:312] Batch 108, accuracy/top5 = 1
I0630 02:07:46.537560 27297 caffe.cpp:312] Batch 108, loss = 0.36
I0630 02:07:46.545775 27297 caffe.cpp:312] Batch 109, accuracy/top1 = 0.9
I0630 02:07:46.545792 27297 caffe.cpp:312] Batch 109, accuracy/top5 = 1
I0630 02:07:46.545796 27297 caffe.cpp:312] Batch 109, loss = 0.14
I0630 02:07:46.553964 27297 caffe.cpp:312] Batch 110, accuracy/top1 = 0.88
I0630 02:07:46.553972 27297 caffe.cpp:312] Batch 110, accuracy/top5 = 0.98
I0630 02:07:46.553977 27297 caffe.cpp:312] Batch 110, loss = 0.6
I0630 02:07:46.562141 27297 caffe.cpp:312] Batch 111, accuracy/top1 = 0.98
I0630 02:07:46.562150 27297 caffe.cpp:312] Batch 111, accuracy/top5 = 1
I0630 02:07:46.562153 27297 caffe.cpp:312] Batch 111, loss = 0.02
I0630 02:07:46.570121 27297 caffe.cpp:312] Batch 112, accuracy/top1 = 0.88
I0630 02:07:46.570128 27297 caffe.cpp:312] Batch 112, accuracy/top5 = 1
I0630 02:07:46.570132 27297 caffe.cpp:312] Batch 112, loss = 0.34
I0630 02:07:46.578361 27297 caffe.cpp:312] Batch 113, accuracy/top1 = 0.86
I0630 02:07:46.578374 27297 caffe.cpp:312] Batch 113, accuracy/top5 = 1
I0630 02:07:46.578378 27297 caffe.cpp:312] Batch 113, loss = 0.28
I0630 02:07:46.586575 27297 caffe.cpp:312] Batch 114, accuracy/top1 = 0.88
I0630 02:07:46.586582 27297 caffe.cpp:312] Batch 114, accuracy/top5 = 1
I0630 02:07:46.586586 27297 caffe.cpp:312] Batch 114, loss = 0.22
I0630 02:07:46.594761 27297 caffe.cpp:312] Batch 115, accuracy/top1 = 0.96
I0630 02:07:46.594769 27297 caffe.cpp:312] Batch 115, accuracy/top5 = 1
I0630 02:07:46.594774 27297 caffe.cpp:312] Batch 115, loss = 0.04
I0630 02:07:46.602962 27297 caffe.cpp:312] Batch 116, accuracy/top1 = 0.84
I0630 02:07:46.602973 27297 caffe.cpp:312] Batch 116, accuracy/top5 = 1
I0630 02:07:46.602977 27297 caffe.cpp:312] Batch 116, loss = 0.2
I0630 02:07:46.611171 27297 caffe.cpp:312] Batch 117, accuracy/top1 = 0.88
I0630 02:07:46.611179 27297 caffe.cpp:312] Batch 117, accuracy/top5 = 1
I0630 02:07:46.611182 27297 caffe.cpp:312] Batch 117, loss = 0.3
I0630 02:07:46.619289 27297 caffe.cpp:312] Batch 118, accuracy/top1 = 0.92
I0630 02:07:46.619297 27297 caffe.cpp:312] Batch 118, accuracy/top5 = 1
I0630 02:07:46.619302 27297 caffe.cpp:312] Batch 118, loss = 0.14
I0630 02:07:46.627470 27297 caffe.cpp:312] Batch 119, accuracy/top1 = 0.92
I0630 02:07:46.627477 27297 caffe.cpp:312] Batch 119, accuracy/top5 = 1
I0630 02:07:46.627481 27297 caffe.cpp:312] Batch 119, loss = 0.18
I0630 02:07:46.635707 27297 caffe.cpp:312] Batch 120, accuracy/top1 = 0.9
I0630 02:07:46.635725 27297 caffe.cpp:312] Batch 120, accuracy/top5 = 1
I0630 02:07:46.635727 27297 caffe.cpp:312] Batch 120, loss = 0.14
I0630 02:07:46.643915 27297 caffe.cpp:312] Batch 121, accuracy/top1 = 0.9
I0630 02:07:46.643923 27297 caffe.cpp:312] Batch 121, accuracy/top5 = 1
I0630 02:07:46.643944 27297 caffe.cpp:312] Batch 121, loss = 0.32
I0630 02:07:46.652132 27297 caffe.cpp:312] Batch 122, accuracy/top1 = 0.92
I0630 02:07:46.652139 27297 caffe.cpp:312] Batch 122, accuracy/top5 = 1
I0630 02:07:46.652143 27297 caffe.cpp:312] Batch 122, loss = 0.08
I0630 02:07:46.660354 27297 caffe.cpp:312] Batch 123, accuracy/top1 = 0.9
I0630 02:07:46.660362 27297 caffe.cpp:312] Batch 123, accuracy/top5 = 1
I0630 02:07:46.660365 27297 caffe.cpp:312] Batch 123, loss = 0.22
I0630 02:07:46.668615 27297 caffe.cpp:312] Batch 124, accuracy/top1 = 0.94
I0630 02:07:46.668642 27297 caffe.cpp:312] Batch 124, accuracy/top5 = 1
I0630 02:07:46.668647 27297 caffe.cpp:312] Batch 124, loss = 0.22
I0630 02:07:46.677000 27297 caffe.cpp:312] Batch 125, accuracy/top1 = 0.96
I0630 02:07:46.677016 27297 caffe.cpp:312] Batch 125, accuracy/top5 = 1
I0630 02:07:46.677021 27297 caffe.cpp:312] Batch 125, loss = 0.1
I0630 02:07:46.685287 27297 caffe.cpp:312] Batch 126, accuracy/top1 = 0.96
I0630 02:07:46.685302 27297 caffe.cpp:312] Batch 126, accuracy/top5 = 1
I0630 02:07:46.685305 27297 caffe.cpp:312] Batch 126, loss = 0.04
I0630 02:07:46.693514 27297 caffe.cpp:312] Batch 127, accuracy/top1 = 0.94
I0630 02:07:46.693524 27297 caffe.cpp:312] Batch 127, accuracy/top5 = 1
I0630 02:07:46.693528 27297 caffe.cpp:312] Batch 127, loss = 0.06
I0630 02:07:46.701731 27297 caffe.cpp:312] Batch 128, accuracy/top1 = 0.86
I0630 02:07:46.701745 27297 caffe.cpp:312] Batch 128, accuracy/top5 = 1
I0630 02:07:46.701748 27297 caffe.cpp:312] Batch 128, loss = 0.58
I0630 02:07:46.709926 27297 caffe.cpp:312] Batch 129, accuracy/top1 = 0.94
I0630 02:07:46.709934 27297 caffe.cpp:312] Batch 129, accuracy/top5 = 1
I0630 02:07:46.709939 27297 caffe.cpp:312] Batch 129, loss = 0.04
I0630 02:07:46.718096 27297 caffe.cpp:312] Batch 130, accuracy/top1 = 0.88
I0630 02:07:46.718102 27297 caffe.cpp:312] Batch 130, accuracy/top5 = 1
I0630 02:07:46.718107 27297 caffe.cpp:312] Batch 130, loss = 0.26
I0630 02:07:46.726217 27297 caffe.cpp:312] Batch 131, accuracy/top1 = 0.94
I0630 02:07:46.726228 27297 caffe.cpp:312] Batch 131, accuracy/top5 = 1
I0630 02:07:46.726233 27297 caffe.cpp:312] Batch 131, loss = 0.22
I0630 02:07:46.734433 27297 caffe.cpp:312] Batch 132, accuracy/top1 = 0.92
I0630 02:07:46.734442 27297 caffe.cpp:312] Batch 132, accuracy/top5 = 1
I0630 02:07:46.734447 27297 caffe.cpp:312] Batch 132, loss = 0.08
I0630 02:07:46.742576 27297 caffe.cpp:312] Batch 133, accuracy/top1 = 0.9
I0630 02:07:46.742584 27297 caffe.cpp:312] Batch 133, accuracy/top5 = 1
I0630 02:07:46.742588 27297 caffe.cpp:312] Batch 133, loss = 0.36
I0630 02:07:46.750699 27297 caffe.cpp:312] Batch 134, accuracy/top1 = 0.94
I0630 02:07:46.750706 27297 caffe.cpp:312] Batch 134, accuracy/top5 = 0.98
I0630 02:07:46.750710 27297 caffe.cpp:312] Batch 134, loss = 0.22
I0630 02:07:46.758954 27297 caffe.cpp:312] Batch 135, accuracy/top1 = 0.86
I0630 02:07:46.758970 27297 caffe.cpp:312] Batch 135, accuracy/top5 = 0.98
I0630 02:07:46.758973 27297 caffe.cpp:312] Batch 135, loss = 0.5
I0630 02:07:46.767140 27297 caffe.cpp:312] Batch 136, accuracy/top1 = 0.94
I0630 02:07:46.767148 27297 caffe.cpp:312] Batch 136, accuracy/top5 = 1
I0630 02:07:46.767153 27297 caffe.cpp:312] Batch 136, loss = 0.08
I0630 02:07:46.775292 27297 caffe.cpp:312] Batch 137, accuracy/top1 = 0.84
I0630 02:07:46.775300 27297 caffe.cpp:312] Batch 137, accuracy/top5 = 1
I0630 02:07:46.775305 27297 caffe.cpp:312] Batch 137, loss = 0.26
I0630 02:07:46.783470 27297 caffe.cpp:312] Batch 138, accuracy/top1 = 0.9
I0630 02:07:46.783478 27297 caffe.cpp:312] Batch 138, accuracy/top5 = 0.98
I0630 02:07:46.783483 27297 caffe.cpp:312] Batch 138, loss = 0.2
I0630 02:07:46.791568 27297 caffe.cpp:312] Batch 139, accuracy/top1 = 0.8
I0630 02:07:46.791584 27297 caffe.cpp:312] Batch 139, accuracy/top5 = 0.98
I0630 02:07:46.791587 27297 caffe.cpp:312] Batch 139, loss = 0.42
I0630 02:07:46.799744 27297 caffe.cpp:312] Batch 140, accuracy/top1 = 0.88
I0630 02:07:46.799753 27297 caffe.cpp:312] Batch 140, accuracy/top5 = 1
I0630 02:07:46.799757 27297 caffe.cpp:312] Batch 140, loss = 0.22
I0630 02:07:46.807893 27297 caffe.cpp:312] Batch 141, accuracy/top1 = 0.9
I0630 02:07:46.807901 27297 caffe.cpp:312] Batch 141, accuracy/top5 = 1
I0630 02:07:46.807905 27297 caffe.cpp:312] Batch 141, loss = 0.36
I0630 02:07:46.816052 27297 caffe.cpp:312] Batch 142, accuracy/top1 = 0.88
I0630 02:07:46.816066 27297 caffe.cpp:312] Batch 142, accuracy/top5 = 1
I0630 02:07:46.816068 27297 caffe.cpp:312] Batch 142, loss = 0.28
I0630 02:07:46.824229 27297 caffe.cpp:312] Batch 143, accuracy/top1 = 0.92
I0630 02:07:46.824239 27297 caffe.cpp:312] Batch 143, accuracy/top5 = 1
I0630 02:07:46.824242 27297 caffe.cpp:312] Batch 143, loss = 0.24
I0630 02:07:46.832402 27297 caffe.cpp:312] Batch 144, accuracy/top1 = 0.92
I0630 02:07:46.832411 27297 caffe.cpp:312] Batch 144, accuracy/top5 = 1
I0630 02:07:46.832414 27297 caffe.cpp:312] Batch 144, loss = 0.1
I0630 02:07:46.840409 27297 caffe.cpp:312] Batch 145, accuracy/top1 = 0.94
I0630 02:07:46.840416 27297 caffe.cpp:312] Batch 145, accuracy/top5 = 1
I0630 02:07:46.840420 27297 caffe.cpp:312] Batch 145, loss = 0.16
I0630 02:07:46.848639 27297 caffe.cpp:312] Batch 146, accuracy/top1 = 0.92
I0630 02:07:46.848650 27297 caffe.cpp:312] Batch 146, accuracy/top5 = 1
I0630 02:07:46.848654 27297 caffe.cpp:312] Batch 146, loss = 0.28
I0630 02:07:46.856772 27297 caffe.cpp:312] Batch 147, accuracy/top1 = 0.9
I0630 02:07:46.856781 27297 caffe.cpp:312] Batch 147, accuracy/top5 = 1
I0630 02:07:46.856784 27297 caffe.cpp:312] Batch 147, loss = 0.3
I0630 02:07:46.864890 27297 caffe.cpp:312] Batch 148, accuracy/top1 = 0.9
I0630 02:07:46.864898 27297 caffe.cpp:312] Batch 148, accuracy/top5 = 1
I0630 02:07:46.864902 27297 caffe.cpp:312] Batch 148, loss = 0.12
I0630 02:07:46.872993 27297 caffe.cpp:312] Batch 149, accuracy/top1 = 0.94
I0630 02:07:46.873001 27297 caffe.cpp:312] Batch 149, accuracy/top5 = 1
I0630 02:07:46.873005 27297 caffe.cpp:312] Batch 149, loss = 0.18
I0630 02:07:46.881178 27297 caffe.cpp:312] Batch 150, accuracy/top1 = 0.94
I0630 02:07:46.881196 27297 caffe.cpp:312] Batch 150, accuracy/top5 = 0.98
I0630 02:07:46.881198 27297 caffe.cpp:312] Batch 150, loss = 0.26
I0630 02:07:46.889343 27297 caffe.cpp:312] Batch 151, accuracy/top1 = 0.92
I0630 02:07:46.889351 27297 caffe.cpp:312] Batch 151, accuracy/top5 = 0.98
I0630 02:07:46.889355 27297 caffe.cpp:312] Batch 151, loss = 0.26
I0630 02:07:46.897523 27297 caffe.cpp:312] Batch 152, accuracy/top1 = 0.9
I0630 02:07:46.897531 27297 caffe.cpp:312] Batch 152, accuracy/top5 = 1
I0630 02:07:46.897536 27297 caffe.cpp:312] Batch 152, loss = 0.26
I0630 02:07:46.905632 27297 caffe.cpp:312] Batch 153, accuracy/top1 = 0.9
I0630 02:07:46.905639 27297 caffe.cpp:312] Batch 153, accuracy/top5 = 0.98
I0630 02:07:46.905643 27297 caffe.cpp:312] Batch 153, loss = 0.42
I0630 02:07:46.913908 27297 caffe.cpp:312] Batch 154, accuracy/top1 = 0.94
I0630 02:07:46.913924 27297 caffe.cpp:312] Batch 154, accuracy/top5 = 1
I0630 02:07:46.913928 27297 caffe.cpp:312] Batch 154, loss = 0.12
I0630 02:07:46.922122 27297 caffe.cpp:312] Batch 155, accuracy/top1 = 0.88
I0630 02:07:46.922129 27297 caffe.cpp:312] Batch 155, accuracy/top5 = 1
I0630 02:07:46.922133 27297 caffe.cpp:312] Batch 155, loss = 0.36
I0630 02:07:46.930361 27297 caffe.cpp:312] Batch 156, accuracy/top1 = 0.86
I0630 02:07:46.930369 27297 caffe.cpp:312] Batch 156, accuracy/top5 = 1
I0630 02:07:46.930372 27297 caffe.cpp:312] Batch 156, loss = 0.26
I0630 02:07:46.938580 27297 caffe.cpp:312] Batch 157, accuracy/top1 = 0.94
I0630 02:07:46.938590 27297 caffe.cpp:312] Batch 157, accuracy/top5 = 0.98
I0630 02:07:46.938592 27297 caffe.cpp:312] Batch 157, loss = 0.24
I0630 02:07:46.946802 27297 caffe.cpp:312] Batch 158, accuracy/top1 = 0.96
I0630 02:07:46.946812 27297 caffe.cpp:312] Batch 158, accuracy/top5 = 0.98
I0630 02:07:46.946816 27297 caffe.cpp:312] Batch 158, loss = 0.18
I0630 02:07:46.954994 27297 caffe.cpp:312] Batch 159, accuracy/top1 = 0.86
I0630 02:07:46.955003 27297 caffe.cpp:312] Batch 159, accuracy/top5 = 1
I0630 02:07:46.955006 27297 caffe.cpp:312] Batch 159, loss = 0.2
I0630 02:07:46.963217 27297 caffe.cpp:312] Batch 160, accuracy/top1 = 0.96
I0630 02:07:46.963232 27297 caffe.cpp:312] Batch 160, accuracy/top5 = 1
I0630 02:07:46.963237 27297 caffe.cpp:312] Batch 160, loss = 0.12
I0630 02:07:46.971408 27297 caffe.cpp:312] Batch 161, accuracy/top1 = 0.92
I0630 02:07:46.971421 27297 caffe.cpp:312] Batch 161, accuracy/top5 = 1
I0630 02:07:46.971424 27297 caffe.cpp:312] Batch 161, loss = 0.16
I0630 02:07:46.979601 27297 caffe.cpp:312] Batch 162, accuracy/top1 = 0.9
I0630 02:07:46.979609 27297 caffe.cpp:312] Batch 162, accuracy/top5 = 0.98
I0630 02:07:46.979614 27297 caffe.cpp:312] Batch 162, loss = 0.24
I0630 02:07:46.987802 27297 caffe.cpp:312] Batch 163, accuracy/top1 = 0.92
I0630 02:07:46.987808 27297 caffe.cpp:312] Batch 163, accuracy/top5 = 0.98
I0630 02:07:46.987812 27297 caffe.cpp:312] Batch 163, loss = 0.28
I0630 02:07:46.995965 27297 caffe.cpp:312] Batch 164, accuracy/top1 = 0.86
I0630 02:07:46.995973 27297 caffe.cpp:312] Batch 164, accuracy/top5 = 1
I0630 02:07:46.995976 27297 caffe.cpp:312] Batch 164, loss = 0.26
I0630 02:07:47.004179 27297 caffe.cpp:312] Batch 165, accuracy/top1 = 0.9
I0630 02:07:47.004195 27297 caffe.cpp:312] Batch 165, accuracy/top5 = 1
I0630 02:07:47.004199 27297 caffe.cpp:312] Batch 165, loss = 0.12
I0630 02:07:47.012322 27297 caffe.cpp:312] Batch 166, accuracy/top1 = 0.94
I0630 02:07:47.012331 27297 caffe.cpp:312] Batch 166, accuracy/top5 = 1
I0630 02:07:47.012334 27297 caffe.cpp:312] Batch 166, loss = 0.1
I0630 02:07:47.020495 27297 caffe.cpp:312] Batch 167, accuracy/top1 = 0.88
I0630 02:07:47.020503 27297 caffe.cpp:312] Batch 167, accuracy/top5 = 1
I0630 02:07:47.020506 27297 caffe.cpp:312] Batch 167, loss = 0.14
I0630 02:07:47.028671 27297 caffe.cpp:312] Batch 168, accuracy/top1 = 0.9
I0630 02:07:47.028681 27297 caffe.cpp:312] Batch 168, accuracy/top5 = 1
I0630 02:07:47.028684 27297 caffe.cpp:312] Batch 168, loss = 0.22
I0630 02:07:47.036855 27297 caffe.cpp:312] Batch 169, accuracy/top1 = 0.82
I0630 02:07:47.036873 27297 caffe.cpp:312] Batch 169, accuracy/top5 = 0.98
I0630 02:07:47.036877 27297 caffe.cpp:312] Batch 169, loss = 0.48
I0630 02:07:47.045079 27297 caffe.cpp:312] Batch 170, accuracy/top1 = 0.92
I0630 02:07:47.045086 27297 caffe.cpp:312] Batch 170, accuracy/top5 = 0.98
I0630 02:07:47.045090 27297 caffe.cpp:312] Batch 170, loss = 0.24
I0630 02:07:47.053200 27297 caffe.cpp:312] Batch 171, accuracy/top1 = 0.88
I0630 02:07:47.053208 27297 caffe.cpp:312] Batch 171, accuracy/top5 = 1
I0630 02:07:47.053212 27297 caffe.cpp:312] Batch 171, loss = 0.46
I0630 02:07:47.061355 27297 caffe.cpp:312] Batch 172, accuracy/top1 = 0.88
I0630 02:07:47.061364 27297 caffe.cpp:312] Batch 172, accuracy/top5 = 1
I0630 02:07:47.061368 27297 caffe.cpp:312] Batch 172, loss = 0.16
I0630 02:07:47.069562 27297 caffe.cpp:312] Batch 173, accuracy/top1 = 0.94
I0630 02:07:47.069572 27297 caffe.cpp:312] Batch 173, accuracy/top5 = 1
I0630 02:07:47.069576 27297 caffe.cpp:312] Batch 173, loss = 0.02
I0630 02:07:47.077726 27297 caffe.cpp:312] Batch 174, accuracy/top1 = 0.86
I0630 02:07:47.077734 27297 caffe.cpp:312] Batch 174, accuracy/top5 = 1
I0630 02:07:47.077739 27297 caffe.cpp:312] Batch 174, loss = 0.44
I0630 02:07:47.085880 27297 caffe.cpp:312] Batch 175, accuracy/top1 = 0.92
I0630 02:07:47.085887 27297 caffe.cpp:312] Batch 175, accuracy/top5 = 1
I0630 02:07:47.085891 27297 caffe.cpp:312] Batch 175, loss = 0.12
I0630 02:07:47.094075 27297 caffe.cpp:312] Batch 176, accuracy/top1 = 0.9
I0630 02:07:47.094087 27297 caffe.cpp:312] Batch 176, accuracy/top5 = 1
I0630 02:07:47.094090 27297 caffe.cpp:312] Batch 176, loss = 0.36
I0630 02:07:47.102241 27297 caffe.cpp:312] Batch 177, accuracy/top1 = 0.96
I0630 02:07:47.102248 27297 caffe.cpp:312] Batch 177, accuracy/top5 = 1
I0630 02:07:47.102252 27297 caffe.cpp:312] Batch 177, loss = 0.06
I0630 02:07:47.110467 27297 caffe.cpp:312] Batch 178, accuracy/top1 = 0.86
I0630 02:07:47.110476 27297 caffe.cpp:312] Batch 178, accuracy/top5 = 1
I0630 02:07:47.110479 27297 caffe.cpp:312] Batch 178, loss = 0.4
I0630 02:07:47.118620 27297 caffe.cpp:312] Batch 179, accuracy/top1 = 0.9
I0630 02:07:47.118634 27297 caffe.cpp:312] Batch 179, accuracy/top5 = 1
I0630 02:07:47.118638 27297 caffe.cpp:312] Batch 179, loss = 0.42
I0630 02:07:47.126811 27297 caffe.cpp:312] Batch 180, accuracy/top1 = 0.9
I0630 02:07:47.126829 27297 caffe.cpp:312] Batch 180, accuracy/top5 = 1
I0630 02:07:47.126837 27297 caffe.cpp:312] Batch 180, loss = 0.16
I0630 02:07:47.135051 27297 caffe.cpp:312] Batch 181, accuracy/top1 = 0.9
I0630 02:07:47.135061 27297 caffe.cpp:312] Batch 181, accuracy/top5 = 1
I0630 02:07:47.135064 27297 caffe.cpp:312] Batch 181, loss = 0.16
I0630 02:07:47.143234 27297 caffe.cpp:312] Batch 182, accuracy/top1 = 0.92
I0630 02:07:47.143241 27297 caffe.cpp:312] Batch 182, accuracy/top5 = 0.98
I0630 02:07:47.143245 27297 caffe.cpp:312] Batch 182, loss = 0.18
I0630 02:07:47.151397 27297 caffe.cpp:312] Batch 183, accuracy/top1 = 0.94
I0630 02:07:47.151404 27297 caffe.cpp:312] Batch 183, accuracy/top5 = 1
I0630 02:07:47.151408 27297 caffe.cpp:312] Batch 183, loss = 0.08
I0630 02:07:47.159597 27297 caffe.cpp:312] Batch 184, accuracy/top1 = 0.88
I0630 02:07:47.159615 27297 caffe.cpp:312] Batch 184, accuracy/top5 = 1
I0630 02:07:47.159617 27297 caffe.cpp:312] Batch 184, loss = 0.46
I0630 02:07:47.167810 27297 caffe.cpp:312] Batch 185, accuracy/top1 = 0.88
I0630 02:07:47.167819 27297 caffe.cpp:312] Batch 185, accuracy/top5 = 1
I0630 02:07:47.167824 27297 caffe.cpp:312] Batch 185, loss = 0.28
I0630 02:07:47.175920 27297 caffe.cpp:312] Batch 186, accuracy/top1 = 0.96
I0630 02:07:47.175927 27297 caffe.cpp:312] Batch 186, accuracy/top5 = 1
I0630 02:07:47.175931 27297 caffe.cpp:312] Batch 186, loss = 0.1
I0630 02:07:47.184053 27297 caffe.cpp:312] Batch 187, accuracy/top1 = 0.84
I0630 02:07:47.184062 27297 caffe.cpp:312] Batch 187, accuracy/top5 = 0.98
I0630 02:07:47.184067 27297 caffe.cpp:312] Batch 187, loss = 0.34
I0630 02:07:47.192291 27297 caffe.cpp:312] Batch 188, accuracy/top1 = 0.86
I0630 02:07:47.192302 27297 caffe.cpp:312] Batch 188, accuracy/top5 = 1
I0630 02:07:47.192306 27297 caffe.cpp:312] Batch 188, loss = 0.16
I0630 02:07:47.200448 27297 caffe.cpp:312] Batch 189, accuracy/top1 = 0.94
I0630 02:07:47.200455 27297 caffe.cpp:312] Batch 189, accuracy/top5 = 0.98
I0630 02:07:47.200459 27297 caffe.cpp:312] Batch 189, loss = 0.2
I0630 02:07:47.208534 27297 caffe.cpp:312] Batch 190, accuracy/top1 = 0.88
I0630 02:07:47.208542 27297 caffe.cpp:312] Batch 190, accuracy/top5 = 1
I0630 02:07:47.208546 27297 caffe.cpp:312] Batch 190, loss = 0.26
I0630 02:07:47.216706 27297 caffe.cpp:312] Batch 191, accuracy/top1 = 0.92
I0630 02:07:47.216718 27297 caffe.cpp:312] Batch 191, accuracy/top5 = 1
I0630 02:07:47.216722 27297 caffe.cpp:312] Batch 191, loss = 0.14
I0630 02:07:47.224875 27297 caffe.cpp:312] Batch 192, accuracy/top1 = 0.9
I0630 02:07:47.224884 27297 caffe.cpp:312] Batch 192, accuracy/top5 = 1
I0630 02:07:47.224887 27297 caffe.cpp:312] Batch 192, loss = 0.26
I0630 02:07:47.233038 27297 caffe.cpp:312] Batch 193, accuracy/top1 = 0.96
I0630 02:07:47.233047 27297 caffe.cpp:312] Batch 193, accuracy/top5 = 1
I0630 02:07:47.233050 27297 caffe.cpp:312] Batch 193, loss = 0.08
I0630 02:07:47.241236 27297 caffe.cpp:312] Batch 194, accuracy/top1 = 0.88
I0630 02:07:47.241245 27297 caffe.cpp:312] Batch 194, accuracy/top5 = 0.98
I0630 02:07:47.241248 27297 caffe.cpp:312] Batch 194, loss = 0.38
I0630 02:07:47.249413 27297 caffe.cpp:312] Batch 195, accuracy/top1 = 0.94
I0630 02:07:47.249428 27297 caffe.cpp:312] Batch 195, accuracy/top5 = 1
I0630 02:07:47.249431 27297 caffe.cpp:312] Batch 195, loss = 0.18
I0630 02:07:47.257593 27297 caffe.cpp:312] Batch 196, accuracy/top1 = 0.86
I0630 02:07:47.257601 27297 caffe.cpp:312] Batch 196, accuracy/top5 = 1
I0630 02:07:47.257606 27297 caffe.cpp:312] Batch 196, loss = 0.66
I0630 02:07:47.265806 27297 caffe.cpp:312] Batch 197, accuracy/top1 = 0.86
I0630 02:07:47.265815 27297 caffe.cpp:312] Batch 197, accuracy/top5 = 0.98
I0630 02:07:47.265818 27297 caffe.cpp:312] Batch 197, loss = 0.22
I0630 02:07:47.273963 27297 caffe.cpp:312] Batch 198, accuracy/top1 = 0.92
I0630 02:07:47.273972 27297 caffe.cpp:312] Batch 198, accuracy/top5 = 0.98
I0630 02:07:47.273985 27297 caffe.cpp:312] Batch 198, loss = 0.14
I0630 02:07:47.282193 27297 caffe.cpp:312] Batch 199, accuracy/top1 = 0.94
I0630 02:07:47.282209 27297 caffe.cpp:312] Batch 199, accuracy/top5 = 1
I0630 02:07:47.282213 27297 caffe.cpp:312] Batch 199, loss = 0.14
I0630 02:07:47.282217 27297 caffe.cpp:317] Loss: 0.2257
I0630 02:07:47.282227 27297 caffe.cpp:329] accuracy/top1 = 0.9068
I0630 02:07:47.282233 27297 caffe.cpp:329] accuracy/top5 = 0.9959
I0630 02:07:47.282241 27297 caffe.cpp:329] loss = 0.2257 (* 1 = 0.2257 loss)
