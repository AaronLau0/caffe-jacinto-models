I0701 14:51:50.347087  1052 caffe.cpp:209] Using GPUs 0, 1, 2
I0701 14:51:50.347551  1052 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0701 14:51:50.347882  1052 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0701 14:51:50.348208  1052 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0701 14:51:50.731715  1052 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: true
iter_size: 1
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.02
sparsity_step_iter: 1000
sparsity_start_iter: 4000
sparsity_start_factor: 0
I0701 14:51:50.731806  1052 solver.cpp:82] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/train.prototxt
I0701 14:51:50.732460  1052 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0701 14:51:50.732466  1052 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0701 14:51:50.732677  1052 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 21
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0701 14:51:50.732796  1052 layer_factory.hpp:77] Creating layer data
I0701 14:51:50.732899  1052 net.cpp:98] Creating Layer data
I0701 14:51:50.732908  1052 net.cpp:413] data -> data
I0701 14:51:50.732928  1052 net.cpp:413] data -> label
I0701 14:51:50.733999  1083 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0701 14:51:50.747509  1052 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0701 14:51:50.747568  1052 data_layer.cpp:83] output data size: 21,3,32,32
I0701 14:51:50.749172  1052 net.cpp:148] Setting up data
I0701 14:51:50.749184  1052 net.cpp:155] Top shape: 21 3 32 32 (64512)
I0701 14:51:50.749187  1052 net.cpp:155] Top shape: 21 (21)
I0701 14:51:50.749189  1052 net.cpp:163] Memory required for data: 258132
I0701 14:51:50.749197  1052 layer_factory.hpp:77] Creating layer data/bias
I0701 14:51:50.749207  1052 net.cpp:98] Creating Layer data/bias
I0701 14:51:50.749210  1052 net.cpp:439] data/bias <- data
I0701 14:51:50.749217  1052 net.cpp:413] data/bias -> data/bias
I0701 14:51:50.750185  1052 net.cpp:148] Setting up data/bias
I0701 14:51:50.750195  1052 net.cpp:155] Top shape: 21 3 32 32 (64512)
I0701 14:51:50.750198  1052 net.cpp:163] Memory required for data: 516180
I0701 14:51:50.750206  1052 layer_factory.hpp:77] Creating layer conv1a
I0701 14:51:50.750214  1052 net.cpp:98] Creating Layer conv1a
I0701 14:51:50.750217  1052 net.cpp:439] conv1a <- data/bias
I0701 14:51:50.750221  1052 net.cpp:413] conv1a -> conv1a
I0701 14:51:50.750473  1085 blocking_queue.cpp:50] Waiting for data
I0701 14:51:50.751502  1052 net.cpp:148] Setting up conv1a
I0701 14:51:50.751512  1052 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0701 14:51:50.751514  1052 net.cpp:163] Memory required for data: 3268692
I0701 14:51:50.751518  1052 layer_factory.hpp:77] Creating layer conv1a/bn
I0701 14:51:50.751525  1052 net.cpp:98] Creating Layer conv1a/bn
I0701 14:51:50.751528  1052 net.cpp:439] conv1a/bn <- conv1a
I0701 14:51:50.751530  1052 net.cpp:413] conv1a/bn -> conv1a/bn
I0701 14:51:50.752204  1052 net.cpp:148] Setting up conv1a/bn
I0701 14:51:50.752210  1052 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0701 14:51:50.752213  1052 net.cpp:163] Memory required for data: 6021204
I0701 14:51:50.752218  1052 layer_factory.hpp:77] Creating layer conv1a/relu
I0701 14:51:50.752223  1052 net.cpp:98] Creating Layer conv1a/relu
I0701 14:51:50.752225  1052 net.cpp:439] conv1a/relu <- conv1a/bn
I0701 14:51:50.752228  1052 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0701 14:51:50.752236  1052 net.cpp:148] Setting up conv1a/relu
I0701 14:51:50.752239  1052 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0701 14:51:50.752249  1052 net.cpp:163] Memory required for data: 8773716
I0701 14:51:50.752252  1052 layer_factory.hpp:77] Creating layer conv1b
I0701 14:51:50.752256  1052 net.cpp:98] Creating Layer conv1b
I0701 14:51:50.752259  1052 net.cpp:439] conv1b <- conv1a/bn
I0701 14:51:50.752261  1052 net.cpp:413] conv1b -> conv1b
I0701 14:51:50.752581  1052 net.cpp:148] Setting up conv1b
I0701 14:51:50.752586  1052 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0701 14:51:50.752588  1052 net.cpp:163] Memory required for data: 11526228
I0701 14:51:50.752593  1052 layer_factory.hpp:77] Creating layer conv1b/bn
I0701 14:51:50.752596  1052 net.cpp:98] Creating Layer conv1b/bn
I0701 14:51:50.752599  1052 net.cpp:439] conv1b/bn <- conv1b
I0701 14:51:50.752600  1052 net.cpp:413] conv1b/bn -> conv1b/bn
I0701 14:51:50.753248  1052 net.cpp:148] Setting up conv1b/bn
I0701 14:51:50.753253  1052 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0701 14:51:50.753255  1052 net.cpp:163] Memory required for data: 14278740
I0701 14:51:50.753260  1052 layer_factory.hpp:77] Creating layer conv1b/relu
I0701 14:51:50.753263  1052 net.cpp:98] Creating Layer conv1b/relu
I0701 14:51:50.753265  1052 net.cpp:439] conv1b/relu <- conv1b/bn
I0701 14:51:50.753268  1052 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0701 14:51:50.753271  1052 net.cpp:148] Setting up conv1b/relu
I0701 14:51:50.753274  1052 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0701 14:51:50.753276  1052 net.cpp:163] Memory required for data: 17031252
I0701 14:51:50.753278  1052 layer_factory.hpp:77] Creating layer pool1
I0701 14:51:50.753283  1052 net.cpp:98] Creating Layer pool1
I0701 14:51:50.753285  1052 net.cpp:439] pool1 <- conv1b/bn
I0701 14:51:50.753288  1052 net.cpp:413] pool1 -> pool1
I0701 14:51:50.753329  1052 net.cpp:148] Setting up pool1
I0701 14:51:50.753334  1052 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0701 14:51:50.753334  1052 net.cpp:163] Memory required for data: 19783764
I0701 14:51:50.753336  1052 layer_factory.hpp:77] Creating layer res2a_branch2a
I0701 14:51:50.753340  1052 net.cpp:98] Creating Layer res2a_branch2a
I0701 14:51:50.753342  1052 net.cpp:439] res2a_branch2a <- pool1
I0701 14:51:50.753345  1052 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0701 14:51:50.753983  1052 net.cpp:148] Setting up res2a_branch2a
I0701 14:51:50.753988  1052 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0701 14:51:50.753990  1052 net.cpp:163] Memory required for data: 25288788
I0701 14:51:50.753995  1052 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0701 14:51:50.753998  1052 net.cpp:98] Creating Layer res2a_branch2a/bn
I0701 14:51:50.754000  1052 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0701 14:51:50.754004  1052 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0701 14:51:50.754672  1052 net.cpp:148] Setting up res2a_branch2a/bn
I0701 14:51:50.754679  1052 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0701 14:51:50.754681  1052 net.cpp:163] Memory required for data: 30793812
I0701 14:51:50.754686  1052 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0701 14:51:50.754690  1052 net.cpp:98] Creating Layer res2a_branch2a/relu
I0701 14:51:50.754693  1052 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0701 14:51:50.754695  1052 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0701 14:51:50.754700  1052 net.cpp:148] Setting up res2a_branch2a/relu
I0701 14:51:50.754703  1052 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0701 14:51:50.754705  1052 net.cpp:163] Memory required for data: 36298836
I0701 14:51:50.754707  1052 layer_factory.hpp:77] Creating layer res2a_branch2b
I0701 14:51:50.754712  1052 net.cpp:98] Creating Layer res2a_branch2b
I0701 14:51:50.754715  1052 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0701 14:51:50.754721  1052 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0701 14:51:50.756182  1052 net.cpp:148] Setting up res2a_branch2b
I0701 14:51:50.756202  1052 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0701 14:51:50.756204  1052 net.cpp:163] Memory required for data: 41803860
I0701 14:51:50.756223  1052 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0701 14:51:50.756232  1052 net.cpp:98] Creating Layer res2a_branch2b/bn
I0701 14:51:50.756235  1052 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0701 14:51:50.756240  1052 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0701 14:51:50.756942  1052 net.cpp:148] Setting up res2a_branch2b/bn
I0701 14:51:50.756950  1052 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0701 14:51:50.756953  1052 net.cpp:163] Memory required for data: 47308884
I0701 14:51:50.756958  1052 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0701 14:51:50.756961  1052 net.cpp:98] Creating Layer res2a_branch2b/relu
I0701 14:51:50.756963  1052 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0701 14:51:50.756966  1052 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0701 14:51:50.756973  1052 net.cpp:148] Setting up res2a_branch2b/relu
I0701 14:51:50.756976  1052 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0701 14:51:50.756978  1052 net.cpp:163] Memory required for data: 52813908
I0701 14:51:50.756980  1052 layer_factory.hpp:77] Creating layer pool2
I0701 14:51:50.756984  1052 net.cpp:98] Creating Layer pool2
I0701 14:51:50.756986  1052 net.cpp:439] pool2 <- res2a_branch2b/bn
I0701 14:51:50.756989  1052 net.cpp:413] pool2 -> pool2
I0701 14:51:50.757030  1052 net.cpp:148] Setting up pool2
I0701 14:51:50.757035  1052 net.cpp:155] Top shape: 21 64 16 16 (344064)
I0701 14:51:50.757037  1052 net.cpp:163] Memory required for data: 54190164
I0701 14:51:50.757040  1052 layer_factory.hpp:77] Creating layer res3a_branch2a
I0701 14:51:50.757050  1052 net.cpp:98] Creating Layer res3a_branch2a
I0701 14:51:50.757052  1052 net.cpp:439] res3a_branch2a <- pool2
I0701 14:51:50.757055  1052 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0701 14:51:50.759904  1052 net.cpp:148] Setting up res3a_branch2a
I0701 14:51:50.759914  1052 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0701 14:51:50.759917  1052 net.cpp:163] Memory required for data: 56942676
I0701 14:51:50.759922  1052 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0701 14:51:50.759927  1052 net.cpp:98] Creating Layer res3a_branch2a/bn
I0701 14:51:50.759929  1052 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0701 14:51:50.759933  1052 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0701 14:51:50.760543  1052 net.cpp:148] Setting up res3a_branch2a/bn
I0701 14:51:50.760550  1052 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0701 14:51:50.760551  1052 net.cpp:163] Memory required for data: 59695188
I0701 14:51:50.760561  1052 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0701 14:51:50.760565  1052 net.cpp:98] Creating Layer res3a_branch2a/relu
I0701 14:51:50.760566  1052 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0701 14:51:50.760568  1052 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0701 14:51:50.760572  1052 net.cpp:148] Setting up res3a_branch2a/relu
I0701 14:51:50.760574  1052 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0701 14:51:50.760576  1052 net.cpp:163] Memory required for data: 62447700
I0701 14:51:50.760578  1052 layer_factory.hpp:77] Creating layer res3a_branch2b
I0701 14:51:50.760582  1052 net.cpp:98] Creating Layer res3a_branch2b
I0701 14:51:50.760584  1052 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0701 14:51:50.760587  1052 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0701 14:51:50.761595  1052 net.cpp:148] Setting up res3a_branch2b
I0701 14:51:50.761600  1052 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0701 14:51:50.761602  1052 net.cpp:163] Memory required for data: 65200212
I0701 14:51:50.761605  1052 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0701 14:51:50.761610  1052 net.cpp:98] Creating Layer res3a_branch2b/bn
I0701 14:51:50.761612  1052 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0701 14:51:50.761615  1052 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0701 14:51:50.762214  1052 net.cpp:148] Setting up res3a_branch2b/bn
I0701 14:51:50.762220  1052 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0701 14:51:50.762229  1052 net.cpp:163] Memory required for data: 67952724
I0701 14:51:50.762234  1052 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0701 14:51:50.762238  1052 net.cpp:98] Creating Layer res3a_branch2b/relu
I0701 14:51:50.762239  1052 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0701 14:51:50.762241  1052 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0701 14:51:50.762245  1052 net.cpp:148] Setting up res3a_branch2b/relu
I0701 14:51:50.762248  1052 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0701 14:51:50.762249  1052 net.cpp:163] Memory required for data: 70705236
I0701 14:51:50.762251  1052 layer_factory.hpp:77] Creating layer pool3
I0701 14:51:50.762254  1052 net.cpp:98] Creating Layer pool3
I0701 14:51:50.762257  1052 net.cpp:439] pool3 <- res3a_branch2b/bn
I0701 14:51:50.762260  1052 net.cpp:413] pool3 -> pool3
I0701 14:51:50.762298  1052 net.cpp:148] Setting up pool3
I0701 14:51:50.762303  1052 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0701 14:51:50.762305  1052 net.cpp:163] Memory required for data: 73457748
I0701 14:51:50.762310  1052 layer_factory.hpp:77] Creating layer res4a_branch2a
I0701 14:51:50.762315  1052 net.cpp:98] Creating Layer res4a_branch2a
I0701 14:51:50.762320  1052 net.cpp:439] res4a_branch2a <- pool3
I0701 14:51:50.762323  1052 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0701 14:51:50.768401  1052 net.cpp:148] Setting up res4a_branch2a
I0701 14:51:50.768409  1052 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0701 14:51:50.768410  1052 net.cpp:163] Memory required for data: 78962772
I0701 14:51:50.768414  1052 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0701 14:51:50.768417  1052 net.cpp:98] Creating Layer res4a_branch2a/bn
I0701 14:51:50.768419  1052 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0701 14:51:50.768422  1052 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0701 14:51:50.769026  1052 net.cpp:148] Setting up res4a_branch2a/bn
I0701 14:51:50.769032  1052 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0701 14:51:50.769033  1052 net.cpp:163] Memory required for data: 84467796
I0701 14:51:50.769037  1052 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0701 14:51:50.769040  1052 net.cpp:98] Creating Layer res4a_branch2a/relu
I0701 14:51:50.769043  1052 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0701 14:51:50.769045  1052 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0701 14:51:50.769048  1052 net.cpp:148] Setting up res4a_branch2a/relu
I0701 14:51:50.769050  1052 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0701 14:51:50.769052  1052 net.cpp:163] Memory required for data: 89972820
I0701 14:51:50.769054  1052 layer_factory.hpp:77] Creating layer res4a_branch2b
I0701 14:51:50.769057  1052 net.cpp:98] Creating Layer res4a_branch2b
I0701 14:51:50.769059  1052 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0701 14:51:50.769062  1052 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0701 14:51:50.772238  1052 net.cpp:148] Setting up res4a_branch2b
I0701 14:51:50.772244  1052 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0701 14:51:50.772246  1052 net.cpp:163] Memory required for data: 95477844
I0701 14:51:50.772249  1052 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0701 14:51:50.772253  1052 net.cpp:98] Creating Layer res4a_branch2b/bn
I0701 14:51:50.772256  1052 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0701 14:51:50.772260  1052 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0701 14:51:50.772850  1052 net.cpp:148] Setting up res4a_branch2b/bn
I0701 14:51:50.772855  1052 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0701 14:51:50.772857  1052 net.cpp:163] Memory required for data: 100982868
I0701 14:51:50.772861  1052 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0701 14:51:50.772864  1052 net.cpp:98] Creating Layer res4a_branch2b/relu
I0701 14:51:50.772866  1052 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0701 14:51:50.772868  1052 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0701 14:51:50.772879  1052 net.cpp:148] Setting up res4a_branch2b/relu
I0701 14:51:50.772882  1052 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0701 14:51:50.772883  1052 net.cpp:163] Memory required for data: 106487892
I0701 14:51:50.772886  1052 layer_factory.hpp:77] Creating layer pool4
I0701 14:51:50.772888  1052 net.cpp:98] Creating Layer pool4
I0701 14:51:50.772891  1052 net.cpp:439] pool4 <- res4a_branch2b/bn
I0701 14:51:50.772894  1052 net.cpp:413] pool4 -> pool4
I0701 14:51:50.772928  1052 net.cpp:148] Setting up pool4
I0701 14:51:50.772933  1052 net.cpp:155] Top shape: 21 256 8 8 (344064)
I0701 14:51:50.772934  1052 net.cpp:163] Memory required for data: 107864148
I0701 14:51:50.772936  1052 layer_factory.hpp:77] Creating layer res5a_branch2a
I0701 14:51:50.772939  1052 net.cpp:98] Creating Layer res5a_branch2a
I0701 14:51:50.772943  1052 net.cpp:439] res5a_branch2a <- pool4
I0701 14:51:50.772944  1052 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0701 14:51:50.801746  1052 net.cpp:148] Setting up res5a_branch2a
I0701 14:51:50.801767  1052 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0701 14:51:50.801770  1052 net.cpp:163] Memory required for data: 110616660
I0701 14:51:50.801776  1052 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0701 14:51:50.801784  1052 net.cpp:98] Creating Layer res5a_branch2a/bn
I0701 14:51:50.801786  1052 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0701 14:51:50.801790  1052 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0701 14:51:50.802461  1052 net.cpp:148] Setting up res5a_branch2a/bn
I0701 14:51:50.802467  1052 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0701 14:51:50.802469  1052 net.cpp:163] Memory required for data: 113369172
I0701 14:51:50.802474  1052 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0701 14:51:50.802479  1052 net.cpp:98] Creating Layer res5a_branch2a/relu
I0701 14:51:50.802481  1052 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0701 14:51:50.802484  1052 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0701 14:51:50.802489  1052 net.cpp:148] Setting up res5a_branch2a/relu
I0701 14:51:50.802490  1052 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0701 14:51:50.802492  1052 net.cpp:163] Memory required for data: 116121684
I0701 14:51:50.802495  1052 layer_factory.hpp:77] Creating layer res5a_branch2b
I0701 14:51:50.802502  1052 net.cpp:98] Creating Layer res5a_branch2b
I0701 14:51:50.802505  1052 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0701 14:51:50.802508  1052 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0701 14:51:50.815462  1052 net.cpp:148] Setting up res5a_branch2b
I0701 14:51:50.815482  1052 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0701 14:51:50.815485  1052 net.cpp:163] Memory required for data: 118874196
I0701 14:51:50.815495  1052 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0701 14:51:50.815501  1052 net.cpp:98] Creating Layer res5a_branch2b/bn
I0701 14:51:50.815505  1052 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0701 14:51:50.815508  1052 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0701 14:51:50.816200  1052 net.cpp:148] Setting up res5a_branch2b/bn
I0701 14:51:50.816207  1052 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0701 14:51:50.816210  1052 net.cpp:163] Memory required for data: 121626708
I0701 14:51:50.816215  1052 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0701 14:51:50.816218  1052 net.cpp:98] Creating Layer res5a_branch2b/relu
I0701 14:51:50.816220  1052 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0701 14:51:50.816222  1052 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0701 14:51:50.816227  1052 net.cpp:148] Setting up res5a_branch2b/relu
I0701 14:51:50.816229  1052 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0701 14:51:50.816231  1052 net.cpp:163] Memory required for data: 124379220
I0701 14:51:50.816232  1052 layer_factory.hpp:77] Creating layer pool5
I0701 14:51:50.816238  1052 net.cpp:98] Creating Layer pool5
I0701 14:51:50.816239  1052 net.cpp:439] pool5 <- res5a_branch2b/bn
I0701 14:51:50.816252  1052 net.cpp:413] pool5 -> pool5
I0701 14:51:50.816283  1052 net.cpp:148] Setting up pool5
I0701 14:51:50.816287  1052 net.cpp:155] Top shape: 21 512 1 1 (10752)
I0701 14:51:50.816289  1052 net.cpp:163] Memory required for data: 124422228
I0701 14:51:50.816292  1052 layer_factory.hpp:77] Creating layer fc10
I0701 14:51:50.816295  1052 net.cpp:98] Creating Layer fc10
I0701 14:51:50.816298  1052 net.cpp:439] fc10 <- pool5
I0701 14:51:50.816300  1052 net.cpp:413] fc10 -> fc10
I0701 14:51:50.816535  1052 net.cpp:148] Setting up fc10
I0701 14:51:50.816540  1052 net.cpp:155] Top shape: 21 10 (210)
I0701 14:51:50.816542  1052 net.cpp:163] Memory required for data: 124423068
I0701 14:51:50.816545  1052 layer_factory.hpp:77] Creating layer loss
I0701 14:51:50.816550  1052 net.cpp:98] Creating Layer loss
I0701 14:51:50.816551  1052 net.cpp:439] loss <- fc10
I0701 14:51:50.816553  1052 net.cpp:439] loss <- label
I0701 14:51:50.816557  1052 net.cpp:413] loss -> loss
I0701 14:51:50.816565  1052 layer_factory.hpp:77] Creating layer loss
I0701 14:51:50.816684  1052 net.cpp:148] Setting up loss
I0701 14:51:50.816689  1052 net.cpp:155] Top shape: (1)
I0701 14:51:50.816694  1052 net.cpp:158]     with loss weight 1
I0701 14:51:50.816702  1052 net.cpp:163] Memory required for data: 124423072
I0701 14:51:50.816705  1052 net.cpp:224] loss needs backward computation.
I0701 14:51:50.816707  1052 net.cpp:224] fc10 needs backward computation.
I0701 14:51:50.816710  1052 net.cpp:224] pool5 needs backward computation.
I0701 14:51:50.816712  1052 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0701 14:51:50.816715  1052 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0701 14:51:50.816716  1052 net.cpp:224] res5a_branch2b needs backward computation.
I0701 14:51:50.816718  1052 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0701 14:51:50.816722  1052 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0701 14:51:50.816726  1052 net.cpp:224] res5a_branch2a needs backward computation.
I0701 14:51:50.816728  1052 net.cpp:224] pool4 needs backward computation.
I0701 14:51:50.816730  1052 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0701 14:51:50.816732  1052 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0701 14:51:50.816735  1052 net.cpp:224] res4a_branch2b needs backward computation.
I0701 14:51:50.816736  1052 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0701 14:51:50.816740  1052 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0701 14:51:50.816741  1052 net.cpp:224] res4a_branch2a needs backward computation.
I0701 14:51:50.816743  1052 net.cpp:224] pool3 needs backward computation.
I0701 14:51:50.816746  1052 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0701 14:51:50.816747  1052 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0701 14:51:50.816750  1052 net.cpp:224] res3a_branch2b needs backward computation.
I0701 14:51:50.816752  1052 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0701 14:51:50.816754  1052 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0701 14:51:50.816756  1052 net.cpp:224] res3a_branch2a needs backward computation.
I0701 14:51:50.816758  1052 net.cpp:224] pool2 needs backward computation.
I0701 14:51:50.816761  1052 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0701 14:51:50.816762  1052 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0701 14:51:50.816764  1052 net.cpp:224] res2a_branch2b needs backward computation.
I0701 14:51:50.816766  1052 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0701 14:51:50.816768  1052 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0701 14:51:50.816771  1052 net.cpp:224] res2a_branch2a needs backward computation.
I0701 14:51:50.816772  1052 net.cpp:224] pool1 needs backward computation.
I0701 14:51:50.816774  1052 net.cpp:224] conv1b/relu needs backward computation.
I0701 14:51:50.816776  1052 net.cpp:224] conv1b/bn needs backward computation.
I0701 14:51:50.816784  1052 net.cpp:224] conv1b needs backward computation.
I0701 14:51:50.816787  1052 net.cpp:224] conv1a/relu needs backward computation.
I0701 14:51:50.816788  1052 net.cpp:224] conv1a/bn needs backward computation.
I0701 14:51:50.816792  1052 net.cpp:224] conv1a needs backward computation.
I0701 14:51:50.816793  1052 net.cpp:226] data/bias does not need backward computation.
I0701 14:51:50.816795  1052 net.cpp:226] data does not need backward computation.
I0701 14:51:50.816798  1052 net.cpp:268] This network produces output loss
I0701 14:51:50.816815  1052 net.cpp:288] Network initialization done.
I0701 14:51:50.817245  1052 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/test.prototxt
I0701 14:51:50.817436  1052 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0701 14:51:50.817544  1052 layer_factory.hpp:77] Creating layer data
I0701 14:51:50.817617  1052 net.cpp:98] Creating Layer data
I0701 14:51:50.817625  1052 net.cpp:413] data -> data
I0701 14:51:50.817632  1052 net.cpp:413] data -> label
I0701 14:51:50.819277  1095 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0701 14:51:50.819423  1052 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0701 14:51:50.819483  1052 data_layer.cpp:83] output data size: 50,3,32,32
I0701 14:51:50.821696  1052 net.cpp:148] Setting up data
I0701 14:51:50.821709  1052 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0701 14:51:50.821712  1052 net.cpp:155] Top shape: 50 (50)
I0701 14:51:50.821714  1052 net.cpp:163] Memory required for data: 614600
I0701 14:51:50.821718  1052 layer_factory.hpp:77] Creating layer label_data_1_split
I0701 14:51:50.821729  1052 net.cpp:98] Creating Layer label_data_1_split
I0701 14:51:50.821733  1052 net.cpp:439] label_data_1_split <- label
I0701 14:51:50.821738  1052 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0701 14:51:50.821754  1052 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0701 14:51:50.821760  1052 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0701 14:51:50.821868  1052 net.cpp:148] Setting up label_data_1_split
I0701 14:51:50.821874  1052 net.cpp:155] Top shape: 50 (50)
I0701 14:51:50.821878  1052 net.cpp:155] Top shape: 50 (50)
I0701 14:51:50.821883  1052 net.cpp:155] Top shape: 50 (50)
I0701 14:51:50.821887  1052 net.cpp:163] Memory required for data: 615200
I0701 14:51:50.821890  1052 layer_factory.hpp:77] Creating layer data/bias
I0701 14:51:50.821897  1052 net.cpp:98] Creating Layer data/bias
I0701 14:51:50.821900  1052 net.cpp:439] data/bias <- data
I0701 14:51:50.821905  1052 net.cpp:413] data/bias -> data/bias
I0701 14:51:50.822015  1052 net.cpp:148] Setting up data/bias
I0701 14:51:50.822019  1052 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0701 14:51:50.822023  1052 net.cpp:163] Memory required for data: 1229600
I0701 14:51:50.822031  1052 layer_factory.hpp:77] Creating layer conv1a
I0701 14:51:50.822037  1052 net.cpp:98] Creating Layer conv1a
I0701 14:51:50.822041  1052 net.cpp:439] conv1a <- data/bias
I0701 14:51:50.822046  1052 net.cpp:413] conv1a -> conv1a
I0701 14:51:50.822420  1052 net.cpp:148] Setting up conv1a
I0701 14:51:50.822428  1052 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0701 14:51:50.822432  1052 net.cpp:163] Memory required for data: 7783200
I0701 14:51:50.822439  1052 layer_factory.hpp:77] Creating layer conv1a/bn
I0701 14:51:50.822446  1052 net.cpp:98] Creating Layer conv1a/bn
I0701 14:51:50.822449  1052 net.cpp:439] conv1a/bn <- conv1a
I0701 14:51:50.822455  1052 net.cpp:413] conv1a/bn -> conv1a/bn
I0701 14:51:50.823333  1052 net.cpp:148] Setting up conv1a/bn
I0701 14:51:50.823344  1052 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0701 14:51:50.823348  1052 net.cpp:163] Memory required for data: 14336800
I0701 14:51:50.823370  1052 layer_factory.hpp:77] Creating layer conv1a/relu
I0701 14:51:50.823377  1052 net.cpp:98] Creating Layer conv1a/relu
I0701 14:51:50.823381  1052 net.cpp:439] conv1a/relu <- conv1a/bn
I0701 14:51:50.823386  1052 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0701 14:51:50.823393  1052 net.cpp:148] Setting up conv1a/relu
I0701 14:51:50.823397  1052 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0701 14:51:50.823401  1052 net.cpp:163] Memory required for data: 20890400
I0701 14:51:50.823405  1052 layer_factory.hpp:77] Creating layer conv1b
I0701 14:51:50.823413  1052 net.cpp:98] Creating Layer conv1b
I0701 14:51:50.823416  1052 net.cpp:439] conv1b <- conv1a/bn
I0701 14:51:50.823421  1052 net.cpp:413] conv1b -> conv1b
I0701 14:51:50.823798  1052 net.cpp:148] Setting up conv1b
I0701 14:51:50.823807  1052 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0701 14:51:50.823809  1052 net.cpp:163] Memory required for data: 27444000
I0701 14:51:50.823817  1052 layer_factory.hpp:77] Creating layer conv1b/bn
I0701 14:51:50.823824  1052 net.cpp:98] Creating Layer conv1b/bn
I0701 14:51:50.823827  1052 net.cpp:439] conv1b/bn <- conv1b
I0701 14:51:50.823833  1052 net.cpp:413] conv1b/bn -> conv1b/bn
I0701 14:51:50.824563  1052 net.cpp:148] Setting up conv1b/bn
I0701 14:51:50.824571  1052 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0701 14:51:50.824574  1052 net.cpp:163] Memory required for data: 33997600
I0701 14:51:50.824584  1052 layer_factory.hpp:77] Creating layer conv1b/relu
I0701 14:51:50.824589  1052 net.cpp:98] Creating Layer conv1b/relu
I0701 14:51:50.824594  1052 net.cpp:439] conv1b/relu <- conv1b/bn
I0701 14:51:50.824599  1052 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0701 14:51:50.824604  1052 net.cpp:148] Setting up conv1b/relu
I0701 14:51:50.824609  1052 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0701 14:51:50.824612  1052 net.cpp:163] Memory required for data: 40551200
I0701 14:51:50.824615  1052 layer_factory.hpp:77] Creating layer pool1
I0701 14:51:50.824622  1052 net.cpp:98] Creating Layer pool1
I0701 14:51:50.824625  1052 net.cpp:439] pool1 <- conv1b/bn
I0701 14:51:50.824630  1052 net.cpp:413] pool1 -> pool1
I0701 14:51:50.824669  1052 net.cpp:148] Setting up pool1
I0701 14:51:50.824674  1052 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0701 14:51:50.824677  1052 net.cpp:163] Memory required for data: 47104800
I0701 14:51:50.824681  1052 layer_factory.hpp:77] Creating layer res2a_branch2a
I0701 14:51:50.824689  1052 net.cpp:98] Creating Layer res2a_branch2a
I0701 14:51:50.824693  1052 net.cpp:439] res2a_branch2a <- pool1
I0701 14:51:50.824697  1052 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0701 14:51:50.825384  1052 net.cpp:148] Setting up res2a_branch2a
I0701 14:51:50.825390  1052 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0701 14:51:50.825393  1052 net.cpp:163] Memory required for data: 60212000
I0701 14:51:50.825402  1052 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0701 14:51:50.825407  1052 net.cpp:98] Creating Layer res2a_branch2a/bn
I0701 14:51:50.825410  1052 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0701 14:51:50.825415  1052 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0701 14:51:50.826113  1052 net.cpp:148] Setting up res2a_branch2a/bn
I0701 14:51:50.826120  1052 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0701 14:51:50.826123  1052 net.cpp:163] Memory required for data: 73319200
I0701 14:51:50.826131  1052 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0701 14:51:50.826135  1052 net.cpp:98] Creating Layer res2a_branch2a/relu
I0701 14:51:50.826139  1052 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0701 14:51:50.826143  1052 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0701 14:51:50.826149  1052 net.cpp:148] Setting up res2a_branch2a/relu
I0701 14:51:50.826153  1052 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0701 14:51:50.826158  1052 net.cpp:163] Memory required for data: 86426400
I0701 14:51:50.826160  1052 layer_factory.hpp:77] Creating layer res2a_branch2b
I0701 14:51:50.826167  1052 net.cpp:98] Creating Layer res2a_branch2b
I0701 14:51:50.826181  1052 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0701 14:51:50.826186  1052 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0701 14:51:50.826684  1052 net.cpp:148] Setting up res2a_branch2b
I0701 14:51:50.826691  1052 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0701 14:51:50.826695  1052 net.cpp:163] Memory required for data: 99533600
I0701 14:51:50.826701  1052 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0701 14:51:50.826707  1052 net.cpp:98] Creating Layer res2a_branch2b/bn
I0701 14:51:50.826711  1052 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0701 14:51:50.826716  1052 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0701 14:51:50.827405  1052 net.cpp:148] Setting up res2a_branch2b/bn
I0701 14:51:50.827411  1052 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0701 14:51:50.827415  1052 net.cpp:163] Memory required for data: 112640800
I0701 14:51:50.827425  1052 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0701 14:51:50.827428  1052 net.cpp:98] Creating Layer res2a_branch2b/relu
I0701 14:51:50.827432  1052 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0701 14:51:50.827436  1052 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0701 14:51:50.827442  1052 net.cpp:148] Setting up res2a_branch2b/relu
I0701 14:51:50.827447  1052 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0701 14:51:50.827450  1052 net.cpp:163] Memory required for data: 125748000
I0701 14:51:50.827455  1052 layer_factory.hpp:77] Creating layer pool2
I0701 14:51:50.827460  1052 net.cpp:98] Creating Layer pool2
I0701 14:51:50.827462  1052 net.cpp:439] pool2 <- res2a_branch2b/bn
I0701 14:51:50.827466  1052 net.cpp:413] pool2 -> pool2
I0701 14:51:50.827507  1052 net.cpp:148] Setting up pool2
I0701 14:51:50.827512  1052 net.cpp:155] Top shape: 50 64 16 16 (819200)
I0701 14:51:50.827517  1052 net.cpp:163] Memory required for data: 129024800
I0701 14:51:50.827520  1052 layer_factory.hpp:77] Creating layer res3a_branch2a
I0701 14:51:50.827527  1052 net.cpp:98] Creating Layer res3a_branch2a
I0701 14:51:50.827530  1052 net.cpp:439] res3a_branch2a <- pool2
I0701 14:51:50.827535  1052 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0701 14:51:50.830276  1052 net.cpp:148] Setting up res3a_branch2a
I0701 14:51:50.830291  1052 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0701 14:51:50.830294  1052 net.cpp:163] Memory required for data: 135578400
I0701 14:51:50.830302  1052 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0701 14:51:50.830310  1052 net.cpp:98] Creating Layer res3a_branch2a/bn
I0701 14:51:50.830314  1052 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0701 14:51:50.830322  1052 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0701 14:51:50.831073  1052 net.cpp:148] Setting up res3a_branch2a/bn
I0701 14:51:50.831080  1052 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0701 14:51:50.831084  1052 net.cpp:163] Memory required for data: 142132000
I0701 14:51:50.831094  1052 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0701 14:51:50.831104  1052 net.cpp:98] Creating Layer res3a_branch2a/relu
I0701 14:51:50.831107  1052 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0701 14:51:50.831112  1052 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0701 14:51:50.831120  1052 net.cpp:148] Setting up res3a_branch2a/relu
I0701 14:51:50.831123  1052 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0701 14:51:50.831127  1052 net.cpp:163] Memory required for data: 148685600
I0701 14:51:50.831131  1052 layer_factory.hpp:77] Creating layer res3a_branch2b
I0701 14:51:50.831138  1052 net.cpp:98] Creating Layer res3a_branch2b
I0701 14:51:50.831142  1052 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0701 14:51:50.831146  1052 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0701 14:51:50.832196  1052 net.cpp:148] Setting up res3a_branch2b
I0701 14:51:50.832203  1052 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0701 14:51:50.832206  1052 net.cpp:163] Memory required for data: 155239200
I0701 14:51:50.832219  1052 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0701 14:51:50.832226  1052 net.cpp:98] Creating Layer res3a_branch2b/bn
I0701 14:51:50.832231  1052 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0701 14:51:50.832235  1052 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0701 14:51:50.832881  1052 net.cpp:148] Setting up res3a_branch2b/bn
I0701 14:51:50.832888  1052 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0701 14:51:50.832892  1052 net.cpp:163] Memory required for data: 161792800
I0701 14:51:50.832901  1052 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0701 14:51:50.832906  1052 net.cpp:98] Creating Layer res3a_branch2b/relu
I0701 14:51:50.832909  1052 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0701 14:51:50.832913  1052 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0701 14:51:50.832919  1052 net.cpp:148] Setting up res3a_branch2b/relu
I0701 14:51:50.832924  1052 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0701 14:51:50.832927  1052 net.cpp:163] Memory required for data: 168346400
I0701 14:51:50.832931  1052 layer_factory.hpp:77] Creating layer pool3
I0701 14:51:50.832937  1052 net.cpp:98] Creating Layer pool3
I0701 14:51:50.832940  1052 net.cpp:439] pool3 <- res3a_branch2b/bn
I0701 14:51:50.832945  1052 net.cpp:413] pool3 -> pool3
I0701 14:51:50.832988  1052 net.cpp:148] Setting up pool3
I0701 14:51:50.832993  1052 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0701 14:51:50.832998  1052 net.cpp:163] Memory required for data: 174900000
I0701 14:51:50.833000  1052 layer_factory.hpp:77] Creating layer res4a_branch2a
I0701 14:51:50.833008  1052 net.cpp:98] Creating Layer res4a_branch2a
I0701 14:51:50.833010  1052 net.cpp:439] res4a_branch2a <- pool3
I0701 14:51:50.833015  1052 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0701 14:51:50.839099  1052 net.cpp:148] Setting up res4a_branch2a
I0701 14:51:50.839107  1052 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0701 14:51:50.839109  1052 net.cpp:163] Memory required for data: 188007200
I0701 14:51:50.839117  1052 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0701 14:51:50.839121  1052 net.cpp:98] Creating Layer res4a_branch2a/bn
I0701 14:51:50.839126  1052 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0701 14:51:50.839131  1052 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0701 14:51:50.839771  1052 net.cpp:148] Setting up res4a_branch2a/bn
I0701 14:51:50.839777  1052 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0701 14:51:50.839782  1052 net.cpp:163] Memory required for data: 201114400
I0701 14:51:50.839790  1052 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0701 14:51:50.839794  1052 net.cpp:98] Creating Layer res4a_branch2a/relu
I0701 14:51:50.839798  1052 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0701 14:51:50.839803  1052 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0701 14:51:50.839809  1052 net.cpp:148] Setting up res4a_branch2a/relu
I0701 14:51:50.839813  1052 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0701 14:51:50.839818  1052 net.cpp:163] Memory required for data: 214221600
I0701 14:51:50.839820  1052 layer_factory.hpp:77] Creating layer res4a_branch2b
I0701 14:51:50.839826  1052 net.cpp:98] Creating Layer res4a_branch2b
I0701 14:51:50.839830  1052 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0701 14:51:50.839835  1052 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0701 14:51:50.843037  1052 net.cpp:148] Setting up res4a_branch2b
I0701 14:51:50.843044  1052 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0701 14:51:50.843047  1052 net.cpp:163] Memory required for data: 227328800
I0701 14:51:50.843053  1052 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0701 14:51:50.843058  1052 net.cpp:98] Creating Layer res4a_branch2b/bn
I0701 14:51:50.843062  1052 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0701 14:51:50.843072  1052 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0701 14:51:50.843719  1052 net.cpp:148] Setting up res4a_branch2b/bn
I0701 14:51:50.843725  1052 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0701 14:51:50.843737  1052 net.cpp:163] Memory required for data: 240436000
I0701 14:51:50.843746  1052 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0701 14:51:50.843751  1052 net.cpp:98] Creating Layer res4a_branch2b/relu
I0701 14:51:50.843755  1052 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0701 14:51:50.843760  1052 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0701 14:51:50.843766  1052 net.cpp:148] Setting up res4a_branch2b/relu
I0701 14:51:50.843770  1052 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0701 14:51:50.843775  1052 net.cpp:163] Memory required for data: 253543200
I0701 14:51:50.843777  1052 layer_factory.hpp:77] Creating layer pool4
I0701 14:51:50.843783  1052 net.cpp:98] Creating Layer pool4
I0701 14:51:50.843787  1052 net.cpp:439] pool4 <- res4a_branch2b/bn
I0701 14:51:50.843792  1052 net.cpp:413] pool4 -> pool4
I0701 14:51:50.843839  1052 net.cpp:148] Setting up pool4
I0701 14:51:50.843844  1052 net.cpp:155] Top shape: 50 256 8 8 (819200)
I0701 14:51:50.843848  1052 net.cpp:163] Memory required for data: 256820000
I0701 14:51:50.843852  1052 layer_factory.hpp:77] Creating layer res5a_branch2a
I0701 14:51:50.843859  1052 net.cpp:98] Creating Layer res5a_branch2a
I0701 14:51:50.843863  1052 net.cpp:439] res5a_branch2a <- pool4
I0701 14:51:50.843868  1052 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0701 14:51:50.868707  1052 net.cpp:148] Setting up res5a_branch2a
I0701 14:51:50.868726  1052 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0701 14:51:50.868729  1052 net.cpp:163] Memory required for data: 263373600
I0701 14:51:50.868736  1052 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0701 14:51:50.868747  1052 net.cpp:98] Creating Layer res5a_branch2a/bn
I0701 14:51:50.868752  1052 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0701 14:51:50.868758  1052 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0701 14:51:50.869464  1052 net.cpp:148] Setting up res5a_branch2a/bn
I0701 14:51:50.869477  1052 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0701 14:51:50.869482  1052 net.cpp:163] Memory required for data: 269927200
I0701 14:51:50.869489  1052 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0701 14:51:50.869494  1052 net.cpp:98] Creating Layer res5a_branch2a/relu
I0701 14:51:50.869498  1052 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0701 14:51:50.869503  1052 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0701 14:51:50.869509  1052 net.cpp:148] Setting up res5a_branch2a/relu
I0701 14:51:50.869514  1052 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0701 14:51:50.869518  1052 net.cpp:163] Memory required for data: 276480800
I0701 14:51:50.869523  1052 layer_factory.hpp:77] Creating layer res5a_branch2b
I0701 14:51:50.869529  1052 net.cpp:98] Creating Layer res5a_branch2b
I0701 14:51:50.869532  1052 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0701 14:51:50.869537  1052 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0701 14:51:50.882344  1052 net.cpp:148] Setting up res5a_branch2b
I0701 14:51:50.882354  1052 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0701 14:51:50.882356  1052 net.cpp:163] Memory required for data: 283034400
I0701 14:51:50.882369  1052 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0701 14:51:50.882376  1052 net.cpp:98] Creating Layer res5a_branch2b/bn
I0701 14:51:50.882380  1052 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0701 14:51:50.882388  1052 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0701 14:51:50.883157  1052 net.cpp:148] Setting up res5a_branch2b/bn
I0701 14:51:50.883164  1052 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0701 14:51:50.883168  1052 net.cpp:163] Memory required for data: 289588000
I0701 14:51:50.883177  1052 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0701 14:51:50.883182  1052 net.cpp:98] Creating Layer res5a_branch2b/relu
I0701 14:51:50.883185  1052 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0701 14:51:50.883190  1052 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0701 14:51:50.883204  1052 net.cpp:148] Setting up res5a_branch2b/relu
I0701 14:51:50.883209  1052 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0701 14:51:50.883213  1052 net.cpp:163] Memory required for data: 296141600
I0701 14:51:50.883216  1052 layer_factory.hpp:77] Creating layer pool5
I0701 14:51:50.883222  1052 net.cpp:98] Creating Layer pool5
I0701 14:51:50.883225  1052 net.cpp:439] pool5 <- res5a_branch2b/bn
I0701 14:51:50.883230  1052 net.cpp:413] pool5 -> pool5
I0701 14:51:50.883258  1052 net.cpp:148] Setting up pool5
I0701 14:51:50.883263  1052 net.cpp:155] Top shape: 50 512 1 1 (25600)
I0701 14:51:50.883267  1052 net.cpp:163] Memory required for data: 296244000
I0701 14:51:50.883271  1052 layer_factory.hpp:77] Creating layer fc10
I0701 14:51:50.883283  1052 net.cpp:98] Creating Layer fc10
I0701 14:51:50.883287  1052 net.cpp:439] fc10 <- pool5
I0701 14:51:50.883293  1052 net.cpp:413] fc10 -> fc10
I0701 14:51:50.883548  1052 net.cpp:148] Setting up fc10
I0701 14:51:50.883554  1052 net.cpp:155] Top shape: 50 10 (500)
I0701 14:51:50.883558  1052 net.cpp:163] Memory required for data: 296246000
I0701 14:51:50.883564  1052 layer_factory.hpp:77] Creating layer fc10_fc10_0_split
I0701 14:51:50.883570  1052 net.cpp:98] Creating Layer fc10_fc10_0_split
I0701 14:51:50.883574  1052 net.cpp:439] fc10_fc10_0_split <- fc10
I0701 14:51:50.883579  1052 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0701 14:51:50.883584  1052 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0701 14:51:50.883589  1052 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0701 14:51:50.883656  1052 net.cpp:148] Setting up fc10_fc10_0_split
I0701 14:51:50.883661  1052 net.cpp:155] Top shape: 50 10 (500)
I0701 14:51:50.883666  1052 net.cpp:155] Top shape: 50 10 (500)
I0701 14:51:50.883671  1052 net.cpp:155] Top shape: 50 10 (500)
I0701 14:51:50.883674  1052 net.cpp:163] Memory required for data: 296252000
I0701 14:51:50.883678  1052 layer_factory.hpp:77] Creating layer loss
I0701 14:51:50.883684  1052 net.cpp:98] Creating Layer loss
I0701 14:51:50.883688  1052 net.cpp:439] loss <- fc10_fc10_0_split_0
I0701 14:51:50.883692  1052 net.cpp:439] loss <- label_data_1_split_0
I0701 14:51:50.883697  1052 net.cpp:413] loss -> loss
I0701 14:51:50.883704  1052 layer_factory.hpp:77] Creating layer loss
I0701 14:51:50.883821  1052 net.cpp:148] Setting up loss
I0701 14:51:50.883826  1052 net.cpp:155] Top shape: (1)
I0701 14:51:50.883831  1052 net.cpp:158]     with loss weight 1
I0701 14:51:50.883841  1052 net.cpp:163] Memory required for data: 296252004
I0701 14:51:50.883843  1052 layer_factory.hpp:77] Creating layer accuracy/top1
I0701 14:51:50.883851  1052 net.cpp:98] Creating Layer accuracy/top1
I0701 14:51:50.883853  1052 net.cpp:439] accuracy/top1 <- fc10_fc10_0_split_1
I0701 14:51:50.883858  1052 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0701 14:51:50.883863  1052 net.cpp:413] accuracy/top1 -> accuracy/top1
I0701 14:51:50.883877  1052 net.cpp:148] Setting up accuracy/top1
I0701 14:51:50.883880  1052 net.cpp:155] Top shape: (1)
I0701 14:51:50.883884  1052 net.cpp:163] Memory required for data: 296252008
I0701 14:51:50.883888  1052 layer_factory.hpp:77] Creating layer accuracy/top5
I0701 14:51:50.883893  1052 net.cpp:98] Creating Layer accuracy/top5
I0701 14:51:50.883898  1052 net.cpp:439] accuracy/top5 <- fc10_fc10_0_split_2
I0701 14:51:50.883901  1052 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0701 14:51:50.883906  1052 net.cpp:413] accuracy/top5 -> accuracy/top5
I0701 14:51:50.883913  1052 net.cpp:148] Setting up accuracy/top5
I0701 14:51:50.883916  1052 net.cpp:155] Top shape: (1)
I0701 14:51:50.883919  1052 net.cpp:163] Memory required for data: 296252012
I0701 14:51:50.883924  1052 net.cpp:226] accuracy/top5 does not need backward computation.
I0701 14:51:50.883927  1052 net.cpp:226] accuracy/top1 does not need backward computation.
I0701 14:51:50.883931  1052 net.cpp:224] loss needs backward computation.
I0701 14:51:50.883935  1052 net.cpp:224] fc10_fc10_0_split needs backward computation.
I0701 14:51:50.883946  1052 net.cpp:224] fc10 needs backward computation.
I0701 14:51:50.883949  1052 net.cpp:224] pool5 needs backward computation.
I0701 14:51:50.883954  1052 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0701 14:51:50.883956  1052 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0701 14:51:50.883961  1052 net.cpp:224] res5a_branch2b needs backward computation.
I0701 14:51:50.883965  1052 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0701 14:51:50.883968  1052 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0701 14:51:50.883972  1052 net.cpp:224] res5a_branch2a needs backward computation.
I0701 14:51:50.883976  1052 net.cpp:224] pool4 needs backward computation.
I0701 14:51:50.883980  1052 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0701 14:51:50.883985  1052 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0701 14:51:50.883990  1052 net.cpp:224] res4a_branch2b needs backward computation.
I0701 14:51:50.883993  1052 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0701 14:51:50.883996  1052 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0701 14:51:50.884001  1052 net.cpp:224] res4a_branch2a needs backward computation.
I0701 14:51:50.884004  1052 net.cpp:224] pool3 needs backward computation.
I0701 14:51:50.884008  1052 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0701 14:51:50.884012  1052 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0701 14:51:50.884016  1052 net.cpp:224] res3a_branch2b needs backward computation.
I0701 14:51:50.884021  1052 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0701 14:51:50.884024  1052 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0701 14:51:50.884027  1052 net.cpp:224] res3a_branch2a needs backward computation.
I0701 14:51:50.884032  1052 net.cpp:224] pool2 needs backward computation.
I0701 14:51:50.884035  1052 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0701 14:51:50.884039  1052 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0701 14:51:50.884043  1052 net.cpp:224] res2a_branch2b needs backward computation.
I0701 14:51:50.884047  1052 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0701 14:51:50.884050  1052 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0701 14:51:50.884054  1052 net.cpp:224] res2a_branch2a needs backward computation.
I0701 14:51:50.884059  1052 net.cpp:224] pool1 needs backward computation.
I0701 14:51:50.884063  1052 net.cpp:224] conv1b/relu needs backward computation.
I0701 14:51:50.884066  1052 net.cpp:224] conv1b/bn needs backward computation.
I0701 14:51:50.884070  1052 net.cpp:224] conv1b needs backward computation.
I0701 14:51:50.884074  1052 net.cpp:224] conv1a/relu needs backward computation.
I0701 14:51:50.884078  1052 net.cpp:224] conv1a/bn needs backward computation.
I0701 14:51:50.884083  1052 net.cpp:224] conv1a needs backward computation.
I0701 14:51:50.884086  1052 net.cpp:226] data/bias does not need backward computation.
I0701 14:51:50.884091  1052 net.cpp:226] label_data_1_split does not need backward computation.
I0701 14:51:50.884095  1052 net.cpp:226] data does not need backward computation.
I0701 14:51:50.884099  1052 net.cpp:268] This network produces output accuracy/top1
I0701 14:51:50.884104  1052 net.cpp:268] This network produces output accuracy/top5
I0701 14:51:50.884107  1052 net.cpp:268] This network produces output loss
I0701 14:51:50.884132  1052 net.cpp:288] Network initialization done.
I0701 14:51:50.884202  1052 solver.cpp:60] Solver scaffolding done.
I0701 14:51:50.887645  1052 caffe.cpp:145] Finetuning from training/cifar10_jacintonet11v2_2017-07-01_14-28-09/initial/cifar10_jacintonet11v2_iter_64000.caffemodel
I0701 14:51:50.914508  1052 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0701 14:51:50.914577  1052 data_layer.cpp:83] output data size: 21,3,32,32
I0701 14:51:51.357589  1052 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0701 14:51:51.357663  1052 data_layer.cpp:83] output data size: 21,3,32,32
I0701 14:51:51.832545  1052 parallel.cpp:334] Starting Optimization
I0701 14:51:51.832597  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:51:51.841852  1052 solver.cpp:415] Solving jacintonet11v2_train
I0701 14:51:51.841871  1052 solver.cpp:416] Learning Rate Policy: poly
I0701 14:51:51.843724  1052 solver.cpp:473] Iteration 0, Testing net (#0)
I0701 14:51:53.520386  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9171
I0701 14:51:53.520406  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9974
I0701 14:51:53.520411  1052 solver.cpp:546]     Test net output #2: loss = 0.2066 (* 1 = 0.2066 loss)
I0701 14:51:53.622195  1052 solver.cpp:290] Iteration 0 (0 iter/s, 1.78026s/100 iter), loss = 0
I0701 14:51:53.622216  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:51:53.622223  1052 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0701 14:51:55.715358  1052 solver.cpp:290] Iteration 100 (47.7765 iter/s, 2.09308s/100 iter), loss = 0
I0701 14:51:55.715381  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:51:55.715387  1052 sgd_solver.cpp:106] Iteration 100, lr = 0.00998437
I0701 14:51:57.850760  1052 solver.cpp:290] Iteration 200 (46.8315 iter/s, 2.13531s/100 iter), loss = 0
I0701 14:51:57.850782  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:51:57.850788  1052 sgd_solver.cpp:106] Iteration 200, lr = 0.00996875
I0701 14:51:59.925151  1052 solver.cpp:290] Iteration 300 (48.2089 iter/s, 2.07431s/100 iter), loss = 0
I0701 14:51:59.925173  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:51:59.925180  1052 sgd_solver.cpp:106] Iteration 300, lr = 0.00995312
I0701 14:52:01.998052  1052 solver.cpp:290] Iteration 400 (48.2435 iter/s, 2.07282s/100 iter), loss = 0
I0701 14:52:01.998075  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:01.998085  1052 sgd_solver.cpp:106] Iteration 400, lr = 0.0099375
I0701 14:52:04.072420  1052 solver.cpp:290] Iteration 500 (48.2095 iter/s, 2.07428s/100 iter), loss = 0
I0701 14:52:04.072443  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:04.072451  1052 sgd_solver.cpp:106] Iteration 500, lr = 0.00992187
I0701 14:52:06.145853  1052 solver.cpp:290] Iteration 600 (48.2312 iter/s, 2.07335s/100 iter), loss = 0
I0701 14:52:06.145874  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:06.145881  1052 sgd_solver.cpp:106] Iteration 600, lr = 0.00990625
I0701 14:52:08.219409  1052 solver.cpp:290] Iteration 700 (48.2283 iter/s, 2.07347s/100 iter), loss = 0
I0701 14:52:08.219434  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:08.219442  1052 sgd_solver.cpp:106] Iteration 700, lr = 0.00989062
I0701 14:52:10.299667  1052 solver.cpp:290] Iteration 800 (48.0729 iter/s, 2.08017s/100 iter), loss = 0
I0701 14:52:10.299691  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:10.299700  1052 sgd_solver.cpp:106] Iteration 800, lr = 0.009875
I0701 14:52:12.377670  1052 solver.cpp:290] Iteration 900 (48.1251 iter/s, 2.07792s/100 iter), loss = 0
I0701 14:52:12.377694  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:12.377702  1052 sgd_solver.cpp:106] Iteration 900, lr = 0.00985937
I0701 14:52:14.435596  1052 solver.cpp:354] Sparsity after update:
I0701 14:52:14.436947  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:52:14.436954  1052 net.cpp:1851] conv1a_param_0(0) 
I0701 14:52:14.436965  1052 net.cpp:1851] conv1b_param_0(0) 
I0701 14:52:14.436967  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:52:14.436969  1052 net.cpp:1851] res2a_branch2a_param_0(0) 
I0701 14:52:14.436971  1052 net.cpp:1851] res2a_branch2b_param_0(0) 
I0701 14:52:14.436974  1052 net.cpp:1851] res3a_branch2a_param_0(0) 
I0701 14:52:14.436976  1052 net.cpp:1851] res3a_branch2b_param_0(0) 
I0701 14:52:14.436978  1052 net.cpp:1851] res4a_branch2a_param_0(0) 
I0701 14:52:14.436991  1052 net.cpp:1851] res4a_branch2b_param_0(0) 
I0701 14:52:14.436993  1052 net.cpp:1851] res5a_branch2a_param_0(0) 
I0701 14:52:14.436995  1052 net.cpp:1851] res5a_branch2b_param_0(0) 
I0701 14:52:14.436998  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0701 14:52:14.437085  1052 solver.cpp:473] Iteration 1000, Testing net (#0)
I0701 14:52:16.077884  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9186
I0701 14:52:16.077903  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9972
I0701 14:52:16.077909  1052 solver.cpp:546]     Test net output #2: loss = 0.2066 (* 1 = 0.2066 loss)
I0701 14:52:16.097882  1052 solver.cpp:290] Iteration 1000 (26.8811 iter/s, 3.72009s/100 iter), loss = 0
I0701 14:52:16.097900  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:16.097910  1052 sgd_solver.cpp:106] Iteration 1000, lr = 0.00984375
I0701 14:52:18.181015  1052 solver.cpp:290] Iteration 1100 (48.0065 iter/s, 2.08305s/100 iter), loss = 0
I0701 14:52:18.181035  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:18.181042  1052 sgd_solver.cpp:106] Iteration 1100, lr = 0.00982813
I0701 14:52:20.256978  1052 solver.cpp:290] Iteration 1200 (48.1723 iter/s, 2.07588s/100 iter), loss = 0
I0701 14:52:20.257000  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:20.257006  1052 sgd_solver.cpp:106] Iteration 1200, lr = 0.0098125
I0701 14:52:22.331051  1052 solver.cpp:290] Iteration 1300 (48.2163 iter/s, 2.07399s/100 iter), loss = 0
I0701 14:52:22.331109  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:22.331115  1052 sgd_solver.cpp:106] Iteration 1300, lr = 0.00979687
I0701 14:52:24.404567  1052 solver.cpp:290] Iteration 1400 (48.23 iter/s, 2.0734s/100 iter), loss = 0
I0701 14:52:24.404588  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:24.404594  1052 sgd_solver.cpp:106] Iteration 1400, lr = 0.00978125
I0701 14:52:26.479073  1052 solver.cpp:290] Iteration 1500 (48.2062 iter/s, 2.07442s/100 iter), loss = 0
I0701 14:52:26.479094  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:26.479102  1052 sgd_solver.cpp:106] Iteration 1500, lr = 0.00976562
I0701 14:52:28.554941  1052 solver.cpp:290] Iteration 1600 (48.1746 iter/s, 2.07578s/100 iter), loss = 0
I0701 14:52:28.554962  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:28.554970  1052 sgd_solver.cpp:106] Iteration 1600, lr = 0.00975
I0701 14:52:30.634445  1052 solver.cpp:290] Iteration 1700 (48.0903 iter/s, 2.07942s/100 iter), loss = 0
I0701 14:52:30.634466  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:30.634474  1052 sgd_solver.cpp:106] Iteration 1700, lr = 0.00973437
I0701 14:52:32.709966  1052 solver.cpp:290] Iteration 1800 (48.1826 iter/s, 2.07544s/100 iter), loss = 0
I0701 14:52:32.709987  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:32.709995  1052 sgd_solver.cpp:106] Iteration 1800, lr = 0.00971875
I0701 14:52:34.781695  1052 solver.cpp:290] Iteration 1900 (48.2708 iter/s, 2.07165s/100 iter), loss = 0
I0701 14:52:34.781718  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:34.781724  1052 sgd_solver.cpp:106] Iteration 1900, lr = 0.00970312
I0701 14:52:36.836957  1052 solver.cpp:354] Sparsity after update:
I0701 14:52:36.838312  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:52:36.838320  1052 net.cpp:1851] conv1a_param_0(0) 
I0701 14:52:36.838326  1052 net.cpp:1851] conv1b_param_0(0) 
I0701 14:52:36.838328  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:52:36.838330  1052 net.cpp:1851] res2a_branch2a_param_0(0) 
I0701 14:52:36.838331  1052 net.cpp:1851] res2a_branch2b_param_0(0) 
I0701 14:52:36.838333  1052 net.cpp:1851] res3a_branch2a_param_0(0) 
I0701 14:52:36.838335  1052 net.cpp:1851] res3a_branch2b_param_0(0) 
I0701 14:52:36.838337  1052 net.cpp:1851] res4a_branch2a_param_0(0) 
I0701 14:52:36.838340  1052 net.cpp:1851] res4a_branch2b_param_0(0) 
I0701 14:52:36.838341  1052 net.cpp:1851] res5a_branch2a_param_0(0) 
I0701 14:52:36.838342  1052 net.cpp:1851] res5a_branch2b_param_0(0) 
I0701 14:52:36.838345  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0701 14:52:36.838434  1052 solver.cpp:473] Iteration 2000, Testing net (#0)
I0701 14:52:38.478858  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9173
I0701 14:52:38.478878  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9975
I0701 14:52:38.478883  1052 solver.cpp:546]     Test net output #2: loss = 0.2061 (* 1 = 0.2061 loss)
I0701 14:52:38.499133  1052 solver.cpp:290] Iteration 2000 (26.9012 iter/s, 3.71731s/100 iter), loss = 0
I0701 14:52:38.499151  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:38.499163  1052 sgd_solver.cpp:106] Iteration 2000, lr = 0.0096875
I0701 14:52:40.579850  1052 solver.cpp:290] Iteration 2100 (48.0622 iter/s, 2.08064s/100 iter), loss = 0
I0701 14:52:40.579872  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:40.579879  1052 sgd_solver.cpp:106] Iteration 2100, lr = 0.00967188
I0701 14:52:42.660487  1052 solver.cpp:290] Iteration 2200 (48.0641 iter/s, 2.08055s/100 iter), loss = 0
I0701 14:52:42.660511  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:42.660518  1052 sgd_solver.cpp:106] Iteration 2200, lr = 0.00965625
I0701 14:52:44.737766  1052 solver.cpp:290] Iteration 2300 (48.1419 iter/s, 2.07719s/100 iter), loss = 0
I0701 14:52:44.737807  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:44.737814  1052 sgd_solver.cpp:106] Iteration 2300, lr = 0.00964062
I0701 14:52:46.812765  1052 solver.cpp:290] Iteration 2400 (48.1952 iter/s, 2.0749s/100 iter), loss = 0
I0701 14:52:46.812788  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:46.812794  1052 sgd_solver.cpp:106] Iteration 2400, lr = 0.009625
I0701 14:52:48.884222  1052 solver.cpp:290] Iteration 2500 (48.2773 iter/s, 2.07137s/100 iter), loss = 0
I0701 14:52:48.884248  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:48.884258  1052 sgd_solver.cpp:106] Iteration 2500, lr = 0.00960938
I0701 14:52:50.960265  1052 solver.cpp:290] Iteration 2600 (48.1705 iter/s, 2.07596s/100 iter), loss = 0
I0701 14:52:50.960286  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:50.960294  1052 sgd_solver.cpp:106] Iteration 2600, lr = 0.00959375
I0701 14:52:53.036571  1052 solver.cpp:290] Iteration 2700 (48.1644 iter/s, 2.07622s/100 iter), loss = 0
I0701 14:52:53.036623  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:53.036630  1052 sgd_solver.cpp:106] Iteration 2700, lr = 0.00957812
I0701 14:52:55.113955  1052 solver.cpp:290] Iteration 2800 (48.1401 iter/s, 2.07727s/100 iter), loss = 0
I0701 14:52:55.113977  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:55.113984  1052 sgd_solver.cpp:106] Iteration 2800, lr = 0.0095625
I0701 14:52:57.225872  1052 solver.cpp:290] Iteration 2900 (47.3523 iter/s, 2.11183s/100 iter), loss = 0
I0701 14:52:57.225898  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:52:57.225906  1052 sgd_solver.cpp:106] Iteration 2900, lr = 0.00954687
I0701 14:52:59.289978  1052 solver.cpp:354] Sparsity after update:
I0701 14:52:59.291328  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:52:59.291335  1052 net.cpp:1851] conv1a_param_0(0) 
I0701 14:52:59.291342  1052 net.cpp:1851] conv1b_param_0(0) 
I0701 14:52:59.291344  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:52:59.291347  1052 net.cpp:1851] res2a_branch2a_param_0(0) 
I0701 14:52:59.291348  1052 net.cpp:1851] res2a_branch2b_param_0(0) 
I0701 14:52:59.291349  1052 net.cpp:1851] res3a_branch2a_param_0(0) 
I0701 14:52:59.291352  1052 net.cpp:1851] res3a_branch2b_param_0(0) 
I0701 14:52:59.291353  1052 net.cpp:1851] res4a_branch2a_param_0(0) 
I0701 14:52:59.291355  1052 net.cpp:1851] res4a_branch2b_param_0(0) 
I0701 14:52:59.291357  1052 net.cpp:1851] res5a_branch2a_param_0(0) 
I0701 14:52:59.291358  1052 net.cpp:1851] res5a_branch2b_param_0(0) 
I0701 14:52:59.291360  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0701 14:52:59.291445  1052 solver.cpp:473] Iteration 3000, Testing net (#0)
I0701 14:53:00.943097  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9178
I0701 14:53:00.943115  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9972
I0701 14:53:00.943120  1052 solver.cpp:546]     Test net output #2: loss = 0.208 (* 1 = 0.208 loss)
I0701 14:53:00.963281  1052 solver.cpp:290] Iteration 3000 (26.7574 iter/s, 3.73728s/100 iter), loss = 0
I0701 14:53:00.963299  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:00.963310  1052 sgd_solver.cpp:106] Iteration 3000, lr = 0.00953125
I0701 14:53:03.038866  1052 solver.cpp:290] Iteration 3100 (48.1811 iter/s, 2.0755s/100 iter), loss = 0
I0701 14:53:03.038888  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:03.038894  1052 sgd_solver.cpp:106] Iteration 3100, lr = 0.00951563
I0701 14:53:05.111137  1052 solver.cpp:290] Iteration 3200 (48.2582 iter/s, 2.07219s/100 iter), loss = 0
I0701 14:53:05.111160  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:05.111166  1052 sgd_solver.cpp:106] Iteration 3200, lr = 0.0095
I0701 14:53:07.184571  1052 solver.cpp:290] Iteration 3300 (48.2311 iter/s, 2.07335s/100 iter), loss = 0
I0701 14:53:07.184592  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:07.184599  1052 sgd_solver.cpp:106] Iteration 3300, lr = 0.00948437
I0701 14:53:09.264938  1052 solver.cpp:290] Iteration 3400 (48.0704 iter/s, 2.08028s/100 iter), loss = 0
I0701 14:53:09.264961  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:09.264967  1052 sgd_solver.cpp:106] Iteration 3400, lr = 0.00946875
I0701 14:53:11.340593  1052 solver.cpp:290] Iteration 3500 (48.1795 iter/s, 2.07557s/100 iter), loss = 0
I0701 14:53:11.340615  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:11.340621  1052 sgd_solver.cpp:106] Iteration 3500, lr = 0.00945312
I0701 14:53:13.414933  1052 solver.cpp:290] Iteration 3600 (48.2101 iter/s, 2.07426s/100 iter), loss = 0
I0701 14:53:13.414957  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:13.414963  1052 sgd_solver.cpp:106] Iteration 3600, lr = 0.0094375
I0701 14:53:15.488169  1052 solver.cpp:290] Iteration 3700 (48.2357 iter/s, 2.07315s/100 iter), loss = 0
I0701 14:53:15.488214  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:15.488220  1052 sgd_solver.cpp:106] Iteration 3700, lr = 0.00942187
I0701 14:53:17.561799  1052 solver.cpp:290] Iteration 3800 (48.2271 iter/s, 2.07352s/100 iter), loss = 0
I0701 14:53:17.561820  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:17.561827  1052 sgd_solver.cpp:106] Iteration 3800, lr = 0.00940625
I0701 14:53:19.633285  1052 solver.cpp:290] Iteration 3900 (48.2765 iter/s, 2.0714s/100 iter), loss = 0
I0701 14:53:19.633307  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:19.633314  1052 sgd_solver.cpp:106] Iteration 3900, lr = 0.00939062
I0701 14:53:21.690708  1052 solver.cpp:354] Sparsity after update:
I0701 14:53:21.692066  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:53:21.692073  1052 net.cpp:1851] conv1a_param_0(0) 
I0701 14:53:21.692082  1052 net.cpp:1851] conv1b_param_0(0) 
I0701 14:53:21.692087  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:53:21.692091  1052 net.cpp:1851] res2a_branch2a_param_0(0) 
I0701 14:53:21.692095  1052 net.cpp:1851] res2a_branch2b_param_0(0) 
I0701 14:53:21.692098  1052 net.cpp:1851] res3a_branch2a_param_0(0) 
I0701 14:53:21.692102  1052 net.cpp:1851] res3a_branch2b_param_0(0) 
I0701 14:53:21.692106  1052 net.cpp:1851] res4a_branch2a_param_0(0) 
I0701 14:53:21.692109  1052 net.cpp:1851] res4a_branch2b_param_0(0) 
I0701 14:53:21.692112  1052 net.cpp:1851] res5a_branch2a_param_0(0) 
I0701 14:53:21.692116  1052 net.cpp:1851] res5a_branch2b_param_0(0) 
I0701 14:53:21.692122  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0701 14:53:21.692212  1052 solver.cpp:473] Iteration 4000, Testing net (#0)
I0701 14:53:23.339126  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9187
I0701 14:53:23.339223  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9972
I0701 14:53:23.339232  1052 solver.cpp:546]     Test net output #2: loss = 0.2076 (* 1 = 0.2076 loss)
I0701 14:53:23.359046  1052 solver.cpp:290] Iteration 4000 (26.841 iter/s, 3.72564s/100 iter), loss = 0
I0701 14:53:23.359066  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:23.359077  1052 sgd_solver.cpp:106] Iteration 4000, lr = 0.009375
I0701 14:53:23.359712  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.02
I0701 14:53:23.375741  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:53:25.451705  1052 solver.cpp:290] Iteration 4100 (47.7879 iter/s, 2.09258s/100 iter), loss = 0
I0701 14:53:25.451727  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:25.451735  1052 sgd_solver.cpp:106] Iteration 4100, lr = 0.00935937
I0701 14:53:27.525647  1052 solver.cpp:290] Iteration 4200 (48.2193 iter/s, 2.07386s/100 iter), loss = 0
I0701 14:53:27.525671  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:27.525676  1052 sgd_solver.cpp:106] Iteration 4200, lr = 0.00934375
I0701 14:53:29.598316  1052 solver.cpp:290] Iteration 4300 (48.249 iter/s, 2.07258s/100 iter), loss = 0
I0701 14:53:29.598338  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:29.598346  1052 sgd_solver.cpp:106] Iteration 4300, lr = 0.00932813
I0701 14:53:31.672260  1052 solver.cpp:290] Iteration 4400 (48.2193 iter/s, 2.07386s/100 iter), loss = 0
I0701 14:53:31.672281  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:31.672288  1052 sgd_solver.cpp:106] Iteration 4400, lr = 0.0093125
I0701 14:53:33.744017  1052 solver.cpp:290] Iteration 4500 (48.2701 iter/s, 2.07167s/100 iter), loss = 0
I0701 14:53:33.744038  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:33.744046  1052 sgd_solver.cpp:106] Iteration 4500, lr = 0.00929687
I0701 14:53:35.816669  1052 solver.cpp:290] Iteration 4600 (48.2493 iter/s, 2.07257s/100 iter), loss = 0
I0701 14:53:35.816691  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:35.816697  1052 sgd_solver.cpp:106] Iteration 4600, lr = 0.00928125
I0701 14:53:37.897264  1052 solver.cpp:290] Iteration 4700 (48.0652 iter/s, 2.08051s/100 iter), loss = 0
I0701 14:53:37.897289  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:37.897296  1052 sgd_solver.cpp:106] Iteration 4700, lr = 0.00926562
I0701 14:53:39.971644  1052 solver.cpp:290] Iteration 4800 (48.2092 iter/s, 2.07429s/100 iter), loss = 0
I0701 14:53:39.971664  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:39.971673  1052 sgd_solver.cpp:106] Iteration 4800, lr = 0.00925
I0701 14:53:42.045174  1052 solver.cpp:290] Iteration 4900 (48.2289 iter/s, 2.07345s/100 iter), loss = 0
I0701 14:53:42.045194  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:42.045202  1052 sgd_solver.cpp:106] Iteration 4900, lr = 0.00923437
I0701 14:53:44.102216  1052 solver.cpp:354] Sparsity after update:
I0701 14:53:44.103546  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:53:44.103554  1052 net.cpp:1851] conv1a_param_0(0) 
I0701 14:53:44.103564  1052 net.cpp:1851] conv1b_param_0(0.0195) 
I0701 14:53:44.103569  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:53:44.103574  1052 net.cpp:1851] res2a_branch2a_param_0(0.0195) 
I0701 14:53:44.103577  1052 net.cpp:1851] res2a_branch2b_param_0(0.0199) 
I0701 14:53:44.103581  1052 net.cpp:1851] res3a_branch2a_param_0(0.0196) 
I0701 14:53:44.103585  1052 net.cpp:1851] res3a_branch2b_param_0(0.0195) 
I0701 14:53:44.103590  1052 net.cpp:1851] res4a_branch2a_param_0(0.0196) 
I0701 14:53:44.103595  1052 net.cpp:1851] res4a_branch2b_param_0(0.0192) 
I0701 14:53:44.103600  1052 net.cpp:1851] res5a_branch2a_param_0(0.0171) 
I0701 14:53:44.103603  1052 net.cpp:1851] res5a_branch2b_param_0(0) 
I0701 14:53:44.103608  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (31510/2.3599e+06) 0.0134
I0701 14:53:44.103709  1052 solver.cpp:473] Iteration 5000, Testing net (#0)
I0701 14:53:45.744323  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.917
I0701 14:53:45.744343  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9968
I0701 14:53:45.744348  1052 solver.cpp:546]     Test net output #2: loss = 0.2084 (* 1 = 0.2084 loss)
I0701 14:53:45.764364  1052 solver.cpp:290] Iteration 5000 (26.8885 iter/s, 3.71907s/100 iter), loss = 0
I0701 14:53:45.764380  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:45.764395  1052 sgd_solver.cpp:106] Iteration 5000, lr = 0.00921875
I0701 14:53:45.765043  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.04
I0701 14:53:45.787500  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:53:47.864282  1052 solver.cpp:290] Iteration 5100 (47.6227 iter/s, 2.09984s/100 iter), loss = 0
I0701 14:53:47.864305  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:47.864311  1052 sgd_solver.cpp:106] Iteration 5100, lr = 0.00920312
I0701 14:53:49.937153  1052 solver.cpp:290] Iteration 5200 (48.2443 iter/s, 2.07278s/100 iter), loss = 0
I0701 14:53:49.937180  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:49.937188  1052 sgd_solver.cpp:106] Iteration 5200, lr = 0.0091875
I0701 14:53:52.017175  1052 solver.cpp:290] Iteration 5300 (48.0784 iter/s, 2.07994s/100 iter), loss = 0
I0701 14:53:52.017197  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:52.017204  1052 sgd_solver.cpp:106] Iteration 5300, lr = 0.00917188
I0701 14:53:54.089769  1052 solver.cpp:290] Iteration 5400 (48.2507 iter/s, 2.07251s/100 iter), loss = 0
I0701 14:53:54.089818  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:54.089828  1052 sgd_solver.cpp:106] Iteration 5400, lr = 0.00915625
I0701 14:53:56.161350  1052 solver.cpp:290] Iteration 5500 (48.2749 iter/s, 2.07147s/100 iter), loss = 0
I0701 14:53:56.161370  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:56.161378  1052 sgd_solver.cpp:106] Iteration 5500, lr = 0.00914062
I0701 14:53:58.256955  1052 solver.cpp:290] Iteration 5600 (47.7208 iter/s, 2.09552s/100 iter), loss = 0
I0701 14:53:58.256978  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:53:58.256984  1052 sgd_solver.cpp:106] Iteration 5600, lr = 0.009125
I0701 14:54:00.330370  1052 solver.cpp:290] Iteration 5700 (48.2316 iter/s, 2.07333s/100 iter), loss = 0
I0701 14:54:00.330396  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:00.330404  1052 sgd_solver.cpp:106] Iteration 5700, lr = 0.00910938
I0701 14:54:02.410126  1052 solver.cpp:290] Iteration 5800 (48.0846 iter/s, 2.07967s/100 iter), loss = 0
I0701 14:54:02.410148  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:02.410154  1052 sgd_solver.cpp:106] Iteration 5800, lr = 0.00909375
I0701 14:54:04.484351  1052 solver.cpp:290] Iteration 5900 (48.2127 iter/s, 2.07414s/100 iter), loss = 0
I0701 14:54:04.484375  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:04.484380  1052 sgd_solver.cpp:106] Iteration 5900, lr = 0.00907812
I0701 14:54:06.538925  1052 solver.cpp:354] Sparsity after update:
I0701 14:54:06.540278  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:54:06.540285  1052 net.cpp:1851] conv1a_param_0(0.0196) 
I0701 14:54:06.540292  1052 net.cpp:1851] conv1b_param_0(0.0378) 
I0701 14:54:06.540295  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:54:06.540298  1052 net.cpp:1851] res2a_branch2a_param_0(0.0394) 
I0701 14:54:06.540300  1052 net.cpp:1851] res2a_branch2b_param_0(0.0399) 
I0701 14:54:06.540302  1052 net.cpp:1851] res3a_branch2a_param_0(0.0397) 
I0701 14:54:06.540304  1052 net.cpp:1851] res3a_branch2b_param_0(0.0398) 
I0701 14:54:06.540307  1052 net.cpp:1851] res4a_branch2a_param_0(0.0394) 
I0701 14:54:06.540308  1052 net.cpp:1851] res4a_branch2b_param_0(0.0397) 
I0701 14:54:06.540310  1052 net.cpp:1851] res5a_branch2a_param_0(0.0397) 
I0701 14:54:06.540313  1052 net.cpp:1851] res5a_branch2b_param_0(0) 
I0701 14:54:06.540315  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (69939/2.3599e+06) 0.0296
I0701 14:54:06.540401  1052 solver.cpp:473] Iteration 6000, Testing net (#0)
I0701 14:54:08.183444  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9175
I0701 14:54:08.183462  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.997
I0701 14:54:08.183467  1052 solver.cpp:546]     Test net output #2: loss = 0.2068 (* 1 = 0.2068 loss)
I0701 14:54:08.203595  1052 solver.cpp:290] Iteration 6000 (26.8881 iter/s, 3.71912s/100 iter), loss = 0
I0701 14:54:08.203610  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:08.203624  1052 sgd_solver.cpp:106] Iteration 6000, lr = 0.0090625
I0701 14:54:08.204249  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.06
I0701 14:54:08.231411  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:54:10.306879  1052 solver.cpp:290] Iteration 6100 (47.5465 iter/s, 2.1032s/100 iter), loss = 0
I0701 14:54:10.306901  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:10.306908  1052 sgd_solver.cpp:106] Iteration 6100, lr = 0.00904687
I0701 14:54:12.380933  1052 solver.cpp:290] Iteration 6200 (48.2168 iter/s, 2.07397s/100 iter), loss = 0
I0701 14:54:12.380957  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:12.380965  1052 sgd_solver.cpp:106] Iteration 6200, lr = 0.00903125
I0701 14:54:14.459887  1052 solver.cpp:290] Iteration 6300 (48.1031 iter/s, 2.07887s/100 iter), loss = 0
I0701 14:54:14.459910  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:14.461437  1052 sgd_solver.cpp:106] Iteration 6300, lr = 0.00901563
I0701 14:54:16.534310  1052 solver.cpp:290] Iteration 6400 (48.2081 iter/s, 2.07434s/100 iter), loss = 0
I0701 14:54:16.534333  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:16.534340  1052 sgd_solver.cpp:106] Iteration 6400, lr = 0.009
I0701 14:54:18.611703  1052 solver.cpp:290] Iteration 6500 (48.1392 iter/s, 2.07731s/100 iter), loss = 0
I0701 14:54:18.611726  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:18.611732  1052 sgd_solver.cpp:106] Iteration 6500, lr = 0.00898437
I0701 14:54:20.683665  1052 solver.cpp:290] Iteration 6600 (48.2654 iter/s, 2.07188s/100 iter), loss = 0
I0701 14:54:20.683686  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:20.683692  1052 sgd_solver.cpp:106] Iteration 6600, lr = 0.00896875
I0701 14:54:22.757944  1052 solver.cpp:290] Iteration 6700 (48.2114 iter/s, 2.0742s/100 iter), loss = 0
I0701 14:54:22.757967  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:22.757973  1052 sgd_solver.cpp:106] Iteration 6700, lr = 0.00895312
I0701 14:54:24.833917  1052 solver.cpp:290] Iteration 6800 (48.1722 iter/s, 2.07589s/100 iter), loss = 0
I0701 14:54:24.833977  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:24.833988  1052 sgd_solver.cpp:106] Iteration 6800, lr = 0.0089375
I0701 14:54:26.917374  1052 solver.cpp:290] Iteration 6900 (47.9999 iter/s, 2.08334s/100 iter), loss = 0
I0701 14:54:26.917398  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:26.917407  1052 sgd_solver.cpp:106] Iteration 6900, lr = 0.00892187
I0701 14:54:28.970208  1052 solver.cpp:354] Sparsity after update:
I0701 14:54:28.971541  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:54:28.971549  1052 net.cpp:1851] conv1a_param_0(0.0288) 
I0701 14:54:28.971556  1052 net.cpp:1851] conv1b_param_0(0.0599) 
I0701 14:54:28.971560  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:54:28.971561  1052 net.cpp:1851] res2a_branch2a_param_0(0.059) 
I0701 14:54:28.971563  1052 net.cpp:1851] res2a_branch2b_param_0(0.0598) 
I0701 14:54:28.971566  1052 net.cpp:1851] res3a_branch2a_param_0(0.0594) 
I0701 14:54:28.971568  1052 net.cpp:1851] res3a_branch2b_param_0(0.0599) 
I0701 14:54:28.971570  1052 net.cpp:1851] res4a_branch2a_param_0(0.0583) 
I0701 14:54:28.971572  1052 net.cpp:1851] res4a_branch2b_param_0(0.0592) 
I0701 14:54:28.971575  1052 net.cpp:1851] res5a_branch2a_param_0(0.0524) 
I0701 14:54:28.971576  1052 net.cpp:1851] res5a_branch2b_param_0(0.0406) 
I0701 14:54:28.971578  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (120124/2.3599e+06) 0.0509
I0701 14:54:28.971711  1052 solver.cpp:473] Iteration 7000, Testing net (#0)
I0701 14:54:30.611065  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9186
I0701 14:54:30.611085  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9973
I0701 14:54:30.611090  1052 solver.cpp:546]     Test net output #2: loss = 0.2087 (* 1 = 0.2087 loss)
I0701 14:54:30.631289  1052 solver.cpp:290] Iteration 7000 (26.9267 iter/s, 3.71379s/100 iter), loss = 0
I0701 14:54:30.631306  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:30.631319  1052 sgd_solver.cpp:106] Iteration 7000, lr = 0.00890625
I0701 14:54:30.631958  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.08
I0701 14:54:30.669528  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:54:32.743878  1052 solver.cpp:290] Iteration 7100 (47.3371 iter/s, 2.11251s/100 iter), loss = 0
I0701 14:54:32.743901  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:32.743907  1052 sgd_solver.cpp:106] Iteration 7100, lr = 0.00889063
I0701 14:54:34.815923  1052 solver.cpp:290] Iteration 7200 (48.2635 iter/s, 2.07196s/100 iter), loss = 0
I0701 14:54:34.815945  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:34.815951  1052 sgd_solver.cpp:106] Iteration 7200, lr = 0.008875
I0701 14:54:36.890635  1052 solver.cpp:290] Iteration 7300 (48.2014 iter/s, 2.07463s/100 iter), loss = 0
I0701 14:54:36.890655  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:36.890662  1052 sgd_solver.cpp:106] Iteration 7300, lr = 0.00885937
I0701 14:54:38.969588  1052 solver.cpp:290] Iteration 7400 (48.103 iter/s, 2.07887s/100 iter), loss = 0
I0701 14:54:38.969610  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:38.969616  1052 sgd_solver.cpp:106] Iteration 7400, lr = 0.00884375
I0701 14:54:41.043184  1052 solver.cpp:290] Iteration 7500 (48.2274 iter/s, 2.07351s/100 iter), loss = 0
I0701 14:54:41.043206  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:41.043212  1052 sgd_solver.cpp:106] Iteration 7500, lr = 0.00882812
I0701 14:54:43.125360  1052 solver.cpp:290] Iteration 7600 (48.0286 iter/s, 2.08209s/100 iter), loss = 0
I0701 14:54:43.125382  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:43.125388  1052 sgd_solver.cpp:106] Iteration 7600, lr = 0.0088125
I0701 14:54:45.202054  1052 solver.cpp:290] Iteration 7700 (48.1554 iter/s, 2.07661s/100 iter), loss = 0
I0701 14:54:45.202093  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:45.202102  1052 sgd_solver.cpp:106] Iteration 7700, lr = 0.00879687
I0701 14:54:47.277709  1052 solver.cpp:290] Iteration 7800 (48.1799 iter/s, 2.07555s/100 iter), loss = 0
I0701 14:54:47.277731  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:47.277737  1052 sgd_solver.cpp:106] Iteration 7800, lr = 0.00878125
I0701 14:54:49.350203  1052 solver.cpp:290] Iteration 7900 (48.253 iter/s, 2.07241s/100 iter), loss = 0
I0701 14:54:49.350226  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:49.350234  1052 sgd_solver.cpp:106] Iteration 7900, lr = 0.00876562
I0701 14:54:51.405205  1052 solver.cpp:354] Sparsity after update:
I0701 14:54:51.406550  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:54:51.406558  1052 net.cpp:1851] conv1a_param_0(0.04) 
I0701 14:54:51.406565  1052 net.cpp:1851] conv1b_param_0(0.0799) 
I0701 14:54:51.406569  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:54:51.406570  1052 net.cpp:1851] res2a_branch2a_param_0(0.0799) 
I0701 14:54:51.406574  1052 net.cpp:1851] res2a_branch2b_param_0(0.08) 
I0701 14:54:51.406575  1052 net.cpp:1851] res3a_branch2a_param_0(0.0797) 
I0701 14:54:51.406577  1052 net.cpp:1851] res3a_branch2b_param_0(0.0799) 
I0701 14:54:51.406579  1052 net.cpp:1851] res4a_branch2a_param_0(0.0794) 
I0701 14:54:51.406581  1052 net.cpp:1851] res4a_branch2b_param_0(0.0797) 
I0701 14:54:51.406584  1052 net.cpp:1851] res5a_branch2a_param_0(0.0743) 
I0701 14:54:51.406586  1052 net.cpp:1851] res5a_branch2b_param_0(0.0513) 
I0701 14:54:51.406589  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (164447/2.3599e+06) 0.0697
I0701 14:54:51.406673  1052 solver.cpp:473] Iteration 8000, Testing net (#0)
I0701 14:54:53.047173  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9171
I0701 14:54:53.047189  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9971
I0701 14:54:53.047195  1052 solver.cpp:546]     Test net output #2: loss = 0.211 (* 1 = 0.211 loss)
I0701 14:54:53.067334  1052 solver.cpp:290] Iteration 8000 (26.9034 iter/s, 3.717s/100 iter), loss = 0
I0701 14:54:53.067351  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:53.067365  1052 sgd_solver.cpp:106] Iteration 8000, lr = 0.00875
I0701 14:54:53.068033  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.1
I0701 14:54:53.116209  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:54:55.193557  1052 solver.cpp:290] Iteration 8100 (47.0335 iter/s, 2.12614s/100 iter), loss = 0
I0701 14:54:55.193625  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:55.193634  1052 sgd_solver.cpp:106] Iteration 8100, lr = 0.00873438
I0701 14:54:57.293145  1052 solver.cpp:290] Iteration 8200 (47.6313 iter/s, 2.09946s/100 iter), loss = 0
I0701 14:54:57.293169  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:57.293174  1052 sgd_solver.cpp:106] Iteration 8200, lr = 0.00871875
I0701 14:54:59.368643  1052 solver.cpp:290] Iteration 8300 (48.1832 iter/s, 2.07541s/100 iter), loss = 0
I0701 14:54:59.368665  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:54:59.368671  1052 sgd_solver.cpp:106] Iteration 8300, lr = 0.00870312
I0701 14:55:01.444690  1052 solver.cpp:290] Iteration 8400 (48.1704 iter/s, 2.07596s/100 iter), loss = 0
I0701 14:55:01.444710  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:01.444718  1052 sgd_solver.cpp:106] Iteration 8400, lr = 0.0086875
I0701 14:55:03.514876  1052 solver.cpp:290] Iteration 8500 (48.3068 iter/s, 2.0701s/100 iter), loss = 0
I0701 14:55:03.514899  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:03.514905  1052 sgd_solver.cpp:106] Iteration 8500, lr = 0.00867188
I0701 14:55:05.587023  1052 solver.cpp:290] Iteration 8600 (48.2611 iter/s, 2.07206s/100 iter), loss = 0
I0701 14:55:05.587045  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:05.587052  1052 sgd_solver.cpp:106] Iteration 8600, lr = 0.00865625
I0701 14:55:07.660287  1052 solver.cpp:290] Iteration 8700 (48.2351 iter/s, 2.07318s/100 iter), loss = 0
I0701 14:55:07.660308  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:07.660315  1052 sgd_solver.cpp:106] Iteration 8700, lr = 0.00864062
I0701 14:55:09.739429  1052 solver.cpp:290] Iteration 8800 (48.0987 iter/s, 2.07906s/100 iter), loss = 0
I0701 14:55:09.739450  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:09.739456  1052 sgd_solver.cpp:106] Iteration 8800, lr = 0.008625
I0701 14:55:11.816689  1052 solver.cpp:290] Iteration 8900 (48.1423 iter/s, 2.07718s/100 iter), loss = 0
I0701 14:55:11.816710  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:11.816717  1052 sgd_solver.cpp:106] Iteration 8900, lr = 0.00860937
I0701 14:55:13.877259  1052 solver.cpp:354] Sparsity after update:
I0701 14:55:13.878608  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:55:13.878615  1052 net.cpp:1851] conv1a_param_0(0.0492) 
I0701 14:55:13.878623  1052 net.cpp:1851] conv1b_param_0(0.0998) 
I0701 14:55:13.878626  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:55:13.878628  1052 net.cpp:1851] res2a_branch2a_param_0(0.0998) 
I0701 14:55:13.878630  1052 net.cpp:1851] res2a_branch2b_param_0(0.0998) 
I0701 14:55:13.878633  1052 net.cpp:1851] res3a_branch2a_param_0(0.1) 
I0701 14:55:13.878634  1052 net.cpp:1851] res3a_branch2b_param_0(0.1) 
I0701 14:55:13.878636  1052 net.cpp:1851] res4a_branch2a_param_0(0.0989) 
I0701 14:55:13.878639  1052 net.cpp:1851] res4a_branch2b_param_0(0.1) 
I0701 14:55:13.878643  1052 net.cpp:1851] res5a_branch2a_param_0(0.0993) 
I0701 14:55:13.878644  1052 net.cpp:1851] res5a_branch2b_param_0(0.0982) 
I0701 14:55:13.878646  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (233121/2.3599e+06) 0.0988
I0701 14:55:13.878741  1052 solver.cpp:473] Iteration 9000, Testing net (#0)
I0701 14:55:15.519621  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9176
I0701 14:55:15.519639  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9969
I0701 14:55:15.519644  1052 solver.cpp:546]     Test net output #2: loss = 0.2036 (* 1 = 0.2036 loss)
I0701 14:55:15.539413  1052 solver.cpp:290] Iteration 9000 (26.863 iter/s, 3.7226s/100 iter), loss = 0
I0701 14:55:15.539438  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:15.539444  1052 sgd_solver.cpp:106] Iteration 9000, lr = 0.00859375
I0701 14:55:15.540091  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.12
I0701 14:55:15.590929  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:55:17.666021  1052 solver.cpp:290] Iteration 9100 (47.0252 iter/s, 2.12652s/100 iter), loss = 0
I0701 14:55:17.666045  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:17.666051  1052 sgd_solver.cpp:106] Iteration 9100, lr = 0.00857813
I0701 14:55:19.740772  1052 solver.cpp:290] Iteration 9200 (48.2006 iter/s, 2.07466s/100 iter), loss = 0
I0701 14:55:19.740797  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:19.740804  1052 sgd_solver.cpp:106] Iteration 9200, lr = 0.0085625
I0701 14:55:21.814779  1052 solver.cpp:290] Iteration 9300 (48.2179 iter/s, 2.07392s/100 iter), loss = 0
I0701 14:55:21.814801  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:21.814810  1052 sgd_solver.cpp:106] Iteration 9300, lr = 0.00854687
I0701 14:55:23.894500  1052 solver.cpp:290] Iteration 9400 (48.0853 iter/s, 2.07964s/100 iter), loss = 0
I0701 14:55:23.894521  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:23.894527  1052 sgd_solver.cpp:106] Iteration 9400, lr = 0.00853125
I0701 14:55:25.972079  1052 solver.cpp:290] Iteration 9500 (48.1349 iter/s, 2.07749s/100 iter), loss = 0
I0701 14:55:25.972133  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:25.972141  1052 sgd_solver.cpp:106] Iteration 9500, lr = 0.00851563
I0701 14:55:28.046365  1052 solver.cpp:290] Iteration 9600 (48.2121 iter/s, 2.07417s/100 iter), loss = 0
I0701 14:55:28.046406  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:28.046417  1052 sgd_solver.cpp:106] Iteration 9600, lr = 0.0085
I0701 14:55:30.120965  1052 solver.cpp:290] Iteration 9700 (48.2044 iter/s, 2.0745s/100 iter), loss = 0
I0701 14:55:30.120985  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:30.120993  1052 sgd_solver.cpp:106] Iteration 9700, lr = 0.00848437
I0701 14:55:32.197757  1052 solver.cpp:290] Iteration 9800 (48.1531 iter/s, 2.07671s/100 iter), loss = 0
I0701 14:55:32.197779  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:32.197787  1052 sgd_solver.cpp:106] Iteration 9800, lr = 0.00846875
I0701 14:55:34.272476  1052 solver.cpp:290] Iteration 9900 (48.2013 iter/s, 2.07463s/100 iter), loss = 0
I0701 14:55:34.272500  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:34.272507  1052 sgd_solver.cpp:106] Iteration 9900, lr = 0.00845312
I0701 14:55:36.324594  1052 solver.cpp:600] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/cifar10_jacintonet11v2_iter_10000.caffemodel
I0701 14:55:36.348734  1052 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/cifar10_jacintonet11v2_iter_10000.solverstate
I0701 14:55:36.356487  1052 solver.cpp:354] Sparsity after update:
I0701 14:55:36.357470  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:55:36.357481  1052 net.cpp:1851] conv1a_param_0(0.0596) 
I0701 14:55:36.357487  1052 net.cpp:1851] conv1b_param_0(0.12) 
I0701 14:55:36.357491  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:55:36.357492  1052 net.cpp:1851] res2a_branch2a_param_0(0.12) 
I0701 14:55:36.357494  1052 net.cpp:1851] res2a_branch2b_param_0(0.12) 
I0701 14:55:36.357496  1052 net.cpp:1851] res3a_branch2a_param_0(0.12) 
I0701 14:55:36.357498  1052 net.cpp:1851] res3a_branch2b_param_0(0.12) 
I0701 14:55:36.357499  1052 net.cpp:1851] res4a_branch2a_param_0(0.12) 
I0701 14:55:36.357501  1052 net.cpp:1851] res4a_branch2b_param_0(0.12) 
I0701 14:55:36.357503  1052 net.cpp:1851] res5a_branch2a_param_0(0.114) 
I0701 14:55:36.357506  1052 net.cpp:1851] res5a_branch2b_param_0(0.0679) 
I0701 14:55:36.357507  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (245001/2.3599e+06) 0.104
I0701 14:55:36.357617  1052 solver.cpp:473] Iteration 10000, Testing net (#0)
I0701 14:55:37.997452  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.918
I0701 14:55:37.997470  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.997
I0701 14:55:37.997475  1052 solver.cpp:546]     Test net output #2: loss = 0.2052 (* 1 = 0.2052 loss)
I0701 14:55:38.017151  1052 solver.cpp:290] Iteration 10000 (26.7055 iter/s, 3.74455s/100 iter), loss = 0
I0701 14:55:38.017168  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:38.017180  1052 sgd_solver.cpp:106] Iteration 10000, lr = 0.0084375
I0701 14:55:38.017810  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.14
I0701 14:55:38.068706  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:55:40.140461  1052 solver.cpp:290] Iteration 10100 (47.0981 iter/s, 2.12323s/100 iter), loss = 0
I0701 14:55:40.140482  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:40.140489  1052 sgd_solver.cpp:106] Iteration 10100, lr = 0.00842187
I0701 14:55:42.221897  1052 solver.cpp:290] Iteration 10200 (48.0457 iter/s, 2.08135s/100 iter), loss = 0
I0701 14:55:42.221920  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:42.221926  1052 sgd_solver.cpp:106] Iteration 10200, lr = 0.00840625
I0701 14:55:44.295156  1052 solver.cpp:290] Iteration 10300 (48.2352 iter/s, 2.07317s/100 iter), loss = 0
I0701 14:55:44.295202  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:44.295208  1052 sgd_solver.cpp:106] Iteration 10300, lr = 0.00839063
I0701 14:55:46.371007  1052 solver.cpp:290] Iteration 10400 (48.1754 iter/s, 2.07575s/100 iter), loss = 0
I0701 14:55:46.371031  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:46.371037  1052 sgd_solver.cpp:106] Iteration 10400, lr = 0.008375
I0701 14:55:48.444823  1052 solver.cpp:290] Iteration 10500 (48.2223 iter/s, 2.07373s/100 iter), loss = 0
I0701 14:55:48.444847  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:48.444856  1052 sgd_solver.cpp:106] Iteration 10500, lr = 0.00835937
I0701 14:55:50.518613  1052 solver.cpp:290] Iteration 10600 (48.2229 iter/s, 2.0737s/100 iter), loss = 0
I0701 14:55:50.518633  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:50.518640  1052 sgd_solver.cpp:106] Iteration 10600, lr = 0.00834375
I0701 14:55:52.591516  1052 solver.cpp:290] Iteration 10700 (48.2435 iter/s, 2.07282s/100 iter), loss = 0
I0701 14:55:52.591536  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:52.591543  1052 sgd_solver.cpp:106] Iteration 10700, lr = 0.00832812
I0701 14:55:54.666877  1052 solver.cpp:290] Iteration 10800 (48.1863 iter/s, 2.07528s/100 iter), loss = 0
I0701 14:55:54.666898  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:54.666906  1052 sgd_solver.cpp:106] Iteration 10800, lr = 0.0083125
I0701 14:55:56.741940  1052 solver.cpp:290] Iteration 10900 (48.1933 iter/s, 2.07498s/100 iter), loss = 0
I0701 14:55:56.742017  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:55:56.742024  1052 sgd_solver.cpp:106] Iteration 10900, lr = 0.00829687
I0701 14:55:58.800869  1052 solver.cpp:354] Sparsity after update:
I0701 14:55:58.802192  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:55:58.802199  1052 net.cpp:1851] conv1a_param_0(0.0696) 
I0701 14:55:58.802206  1052 net.cpp:1851] conv1b_param_0(0.14) 
I0701 14:55:58.802208  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:55:58.802211  1052 net.cpp:1851] res2a_branch2a_param_0(0.14) 
I0701 14:55:58.802212  1052 net.cpp:1851] res2a_branch2b_param_0(0.14) 
I0701 14:55:58.802214  1052 net.cpp:1851] res3a_branch2a_param_0(0.14) 
I0701 14:55:58.802217  1052 net.cpp:1851] res3a_branch2b_param_0(0.14) 
I0701 14:55:58.802218  1052 net.cpp:1851] res4a_branch2a_param_0(0.139) 
I0701 14:55:58.802220  1052 net.cpp:1851] res4a_branch2b_param_0(0.14) 
I0701 14:55:58.802222  1052 net.cpp:1851] res5a_branch2a_param_0(0.129) 
I0701 14:55:58.802224  1052 net.cpp:1851] res5a_branch2b_param_0(0.102) 
I0701 14:55:58.802225  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (293506/2.3599e+06) 0.124
I0701 14:55:58.802309  1052 solver.cpp:473] Iteration 11000, Testing net (#0)
I0701 14:56:00.443706  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9169
I0701 14:56:00.443725  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9972
I0701 14:56:00.443732  1052 solver.cpp:546]     Test net output #2: loss = 0.2104 (* 1 = 0.2104 loss)
I0701 14:56:00.463626  1052 solver.cpp:290] Iteration 11000 (26.8708 iter/s, 3.72151s/100 iter), loss = 0
I0701 14:56:00.463649  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:00.463655  1052 sgd_solver.cpp:106] Iteration 11000, lr = 0.00828125
I0701 14:56:00.464279  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.16
I0701 14:56:00.529772  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:56:02.611732  1052 solver.cpp:290] Iteration 11100 (46.5546 iter/s, 2.14802s/100 iter), loss = 0
I0701 14:56:02.611757  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:02.611764  1052 sgd_solver.cpp:106] Iteration 11100, lr = 0.00826562
I0701 14:56:04.684559  1052 solver.cpp:290] Iteration 11200 (48.2454 iter/s, 2.07274s/100 iter), loss = 0
I0701 14:56:04.684588  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:04.684597  1052 sgd_solver.cpp:106] Iteration 11200, lr = 0.00825
I0701 14:56:06.757217  1052 solver.cpp:290] Iteration 11300 (48.2494 iter/s, 2.07257s/100 iter), loss = 0
I0701 14:56:06.757243  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:06.757252  1052 sgd_solver.cpp:106] Iteration 11300, lr = 0.00823438
I0701 14:56:08.829772  1052 solver.cpp:290] Iteration 11400 (48.2517 iter/s, 2.07247s/100 iter), loss = 0
I0701 14:56:08.829792  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:08.829800  1052 sgd_solver.cpp:106] Iteration 11400, lr = 0.00821875
I0701 14:56:10.903398  1052 solver.cpp:290] Iteration 11500 (48.2266 iter/s, 2.07354s/100 iter), loss = 0
I0701 14:56:10.903420  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:10.903426  1052 sgd_solver.cpp:106] Iteration 11500, lr = 0.00820312
I0701 14:56:12.978557  1052 solver.cpp:290] Iteration 11600 (48.191 iter/s, 2.07507s/100 iter), loss = 0
I0701 14:56:12.978579  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:12.978585  1052 sgd_solver.cpp:106] Iteration 11600, lr = 0.0081875
I0701 14:56:15.048897  1052 solver.cpp:290] Iteration 11700 (48.3032 iter/s, 2.07026s/100 iter), loss = 0
I0701 14:56:15.048918  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:15.048925  1052 sgd_solver.cpp:106] Iteration 11700, lr = 0.00817188
I0701 14:56:17.120400  1052 solver.cpp:290] Iteration 11800 (48.2761 iter/s, 2.07142s/100 iter), loss = 0
I0701 14:56:17.120445  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:17.120455  1052 sgd_solver.cpp:106] Iteration 11800, lr = 0.00815625
I0701 14:56:19.196141  1052 solver.cpp:290] Iteration 11900 (48.178 iter/s, 2.07564s/100 iter), loss = 0
I0701 14:56:19.196164  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:19.196173  1052 sgd_solver.cpp:106] Iteration 11900, lr = 0.00814062
I0701 14:56:21.247362  1052 solver.cpp:354] Sparsity after update:
I0701 14:56:21.248698  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:56:21.248705  1052 net.cpp:1851] conv1a_param_0(0.0796) 
I0701 14:56:21.248713  1052 net.cpp:1851] conv1b_param_0(0.16) 
I0701 14:56:21.248718  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:56:21.248723  1052 net.cpp:1851] res2a_branch2a_param_0(0.16) 
I0701 14:56:21.248726  1052 net.cpp:1851] res2a_branch2b_param_0(0.16) 
I0701 14:56:21.248730  1052 net.cpp:1851] res3a_branch2a_param_0(0.16) 
I0701 14:56:21.248734  1052 net.cpp:1851] res3a_branch2b_param_0(0.16) 
I0701 14:56:21.248739  1052 net.cpp:1851] res4a_branch2a_param_0(0.159) 
I0701 14:56:21.248744  1052 net.cpp:1851] res4a_branch2b_param_0(0.16) 
I0701 14:56:21.248747  1052 net.cpp:1851] res5a_branch2a_param_0(0.158) 
I0701 14:56:21.248751  1052 net.cpp:1851] res5a_branch2b_param_0(0.136) 
I0701 14:56:21.248755  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (360076/2.3599e+06) 0.153
I0701 14:56:21.248890  1052 solver.cpp:473] Iteration 12000, Testing net (#0)
I0701 14:56:22.889287  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9175
I0701 14:56:22.889307  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9968
I0701 14:56:22.889312  1052 solver.cpp:546]     Test net output #2: loss = 0.2082 (* 1 = 0.2082 loss)
I0701 14:56:22.909430  1052 solver.cpp:290] Iteration 12000 (26.9312 iter/s, 3.71316s/100 iter), loss = 0
I0701 14:56:22.909446  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:22.909461  1052 sgd_solver.cpp:106] Iteration 12000, lr = 0.008125
I0701 14:56:22.910044  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.18
I0701 14:56:22.983129  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:56:25.055733  1052 solver.cpp:290] Iteration 12100 (46.5935 iter/s, 2.14622s/100 iter), loss = 0
I0701 14:56:25.055754  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:25.055761  1052 sgd_solver.cpp:106] Iteration 12100, lr = 0.00810937
I0701 14:56:27.127713  1052 solver.cpp:290] Iteration 12200 (48.265 iter/s, 2.07189s/100 iter), loss = 0
I0701 14:56:27.127843  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:27.127862  1052 sgd_solver.cpp:106] Iteration 12200, lr = 0.00809375
I0701 14:56:29.202370  1052 solver.cpp:290] Iteration 12300 (48.2051 iter/s, 2.07447s/100 iter), loss = 0
I0701 14:56:29.202395  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:29.202402  1052 sgd_solver.cpp:106] Iteration 12300, lr = 0.00807813
I0701 14:56:31.274754  1052 solver.cpp:290] Iteration 12400 (48.2557 iter/s, 2.0723s/100 iter), loss = 0
I0701 14:56:31.274775  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:31.274783  1052 sgd_solver.cpp:106] Iteration 12400, lr = 0.0080625
I0701 14:56:33.350996  1052 solver.cpp:290] Iteration 12500 (48.1658 iter/s, 2.07616s/100 iter), loss = 0
I0701 14:56:33.351019  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:33.351025  1052 sgd_solver.cpp:106] Iteration 12500, lr = 0.00804687
I0701 14:56:35.427017  1052 solver.cpp:290] Iteration 12600 (48.171 iter/s, 2.07594s/100 iter), loss = 0
I0701 14:56:35.427039  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:35.427047  1052 sgd_solver.cpp:106] Iteration 12600, lr = 0.00803125
I0701 14:56:37.502928  1052 solver.cpp:290] Iteration 12700 (48.1736 iter/s, 2.07583s/100 iter), loss = 0
I0701 14:56:37.502952  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:37.502959  1052 sgd_solver.cpp:106] Iteration 12700, lr = 0.00801562
I0701 14:56:39.575284  1052 solver.cpp:290] Iteration 12800 (48.2563 iter/s, 2.07227s/100 iter), loss = 0
I0701 14:56:39.575306  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:39.575314  1052 sgd_solver.cpp:106] Iteration 12800, lr = 0.008
I0701 14:56:41.646704  1052 solver.cpp:290] Iteration 12900 (48.278 iter/s, 2.07134s/100 iter), loss = 0
I0701 14:56:41.646728  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:41.646736  1052 sgd_solver.cpp:106] Iteration 12900, lr = 0.00798437
I0701 14:56:43.698079  1052 solver.cpp:354] Sparsity after update:
I0701 14:56:43.699419  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:56:43.699426  1052 net.cpp:1851] conv1a_param_0(0.0896) 
I0701 14:56:43.699434  1052 net.cpp:1851] conv1b_param_0(0.18) 
I0701 14:56:43.699435  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:56:43.699437  1052 net.cpp:1851] res2a_branch2a_param_0(0.18) 
I0701 14:56:43.699440  1052 net.cpp:1851] res2a_branch2b_param_0(0.18) 
I0701 14:56:43.699441  1052 net.cpp:1851] res3a_branch2a_param_0(0.18) 
I0701 14:56:43.699443  1052 net.cpp:1851] res3a_branch2b_param_0(0.18) 
I0701 14:56:43.699445  1052 net.cpp:1851] res4a_branch2a_param_0(0.18) 
I0701 14:56:43.699447  1052 net.cpp:1851] res4a_branch2b_param_0(0.18) 
I0701 14:56:43.699450  1052 net.cpp:1851] res5a_branch2a_param_0(0.179) 
I0701 14:56:43.699451  1052 net.cpp:1851] res5a_branch2b_param_0(0.179) 
I0701 14:56:43.699453  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (421543/2.3599e+06) 0.179
I0701 14:56:43.699548  1052 solver.cpp:473] Iteration 13000, Testing net (#0)
I0701 14:56:45.340271  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9169
I0701 14:56:45.340289  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9971
I0701 14:56:45.340294  1052 solver.cpp:546]     Test net output #2: loss = 0.21 (* 1 = 0.21 loss)
I0701 14:56:45.359966  1052 solver.cpp:290] Iteration 13000 (26.9314 iter/s, 3.71314s/100 iter), loss = 0
I0701 14:56:45.359984  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:45.359994  1052 sgd_solver.cpp:106] Iteration 13000, lr = 0.00796875
I0701 14:56:45.360642  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.2
I0701 14:56:45.454592  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:56:47.526427  1052 solver.cpp:290] Iteration 13100 (46.16 iter/s, 2.16638s/100 iter), loss = 0
I0701 14:56:47.526449  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:47.526470  1052 sgd_solver.cpp:106] Iteration 13100, lr = 0.00795313
I0701 14:56:49.597240  1052 solver.cpp:290] Iteration 13200 (48.2922 iter/s, 2.07073s/100 iter), loss = 0
I0701 14:56:49.597263  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:49.597272  1052 sgd_solver.cpp:106] Iteration 13200, lr = 0.0079375
I0701 14:56:51.674103  1052 solver.cpp:290] Iteration 13300 (48.1516 iter/s, 2.07678s/100 iter), loss = 0
I0701 14:56:51.674127  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:51.674134  1052 sgd_solver.cpp:106] Iteration 13300, lr = 0.00792187
I0701 14:56:53.750077  1052 solver.cpp:290] Iteration 13400 (48.1722 iter/s, 2.07589s/100 iter), loss = 0
I0701 14:56:53.750102  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:53.750108  1052 sgd_solver.cpp:106] Iteration 13400, lr = 0.00790625
I0701 14:56:55.822271  1052 solver.cpp:290] Iteration 13500 (48.26 iter/s, 2.07211s/100 iter), loss = 0
I0701 14:56:55.822294  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:55.822300  1052 sgd_solver.cpp:106] Iteration 13500, lr = 0.00789062
I0701 14:56:57.941469  1052 solver.cpp:290] Iteration 13600 (47.1896 iter/s, 2.11911s/100 iter), loss = 0
I0701 14:56:57.941531  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:56:57.941542  1052 sgd_solver.cpp:106] Iteration 13600, lr = 0.007875
I0701 14:57:00.014667  1052 solver.cpp:290] Iteration 13700 (48.2375 iter/s, 2.07308s/100 iter), loss = 0
I0701 14:57:00.014689  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:00.014696  1052 sgd_solver.cpp:106] Iteration 13700, lr = 0.00785937
I0701 14:57:02.089123  1052 solver.cpp:290] Iteration 13800 (48.2073 iter/s, 2.07437s/100 iter), loss = 0
I0701 14:57:02.089145  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:02.089153  1052 sgd_solver.cpp:106] Iteration 13800, lr = 0.00784375
I0701 14:57:04.161664  1052 solver.cpp:290] Iteration 13900 (48.2519 iter/s, 2.07246s/100 iter), loss = 0
I0701 14:57:04.161689  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:04.161695  1052 sgd_solver.cpp:106] Iteration 13900, lr = 0.00782812
I0701 14:57:06.213474  1052 solver.cpp:354] Sparsity after update:
I0701 14:57:06.214818  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:57:06.214825  1052 net.cpp:1851] conv1a_param_0(0.0996) 
I0701 14:57:06.214833  1052 net.cpp:1851] conv1b_param_0(0.2) 
I0701 14:57:06.214834  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:57:06.214836  1052 net.cpp:1851] res2a_branch2a_param_0(0.2) 
I0701 14:57:06.214838  1052 net.cpp:1851] res2a_branch2b_param_0(0.2) 
I0701 14:57:06.214840  1052 net.cpp:1851] res3a_branch2a_param_0(0.2) 
I0701 14:57:06.214843  1052 net.cpp:1851] res3a_branch2b_param_0(0.2) 
I0701 14:57:06.214844  1052 net.cpp:1851] res4a_branch2a_param_0(0.2) 
I0701 14:57:06.214845  1052 net.cpp:1851] res4a_branch2b_param_0(0.2) 
I0701 14:57:06.214848  1052 net.cpp:1851] res5a_branch2a_param_0(0.199) 
I0701 14:57:06.214849  1052 net.cpp:1851] res5a_branch2b_param_0(0.196) 
I0701 14:57:06.214851  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (466487/2.3599e+06) 0.198
I0701 14:57:06.214936  1052 solver.cpp:473] Iteration 14000, Testing net (#0)
I0701 14:57:07.857388  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9164
I0701 14:57:07.857408  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9969
I0701 14:57:07.857414  1052 solver.cpp:546]     Test net output #2: loss = 0.2086 (* 1 = 0.2086 loss)
I0701 14:57:07.877246  1052 solver.cpp:290] Iteration 14000 (26.9146 iter/s, 3.71546s/100 iter), loss = 0
I0701 14:57:07.877264  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:07.877275  1052 sgd_solver.cpp:106] Iteration 14000, lr = 0.0078125
I0701 14:57:07.877899  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.22
I0701 14:57:07.964072  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:57:10.036324  1052 solver.cpp:290] Iteration 14100 (46.3178 iter/s, 2.159s/100 iter), loss = 0
I0701 14:57:10.036347  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:10.036356  1052 sgd_solver.cpp:106] Iteration 14100, lr = 0.00779688
I0701 14:57:12.111448  1052 solver.cpp:290] Iteration 14200 (48.1919 iter/s, 2.07504s/100 iter), loss = 0
I0701 14:57:12.111471  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:12.111479  1052 sgd_solver.cpp:106] Iteration 14200, lr = 0.00778125
I0701 14:57:14.185742  1052 solver.cpp:290] Iteration 14300 (48.2112 iter/s, 2.07421s/100 iter), loss = 0
I0701 14:57:14.185770  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:14.185778  1052 sgd_solver.cpp:106] Iteration 14300, lr = 0.00776563
I0701 14:57:16.258656  1052 solver.cpp:290] Iteration 14400 (48.2433 iter/s, 2.07283s/100 iter), loss = 0
I0701 14:57:16.258677  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:16.258687  1052 sgd_solver.cpp:106] Iteration 14400, lr = 0.00775
I0701 14:57:18.329154  1052 solver.cpp:290] Iteration 14500 (48.2995 iter/s, 2.07041s/100 iter), loss = 0
I0701 14:57:18.329174  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:18.329202  1052 sgd_solver.cpp:106] Iteration 14500, lr = 0.00773437
I0701 14:57:20.403975  1052 solver.cpp:290] Iteration 14600 (48.1989 iter/s, 2.07474s/100 iter), loss = 0
I0701 14:57:20.403995  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:20.404002  1052 sgd_solver.cpp:106] Iteration 14600, lr = 0.00771875
I0701 14:57:22.479636  1052 solver.cpp:290] Iteration 14700 (48.1793 iter/s, 2.07558s/100 iter), loss = 0
I0701 14:57:22.479660  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:22.479665  1052 sgd_solver.cpp:106] Iteration 14700, lr = 0.00770312
I0701 14:57:24.552562  1052 solver.cpp:290] Iteration 14800 (48.243 iter/s, 2.07284s/100 iter), loss = 0
I0701 14:57:24.552584  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:24.552592  1052 sgd_solver.cpp:106] Iteration 14800, lr = 0.0076875
I0701 14:57:26.625819  1052 solver.cpp:290] Iteration 14900 (48.2352 iter/s, 2.07317s/100 iter), loss = 0
I0701 14:57:26.625841  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:26.625850  1052 sgd_solver.cpp:106] Iteration 14900, lr = 0.00767187
I0701 14:57:28.684590  1052 solver.cpp:354] Sparsity after update:
I0701 14:57:28.685950  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:57:28.685958  1052 net.cpp:1851] conv1a_param_0(0.11) 
I0701 14:57:28.685964  1052 net.cpp:1851] conv1b_param_0(0.22) 
I0701 14:57:28.685967  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:57:28.685969  1052 net.cpp:1851] res2a_branch2a_param_0(0.22) 
I0701 14:57:28.685971  1052 net.cpp:1851] res2a_branch2b_param_0(0.22) 
I0701 14:57:28.685973  1052 net.cpp:1851] res3a_branch2a_param_0(0.22) 
I0701 14:57:28.685976  1052 net.cpp:1851] res3a_branch2b_param_0(0.22) 
I0701 14:57:28.685977  1052 net.cpp:1851] res4a_branch2a_param_0(0.22) 
I0701 14:57:28.685978  1052 net.cpp:1851] res4a_branch2b_param_0(0.22) 
I0701 14:57:28.685981  1052 net.cpp:1851] res5a_branch2a_param_0(0.217) 
I0701 14:57:28.685982  1052 net.cpp:1851] res5a_branch2b_param_0(0.216) 
I0701 14:57:28.685984  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (511513/2.3599e+06) 0.217
I0701 14:57:28.686108  1052 solver.cpp:473] Iteration 15000, Testing net (#0)
I0701 14:57:30.326439  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9153
I0701 14:57:30.326458  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9969
I0701 14:57:30.326463  1052 solver.cpp:546]     Test net output #2: loss = 0.2154 (* 1 = 0.2154 loss)
I0701 14:57:30.346267  1052 solver.cpp:290] Iteration 15000 (26.8794 iter/s, 3.72032s/100 iter), loss = 0
I0701 14:57:30.346284  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:30.346295  1052 sgd_solver.cpp:106] Iteration 15000, lr = 0.00765625
I0701 14:57:30.346925  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.24
I0701 14:57:30.442493  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:57:32.523124  1052 solver.cpp:290] Iteration 15100 (45.9395 iter/s, 2.17678s/100 iter), loss = 0
I0701 14:57:32.523147  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:32.523154  1052 sgd_solver.cpp:106] Iteration 15100, lr = 0.00764062
I0701 14:57:34.598183  1052 solver.cpp:290] Iteration 15200 (48.1934 iter/s, 2.07497s/100 iter), loss = 0
I0701 14:57:34.598206  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:34.598212  1052 sgd_solver.cpp:106] Iteration 15200, lr = 0.007625
I0701 14:57:36.670677  1052 solver.cpp:290] Iteration 15300 (48.253 iter/s, 2.07241s/100 iter), loss = 0
I0701 14:57:36.670699  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:36.670706  1052 sgd_solver.cpp:106] Iteration 15300, lr = 0.00760937
I0701 14:57:38.743134  1052 solver.cpp:290] Iteration 15400 (48.2539 iter/s, 2.07237s/100 iter), loss = 0
I0701 14:57:38.743155  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:38.743162  1052 sgd_solver.cpp:106] Iteration 15400, lr = 0.00759375
I0701 14:57:40.814564  1052 solver.cpp:290] Iteration 15500 (48.2778 iter/s, 2.07134s/100 iter), loss = 0
I0701 14:57:40.814597  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:40.814609  1052 sgd_solver.cpp:106] Iteration 15500, lr = 0.00757812
I0701 14:57:42.886926  1052 solver.cpp:290] Iteration 15600 (48.2563 iter/s, 2.07227s/100 iter), loss = 0
I0701 14:57:42.886950  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:42.886956  1052 sgd_solver.cpp:106] Iteration 15600, lr = 0.0075625
I0701 14:57:44.958629  1052 solver.cpp:290] Iteration 15700 (48.2714 iter/s, 2.07162s/100 iter), loss = 0
I0701 14:57:44.958652  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:44.958657  1052 sgd_solver.cpp:106] Iteration 15700, lr = 0.00754687
I0701 14:57:47.032946  1052 solver.cpp:290] Iteration 15800 (48.2106 iter/s, 2.07423s/100 iter), loss = 0
I0701 14:57:47.032968  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:47.032975  1052 sgd_solver.cpp:106] Iteration 15800, lr = 0.00753125
I0701 14:57:49.102716  1052 solver.cpp:290] Iteration 15900 (48.3165 iter/s, 2.06969s/100 iter), loss = 0
I0701 14:57:49.102757  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:49.102767  1052 sgd_solver.cpp:106] Iteration 15900, lr = 0.00751562
I0701 14:57:51.158587  1052 solver.cpp:354] Sparsity after update:
I0701 14:57:51.159782  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:57:51.159790  1052 net.cpp:1851] conv1a_param_0(0.12) 
I0701 14:57:51.159797  1052 net.cpp:1851] conv1b_param_0(0.24) 
I0701 14:57:51.159799  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:57:51.159802  1052 net.cpp:1851] res2a_branch2a_param_0(0.24) 
I0701 14:57:51.159804  1052 net.cpp:1851] res2a_branch2b_param_0(0.24) 
I0701 14:57:51.159806  1052 net.cpp:1851] res3a_branch2a_param_0(0.24) 
I0701 14:57:51.159809  1052 net.cpp:1851] res3a_branch2b_param_0(0.24) 
I0701 14:57:51.159811  1052 net.cpp:1851] res4a_branch2a_param_0(0.24) 
I0701 14:57:51.159813  1052 net.cpp:1851] res4a_branch2b_param_0(0.24) 
I0701 14:57:51.159816  1052 net.cpp:1851] res5a_branch2a_param_0(0.237) 
I0701 14:57:51.159818  1052 net.cpp:1851] res5a_branch2b_param_0(0.234) 
I0701 14:57:51.159821  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (557643/2.3599e+06) 0.236
I0701 14:57:51.159904  1052 solver.cpp:473] Iteration 16000, Testing net (#0)
I0701 14:57:52.799790  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9166
I0701 14:57:52.799809  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9969
I0701 14:57:52.799813  1052 solver.cpp:546]     Test net output #2: loss = 0.2124 (* 1 = 0.2124 loss)
I0701 14:57:52.820827  1052 solver.cpp:290] Iteration 16000 (26.8964 iter/s, 3.71797s/100 iter), loss = 0
I0701 14:57:52.820844  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:52.820855  1052 sgd_solver.cpp:106] Iteration 16000, lr = 0.0075
I0701 14:57:52.821511  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.26
I0701 14:57:52.916638  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:57:54.997517  1052 solver.cpp:290] Iteration 16100 (45.943 iter/s, 2.17661s/100 iter), loss = 0
I0701 14:57:54.997540  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:54.997546  1052 sgd_solver.cpp:106] Iteration 16100, lr = 0.00748438
I0701 14:57:57.121526  1052 solver.cpp:290] Iteration 16200 (47.0827 iter/s, 2.12392s/100 iter), loss = 0
I0701 14:57:57.121547  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:57.121554  1052 sgd_solver.cpp:106] Iteration 16200, lr = 0.00746875
I0701 14:57:59.196969  1052 solver.cpp:290] Iteration 16300 (48.1844 iter/s, 2.07536s/100 iter), loss = 0
I0701 14:57:59.197046  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:57:59.197054  1052 sgd_solver.cpp:106] Iteration 16300, lr = 0.00745312
I0701 14:58:01.268970  1052 solver.cpp:290] Iteration 16400 (48.2657 iter/s, 2.07186s/100 iter), loss = 0
I0701 14:58:01.268991  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:01.268998  1052 sgd_solver.cpp:106] Iteration 16400, lr = 0.0074375
I0701 14:58:03.341953  1052 solver.cpp:290] Iteration 16500 (48.2416 iter/s, 2.0729s/100 iter), loss = 0
I0701 14:58:03.341975  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:03.341982  1052 sgd_solver.cpp:106] Iteration 16500, lr = 0.00742187
I0701 14:58:05.417023  1052 solver.cpp:290] Iteration 16600 (48.1931 iter/s, 2.07499s/100 iter), loss = 0
I0701 14:58:05.417047  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:05.417052  1052 sgd_solver.cpp:106] Iteration 16600, lr = 0.00740625
I0701 14:58:07.490671  1052 solver.cpp:290] Iteration 16700 (48.2262 iter/s, 2.07356s/100 iter), loss = 0
I0701 14:58:07.490694  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:07.490702  1052 sgd_solver.cpp:106] Iteration 16700, lr = 0.00739062
I0701 14:58:09.565937  1052 solver.cpp:290] Iteration 16800 (48.1886 iter/s, 2.07518s/100 iter), loss = 0
I0701 14:58:09.565958  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:09.565964  1052 sgd_solver.cpp:106] Iteration 16800, lr = 0.007375
I0701 14:58:11.639068  1052 solver.cpp:290] Iteration 16900 (48.2381 iter/s, 2.07305s/100 iter), loss = 0
I0701 14:58:11.639091  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:11.639097  1052 sgd_solver.cpp:106] Iteration 16900, lr = 0.00735937
I0701 14:58:13.696557  1052 solver.cpp:354] Sparsity after update:
I0701 14:58:13.697916  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:58:13.697922  1052 net.cpp:1851] conv1a_param_0(0.13) 
I0701 14:58:13.697931  1052 net.cpp:1851] conv1b_param_0(0.26) 
I0701 14:58:13.697932  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:58:13.697934  1052 net.cpp:1851] res2a_branch2a_param_0(0.26) 
I0701 14:58:13.697937  1052 net.cpp:1851] res2a_branch2b_param_0(0.26) 
I0701 14:58:13.697940  1052 net.cpp:1851] res3a_branch2a_param_0(0.26) 
I0701 14:58:13.697942  1052 net.cpp:1851] res3a_branch2b_param_0(0.26) 
I0701 14:58:13.697944  1052 net.cpp:1851] res4a_branch2a_param_0(0.26) 
I0701 14:58:13.697947  1052 net.cpp:1851] res4a_branch2b_param_0(0.26) 
I0701 14:58:13.697948  1052 net.cpp:1851] res5a_branch2a_param_0(0.259) 
I0701 14:58:13.697950  1052 net.cpp:1851] res5a_branch2b_param_0(0.247) 
I0701 14:58:13.697952  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (602813/2.3599e+06) 0.255
I0701 14:58:13.698036  1052 solver.cpp:473] Iteration 17000, Testing net (#0)
I0701 14:58:15.338090  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9174
I0701 14:58:15.338109  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.997
I0701 14:58:15.338114  1052 solver.cpp:546]     Test net output #2: loss = 0.2128 (* 1 = 0.2128 loss)
I0701 14:58:15.357781  1052 solver.cpp:290] Iteration 17000 (26.8919 iter/s, 3.71859s/100 iter), loss = 0
I0701 14:58:15.357798  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:15.357812  1052 sgd_solver.cpp:106] Iteration 17000, lr = 0.00734375
I0701 14:58:15.358429  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.28
I0701 14:58:15.465633  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:58:17.539340  1052 solver.cpp:290] Iteration 17100 (45.8405 iter/s, 2.18148s/100 iter), loss = 0
I0701 14:58:17.539364  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:17.539372  1052 sgd_solver.cpp:106] Iteration 17100, lr = 0.00732813
I0701 14:58:19.617242  1052 solver.cpp:290] Iteration 17200 (48.1274 iter/s, 2.07782s/100 iter), loss = 0
I0701 14:58:19.617280  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:19.617287  1052 sgd_solver.cpp:106] Iteration 17200, lr = 0.0073125
I0701 14:58:21.690240  1052 solver.cpp:290] Iteration 17300 (48.2416 iter/s, 2.0729s/100 iter), loss = 0
I0701 14:58:21.690263  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:21.690270  1052 sgd_solver.cpp:106] Iteration 17300, lr = 0.00729688
I0701 14:58:23.760582  1052 solver.cpp:290] Iteration 17400 (48.3032 iter/s, 2.07026s/100 iter), loss = 0
I0701 14:58:23.760604  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:23.760610  1052 sgd_solver.cpp:106] Iteration 17400, lr = 0.00728125
I0701 14:58:25.831326  1052 solver.cpp:290] Iteration 17500 (48.2938 iter/s, 2.07066s/100 iter), loss = 0
I0701 14:58:25.831348  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:25.831354  1052 sgd_solver.cpp:106] Iteration 17500, lr = 0.00726563
I0701 14:58:27.905176  1052 solver.cpp:290] Iteration 17600 (48.2215 iter/s, 2.07376s/100 iter), loss = 0
I0701 14:58:27.905199  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:27.905205  1052 sgd_solver.cpp:106] Iteration 17600, lr = 0.00725
I0701 14:58:29.979594  1052 solver.cpp:290] Iteration 17700 (48.2083 iter/s, 2.07433s/100 iter), loss = 0
I0701 14:58:29.979653  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:29.979661  1052 sgd_solver.cpp:106] Iteration 17700, lr = 0.00723437
I0701 14:58:32.053045  1052 solver.cpp:290] Iteration 17800 (48.2316 iter/s, 2.07333s/100 iter), loss = 0
I0701 14:58:32.053066  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:32.053073  1052 sgd_solver.cpp:106] Iteration 17800, lr = 0.00721875
I0701 14:58:34.126571  1052 solver.cpp:290] Iteration 17900 (48.229 iter/s, 2.07344s/100 iter), loss = 0
I0701 14:58:34.126605  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:34.126612  1052 sgd_solver.cpp:106] Iteration 17900, lr = 0.00720312
I0701 14:58:36.177155  1052 solver.cpp:354] Sparsity after update:
I0701 14:58:36.178555  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:58:36.178565  1052 net.cpp:1851] conv1a_param_0(0.14) 
I0701 14:58:36.178572  1052 net.cpp:1851] conv1b_param_0(0.28) 
I0701 14:58:36.178577  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:58:36.178582  1052 net.cpp:1851] res2a_branch2a_param_0(0.28) 
I0701 14:58:36.178586  1052 net.cpp:1851] res2a_branch2b_param_0(0.28) 
I0701 14:58:36.178591  1052 net.cpp:1851] res3a_branch2a_param_0(0.28) 
I0701 14:58:36.178596  1052 net.cpp:1851] res3a_branch2b_param_0(0.28) 
I0701 14:58:36.178601  1052 net.cpp:1851] res4a_branch2a_param_0(0.28) 
I0701 14:58:36.178606  1052 net.cpp:1851] res4a_branch2b_param_0(0.28) 
I0701 14:58:36.178609  1052 net.cpp:1851] res5a_branch2a_param_0(0.278) 
I0701 14:58:36.178613  1052 net.cpp:1851] res5a_branch2b_param_0(0.274) 
I0701 14:58:36.178618  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (652432/2.3599e+06) 0.276
I0701 14:58:36.178719  1052 solver.cpp:473] Iteration 18000, Testing net (#0)
I0701 14:58:37.827147  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9159
I0701 14:58:37.827167  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.997
I0701 14:58:37.827172  1052 solver.cpp:546]     Test net output #2: loss = 0.2133 (* 1 = 0.2133 loss)
I0701 14:58:37.846947  1052 solver.cpp:290] Iteration 18000 (26.88 iter/s, 3.72024s/100 iter), loss = 0
I0701 14:58:37.846963  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:37.846977  1052 sgd_solver.cpp:106] Iteration 18000, lr = 0.0071875
I0701 14:58:37.847612  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.3
I0701 14:58:37.966275  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:58:40.040916  1052 solver.cpp:290] Iteration 18100 (45.5812 iter/s, 2.19389s/100 iter), loss = 0
I0701 14:58:40.040938  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:40.040946  1052 sgd_solver.cpp:106] Iteration 18100, lr = 0.00717187
I0701 14:58:42.114106  1052 solver.cpp:290] Iteration 18200 (48.2368 iter/s, 2.07311s/100 iter), loss = 0
I0701 14:58:42.114127  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:42.114135  1052 sgd_solver.cpp:106] Iteration 18200, lr = 0.00715625
I0701 14:58:44.191133  1052 solver.cpp:290] Iteration 18300 (48.1477 iter/s, 2.07694s/100 iter), loss = 0
I0701 14:58:44.191155  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:44.191161  1052 sgd_solver.cpp:106] Iteration 18300, lr = 0.00714062
I0701 14:58:46.265734  1052 solver.cpp:290] Iteration 18400 (48.2041 iter/s, 2.07451s/100 iter), loss = 0
I0701 14:58:46.265763  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:46.265771  1052 sgd_solver.cpp:106] Iteration 18400, lr = 0.007125
I0701 14:58:48.339233  1052 solver.cpp:290] Iteration 18500 (48.2297 iter/s, 2.07341s/100 iter), loss = 0
I0701 14:58:48.339253  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:48.339262  1052 sgd_solver.cpp:106] Iteration 18500, lr = 0.00710937
I0701 14:58:50.413298  1052 solver.cpp:290] Iteration 18600 (48.2164 iter/s, 2.07398s/100 iter), loss = 0
I0701 14:58:50.413321  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:50.413347  1052 sgd_solver.cpp:106] Iteration 18600, lr = 0.00709375
I0701 14:58:52.491051  1052 solver.cpp:290] Iteration 18700 (48.1309 iter/s, 2.07767s/100 iter), loss = 0
I0701 14:58:52.491073  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:52.491080  1052 sgd_solver.cpp:106] Iteration 18700, lr = 0.00707812
I0701 14:58:54.567090  1052 solver.cpp:290] Iteration 18800 (48.1706 iter/s, 2.07596s/100 iter), loss = 0
I0701 14:58:54.567111  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:54.567118  1052 sgd_solver.cpp:106] Iteration 18800, lr = 0.0070625
I0701 14:58:56.638250  1052 solver.cpp:290] Iteration 18900 (48.2841 iter/s, 2.07108s/100 iter), loss = 0
I0701 14:58:56.638273  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:58:56.638278  1052 sgd_solver.cpp:106] Iteration 18900, lr = 0.00704687
I0701 14:58:58.712405  1052 solver.cpp:354] Sparsity after update:
I0701 14:58:58.713758  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:58:58.713766  1052 net.cpp:1851] conv1a_param_0(0.15) 
I0701 14:58:58.713773  1052 net.cpp:1851] conv1b_param_0(0.3) 
I0701 14:58:58.713775  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:58:58.713778  1052 net.cpp:1851] res2a_branch2a_param_0(0.3) 
I0701 14:58:58.713779  1052 net.cpp:1851] res2a_branch2b_param_0(0.3) 
I0701 14:58:58.713781  1052 net.cpp:1851] res3a_branch2a_param_0(0.3) 
I0701 14:58:58.713783  1052 net.cpp:1851] res3a_branch2b_param_0(0.3) 
I0701 14:58:58.713785  1052 net.cpp:1851] res4a_branch2a_param_0(0.3) 
I0701 14:58:58.713788  1052 net.cpp:1851] res4a_branch2b_param_0(0.3) 
I0701 14:58:58.713789  1052 net.cpp:1851] res5a_branch2a_param_0(0.298) 
I0701 14:58:58.713791  1052 net.cpp:1851] res5a_branch2b_param_0(0.3) 
I0701 14:58:58.713793  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (704160/2.3599e+06) 0.298
I0701 14:58:58.713886  1052 solver.cpp:473] Iteration 19000, Testing net (#0)
I0701 14:59:00.355278  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9145
I0701 14:59:00.355372  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9967
I0701 14:59:00.355379  1052 solver.cpp:546]     Test net output #2: loss = 0.2196 (* 1 = 0.2196 loss)
I0701 14:59:00.375772  1052 solver.cpp:290] Iteration 19000 (26.7566 iter/s, 3.7374s/100 iter), loss = 0
I0701 14:59:00.375789  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:00.375802  1052 sgd_solver.cpp:106] Iteration 19000, lr = 0.00703125
I0701 14:59:00.376437  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.32
I0701 14:59:00.497370  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:59:02.576351  1052 solver.cpp:290] Iteration 19100 (45.4443 iter/s, 2.2005s/100 iter), loss = 0
I0701 14:59:02.576373  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:02.576380  1052 sgd_solver.cpp:106] Iteration 19100, lr = 0.00701563
I0701 14:59:04.647954  1052 solver.cpp:290] Iteration 19200 (48.2738 iter/s, 2.07152s/100 iter), loss = 0
I0701 14:59:04.647976  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:04.647984  1052 sgd_solver.cpp:106] Iteration 19200, lr = 0.007
I0701 14:59:06.723918  1052 solver.cpp:290] Iteration 19300 (48.1723 iter/s, 2.07588s/100 iter), loss = 0
I0701 14:59:06.723942  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:06.723948  1052 sgd_solver.cpp:106] Iteration 19300, lr = 0.00698437
I0701 14:59:08.796815  1052 solver.cpp:290] Iteration 19400 (48.2436 iter/s, 2.07281s/100 iter), loss = 0
I0701 14:59:08.796838  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:08.796844  1052 sgd_solver.cpp:106] Iteration 19400, lr = 0.00696875
I0701 14:59:10.869602  1052 solver.cpp:290] Iteration 19500 (48.2462 iter/s, 2.0727s/100 iter), loss = 0
I0701 14:59:10.869626  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:10.869632  1052 sgd_solver.cpp:106] Iteration 19500, lr = 0.00695312
I0701 14:59:12.942423  1052 solver.cpp:290] Iteration 19600 (48.2454 iter/s, 2.07274s/100 iter), loss = 0
I0701 14:59:12.942445  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:12.942451  1052 sgd_solver.cpp:106] Iteration 19600, lr = 0.0069375
I0701 14:59:15.018015  1052 solver.cpp:290] Iteration 19700 (48.181 iter/s, 2.07551s/100 iter), loss = 0
I0701 14:59:15.018038  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:15.018044  1052 sgd_solver.cpp:106] Iteration 19700, lr = 0.00692187
I0701 14:59:17.094257  1052 solver.cpp:290] Iteration 19800 (48.1659 iter/s, 2.07616s/100 iter), loss = 0
I0701 14:59:17.094277  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:17.094286  1052 sgd_solver.cpp:106] Iteration 19800, lr = 0.00690625
I0701 14:59:19.167104  1052 solver.cpp:290] Iteration 19900 (48.2448 iter/s, 2.07276s/100 iter), loss = 0
I0701 14:59:19.167125  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:19.167131  1052 sgd_solver.cpp:106] Iteration 19900, lr = 0.00689062
I0701 14:59:21.221900  1052 solver.cpp:600] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/cifar10_jacintonet11v2_iter_20000.caffemodel
I0701 14:59:21.238241  1052 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/cifar10_jacintonet11v2_iter_20000.solverstate
I0701 14:59:21.245537  1052 solver.cpp:354] Sparsity after update:
I0701 14:59:21.246471  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:59:21.246479  1052 net.cpp:1851] conv1a_param_0(0.16) 
I0701 14:59:21.246486  1052 net.cpp:1851] conv1b_param_0(0.32) 
I0701 14:59:21.246490  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:59:21.246491  1052 net.cpp:1851] res2a_branch2a_param_0(0.32) 
I0701 14:59:21.246493  1052 net.cpp:1851] res2a_branch2b_param_0(0.32) 
I0701 14:59:21.246495  1052 net.cpp:1851] res3a_branch2a_param_0(0.32) 
I0701 14:59:21.246505  1052 net.cpp:1851] res3a_branch2b_param_0(0.32) 
I0701 14:59:21.246507  1052 net.cpp:1851] res4a_branch2a_param_0(0.32) 
I0701 14:59:21.246510  1052 net.cpp:1851] res4a_branch2b_param_0(0.32) 
I0701 14:59:21.246511  1052 net.cpp:1851] res5a_branch2a_param_0(0.319) 
I0701 14:59:21.246513  1052 net.cpp:1851] res5a_branch2b_param_0(0.314) 
I0701 14:59:21.246515  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (747795/2.3599e+06) 0.317
I0701 14:59:21.246610  1052 solver.cpp:473] Iteration 20000, Testing net (#0)
I0701 14:59:22.885303  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9137
I0701 14:59:22.885321  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9965
I0701 14:59:22.885326  1052 solver.cpp:546]     Test net output #2: loss = 0.2236 (* 1 = 0.2236 loss)
I0701 14:59:22.904932  1052 solver.cpp:290] Iteration 20000 (26.7544 iter/s, 3.7377s/100 iter), loss = 0
I0701 14:59:22.904952  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:22.904963  1052 sgd_solver.cpp:106] Iteration 20000, lr = 0.006875
I0701 14:59:22.905817  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.34
I0701 14:59:23.034603  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:59:25.113517  1052 solver.cpp:290] Iteration 20100 (45.2797 iter/s, 2.2085s/100 iter), loss = 0
I0701 14:59:25.113540  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:25.113548  1052 sgd_solver.cpp:106] Iteration 20100, lr = 0.00685938
I0701 14:59:27.186848  1052 solver.cpp:290] Iteration 20200 (48.2335 iter/s, 2.07325s/100 iter), loss = 0
I0701 14:59:27.186870  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:27.186877  1052 sgd_solver.cpp:106] Iteration 20200, lr = 0.00684375
I0701 14:59:29.263243  1052 solver.cpp:290] Iteration 20300 (48.1624 iter/s, 2.07631s/100 iter), loss = 0
I0701 14:59:29.263264  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:29.263272  1052 sgd_solver.cpp:106] Iteration 20300, lr = 0.00682813
I0701 14:59:31.337349  1052 solver.cpp:290] Iteration 20400 (48.2155 iter/s, 2.07402s/100 iter), loss = 0
I0701 14:59:31.337456  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:31.337467  1052 sgd_solver.cpp:106] Iteration 20400, lr = 0.0068125
I0701 14:59:33.408808  1052 solver.cpp:290] Iteration 20500 (48.279 iter/s, 2.07129s/100 iter), loss = 0
I0701 14:59:33.408829  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:33.408838  1052 sgd_solver.cpp:106] Iteration 20500, lr = 0.00679688
I0701 14:59:35.482383  1052 solver.cpp:290] Iteration 20600 (48.2279 iter/s, 2.07349s/100 iter), loss = 0
I0701 14:59:35.482409  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:35.482416  1052 sgd_solver.cpp:106] Iteration 20600, lr = 0.00678125
I0701 14:59:37.553769  1052 solver.cpp:290] Iteration 20700 (48.2789 iter/s, 2.0713s/100 iter), loss = 0
I0701 14:59:37.553791  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:37.553797  1052 sgd_solver.cpp:106] Iteration 20700, lr = 0.00676562
I0701 14:59:39.626327  1052 solver.cpp:290] Iteration 20800 (48.2515 iter/s, 2.07247s/100 iter), loss = 0
I0701 14:59:39.626349  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:39.626358  1052 sgd_solver.cpp:106] Iteration 20800, lr = 0.00675
I0701 14:59:41.699617  1052 solver.cpp:290] Iteration 20900 (48.2345 iter/s, 2.07321s/100 iter), loss = 0
I0701 14:59:41.699638  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:41.699645  1052 sgd_solver.cpp:106] Iteration 20900, lr = 0.00673437
I0701 14:59:43.755303  1052 solver.cpp:354] Sparsity after update:
I0701 14:59:43.756633  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 14:59:43.756639  1052 net.cpp:1851] conv1a_param_0(0.17) 
I0701 14:59:43.756646  1052 net.cpp:1851] conv1b_param_0(0.34) 
I0701 14:59:43.756649  1052 net.cpp:1851] fc10_param_0(0) 
I0701 14:59:43.756650  1052 net.cpp:1851] res2a_branch2a_param_0(0.34) 
I0701 14:59:43.756652  1052 net.cpp:1851] res2a_branch2b_param_0(0.34) 
I0701 14:59:43.756654  1052 net.cpp:1851] res3a_branch2a_param_0(0.34) 
I0701 14:59:43.756656  1052 net.cpp:1851] res3a_branch2b_param_0(0.34) 
I0701 14:59:43.756659  1052 net.cpp:1851] res4a_branch2a_param_0(0.34) 
I0701 14:59:43.756659  1052 net.cpp:1851] res4a_branch2b_param_0(0.34) 
I0701 14:59:43.756661  1052 net.cpp:1851] res5a_branch2a_param_0(0.339) 
I0701 14:59:43.756664  1052 net.cpp:1851] res5a_branch2b_param_0(0.339) 
I0701 14:59:43.756665  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (797937/2.3599e+06) 0.338
I0701 14:59:43.756757  1052 solver.cpp:473] Iteration 21000, Testing net (#0)
I0701 14:59:45.397343  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9157
I0701 14:59:45.397362  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9969
I0701 14:59:45.397368  1052 solver.cpp:546]     Test net output #2: loss = 0.2201 (* 1 = 0.2201 loss)
I0701 14:59:45.418704  1052 solver.cpp:290] Iteration 21000 (26.8892 iter/s, 3.71896s/100 iter), loss = 0
I0701 14:59:45.418725  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:45.418732  1052 sgd_solver.cpp:106] Iteration 21000, lr = 0.00671875
I0701 14:59:45.419602  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.36
I0701 14:59:45.562152  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 14:59:47.637367  1052 solver.cpp:290] Iteration 21100 (45.074 iter/s, 2.21858s/100 iter), loss = 0
I0701 14:59:47.637387  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:47.637393  1052 sgd_solver.cpp:106] Iteration 21100, lr = 0.00670313
I0701 14:59:49.707245  1052 solver.cpp:290] Iteration 21200 (48.3139 iter/s, 2.0698s/100 iter), loss = 0
I0701 14:59:49.707267  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:49.707273  1052 sgd_solver.cpp:106] Iteration 21200, lr = 0.0066875
I0701 14:59:51.781184  1052 solver.cpp:290] Iteration 21300 (48.2194 iter/s, 2.07385s/100 iter), loss = 0
I0701 14:59:51.781205  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:51.781230  1052 sgd_solver.cpp:106] Iteration 21300, lr = 0.00667187
I0701 14:59:53.852942  1052 solver.cpp:290] Iteration 21400 (48.2701 iter/s, 2.07167s/100 iter), loss = 0
I0701 14:59:53.852967  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:53.852975  1052 sgd_solver.cpp:106] Iteration 21400, lr = 0.00665625
I0701 14:59:55.925015  1052 solver.cpp:290] Iteration 21500 (48.2629 iter/s, 2.07199s/100 iter), loss = 0
I0701 14:59:55.925038  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:55.925047  1052 sgd_solver.cpp:106] Iteration 21500, lr = 0.00664062
I0701 14:59:58.001672  1052 solver.cpp:290] Iteration 21600 (48.1563 iter/s, 2.07657s/100 iter), loss = 0
I0701 14:59:58.001694  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 14:59:58.001700  1052 sgd_solver.cpp:106] Iteration 21600, lr = 0.006625
I0701 15:00:00.070660  1052 solver.cpp:290] Iteration 21700 (48.3348 iter/s, 2.0689s/100 iter), loss = 0
I0701 15:00:00.070683  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:00.070689  1052 sgd_solver.cpp:106] Iteration 21700, lr = 0.00660937
I0701 15:00:02.143307  1052 solver.cpp:290] Iteration 21800 (48.2494 iter/s, 2.07256s/100 iter), loss = 0
I0701 15:00:02.143383  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:02.143390  1052 sgd_solver.cpp:106] Iteration 21800, lr = 0.00659375
I0701 15:00:04.218446  1052 solver.cpp:290] Iteration 21900 (48.1927 iter/s, 2.075s/100 iter), loss = 0
I0701 15:00:04.218466  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:04.218472  1052 sgd_solver.cpp:106] Iteration 21900, lr = 0.00657812
I0701 15:00:06.274930  1052 solver.cpp:354] Sparsity after update:
I0701 15:00:06.276278  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:00:06.276285  1052 net.cpp:1851] conv1a_param_0(0.18) 
I0701 15:00:06.276293  1052 net.cpp:1851] conv1b_param_0(0.36) 
I0701 15:00:06.276295  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:00:06.276298  1052 net.cpp:1851] res2a_branch2a_param_0(0.36) 
I0701 15:00:06.276299  1052 net.cpp:1851] res2a_branch2b_param_0(0.36) 
I0701 15:00:06.276302  1052 net.cpp:1851] res3a_branch2a_param_0(0.36) 
I0701 15:00:06.276304  1052 net.cpp:1851] res3a_branch2b_param_0(0.36) 
I0701 15:00:06.276306  1052 net.cpp:1851] res4a_branch2a_param_0(0.36) 
I0701 15:00:06.276309  1052 net.cpp:1851] res4a_branch2b_param_0(0.36) 
I0701 15:00:06.276310  1052 net.cpp:1851] res5a_branch2a_param_0(0.358) 
I0701 15:00:06.276312  1052 net.cpp:1851] res5a_branch2b_param_0(0.356) 
I0701 15:00:06.276321  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (842397/2.3599e+06) 0.357
I0701 15:00:06.276407  1052 solver.cpp:473] Iteration 22000, Testing net (#0)
I0701 15:00:07.915347  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9132
I0701 15:00:07.915366  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9965
I0701 15:00:07.915371  1052 solver.cpp:546]     Test net output #2: loss = 0.2287 (* 1 = 0.2287 loss)
I0701 15:00:07.934985  1052 solver.cpp:290] Iteration 22000 (26.9076 iter/s, 3.71642s/100 iter), loss = 0
I0701 15:00:07.935001  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:07.935016  1052 sgd_solver.cpp:106] Iteration 22000, lr = 0.0065625
I0701 15:00:07.935637  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.38
I0701 15:00:08.096724  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:00:10.171809  1052 solver.cpp:290] Iteration 22100 (44.708 iter/s, 2.23674s/100 iter), loss = 0
I0701 15:00:10.171842  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:10.171854  1052 sgd_solver.cpp:106] Iteration 22100, lr = 0.00654687
I0701 15:00:12.242908  1052 solver.cpp:290] Iteration 22200 (48.2857 iter/s, 2.07101s/100 iter), loss = 0
I0701 15:00:12.242929  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:12.242935  1052 sgd_solver.cpp:106] Iteration 22200, lr = 0.00653125
I0701 15:00:14.314033  1052 solver.cpp:290] Iteration 22300 (48.2849 iter/s, 2.07104s/100 iter), loss = 0
I0701 15:00:14.314054  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:14.314062  1052 sgd_solver.cpp:106] Iteration 22300, lr = 0.00651562
I0701 15:00:16.388934  1052 solver.cpp:290] Iteration 22400 (48.1971 iter/s, 2.07481s/100 iter), loss = 0
I0701 15:00:16.388957  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:16.388964  1052 sgd_solver.cpp:106] Iteration 22400, lr = 0.0065
I0701 15:00:18.463380  1052 solver.cpp:290] Iteration 22500 (48.2076 iter/s, 2.07436s/100 iter), loss = 0
I0701 15:00:18.463400  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:18.463409  1052 sgd_solver.cpp:106] Iteration 22500, lr = 0.00648437
I0701 15:00:20.539101  1052 solver.cpp:290] Iteration 22600 (48.1779 iter/s, 2.07564s/100 iter), loss = 0
I0701 15:00:20.539122  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:20.539129  1052 sgd_solver.cpp:106] Iteration 22600, lr = 0.00646875
I0701 15:00:22.610502  1052 solver.cpp:290] Iteration 22700 (48.2785 iter/s, 2.07132s/100 iter), loss = 0
I0701 15:00:22.610525  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:22.610543  1052 sgd_solver.cpp:106] Iteration 22700, lr = 0.00645312
I0701 15:00:24.693601  1052 solver.cpp:290] Iteration 22800 (48.0074 iter/s, 2.08301s/100 iter), loss = 0
I0701 15:00:24.693624  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:24.693632  1052 sgd_solver.cpp:106] Iteration 22800, lr = 0.0064375
I0701 15:00:26.768054  1052 solver.cpp:290] Iteration 22900 (48.2075 iter/s, 2.07437s/100 iter), loss = 0
I0701 15:00:26.768075  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:26.768081  1052 sgd_solver.cpp:106] Iteration 22900, lr = 0.00642187
I0701 15:00:28.820425  1052 solver.cpp:354] Sparsity after update:
I0701 15:00:28.821770  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:00:28.821777  1052 net.cpp:1851] conv1a_param_0(0.19) 
I0701 15:00:28.821786  1052 net.cpp:1851] conv1b_param_0(0.38) 
I0701 15:00:28.821791  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:00:28.821795  1052 net.cpp:1851] res2a_branch2a_param_0(0.38) 
I0701 15:00:28.821799  1052 net.cpp:1851] res2a_branch2b_param_0(0.38) 
I0701 15:00:28.821804  1052 net.cpp:1851] res3a_branch2a_param_0(0.38) 
I0701 15:00:28.821807  1052 net.cpp:1851] res3a_branch2b_param_0(0.38) 
I0701 15:00:28.821811  1052 net.cpp:1851] res4a_branch2a_param_0(0.38) 
I0701 15:00:28.821815  1052 net.cpp:1851] res4a_branch2b_param_0(0.38) 
I0701 15:00:28.821820  1052 net.cpp:1851] res5a_branch2a_param_0(0.38) 
I0701 15:00:28.821823  1052 net.cpp:1851] res5a_branch2b_param_0(0.378) 
I0701 15:00:28.821826  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (893071/2.3599e+06) 0.378
I0701 15:00:28.821961  1052 solver.cpp:473] Iteration 23000, Testing net (#0)
I0701 15:00:30.461130  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9147
I0701 15:00:30.461148  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9966
I0701 15:00:30.461153  1052 solver.cpp:546]     Test net output #2: loss = 0.2244 (* 1 = 0.2244 loss)
I0701 15:00:30.480662  1052 solver.cpp:290] Iteration 23000 (26.9361 iter/s, 3.71248s/100 iter), loss = 0
I0701 15:00:30.480677  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:30.480692  1052 sgd_solver.cpp:106] Iteration 23000, lr = 0.00640625
I0701 15:00:30.481314  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.4
I0701 15:00:30.644665  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:00:32.719564  1052 solver.cpp:290] Iteration 23100 (44.6664 iter/s, 2.23882s/100 iter), loss = 0
I0701 15:00:32.719648  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:32.719660  1052 sgd_solver.cpp:106] Iteration 23100, lr = 0.00639063
I0701 15:00:34.798630  1052 solver.cpp:290] Iteration 23200 (48.1019 iter/s, 2.07892s/100 iter), loss = 0
I0701 15:00:34.798652  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:34.798660  1052 sgd_solver.cpp:106] Iteration 23200, lr = 0.006375
I0701 15:00:36.871207  1052 solver.cpp:290] Iteration 23300 (48.2511 iter/s, 2.07249s/100 iter), loss = 0
I0701 15:00:36.871229  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:36.871237  1052 sgd_solver.cpp:106] Iteration 23300, lr = 0.00635938
I0701 15:00:38.943619  1052 solver.cpp:290] Iteration 23400 (48.255 iter/s, 2.07233s/100 iter), loss = 0
I0701 15:00:38.943641  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:38.943648  1052 sgd_solver.cpp:106] Iteration 23400, lr = 0.00634375
I0701 15:00:41.014850  1052 solver.cpp:290] Iteration 23500 (48.2824 iter/s, 2.07115s/100 iter), loss = 0
I0701 15:00:41.014871  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:41.014878  1052 sgd_solver.cpp:106] Iteration 23500, lr = 0.00632813
I0701 15:00:43.088798  1052 solver.cpp:290] Iteration 23600 (48.2192 iter/s, 2.07386s/100 iter), loss = 0
I0701 15:00:43.088819  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:43.088827  1052 sgd_solver.cpp:106] Iteration 23600, lr = 0.0063125
I0701 15:00:45.167387  1052 solver.cpp:290] Iteration 23700 (48.1115 iter/s, 2.0785s/100 iter), loss = 0
I0701 15:00:45.167412  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:45.167419  1052 sgd_solver.cpp:106] Iteration 23700, lr = 0.00629687
I0701 15:00:47.242435  1052 solver.cpp:290] Iteration 23800 (48.1937 iter/s, 2.07496s/100 iter), loss = 0
I0701 15:00:47.242458  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:47.242465  1052 sgd_solver.cpp:106] Iteration 23800, lr = 0.00628125
I0701 15:00:49.315639  1052 solver.cpp:290] Iteration 23900 (48.2365 iter/s, 2.07312s/100 iter), loss = 0
I0701 15:00:49.315661  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:49.315668  1052 sgd_solver.cpp:106] Iteration 23900, lr = 0.00626562
I0701 15:00:51.369246  1052 solver.cpp:354] Sparsity after update:
I0701 15:00:51.370576  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:00:51.370584  1052 net.cpp:1851] conv1a_param_0(0.2) 
I0701 15:00:51.370589  1052 net.cpp:1851] conv1b_param_0(0.4) 
I0701 15:00:51.370592  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:00:51.370594  1052 net.cpp:1851] res2a_branch2a_param_0(0.4) 
I0701 15:00:51.370596  1052 net.cpp:1851] res2a_branch2b_param_0(0.4) 
I0701 15:00:51.370599  1052 net.cpp:1851] res3a_branch2a_param_0(0.4) 
I0701 15:00:51.370600  1052 net.cpp:1851] res3a_branch2b_param_0(0.4) 
I0701 15:00:51.370601  1052 net.cpp:1851] res4a_branch2a_param_0(0.4) 
I0701 15:00:51.370604  1052 net.cpp:1851] res4a_branch2b_param_0(0.4) 
I0701 15:00:51.370605  1052 net.cpp:1851] res5a_branch2a_param_0(0.399) 
I0701 15:00:51.370607  1052 net.cpp:1851] res5a_branch2b_param_0(0.4) 
I0701 15:00:51.370609  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (940680/2.3599e+06) 0.399
I0701 15:00:51.370692  1052 solver.cpp:473] Iteration 24000, Testing net (#0)
I0701 15:00:53.011499  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9132
I0701 15:00:53.011521  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9965
I0701 15:00:53.011526  1052 solver.cpp:546]     Test net output #2: loss = 0.2272 (* 1 = 0.2272 loss)
I0701 15:00:53.031148  1052 solver.cpp:290] Iteration 24000 (26.9151 iter/s, 3.71538s/100 iter), loss = 0
I0701 15:00:53.031167  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:53.031177  1052 sgd_solver.cpp:106] Iteration 24000, lr = 0.00625
I0701 15:00:53.031738  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.42
I0701 15:00:53.204202  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:00:55.279357  1052 solver.cpp:290] Iteration 24100 (44.4815 iter/s, 2.24812s/100 iter), loss = 0
I0701 15:00:55.279378  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:55.279386  1052 sgd_solver.cpp:106] Iteration 24100, lr = 0.00623438
I0701 15:00:57.363240  1052 solver.cpp:290] Iteration 24200 (47.9893 iter/s, 2.0838s/100 iter), loss = 0
I0701 15:00:57.363266  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:57.363276  1052 sgd_solver.cpp:106] Iteration 24200, lr = 0.00621875
I0701 15:00:59.437857  1052 solver.cpp:290] Iteration 24300 (48.2037 iter/s, 2.07453s/100 iter), loss = 0
I0701 15:00:59.437878  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:00:59.437885  1052 sgd_solver.cpp:106] Iteration 24300, lr = 0.00620312
I0701 15:01:01.510535  1052 solver.cpp:290] Iteration 24400 (48.2487 iter/s, 2.07259s/100 iter), loss = 0
I0701 15:01:01.510568  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:01.510579  1052 sgd_solver.cpp:106] Iteration 24400, lr = 0.0061875
I0701 15:01:03.583057  1052 solver.cpp:290] Iteration 24500 (48.2526 iter/s, 2.07243s/100 iter), loss = 0
I0701 15:01:03.583142  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:03.583151  1052 sgd_solver.cpp:106] Iteration 24500, lr = 0.00617187
I0701 15:01:05.655800  1052 solver.cpp:290] Iteration 24600 (48.2487 iter/s, 2.0726s/100 iter), loss = 0
I0701 15:01:05.655827  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:05.655835  1052 sgd_solver.cpp:106] Iteration 24600, lr = 0.00615625
I0701 15:01:07.726470  1052 solver.cpp:290] Iteration 24700 (48.2956 iter/s, 2.07058s/100 iter), loss = 0
I0701 15:01:07.726490  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:07.726500  1052 sgd_solver.cpp:106] Iteration 24700, lr = 0.00614062
I0701 15:01:09.797238  1052 solver.cpp:290] Iteration 24800 (48.2932 iter/s, 2.07069s/100 iter), loss = 0
I0701 15:01:09.797260  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:09.797266  1052 sgd_solver.cpp:106] Iteration 24800, lr = 0.006125
I0701 15:01:11.870394  1052 solver.cpp:290] Iteration 24900 (48.2376 iter/s, 2.07307s/100 iter), loss = 0
I0701 15:01:11.870416  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:11.870424  1052 sgd_solver.cpp:106] Iteration 24900, lr = 0.00610937
I0701 15:01:13.927269  1052 solver.cpp:354] Sparsity after update:
I0701 15:01:13.928617  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:01:13.928624  1052 net.cpp:1851] conv1a_param_0(0.21) 
I0701 15:01:13.928630  1052 net.cpp:1851] conv1b_param_0(0.42) 
I0701 15:01:13.928632  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:01:13.928634  1052 net.cpp:1851] res2a_branch2a_param_0(0.42) 
I0701 15:01:13.928637  1052 net.cpp:1851] res2a_branch2b_param_0(0.42) 
I0701 15:01:13.928638  1052 net.cpp:1851] res3a_branch2a_param_0(0.42) 
I0701 15:01:13.928640  1052 net.cpp:1851] res3a_branch2b_param_0(0.42) 
I0701 15:01:13.928642  1052 net.cpp:1851] res4a_branch2a_param_0(0.42) 
I0701 15:01:13.928644  1052 net.cpp:1851] res4a_branch2b_param_0(0.42) 
I0701 15:01:13.928647  1052 net.cpp:1851] res5a_branch2a_param_0(0.42) 
I0701 15:01:13.928647  1052 net.cpp:1851] res5a_branch2b_param_0(0.419) 
I0701 15:01:13.928649  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (987746/2.3599e+06) 0.419
I0701 15:01:13.928733  1052 solver.cpp:473] Iteration 25000, Testing net (#0)
I0701 15:01:15.568512  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9137
I0701 15:01:15.568531  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9961
I0701 15:01:15.568536  1052 solver.cpp:546]     Test net output #2: loss = 0.23 (* 1 = 0.23 loss)
I0701 15:01:15.588017  1052 solver.cpp:290] Iteration 25000 (26.8998 iter/s, 3.7175s/100 iter), loss = 0
I0701 15:01:15.588032  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:15.588047  1052 sgd_solver.cpp:106] Iteration 25000, lr = 0.00609375
I0701 15:01:15.588685  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.44
I0701 15:01:15.780843  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:01:17.856815  1052 solver.cpp:290] Iteration 25100 (44.0778 iter/s, 2.26872s/100 iter), loss = 0
I0701 15:01:17.856835  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:17.856842  1052 sgd_solver.cpp:106] Iteration 25100, lr = 0.00607812
I0701 15:01:19.932325  1052 solver.cpp:290] Iteration 25200 (48.1828 iter/s, 2.07543s/100 iter), loss = 0
I0701 15:01:19.932345  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:19.932353  1052 sgd_solver.cpp:106] Iteration 25200, lr = 0.0060625
I0701 15:01:22.004211  1052 solver.cpp:290] Iteration 25300 (48.2672 iter/s, 2.0718s/100 iter), loss = 0
I0701 15:01:22.004232  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:22.004241  1052 sgd_solver.cpp:106] Iteration 25300, lr = 0.00604687
I0701 15:01:24.083503  1052 solver.cpp:290] Iteration 25400 (48.0952 iter/s, 2.07921s/100 iter), loss = 0
I0701 15:01:24.083524  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:24.083545  1052 sgd_solver.cpp:106] Iteration 25400, lr = 0.00603125
I0701 15:01:26.155772  1052 solver.cpp:290] Iteration 25500 (48.2582 iter/s, 2.07219s/100 iter), loss = 0
I0701 15:01:26.155793  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:26.155802  1052 sgd_solver.cpp:106] Iteration 25500, lr = 0.00601562
I0701 15:01:28.229516  1052 solver.cpp:290] Iteration 25600 (48.2239 iter/s, 2.07366s/100 iter), loss = 0
I0701 15:01:28.229542  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:28.229549  1052 sgd_solver.cpp:106] Iteration 25600, lr = 0.006
I0701 15:01:30.305711  1052 solver.cpp:290] Iteration 25700 (48.167 iter/s, 2.07611s/100 iter), loss = 0
I0701 15:01:30.305732  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:30.305740  1052 sgd_solver.cpp:106] Iteration 25700, lr = 0.00598437
I0701 15:01:32.376921  1052 solver.cpp:290] Iteration 25800 (48.2829 iter/s, 2.07113s/100 iter), loss = 0
I0701 15:01:32.376943  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:32.376950  1052 sgd_solver.cpp:106] Iteration 25800, lr = 0.00596875
I0701 15:01:34.451064  1052 solver.cpp:290] Iteration 25900 (48.2147 iter/s, 2.07405s/100 iter), loss = 0
I0701 15:01:34.451136  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:34.451148  1052 sgd_solver.cpp:106] Iteration 25900, lr = 0.00595312
I0701 15:01:36.507050  1052 solver.cpp:354] Sparsity after update:
I0701 15:01:36.508404  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:01:36.508410  1052 net.cpp:1851] conv1a_param_0(0.22) 
I0701 15:01:36.508417  1052 net.cpp:1851] conv1b_param_0(0.44) 
I0701 15:01:36.508419  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:01:36.508421  1052 net.cpp:1851] res2a_branch2a_param_0(0.44) 
I0701 15:01:36.508424  1052 net.cpp:1851] res2a_branch2b_param_0(0.44) 
I0701 15:01:36.508425  1052 net.cpp:1851] res3a_branch2a_param_0(0.44) 
I0701 15:01:36.508427  1052 net.cpp:1851] res3a_branch2b_param_0(0.44) 
I0701 15:01:36.508429  1052 net.cpp:1851] res4a_branch2a_param_0(0.44) 
I0701 15:01:36.508430  1052 net.cpp:1851] res4a_branch2b_param_0(0.44) 
I0701 15:01:36.508432  1052 net.cpp:1851] res5a_branch2a_param_0(0.439) 
I0701 15:01:36.508435  1052 net.cpp:1851] res5a_branch2b_param_0(0.438) 
I0701 15:01:36.508436  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.03318e+06/2.3599e+06) 0.438
I0701 15:01:36.508520  1052 solver.cpp:473] Iteration 26000, Testing net (#0)
I0701 15:01:38.152555  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9123
I0701 15:01:38.152575  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9965
I0701 15:01:38.152580  1052 solver.cpp:546]     Test net output #2: loss = 0.2308 (* 1 = 0.2308 loss)
I0701 15:01:38.172178  1052 solver.cpp:290] Iteration 26000 (26.8749 iter/s, 3.72094s/100 iter), loss = 0
I0701 15:01:38.172195  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:38.172209  1052 sgd_solver.cpp:106] Iteration 26000, lr = 0.0059375
I0701 15:01:38.172833  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.46
I0701 15:01:38.358887  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:01:40.432869  1052 solver.cpp:290] Iteration 26100 (44.2359 iter/s, 2.26061s/100 iter), loss = 0
I0701 15:01:40.432893  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:40.432898  1052 sgd_solver.cpp:106] Iteration 26100, lr = 0.00592188
I0701 15:01:42.506875  1052 solver.cpp:290] Iteration 26200 (48.2178 iter/s, 2.07392s/100 iter), loss = 0
I0701 15:01:42.506896  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:42.506903  1052 sgd_solver.cpp:106] Iteration 26200, lr = 0.00590625
I0701 15:01:44.578145  1052 solver.cpp:290] Iteration 26300 (48.2815 iter/s, 2.07119s/100 iter), loss = 0
I0701 15:01:44.578166  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:44.578173  1052 sgd_solver.cpp:106] Iteration 26300, lr = 0.00589063
I0701 15:01:46.660135  1052 solver.cpp:290] Iteration 26400 (48.0329 iter/s, 2.08191s/100 iter), loss = 0
I0701 15:01:46.660156  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:46.660164  1052 sgd_solver.cpp:106] Iteration 26400, lr = 0.005875
I0701 15:01:48.732866  1052 solver.cpp:290] Iteration 26500 (48.2474 iter/s, 2.07265s/100 iter), loss = 0
I0701 15:01:48.732887  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:48.732893  1052 sgd_solver.cpp:106] Iteration 26500, lr = 0.00585938
I0701 15:01:50.806586  1052 solver.cpp:290] Iteration 26600 (48.2244 iter/s, 2.07364s/100 iter), loss = 0
I0701 15:01:50.806608  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:50.806615  1052 sgd_solver.cpp:106] Iteration 26600, lr = 0.00584375
I0701 15:01:52.878772  1052 solver.cpp:290] Iteration 26700 (48.2602 iter/s, 2.0721s/100 iter), loss = 0
I0701 15:01:52.878794  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:52.878800  1052 sgd_solver.cpp:106] Iteration 26700, lr = 0.00582812
I0701 15:01:54.950995  1052 solver.cpp:290] Iteration 26800 (48.2593 iter/s, 2.07214s/100 iter), loss = 0
I0701 15:01:54.951030  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:54.951036  1052 sgd_solver.cpp:106] Iteration 26800, lr = 0.0058125
I0701 15:01:57.053745  1052 solver.cpp:290] Iteration 26900 (47.559 iter/s, 2.10265s/100 iter), loss = 0
I0701 15:01:57.053771  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:01:57.053777  1052 sgd_solver.cpp:106] Iteration 26900, lr = 0.00579687
I0701 15:01:59.105934  1052 solver.cpp:354] Sparsity after update:
I0701 15:01:59.107302  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:01:59.107311  1052 net.cpp:1851] conv1a_param_0(0.23) 
I0701 15:01:59.107319  1052 net.cpp:1851] conv1b_param_0(0.46) 
I0701 15:01:59.107324  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:01:59.107329  1052 net.cpp:1851] res2a_branch2a_param_0(0.46) 
I0701 15:01:59.107333  1052 net.cpp:1851] res2a_branch2b_param_0(0.46) 
I0701 15:01:59.107337  1052 net.cpp:1851] res3a_branch2a_param_0(0.46) 
I0701 15:01:59.107342  1052 net.cpp:1851] res3a_branch2b_param_0(0.46) 
I0701 15:01:59.107345  1052 net.cpp:1851] res4a_branch2a_param_0(0.46) 
I0701 15:01:59.107349  1052 net.cpp:1851] res4a_branch2b_param_0(0.46) 
I0701 15:01:59.107354  1052 net.cpp:1851] res5a_branch2a_param_0(0.459) 
I0701 15:01:59.107358  1052 net.cpp:1851] res5a_branch2b_param_0(0.459) 
I0701 15:01:59.107362  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.08165e+06/2.3599e+06) 0.458
I0701 15:01:59.107455  1052 solver.cpp:473] Iteration 27000, Testing net (#0)
I0701 15:02:00.751395  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9116
I0701 15:02:00.751415  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9965
I0701 15:02:00.751421  1052 solver.cpp:546]     Test net output #2: loss = 0.2377 (* 1 = 0.2377 loss)
I0701 15:02:00.771739  1052 solver.cpp:290] Iteration 27000 (26.8971 iter/s, 3.71787s/100 iter), loss = 0
I0701 15:02:00.771759  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:00.771770  1052 sgd_solver.cpp:106] Iteration 27000, lr = 0.00578125
I0701 15:02:00.772419  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.48
I0701 15:02:00.983639  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:02:03.055547  1052 solver.cpp:290] Iteration 27100 (43.7882 iter/s, 2.28372s/100 iter), loss = 0
I0701 15:02:03.055570  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:03.055578  1052 sgd_solver.cpp:106] Iteration 27100, lr = 0.00576563
I0701 15:02:05.134080  1052 solver.cpp:290] Iteration 27200 (48.1128 iter/s, 2.07845s/100 iter), loss = 0
I0701 15:02:05.134155  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:05.134171  1052 sgd_solver.cpp:106] Iteration 27200, lr = 0.00575
I0701 15:02:07.206980  1052 solver.cpp:290] Iteration 27300 (48.2447 iter/s, 2.07277s/100 iter), loss = 0
I0701 15:02:07.207003  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:07.207010  1052 sgd_solver.cpp:106] Iteration 27300, lr = 0.00573438
I0701 15:02:09.277055  1052 solver.cpp:290] Iteration 27400 (48.3094 iter/s, 2.06999s/100 iter), loss = 0
I0701 15:02:09.277077  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:09.277086  1052 sgd_solver.cpp:106] Iteration 27400, lr = 0.00571875
I0701 15:02:11.349582  1052 solver.cpp:290] Iteration 27500 (48.2522 iter/s, 2.07244s/100 iter), loss = 0
I0701 15:02:11.349604  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:11.349611  1052 sgd_solver.cpp:106] Iteration 27500, lr = 0.00570312
I0701 15:02:13.421475  1052 solver.cpp:290] Iteration 27600 (48.267 iter/s, 2.07181s/100 iter), loss = 0
I0701 15:02:13.421499  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:13.421504  1052 sgd_solver.cpp:106] Iteration 27600, lr = 0.0056875
I0701 15:02:15.499193  1052 solver.cpp:290] Iteration 27700 (48.1318 iter/s, 2.07763s/100 iter), loss = 0
I0701 15:02:15.499224  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:15.499233  1052 sgd_solver.cpp:106] Iteration 27700, lr = 0.00567187
I0701 15:02:17.572474  1052 solver.cpp:290] Iteration 27800 (48.2348 iter/s, 2.07319s/100 iter), loss = 0
I0701 15:02:17.572495  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:17.572504  1052 sgd_solver.cpp:106] Iteration 27800, lr = 0.00565625
I0701 15:02:19.645678  1052 solver.cpp:290] Iteration 27900 (48.2365 iter/s, 2.07312s/100 iter), loss = 0
I0701 15:02:19.645700  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:19.645707  1052 sgd_solver.cpp:106] Iteration 27900, lr = 0.00564062
I0701 15:02:21.722249  1052 solver.cpp:354] Sparsity after update:
I0701 15:02:21.723440  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:02:21.723448  1052 net.cpp:1851] conv1a_param_0(0.24) 
I0701 15:02:21.723454  1052 net.cpp:1851] conv1b_param_0(0.48) 
I0701 15:02:21.723456  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:02:21.723459  1052 net.cpp:1851] res2a_branch2a_param_0(0.48) 
I0701 15:02:21.723461  1052 net.cpp:1851] res2a_branch2b_param_0(0.48) 
I0701 15:02:21.723462  1052 net.cpp:1851] res3a_branch2a_param_0(0.48) 
I0701 15:02:21.723464  1052 net.cpp:1851] res3a_branch2b_param_0(0.48) 
I0701 15:02:21.723466  1052 net.cpp:1851] res4a_branch2a_param_0(0.48) 
I0701 15:02:21.723469  1052 net.cpp:1851] res4a_branch2b_param_0(0.48) 
I0701 15:02:21.723470  1052 net.cpp:1851] res5a_branch2a_param_0(0.48) 
I0701 15:02:21.723472  1052 net.cpp:1851] res5a_branch2b_param_0(0.479) 
I0701 15:02:21.723474  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.12924e+06/2.3599e+06) 0.479
I0701 15:02:21.723561  1052 solver.cpp:473] Iteration 28000, Testing net (#0)
I0701 15:02:23.366746  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9128
I0701 15:02:23.366765  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9967
I0701 15:02:23.366771  1052 solver.cpp:546]     Test net output #2: loss = 0.2428 (* 1 = 0.2428 loss)
I0701 15:02:23.386467  1052 solver.cpp:290] Iteration 28000 (26.7332 iter/s, 3.74066s/100 iter), loss = 0
I0701 15:02:23.386485  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:23.386494  1052 sgd_solver.cpp:106] Iteration 28000, lr = 0.005625
I0701 15:02:23.387351  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.5
I0701 15:02:23.605900  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:02:25.687819  1052 solver.cpp:290] Iteration 28100 (43.4543 iter/s, 2.30127s/100 iter), loss = 0
I0701 15:02:25.687855  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:25.687860  1052 sgd_solver.cpp:106] Iteration 28100, lr = 0.00560937
I0701 15:02:27.762563  1052 solver.cpp:290] Iteration 28200 (48.201 iter/s, 2.07465s/100 iter), loss = 0
I0701 15:02:27.762585  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:27.762591  1052 sgd_solver.cpp:106] Iteration 28200, lr = 0.00559375
I0701 15:02:29.840180  1052 solver.cpp:290] Iteration 28300 (48.134 iter/s, 2.07753s/100 iter), loss = 0
I0701 15:02:29.840203  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:29.840209  1052 sgd_solver.cpp:106] Iteration 28300, lr = 0.00557812
I0701 15:02:31.912221  1052 solver.cpp:290] Iteration 28400 (48.2635 iter/s, 2.07196s/100 iter), loss = 0
I0701 15:02:31.912243  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:31.912251  1052 sgd_solver.cpp:106] Iteration 28400, lr = 0.0055625
I0701 15:02:33.988281  1052 solver.cpp:290] Iteration 28500 (48.1702 iter/s, 2.07597s/100 iter), loss = 0
I0701 15:02:33.988302  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:33.988309  1052 sgd_solver.cpp:106] Iteration 28500, lr = 0.00554687
I0701 15:02:36.058120  1052 solver.cpp:290] Iteration 28600 (48.3149 iter/s, 2.06976s/100 iter), loss = 0
I0701 15:02:36.058182  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:36.058190  1052 sgd_solver.cpp:106] Iteration 28600, lr = 0.00553125
I0701 15:02:38.129964  1052 solver.cpp:290] Iteration 28700 (48.269 iter/s, 2.07172s/100 iter), loss = 0
I0701 15:02:38.129986  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:38.129992  1052 sgd_solver.cpp:106] Iteration 28700, lr = 0.00551562
I0701 15:02:40.208613  1052 solver.cpp:290] Iteration 28800 (48.1101 iter/s, 2.07857s/100 iter), loss = 0
I0701 15:02:40.208636  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:40.208642  1052 sgd_solver.cpp:106] Iteration 28800, lr = 0.0055
I0701 15:02:42.282272  1052 solver.cpp:290] Iteration 28900 (48.2259 iter/s, 2.07357s/100 iter), loss = 0
I0701 15:02:42.282294  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:42.282300  1052 sgd_solver.cpp:106] Iteration 28900, lr = 0.00548437
I0701 15:02:44.334488  1052 solver.cpp:354] Sparsity after update:
I0701 15:02:44.335827  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:02:44.335834  1052 net.cpp:1851] conv1a_param_0(0.25) 
I0701 15:02:44.335841  1052 net.cpp:1851] conv1b_param_0(0.5) 
I0701 15:02:44.335844  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:02:44.335845  1052 net.cpp:1851] res2a_branch2a_param_0(0.5) 
I0701 15:02:44.335847  1052 net.cpp:1851] res2a_branch2b_param_0(0.5) 
I0701 15:02:44.335850  1052 net.cpp:1851] res3a_branch2a_param_0(0.5) 
I0701 15:02:44.335851  1052 net.cpp:1851] res3a_branch2b_param_0(0.5) 
I0701 15:02:44.335853  1052 net.cpp:1851] res4a_branch2a_param_0(0.5) 
I0701 15:02:44.335855  1052 net.cpp:1851] res4a_branch2b_param_0(0.5) 
I0701 15:02:44.335856  1052 net.cpp:1851] res5a_branch2a_param_0(0.5) 
I0701 15:02:44.335858  1052 net.cpp:1851] res5a_branch2b_param_0(0.499) 
I0701 15:02:44.335860  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.17607e+06/2.3599e+06) 0.498
I0701 15:02:44.335950  1052 solver.cpp:473] Iteration 29000, Testing net (#0)
I0701 15:02:45.976649  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9115
I0701 15:02:45.976668  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9959
I0701 15:02:45.976673  1052 solver.cpp:546]     Test net output #2: loss = 0.2426 (* 1 = 0.2426 loss)
I0701 15:02:45.998435  1052 solver.cpp:290] Iteration 29000 (26.9104 iter/s, 3.71603s/100 iter), loss = 0
I0701 15:02:45.998458  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:45.998466  1052 sgd_solver.cpp:106] Iteration 29000, lr = 0.00546875
I0701 15:02:45.999063  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.52
I0701 15:02:46.232348  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:02:48.314609  1052 solver.cpp:290] Iteration 29100 (43.1764 iter/s, 2.31608s/100 iter), loss = 0
I0701 15:02:48.314630  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:48.314636  1052 sgd_solver.cpp:106] Iteration 29100, lr = 0.00545313
I0701 15:02:50.389338  1052 solver.cpp:290] Iteration 29200 (48.201 iter/s, 2.07464s/100 iter), loss = 0
I0701 15:02:50.389358  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:50.389364  1052 sgd_solver.cpp:106] Iteration 29200, lr = 0.0054375
I0701 15:02:52.461716  1052 solver.cpp:290] Iteration 29300 (48.2557 iter/s, 2.0723s/100 iter), loss = 0
I0701 15:02:52.461748  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:52.461760  1052 sgd_solver.cpp:106] Iteration 29300, lr = 0.00542188
I0701 15:02:54.533905  1052 solver.cpp:290] Iteration 29400 (48.2603 iter/s, 2.07209s/100 iter), loss = 0
I0701 15:02:54.533941  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:54.533952  1052 sgd_solver.cpp:106] Iteration 29400, lr = 0.00540625
I0701 15:02:56.606051  1052 solver.cpp:290] Iteration 29500 (48.2614 iter/s, 2.07205s/100 iter), loss = 0
I0701 15:02:56.606072  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:56.606096  1052 sgd_solver.cpp:106] Iteration 29500, lr = 0.00539062
I0701 15:02:58.696468  1052 solver.cpp:290] Iteration 29600 (47.8393 iter/s, 2.09033s/100 iter), loss = 0
I0701 15:02:58.696488  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:02:58.696494  1052 sgd_solver.cpp:106] Iteration 29600, lr = 0.005375
I0701 15:03:00.771268  1052 solver.cpp:290] Iteration 29700 (48.1993 iter/s, 2.07472s/100 iter), loss = 0
I0701 15:03:00.771291  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:00.771301  1052 sgd_solver.cpp:106] Iteration 29700, lr = 0.00535937
I0701 15:03:02.845978  1052 solver.cpp:290] Iteration 29800 (48.2015 iter/s, 2.07462s/100 iter), loss = 0
I0701 15:03:02.845999  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:02.846006  1052 sgd_solver.cpp:106] Iteration 29800, lr = 0.00534375
I0701 15:03:04.917665  1052 solver.cpp:290] Iteration 29900 (48.2718 iter/s, 2.0716s/100 iter), loss = 0
I0701 15:03:04.917690  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:04.917697  1052 sgd_solver.cpp:106] Iteration 29900, lr = 0.00532812
I0701 15:03:06.971614  1052 solver.cpp:600] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/cifar10_jacintonet11v2_iter_30000.caffemodel
I0701 15:03:06.988145  1052 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/cifar10_jacintonet11v2_iter_30000.solverstate
I0701 15:03:06.995332  1052 solver.cpp:354] Sparsity after update:
I0701 15:03:06.996276  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:03:06.996285  1052 net.cpp:1851] conv1a_param_0(0.26) 
I0701 15:03:06.996292  1052 net.cpp:1851] conv1b_param_0(0.52) 
I0701 15:03:06.996294  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:03:06.996297  1052 net.cpp:1851] res2a_branch2a_param_0(0.52) 
I0701 15:03:06.996299  1052 net.cpp:1851] res2a_branch2b_param_0(0.52) 
I0701 15:03:06.996300  1052 net.cpp:1851] res3a_branch2a_param_0(0.52) 
I0701 15:03:06.996302  1052 net.cpp:1851] res3a_branch2b_param_0(0.52) 
I0701 15:03:06.996304  1052 net.cpp:1851] res4a_branch2a_param_0(0.52) 
I0701 15:03:06.996306  1052 net.cpp:1851] res4a_branch2b_param_0(0.52) 
I0701 15:03:06.996309  1052 net.cpp:1851] res5a_branch2a_param_0(0.52) 
I0701 15:03:06.996310  1052 net.cpp:1851] res5a_branch2b_param_0(0.52) 
I0701 15:03:06.996311  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.22344e+06/2.3599e+06) 0.518
I0701 15:03:06.996412  1052 solver.cpp:473] Iteration 30000, Testing net (#0)
I0701 15:03:08.635639  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9096
I0701 15:03:08.635658  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9961
I0701 15:03:08.635663  1052 solver.cpp:546]     Test net output #2: loss = 0.2469 (* 1 = 0.2469 loss)
I0701 15:03:08.655207  1052 solver.cpp:290] Iteration 30000 (26.7564 iter/s, 3.73742s/100 iter), loss = 0
I0701 15:03:08.655222  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:08.655236  1052 sgd_solver.cpp:106] Iteration 30000, lr = 0.0053125
I0701 15:03:08.655871  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.54
I0701 15:03:08.912091  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:03:10.988140  1052 solver.cpp:290] Iteration 30100 (42.866 iter/s, 2.33285s/100 iter), loss = 0
I0701 15:03:10.988162  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:10.988169  1052 sgd_solver.cpp:106] Iteration 30100, lr = 0.00529688
I0701 15:03:13.058857  1052 solver.cpp:290] Iteration 30200 (48.2944 iter/s, 2.07063s/100 iter), loss = 0
I0701 15:03:13.058878  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:13.058887  1052 sgd_solver.cpp:106] Iteration 30200, lr = 0.00528125
I0701 15:03:15.134234  1052 solver.cpp:290] Iteration 30300 (48.186 iter/s, 2.07529s/100 iter), loss = 0
I0701 15:03:15.134258  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:15.134266  1052 sgd_solver.cpp:106] Iteration 30300, lr = 0.00526563
I0701 15:03:17.206900  1052 solver.cpp:290] Iteration 30400 (48.249 iter/s, 2.07258s/100 iter), loss = 0
I0701 15:03:17.206923  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:17.206928  1052 sgd_solver.cpp:106] Iteration 30400, lr = 0.00525
I0701 15:03:19.279455  1052 solver.cpp:290] Iteration 30500 (48.2516 iter/s, 2.07247s/100 iter), loss = 0
I0701 15:03:19.279479  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:19.279484  1052 sgd_solver.cpp:106] Iteration 30500, lr = 0.00523437
I0701 15:03:21.353055  1052 solver.cpp:290] Iteration 30600 (48.2273 iter/s, 2.07351s/100 iter), loss = 0
I0701 15:03:21.353078  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:21.353085  1052 sgd_solver.cpp:106] Iteration 30600, lr = 0.00521875
I0701 15:03:23.426551  1052 solver.cpp:290] Iteration 30700 (48.2297 iter/s, 2.07341s/100 iter), loss = 0
I0701 15:03:23.426573  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:23.426580  1052 sgd_solver.cpp:106] Iteration 30700, lr = 0.00520312
I0701 15:03:25.501813  1052 solver.cpp:290] Iteration 30800 (48.1887 iter/s, 2.07518s/100 iter), loss = 0
I0701 15:03:25.501857  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:25.501863  1052 sgd_solver.cpp:106] Iteration 30800, lr = 0.0051875
I0701 15:03:27.573107  1052 solver.cpp:290] Iteration 30900 (48.2815 iter/s, 2.07119s/100 iter), loss = 0
I0701 15:03:27.573137  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:27.573144  1052 sgd_solver.cpp:106] Iteration 30900, lr = 0.00517187
I0701 15:03:29.637441  1052 solver.cpp:354] Sparsity after update:
I0701 15:03:29.638787  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:03:29.638794  1052 net.cpp:1851] conv1a_param_0(0.27) 
I0701 15:03:29.638801  1052 net.cpp:1851] conv1b_param_0(0.54) 
I0701 15:03:29.638803  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:03:29.638805  1052 net.cpp:1851] res2a_branch2a_param_0(0.54) 
I0701 15:03:29.638808  1052 net.cpp:1851] res2a_branch2b_param_0(0.54) 
I0701 15:03:29.638809  1052 net.cpp:1851] res3a_branch2a_param_0(0.54) 
I0701 15:03:29.638811  1052 net.cpp:1851] res3a_branch2b_param_0(0.54) 
I0701 15:03:29.638813  1052 net.cpp:1851] res4a_branch2a_param_0(0.54) 
I0701 15:03:29.638814  1052 net.cpp:1851] res4a_branch2b_param_0(0.54) 
I0701 15:03:29.638816  1052 net.cpp:1851] res5a_branch2a_param_0(0.54) 
I0701 15:03:29.638818  1052 net.cpp:1851] res5a_branch2b_param_0(0.54) 
I0701 15:03:29.638820  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.27063e+06/2.3599e+06) 0.538
I0701 15:03:29.638905  1052 solver.cpp:473] Iteration 31000, Testing net (#0)
I0701 15:03:31.279403  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9103
I0701 15:03:31.279422  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9955
I0701 15:03:31.279428  1052 solver.cpp:546]     Test net output #2: loss = 0.2504 (* 1 = 0.2504 loss)
I0701 15:03:31.299165  1052 solver.cpp:290] Iteration 31000 (26.8389 iter/s, 3.72593s/100 iter), loss = 0
I0701 15:03:31.299182  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:31.299196  1052 sgd_solver.cpp:106] Iteration 31000, lr = 0.00515625
I0701 15:03:31.299813  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.56
I0701 15:03:31.560353  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:03:33.638080  1052 solver.cpp:290] Iteration 31100 (42.7565 iter/s, 2.33883s/100 iter), loss = 0
I0701 15:03:33.638101  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:33.638109  1052 sgd_solver.cpp:106] Iteration 31100, lr = 0.00514062
I0701 15:03:35.709573  1052 solver.cpp:290] Iteration 31200 (48.2763 iter/s, 2.07141s/100 iter), loss = 0
I0701 15:03:35.709595  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:35.709602  1052 sgd_solver.cpp:106] Iteration 31200, lr = 0.005125
I0701 15:03:37.788341  1052 solver.cpp:290] Iteration 31300 (48.1074 iter/s, 2.07868s/100 iter), loss = 0
I0701 15:03:37.788414  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:37.788424  1052 sgd_solver.cpp:106] Iteration 31300, lr = 0.00510937
I0701 15:03:39.861642  1052 solver.cpp:290] Iteration 31400 (48.2354 iter/s, 2.07317s/100 iter), loss = 0
I0701 15:03:39.861663  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:39.861670  1052 sgd_solver.cpp:106] Iteration 31400, lr = 0.00509375
I0701 15:03:41.932555  1052 solver.cpp:290] Iteration 31500 (48.2898 iter/s, 2.07083s/100 iter), loss = 0
I0701 15:03:41.932576  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:41.932584  1052 sgd_solver.cpp:106] Iteration 31500, lr = 0.00507812
I0701 15:03:44.010284  1052 solver.cpp:290] Iteration 31600 (48.1315 iter/s, 2.07764s/100 iter), loss = 0
I0701 15:03:44.010309  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:44.010318  1052 sgd_solver.cpp:106] Iteration 31600, lr = 0.0050625
I0701 15:03:46.081310  1052 solver.cpp:290] Iteration 31700 (48.2873 iter/s, 2.07094s/100 iter), loss = 0
I0701 15:03:46.081332  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:46.081338  1052 sgd_solver.cpp:106] Iteration 31700, lr = 0.00504687
I0701 15:03:48.154903  1052 solver.cpp:290] Iteration 31800 (48.2275 iter/s, 2.07351s/100 iter), loss = 0
I0701 15:03:48.154924  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:48.154932  1052 sgd_solver.cpp:106] Iteration 31800, lr = 0.00503125
I0701 15:03:50.228189  1052 solver.cpp:290] Iteration 31900 (48.2346 iter/s, 2.0732s/100 iter), loss = 0
I0701 15:03:50.228212  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:50.228221  1052 sgd_solver.cpp:106] Iteration 31900, lr = 0.00501562
I0701 15:03:52.282536  1052 solver.cpp:354] Sparsity after update:
I0701 15:03:52.283890  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:03:52.283897  1052 net.cpp:1851] conv1a_param_0(0.28) 
I0701 15:03:52.283903  1052 net.cpp:1851] conv1b_param_0(0.56) 
I0701 15:03:52.283905  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:03:52.283908  1052 net.cpp:1851] res2a_branch2a_param_0(0.56) 
I0701 15:03:52.283910  1052 net.cpp:1851] res2a_branch2b_param_0(0.56) 
I0701 15:03:52.283911  1052 net.cpp:1851] res3a_branch2a_param_0(0.56) 
I0701 15:03:52.283913  1052 net.cpp:1851] res3a_branch2b_param_0(0.56) 
I0701 15:03:52.283915  1052 net.cpp:1851] res4a_branch2a_param_0(0.56) 
I0701 15:03:52.283917  1052 net.cpp:1851] res4a_branch2b_param_0(0.56) 
I0701 15:03:52.283920  1052 net.cpp:1851] res5a_branch2a_param_0(0.56) 
I0701 15:03:52.283921  1052 net.cpp:1851] res5a_branch2b_param_0(0.56) 
I0701 15:03:52.283923  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.31788e+06/2.3599e+06) 0.558
I0701 15:03:52.284008  1052 solver.cpp:473] Iteration 32000, Testing net (#0)
I0701 15:03:53.923701  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9084
I0701 15:03:53.923719  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9958
I0701 15:03:53.923725  1052 solver.cpp:546]     Test net output #2: loss = 0.2519 (* 1 = 0.2519 loss)
I0701 15:03:53.943660  1052 solver.cpp:290] Iteration 32000 (26.9154 iter/s, 3.71535s/100 iter), loss = 0
I0701 15:03:53.943675  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:53.943691  1052 sgd_solver.cpp:106] Iteration 32000, lr = 0.005
I0701 15:03:53.944325  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.58
I0701 15:03:54.243888  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:03:56.318377  1052 solver.cpp:290] Iteration 32100 (42.1118 iter/s, 2.37463s/100 iter), loss = 0
I0701 15:03:56.318403  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:56.318411  1052 sgd_solver.cpp:106] Iteration 32100, lr = 0.00498438
I0701 15:03:58.417080  1052 solver.cpp:290] Iteration 32200 (47.6505 iter/s, 2.09862s/100 iter), loss = 0
I0701 15:03:58.417119  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:03:58.417127  1052 sgd_solver.cpp:106] Iteration 32200, lr = 0.00496875
I0701 15:04:00.494045  1052 solver.cpp:290] Iteration 32300 (48.1495 iter/s, 2.07687s/100 iter), loss = 0
I0701 15:04:00.494066  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:00.494073  1052 sgd_solver.cpp:106] Iteration 32300, lr = 0.00495313
I0701 15:04:02.564651  1052 solver.cpp:290] Iteration 32400 (48.297 iter/s, 2.07052s/100 iter), loss = 0
I0701 15:04:02.564673  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:02.564680  1052 sgd_solver.cpp:106] Iteration 32400, lr = 0.0049375
I0701 15:04:04.635526  1052 solver.cpp:290] Iteration 32500 (48.2908 iter/s, 2.07079s/100 iter), loss = 0
I0701 15:04:04.635555  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:04.635562  1052 sgd_solver.cpp:106] Iteration 32500, lr = 0.00492187
I0701 15:04:06.712625  1052 solver.cpp:290] Iteration 32600 (48.1461 iter/s, 2.07701s/100 iter), loss = 0
I0701 15:04:06.712646  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:06.712653  1052 sgd_solver.cpp:106] Iteration 32600, lr = 0.00490625
I0701 15:04:08.789420  1052 solver.cpp:290] Iteration 32700 (48.153 iter/s, 2.07671s/100 iter), loss = 0
I0701 15:04:08.789486  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:08.789494  1052 sgd_solver.cpp:106] Iteration 32700, lr = 0.00489062
I0701 15:04:10.859503  1052 solver.cpp:290] Iteration 32800 (48.3102 iter/s, 2.06996s/100 iter), loss = 0
I0701 15:04:10.859525  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:10.859531  1052 sgd_solver.cpp:106] Iteration 32800, lr = 0.004875
I0701 15:04:12.935317  1052 solver.cpp:290] Iteration 32900 (48.1758 iter/s, 2.07573s/100 iter), loss = 0
I0701 15:04:12.935340  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:12.935346  1052 sgd_solver.cpp:106] Iteration 32900, lr = 0.00485937
I0701 15:04:14.987220  1052 solver.cpp:354] Sparsity after update:
I0701 15:04:14.988549  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:04:14.988555  1052 net.cpp:1851] conv1a_param_0(0.29) 
I0701 15:04:14.988562  1052 net.cpp:1851] conv1b_param_0(0.58) 
I0701 15:04:14.988564  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:04:14.988566  1052 net.cpp:1851] res2a_branch2a_param_0(0.58) 
I0701 15:04:14.988569  1052 net.cpp:1851] res2a_branch2b_param_0(0.58) 
I0701 15:04:14.988570  1052 net.cpp:1851] res3a_branch2a_param_0(0.58) 
I0701 15:04:14.988572  1052 net.cpp:1851] res3a_branch2b_param_0(0.58) 
I0701 15:04:14.988574  1052 net.cpp:1851] res4a_branch2a_param_0(0.58) 
I0701 15:04:14.988576  1052 net.cpp:1851] res4a_branch2b_param_0(0.58) 
I0701 15:04:14.988577  1052 net.cpp:1851] res5a_branch2a_param_0(0.58) 
I0701 15:04:14.988579  1052 net.cpp:1851] res5a_branch2b_param_0(0.58) 
I0701 15:04:14.988581  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.36502e+06/2.3599e+06) 0.578
I0701 15:04:14.988664  1052 solver.cpp:473] Iteration 33000, Testing net (#0)
I0701 15:04:16.627193  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9068
I0701 15:04:16.627213  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9954
I0701 15:04:16.627218  1052 solver.cpp:546]     Test net output #2: loss = 0.2626 (* 1 = 0.2626 loss)
I0701 15:04:16.647588  1052 solver.cpp:290] Iteration 33000 (26.9386 iter/s, 3.71214s/100 iter), loss = 0
I0701 15:04:16.647617  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:16.647624  1052 sgd_solver.cpp:106] Iteration 33000, lr = 0.00484375
I0701 15:04:16.648396  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.6
I0701 15:04:16.947458  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:04:19.033354  1052 solver.cpp:290] Iteration 33100 (41.917 iter/s, 2.38566s/100 iter), loss = 0
I0701 15:04:19.033378  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:19.033387  1052 sgd_solver.cpp:106] Iteration 33100, lr = 0.00482813
I0701 15:04:21.111152  1052 solver.cpp:290] Iteration 33200 (48.1299 iter/s, 2.07771s/100 iter), loss = 0
I0701 15:04:21.111174  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:21.111181  1052 sgd_solver.cpp:106] Iteration 33200, lr = 0.0048125
I0701 15:04:23.187947  1052 solver.cpp:290] Iteration 33300 (48.153 iter/s, 2.07671s/100 iter), loss = 0
I0701 15:04:23.187970  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:23.187978  1052 sgd_solver.cpp:106] Iteration 33300, lr = 0.00479688
I0701 15:04:25.264374  1052 solver.cpp:290] Iteration 33400 (48.1616 iter/s, 2.07634s/100 iter), loss = 0
I0701 15:04:25.264398  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:25.264406  1052 sgd_solver.cpp:106] Iteration 33400, lr = 0.00478125
I0701 15:04:27.337709  1052 solver.cpp:290] Iteration 33500 (48.2335 iter/s, 2.07325s/100 iter), loss = 0
I0701 15:04:27.337733  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:27.337741  1052 sgd_solver.cpp:106] Iteration 33500, lr = 0.00476563
I0701 15:04:29.407198  1052 solver.cpp:290] Iteration 33600 (48.3231 iter/s, 2.0694s/100 iter), loss = 0
I0701 15:04:29.407239  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:29.407246  1052 sgd_solver.cpp:106] Iteration 33600, lr = 0.00475
I0701 15:04:31.479643  1052 solver.cpp:290] Iteration 33700 (48.2545 iter/s, 2.07234s/100 iter), loss = 0
I0701 15:04:31.479665  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:31.479671  1052 sgd_solver.cpp:106] Iteration 33700, lr = 0.00473437
I0701 15:04:33.550963  1052 solver.cpp:290] Iteration 33800 (48.2803 iter/s, 2.07124s/100 iter), loss = 0
I0701 15:04:33.550987  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:33.550993  1052 sgd_solver.cpp:106] Iteration 33800, lr = 0.00471875
I0701 15:04:35.625618  1052 solver.cpp:290] Iteration 33900 (48.2028 iter/s, 2.07457s/100 iter), loss = 0
I0701 15:04:35.625640  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:35.625646  1052 sgd_solver.cpp:106] Iteration 33900, lr = 0.00470312
I0701 15:04:37.676743  1052 solver.cpp:354] Sparsity after update:
I0701 15:04:37.678114  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:04:37.678122  1052 net.cpp:1851] conv1a_param_0(0.3) 
I0701 15:04:37.678128  1052 net.cpp:1851] conv1b_param_0(0.6) 
I0701 15:04:37.678131  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:04:37.678133  1052 net.cpp:1851] res2a_branch2a_param_0(0.6) 
I0701 15:04:37.678135  1052 net.cpp:1851] res2a_branch2b_param_0(0.6) 
I0701 15:04:37.678138  1052 net.cpp:1851] res3a_branch2a_param_0(0.6) 
I0701 15:04:37.678139  1052 net.cpp:1851] res3a_branch2b_param_0(0.6) 
I0701 15:04:37.678140  1052 net.cpp:1851] res4a_branch2a_param_0(0.6) 
I0701 15:04:37.678143  1052 net.cpp:1851] res4a_branch2b_param_0(0.6) 
I0701 15:04:37.678144  1052 net.cpp:1851] res5a_branch2a_param_0(0.6) 
I0701 15:04:37.678146  1052 net.cpp:1851] res5a_branch2b_param_0(0.6) 
I0701 15:04:37.678148  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.41206e+06/2.3599e+06) 0.598
I0701 15:04:37.678236  1052 solver.cpp:473] Iteration 34000, Testing net (#0)
I0701 15:04:39.322887  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9098
I0701 15:04:39.322939  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.996
I0701 15:04:39.322947  1052 solver.cpp:546]     Test net output #2: loss = 0.2575 (* 1 = 0.2575 loss)
I0701 15:04:39.342883  1052 solver.cpp:290] Iteration 34000 (26.9024 iter/s, 3.71714s/100 iter), loss = 0
I0701 15:04:39.342898  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:39.342914  1052 sgd_solver.cpp:106] Iteration 34000, lr = 0.0046875
I0701 15:04:39.343538  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.62
I0701 15:04:39.661244  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:04:41.737977  1052 solver.cpp:290] Iteration 34100 (41.7535 iter/s, 2.39501s/100 iter), loss = 0
I0701 15:04:41.737998  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:41.738004  1052 sgd_solver.cpp:106] Iteration 34100, lr = 0.00467187
I0701 15:04:43.813228  1052 solver.cpp:290] Iteration 34200 (48.1889 iter/s, 2.07517s/100 iter), loss = 0
I0701 15:04:43.813251  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:43.813257  1052 sgd_solver.cpp:106] Iteration 34200, lr = 0.00465625
I0701 15:04:45.887528  1052 solver.cpp:290] Iteration 34300 (48.211 iter/s, 2.07422s/100 iter), loss = 0
I0701 15:04:45.887552  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:45.887559  1052 sgd_solver.cpp:106] Iteration 34300, lr = 0.00464062
I0701 15:04:47.961678  1052 solver.cpp:290] Iteration 34400 (48.2145 iter/s, 2.07406s/100 iter), loss = 0
I0701 15:04:47.961699  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:47.961706  1052 sgd_solver.cpp:106] Iteration 34400, lr = 0.004625
I0701 15:04:50.032259  1052 solver.cpp:290] Iteration 34500 (48.2976 iter/s, 2.0705s/100 iter), loss = 0
I0701 15:04:50.032279  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:50.032287  1052 sgd_solver.cpp:106] Iteration 34500, lr = 0.00460937
I0701 15:04:52.107586  1052 solver.cpp:290] Iteration 34600 (48.1871 iter/s, 2.07524s/100 iter), loss = 0
I0701 15:04:52.107607  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:52.107615  1052 sgd_solver.cpp:106] Iteration 34600, lr = 0.00459375
I0701 15:04:54.183130  1052 solver.cpp:290] Iteration 34700 (48.1821 iter/s, 2.07546s/100 iter), loss = 0
I0701 15:04:54.183151  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:54.183158  1052 sgd_solver.cpp:106] Iteration 34700, lr = 0.00457812
I0701 15:04:56.270223  1052 solver.cpp:290] Iteration 34800 (47.9155 iter/s, 2.08701s/100 iter), loss = 0
I0701 15:04:56.270246  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:56.270252  1052 sgd_solver.cpp:106] Iteration 34800, lr = 0.0045625
I0701 15:04:58.359833  1052 solver.cpp:290] Iteration 34900 (47.8578 iter/s, 2.08952s/100 iter), loss = 0
I0701 15:04:58.359858  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:04:58.359864  1052 sgd_solver.cpp:106] Iteration 34900, lr = 0.00454687
I0701 15:05:00.410311  1052 solver.cpp:354] Sparsity after update:
I0701 15:05:00.411698  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:05:00.411710  1052 net.cpp:1851] conv1a_param_0(0.31) 
I0701 15:05:00.411720  1052 net.cpp:1851] conv1b_param_0(0.62) 
I0701 15:05:00.411723  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:05:00.411728  1052 net.cpp:1851] res2a_branch2a_param_0(0.62) 
I0701 15:05:00.411732  1052 net.cpp:1851] res2a_branch2b_param_0(0.62) 
I0701 15:05:00.411736  1052 net.cpp:1851] res3a_branch2a_param_0(0.62) 
I0701 15:05:00.411741  1052 net.cpp:1851] res3a_branch2b_param_0(0.62) 
I0701 15:05:00.411744  1052 net.cpp:1851] res4a_branch2a_param_0(0.62) 
I0701 15:05:00.411749  1052 net.cpp:1851] res4a_branch2b_param_0(0.62) 
I0701 15:05:00.411752  1052 net.cpp:1851] res5a_branch2a_param_0(0.62) 
I0701 15:05:00.411756  1052 net.cpp:1851] res5a_branch2b_param_0(0.62) 
I0701 15:05:00.411772  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.45919e+06/2.3599e+06) 0.618
I0701 15:05:00.411869  1052 solver.cpp:473] Iteration 35000, Testing net (#0)
I0701 15:05:02.050586  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.909
I0701 15:05:02.050606  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9951
I0701 15:05:02.050612  1052 solver.cpp:546]     Test net output #2: loss = 0.2689 (* 1 = 0.2689 loss)
I0701 15:05:02.071806  1052 solver.cpp:290] Iteration 35000 (26.9408 iter/s, 3.71185s/100 iter), loss = 0
I0701 15:05:02.071825  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:02.071835  1052 sgd_solver.cpp:106] Iteration 35000, lr = 0.00453125
I0701 15:05:02.072482  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.64
I0701 15:05:02.415290  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:05:04.488059  1052 solver.cpp:290] Iteration 35100 (41.3879 iter/s, 2.41616s/100 iter), loss = 0
I0701 15:05:04.488080  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:04.488087  1052 sgd_solver.cpp:106] Iteration 35100, lr = 0.00451563
I0701 15:05:06.563498  1052 solver.cpp:290] Iteration 35200 (48.1845 iter/s, 2.07536s/100 iter), loss = 0
I0701 15:05:06.563521  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:06.563527  1052 sgd_solver.cpp:106] Iteration 35200, lr = 0.0045
I0701 15:05:08.636032  1052 solver.cpp:290] Iteration 35300 (48.2521 iter/s, 2.07245s/100 iter), loss = 0
I0701 15:05:08.636054  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:08.636061  1052 sgd_solver.cpp:106] Iteration 35300, lr = 0.00448438
I0701 15:05:10.714990  1052 solver.cpp:290] Iteration 35400 (48.103 iter/s, 2.07887s/100 iter), loss = 0
I0701 15:05:10.715060  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:10.715070  1052 sgd_solver.cpp:106] Iteration 35400, lr = 0.00446875
I0701 15:05:12.787228  1052 solver.cpp:290] Iteration 35500 (48.26 iter/s, 2.07211s/100 iter), loss = 0
I0701 15:05:12.787250  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:12.787257  1052 sgd_solver.cpp:106] Iteration 35500, lr = 0.00445312
I0701 15:05:14.860277  1052 solver.cpp:290] Iteration 35600 (48.2401 iter/s, 2.07297s/100 iter), loss = 0
I0701 15:05:14.860299  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:14.860306  1052 sgd_solver.cpp:106] Iteration 35600, lr = 0.0044375
I0701 15:05:16.933955  1052 solver.cpp:290] Iteration 35700 (48.2255 iter/s, 2.07359s/100 iter), loss = 0
I0701 15:05:16.933979  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:16.933986  1052 sgd_solver.cpp:106] Iteration 35700, lr = 0.00442187
I0701 15:05:19.006995  1052 solver.cpp:290] Iteration 35800 (48.2403 iter/s, 2.07295s/100 iter), loss = 0
I0701 15:05:19.007017  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:19.007024  1052 sgd_solver.cpp:106] Iteration 35800, lr = 0.00440625
I0701 15:05:21.080409  1052 solver.cpp:290] Iteration 35900 (48.2316 iter/s, 2.07333s/100 iter), loss = 0
I0701 15:05:21.080430  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:21.080436  1052 sgd_solver.cpp:106] Iteration 35900, lr = 0.00439062
I0701 15:05:23.133664  1052 solver.cpp:354] Sparsity after update:
I0701 15:05:23.135005  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:05:23.135012  1052 net.cpp:1851] conv1a_param_0(0.32) 
I0701 15:05:23.135020  1052 net.cpp:1851] conv1b_param_0(0.64) 
I0701 15:05:23.135022  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:05:23.135025  1052 net.cpp:1851] res2a_branch2a_param_0(0.64) 
I0701 15:05:23.135027  1052 net.cpp:1851] res2a_branch2b_param_0(0.64) 
I0701 15:05:23.135030  1052 net.cpp:1851] res3a_branch2a_param_0(0.64) 
I0701 15:05:23.135031  1052 net.cpp:1851] res3a_branch2b_param_0(0.64) 
I0701 15:05:23.135033  1052 net.cpp:1851] res4a_branch2a_param_0(0.64) 
I0701 15:05:23.135035  1052 net.cpp:1851] res4a_branch2b_param_0(0.64) 
I0701 15:05:23.135037  1052 net.cpp:1851] res5a_branch2a_param_0(0.64) 
I0701 15:05:23.135040  1052 net.cpp:1851] res5a_branch2b_param_0(0.64) 
I0701 15:05:23.135042  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.50624e+06/2.3599e+06) 0.638
I0701 15:05:23.135125  1052 solver.cpp:473] Iteration 36000, Testing net (#0)
I0701 15:05:24.776316  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.8683
I0701 15:05:24.776335  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9928
I0701 15:05:24.776340  1052 solver.cpp:546]     Test net output #2: loss = 0.4828 (* 1 = 0.4828 loss)
I0701 15:05:24.795950  1052 solver.cpp:290] Iteration 36000 (26.9149 iter/s, 3.71542s/100 iter), loss = 0
I0701 15:05:24.795965  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:24.795981  1052 sgd_solver.cpp:106] Iteration 36000, lr = 0.004375
I0701 15:05:24.796558  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.66
I0701 15:05:25.164038  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:05:27.244323  1052 solver.cpp:290] Iteration 36100 (40.8449 iter/s, 2.44828s/100 iter), loss = 0
I0701 15:05:27.244345  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:27.244352  1052 sgd_solver.cpp:106] Iteration 36100, lr = 0.00435938
I0701 15:05:29.319905  1052 solver.cpp:290] Iteration 36200 (48.1812 iter/s, 2.0755s/100 iter), loss = 0
I0701 15:05:29.319927  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:29.319933  1052 sgd_solver.cpp:106] Iteration 36200, lr = 0.00434375
I0701 15:05:31.395179  1052 solver.cpp:290] Iteration 36300 (48.1884 iter/s, 2.07519s/100 iter), loss = 0
I0701 15:05:31.395221  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:31.395228  1052 sgd_solver.cpp:106] Iteration 36300, lr = 0.00432813
I0701 15:05:33.468698  1052 solver.cpp:290] Iteration 36400 (48.2296 iter/s, 2.07341s/100 iter), loss = 0
I0701 15:05:33.468721  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:33.468727  1052 sgd_solver.cpp:106] Iteration 36400, lr = 0.0043125
I0701 15:05:35.542382  1052 solver.cpp:290] Iteration 36500 (48.2254 iter/s, 2.0736s/100 iter), loss = 0
I0701 15:05:35.542407  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:35.542413  1052 sgd_solver.cpp:106] Iteration 36500, lr = 0.00429688
I0701 15:05:37.614773  1052 solver.cpp:290] Iteration 36600 (48.2555 iter/s, 2.0723s/100 iter), loss = 0
I0701 15:05:37.614796  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:37.614802  1052 sgd_solver.cpp:106] Iteration 36600, lr = 0.00428125
I0701 15:05:39.685159  1052 solver.cpp:290] Iteration 36700 (48.3022 iter/s, 2.0703s/100 iter), loss = 0
I0701 15:05:39.685180  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:39.685189  1052 sgd_solver.cpp:106] Iteration 36700, lr = 0.00426562
I0701 15:05:41.761080  1052 solver.cpp:290] Iteration 36800 (48.1734 iter/s, 2.07584s/100 iter), loss = 0
I0701 15:05:41.761163  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:41.761174  1052 sgd_solver.cpp:106] Iteration 36800, lr = 0.00425
I0701 15:05:43.833134  1052 solver.cpp:290] Iteration 36900 (48.2646 iter/s, 2.07191s/100 iter), loss = 0
I0701 15:05:43.833156  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:43.833164  1052 sgd_solver.cpp:106] Iteration 36900, lr = 0.00423437
I0701 15:05:45.884944  1052 solver.cpp:354] Sparsity after update:
I0701 15:05:45.886281  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:05:45.886287  1052 net.cpp:1851] conv1a_param_0(0.33) 
I0701 15:05:45.886294  1052 net.cpp:1851] conv1b_param_0(0.66) 
I0701 15:05:45.886296  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:05:45.886298  1052 net.cpp:1851] res2a_branch2a_param_0(0.66) 
I0701 15:05:45.886301  1052 net.cpp:1851] res2a_branch2b_param_0(0.66) 
I0701 15:05:45.886302  1052 net.cpp:1851] res3a_branch2a_param_0(0.66) 
I0701 15:05:45.886304  1052 net.cpp:1851] res3a_branch2b_param_0(0.66) 
I0701 15:05:45.886307  1052 net.cpp:1851] res4a_branch2a_param_0(0.66) 
I0701 15:05:45.886308  1052 net.cpp:1851] res4a_branch2b_param_0(0.66) 
I0701 15:05:45.886310  1052 net.cpp:1851] res5a_branch2a_param_0(0.66) 
I0701 15:05:45.886312  1052 net.cpp:1851] res5a_branch2b_param_0(0.66) 
I0701 15:05:45.886313  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.55334e+06/2.3599e+06) 0.658
I0701 15:05:45.886401  1052 solver.cpp:473] Iteration 37000, Testing net (#0)
I0701 15:05:47.526402  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.8796
I0701 15:05:47.526422  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9944
I0701 15:05:47.526427  1052 solver.cpp:546]     Test net output #2: loss = 0.3571 (* 1 = 0.3571 loss)
I0701 15:05:47.550861  1052 solver.cpp:290] Iteration 37000 (26.8991 iter/s, 3.7176s/100 iter), loss = 0
I0701 15:05:47.550879  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:47.550891  1052 sgd_solver.cpp:106] Iteration 37000, lr = 0.00421875
I0701 15:05:47.551532  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.68
I0701 15:05:47.936548  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:05:50.011864  1052 solver.cpp:290] Iteration 37100 (40.6353 iter/s, 2.46091s/100 iter), loss = 0
I0701 15:05:50.011886  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:50.011894  1052 sgd_solver.cpp:106] Iteration 37100, lr = 0.00420313
I0701 15:05:52.084600  1052 solver.cpp:290] Iteration 37200 (48.2474 iter/s, 2.07265s/100 iter), loss = 0
I0701 15:05:52.084622  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:52.084630  1052 sgd_solver.cpp:106] Iteration 37200, lr = 0.0041875
I0701 15:05:54.159637  1052 solver.cpp:290] Iteration 37300 (48.1939 iter/s, 2.07495s/100 iter), loss = 0
I0701 15:05:54.159659  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:54.159667  1052 sgd_solver.cpp:106] Iteration 37300, lr = 0.00417187
I0701 15:05:56.233080  1052 solver.cpp:290] Iteration 37400 (48.2309 iter/s, 2.07336s/100 iter), loss = 0
I0701 15:05:56.233103  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:56.233108  1052 sgd_solver.cpp:106] Iteration 37400, lr = 0.00415625
I0701 15:05:58.318388  1052 solver.cpp:290] Iteration 37500 (47.9566 iter/s, 2.08522s/100 iter), loss = 0
I0701 15:05:58.318410  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:05:58.318416  1052 sgd_solver.cpp:106] Iteration 37500, lr = 0.00414062
I0701 15:06:00.391392  1052 solver.cpp:290] Iteration 37600 (48.2412 iter/s, 2.07292s/100 iter), loss = 0
I0701 15:06:00.391415  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:00.391422  1052 sgd_solver.cpp:106] Iteration 37600, lr = 0.004125
I0701 15:06:02.470079  1052 solver.cpp:290] Iteration 37700 (48.1093 iter/s, 2.0786s/100 iter), loss = 0
I0701 15:06:02.470113  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:02.470119  1052 sgd_solver.cpp:106] Iteration 37700, lr = 0.00410937
I0701 15:06:04.542878  1052 solver.cpp:290] Iteration 37800 (48.2462 iter/s, 2.0727s/100 iter), loss = 0
I0701 15:06:04.542901  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:04.542909  1052 sgd_solver.cpp:106] Iteration 37800, lr = 0.00409375
I0701 15:06:06.619071  1052 solver.cpp:290] Iteration 37900 (48.1671 iter/s, 2.0761s/100 iter), loss = 0
I0701 15:06:06.619099  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:06.619108  1052 sgd_solver.cpp:106] Iteration 37900, lr = 0.00407812
I0701 15:06:08.681866  1052 solver.cpp:354] Sparsity after update:
I0701 15:06:08.683220  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:06:08.683228  1052 net.cpp:1851] conv1a_param_0(0.34) 
I0701 15:06:08.683238  1052 net.cpp:1851] conv1b_param_0(0.68) 
I0701 15:06:08.683243  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:06:08.683246  1052 net.cpp:1851] res2a_branch2a_param_0(0.68) 
I0701 15:06:08.683250  1052 net.cpp:1851] res2a_branch2b_param_0(0.68) 
I0701 15:06:08.683255  1052 net.cpp:1851] res3a_branch2a_param_0(0.68) 
I0701 15:06:08.683259  1052 net.cpp:1851] res3a_branch2b_param_0(0.68) 
I0701 15:06:08.683262  1052 net.cpp:1851] res4a_branch2a_param_0(0.68) 
I0701 15:06:08.683266  1052 net.cpp:1851] res4a_branch2b_param_0(0.68) 
I0701 15:06:08.683270  1052 net.cpp:1851] res5a_branch2a_param_0(0.68) 
I0701 15:06:08.683274  1052 net.cpp:1851] res5a_branch2b_param_0(0.68) 
I0701 15:06:08.683277  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.60043e+06/2.3599e+06) 0.678
I0701 15:06:08.683368  1052 solver.cpp:473] Iteration 38000, Testing net (#0)
I0701 15:06:10.326652  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9017
I0701 15:06:10.326671  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9957
I0701 15:06:10.326676  1052 solver.cpp:546]     Test net output #2: loss = 0.2948 (* 1 = 0.2948 loss)
I0701 15:06:10.346379  1052 solver.cpp:290] Iteration 38000 (26.83 iter/s, 3.72717s/100 iter), loss = 0
I0701 15:06:10.346401  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:10.346408  1052 sgd_solver.cpp:106] Iteration 38000, lr = 0.0040625
I0701 15:06:10.346977  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.7
I0701 15:06:10.747671  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:06:12.828042  1052 solver.cpp:290] Iteration 38100 (40.2971 iter/s, 2.48157s/100 iter), loss = 0
I0701 15:06:12.828115  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:12.828124  1052 sgd_solver.cpp:106] Iteration 38100, lr = 0.00404688
I0701 15:06:14.902555  1052 solver.cpp:290] Iteration 38200 (48.2072 iter/s, 2.07438s/100 iter), loss = 0
I0701 15:06:14.902578  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:14.902585  1052 sgd_solver.cpp:106] Iteration 38200, lr = 0.00403125
I0701 15:06:16.980960  1052 solver.cpp:290] Iteration 38300 (48.1158 iter/s, 2.07832s/100 iter), loss = 0
I0701 15:06:16.980983  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:16.980993  1052 sgd_solver.cpp:106] Iteration 38300, lr = 0.00401562
I0701 15:06:19.065608  1052 solver.cpp:290] Iteration 38400 (47.9717 iter/s, 2.08456s/100 iter), loss = 0
I0701 15:06:19.065630  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:19.065636  1052 sgd_solver.cpp:106] Iteration 38400, lr = 0.004
I0701 15:06:21.140908  1052 solver.cpp:290] Iteration 38500 (48.1878 iter/s, 2.07521s/100 iter), loss = 0
I0701 15:06:21.140929  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:21.140936  1052 sgd_solver.cpp:106] Iteration 38500, lr = 0.00398437
I0701 15:06:23.216217  1052 solver.cpp:290] Iteration 38600 (48.1876 iter/s, 2.07522s/100 iter), loss = 0
I0701 15:06:23.216239  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:23.216246  1052 sgd_solver.cpp:106] Iteration 38600, lr = 0.00396875
I0701 15:06:25.290696  1052 solver.cpp:290] Iteration 38700 (48.2069 iter/s, 2.07439s/100 iter), loss = 0
I0701 15:06:25.290719  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:25.290725  1052 sgd_solver.cpp:106] Iteration 38700, lr = 0.00395312
I0701 15:06:27.363950  1052 solver.cpp:290] Iteration 38800 (48.2354 iter/s, 2.07317s/100 iter), loss = 0
I0701 15:06:27.363971  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:27.363978  1052 sgd_solver.cpp:106] Iteration 38800, lr = 0.0039375
I0701 15:06:29.435075  1052 solver.cpp:290] Iteration 38900 (48.2849 iter/s, 2.07104s/100 iter), loss = 0
I0701 15:06:29.435096  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:29.435103  1052 sgd_solver.cpp:106] Iteration 38900, lr = 0.00392187
I0701 15:06:31.490288  1052 solver.cpp:354] Sparsity after update:
I0701 15:06:31.491608  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:06:31.491616  1052 net.cpp:1851] conv1a_param_0(0.35) 
I0701 15:06:31.491622  1052 net.cpp:1851] conv1b_param_0(0.7) 
I0701 15:06:31.491626  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:06:31.491627  1052 net.cpp:1851] res2a_branch2a_param_0(0.7) 
I0701 15:06:31.491629  1052 net.cpp:1851] res2a_branch2b_param_0(0.7) 
I0701 15:06:31.491631  1052 net.cpp:1851] res3a_branch2a_param_0(0.7) 
I0701 15:06:31.491632  1052 net.cpp:1851] res3a_branch2b_param_0(0.7) 
I0701 15:06:31.491634  1052 net.cpp:1851] res4a_branch2a_param_0(0.7) 
I0701 15:06:31.491636  1052 net.cpp:1851] res4a_branch2b_param_0(0.7) 
I0701 15:06:31.491638  1052 net.cpp:1851] res5a_branch2a_param_0(0.7) 
I0701 15:06:31.491641  1052 net.cpp:1851] res5a_branch2b_param_0(0.7) 
I0701 15:06:31.491642  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.6475e+06/2.3599e+06) 0.698
I0701 15:06:31.491724  1052 solver.cpp:473] Iteration 39000, Testing net (#0)
I0701 15:06:33.132666  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9018
I0701 15:06:33.132684  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9953
I0701 15:06:33.132689  1052 solver.cpp:546]     Test net output #2: loss = 0.2793 (* 1 = 0.2793 loss)
I0701 15:06:33.154287  1052 solver.cpp:290] Iteration 39000 (26.8883 iter/s, 3.71909s/100 iter), loss = 0
I0701 15:06:33.154302  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:33.154319  1052 sgd_solver.cpp:106] Iteration 39000, lr = 0.00390625
I0701 15:06:33.154927  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.72
I0701 15:06:33.600458  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:06:35.678663  1052 solver.cpp:290] Iteration 39100 (39.6152 iter/s, 2.52428s/100 iter), loss = 0
I0701 15:06:35.678685  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:35.678692  1052 sgd_solver.cpp:106] Iteration 39100, lr = 0.00389063
I0701 15:06:37.754403  1052 solver.cpp:290] Iteration 39200 (48.1776 iter/s, 2.07566s/100 iter), loss = 0
I0701 15:06:37.754426  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:37.754431  1052 sgd_solver.cpp:106] Iteration 39200, lr = 0.003875
I0701 15:06:39.829251  1052 solver.cpp:290] Iteration 39300 (48.1983 iter/s, 2.07476s/100 iter), loss = 0
I0701 15:06:39.829273  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:39.829282  1052 sgd_solver.cpp:106] Iteration 39300, lr = 0.00385938
I0701 15:06:41.900652  1052 solver.cpp:290] Iteration 39400 (48.2785 iter/s, 2.07132s/100 iter), loss = 0
I0701 15:06:41.900676  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:41.900683  1052 sgd_solver.cpp:106] Iteration 39400, lr = 0.00384375
I0701 15:06:43.973551  1052 solver.cpp:290] Iteration 39500 (48.2436 iter/s, 2.07281s/100 iter), loss = 0.047619
I0701 15:06:43.973645  1052 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0701 15:06:43.973654  1052 sgd_solver.cpp:106] Iteration 39500, lr = 0.00382812
I0701 15:06:46.045058  1052 solver.cpp:290] Iteration 39600 (48.2777 iter/s, 2.07135s/100 iter), loss = 0
I0701 15:06:46.045080  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:46.045089  1052 sgd_solver.cpp:106] Iteration 39600, lr = 0.0038125
I0701 15:06:48.119784  1052 solver.cpp:290] Iteration 39700 (48.2011 iter/s, 2.07464s/100 iter), loss = 0
I0701 15:06:48.119806  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:48.119812  1052 sgd_solver.cpp:106] Iteration 39700, lr = 0.00379687
I0701 15:06:50.195766  1052 solver.cpp:290] Iteration 39800 (48.172 iter/s, 2.0759s/100 iter), loss = 0
I0701 15:06:50.195789  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:50.195797  1052 sgd_solver.cpp:106] Iteration 39800, lr = 0.00378125
I0701 15:06:52.267448  1052 solver.cpp:290] Iteration 39900 (48.2719 iter/s, 2.0716s/100 iter), loss = 0
I0701 15:06:52.267472  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:52.267480  1052 sgd_solver.cpp:106] Iteration 39900, lr = 0.00376562
I0701 15:06:54.319475  1052 solver.cpp:600] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/cifar10_jacintonet11v2_iter_40000.caffemodel
I0701 15:06:54.336736  1052 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/cifar10_jacintonet11v2_iter_40000.solverstate
I0701 15:06:54.357524  1052 solver.cpp:354] Sparsity after update:
I0701 15:06:54.358479  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:06:54.358487  1052 net.cpp:1851] conv1a_param_0(0.36) 
I0701 15:06:54.358495  1052 net.cpp:1851] conv1b_param_0(0.72) 
I0701 15:06:54.358499  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:06:54.358501  1052 net.cpp:1851] res2a_branch2a_param_0(0.72) 
I0701 15:06:54.358503  1052 net.cpp:1851] res2a_branch2b_param_0(0.72) 
I0701 15:06:54.358505  1052 net.cpp:1851] res3a_branch2a_param_0(0.72) 
I0701 15:06:54.358507  1052 net.cpp:1851] res3a_branch2b_param_0(0.72) 
I0701 15:06:54.358510  1052 net.cpp:1851] res4a_branch2a_param_0(0.72) 
I0701 15:06:54.358512  1052 net.cpp:1851] res4a_branch2b_param_0(0.72) 
I0701 15:06:54.358515  1052 net.cpp:1851] res5a_branch2a_param_0(0.72) 
I0701 15:06:54.358515  1052 net.cpp:1851] res5a_branch2b_param_0(0.72) 
I0701 15:06:54.358518  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.69457e+06/2.3599e+06) 0.718
I0701 15:06:54.358613  1052 solver.cpp:473] Iteration 40000, Testing net (#0)
I0701 15:06:55.998543  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.8959
I0701 15:06:55.998561  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9944
I0701 15:06:55.998566  1052 solver.cpp:546]     Test net output #2: loss = 0.3177 (* 1 = 0.3177 loss)
I0701 15:06:56.018457  1052 solver.cpp:290] Iteration 40000 (26.6604 iter/s, 3.75088s/100 iter), loss = 0
I0701 15:06:56.018474  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:56.018486  1052 sgd_solver.cpp:106] Iteration 40000, lr = 0.00375
I0701 15:06:56.019122  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.74
I0701 15:06:56.519708  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:06:58.624063  1052 solver.cpp:290] Iteration 40100 (38.3802 iter/s, 2.60551s/100 iter), loss = 0
I0701 15:06:58.624084  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:06:58.624092  1052 sgd_solver.cpp:106] Iteration 40100, lr = 0.00373438
I0701 15:07:00.695520  1052 solver.cpp:290] Iteration 40200 (48.2772 iter/s, 2.07137s/100 iter), loss = 0
I0701 15:07:00.695543  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:00.695550  1052 sgd_solver.cpp:106] Iteration 40200, lr = 0.00371875
I0701 15:07:02.769898  1052 solver.cpp:290] Iteration 40300 (48.2092 iter/s, 2.07429s/100 iter), loss = 0
I0701 15:07:02.769920  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:02.769927  1052 sgd_solver.cpp:106] Iteration 40300, lr = 0.00370313
I0701 15:07:04.840912  1052 solver.cpp:290] Iteration 40400 (48.2875 iter/s, 2.07093s/100 iter), loss = 0
I0701 15:07:04.840934  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:04.840940  1052 sgd_solver.cpp:106] Iteration 40400, lr = 0.0036875
I0701 15:07:06.914180  1052 solver.cpp:290] Iteration 40500 (48.235 iter/s, 2.07318s/100 iter), loss = 0
I0701 15:07:06.914201  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:06.914207  1052 sgd_solver.cpp:106] Iteration 40500, lr = 0.00367187
I0701 15:07:08.986132  1052 solver.cpp:290] Iteration 40600 (48.2656 iter/s, 2.07187s/100 iter), loss = 0
I0701 15:07:08.986152  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:08.986160  1052 sgd_solver.cpp:106] Iteration 40600, lr = 0.00365625
I0701 15:07:11.057528  1052 solver.cpp:290] Iteration 40700 (48.2786 iter/s, 2.07131s/100 iter), loss = 0
I0701 15:07:11.057548  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:11.057555  1052 sgd_solver.cpp:106] Iteration 40700, lr = 0.00364062
I0701 15:07:13.129547  1052 solver.cpp:290] Iteration 40800 (48.264 iter/s, 2.07194s/100 iter), loss = 0
I0701 15:07:13.129570  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:13.129575  1052 sgd_solver.cpp:106] Iteration 40800, lr = 0.003625
I0701 15:07:15.202301  1052 solver.cpp:290] Iteration 40900 (48.247 iter/s, 2.07267s/100 iter), loss = 0
I0701 15:07:15.202373  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:15.202381  1052 sgd_solver.cpp:106] Iteration 40900, lr = 0.00360937
I0701 15:07:17.253896  1052 solver.cpp:354] Sparsity after update:
I0701 15:07:17.255237  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:07:17.255245  1052 net.cpp:1851] conv1a_param_0(0.37) 
I0701 15:07:17.255252  1052 net.cpp:1851] conv1b_param_0(0.74) 
I0701 15:07:17.255254  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:07:17.255256  1052 net.cpp:1851] res2a_branch2a_param_0(0.74) 
I0701 15:07:17.255259  1052 net.cpp:1851] res2a_branch2b_param_0(0.74) 
I0701 15:07:17.255260  1052 net.cpp:1851] res3a_branch2a_param_0(0.74) 
I0701 15:07:17.255262  1052 net.cpp:1851] res3a_branch2b_param_0(0.74) 
I0701 15:07:17.255264  1052 net.cpp:1851] res4a_branch2a_param_0(0.74) 
I0701 15:07:17.255266  1052 net.cpp:1851] res4a_branch2b_param_0(0.74) 
I0701 15:07:17.255269  1052 net.cpp:1851] res5a_branch2a_param_0(0.74) 
I0701 15:07:17.255270  1052 net.cpp:1851] res5a_branch2b_param_0(0.74) 
I0701 15:07:17.255272  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.74164e+06/2.3599e+06) 0.738
I0701 15:07:17.255365  1052 solver.cpp:473] Iteration 41000, Testing net (#0)
I0701 15:07:18.895387  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9003
I0701 15:07:18.895406  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9956
I0701 15:07:18.895411  1052 solver.cpp:546]     Test net output #2: loss = 0.2987 (* 1 = 0.2987 loss)
I0701 15:07:18.916050  1052 solver.cpp:290] Iteration 41000 (26.9282 iter/s, 3.71357s/100 iter), loss = 0
I0701 15:07:18.916066  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:18.916080  1052 sgd_solver.cpp:106] Iteration 41000, lr = 0.00359375
I0701 15:07:18.916700  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.76
I0701 15:07:19.454530  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:07:21.529126  1052 solver.cpp:290] Iteration 41100 (38.2705 iter/s, 2.61298s/100 iter), loss = 0
I0701 15:07:21.529147  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:21.529155  1052 sgd_solver.cpp:106] Iteration 41100, lr = 0.00357813
I0701 15:07:23.600281  1052 solver.cpp:290] Iteration 41200 (48.2842 iter/s, 2.07107s/100 iter), loss = 0
I0701 15:07:23.600303  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:23.600309  1052 sgd_solver.cpp:106] Iteration 41200, lr = 0.0035625
I0701 15:07:25.674244  1052 solver.cpp:290] Iteration 41300 (48.2189 iter/s, 2.07388s/100 iter), loss = 0
I0701 15:07:25.674265  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:25.674273  1052 sgd_solver.cpp:106] Iteration 41300, lr = 0.00354687
I0701 15:07:27.747905  1052 solver.cpp:290] Iteration 41400 (48.2259 iter/s, 2.07358s/100 iter), loss = 0
I0701 15:07:27.747926  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:27.747933  1052 sgd_solver.cpp:106] Iteration 41400, lr = 0.00353125
I0701 15:07:29.823629  1052 solver.cpp:290] Iteration 41500 (48.1779 iter/s, 2.07564s/100 iter), loss = 0
I0701 15:07:29.823652  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:29.823658  1052 sgd_solver.cpp:106] Iteration 41500, lr = 0.00351562
I0701 15:07:31.903961  1052 solver.cpp:290] Iteration 41600 (48.0713 iter/s, 2.08024s/100 iter), loss = 0
I0701 15:07:31.903985  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:31.903992  1052 sgd_solver.cpp:106] Iteration 41600, lr = 0.0035
I0701 15:07:33.977845  1052 solver.cpp:290] Iteration 41700 (48.2207 iter/s, 2.0738s/100 iter), loss = 0
I0701 15:07:33.977874  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:33.977881  1052 sgd_solver.cpp:106] Iteration 41700, lr = 0.00348437
I0701 15:07:36.048007  1052 solver.cpp:290] Iteration 41800 (48.3075 iter/s, 2.07007s/100 iter), loss = 0
I0701 15:07:36.048046  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:36.048053  1052 sgd_solver.cpp:106] Iteration 41800, lr = 0.00346875
I0701 15:07:38.126981  1052 solver.cpp:290] Iteration 41900 (48.103 iter/s, 2.07887s/100 iter), loss = 0
I0701 15:07:38.127002  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:38.127009  1052 sgd_solver.cpp:106] Iteration 41900, lr = 0.00345312
I0701 15:07:40.183691  1052 solver.cpp:354] Sparsity after update:
I0701 15:07:40.185030  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:07:40.185037  1052 net.cpp:1851] conv1a_param_0(0.38) 
I0701 15:07:40.185045  1052 net.cpp:1851] conv1b_param_0(0.76) 
I0701 15:07:40.185047  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:07:40.185050  1052 net.cpp:1851] res2a_branch2a_param_0(0.76) 
I0701 15:07:40.185056  1052 net.cpp:1851] res2a_branch2b_param_0(0.76) 
I0701 15:07:40.185058  1052 net.cpp:1851] res3a_branch2a_param_0(0.76) 
I0701 15:07:40.185060  1052 net.cpp:1851] res3a_branch2b_param_0(0.76) 
I0701 15:07:40.185062  1052 net.cpp:1851] res4a_branch2a_param_0(0.76) 
I0701 15:07:40.185065  1052 net.cpp:1851] res4a_branch2b_param_0(0.76) 
I0701 15:07:40.185066  1052 net.cpp:1851] res5a_branch2a_param_0(0.76) 
I0701 15:07:40.185067  1052 net.cpp:1851] res5a_branch2b_param_0(0.76) 
I0701 15:07:40.185070  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.78872e+06/2.3599e+06) 0.758
I0701 15:07:40.185153  1052 solver.cpp:473] Iteration 42000, Testing net (#0)
I0701 15:07:41.822499  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.900199
I0701 15:07:41.822520  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9952
I0701 15:07:41.822525  1052 solver.cpp:546]     Test net output #2: loss = 0.2838 (* 1 = 0.2838 loss)
I0701 15:07:41.842597  1052 solver.cpp:290] Iteration 42000 (26.9144 iter/s, 3.71549s/100 iter), loss = 0
I0701 15:07:41.842620  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:41.842630  1052 sgd_solver.cpp:106] Iteration 42000, lr = 0.0034375
I0701 15:07:41.843258  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.78
I0701 15:07:42.426532  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:07:44.503906  1052 solver.cpp:290] Iteration 42100 (37.5769 iter/s, 2.66121s/100 iter), loss = 0
I0701 15:07:44.503927  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:44.503934  1052 sgd_solver.cpp:106] Iteration 42100, lr = 0.00342188
I0701 15:07:46.577268  1052 solver.cpp:290] Iteration 42200 (48.2328 iter/s, 2.07328s/100 iter), loss = 0
I0701 15:07:46.577349  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:46.577360  1052 sgd_solver.cpp:106] Iteration 42200, lr = 0.00340625
I0701 15:07:48.652755  1052 solver.cpp:290] Iteration 42300 (48.1847 iter/s, 2.07535s/100 iter), loss = 0
I0701 15:07:48.652777  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:48.652784  1052 sgd_solver.cpp:106] Iteration 42300, lr = 0.00339063
I0701 15:07:50.723068  1052 solver.cpp:290] Iteration 42400 (48.3039 iter/s, 2.07023s/100 iter), loss = 0
I0701 15:07:50.723089  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:50.723098  1052 sgd_solver.cpp:106] Iteration 42400, lr = 0.003375
I0701 15:07:52.793658  1052 solver.cpp:290] Iteration 42500 (48.2974 iter/s, 2.07051s/100 iter), loss = 0
I0701 15:07:52.793681  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:52.793689  1052 sgd_solver.cpp:106] Iteration 42500, lr = 0.00335937
I0701 15:07:54.864538  1052 solver.cpp:290] Iteration 42600 (48.2907 iter/s, 2.07079s/100 iter), loss = 0
I0701 15:07:54.864567  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:54.864575  1052 sgd_solver.cpp:106] Iteration 42600, lr = 0.00334375
I0701 15:07:56.936810  1052 solver.cpp:290] Iteration 42700 (48.2583 iter/s, 2.07218s/100 iter), loss = 0
I0701 15:07:56.936832  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:56.936841  1052 sgd_solver.cpp:106] Iteration 42700, lr = 0.00332812
I0701 15:07:59.027246  1052 solver.cpp:290] Iteration 42800 (47.8389 iter/s, 2.09035s/100 iter), loss = 0
I0701 15:07:59.027267  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:07:59.027273  1052 sgd_solver.cpp:106] Iteration 42800, lr = 0.0033125
I0701 15:08:01.099779  1052 solver.cpp:290] Iteration 42900 (48.2522 iter/s, 2.07245s/100 iter), loss = 0
I0701 15:08:01.099804  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:01.099813  1052 sgd_solver.cpp:106] Iteration 42900, lr = 0.00329687
I0701 15:08:03.152734  1052 solver.cpp:354] Sparsity after update:
I0701 15:08:03.154064  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:08:03.154072  1052 net.cpp:1851] conv1a_param_0(0.39) 
I0701 15:08:03.154081  1052 net.cpp:1851] conv1b_param_0(0.78) 
I0701 15:08:03.154086  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:08:03.154090  1052 net.cpp:1851] res2a_branch2a_param_0(0.78) 
I0701 15:08:03.154094  1052 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0701 15:08:03.154098  1052 net.cpp:1851] res3a_branch2a_param_0(0.78) 
I0701 15:08:03.154103  1052 net.cpp:1851] res3a_branch2b_param_0(0.78) 
I0701 15:08:03.154105  1052 net.cpp:1851] res4a_branch2a_param_0(0.78) 
I0701 15:08:03.154109  1052 net.cpp:1851] res4a_branch2b_param_0(0.78) 
I0701 15:08:03.154114  1052 net.cpp:1851] res5a_branch2a_param_0(0.78) 
I0701 15:08:03.154117  1052 net.cpp:1851] res5a_branch2b_param_0(0.78) 
I0701 15:08:03.154121  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.83579e+06/2.3599e+06) 0.778
I0701 15:08:03.154209  1052 solver.cpp:473] Iteration 43000, Testing net (#0)
I0701 15:08:04.793078  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.8983
I0701 15:08:04.793097  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9955
I0701 15:08:04.793102  1052 solver.cpp:546]     Test net output #2: loss = 0.2773 (* 1 = 0.2773 loss)
I0701 15:08:04.812613  1052 solver.cpp:290] Iteration 43000 (26.9345 iter/s, 3.7127s/100 iter), loss = 0
I0701 15:08:04.812628  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:04.812641  1052 sgd_solver.cpp:106] Iteration 43000, lr = 0.00328125
I0701 15:08:04.813263  1052 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.8
I0701 15:08:05.434777  1052 net.cpp:1824] All zero weights of convolution layers are frozen
I0701 15:08:07.509186  1052 solver.cpp:290] Iteration 43100 (37.0854 iter/s, 2.69648s/100 iter), loss = 0
I0701 15:08:07.509228  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:07.509235  1052 sgd_solver.cpp:106] Iteration 43100, lr = 0.00326563
I0701 15:08:09.585624  1052 solver.cpp:290] Iteration 43200 (48.1618 iter/s, 2.07633s/100 iter), loss = -7.45058e-09
I0701 15:08:09.585647  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:09.585654  1052 sgd_solver.cpp:106] Iteration 43200, lr = 0.00325
I0701 15:08:11.657964  1052 solver.cpp:290] Iteration 43300 (48.2567 iter/s, 2.07225s/100 iter), loss = 0.047619
I0701 15:08:11.657986  1052 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0701 15:08:11.657994  1052 sgd_solver.cpp:106] Iteration 43300, lr = 0.00323438
I0701 15:08:13.732048  1052 solver.cpp:290] Iteration 43400 (48.216 iter/s, 2.074s/100 iter), loss = 0
I0701 15:08:13.732069  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:13.732076  1052 sgd_solver.cpp:106] Iteration 43400, lr = 0.00321875
I0701 15:08:15.807276  1052 solver.cpp:290] Iteration 43500 (48.1894 iter/s, 2.07514s/100 iter), loss = 0
I0701 15:08:15.807298  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:15.807304  1052 sgd_solver.cpp:106] Iteration 43500, lr = 0.00320312
I0701 15:08:17.881533  1052 solver.cpp:290] Iteration 43600 (48.212 iter/s, 2.07417s/100 iter), loss = 0
I0701 15:08:17.881603  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:17.881618  1052 sgd_solver.cpp:106] Iteration 43600, lr = 0.0031875
I0701 15:08:19.957825  1052 solver.cpp:290] Iteration 43700 (48.1657 iter/s, 2.07616s/100 iter), loss = 0
I0701 15:08:19.957846  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:19.957854  1052 sgd_solver.cpp:106] Iteration 43700, lr = 0.00317187
I0701 15:08:22.029760  1052 solver.cpp:290] Iteration 43800 (48.266 iter/s, 2.07185s/100 iter), loss = 0
I0701 15:08:22.029780  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:22.029788  1052 sgd_solver.cpp:106] Iteration 43800, lr = 0.00315625
I0701 15:08:24.099591  1052 solver.cpp:290] Iteration 43900 (48.3151 iter/s, 2.06975s/100 iter), loss = 0
I0701 15:08:24.099612  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:24.099618  1052 sgd_solver.cpp:106] Iteration 43900, lr = 0.00314062
I0701 15:08:26.154439  1052 solver.cpp:354] Sparsity after update:
I0701 15:08:26.155815  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:08:26.155823  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:08:26.155830  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:08:26.155834  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:08:26.155838  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:08:26.155843  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:08:26.155848  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:08:26.155851  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:08:26.155855  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:08:26.155859  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:08:26.155864  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:08:26.155869  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:08:26.155872  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:08:26.156006  1052 solver.cpp:473] Iteration 44000, Testing net (#0)
I0701 15:08:27.795007  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.8943
I0701 15:08:27.795027  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9939
I0701 15:08:27.795032  1052 solver.cpp:546]     Test net output #2: loss = 0.3171 (* 1 = 0.3171 loss)
I0701 15:08:27.814724  1052 solver.cpp:290] Iteration 44000 (26.9178 iter/s, 3.71501s/100 iter), loss = 0
I0701 15:08:27.814740  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:27.814754  1052 sgd_solver.cpp:106] Iteration 44000, lr = 0.003125
I0701 15:08:29.889102  1052 solver.cpp:290] Iteration 44100 (48.209 iter/s, 2.0743s/100 iter), loss = 0
I0701 15:08:29.889124  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:29.889132  1052 sgd_solver.cpp:106] Iteration 44100, lr = 0.00310938
I0701 15:08:31.960598  1052 solver.cpp:290] Iteration 44200 (48.2763 iter/s, 2.07141s/100 iter), loss = 0
I0701 15:08:31.960619  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:31.960628  1052 sgd_solver.cpp:106] Iteration 44200, lr = 0.00309375
I0701 15:08:34.034138  1052 solver.cpp:290] Iteration 44300 (48.2287 iter/s, 2.07345s/100 iter), loss = 0
I0701 15:08:34.034159  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:34.034167  1052 sgd_solver.cpp:106] Iteration 44300, lr = 0.00307812
I0701 15:08:36.113406  1052 solver.cpp:290] Iteration 44400 (48.0958 iter/s, 2.07918s/100 iter), loss = 0
I0701 15:08:36.113428  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:36.113435  1052 sgd_solver.cpp:106] Iteration 44400, lr = 0.0030625
I0701 15:08:38.188180  1052 solver.cpp:290] Iteration 44500 (48.2 iter/s, 2.07469s/100 iter), loss = 0
I0701 15:08:38.188202  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:38.188208  1052 sgd_solver.cpp:106] Iteration 44500, lr = 0.00304687
I0701 15:08:40.263411  1052 solver.cpp:290] Iteration 44600 (48.1894 iter/s, 2.07514s/100 iter), loss = 0
I0701 15:08:40.263443  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:40.263450  1052 sgd_solver.cpp:106] Iteration 44600, lr = 0.00303125
I0701 15:08:42.332327  1052 solver.cpp:290] Iteration 44700 (48.3367 iter/s, 2.06882s/100 iter), loss = 0
I0701 15:08:42.332352  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:42.332358  1052 sgd_solver.cpp:106] Iteration 44700, lr = 0.00301562
I0701 15:08:44.403625  1052 solver.cpp:290] Iteration 44800 (48.281 iter/s, 2.07121s/100 iter), loss = 0.047619
I0701 15:08:44.403650  1052 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0701 15:08:44.403657  1052 sgd_solver.cpp:106] Iteration 44800, lr = 0.003
I0701 15:08:46.477176  1052 solver.cpp:290] Iteration 44900 (48.2285 iter/s, 2.07346s/100 iter), loss = 0
I0701 15:08:46.477198  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:46.477206  1052 sgd_solver.cpp:106] Iteration 44900, lr = 0.00298437
I0701 15:08:48.528331  1052 solver.cpp:354] Sparsity after update:
I0701 15:08:48.529670  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:08:48.529676  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:08:48.529685  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:08:48.529686  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:08:48.529688  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:08:48.529690  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:08:48.529693  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:08:48.529695  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:08:48.529697  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:08:48.529700  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:08:48.529701  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:08:48.529705  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:08:48.529706  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:08:48.529795  1052 solver.cpp:473] Iteration 45000, Testing net (#0)
I0701 15:08:50.169348  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.8957
I0701 15:08:50.169368  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9949
I0701 15:08:50.169373  1052 solver.cpp:546]     Test net output #2: loss = 0.2967 (* 1 = 0.2967 loss)
I0701 15:08:50.189182  1052 solver.cpp:290] Iteration 45000 (26.9405 iter/s, 3.71188s/100 iter), loss = 0
I0701 15:08:50.189198  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:50.189211  1052 sgd_solver.cpp:106] Iteration 45000, lr = 0.00296875
I0701 15:08:52.262703  1052 solver.cpp:290] Iteration 45100 (48.229 iter/s, 2.07344s/100 iter), loss = 0
I0701 15:08:52.262725  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:52.262732  1052 sgd_solver.cpp:106] Iteration 45100, lr = 0.00295313
I0701 15:08:54.338541  1052 solver.cpp:290] Iteration 45200 (48.1753 iter/s, 2.07575s/100 iter), loss = 0
I0701 15:08:54.338564  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:54.338572  1052 sgd_solver.cpp:106] Iteration 45200, lr = 0.0029375
I0701 15:08:56.411126  1052 solver.cpp:290] Iteration 45300 (48.2509 iter/s, 2.0725s/100 iter), loss = 0
I0701 15:08:56.411149  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:56.411155  1052 sgd_solver.cpp:106] Iteration 45300, lr = 0.00292188
I0701 15:08:58.514102  1052 solver.cpp:290] Iteration 45400 (47.5536 iter/s, 2.10289s/100 iter), loss = 0
I0701 15:08:58.514124  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:08:58.514132  1052 sgd_solver.cpp:106] Iteration 45400, lr = 0.00290625
I0701 15:09:00.591624  1052 solver.cpp:290] Iteration 45500 (48.1362 iter/s, 2.07744s/100 iter), loss = 0
I0701 15:09:00.591647  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:00.591653  1052 sgd_solver.cpp:106] Iteration 45500, lr = 0.00289063
I0701 15:09:02.662837  1052 solver.cpp:290] Iteration 45600 (48.2829 iter/s, 2.07113s/100 iter), loss = 0
I0701 15:09:02.662858  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:02.662865  1052 sgd_solver.cpp:106] Iteration 45600, lr = 0.002875
I0701 15:09:04.736812  1052 solver.cpp:290] Iteration 45700 (48.2185 iter/s, 2.07389s/100 iter), loss = 0
I0701 15:09:04.736834  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:04.736840  1052 sgd_solver.cpp:106] Iteration 45700, lr = 0.00285937
I0701 15:09:06.817104  1052 solver.cpp:290] Iteration 45800 (48.0721 iter/s, 2.08021s/100 iter), loss = 0
I0701 15:09:06.817127  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:06.817136  1052 sgd_solver.cpp:106] Iteration 45800, lr = 0.00284375
I0701 15:09:08.891165  1052 solver.cpp:290] Iteration 45900 (48.2166 iter/s, 2.07397s/100 iter), loss = 0
I0701 15:09:08.891186  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:08.891192  1052 sgd_solver.cpp:106] Iteration 45900, lr = 0.00282812
I0701 15:09:10.944818  1052 solver.cpp:354] Sparsity after update:
I0701 15:09:10.946089  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:09:10.946099  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:09:10.946106  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:09:10.946110  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:09:10.946120  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:09:10.946125  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:09:10.946128  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:09:10.946132  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:09:10.946137  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:09:10.946141  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:09:10.946146  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:09:10.946149  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:09:10.946153  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:09:10.946295  1052 solver.cpp:473] Iteration 46000, Testing net (#0)
I0701 15:09:12.586824  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.8979
I0701 15:09:12.586843  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9961
I0701 15:09:12.586849  1052 solver.cpp:546]     Test net output #2: loss = 0.2837 (* 1 = 0.2837 loss)
I0701 15:09:12.606549  1052 solver.cpp:290] Iteration 46000 (26.9161 iter/s, 3.71526s/100 iter), loss = 0
I0701 15:09:12.606565  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:12.606578  1052 sgd_solver.cpp:106] Iteration 46000, lr = 0.0028125
I0701 15:09:14.682991  1052 solver.cpp:290] Iteration 46100 (48.1612 iter/s, 2.07636s/100 iter), loss = 0
I0701 15:09:14.683012  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:14.683018  1052 sgd_solver.cpp:106] Iteration 46100, lr = 0.00279688
I0701 15:09:16.756873  1052 solver.cpp:290] Iteration 46200 (48.2207 iter/s, 2.0738s/100 iter), loss = 0
I0701 15:09:16.756896  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:16.756902  1052 sgd_solver.cpp:106] Iteration 46200, lr = 0.00278125
I0701 15:09:18.828413  1052 solver.cpp:290] Iteration 46300 (48.2753 iter/s, 2.07145s/100 iter), loss = 0
I0701 15:09:18.828485  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:18.828491  1052 sgd_solver.cpp:106] Iteration 46300, lr = 0.00276563
I0701 15:09:20.899452  1052 solver.cpp:290] Iteration 46400 (48.288 iter/s, 2.07091s/100 iter), loss = 0
I0701 15:09:20.899474  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:20.899483  1052 sgd_solver.cpp:106] Iteration 46400, lr = 0.00275
I0701 15:09:22.969468  1052 solver.cpp:290] Iteration 46500 (48.3108 iter/s, 2.06993s/100 iter), loss = 0
I0701 15:09:22.969494  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:22.969503  1052 sgd_solver.cpp:106] Iteration 46500, lr = 0.00273437
I0701 15:09:25.043545  1052 solver.cpp:290] Iteration 46600 (48.2163 iter/s, 2.07399s/100 iter), loss = 0
I0701 15:09:25.043571  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:25.043581  1052 sgd_solver.cpp:106] Iteration 46600, lr = 0.00271875
I0701 15:09:27.116369  1052 solver.cpp:290] Iteration 46700 (48.2454 iter/s, 2.07274s/100 iter), loss = 0
I0701 15:09:27.116396  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:27.116405  1052 sgd_solver.cpp:106] Iteration 46700, lr = 0.00270312
I0701 15:09:29.197904  1052 solver.cpp:290] Iteration 46800 (48.0435 iter/s, 2.08145s/100 iter), loss = 0
I0701 15:09:29.197927  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:29.197934  1052 sgd_solver.cpp:106] Iteration 46800, lr = 0.0026875
I0701 15:09:31.270699  1052 solver.cpp:290] Iteration 46900 (48.2461 iter/s, 2.07271s/100 iter), loss = 0
I0701 15:09:31.270720  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:31.270727  1052 sgd_solver.cpp:106] Iteration 46900, lr = 0.00267187
I0701 15:09:33.320818  1052 solver.cpp:354] Sparsity after update:
I0701 15:09:33.322160  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:09:33.322166  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:09:33.322173  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:09:33.322175  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:09:33.322177  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:09:33.322180  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:09:33.322181  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:09:33.322183  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:09:33.322185  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:09:33.322186  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:09:33.322188  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:09:33.322190  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:09:33.322192  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:09:33.322275  1052 solver.cpp:473] Iteration 47000, Testing net (#0)
I0701 15:09:34.962116  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9019
I0701 15:09:34.962141  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9959
I0701 15:09:34.962146  1052 solver.cpp:546]     Test net output #2: loss = 0.2848 (* 1 = 0.2848 loss)
I0701 15:09:34.983624  1052 solver.cpp:290] Iteration 47000 (26.9339 iter/s, 3.71279s/100 iter), loss = 0
I0701 15:09:34.983647  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:34.983654  1052 sgd_solver.cpp:106] Iteration 47000, lr = 0.00265625
I0701 15:09:37.055693  1052 solver.cpp:290] Iteration 47100 (48.263 iter/s, 2.07198s/100 iter), loss = 0
I0701 15:09:37.055716  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:37.055726  1052 sgd_solver.cpp:106] Iteration 47100, lr = 0.00264063
I0701 15:09:39.128352  1052 solver.cpp:290] Iteration 47200 (48.2492 iter/s, 2.07257s/100 iter), loss = 0
I0701 15:09:39.128373  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:39.128381  1052 sgd_solver.cpp:106] Iteration 47200, lr = 0.002625
I0701 15:09:41.200176  1052 solver.cpp:290] Iteration 47300 (48.2686 iter/s, 2.07174s/100 iter), loss = 0
I0701 15:09:41.200214  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:41.200222  1052 sgd_solver.cpp:106] Iteration 47300, lr = 0.00260938
I0701 15:09:43.270733  1052 solver.cpp:290] Iteration 47400 (48.2985 iter/s, 2.07046s/100 iter), loss = 0
I0701 15:09:43.270756  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:43.270764  1052 sgd_solver.cpp:106] Iteration 47400, lr = 0.00259375
I0701 15:09:45.348609  1052 solver.cpp:290] Iteration 47500 (48.1281 iter/s, 2.07779s/100 iter), loss = 0
I0701 15:09:45.348634  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:45.348642  1052 sgd_solver.cpp:106] Iteration 47500, lr = 0.00257812
I0701 15:09:47.423852  1052 solver.cpp:290] Iteration 47600 (48.1891 iter/s, 2.07516s/100 iter), loss = 0
I0701 15:09:47.423877  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:47.423885  1052 sgd_solver.cpp:106] Iteration 47600, lr = 0.0025625
I0701 15:09:49.494818  1052 solver.cpp:290] Iteration 47700 (48.2887 iter/s, 2.07088s/100 iter), loss = 0
I0701 15:09:49.494886  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:49.494894  1052 sgd_solver.cpp:106] Iteration 47700, lr = 0.00254687
I0701 15:09:51.567056  1052 solver.cpp:290] Iteration 47800 (48.26 iter/s, 2.07211s/100 iter), loss = 0
I0701 15:09:51.567080  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:51.567085  1052 sgd_solver.cpp:106] Iteration 47800, lr = 0.00253125
I0701 15:09:53.638643  1052 solver.cpp:290] Iteration 47900 (48.2742 iter/s, 2.0715s/100 iter), loss = 0
I0701 15:09:53.638664  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:53.638670  1052 sgd_solver.cpp:106] Iteration 47900, lr = 0.00251562
I0701 15:09:55.693048  1052 solver.cpp:354] Sparsity after update:
I0701 15:09:55.694406  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:09:55.694414  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:09:55.694420  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:09:55.694423  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:09:55.694425  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:09:55.694427  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:09:55.694429  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:09:55.694432  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:09:55.694432  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:09:55.694434  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:09:55.694437  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:09:55.694438  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:09:55.694440  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:09:55.694524  1052 solver.cpp:473] Iteration 48000, Testing net (#0)
I0701 15:09:57.338568  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.904
I0701 15:09:57.338588  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9955
I0701 15:09:57.338593  1052 solver.cpp:546]     Test net output #2: loss = 0.2825 (* 1 = 0.2825 loss)
I0701 15:09:57.358561  1052 solver.cpp:290] Iteration 48000 (26.8832 iter/s, 3.71979s/100 iter), loss = 0
I0701 15:09:57.358579  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:57.358592  1052 sgd_solver.cpp:106] Iteration 48000, lr = 0.0025
I0701 15:09:59.431337  1052 solver.cpp:290] Iteration 48100 (48.2464 iter/s, 2.07269s/100 iter), loss = 0
I0701 15:09:59.431360  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:09:59.431366  1052 sgd_solver.cpp:106] Iteration 48100, lr = 0.00248438
I0701 15:10:01.503307  1052 solver.cpp:290] Iteration 48200 (48.2653 iter/s, 2.07188s/100 iter), loss = 0
I0701 15:10:01.503329  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:01.503334  1052 sgd_solver.cpp:106] Iteration 48200, lr = 0.00246875
I0701 15:10:03.579568  1052 solver.cpp:290] Iteration 48300 (48.1655 iter/s, 2.07617s/100 iter), loss = 0
I0701 15:10:03.579592  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:03.579602  1052 sgd_solver.cpp:106] Iteration 48300, lr = 0.00245313
I0701 15:10:05.661164  1052 solver.cpp:290] Iteration 48400 (48.0422 iter/s, 2.08151s/100 iter), loss = 0
I0701 15:10:05.661188  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:05.661195  1052 sgd_solver.cpp:106] Iteration 48400, lr = 0.0024375
I0701 15:10:07.737221  1052 solver.cpp:290] Iteration 48500 (48.1703 iter/s, 2.07597s/100 iter), loss = 0
I0701 15:10:07.737249  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:07.737257  1052 sgd_solver.cpp:106] Iteration 48500, lr = 0.00242188
I0701 15:10:09.807723  1052 solver.cpp:290] Iteration 48600 (48.2995 iter/s, 2.07041s/100 iter), loss = 0
I0701 15:10:09.807745  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:09.807752  1052 sgd_solver.cpp:106] Iteration 48600, lr = 0.00240625
I0701 15:10:11.878957  1052 solver.cpp:290] Iteration 48700 (48.2824 iter/s, 2.07115s/100 iter), loss = 0
I0701 15:10:11.878998  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:11.879005  1052 sgd_solver.cpp:106] Iteration 48700, lr = 0.00239062
I0701 15:10:13.950846  1052 solver.cpp:290] Iteration 48800 (48.2675 iter/s, 2.07179s/100 iter), loss = 0
I0701 15:10:13.950868  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:13.950875  1052 sgd_solver.cpp:106] Iteration 48800, lr = 0.002375
I0701 15:10:16.023214  1052 solver.cpp:290] Iteration 48900 (48.256 iter/s, 2.07228s/100 iter), loss = 0
I0701 15:10:16.023236  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:16.023242  1052 sgd_solver.cpp:106] Iteration 48900, lr = 0.00235937
I0701 15:10:18.076557  1052 solver.cpp:354] Sparsity after update:
I0701 15:10:18.077757  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:10:18.077765  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:10:18.077772  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:10:18.077775  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:10:18.077776  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:10:18.077778  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:10:18.077780  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:10:18.077782  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:10:18.077785  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:10:18.077786  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:10:18.077787  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:10:18.077790  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:10:18.077791  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:10:18.077874  1052 solver.cpp:473] Iteration 49000, Testing net (#0)
I0701 15:10:19.717150  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9016
I0701 15:10:19.717247  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9961
I0701 15:10:19.717253  1052 solver.cpp:546]     Test net output #2: loss = 0.2825 (* 1 = 0.2825 loss)
I0701 15:10:19.736929  1052 solver.cpp:290] Iteration 49000 (26.9281 iter/s, 3.71359s/100 iter), loss = 0
I0701 15:10:19.736948  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:19.736958  1052 sgd_solver.cpp:106] Iteration 49000, lr = 0.00234375
I0701 15:10:21.813936  1052 solver.cpp:290] Iteration 49100 (48.1481 iter/s, 2.07693s/100 iter), loss = 0.047619
I0701 15:10:21.813958  1052 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0701 15:10:21.813964  1052 sgd_solver.cpp:106] Iteration 49100, lr = 0.00232813
I0701 15:10:23.886368  1052 solver.cpp:290] Iteration 49200 (48.2545 iter/s, 2.07235s/100 iter), loss = 0
I0701 15:10:23.886394  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:23.886404  1052 sgd_solver.cpp:106] Iteration 49200, lr = 0.0023125
I0701 15:10:25.959590  1052 solver.cpp:290] Iteration 49300 (48.2362 iter/s, 2.07313s/100 iter), loss = 0
I0701 15:10:25.959614  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:25.959619  1052 sgd_solver.cpp:106] Iteration 49300, lr = 0.00229687
I0701 15:10:28.032434  1052 solver.cpp:290] Iteration 49400 (48.245 iter/s, 2.07276s/100 iter), loss = 0
I0701 15:10:28.032457  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:28.032464  1052 sgd_solver.cpp:106] Iteration 49400, lr = 0.00228125
I0701 15:10:30.105556  1052 solver.cpp:290] Iteration 49500 (48.2384 iter/s, 2.07304s/100 iter), loss = 0
I0701 15:10:30.105576  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:30.105584  1052 sgd_solver.cpp:106] Iteration 49500, lr = 0.00226562
I0701 15:10:32.175570  1052 solver.cpp:290] Iteration 49600 (48.3108 iter/s, 2.06993s/100 iter), loss = 0
I0701 15:10:32.175591  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:32.175598  1052 sgd_solver.cpp:106] Iteration 49600, lr = 0.00225
I0701 15:10:34.253938  1052 solver.cpp:290] Iteration 49700 (48.1166 iter/s, 2.07828s/100 iter), loss = 0
I0701 15:10:34.253962  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:34.253968  1052 sgd_solver.cpp:106] Iteration 49700, lr = 0.00223437
I0701 15:10:36.327270  1052 solver.cpp:290] Iteration 49800 (48.2335 iter/s, 2.07325s/100 iter), loss = 0
I0701 15:10:36.327292  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:36.327301  1052 sgd_solver.cpp:106] Iteration 49800, lr = 0.00221875
I0701 15:10:38.405032  1052 solver.cpp:290] Iteration 49900 (48.1307 iter/s, 2.07768s/100 iter), loss = 0
I0701 15:10:38.405055  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:38.405061  1052 sgd_solver.cpp:106] Iteration 49900, lr = 0.00220312
I0701 15:10:40.457765  1052 solver.cpp:600] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/cifar10_jacintonet11v2_iter_50000.caffemodel
I0701 15:10:40.474093  1052 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/cifar10_jacintonet11v2_iter_50000.solverstate
I0701 15:10:40.481266  1052 solver.cpp:354] Sparsity after update:
I0701 15:10:40.482187  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:10:40.482195  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:10:40.482206  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:10:40.482210  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:10:40.482215  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:10:40.482219  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:10:40.482224  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:10:40.482228  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:10:40.482233  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:10:40.482236  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:10:40.482249  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:10:40.482254  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:10:40.482257  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:10:40.482355  1052 solver.cpp:473] Iteration 50000, Testing net (#0)
I0701 15:10:42.122045  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9035
I0701 15:10:42.122066  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9963
I0701 15:10:42.122072  1052 solver.cpp:546]     Test net output #2: loss = 0.2867 (* 1 = 0.2867 loss)
I0701 15:10:42.141832  1052 solver.cpp:290] Iteration 50000 (26.7618 iter/s, 3.73667s/100 iter), loss = 0
I0701 15:10:42.141854  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:42.141861  1052 sgd_solver.cpp:106] Iteration 50000, lr = 0.0021875
I0701 15:10:44.219882  1052 solver.cpp:290] Iteration 50100 (48.124 iter/s, 2.07797s/100 iter), loss = 0
I0701 15:10:44.219903  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:44.219911  1052 sgd_solver.cpp:106] Iteration 50100, lr = 0.00217188
I0701 15:10:46.291302  1052 solver.cpp:290] Iteration 50200 (48.2781 iter/s, 2.07133s/100 iter), loss = 0
I0701 15:10:46.291326  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:46.291334  1052 sgd_solver.cpp:106] Iteration 50200, lr = 0.00215625
I0701 15:10:48.360561  1052 solver.cpp:290] Iteration 50300 (48.3285 iter/s, 2.06917s/100 iter), loss = 0
I0701 15:10:48.360586  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:48.360595  1052 sgd_solver.cpp:106] Iteration 50300, lr = 0.00214063
I0701 15:10:50.431143  1052 solver.cpp:290] Iteration 50400 (48.2977 iter/s, 2.07049s/100 iter), loss = 0
I0701 15:10:50.431252  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:50.431264  1052 sgd_solver.cpp:106] Iteration 50400, lr = 0.002125
I0701 15:10:52.503648  1052 solver.cpp:290] Iteration 50500 (48.2547 iter/s, 2.07234s/100 iter), loss = 0
I0701 15:10:52.503674  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:52.503682  1052 sgd_solver.cpp:106] Iteration 50500, lr = 0.00210937
I0701 15:10:54.575570  1052 solver.cpp:290] Iteration 50600 (48.2664 iter/s, 2.07184s/100 iter), loss = 0
I0701 15:10:54.575595  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:54.575603  1052 sgd_solver.cpp:106] Iteration 50600, lr = 0.00209375
I0701 15:10:56.648465  1052 solver.cpp:290] Iteration 50700 (48.2437 iter/s, 2.07281s/100 iter), loss = 0
I0701 15:10:56.648488  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:56.648494  1052 sgd_solver.cpp:106] Iteration 50700, lr = 0.00207812
I0701 15:10:58.729655  1052 solver.cpp:290] Iteration 50800 (48.0514 iter/s, 2.0811s/100 iter), loss = 0
I0701 15:10:58.729676  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:10:58.729683  1052 sgd_solver.cpp:106] Iteration 50800, lr = 0.0020625
I0701 15:11:00.802285  1052 solver.cpp:290] Iteration 50900 (48.2498 iter/s, 2.07255s/100 iter), loss = 0
I0701 15:11:00.802306  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:00.802314  1052 sgd_solver.cpp:106] Iteration 50900, lr = 0.00204687
I0701 15:11:02.855078  1052 solver.cpp:354] Sparsity after update:
I0701 15:11:02.856420  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:11:02.856426  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:11:02.856434  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:11:02.856436  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:11:02.856439  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:11:02.856441  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:11:02.856443  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:11:02.856447  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:11:02.856448  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:11:02.856451  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:11:02.856453  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:11:02.856456  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:11:02.856457  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:11:02.856542  1052 solver.cpp:473] Iteration 51000, Testing net (#0)
I0701 15:11:04.495090  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9029
I0701 15:11:04.495110  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.996
I0701 15:11:04.495115  1052 solver.cpp:546]     Test net output #2: loss = 0.2855 (* 1 = 0.2855 loss)
I0701 15:11:04.515501  1052 solver.cpp:290] Iteration 51000 (26.9317 iter/s, 3.71309s/100 iter), loss = 0
I0701 15:11:04.515518  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:04.515530  1052 sgd_solver.cpp:106] Iteration 51000, lr = 0.00203125
I0701 15:11:06.586220  1052 solver.cpp:290] Iteration 51100 (48.2943 iter/s, 2.07064s/100 iter), loss = 0
I0701 15:11:06.586242  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:06.586248  1052 sgd_solver.cpp:106] Iteration 51100, lr = 0.00201563
I0701 15:11:08.660064  1052 solver.cpp:290] Iteration 51200 (48.2216 iter/s, 2.07376s/100 iter), loss = 0
I0701 15:11:08.660085  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:08.660094  1052 sgd_solver.cpp:106] Iteration 51200, lr = 0.002
I0701 15:11:10.731592  1052 solver.cpp:290] Iteration 51300 (48.2755 iter/s, 2.07144s/100 iter), loss = 0
I0701 15:11:10.731613  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:10.731621  1052 sgd_solver.cpp:106] Iteration 51300, lr = 0.00198438
I0701 15:11:12.806778  1052 solver.cpp:290] Iteration 51400 (48.1904 iter/s, 2.0751s/100 iter), loss = 0
I0701 15:11:12.806818  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:12.806828  1052 sgd_solver.cpp:106] Iteration 51400, lr = 0.00196875
I0701 15:11:14.880406  1052 solver.cpp:290] Iteration 51500 (48.227 iter/s, 2.07353s/100 iter), loss = 0
I0701 15:11:14.880430  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:14.880439  1052 sgd_solver.cpp:106] Iteration 51500, lr = 0.00195312
I0701 15:11:16.955579  1052 solver.cpp:290] Iteration 51600 (48.1907 iter/s, 2.07509s/100 iter), loss = 0
I0701 15:11:16.955602  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:16.955608  1052 sgd_solver.cpp:106] Iteration 51600, lr = 0.0019375
I0701 15:11:19.027704  1052 solver.cpp:290] Iteration 51700 (48.2617 iter/s, 2.07204s/100 iter), loss = 0.0952381
I0701 15:11:19.027726  1052 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0701 15:11:19.027734  1052 sgd_solver.cpp:106] Iteration 51700, lr = 0.00192187
I0701 15:11:21.103821  1052 solver.cpp:290] Iteration 51800 (48.1689 iter/s, 2.07603s/100 iter), loss = 0
I0701 15:11:21.103899  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:21.103910  1052 sgd_solver.cpp:106] Iteration 51800, lr = 0.00190625
I0701 15:11:23.175449  1052 solver.cpp:290] Iteration 51900 (48.2744 iter/s, 2.07149s/100 iter), loss = 0
I0701 15:11:23.175473  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:23.175482  1052 sgd_solver.cpp:106] Iteration 51900, lr = 0.00189062
I0701 15:11:25.231765  1052 solver.cpp:354] Sparsity after update:
I0701 15:11:25.233068  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:11:25.233074  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:11:25.233083  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:11:25.233084  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:11:25.233088  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:11:25.233089  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:11:25.233090  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:11:25.233093  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:11:25.233095  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:11:25.233098  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:11:25.233099  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:11:25.233101  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:11:25.233104  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:11:25.233187  1052 solver.cpp:473] Iteration 52000, Testing net (#0)
I0701 15:11:26.870683  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9053
I0701 15:11:26.870702  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9957
I0701 15:11:26.870707  1052 solver.cpp:546]     Test net output #2: loss = 0.2838 (* 1 = 0.2838 loss)
I0701 15:11:26.890326  1052 solver.cpp:290] Iteration 52000 (26.9197 iter/s, 3.71475s/100 iter), loss = 0
I0701 15:11:26.890341  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:26.890357  1052 sgd_solver.cpp:106] Iteration 52000, lr = 0.001875
I0701 15:11:28.964016  1052 solver.cpp:290] Iteration 52100 (48.225 iter/s, 2.07361s/100 iter), loss = 0
I0701 15:11:28.964038  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:28.964045  1052 sgd_solver.cpp:106] Iteration 52100, lr = 0.00185938
I0701 15:11:31.037461  1052 solver.cpp:290] Iteration 52200 (48.231 iter/s, 2.07336s/100 iter), loss = 0
I0701 15:11:31.037485  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:31.037492  1052 sgd_solver.cpp:106] Iteration 52200, lr = 0.00184375
I0701 15:11:33.114460  1052 solver.cpp:290] Iteration 52300 (48.1485 iter/s, 2.07691s/100 iter), loss = 0
I0701 15:11:33.114483  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:33.114492  1052 sgd_solver.cpp:106] Iteration 52300, lr = 0.00182813
I0701 15:11:35.188736  1052 solver.cpp:290] Iteration 52400 (48.2116 iter/s, 2.07419s/100 iter), loss = 0
I0701 15:11:35.188763  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:35.188772  1052 sgd_solver.cpp:106] Iteration 52400, lr = 0.0018125
I0701 15:11:37.266635  1052 solver.cpp:290] Iteration 52500 (48.1276 iter/s, 2.07781s/100 iter), loss = 0
I0701 15:11:37.266656  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:37.266662  1052 sgd_solver.cpp:106] Iteration 52500, lr = 0.00179687
I0701 15:11:39.340826  1052 solver.cpp:290] Iteration 52600 (48.2135 iter/s, 2.07411s/100 iter), loss = 0
I0701 15:11:39.340848  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:39.340855  1052 sgd_solver.cpp:106] Iteration 52600, lr = 0.00178125
I0701 15:11:41.417341  1052 solver.cpp:290] Iteration 52700 (48.1596 iter/s, 2.07643s/100 iter), loss = 0
I0701 15:11:41.417363  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:41.417369  1052 sgd_solver.cpp:106] Iteration 52700, lr = 0.00176562
I0701 15:11:43.489948  1052 solver.cpp:290] Iteration 52800 (48.2504 iter/s, 2.07252s/100 iter), loss = 0
I0701 15:11:43.489990  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:43.489997  1052 sgd_solver.cpp:106] Iteration 52800, lr = 0.00175
I0701 15:11:45.564388  1052 solver.cpp:290] Iteration 52900 (48.2082 iter/s, 2.07434s/100 iter), loss = 0
I0701 15:11:45.564411  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:45.564419  1052 sgd_solver.cpp:106] Iteration 52900, lr = 0.00173437
I0701 15:11:47.622789  1052 solver.cpp:354] Sparsity after update:
I0701 15:11:47.623976  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:11:47.623983  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:11:47.623991  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:11:47.623993  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:11:47.623996  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:11:47.623998  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:11:47.624001  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:11:47.624002  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:11:47.624004  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:11:47.624007  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:11:47.624009  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:11:47.624012  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:11:47.624014  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:11:47.624097  1052 solver.cpp:473] Iteration 53000, Testing net (#0)
I0701 15:11:49.262186  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9054
I0701 15:11:49.262204  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9965
I0701 15:11:49.262209  1052 solver.cpp:546]     Test net output #2: loss = 0.2805 (* 1 = 0.2805 loss)
I0701 15:11:49.281908  1052 solver.cpp:290] Iteration 53000 (26.9006 iter/s, 3.71739s/100 iter), loss = 0
I0701 15:11:49.281926  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:49.281939  1052 sgd_solver.cpp:106] Iteration 53000, lr = 0.00171875
I0701 15:11:51.358696  1052 solver.cpp:290] Iteration 53100 (48.1532 iter/s, 2.07671s/100 iter), loss = 0
I0701 15:11:51.358752  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:51.358759  1052 sgd_solver.cpp:106] Iteration 53100, lr = 0.00170313
I0701 15:11:53.430172  1052 solver.cpp:290] Iteration 53200 (48.2775 iter/s, 2.07136s/100 iter), loss = 0
I0701 15:11:53.430194  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:53.430200  1052 sgd_solver.cpp:106] Iteration 53200, lr = 0.0016875
I0701 15:11:55.502728  1052 solver.cpp:290] Iteration 53300 (48.2516 iter/s, 2.07247s/100 iter), loss = 0
I0701 15:11:55.502750  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:55.502758  1052 sgd_solver.cpp:106] Iteration 53300, lr = 0.00167188
I0701 15:11:57.596928  1052 solver.cpp:290] Iteration 53400 (47.7529 iter/s, 2.09411s/100 iter), loss = 0
I0701 15:11:57.596951  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:57.596957  1052 sgd_solver.cpp:106] Iteration 53400, lr = 0.00165625
I0701 15:11:59.668181  1052 solver.cpp:290] Iteration 53500 (48.282 iter/s, 2.07117s/100 iter), loss = 0
I0701 15:11:59.668206  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:11:59.668215  1052 sgd_solver.cpp:106] Iteration 53500, lr = 0.00164062
I0701 15:12:01.742238  1052 solver.cpp:290] Iteration 53600 (48.2167 iter/s, 2.07397s/100 iter), loss = 0
I0701 15:12:01.742260  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:01.742267  1052 sgd_solver.cpp:106] Iteration 53600, lr = 0.001625
I0701 15:12:03.819176  1052 solver.cpp:290] Iteration 53700 (48.1498 iter/s, 2.07685s/100 iter), loss = 0
I0701 15:12:03.819198  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:03.819205  1052 sgd_solver.cpp:106] Iteration 53700, lr = 0.00160937
I0701 15:12:05.891324  1052 solver.cpp:290] Iteration 53800 (48.2611 iter/s, 2.07206s/100 iter), loss = 0
I0701 15:12:05.891345  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:05.891352  1052 sgd_solver.cpp:106] Iteration 53800, lr = 0.00159375
I0701 15:12:07.968186  1052 solver.cpp:290] Iteration 53900 (48.1515 iter/s, 2.07678s/100 iter), loss = 0
I0701 15:12:07.968209  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:07.968215  1052 sgd_solver.cpp:106] Iteration 53900, lr = 0.00157812
I0701 15:12:10.022521  1052 solver.cpp:354] Sparsity after update:
I0701 15:12:10.023880  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:12:10.023887  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:12:10.023895  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:12:10.023897  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:12:10.023900  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:12:10.023902  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:12:10.023905  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:12:10.023906  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:12:10.023908  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:12:10.023911  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:12:10.023913  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:12:10.023916  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:12:10.023917  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:12:10.024049  1052 solver.cpp:473] Iteration 54000, Testing net (#0)
I0701 15:12:11.663216  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9057
I0701 15:12:11.663234  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9959
I0701 15:12:11.663239  1052 solver.cpp:546]     Test net output #2: loss = 0.2807 (* 1 = 0.2807 loss)
I0701 15:12:11.683423  1052 solver.cpp:290] Iteration 54000 (26.9171 iter/s, 3.71511s/100 iter), loss = 0
I0701 15:12:11.683440  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:11.683454  1052 sgd_solver.cpp:106] Iteration 54000, lr = 0.0015625
I0701 15:12:13.756001  1052 solver.cpp:290] Iteration 54100 (48.251 iter/s, 2.0725s/100 iter), loss = 0
I0701 15:12:13.756033  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:13.756041  1052 sgd_solver.cpp:106] Iteration 54100, lr = 0.00154688
I0701 15:12:15.835737  1052 solver.cpp:290] Iteration 54200 (48.0852 iter/s, 2.07964s/100 iter), loss = 0
I0701 15:12:15.835768  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:15.835778  1052 sgd_solver.cpp:106] Iteration 54200, lr = 0.00153125
I0701 15:12:17.906008  1052 solver.cpp:290] Iteration 54300 (48.305 iter/s, 2.07018s/100 iter), loss = 0
I0701 15:12:17.906035  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:17.906044  1052 sgd_solver.cpp:106] Iteration 54300, lr = 0.00151563
I0701 15:12:19.976718  1052 solver.cpp:290] Iteration 54400 (48.2947 iter/s, 2.07062s/100 iter), loss = 0
I0701 15:12:19.976742  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:19.976750  1052 sgd_solver.cpp:106] Iteration 54400, lr = 0.0015
I0701 15:12:22.050171  1052 solver.cpp:290] Iteration 54500 (48.2307 iter/s, 2.07337s/100 iter), loss = 0
I0701 15:12:22.050272  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:22.050281  1052 sgd_solver.cpp:106] Iteration 54500, lr = 0.00148437
I0701 15:12:24.122274  1052 solver.cpp:290] Iteration 54600 (48.2639 iter/s, 2.07194s/100 iter), loss = 0
I0701 15:12:24.122294  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:24.122303  1052 sgd_solver.cpp:106] Iteration 54600, lr = 0.00146875
I0701 15:12:26.194259  1052 solver.cpp:290] Iteration 54700 (48.2649 iter/s, 2.0719s/100 iter), loss = 0
I0701 15:12:26.194280  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:26.194288  1052 sgd_solver.cpp:106] Iteration 54700, lr = 0.00145312
I0701 15:12:28.265162  1052 solver.cpp:290] Iteration 54800 (48.2901 iter/s, 2.07082s/100 iter), loss = 0
I0701 15:12:28.265183  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:28.265192  1052 sgd_solver.cpp:106] Iteration 54800, lr = 0.0014375
I0701 15:12:30.338882  1052 solver.cpp:290] Iteration 54900 (48.2245 iter/s, 2.07363s/100 iter), loss = 0
I0701 15:12:30.338904  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:30.338912  1052 sgd_solver.cpp:106] Iteration 54900, lr = 0.00142187
I0701 15:12:32.388968  1052 solver.cpp:354] Sparsity after update:
I0701 15:12:32.390302  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:12:32.390310  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:12:32.390321  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:12:32.390324  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:12:32.390329  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:12:32.390332  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:12:32.390336  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:12:32.390341  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:12:32.390344  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:12:32.390348  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:12:32.390353  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:12:32.390357  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:12:32.390360  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:12:32.390455  1052 solver.cpp:473] Iteration 55000, Testing net (#0)
I0701 15:12:34.030294  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9046
I0701 15:12:34.030313  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9958
I0701 15:12:34.030319  1052 solver.cpp:546]     Test net output #2: loss = 0.2785 (* 1 = 0.2785 loss)
I0701 15:12:34.050891  1052 solver.cpp:290] Iteration 55000 (26.9405 iter/s, 3.71188s/100 iter), loss = 0
I0701 15:12:34.050909  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:34.050920  1052 sgd_solver.cpp:106] Iteration 55000, lr = 0.00140625
I0701 15:12:36.127663  1052 solver.cpp:290] Iteration 55100 (48.1535 iter/s, 2.07669s/100 iter), loss = 0
I0701 15:12:36.127686  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:36.127693  1052 sgd_solver.cpp:106] Iteration 55100, lr = 0.00139063
I0701 15:12:38.204151  1052 solver.cpp:290] Iteration 55200 (48.1602 iter/s, 2.0764s/100 iter), loss = 0
I0701 15:12:38.204170  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:38.204179  1052 sgd_solver.cpp:106] Iteration 55200, lr = 0.001375
I0701 15:12:40.282009  1052 solver.cpp:290] Iteration 55300 (48.1284 iter/s, 2.07778s/100 iter), loss = 0
I0701 15:12:40.282032  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:40.282039  1052 sgd_solver.cpp:106] Iteration 55300, lr = 0.00135938
I0701 15:12:42.359025  1052 solver.cpp:290] Iteration 55400 (48.148 iter/s, 2.07693s/100 iter), loss = 0
I0701 15:12:42.359047  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:42.359053  1052 sgd_solver.cpp:106] Iteration 55400, lr = 0.00134375
I0701 15:12:44.432562  1052 solver.cpp:290] Iteration 55500 (48.2288 iter/s, 2.07345s/100 iter), loss = 0
I0701 15:12:44.432602  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:44.432610  1052 sgd_solver.cpp:106] Iteration 55500, lr = 0.00132813
I0701 15:12:46.505254  1052 solver.cpp:290] Iteration 55600 (48.2488 iter/s, 2.07259s/100 iter), loss = 0
I0701 15:12:46.505275  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:46.505281  1052 sgd_solver.cpp:106] Iteration 55600, lr = 0.0013125
I0701 15:12:48.580217  1052 solver.cpp:290] Iteration 55700 (48.1956 iter/s, 2.07488s/100 iter), loss = 0
I0701 15:12:48.580237  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:48.580245  1052 sgd_solver.cpp:106] Iteration 55700, lr = 0.00129687
I0701 15:12:50.653173  1052 solver.cpp:290] Iteration 55800 (48.2422 iter/s, 2.07287s/100 iter), loss = 0
I0701 15:12:50.653195  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:50.653203  1052 sgd_solver.cpp:106] Iteration 55800, lr = 0.00128125
I0701 15:12:52.725404  1052 solver.cpp:290] Iteration 55900 (48.2592 iter/s, 2.07214s/100 iter), loss = 0
I0701 15:12:52.725464  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:52.725472  1052 sgd_solver.cpp:106] Iteration 55900, lr = 0.00126562
I0701 15:12:54.777276  1052 solver.cpp:354] Sparsity after update:
I0701 15:12:54.778620  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:12:54.778626  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:12:54.778633  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:12:54.778635  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:12:54.778638  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:12:54.778640  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:12:54.778641  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:12:54.778643  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:12:54.778645  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:12:54.778647  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:12:54.778650  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:12:54.778651  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:12:54.778653  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:12:54.778741  1052 solver.cpp:473] Iteration 56000, Testing net (#0)
I0701 15:12:56.417760  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9047
I0701 15:12:56.417779  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9957
I0701 15:12:56.417784  1052 solver.cpp:546]     Test net output #2: loss = 0.2795 (* 1 = 0.2795 loss)
I0701 15:12:56.437500  1052 solver.cpp:290] Iteration 56000 (26.9401 iter/s, 3.71193s/100 iter), loss = 0
I0701 15:12:56.437517  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:56.437531  1052 sgd_solver.cpp:106] Iteration 56000, lr = 0.00125
I0701 15:12:58.536512  1052 solver.cpp:290] Iteration 56100 (47.6433 iter/s, 2.09893s/100 iter), loss = 0
I0701 15:12:58.536533  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:12:58.536541  1052 sgd_solver.cpp:106] Iteration 56100, lr = 0.00123438
I0701 15:13:00.608695  1052 solver.cpp:290] Iteration 56200 (48.2603 iter/s, 2.0721s/100 iter), loss = 0
I0701 15:13:00.608721  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:00.608727  1052 sgd_solver.cpp:106] Iteration 56200, lr = 0.00121875
I0701 15:13:02.685567  1052 solver.cpp:290] Iteration 56300 (48.1514 iter/s, 2.07678s/100 iter), loss = 0
I0701 15:13:02.685591  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:02.685598  1052 sgd_solver.cpp:106] Iteration 56300, lr = 0.00120313
I0701 15:13:04.759958  1052 solver.cpp:290] Iteration 56400 (48.209 iter/s, 2.0743s/100 iter), loss = 0
I0701 15:13:04.759982  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:04.759991  1052 sgd_solver.cpp:106] Iteration 56400, lr = 0.0011875
I0701 15:13:06.833923  1052 solver.cpp:290] Iteration 56500 (48.2188 iter/s, 2.07388s/100 iter), loss = 0
I0701 15:13:06.833947  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:06.833956  1052 sgd_solver.cpp:106] Iteration 56500, lr = 0.00117187
I0701 15:13:08.912364  1052 solver.cpp:290] Iteration 56600 (48.115 iter/s, 2.07835s/100 iter), loss = 0
I0701 15:13:08.912385  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:08.912394  1052 sgd_solver.cpp:106] Iteration 56600, lr = 0.00115625
I0701 15:13:10.988057  1052 solver.cpp:290] Iteration 56700 (48.1786 iter/s, 2.07561s/100 iter), loss = 0
I0701 15:13:10.988082  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:10.988090  1052 sgd_solver.cpp:106] Iteration 56700, lr = 0.00114062
I0701 15:13:13.073343  1052 solver.cpp:290] Iteration 56800 (47.957 iter/s, 2.0852s/100 iter), loss = 0
I0701 15:13:13.073364  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:13.073371  1052 sgd_solver.cpp:106] Iteration 56800, lr = 0.001125
I0701 15:13:15.149099  1052 solver.cpp:290] Iteration 56900 (48.1772 iter/s, 2.07567s/100 iter), loss = 0
I0701 15:13:15.149142  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:15.149152  1052 sgd_solver.cpp:106] Iteration 56900, lr = 0.00110937
I0701 15:13:17.201664  1052 solver.cpp:354] Sparsity after update:
I0701 15:13:17.203236  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:13:17.203246  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:13:17.203253  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:13:17.203256  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:13:17.203258  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:13:17.203263  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:13:17.203264  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:13:17.203265  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:13:17.203268  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:13:17.203269  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:13:17.203271  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:13:17.203274  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:13:17.203275  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:13:17.203371  1052 solver.cpp:473] Iteration 57000, Testing net (#0)
I0701 15:13:18.842630  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9062
I0701 15:13:18.842648  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9959
I0701 15:13:18.842653  1052 solver.cpp:546]     Test net output #2: loss = 0.2791 (* 1 = 0.2791 loss)
I0701 15:13:18.862259  1052 solver.cpp:290] Iteration 57000 (26.9323 iter/s, 3.71302s/100 iter), loss = 0
I0701 15:13:18.862274  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:18.862289  1052 sgd_solver.cpp:106] Iteration 57000, lr = 0.00109375
I0701 15:13:20.931884  1052 solver.cpp:290] Iteration 57100 (48.3198 iter/s, 2.06955s/100 iter), loss = 0
I0701 15:13:20.931905  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:20.931912  1052 sgd_solver.cpp:106] Iteration 57100, lr = 0.00107813
I0701 15:13:23.004346  1052 solver.cpp:290] Iteration 57200 (48.2537 iter/s, 2.07238s/100 iter), loss = 0
I0701 15:13:23.004396  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:23.004403  1052 sgd_solver.cpp:106] Iteration 57200, lr = 0.0010625
I0701 15:13:25.076750  1052 solver.cpp:290] Iteration 57300 (48.2557 iter/s, 2.07229s/100 iter), loss = 0
I0701 15:13:25.076771  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:25.076777  1052 sgd_solver.cpp:106] Iteration 57300, lr = 0.00104688
I0701 15:13:27.150394  1052 solver.cpp:290] Iteration 57400 (48.2263 iter/s, 2.07356s/100 iter), loss = 0
I0701 15:13:27.150416  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:27.150423  1052 sgd_solver.cpp:106] Iteration 57400, lr = 0.00103125
I0701 15:13:29.223793  1052 solver.cpp:290] Iteration 57500 (48.232 iter/s, 2.07331s/100 iter), loss = 0
I0701 15:13:29.223816  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:29.223822  1052 sgd_solver.cpp:106] Iteration 57500, lr = 0.00101562
I0701 15:13:31.298455  1052 solver.cpp:290] Iteration 57600 (48.2026 iter/s, 2.07458s/100 iter), loss = 0
I0701 15:13:31.298477  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:31.298485  1052 sgd_solver.cpp:106] Iteration 57600, lr = 0.001
I0701 15:13:33.370314  1052 solver.cpp:290] Iteration 57700 (48.2678 iter/s, 2.07177s/100 iter), loss = 0
I0701 15:13:33.370337  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:33.370343  1052 sgd_solver.cpp:106] Iteration 57700, lr = 0.000984375
I0701 15:13:35.442553  1052 solver.cpp:290] Iteration 57800 (48.259 iter/s, 2.07215s/100 iter), loss = 0
I0701 15:13:35.442575  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:35.442581  1052 sgd_solver.cpp:106] Iteration 57800, lr = 0.00096875
I0701 15:13:37.513998  1052 solver.cpp:290] Iteration 57900 (48.2775 iter/s, 2.07136s/100 iter), loss = 0
I0701 15:13:37.514019  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:37.514026  1052 sgd_solver.cpp:106] Iteration 57900, lr = 0.000953125
I0701 15:13:39.566022  1052 solver.cpp:354] Sparsity after update:
I0701 15:13:39.567350  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:13:39.567356  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:13:39.567368  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:13:39.567373  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:13:39.567378  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:13:39.567380  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:13:39.567384  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:13:39.567389  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:13:39.567394  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:13:39.567397  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:13:39.567401  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:13:39.567404  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:13:39.567409  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:13:39.567498  1052 solver.cpp:473] Iteration 58000, Testing net (#0)
I0701 15:13:41.209810  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9051
I0701 15:13:41.209839  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9959
I0701 15:13:41.209846  1052 solver.cpp:546]     Test net output #2: loss = 0.2799 (* 1 = 0.2799 loss)
I0701 15:13:41.229954  1052 solver.cpp:290] Iteration 58000 (26.9119 iter/s, 3.71583s/100 iter), loss = 0
I0701 15:13:41.229982  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:41.229992  1052 sgd_solver.cpp:106] Iteration 58000, lr = 0.0009375
I0701 15:13:43.302819  1052 solver.cpp:290] Iteration 58100 (48.2445 iter/s, 2.07277s/100 iter), loss = 0
I0701 15:13:43.302839  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:43.302846  1052 sgd_solver.cpp:106] Iteration 58100, lr = 0.000921875
I0701 15:13:45.378139  1052 solver.cpp:290] Iteration 58200 (48.1874 iter/s, 2.07523s/100 iter), loss = 0
I0701 15:13:45.378186  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:45.378196  1052 sgd_solver.cpp:106] Iteration 58200, lr = 0.00090625
I0701 15:13:47.448849  1052 solver.cpp:290] Iteration 58300 (48.2951 iter/s, 2.0706s/100 iter), loss = 0
I0701 15:13:47.448874  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:47.448880  1052 sgd_solver.cpp:106] Iteration 58300, lr = 0.000890625
I0701 15:13:49.521821  1052 solver.cpp:290] Iteration 58400 (48.2419 iter/s, 2.07289s/100 iter), loss = 0
I0701 15:13:49.521842  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:49.521850  1052 sgd_solver.cpp:106] Iteration 58400, lr = 0.000875
I0701 15:13:51.595252  1052 solver.cpp:290] Iteration 58500 (48.2312 iter/s, 2.07335s/100 iter), loss = 0
I0701 15:13:51.595273  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:51.595279  1052 sgd_solver.cpp:106] Iteration 58500, lr = 0.000859375
I0701 15:13:53.668828  1052 solver.cpp:290] Iteration 58600 (48.2279 iter/s, 2.07349s/100 iter), loss = 0
I0701 15:13:53.668905  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:53.668912  1052 sgd_solver.cpp:106] Iteration 58600, lr = 0.00084375
I0701 15:13:55.742139  1052 solver.cpp:290] Iteration 58700 (48.2352 iter/s, 2.07317s/100 iter), loss = 0
I0701 15:13:55.742161  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:55.742167  1052 sgd_solver.cpp:106] Iteration 58700, lr = 0.000828125
I0701 15:13:57.818140  1052 solver.cpp:290] Iteration 58800 (48.1715 iter/s, 2.07592s/100 iter), loss = 0
I0701 15:13:57.818161  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:57.818169  1052 sgd_solver.cpp:106] Iteration 58800, lr = 0.0008125
I0701 15:13:59.892568  1052 solver.cpp:290] Iteration 58900 (48.208 iter/s, 2.07434s/100 iter), loss = 0
I0701 15:13:59.892590  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:13:59.892596  1052 sgd_solver.cpp:106] Iteration 58900, lr = 0.000796875
I0701 15:14:01.947872  1052 solver.cpp:354] Sparsity after update:
I0701 15:14:01.949209  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:14:01.949216  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:14:01.949223  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:14:01.949225  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:14:01.949228  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:14:01.949229  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:14:01.949231  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:14:01.949234  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:14:01.949235  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:14:01.949237  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:14:01.949239  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:14:01.949240  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:14:01.949242  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:14:01.949334  1052 solver.cpp:473] Iteration 59000, Testing net (#0)
I0701 15:14:03.588155  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9065
I0701 15:14:03.588174  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9961
I0701 15:14:03.588179  1052 solver.cpp:546]     Test net output #2: loss = 0.2745 (* 1 = 0.2745 loss)
I0701 15:14:03.607730  1052 solver.cpp:290] Iteration 59000 (26.9176 iter/s, 3.71504s/100 iter), loss = 0
I0701 15:14:03.607746  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:03.607760  1052 sgd_solver.cpp:106] Iteration 59000, lr = 0.00078125
I0701 15:14:05.681505  1052 solver.cpp:290] Iteration 59100 (48.2231 iter/s, 2.0737s/100 iter), loss = 0
I0701 15:14:05.681526  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:05.681533  1052 sgd_solver.cpp:106] Iteration 59100, lr = 0.000765625
I0701 15:14:07.752403  1052 solver.cpp:290] Iteration 59200 (48.2902 iter/s, 2.07081s/100 iter), loss = 0
I0701 15:14:07.752424  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:07.752430  1052 sgd_solver.cpp:106] Iteration 59200, lr = 0.00075
I0701 15:14:09.823225  1052 solver.cpp:290] Iteration 59300 (48.292 iter/s, 2.07074s/100 iter), loss = 0
I0701 15:14:09.823248  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:09.823254  1052 sgd_solver.cpp:106] Iteration 59300, lr = 0.000734375
I0701 15:14:11.896019  1052 solver.cpp:290] Iteration 59400 (48.246 iter/s, 2.07271s/100 iter), loss = 0
I0701 15:14:11.896041  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:11.896049  1052 sgd_solver.cpp:106] Iteration 59400, lr = 0.00071875
I0701 15:14:13.972677  1052 solver.cpp:290] Iteration 59500 (48.1563 iter/s, 2.07657s/100 iter), loss = 0
I0701 15:14:13.972702  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:13.972712  1052 sgd_solver.cpp:106] Iteration 59500, lr = 0.000703125
I0701 15:14:16.047492  1052 solver.cpp:290] Iteration 59600 (48.1991 iter/s, 2.07473s/100 iter), loss = 0
I0701 15:14:16.047538  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:16.047547  1052 sgd_solver.cpp:106] Iteration 59600, lr = 0.0006875
I0701 15:14:18.124919  1052 solver.cpp:290] Iteration 59700 (48.1389 iter/s, 2.07732s/100 iter), loss = 0
I0701 15:14:18.124940  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:18.124948  1052 sgd_solver.cpp:106] Iteration 59700, lr = 0.000671875
I0701 15:14:20.196146  1052 solver.cpp:290] Iteration 59800 (48.2825 iter/s, 2.07114s/100 iter), loss = 0
I0701 15:14:20.196168  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:20.196175  1052 sgd_solver.cpp:106] Iteration 59800, lr = 0.00065625
I0701 15:14:22.268791  1052 solver.cpp:290] Iteration 59900 (48.2496 iter/s, 2.07256s/100 iter), loss = 0
I0701 15:14:22.268816  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:22.268826  1052 sgd_solver.cpp:106] Iteration 59900, lr = 0.000640625
I0701 15:14:24.328308  1052 solver.cpp:600] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/cifar10_jacintonet11v2_iter_60000.caffemodel
I0701 15:14:24.365751  1052 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/cifar10_jacintonet11v2_iter_60000.solverstate
I0701 15:14:24.372896  1052 solver.cpp:354] Sparsity after update:
I0701 15:14:24.373841  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:14:24.373847  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:14:24.373855  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:14:24.373857  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:14:24.373859  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:14:24.373862  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:14:24.373863  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:14:24.373865  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:14:24.373867  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:14:24.373868  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:14:24.373870  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:14:24.373872  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:14:24.373874  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:14:24.373972  1052 solver.cpp:473] Iteration 60000, Testing net (#0)
I0701 15:14:26.013275  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9069
I0701 15:14:26.013294  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9958
I0701 15:14:26.013299  1052 solver.cpp:546]     Test net output #2: loss = 0.2768 (* 1 = 0.2768 loss)
I0701 15:14:26.033428  1052 solver.cpp:290] Iteration 60000 (26.5639 iter/s, 3.76451s/100 iter), loss = 0
I0701 15:14:26.033447  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:26.033458  1052 sgd_solver.cpp:106] Iteration 60000, lr = 0.000625
I0701 15:14:28.104794  1052 solver.cpp:290] Iteration 60100 (48.2792 iter/s, 2.07128s/100 iter), loss = 0
I0701 15:14:28.104816  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:28.104825  1052 sgd_solver.cpp:106] Iteration 60100, lr = 0.000609375
I0701 15:14:30.180855  1052 solver.cpp:290] Iteration 60200 (48.1701 iter/s, 2.07598s/100 iter), loss = 0
I0701 15:14:30.180881  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:30.180891  1052 sgd_solver.cpp:106] Iteration 60200, lr = 0.00059375
I0701 15:14:32.252279  1052 solver.cpp:290] Iteration 60300 (48.2781 iter/s, 2.07133s/100 iter), loss = 0
I0701 15:14:32.252303  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:32.252310  1052 sgd_solver.cpp:106] Iteration 60300, lr = 0.000578125
I0701 15:14:34.329731  1052 solver.cpp:290] Iteration 60400 (48.1379 iter/s, 2.07737s/100 iter), loss = 0
I0701 15:14:34.329751  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:34.329759  1052 sgd_solver.cpp:106] Iteration 60400, lr = 0.0005625
I0701 15:14:36.403744  1052 solver.cpp:290] Iteration 60500 (48.2176 iter/s, 2.07393s/100 iter), loss = 0
I0701 15:14:36.403766  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:36.403774  1052 sgd_solver.cpp:106] Iteration 60500, lr = 0.000546875
I0701 15:14:38.476310  1052 solver.cpp:290] Iteration 60600 (48.2514 iter/s, 2.07248s/100 iter), loss = 0
I0701 15:14:38.476333  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:38.476341  1052 sgd_solver.cpp:106] Iteration 60600, lr = 0.00053125
I0701 15:14:40.547819  1052 solver.cpp:290] Iteration 60700 (48.276 iter/s, 2.07142s/100 iter), loss = 0
I0701 15:14:40.547842  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:40.547847  1052 sgd_solver.cpp:106] Iteration 60700, lr = 0.000515625
I0701 15:14:42.624524  1052 solver.cpp:290] Iteration 60800 (48.1552 iter/s, 2.07662s/100 iter), loss = 0
I0701 15:14:42.624546  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:42.624553  1052 sgd_solver.cpp:106] Iteration 60800, lr = 0.0005
I0701 15:14:44.697978  1052 solver.cpp:290] Iteration 60900 (48.2307 iter/s, 2.07337s/100 iter), loss = 0
I0701 15:14:44.698000  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:44.698006  1052 sgd_solver.cpp:106] Iteration 60900, lr = 0.000484375
I0701 15:14:46.750922  1052 solver.cpp:354] Sparsity after update:
I0701 15:14:46.752250  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:14:46.752257  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:14:46.752267  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:14:46.752271  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:14:46.752276  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:14:46.752280  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:14:46.752285  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:14:46.752288  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:14:46.752292  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:14:46.752296  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:14:46.752300  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:14:46.752305  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:14:46.752308  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:14:46.752398  1052 solver.cpp:473] Iteration 61000, Testing net (#0)
I0701 15:14:48.392947  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9068
I0701 15:14:48.392966  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9961
I0701 15:14:48.392973  1052 solver.cpp:546]     Test net output #2: loss = 0.2748 (* 1 = 0.2748 loss)
I0701 15:14:48.412914  1052 solver.cpp:290] Iteration 61000 (26.9193 iter/s, 3.71481s/100 iter), loss = 0
I0701 15:14:48.412930  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:48.412943  1052 sgd_solver.cpp:106] Iteration 61000, lr = 0.00046875
I0701 15:14:50.487335  1052 solver.cpp:290] Iteration 61100 (48.2081 iter/s, 2.07434s/100 iter), loss = 0
I0701 15:14:50.487359  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:50.487366  1052 sgd_solver.cpp:106] Iteration 61100, lr = 0.000453125
I0701 15:14:52.558001  1052 solver.cpp:290] Iteration 61200 (48.2957 iter/s, 2.07058s/100 iter), loss = 0
I0701 15:14:52.558023  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:52.558030  1052 sgd_solver.cpp:106] Iteration 61200, lr = 0.0004375
I0701 15:14:54.631311  1052 solver.cpp:290] Iteration 61300 (48.234 iter/s, 2.07323s/100 iter), loss = 0
I0701 15:14:54.631408  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:54.631417  1052 sgd_solver.cpp:106] Iteration 61300, lr = 0.000421875
I0701 15:14:56.705013  1052 solver.cpp:290] Iteration 61400 (48.2266 iter/s, 2.07354s/100 iter), loss = 0
I0701 15:14:56.705034  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:56.705041  1052 sgd_solver.cpp:106] Iteration 61400, lr = 0.00040625
I0701 15:14:58.785362  1052 solver.cpp:290] Iteration 61500 (48.0708 iter/s, 2.08026s/100 iter), loss = 0
I0701 15:14:58.785384  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:14:58.785393  1052 sgd_solver.cpp:106] Iteration 61500, lr = 0.000390625
I0701 15:15:00.856842  1052 solver.cpp:290] Iteration 61600 (48.2766 iter/s, 2.0714s/100 iter), loss = 0
I0701 15:15:00.856863  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:00.856870  1052 sgd_solver.cpp:106] Iteration 61600, lr = 0.000375
I0701 15:15:02.928439  1052 solver.cpp:290] Iteration 61700 (48.2739 iter/s, 2.07151s/100 iter), loss = 0
I0701 15:15:02.928460  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:02.928468  1052 sgd_solver.cpp:106] Iteration 61700, lr = 0.000359375
I0701 15:15:05.001509  1052 solver.cpp:290] Iteration 61800 (48.2396 iter/s, 2.07298s/100 iter), loss = 0
I0701 15:15:05.001531  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:05.001538  1052 sgd_solver.cpp:106] Iteration 61800, lr = 0.00034375
I0701 15:15:07.075096  1052 solver.cpp:290] Iteration 61900 (48.2276 iter/s, 2.0735s/100 iter), loss = 0
I0701 15:15:07.075117  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:07.075125  1052 sgd_solver.cpp:106] Iteration 61900, lr = 0.000328125
I0701 15:15:09.130596  1052 solver.cpp:354] Sparsity after update:
I0701 15:15:09.131899  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:15:09.131906  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:15:09.131913  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:15:09.131916  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:15:09.131918  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:15:09.131922  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:15:09.131923  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:15:09.131927  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:15:09.131928  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:15:09.131932  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:15:09.131933  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:15:09.131935  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:15:09.131937  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:15:09.132025  1052 solver.cpp:473] Iteration 62000, Testing net (#0)
I0701 15:15:10.770891  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9068
I0701 15:15:10.770910  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9959
I0701 15:15:10.770915  1052 solver.cpp:546]     Test net output #2: loss = 0.2767 (* 1 = 0.2767 loss)
I0701 15:15:10.790518  1052 solver.cpp:290] Iteration 62000 (26.9158 iter/s, 3.7153s/100 iter), loss = 0
I0701 15:15:10.790535  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:10.790549  1052 sgd_solver.cpp:106] Iteration 62000, lr = 0.0003125
I0701 15:15:12.861302  1052 solver.cpp:290] Iteration 62100 (48.2928 iter/s, 2.0707s/100 iter), loss = 0
I0701 15:15:12.861335  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:12.861344  1052 sgd_solver.cpp:106] Iteration 62100, lr = 0.000296875
I0701 15:15:14.933246  1052 solver.cpp:290] Iteration 62200 (48.266 iter/s, 2.07185s/100 iter), loss = 0
I0701 15:15:14.933274  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:14.933284  1052 sgd_solver.cpp:106] Iteration 62200, lr = 0.00028125
I0701 15:15:17.009932  1052 solver.cpp:290] Iteration 62300 (48.1557 iter/s, 2.0766s/100 iter), loss = 0
I0701 15:15:17.009968  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:17.009974  1052 sgd_solver.cpp:106] Iteration 62300, lr = 0.000265625
I0701 15:15:19.088891  1052 solver.cpp:290] Iteration 62400 (48.1033 iter/s, 2.07886s/100 iter), loss = 0
I0701 15:15:19.088912  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:19.088918  1052 sgd_solver.cpp:106] Iteration 62400, lr = 0.00025
I0701 15:15:21.165277  1052 solver.cpp:290] Iteration 62500 (48.1625 iter/s, 2.0763s/100 iter), loss = 0
I0701 15:15:21.165298  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:21.165304  1052 sgd_solver.cpp:106] Iteration 62500, lr = 0.000234375
I0701 15:15:23.238895  1052 solver.cpp:290] Iteration 62600 (48.2269 iter/s, 2.07353s/100 iter), loss = 0
I0701 15:15:23.238917  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:23.238924  1052 sgd_solver.cpp:106] Iteration 62600, lr = 0.00021875
I0701 15:15:25.312592  1052 solver.cpp:290] Iteration 62700 (48.2251 iter/s, 2.07361s/100 iter), loss = 0
I0701 15:15:25.312667  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:25.312680  1052 sgd_solver.cpp:106] Iteration 62700, lr = 0.000203125
I0701 15:15:27.383777  1052 solver.cpp:290] Iteration 62800 (48.2847 iter/s, 2.07105s/100 iter), loss = 0
I0701 15:15:27.383800  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:27.383808  1052 sgd_solver.cpp:106] Iteration 62800, lr = 0.0001875
I0701 15:15:29.465044  1052 solver.cpp:290] Iteration 62900 (48.0497 iter/s, 2.08118s/100 iter), loss = 0
I0701 15:15:29.465066  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:29.465075  1052 sgd_solver.cpp:106] Iteration 62900, lr = 0.000171875
I0701 15:15:31.517002  1052 solver.cpp:354] Sparsity after update:
I0701 15:15:31.518321  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:15:31.518328  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:15:31.518335  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:15:31.518337  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:15:31.518339  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:15:31.518342  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:15:31.518343  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:15:31.518345  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:15:31.518347  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:15:31.518348  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:15:31.518350  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:15:31.518352  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:15:31.518354  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:15:31.518457  1052 solver.cpp:473] Iteration 63000, Testing net (#0)
I0701 15:15:33.159016  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9056
I0701 15:15:33.159035  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9962
I0701 15:15:33.159039  1052 solver.cpp:546]     Test net output #2: loss = 0.2779 (* 1 = 0.2779 loss)
I0701 15:15:33.178573  1052 solver.cpp:290] Iteration 63000 (26.9295 iter/s, 3.7134s/100 iter), loss = 0
I0701 15:15:33.178589  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:33.178602  1052 sgd_solver.cpp:106] Iteration 63000, lr = 0.00015625
I0701 15:15:35.257236  1052 solver.cpp:290] Iteration 63100 (48.1097 iter/s, 2.07858s/100 iter), loss = 0
I0701 15:15:35.257261  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:35.257269  1052 sgd_solver.cpp:106] Iteration 63100, lr = 0.000140625
I0701 15:15:37.328631  1052 solver.cpp:290] Iteration 63200 (48.2787 iter/s, 2.07131s/100 iter), loss = 0
I0701 15:15:37.328658  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:37.328667  1052 sgd_solver.cpp:106] Iteration 63200, lr = 0.000125
I0701 15:15:39.402906  1052 solver.cpp:290] Iteration 63300 (48.2116 iter/s, 2.07419s/100 iter), loss = 0
I0701 15:15:39.402927  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:39.402935  1052 sgd_solver.cpp:106] Iteration 63300, lr = 0.000109375
I0701 15:15:41.476460  1052 solver.cpp:290] Iteration 63400 (48.2284 iter/s, 2.07347s/100 iter), loss = 0
I0701 15:15:41.476481  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:41.476490  1052 sgd_solver.cpp:106] Iteration 63400, lr = 9.37498e-05
I0701 15:15:43.549033  1052 solver.cpp:290] Iteration 63500 (48.2512 iter/s, 2.07249s/100 iter), loss = 0
I0701 15:15:43.549055  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:43.549064  1052 sgd_solver.cpp:106] Iteration 63500, lr = 7.8125e-05
I0701 15:15:45.618161  1052 solver.cpp:290] Iteration 63600 (48.3315 iter/s, 2.06904s/100 iter), loss = 0
I0701 15:15:45.618183  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:45.618191  1052 sgd_solver.cpp:106] Iteration 63600, lr = 6.25002e-05
I0701 15:15:47.691979  1052 solver.cpp:290] Iteration 63700 (48.2222 iter/s, 2.07373s/100 iter), loss = 0
I0701 15:15:47.692016  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:47.692023  1052 sgd_solver.cpp:106] Iteration 63700, lr = 4.68749e-05
I0701 15:15:49.767176  1052 solver.cpp:290] Iteration 63800 (48.1905 iter/s, 2.0751s/100 iter), loss = 0
I0701 15:15:49.767199  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:49.767205  1052 sgd_solver.cpp:106] Iteration 63800, lr = 3.12501e-05
I0701 15:15:51.839397  1052 solver.cpp:290] Iteration 63900 (48.2594 iter/s, 2.07214s/100 iter), loss = 0
I0701 15:15:51.839421  1052 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0701 15:15:51.839426  1052 sgd_solver.cpp:106] Iteration 63900, lr = 1.56248e-05
I0701 15:15:53.894714  1052 solver.cpp:354] Sparsity after update:
I0701 15:15:53.896200  1052 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0701 15:15:53.896208  1052 net.cpp:1851] conv1a_param_0(0.4) 
I0701 15:15:53.896214  1052 net.cpp:1851] conv1b_param_0(0.8) 
I0701 15:15:53.896216  1052 net.cpp:1851] fc10_param_0(0) 
I0701 15:15:53.896219  1052 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 15:15:53.896220  1052 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 15:15:53.896222  1052 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 15:15:53.896224  1052 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 15:15:53.896226  1052 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 15:15:53.896229  1052 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 15:15:53.896230  1052 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 15:15:53.896231  1052 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 15:15:53.896234  1052 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0701 15:15:53.896241  1052 solver.cpp:600] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/cifar10_jacintonet11v2_iter_64000.caffemodel
I0701 15:15:53.912081  1052 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-01_14-28-09/sparse/cifar10_jacintonet11v2_iter_64000.solverstate
I0701 15:15:53.923957  1052 solver.cpp:453] Iteration 64000, loss = 0
I0701 15:15:53.923976  1052 solver.cpp:473] Iteration 64000, Testing net (#0)
I0701 15:15:55.562881  1052 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9074
I0701 15:15:55.562994  1052 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.9961
I0701 15:15:55.563000  1052 solver.cpp:546]     Test net output #2: loss = 0.2748 (* 1 = 0.2748 loss)
I0701 15:15:55.563004  1052 solver.cpp:458] Optimization Done.
I0701 15:15:55.607966  1052 caffe.cpp:246] Optimization Done.
