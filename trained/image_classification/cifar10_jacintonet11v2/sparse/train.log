I0704 07:43:18.417459 25348 caffe.cpp:209] Using GPUs 0, 1, 2
I0704 07:43:18.417932 25348 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0704 07:43:18.418257 25348 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0704 07:43:18.418592 25348 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0704 07:43:18.813833 25348 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.001
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: true
iter_size: 1
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.02
sparsity_step_iter: 1000
sparsity_start_iter: 4000
sparsity_start_factor: 0
I0704 07:43:18.813925 25348 solver.cpp:82] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/train.prototxt
I0704 07:43:18.814389 25348 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0704 07:43:18.814395 25348 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0704 07:43:18.814551 25348 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 21
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0704 07:43:18.814636 25348 layer_factory.hpp:77] Creating layer data
I0704 07:43:18.814719 25348 net.cpp:98] Creating Layer data
I0704 07:43:18.814725 25348 net.cpp:413] data -> data
I0704 07:43:18.814741 25348 net.cpp:413] data -> label
I0704 07:43:18.815533 25377 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0704 07:43:18.831476 25348 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0704 07:43:18.831531 25348 data_layer.cpp:83] output data size: 21,3,32,32
I0704 07:43:18.833170 25348 net.cpp:148] Setting up data
I0704 07:43:18.833183 25348 net.cpp:155] Top shape: 21 3 32 32 (64512)
I0704 07:43:18.833185 25348 net.cpp:155] Top shape: 21 (21)
I0704 07:43:18.833187 25348 net.cpp:163] Memory required for data: 258132
I0704 07:43:18.833192 25348 layer_factory.hpp:77] Creating layer data/bias
I0704 07:43:18.833199 25348 net.cpp:98] Creating Layer data/bias
I0704 07:43:18.833202 25348 net.cpp:439] data/bias <- data
I0704 07:43:18.833209 25348 net.cpp:413] data/bias -> data/bias
I0704 07:43:18.834234 25348 net.cpp:148] Setting up data/bias
I0704 07:43:18.834244 25348 net.cpp:155] Top shape: 21 3 32 32 (64512)
I0704 07:43:18.834246 25348 net.cpp:163] Memory required for data: 516180
I0704 07:43:18.834254 25348 layer_factory.hpp:77] Creating layer conv1a
I0704 07:43:18.834262 25348 net.cpp:98] Creating Layer conv1a
I0704 07:43:18.834264 25348 net.cpp:439] conv1a <- data/bias
I0704 07:43:18.834267 25348 net.cpp:413] conv1a -> conv1a
I0704 07:43:18.834519 25379 blocking_queue.cpp:50] Waiting for data
I0704 07:43:18.835582 25348 net.cpp:148] Setting up conv1a
I0704 07:43:18.835590 25348 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:43:18.835592 25348 net.cpp:163] Memory required for data: 3268692
I0704 07:43:18.835597 25348 layer_factory.hpp:77] Creating layer conv1a/bn
I0704 07:43:18.835603 25348 net.cpp:98] Creating Layer conv1a/bn
I0704 07:43:18.835605 25348 net.cpp:439] conv1a/bn <- conv1a
I0704 07:43:18.835608 25348 net.cpp:413] conv1a/bn -> conv1a/bn
I0704 07:43:18.836293 25348 net.cpp:148] Setting up conv1a/bn
I0704 07:43:18.836299 25348 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:43:18.836302 25348 net.cpp:163] Memory required for data: 6021204
I0704 07:43:18.836308 25348 layer_factory.hpp:77] Creating layer conv1a/relu
I0704 07:43:18.836311 25348 net.cpp:98] Creating Layer conv1a/relu
I0704 07:43:18.836313 25348 net.cpp:439] conv1a/relu <- conv1a/bn
I0704 07:43:18.836316 25348 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0704 07:43:18.836328 25348 net.cpp:148] Setting up conv1a/relu
I0704 07:43:18.836330 25348 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:43:18.836341 25348 net.cpp:163] Memory required for data: 8773716
I0704 07:43:18.836344 25348 layer_factory.hpp:77] Creating layer conv1b
I0704 07:43:18.836347 25348 net.cpp:98] Creating Layer conv1b
I0704 07:43:18.836350 25348 net.cpp:439] conv1b <- conv1a/bn
I0704 07:43:18.836352 25348 net.cpp:413] conv1b -> conv1b
I0704 07:43:18.836679 25348 net.cpp:148] Setting up conv1b
I0704 07:43:18.836685 25348 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:43:18.836688 25348 net.cpp:163] Memory required for data: 11526228
I0704 07:43:18.836693 25348 layer_factory.hpp:77] Creating layer conv1b/bn
I0704 07:43:18.836695 25348 net.cpp:98] Creating Layer conv1b/bn
I0704 07:43:18.836699 25348 net.cpp:439] conv1b/bn <- conv1b
I0704 07:43:18.836700 25348 net.cpp:413] conv1b/bn -> conv1b/bn
I0704 07:43:18.837368 25348 net.cpp:148] Setting up conv1b/bn
I0704 07:43:18.837373 25348 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:43:18.837375 25348 net.cpp:163] Memory required for data: 14278740
I0704 07:43:18.837380 25348 layer_factory.hpp:77] Creating layer conv1b/relu
I0704 07:43:18.837384 25348 net.cpp:98] Creating Layer conv1b/relu
I0704 07:43:18.837386 25348 net.cpp:439] conv1b/relu <- conv1b/bn
I0704 07:43:18.837388 25348 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0704 07:43:18.837393 25348 net.cpp:148] Setting up conv1b/relu
I0704 07:43:18.837395 25348 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:43:18.837397 25348 net.cpp:163] Memory required for data: 17031252
I0704 07:43:18.837399 25348 layer_factory.hpp:77] Creating layer pool1
I0704 07:43:18.837405 25348 net.cpp:98] Creating Layer pool1
I0704 07:43:18.837407 25348 net.cpp:439] pool1 <- conv1b/bn
I0704 07:43:18.837410 25348 net.cpp:413] pool1 -> pool1
I0704 07:43:18.837458 25348 net.cpp:148] Setting up pool1
I0704 07:43:18.837463 25348 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:43:18.837466 25348 net.cpp:163] Memory required for data: 19783764
I0704 07:43:18.837467 25348 layer_factory.hpp:77] Creating layer res2a_branch2a
I0704 07:43:18.837471 25348 net.cpp:98] Creating Layer res2a_branch2a
I0704 07:43:18.837474 25348 net.cpp:439] res2a_branch2a <- pool1
I0704 07:43:18.837477 25348 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0704 07:43:18.838111 25348 net.cpp:148] Setting up res2a_branch2a
I0704 07:43:18.838117 25348 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:43:18.838119 25348 net.cpp:163] Memory required for data: 25288788
I0704 07:43:18.838124 25348 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0704 07:43:18.838127 25348 net.cpp:98] Creating Layer res2a_branch2a/bn
I0704 07:43:18.838130 25348 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0704 07:43:18.838132 25348 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0704 07:43:18.838798 25348 net.cpp:148] Setting up res2a_branch2a/bn
I0704 07:43:18.838805 25348 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:43:18.838807 25348 net.cpp:163] Memory required for data: 30793812
I0704 07:43:18.838812 25348 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0704 07:43:18.838815 25348 net.cpp:98] Creating Layer res2a_branch2a/relu
I0704 07:43:18.838819 25348 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0704 07:43:18.838821 25348 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0704 07:43:18.838825 25348 net.cpp:148] Setting up res2a_branch2a/relu
I0704 07:43:18.838829 25348 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:43:18.838830 25348 net.cpp:163] Memory required for data: 36298836
I0704 07:43:18.838832 25348 layer_factory.hpp:77] Creating layer res2a_branch2b
I0704 07:43:18.838836 25348 net.cpp:98] Creating Layer res2a_branch2b
I0704 07:43:18.838840 25348 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0704 07:43:18.838845 25348 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0704 07:43:18.840179 25348 net.cpp:148] Setting up res2a_branch2b
I0704 07:43:18.840188 25348 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:43:18.840190 25348 net.cpp:163] Memory required for data: 41803860
I0704 07:43:18.840203 25348 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0704 07:43:18.840206 25348 net.cpp:98] Creating Layer res2a_branch2b/bn
I0704 07:43:18.840209 25348 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0704 07:43:18.840214 25348 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0704 07:43:18.840867 25348 net.cpp:148] Setting up res2a_branch2b/bn
I0704 07:43:18.840873 25348 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:43:18.840874 25348 net.cpp:163] Memory required for data: 47308884
I0704 07:43:18.840881 25348 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0704 07:43:18.840886 25348 net.cpp:98] Creating Layer res2a_branch2b/relu
I0704 07:43:18.840891 25348 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0704 07:43:18.840895 25348 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0704 07:43:18.840903 25348 net.cpp:148] Setting up res2a_branch2b/relu
I0704 07:43:18.840905 25348 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:43:18.840908 25348 net.cpp:163] Memory required for data: 52813908
I0704 07:43:18.840909 25348 layer_factory.hpp:77] Creating layer pool2
I0704 07:43:18.840912 25348 net.cpp:98] Creating Layer pool2
I0704 07:43:18.840914 25348 net.cpp:439] pool2 <- res2a_branch2b/bn
I0704 07:43:18.840916 25348 net.cpp:413] pool2 -> pool2
I0704 07:43:18.840952 25348 net.cpp:148] Setting up pool2
I0704 07:43:18.840956 25348 net.cpp:155] Top shape: 21 64 16 16 (344064)
I0704 07:43:18.840958 25348 net.cpp:163] Memory required for data: 54190164
I0704 07:43:18.840960 25348 layer_factory.hpp:77] Creating layer res3a_branch2a
I0704 07:43:18.840965 25348 net.cpp:98] Creating Layer res3a_branch2a
I0704 07:43:18.840967 25348 net.cpp:439] res3a_branch2a <- pool2
I0704 07:43:18.840970 25348 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0704 07:43:18.843564 25348 net.cpp:148] Setting up res3a_branch2a
I0704 07:43:18.843574 25348 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:43:18.843576 25348 net.cpp:163] Memory required for data: 56942676
I0704 07:43:18.843580 25348 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0704 07:43:18.843585 25348 net.cpp:98] Creating Layer res3a_branch2a/bn
I0704 07:43:18.843588 25348 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0704 07:43:18.843591 25348 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0704 07:43:18.844173 25348 net.cpp:148] Setting up res3a_branch2a/bn
I0704 07:43:18.844179 25348 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:43:18.844182 25348 net.cpp:163] Memory required for data: 59695188
I0704 07:43:18.844192 25348 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0704 07:43:18.844195 25348 net.cpp:98] Creating Layer res3a_branch2a/relu
I0704 07:43:18.844197 25348 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0704 07:43:18.844199 25348 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0704 07:43:18.844203 25348 net.cpp:148] Setting up res3a_branch2a/relu
I0704 07:43:18.844208 25348 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:43:18.844208 25348 net.cpp:163] Memory required for data: 62447700
I0704 07:43:18.844210 25348 layer_factory.hpp:77] Creating layer res3a_branch2b
I0704 07:43:18.844214 25348 net.cpp:98] Creating Layer res3a_branch2b
I0704 07:43:18.844216 25348 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0704 07:43:18.844219 25348 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0704 07:43:18.845216 25348 net.cpp:148] Setting up res3a_branch2b
I0704 07:43:18.845221 25348 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:43:18.845223 25348 net.cpp:163] Memory required for data: 65200212
I0704 07:43:18.845227 25348 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0704 07:43:18.845232 25348 net.cpp:98] Creating Layer res3a_branch2b/bn
I0704 07:43:18.845234 25348 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0704 07:43:18.845237 25348 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0704 07:43:18.845834 25348 net.cpp:148] Setting up res3a_branch2b/bn
I0704 07:43:18.845839 25348 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:43:18.845850 25348 net.cpp:163] Memory required for data: 67952724
I0704 07:43:18.845856 25348 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0704 07:43:18.845860 25348 net.cpp:98] Creating Layer res3a_branch2b/relu
I0704 07:43:18.845863 25348 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0704 07:43:18.845865 25348 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0704 07:43:18.845870 25348 net.cpp:148] Setting up res3a_branch2b/relu
I0704 07:43:18.845873 25348 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:43:18.845875 25348 net.cpp:163] Memory required for data: 70705236
I0704 07:43:18.845877 25348 layer_factory.hpp:77] Creating layer pool3
I0704 07:43:18.845880 25348 net.cpp:98] Creating Layer pool3
I0704 07:43:18.845883 25348 net.cpp:439] pool3 <- res3a_branch2b/bn
I0704 07:43:18.845888 25348 net.cpp:413] pool3 -> pool3
I0704 07:43:18.845933 25348 net.cpp:148] Setting up pool3
I0704 07:43:18.845939 25348 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:43:18.845942 25348 net.cpp:163] Memory required for data: 73457748
I0704 07:43:18.845943 25348 layer_factory.hpp:77] Creating layer res4a_branch2a
I0704 07:43:18.845948 25348 net.cpp:98] Creating Layer res4a_branch2a
I0704 07:43:18.845952 25348 net.cpp:439] res4a_branch2a <- pool3
I0704 07:43:18.845954 25348 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0704 07:43:18.852285 25348 net.cpp:148] Setting up res4a_branch2a
I0704 07:43:18.852298 25348 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:43:18.852300 25348 net.cpp:163] Memory required for data: 78962772
I0704 07:43:18.852305 25348 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0704 07:43:18.852311 25348 net.cpp:98] Creating Layer res4a_branch2a/bn
I0704 07:43:18.852313 25348 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0704 07:43:18.852318 25348 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0704 07:43:18.852974 25348 net.cpp:148] Setting up res4a_branch2a/bn
I0704 07:43:18.852982 25348 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:43:18.852984 25348 net.cpp:163] Memory required for data: 84467796
I0704 07:43:18.852990 25348 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0704 07:43:18.852994 25348 net.cpp:98] Creating Layer res4a_branch2a/relu
I0704 07:43:18.852998 25348 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0704 07:43:18.853000 25348 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0704 07:43:18.853004 25348 net.cpp:148] Setting up res4a_branch2a/relu
I0704 07:43:18.853008 25348 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:43:18.853009 25348 net.cpp:163] Memory required for data: 89972820
I0704 07:43:18.853011 25348 layer_factory.hpp:77] Creating layer res4a_branch2b
I0704 07:43:18.853016 25348 net.cpp:98] Creating Layer res4a_branch2b
I0704 07:43:18.853018 25348 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0704 07:43:18.853021 25348 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0704 07:43:18.856284 25348 net.cpp:148] Setting up res4a_branch2b
I0704 07:43:18.856292 25348 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:43:18.856294 25348 net.cpp:163] Memory required for data: 95477844
I0704 07:43:18.856297 25348 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0704 07:43:18.856302 25348 net.cpp:98] Creating Layer res4a_branch2b/bn
I0704 07:43:18.856304 25348 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0704 07:43:18.856308 25348 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0704 07:43:18.856950 25348 net.cpp:148] Setting up res4a_branch2b/bn
I0704 07:43:18.856957 25348 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:43:18.856959 25348 net.cpp:163] Memory required for data: 100982868
I0704 07:43:18.856966 25348 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0704 07:43:18.856968 25348 net.cpp:98] Creating Layer res4a_branch2b/relu
I0704 07:43:18.856971 25348 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0704 07:43:18.856974 25348 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0704 07:43:18.856986 25348 net.cpp:148] Setting up res4a_branch2b/relu
I0704 07:43:18.856990 25348 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:43:18.856992 25348 net.cpp:163] Memory required for data: 106487892
I0704 07:43:18.856994 25348 layer_factory.hpp:77] Creating layer pool4
I0704 07:43:18.856998 25348 net.cpp:98] Creating Layer pool4
I0704 07:43:18.857000 25348 net.cpp:439] pool4 <- res4a_branch2b/bn
I0704 07:43:18.857003 25348 net.cpp:413] pool4 -> pool4
I0704 07:43:18.857046 25348 net.cpp:148] Setting up pool4
I0704 07:43:18.857053 25348 net.cpp:155] Top shape: 21 256 8 8 (344064)
I0704 07:43:18.857056 25348 net.cpp:163] Memory required for data: 107864148
I0704 07:43:18.857060 25348 layer_factory.hpp:77] Creating layer res5a_branch2a
I0704 07:43:18.857067 25348 net.cpp:98] Creating Layer res5a_branch2a
I0704 07:43:18.857071 25348 net.cpp:439] res5a_branch2a <- pool4
I0704 07:43:18.857076 25348 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0704 07:43:18.882076 25348 net.cpp:148] Setting up res5a_branch2a
I0704 07:43:18.882092 25348 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:43:18.882094 25348 net.cpp:163] Memory required for data: 110616660
I0704 07:43:18.882099 25348 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0704 07:43:18.882105 25348 net.cpp:98] Creating Layer res5a_branch2a/bn
I0704 07:43:18.882108 25348 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0704 07:43:18.882112 25348 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0704 07:43:18.882800 25348 net.cpp:148] Setting up res5a_branch2a/bn
I0704 07:43:18.882808 25348 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:43:18.882810 25348 net.cpp:163] Memory required for data: 113369172
I0704 07:43:18.882815 25348 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0704 07:43:18.882819 25348 net.cpp:98] Creating Layer res5a_branch2a/relu
I0704 07:43:18.882822 25348 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0704 07:43:18.882825 25348 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0704 07:43:18.882829 25348 net.cpp:148] Setting up res5a_branch2a/relu
I0704 07:43:18.882832 25348 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:43:18.882833 25348 net.cpp:163] Memory required for data: 116121684
I0704 07:43:18.882835 25348 layer_factory.hpp:77] Creating layer res5a_branch2b
I0704 07:43:18.882843 25348 net.cpp:98] Creating Layer res5a_branch2b
I0704 07:43:18.882846 25348 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0704 07:43:18.882849 25348 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0704 07:43:18.895859 25348 net.cpp:148] Setting up res5a_branch2b
I0704 07:43:18.895879 25348 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:43:18.895881 25348 net.cpp:163] Memory required for data: 118874196
I0704 07:43:18.895889 25348 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0704 07:43:18.895896 25348 net.cpp:98] Creating Layer res5a_branch2b/bn
I0704 07:43:18.895900 25348 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0704 07:43:18.895903 25348 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0704 07:43:18.896600 25348 net.cpp:148] Setting up res5a_branch2b/bn
I0704 07:43:18.896608 25348 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:43:18.896610 25348 net.cpp:163] Memory required for data: 121626708
I0704 07:43:18.896616 25348 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0704 07:43:18.896620 25348 net.cpp:98] Creating Layer res5a_branch2b/relu
I0704 07:43:18.896622 25348 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0704 07:43:18.896625 25348 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0704 07:43:18.896628 25348 net.cpp:148] Setting up res5a_branch2b/relu
I0704 07:43:18.896631 25348 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:43:18.896632 25348 net.cpp:163] Memory required for data: 124379220
I0704 07:43:18.896636 25348 layer_factory.hpp:77] Creating layer pool5
I0704 07:43:18.896641 25348 net.cpp:98] Creating Layer pool5
I0704 07:43:18.896643 25348 net.cpp:439] pool5 <- res5a_branch2b/bn
I0704 07:43:18.896656 25348 net.cpp:413] pool5 -> pool5
I0704 07:43:18.896687 25348 net.cpp:148] Setting up pool5
I0704 07:43:18.896693 25348 net.cpp:155] Top shape: 21 512 1 1 (10752)
I0704 07:43:18.896697 25348 net.cpp:163] Memory required for data: 124422228
I0704 07:43:18.896699 25348 layer_factory.hpp:77] Creating layer fc10
I0704 07:43:18.896703 25348 net.cpp:98] Creating Layer fc10
I0704 07:43:18.896704 25348 net.cpp:439] fc10 <- pool5
I0704 07:43:18.896708 25348 net.cpp:413] fc10 -> fc10
I0704 07:43:18.896934 25348 net.cpp:148] Setting up fc10
I0704 07:43:18.896939 25348 net.cpp:155] Top shape: 21 10 (210)
I0704 07:43:18.896940 25348 net.cpp:163] Memory required for data: 124423068
I0704 07:43:18.896944 25348 layer_factory.hpp:77] Creating layer loss
I0704 07:43:18.896952 25348 net.cpp:98] Creating Layer loss
I0704 07:43:18.896955 25348 net.cpp:439] loss <- fc10
I0704 07:43:18.896956 25348 net.cpp:439] loss <- label
I0704 07:43:18.896960 25348 net.cpp:413] loss -> loss
I0704 07:43:18.896966 25348 layer_factory.hpp:77] Creating layer loss
I0704 07:43:18.897073 25348 net.cpp:148] Setting up loss
I0704 07:43:18.897076 25348 net.cpp:155] Top shape: (1)
I0704 07:43:18.897078 25348 net.cpp:158]     with loss weight 1
I0704 07:43:18.897089 25348 net.cpp:163] Memory required for data: 124423072
I0704 07:43:18.897091 25348 net.cpp:224] loss needs backward computation.
I0704 07:43:18.897094 25348 net.cpp:224] fc10 needs backward computation.
I0704 07:43:18.897095 25348 net.cpp:224] pool5 needs backward computation.
I0704 07:43:18.897096 25348 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0704 07:43:18.897099 25348 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0704 07:43:18.897100 25348 net.cpp:224] res5a_branch2b needs backward computation.
I0704 07:43:18.897102 25348 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0704 07:43:18.897104 25348 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0704 07:43:18.897106 25348 net.cpp:224] res5a_branch2a needs backward computation.
I0704 07:43:18.897109 25348 net.cpp:224] pool4 needs backward computation.
I0704 07:43:18.897110 25348 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0704 07:43:18.897112 25348 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0704 07:43:18.897114 25348 net.cpp:224] res4a_branch2b needs backward computation.
I0704 07:43:18.897117 25348 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0704 07:43:18.897119 25348 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0704 07:43:18.897121 25348 net.cpp:224] res4a_branch2a needs backward computation.
I0704 07:43:18.897123 25348 net.cpp:224] pool3 needs backward computation.
I0704 07:43:18.897126 25348 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0704 07:43:18.897128 25348 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0704 07:43:18.897130 25348 net.cpp:224] res3a_branch2b needs backward computation.
I0704 07:43:18.897132 25348 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0704 07:43:18.897135 25348 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0704 07:43:18.897137 25348 net.cpp:224] res3a_branch2a needs backward computation.
I0704 07:43:18.897140 25348 net.cpp:224] pool2 needs backward computation.
I0704 07:43:18.897142 25348 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0704 07:43:18.897145 25348 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0704 07:43:18.897150 25348 net.cpp:224] res2a_branch2b needs backward computation.
I0704 07:43:18.897152 25348 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0704 07:43:18.897156 25348 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0704 07:43:18.897161 25348 net.cpp:224] res2a_branch2a needs backward computation.
I0704 07:43:18.897164 25348 net.cpp:224] pool1 needs backward computation.
I0704 07:43:18.897168 25348 net.cpp:224] conv1b/relu needs backward computation.
I0704 07:43:18.897171 25348 net.cpp:224] conv1b/bn needs backward computation.
I0704 07:43:18.897178 25348 net.cpp:224] conv1b needs backward computation.
I0704 07:43:18.897181 25348 net.cpp:224] conv1a/relu needs backward computation.
I0704 07:43:18.897182 25348 net.cpp:224] conv1a/bn needs backward computation.
I0704 07:43:18.897184 25348 net.cpp:224] conv1a needs backward computation.
I0704 07:43:18.897187 25348 net.cpp:226] data/bias does not need backward computation.
I0704 07:43:18.897191 25348 net.cpp:226] data does not need backward computation.
I0704 07:43:18.897192 25348 net.cpp:268] This network produces output loss
I0704 07:43:18.897209 25348 net.cpp:288] Network initialization done.
I0704 07:43:18.897649 25348 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/test.prototxt
I0704 07:43:18.897832 25348 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0704 07:43:18.897933 25348 layer_factory.hpp:77] Creating layer data
I0704 07:43:18.898000 25348 net.cpp:98] Creating Layer data
I0704 07:43:18.898006 25348 net.cpp:413] data -> data
I0704 07:43:18.898012 25348 net.cpp:413] data -> label
I0704 07:43:18.899343 25380 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0704 07:43:18.899446 25348 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0704 07:43:18.899503 25348 data_layer.cpp:83] output data size: 50,3,32,32
I0704 07:43:18.901779 25348 net.cpp:148] Setting up data
I0704 07:43:18.901787 25348 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0704 07:43:18.901790 25348 net.cpp:155] Top shape: 50 (50)
I0704 07:43:18.901793 25348 net.cpp:163] Memory required for data: 614600
I0704 07:43:18.901795 25348 layer_factory.hpp:77] Creating layer label_data_1_split
I0704 07:43:18.901799 25348 net.cpp:98] Creating Layer label_data_1_split
I0704 07:43:18.901801 25348 net.cpp:439] label_data_1_split <- label
I0704 07:43:18.901804 25348 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0704 07:43:18.901808 25348 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0704 07:43:18.901811 25348 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0704 07:43:18.901940 25348 net.cpp:148] Setting up label_data_1_split
I0704 07:43:18.901952 25348 net.cpp:155] Top shape: 50 (50)
I0704 07:43:18.901957 25348 net.cpp:155] Top shape: 50 (50)
I0704 07:43:18.901960 25348 net.cpp:155] Top shape: 50 (50)
I0704 07:43:18.901963 25348 net.cpp:163] Memory required for data: 615200
I0704 07:43:18.901968 25348 layer_factory.hpp:77] Creating layer data/bias
I0704 07:43:18.901973 25348 net.cpp:98] Creating Layer data/bias
I0704 07:43:18.901978 25348 net.cpp:439] data/bias <- data
I0704 07:43:18.901981 25348 net.cpp:413] data/bias -> data/bias
I0704 07:43:18.902103 25348 net.cpp:148] Setting up data/bias
I0704 07:43:18.902110 25348 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0704 07:43:18.902113 25348 net.cpp:163] Memory required for data: 1229600
I0704 07:43:18.902119 25348 layer_factory.hpp:77] Creating layer conv1a
I0704 07:43:18.902125 25348 net.cpp:98] Creating Layer conv1a
I0704 07:43:18.902129 25348 net.cpp:439] conv1a <- data/bias
I0704 07:43:18.902133 25348 net.cpp:413] conv1a -> conv1a
I0704 07:43:18.902578 25348 net.cpp:148] Setting up conv1a
I0704 07:43:18.902585 25348 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:43:18.902588 25348 net.cpp:163] Memory required for data: 7783200
I0704 07:43:18.902592 25348 layer_factory.hpp:77] Creating layer conv1a/bn
I0704 07:43:18.902596 25348 net.cpp:98] Creating Layer conv1a/bn
I0704 07:43:18.902598 25348 net.cpp:439] conv1a/bn <- conv1a
I0704 07:43:18.902601 25348 net.cpp:413] conv1a/bn -> conv1a/bn
I0704 07:43:18.903496 25348 net.cpp:148] Setting up conv1a/bn
I0704 07:43:18.903506 25348 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:43:18.903508 25348 net.cpp:163] Memory required for data: 14336800
I0704 07:43:18.903514 25348 layer_factory.hpp:77] Creating layer conv1a/relu
I0704 07:43:18.903529 25348 net.cpp:98] Creating Layer conv1a/relu
I0704 07:43:18.903535 25348 net.cpp:439] conv1a/relu <- conv1a/bn
I0704 07:43:18.903540 25348 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0704 07:43:18.903548 25348 net.cpp:148] Setting up conv1a/relu
I0704 07:43:18.903553 25348 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:43:18.903558 25348 net.cpp:163] Memory required for data: 20890400
I0704 07:43:18.903560 25348 layer_factory.hpp:77] Creating layer conv1b
I0704 07:43:18.903568 25348 net.cpp:98] Creating Layer conv1b
I0704 07:43:18.903571 25348 net.cpp:439] conv1b <- conv1a/bn
I0704 07:43:18.903578 25348 net.cpp:413] conv1b -> conv1b
I0704 07:43:18.904013 25348 net.cpp:148] Setting up conv1b
I0704 07:43:18.904021 25348 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:43:18.904023 25348 net.cpp:163] Memory required for data: 27444000
I0704 07:43:18.904028 25348 layer_factory.hpp:77] Creating layer conv1b/bn
I0704 07:43:18.904036 25348 net.cpp:98] Creating Layer conv1b/bn
I0704 07:43:18.904040 25348 net.cpp:439] conv1b/bn <- conv1b
I0704 07:43:18.904045 25348 net.cpp:413] conv1b/bn -> conv1b/bn
I0704 07:43:18.904844 25348 net.cpp:148] Setting up conv1b/bn
I0704 07:43:18.904851 25348 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:43:18.904855 25348 net.cpp:163] Memory required for data: 33997600
I0704 07:43:18.904861 25348 layer_factory.hpp:77] Creating layer conv1b/relu
I0704 07:43:18.904867 25348 net.cpp:98] Creating Layer conv1b/relu
I0704 07:43:18.904871 25348 net.cpp:439] conv1b/relu <- conv1b/bn
I0704 07:43:18.904876 25348 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0704 07:43:18.904882 25348 net.cpp:148] Setting up conv1b/relu
I0704 07:43:18.904887 25348 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:43:18.904891 25348 net.cpp:163] Memory required for data: 40551200
I0704 07:43:18.904896 25348 layer_factory.hpp:77] Creating layer pool1
I0704 07:43:18.904901 25348 net.cpp:98] Creating Layer pool1
I0704 07:43:18.904904 25348 net.cpp:439] pool1 <- conv1b/bn
I0704 07:43:18.904908 25348 net.cpp:413] pool1 -> pool1
I0704 07:43:18.904959 25348 net.cpp:148] Setting up pool1
I0704 07:43:18.904966 25348 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:43:18.904969 25348 net.cpp:163] Memory required for data: 47104800
I0704 07:43:18.904973 25348 layer_factory.hpp:77] Creating layer res2a_branch2a
I0704 07:43:18.904980 25348 net.cpp:98] Creating Layer res2a_branch2a
I0704 07:43:18.904985 25348 net.cpp:439] res2a_branch2a <- pool1
I0704 07:43:18.904990 25348 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0704 07:43:18.905736 25348 net.cpp:148] Setting up res2a_branch2a
I0704 07:43:18.905742 25348 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:43:18.905745 25348 net.cpp:163] Memory required for data: 60212000
I0704 07:43:18.905751 25348 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0704 07:43:18.905757 25348 net.cpp:98] Creating Layer res2a_branch2a/bn
I0704 07:43:18.905761 25348 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0704 07:43:18.905766 25348 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0704 07:43:18.906514 25348 net.cpp:148] Setting up res2a_branch2a/bn
I0704 07:43:18.906522 25348 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:43:18.906524 25348 net.cpp:163] Memory required for data: 73319200
I0704 07:43:18.906533 25348 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0704 07:43:18.906538 25348 net.cpp:98] Creating Layer res2a_branch2a/relu
I0704 07:43:18.906543 25348 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0704 07:43:18.906548 25348 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0704 07:43:18.906553 25348 net.cpp:148] Setting up res2a_branch2a/relu
I0704 07:43:18.906558 25348 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:43:18.906561 25348 net.cpp:163] Memory required for data: 86426400
I0704 07:43:18.906565 25348 layer_factory.hpp:77] Creating layer res2a_branch2b
I0704 07:43:18.906572 25348 net.cpp:98] Creating Layer res2a_branch2b
I0704 07:43:18.906584 25348 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0704 07:43:18.906589 25348 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0704 07:43:18.907138 25348 net.cpp:148] Setting up res2a_branch2b
I0704 07:43:18.907145 25348 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:43:18.907148 25348 net.cpp:163] Memory required for data: 99533600
I0704 07:43:18.907153 25348 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0704 07:43:18.907158 25348 net.cpp:98] Creating Layer res2a_branch2b/bn
I0704 07:43:18.907163 25348 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0704 07:43:18.907168 25348 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0704 07:43:18.907902 25348 net.cpp:148] Setting up res2a_branch2b/bn
I0704 07:43:18.907908 25348 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:43:18.907912 25348 net.cpp:163] Memory required for data: 112640800
I0704 07:43:18.907918 25348 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0704 07:43:18.907923 25348 net.cpp:98] Creating Layer res2a_branch2b/relu
I0704 07:43:18.907927 25348 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0704 07:43:18.907932 25348 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0704 07:43:18.907938 25348 net.cpp:148] Setting up res2a_branch2b/relu
I0704 07:43:18.907943 25348 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:43:18.907948 25348 net.cpp:163] Memory required for data: 125748000
I0704 07:43:18.907950 25348 layer_factory.hpp:77] Creating layer pool2
I0704 07:43:18.907956 25348 net.cpp:98] Creating Layer pool2
I0704 07:43:18.907960 25348 net.cpp:439] pool2 <- res2a_branch2b/bn
I0704 07:43:18.907964 25348 net.cpp:413] pool2 -> pool2
I0704 07:43:18.908015 25348 net.cpp:148] Setting up pool2
I0704 07:43:18.908021 25348 net.cpp:155] Top shape: 50 64 16 16 (819200)
I0704 07:43:18.908025 25348 net.cpp:163] Memory required for data: 129024800
I0704 07:43:18.908030 25348 layer_factory.hpp:77] Creating layer res3a_branch2a
I0704 07:43:18.908035 25348 net.cpp:98] Creating Layer res3a_branch2a
I0704 07:43:18.908041 25348 net.cpp:439] res3a_branch2a <- pool2
I0704 07:43:18.908046 25348 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0704 07:43:18.910835 25348 net.cpp:148] Setting up res3a_branch2a
I0704 07:43:18.910845 25348 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:43:18.910847 25348 net.cpp:163] Memory required for data: 135578400
I0704 07:43:18.910853 25348 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0704 07:43:18.910859 25348 net.cpp:98] Creating Layer res3a_branch2a/bn
I0704 07:43:18.910864 25348 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0704 07:43:18.910871 25348 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0704 07:43:18.911497 25348 net.cpp:148] Setting up res3a_branch2a/bn
I0704 07:43:18.911505 25348 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:43:18.911507 25348 net.cpp:163] Memory required for data: 142132000
I0704 07:43:18.911516 25348 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0704 07:43:18.911523 25348 net.cpp:98] Creating Layer res3a_branch2a/relu
I0704 07:43:18.911527 25348 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0704 07:43:18.911532 25348 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0704 07:43:18.911538 25348 net.cpp:148] Setting up res3a_branch2a/relu
I0704 07:43:18.911543 25348 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:43:18.911547 25348 net.cpp:163] Memory required for data: 148685600
I0704 07:43:18.911551 25348 layer_factory.hpp:77] Creating layer res3a_branch2b
I0704 07:43:18.911558 25348 net.cpp:98] Creating Layer res3a_branch2b
I0704 07:43:18.911562 25348 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0704 07:43:18.911567 25348 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0704 07:43:18.912601 25348 net.cpp:148] Setting up res3a_branch2b
I0704 07:43:18.912608 25348 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:43:18.912611 25348 net.cpp:163] Memory required for data: 155239200
I0704 07:43:18.912616 25348 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0704 07:43:18.912631 25348 net.cpp:98] Creating Layer res3a_branch2b/bn
I0704 07:43:18.912634 25348 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0704 07:43:18.912641 25348 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0704 07:43:18.913298 25348 net.cpp:148] Setting up res3a_branch2b/bn
I0704 07:43:18.913305 25348 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:43:18.913308 25348 net.cpp:163] Memory required for data: 161792800
I0704 07:43:18.913314 25348 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0704 07:43:18.913321 25348 net.cpp:98] Creating Layer res3a_branch2b/relu
I0704 07:43:18.913324 25348 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0704 07:43:18.913328 25348 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0704 07:43:18.913336 25348 net.cpp:148] Setting up res3a_branch2b/relu
I0704 07:43:18.913341 25348 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:43:18.913343 25348 net.cpp:163] Memory required for data: 168346400
I0704 07:43:18.913347 25348 layer_factory.hpp:77] Creating layer pool3
I0704 07:43:18.913352 25348 net.cpp:98] Creating Layer pool3
I0704 07:43:18.913357 25348 net.cpp:439] pool3 <- res3a_branch2b/bn
I0704 07:43:18.913360 25348 net.cpp:413] pool3 -> pool3
I0704 07:43:18.913413 25348 net.cpp:148] Setting up pool3
I0704 07:43:18.913419 25348 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:43:18.913422 25348 net.cpp:163] Memory required for data: 174900000
I0704 07:43:18.913426 25348 layer_factory.hpp:77] Creating layer res4a_branch2a
I0704 07:43:18.913434 25348 net.cpp:98] Creating Layer res4a_branch2a
I0704 07:43:18.913437 25348 net.cpp:439] res4a_branch2a <- pool3
I0704 07:43:18.913442 25348 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0704 07:43:18.919603 25348 net.cpp:148] Setting up res4a_branch2a
I0704 07:43:18.919611 25348 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:43:18.919613 25348 net.cpp:163] Memory required for data: 188007200
I0704 07:43:18.919617 25348 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0704 07:43:18.919622 25348 net.cpp:98] Creating Layer res4a_branch2a/bn
I0704 07:43:18.919625 25348 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0704 07:43:18.919627 25348 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0704 07:43:18.920300 25348 net.cpp:148] Setting up res4a_branch2a/bn
I0704 07:43:18.920306 25348 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:43:18.920308 25348 net.cpp:163] Memory required for data: 201114400
I0704 07:43:18.920313 25348 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0704 07:43:18.920317 25348 net.cpp:98] Creating Layer res4a_branch2a/relu
I0704 07:43:18.920320 25348 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0704 07:43:18.920322 25348 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0704 07:43:18.920326 25348 net.cpp:148] Setting up res4a_branch2a/relu
I0704 07:43:18.920328 25348 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:43:18.920331 25348 net.cpp:163] Memory required for data: 214221600
I0704 07:43:18.920333 25348 layer_factory.hpp:77] Creating layer res4a_branch2b
I0704 07:43:18.920336 25348 net.cpp:98] Creating Layer res4a_branch2b
I0704 07:43:18.920341 25348 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0704 07:43:18.920346 25348 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0704 07:43:18.923609 25348 net.cpp:148] Setting up res4a_branch2b
I0704 07:43:18.923616 25348 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:43:18.923619 25348 net.cpp:163] Memory required for data: 227328800
I0704 07:43:18.923624 25348 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0704 07:43:18.923629 25348 net.cpp:98] Creating Layer res4a_branch2b/bn
I0704 07:43:18.923633 25348 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0704 07:43:18.923641 25348 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0704 07:43:18.924309 25348 net.cpp:148] Setting up res4a_branch2b/bn
I0704 07:43:18.924314 25348 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:43:18.924324 25348 net.cpp:163] Memory required for data: 240436000
I0704 07:43:18.924329 25348 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0704 07:43:18.924332 25348 net.cpp:98] Creating Layer res4a_branch2b/relu
I0704 07:43:18.924335 25348 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0704 07:43:18.924338 25348 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0704 07:43:18.924342 25348 net.cpp:148] Setting up res4a_branch2b/relu
I0704 07:43:18.924345 25348 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:43:18.924346 25348 net.cpp:163] Memory required for data: 253543200
I0704 07:43:18.924350 25348 layer_factory.hpp:77] Creating layer pool4
I0704 07:43:18.924355 25348 net.cpp:98] Creating Layer pool4
I0704 07:43:18.924358 25348 net.cpp:439] pool4 <- res4a_branch2b/bn
I0704 07:43:18.924363 25348 net.cpp:413] pool4 -> pool4
I0704 07:43:18.924415 25348 net.cpp:148] Setting up pool4
I0704 07:43:18.924422 25348 net.cpp:155] Top shape: 50 256 8 8 (819200)
I0704 07:43:18.924425 25348 net.cpp:163] Memory required for data: 256820000
I0704 07:43:18.924429 25348 layer_factory.hpp:77] Creating layer res5a_branch2a
I0704 07:43:18.924435 25348 net.cpp:98] Creating Layer res5a_branch2a
I0704 07:43:18.924439 25348 net.cpp:439] res5a_branch2a <- pool4
I0704 07:43:18.924444 25348 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0704 07:43:18.949861 25348 net.cpp:148] Setting up res5a_branch2a
I0704 07:43:18.949882 25348 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:43:18.949884 25348 net.cpp:163] Memory required for data: 263373600
I0704 07:43:18.949892 25348 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0704 07:43:18.949911 25348 net.cpp:98] Creating Layer res5a_branch2a/bn
I0704 07:43:18.949916 25348 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0704 07:43:18.949923 25348 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0704 07:43:18.950729 25348 net.cpp:148] Setting up res5a_branch2a/bn
I0704 07:43:18.950743 25348 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:43:18.950747 25348 net.cpp:163] Memory required for data: 269927200
I0704 07:43:18.950754 25348 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0704 07:43:18.950762 25348 net.cpp:98] Creating Layer res5a_branch2a/relu
I0704 07:43:18.950767 25348 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0704 07:43:18.950772 25348 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0704 07:43:18.950778 25348 net.cpp:148] Setting up res5a_branch2a/relu
I0704 07:43:18.950783 25348 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:43:18.950788 25348 net.cpp:163] Memory required for data: 276480800
I0704 07:43:18.950791 25348 layer_factory.hpp:77] Creating layer res5a_branch2b
I0704 07:43:18.950799 25348 net.cpp:98] Creating Layer res5a_branch2b
I0704 07:43:18.950803 25348 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0704 07:43:18.950809 25348 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0704 07:43:18.964371 25348 net.cpp:148] Setting up res5a_branch2b
I0704 07:43:18.964390 25348 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:43:18.964392 25348 net.cpp:163] Memory required for data: 283034400
I0704 07:43:18.964401 25348 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0704 07:43:18.964409 25348 net.cpp:98] Creating Layer res5a_branch2b/bn
I0704 07:43:18.964412 25348 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0704 07:43:18.964416 25348 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0704 07:43:18.965201 25348 net.cpp:148] Setting up res5a_branch2b/bn
I0704 07:43:18.965210 25348 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:43:18.965212 25348 net.cpp:163] Memory required for data: 289588000
I0704 07:43:18.965217 25348 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0704 07:43:18.965221 25348 net.cpp:98] Creating Layer res5a_branch2b/relu
I0704 07:43:18.965224 25348 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0704 07:43:18.965226 25348 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0704 07:43:18.965240 25348 net.cpp:148] Setting up res5a_branch2b/relu
I0704 07:43:18.965242 25348 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:43:18.965245 25348 net.cpp:163] Memory required for data: 296141600
I0704 07:43:18.965246 25348 layer_factory.hpp:77] Creating layer pool5
I0704 07:43:18.965250 25348 net.cpp:98] Creating Layer pool5
I0704 07:43:18.965252 25348 net.cpp:439] pool5 <- res5a_branch2b/bn
I0704 07:43:18.965255 25348 net.cpp:413] pool5 -> pool5
I0704 07:43:18.965282 25348 net.cpp:148] Setting up pool5
I0704 07:43:18.965288 25348 net.cpp:155] Top shape: 50 512 1 1 (25600)
I0704 07:43:18.965293 25348 net.cpp:163] Memory required for data: 296244000
I0704 07:43:18.965297 25348 layer_factory.hpp:77] Creating layer fc10
I0704 07:43:18.965307 25348 net.cpp:98] Creating Layer fc10
I0704 07:43:18.965312 25348 net.cpp:439] fc10 <- pool5
I0704 07:43:18.965317 25348 net.cpp:413] fc10 -> fc10
I0704 07:43:18.965646 25348 net.cpp:148] Setting up fc10
I0704 07:43:18.965653 25348 net.cpp:155] Top shape: 50 10 (500)
I0704 07:43:18.965657 25348 net.cpp:163] Memory required for data: 296246000
I0704 07:43:18.965663 25348 layer_factory.hpp:77] Creating layer fc10_fc10_0_split
I0704 07:43:18.965669 25348 net.cpp:98] Creating Layer fc10_fc10_0_split
I0704 07:43:18.965673 25348 net.cpp:439] fc10_fc10_0_split <- fc10
I0704 07:43:18.965678 25348 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0704 07:43:18.965684 25348 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0704 07:43:18.965690 25348 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0704 07:43:18.965768 25348 net.cpp:148] Setting up fc10_fc10_0_split
I0704 07:43:18.965775 25348 net.cpp:155] Top shape: 50 10 (500)
I0704 07:43:18.965780 25348 net.cpp:155] Top shape: 50 10 (500)
I0704 07:43:18.965783 25348 net.cpp:155] Top shape: 50 10 (500)
I0704 07:43:18.965787 25348 net.cpp:163] Memory required for data: 296252000
I0704 07:43:18.965791 25348 layer_factory.hpp:77] Creating layer loss
I0704 07:43:18.965796 25348 net.cpp:98] Creating Layer loss
I0704 07:43:18.965801 25348 net.cpp:439] loss <- fc10_fc10_0_split_0
I0704 07:43:18.965806 25348 net.cpp:439] loss <- label_data_1_split_0
I0704 07:43:18.965811 25348 net.cpp:413] loss -> loss
I0704 07:43:18.965818 25348 layer_factory.hpp:77] Creating layer loss
I0704 07:43:18.965968 25348 net.cpp:148] Setting up loss
I0704 07:43:18.965976 25348 net.cpp:155] Top shape: (1)
I0704 07:43:18.965981 25348 net.cpp:158]     with loss weight 1
I0704 07:43:18.965991 25348 net.cpp:163] Memory required for data: 296252004
I0704 07:43:18.965994 25348 layer_factory.hpp:77] Creating layer accuracy/top1
I0704 07:43:18.966006 25348 net.cpp:98] Creating Layer accuracy/top1
I0704 07:43:18.966011 25348 net.cpp:439] accuracy/top1 <- fc10_fc10_0_split_1
I0704 07:43:18.966015 25348 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0704 07:43:18.966020 25348 net.cpp:413] accuracy/top1 -> accuracy/top1
I0704 07:43:18.966028 25348 net.cpp:148] Setting up accuracy/top1
I0704 07:43:18.966033 25348 net.cpp:155] Top shape: (1)
I0704 07:43:18.966037 25348 net.cpp:163] Memory required for data: 296252008
I0704 07:43:18.966042 25348 layer_factory.hpp:77] Creating layer accuracy/top5
I0704 07:43:18.966048 25348 net.cpp:98] Creating Layer accuracy/top5
I0704 07:43:18.966051 25348 net.cpp:439] accuracy/top5 <- fc10_fc10_0_split_2
I0704 07:43:18.966056 25348 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0704 07:43:18.966061 25348 net.cpp:413] accuracy/top5 -> accuracy/top5
I0704 07:43:18.966068 25348 net.cpp:148] Setting up accuracy/top5
I0704 07:43:18.966073 25348 net.cpp:155] Top shape: (1)
I0704 07:43:18.966078 25348 net.cpp:163] Memory required for data: 296252012
I0704 07:43:18.966080 25348 net.cpp:226] accuracy/top5 does not need backward computation.
I0704 07:43:18.966085 25348 net.cpp:226] accuracy/top1 does not need backward computation.
I0704 07:43:18.966089 25348 net.cpp:224] loss needs backward computation.
I0704 07:43:18.966094 25348 net.cpp:224] fc10_fc10_0_split needs backward computation.
I0704 07:43:18.966097 25348 net.cpp:224] fc10 needs backward computation.
I0704 07:43:18.966109 25348 net.cpp:224] pool5 needs backward computation.
I0704 07:43:18.966114 25348 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0704 07:43:18.966116 25348 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0704 07:43:18.966120 25348 net.cpp:224] res5a_branch2b needs backward computation.
I0704 07:43:18.966125 25348 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0704 07:43:18.966128 25348 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0704 07:43:18.966132 25348 net.cpp:224] res5a_branch2a needs backward computation.
I0704 07:43:18.966136 25348 net.cpp:224] pool4 needs backward computation.
I0704 07:43:18.966140 25348 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0704 07:43:18.966145 25348 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0704 07:43:18.966150 25348 net.cpp:224] res4a_branch2b needs backward computation.
I0704 07:43:18.966153 25348 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0704 07:43:18.966157 25348 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0704 07:43:18.966161 25348 net.cpp:224] res4a_branch2a needs backward computation.
I0704 07:43:18.966166 25348 net.cpp:224] pool3 needs backward computation.
I0704 07:43:18.966169 25348 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0704 07:43:18.966173 25348 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0704 07:43:18.966177 25348 net.cpp:224] res3a_branch2b needs backward computation.
I0704 07:43:18.966181 25348 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0704 07:43:18.966186 25348 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0704 07:43:18.966189 25348 net.cpp:224] res3a_branch2a needs backward computation.
I0704 07:43:18.966193 25348 net.cpp:224] pool2 needs backward computation.
I0704 07:43:18.966197 25348 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0704 07:43:18.966200 25348 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0704 07:43:18.966204 25348 net.cpp:224] res2a_branch2b needs backward computation.
I0704 07:43:18.966209 25348 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0704 07:43:18.966212 25348 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0704 07:43:18.966217 25348 net.cpp:224] res2a_branch2a needs backward computation.
I0704 07:43:18.966220 25348 net.cpp:224] pool1 needs backward computation.
I0704 07:43:18.966224 25348 net.cpp:224] conv1b/relu needs backward computation.
I0704 07:43:18.966228 25348 net.cpp:224] conv1b/bn needs backward computation.
I0704 07:43:18.966233 25348 net.cpp:224] conv1b needs backward computation.
I0704 07:43:18.966238 25348 net.cpp:224] conv1a/relu needs backward computation.
I0704 07:43:18.966240 25348 net.cpp:224] conv1a/bn needs backward computation.
I0704 07:43:18.966244 25348 net.cpp:224] conv1a needs backward computation.
I0704 07:43:18.966248 25348 net.cpp:226] data/bias does not need backward computation.
I0704 07:43:18.966253 25348 net.cpp:226] label_data_1_split does not need backward computation.
I0704 07:43:18.966259 25348 net.cpp:226] data does not need backward computation.
I0704 07:43:18.966261 25348 net.cpp:268] This network produces output accuracy/top1
I0704 07:43:18.966265 25348 net.cpp:268] This network produces output accuracy/top5
I0704 07:43:18.966269 25348 net.cpp:268] This network produces output loss
I0704 07:43:18.966297 25348 net.cpp:288] Network initialization done.
I0704 07:43:18.966397 25348 solver.cpp:60] Solver scaffolding done.
I0704 07:43:18.970155 25348 caffe.cpp:145] Finetuning from training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/cifar10_jacintonet11v2_iter_64000.caffemodel
I0704 07:43:18.998423 25348 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0704 07:43:18.998492 25348 data_layer.cpp:83] output data size: 21,3,32,32
I0704 07:43:19.464810 25348 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0704 07:43:19.464876 25348 data_layer.cpp:83] output data size: 21,3,32,32
I0704 07:43:19.959833 25348 parallel.cpp:334] Starting Optimization
I0704 07:43:19.959899 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:43:19.970299 25348 solver.cpp:408] Solving jacintonet11v2_train
I0704 07:43:19.970319 25348 solver.cpp:409] Learning Rate Policy: poly
I0704 07:43:19.972193 25348 solver.cpp:466] Iteration 0, Testing net (#0)
I0704 07:43:21.647377 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9171
I0704 07:43:21.647399 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9974
I0704 07:43:21.647405 25348 solver.cpp:539]     Test net output #2: loss = 0.2066 (* 1 = 0.2066 loss)
I0704 07:43:21.756060 25348 solver.cpp:290] Iteration 0 (0 iter/s, 1.78567s/100 iter), loss = 0
I0704 07:43:21.756083 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:21.756089 25348 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0704 07:43:23.826246 25348 solver.cpp:290] Iteration 100 (48.3069 iter/s, 2.0701s/100 iter), loss = 0
I0704 07:43:23.826277 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:23.826287 25348 sgd_solver.cpp:106] Iteration 100, lr = 0.00998437
I0704 07:43:25.884728 25348 solver.cpp:290] Iteration 200 (48.5816 iter/s, 2.05839s/100 iter), loss = 0
I0704 07:43:25.884750 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:25.884757 25348 sgd_solver.cpp:106] Iteration 200, lr = 0.00996875
I0704 07:43:27.942723 25348 solver.cpp:290] Iteration 300 (48.5931 iter/s, 2.05791s/100 iter), loss = 0
I0704 07:43:27.942744 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:27.942751 25348 sgd_solver.cpp:106] Iteration 300, lr = 0.00995312
I0704 07:43:30.006409 25348 solver.cpp:290] Iteration 400 (48.459 iter/s, 2.0636s/100 iter), loss = 0
I0704 07:43:30.006431 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:30.006438 25348 sgd_solver.cpp:106] Iteration 400, lr = 0.0099375
I0704 07:43:32.065022 25348 solver.cpp:290] Iteration 500 (48.5785 iter/s, 2.05853s/100 iter), loss = 0
I0704 07:43:32.065047 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:32.065054 25348 sgd_solver.cpp:106] Iteration 500, lr = 0.00992187
I0704 07:43:34.139252 25348 solver.cpp:290] Iteration 600 (48.2127 iter/s, 2.07414s/100 iter), loss = 0
I0704 07:43:34.139274 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:34.139283 25348 sgd_solver.cpp:106] Iteration 600, lr = 0.00990625
I0704 07:43:36.198233 25348 solver.cpp:290] Iteration 700 (48.5697 iter/s, 2.0589s/100 iter), loss = 0
I0704 07:43:36.198256 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:36.198262 25348 sgd_solver.cpp:106] Iteration 700, lr = 0.00989062
I0704 07:43:38.260052 25348 solver.cpp:290] Iteration 800 (48.5029 iter/s, 2.06173s/100 iter), loss = 0
I0704 07:43:38.260074 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:38.260083 25348 sgd_solver.cpp:106] Iteration 800, lr = 0.009875
I0704 07:43:40.317914 25348 solver.cpp:290] Iteration 900 (48.5961 iter/s, 2.05778s/100 iter), loss = 0
I0704 07:43:40.317936 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:40.317944 25348 sgd_solver.cpp:106] Iteration 900, lr = 0.00985937
I0704 07:43:42.354902 25348 solver.cpp:354] Sparsity after update:
I0704 07:43:42.356168 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:43:42.356176 25348 net.cpp:1851] conv1a_param_0(0) 
I0704 07:43:42.356189 25348 net.cpp:1851] conv1b_param_0(0) 
I0704 07:43:42.356194 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:43:42.356196 25348 net.cpp:1851] res2a_branch2a_param_0(0) 
I0704 07:43:42.356200 25348 net.cpp:1851] res2a_branch2b_param_0(0) 
I0704 07:43:42.356204 25348 net.cpp:1851] res3a_branch2a_param_0(0) 
I0704 07:43:42.356209 25348 net.cpp:1851] res3a_branch2b_param_0(0) 
I0704 07:43:42.356211 25348 net.cpp:1851] res4a_branch2a_param_0(0) 
I0704 07:43:42.356227 25348 net.cpp:1851] res4a_branch2b_param_0(0) 
I0704 07:43:42.356231 25348 net.cpp:1851] res5a_branch2a_param_0(0) 
I0704 07:43:42.356235 25348 net.cpp:1851] res5a_branch2b_param_0(0) 
I0704 07:43:42.356240 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0704 07:43:42.356329 25348 solver.cpp:466] Iteration 1000, Testing net (#0)
I0704 07:43:43.999199 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9143
I0704 07:43:43.999220 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9977
I0704 07:43:43.999228 25348 solver.cpp:539]     Test net output #2: loss = 0.1789 (* 1 = 0.1789 loss)
I0704 07:43:44.019098 25348 solver.cpp:290] Iteration 1000 (27.0193 iter/s, 3.70105s/100 iter), loss = 0
I0704 07:43:44.019125 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:44.019134 25348 sgd_solver.cpp:106] Iteration 1000, lr = 0.00984375
I0704 07:43:46.080176 25348 solver.cpp:290] Iteration 1100 (48.5204 iter/s, 2.06099s/100 iter), loss = 0
I0704 07:43:46.080196 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:46.080204 25348 sgd_solver.cpp:106] Iteration 1100, lr = 0.00982813
I0704 07:43:48.137805 25348 solver.cpp:290] Iteration 1200 (48.6016 iter/s, 2.05754s/100 iter), loss = 0
I0704 07:43:48.137827 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:48.137835 25348 sgd_solver.cpp:106] Iteration 1200, lr = 0.0098125
I0704 07:43:50.194833 25348 solver.cpp:290] Iteration 1300 (48.6158 iter/s, 2.05694s/100 iter), loss = 0
I0704 07:43:50.196455 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:50.196465 25348 sgd_solver.cpp:106] Iteration 1300, lr = 0.00979687
I0704 07:43:52.256178 25348 solver.cpp:290] Iteration 1400 (48.5516 iter/s, 2.05966s/100 iter), loss = 0
I0704 07:43:52.256201 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:52.256207 25348 sgd_solver.cpp:106] Iteration 1400, lr = 0.00978125
I0704 07:43:54.312544 25348 solver.cpp:290] Iteration 1500 (48.6315 iter/s, 2.05628s/100 iter), loss = 0
I0704 07:43:54.312567 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:54.312573 25348 sgd_solver.cpp:106] Iteration 1500, lr = 0.00976562
I0704 07:43:56.370326 25348 solver.cpp:290] Iteration 1600 (48.598 iter/s, 2.0577s/100 iter), loss = 0
I0704 07:43:56.370348 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:56.370355 25348 sgd_solver.cpp:106] Iteration 1600, lr = 0.00975
I0704 07:43:58.426820 25348 solver.cpp:290] Iteration 1700 (48.6285 iter/s, 2.05641s/100 iter), loss = 0
I0704 07:43:58.426842 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:58.426849 25348 sgd_solver.cpp:106] Iteration 1700, lr = 0.00973437
I0704 07:44:00.484913 25348 solver.cpp:290] Iteration 1800 (48.5907 iter/s, 2.05801s/100 iter), loss = 0
I0704 07:44:00.484935 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:00.484941 25348 sgd_solver.cpp:106] Iteration 1800, lr = 0.00971875
I0704 07:44:02.545107 25348 solver.cpp:290] Iteration 1900 (48.5411 iter/s, 2.06011s/100 iter), loss = 0
I0704 07:44:02.545130 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:02.545137 25348 sgd_solver.cpp:106] Iteration 1900, lr = 0.00970312
I0704 07:44:04.582180 25348 solver.cpp:354] Sparsity after update:
I0704 07:44:04.583443 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:44:04.583451 25348 net.cpp:1851] conv1a_param_0(0) 
I0704 07:44:04.583458 25348 net.cpp:1851] conv1b_param_0(0) 
I0704 07:44:04.583461 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:44:04.583464 25348 net.cpp:1851] res2a_branch2a_param_0(0) 
I0704 07:44:04.583467 25348 net.cpp:1851] res2a_branch2b_param_0(0) 
I0704 07:44:04.583468 25348 net.cpp:1851] res3a_branch2a_param_0(0) 
I0704 07:44:04.583472 25348 net.cpp:1851] res3a_branch2b_param_0(0) 
I0704 07:44:04.583473 25348 net.cpp:1851] res4a_branch2a_param_0(0) 
I0704 07:44:04.583475 25348 net.cpp:1851] res4a_branch2b_param_0(0) 
I0704 07:44:04.583477 25348 net.cpp:1851] res5a_branch2a_param_0(0) 
I0704 07:44:04.583479 25348 net.cpp:1851] res5a_branch2b_param_0(0) 
I0704 07:44:04.583482 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0704 07:44:04.583614 25348 solver.cpp:466] Iteration 2000, Testing net (#0)
I0704 07:44:06.224511 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.914
I0704 07:44:06.224531 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9976
I0704 07:44:06.224536 25348 solver.cpp:539]     Test net output #2: loss = 0.1599 (* 1 = 0.1599 loss)
I0704 07:44:06.244096 25348 solver.cpp:290] Iteration 2000 (27.0353 iter/s, 3.69886s/100 iter), loss = 0
I0704 07:44:06.244112 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:06.244125 25348 sgd_solver.cpp:106] Iteration 2000, lr = 0.0096875
I0704 07:44:08.305510 25348 solver.cpp:290] Iteration 2100 (48.5123 iter/s, 2.06133s/100 iter), loss = 0
I0704 07:44:08.305532 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:08.305539 25348 sgd_solver.cpp:106] Iteration 2100, lr = 0.00967188
I0704 07:44:10.372706 25348 solver.cpp:290] Iteration 2200 (48.3767 iter/s, 2.06711s/100 iter), loss = 0
I0704 07:44:10.372730 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:10.372736 25348 sgd_solver.cpp:106] Iteration 2200, lr = 0.00965625
I0704 07:44:12.432227 25348 solver.cpp:290] Iteration 2300 (48.557 iter/s, 2.05943s/100 iter), loss = 0
I0704 07:44:12.432265 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:12.432271 25348 sgd_solver.cpp:106] Iteration 2300, lr = 0.00964062
I0704 07:44:14.489509 25348 solver.cpp:290] Iteration 2400 (48.6102 iter/s, 2.05718s/100 iter), loss = 0
I0704 07:44:14.489531 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:14.489537 25348 sgd_solver.cpp:106] Iteration 2400, lr = 0.009625
I0704 07:44:16.549288 25348 solver.cpp:290] Iteration 2500 (48.5509 iter/s, 2.05969s/100 iter), loss = 0
I0704 07:44:16.549311 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:16.549317 25348 sgd_solver.cpp:106] Iteration 2500, lr = 0.00960938
I0704 07:44:18.613245 25348 solver.cpp:290] Iteration 2600 (48.4526 iter/s, 2.06387s/100 iter), loss = 0
I0704 07:44:18.613267 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:18.613275 25348 sgd_solver.cpp:106] Iteration 2600, lr = 0.00959375
I0704 07:44:20.672076 25348 solver.cpp:290] Iteration 2700 (48.5733 iter/s, 2.05875s/100 iter), loss = 0
I0704 07:44:20.672139 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:20.672147 25348 sgd_solver.cpp:106] Iteration 2700, lr = 0.00957812
I0704 07:44:22.728775 25348 solver.cpp:290] Iteration 2800 (48.6246 iter/s, 2.05657s/100 iter), loss = 0
I0704 07:44:22.728797 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:22.728803 25348 sgd_solver.cpp:106] Iteration 2800, lr = 0.0095625
I0704 07:44:24.789860 25348 solver.cpp:290] Iteration 2900 (48.5202 iter/s, 2.061s/100 iter), loss = 0
I0704 07:44:24.789881 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:24.789890 25348 sgd_solver.cpp:106] Iteration 2900, lr = 0.00954687
I0704 07:44:26.827599 25348 solver.cpp:354] Sparsity after update:
I0704 07:44:26.828850 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:44:26.828857 25348 net.cpp:1851] conv1a_param_0(0) 
I0704 07:44:26.828867 25348 net.cpp:1851] conv1b_param_0(0) 
I0704 07:44:26.828871 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:44:26.828876 25348 net.cpp:1851] res2a_branch2a_param_0(0) 
I0704 07:44:26.828879 25348 net.cpp:1851] res2a_branch2b_param_0(0) 
I0704 07:44:26.828883 25348 net.cpp:1851] res3a_branch2a_param_0(0) 
I0704 07:44:26.828887 25348 net.cpp:1851] res3a_branch2b_param_0(0) 
I0704 07:44:26.828891 25348 net.cpp:1851] res4a_branch2a_param_0(0) 
I0704 07:44:26.828894 25348 net.cpp:1851] res4a_branch2b_param_0(0) 
I0704 07:44:26.828898 25348 net.cpp:1851] res5a_branch2a_param_0(0) 
I0704 07:44:26.828902 25348 net.cpp:1851] res5a_branch2b_param_0(0) 
I0704 07:44:26.828907 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0704 07:44:26.828999 25348 solver.cpp:466] Iteration 3000, Testing net (#0)
I0704 07:44:28.473506 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9142
I0704 07:44:28.473525 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9975
I0704 07:44:28.473529 25348 solver.cpp:539]     Test net output #2: loss = 0.1568 (* 1 = 0.1568 loss)
I0704 07:44:28.494060 25348 solver.cpp:290] Iteration 3000 (26.9973 iter/s, 3.70407s/100 iter), loss = 0
I0704 07:44:28.494077 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:28.494091 25348 sgd_solver.cpp:106] Iteration 3000, lr = 0.00953125
I0704 07:44:30.554286 25348 solver.cpp:290] Iteration 3100 (48.5403 iter/s, 2.06014s/100 iter), loss = 0
I0704 07:44:30.554308 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:30.554316 25348 sgd_solver.cpp:106] Iteration 3100, lr = 0.00951563
I0704 07:44:32.613104 25348 solver.cpp:290] Iteration 3200 (48.5735 iter/s, 2.05873s/100 iter), loss = 0
I0704 07:44:32.613127 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:32.613134 25348 sgd_solver.cpp:106] Iteration 3200, lr = 0.0095
I0704 07:44:34.688091 25348 solver.cpp:290] Iteration 3300 (48.1951 iter/s, 2.0749s/100 iter), loss = 0
I0704 07:44:34.688113 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:34.688122 25348 sgd_solver.cpp:106] Iteration 3300, lr = 0.00948437
I0704 07:44:36.756675 25348 solver.cpp:290] Iteration 3400 (48.3443 iter/s, 2.0685s/100 iter), loss = 0
I0704 07:44:36.756696 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:36.756703 25348 sgd_solver.cpp:106] Iteration 3400, lr = 0.00946875
I0704 07:44:38.816352 25348 solver.cpp:290] Iteration 3500 (48.5533 iter/s, 2.05959s/100 iter), loss = 0
I0704 07:44:38.816375 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:38.816383 25348 sgd_solver.cpp:106] Iteration 3500, lr = 0.00945312
I0704 07:44:40.872460 25348 solver.cpp:290] Iteration 3600 (48.6376 iter/s, 2.05602s/100 iter), loss = 0
I0704 07:44:40.872481 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:40.872488 25348 sgd_solver.cpp:106] Iteration 3600, lr = 0.0094375
I0704 07:44:42.936831 25348 solver.cpp:290] Iteration 3700 (48.4429 iter/s, 2.06429s/100 iter), loss = 0
I0704 07:44:42.936882 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:42.936893 25348 sgd_solver.cpp:106] Iteration 3700, lr = 0.00942187
I0704 07:44:44.995880 25348 solver.cpp:290] Iteration 3800 (48.5687 iter/s, 2.05894s/100 iter), loss = 0
I0704 07:44:44.995909 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:44.995918 25348 sgd_solver.cpp:106] Iteration 3800, lr = 0.00940625
I0704 07:44:47.054013 25348 solver.cpp:290] Iteration 3900 (48.59 iter/s, 2.05804s/100 iter), loss = 0
I0704 07:44:47.054049 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:47.054059 25348 sgd_solver.cpp:106] Iteration 3900, lr = 0.00939062
I0704 07:44:49.091100 25348 solver.cpp:354] Sparsity after update:
I0704 07:44:49.092226 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:44:49.092233 25348 net.cpp:1851] conv1a_param_0(0) 
I0704 07:44:49.092239 25348 net.cpp:1851] conv1b_param_0(0) 
I0704 07:44:49.092242 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:44:49.092243 25348 net.cpp:1851] res2a_branch2a_param_0(0) 
I0704 07:44:49.092245 25348 net.cpp:1851] res2a_branch2b_param_0(0) 
I0704 07:44:49.092247 25348 net.cpp:1851] res3a_branch2a_param_0(0) 
I0704 07:44:49.092249 25348 net.cpp:1851] res3a_branch2b_param_0(0) 
I0704 07:44:49.092252 25348 net.cpp:1851] res4a_branch2a_param_0(0) 
I0704 07:44:49.092253 25348 net.cpp:1851] res4a_branch2b_param_0(0) 
I0704 07:44:49.092255 25348 net.cpp:1851] res5a_branch2a_param_0(0) 
I0704 07:44:49.092257 25348 net.cpp:1851] res5a_branch2b_param_0(0) 
I0704 07:44:49.092259 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0704 07:44:49.092345 25348 solver.cpp:466] Iteration 4000, Testing net (#0)
I0704 07:44:50.733412 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.917
I0704 07:44:50.733518 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9969
I0704 07:44:50.733525 25348 solver.cpp:539]     Test net output #2: loss = 0.1545 (* 1 = 0.1545 loss)
I0704 07:44:50.754206 25348 solver.cpp:290] Iteration 4000 (27.0266 iter/s, 3.70006s/100 iter), loss = 0
I0704 07:44:50.754222 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:50.754236 25348 sgd_solver.cpp:106] Iteration 4000, lr = 0.009375
I0704 07:44:50.754789 25348 solver.cpp:375] Finding and applying sparsity: 0
I0704 07:44:50.779611 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:44:52.870257 25348 solver.cpp:290] Iteration 4100 (47.2597 iter/s, 2.11597s/100 iter), loss = 0
I0704 07:44:52.870280 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:52.870285 25348 sgd_solver.cpp:106] Iteration 4100, lr = 0.00935937
I0704 07:44:54.943976 25348 solver.cpp:290] Iteration 4200 (48.2245 iter/s, 2.07363s/100 iter), loss = 0
I0704 07:44:54.944000 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:54.944008 25348 sgd_solver.cpp:106] Iteration 4200, lr = 0.00934375
I0704 07:44:57.020900 25348 solver.cpp:290] Iteration 4300 (48.1501 iter/s, 2.07684s/100 iter), loss = 0
I0704 07:44:57.020920 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:57.020927 25348 sgd_solver.cpp:106] Iteration 4300, lr = 0.00932813
I0704 07:44:59.093367 25348 solver.cpp:290] Iteration 4400 (48.2537 iter/s, 2.07238s/100 iter), loss = 0
I0704 07:44:59.093394 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:59.093402 25348 sgd_solver.cpp:106] Iteration 4400, lr = 0.0093125
I0704 07:45:01.165673 25348 solver.cpp:290] Iteration 4500 (48.2575 iter/s, 2.07222s/100 iter), loss = 0
I0704 07:45:01.165694 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:01.165702 25348 sgd_solver.cpp:106] Iteration 4500, lr = 0.00929687
I0704 07:45:03.239133 25348 solver.cpp:290] Iteration 4600 (48.2305 iter/s, 2.07337s/100 iter), loss = 0
I0704 07:45:03.239156 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:03.239162 25348 sgd_solver.cpp:106] Iteration 4600, lr = 0.00928125
I0704 07:45:05.311424 25348 solver.cpp:290] Iteration 4700 (48.2578 iter/s, 2.0722s/100 iter), loss = 0
I0704 07:45:05.311447 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:05.311456 25348 sgd_solver.cpp:106] Iteration 4700, lr = 0.00926562
I0704 07:45:07.393344 25348 solver.cpp:290] Iteration 4800 (48.0346 iter/s, 2.08183s/100 iter), loss = 0
I0704 07:45:07.393368 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:07.393376 25348 sgd_solver.cpp:106] Iteration 4800, lr = 0.00925
I0704 07:45:09.463078 25348 solver.cpp:290] Iteration 4900 (48.3174 iter/s, 2.06965s/100 iter), loss = 0
I0704 07:45:09.463101 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:09.463109 25348 sgd_solver.cpp:106] Iteration 4900, lr = 0.00923437
I0704 07:45:11.526651 25348 solver.cpp:354] Sparsity after update:
I0704 07:45:11.528034 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:45:11.528041 25348 net.cpp:1851] conv1a_param_0(0) 
I0704 07:45:11.528048 25348 net.cpp:1851] conv1b_param_0(0) 
I0704 07:45:11.528050 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:45:11.528053 25348 net.cpp:1851] res2a_branch2a_param_0(0) 
I0704 07:45:11.528054 25348 net.cpp:1851] res2a_branch2b_param_0(0) 
I0704 07:45:11.528056 25348 net.cpp:1851] res3a_branch2a_param_0(0) 
I0704 07:45:11.528059 25348 net.cpp:1851] res3a_branch2b_param_0(0) 
I0704 07:45:11.528060 25348 net.cpp:1851] res4a_branch2a_param_0(0) 
I0704 07:45:11.528062 25348 net.cpp:1851] res4a_branch2b_param_0(0) 
I0704 07:45:11.528064 25348 net.cpp:1851] res5a_branch2a_param_0(0) 
I0704 07:45:11.528065 25348 net.cpp:1851] res5a_branch2b_param_0(0) 
I0704 07:45:11.528067 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0704 07:45:11.528164 25348 solver.cpp:466] Iteration 5000, Testing net (#0)
I0704 07:45:13.172219 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9122
I0704 07:45:13.172238 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9978
I0704 07:45:13.172245 25348 solver.cpp:539]     Test net output #2: loss = 0.1563 (* 1 = 0.1563 loss)
I0704 07:45:13.192526 25348 solver.cpp:290] Iteration 5000 (26.8146 iter/s, 3.72932s/100 iter), loss = 0
I0704 07:45:13.192543 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:13.192556 25348 sgd_solver.cpp:106] Iteration 5000, lr = 0.00921875
I0704 07:45:13.193089 25348 solver.cpp:375] Finding and applying sparsity: 0.02
I0704 07:45:13.449759 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:45:15.537963 25348 solver.cpp:290] Iteration 5100 (42.6376 iter/s, 2.34535s/100 iter), loss = 0
I0704 07:45:15.537986 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:15.537992 25348 sgd_solver.cpp:106] Iteration 5100, lr = 0.00920312
I0704 07:45:17.613046 25348 solver.cpp:290] Iteration 5200 (48.1929 iter/s, 2.075s/100 iter), loss = 0
I0704 07:45:17.613068 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:17.613075 25348 sgd_solver.cpp:106] Iteration 5200, lr = 0.0091875
I0704 07:45:19.686949 25348 solver.cpp:290] Iteration 5300 (48.2202 iter/s, 2.07382s/100 iter), loss = 0
I0704 07:45:19.686970 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:19.686976 25348 sgd_solver.cpp:106] Iteration 5300, lr = 0.00917188
I0704 07:45:21.759461 25348 solver.cpp:290] Iteration 5400 (48.2526 iter/s, 2.07243s/100 iter), loss = 0
I0704 07:45:21.759538 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:21.759547 25348 sgd_solver.cpp:106] Iteration 5400, lr = 0.00915625
I0704 07:45:23.840195 25348 solver.cpp:290] Iteration 5500 (48.0632 iter/s, 2.0806s/100 iter), loss = 0
I0704 07:45:23.840219 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:23.840224 25348 sgd_solver.cpp:106] Iteration 5500, lr = 0.00914062
I0704 07:45:25.913897 25348 solver.cpp:290] Iteration 5600 (48.2249 iter/s, 2.07362s/100 iter), loss = 0
I0704 07:45:25.913920 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:25.913926 25348 sgd_solver.cpp:106] Iteration 5600, lr = 0.009125
I0704 07:45:27.992074 25348 solver.cpp:290] Iteration 5700 (48.1211 iter/s, 2.07809s/100 iter), loss = 0
I0704 07:45:27.992095 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:27.992101 25348 sgd_solver.cpp:106] Iteration 5700, lr = 0.00910938
I0704 07:45:30.065008 25348 solver.cpp:290] Iteration 5800 (48.2428 iter/s, 2.07285s/100 iter), loss = 0
I0704 07:45:30.065029 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:30.065037 25348 sgd_solver.cpp:106] Iteration 5800, lr = 0.00909375
I0704 07:45:32.141949 25348 solver.cpp:290] Iteration 5900 (48.1497 iter/s, 2.07685s/100 iter), loss = 0
I0704 07:45:32.141969 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:32.141976 25348 sgd_solver.cpp:106] Iteration 5900, lr = 0.00907812
I0704 07:45:34.201381 25348 solver.cpp:354] Sparsity after update:
I0704 07:45:34.202790 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:45:34.202797 25348 net.cpp:1851] conv1a_param_0(0.01) 
I0704 07:45:34.202805 25348 net.cpp:1851] conv1b_param_0(0.02) 
I0704 07:45:34.202807 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:45:34.202810 25348 net.cpp:1851] res2a_branch2a_param_0(0.02) 
I0704 07:45:34.202811 25348 net.cpp:1851] res2a_branch2b_param_0(0.02) 
I0704 07:45:34.202813 25348 net.cpp:1851] res3a_branch2a_param_0(0.02) 
I0704 07:45:34.202816 25348 net.cpp:1851] res3a_branch2b_param_0(0.02) 
I0704 07:45:34.202817 25348 net.cpp:1851] res4a_branch2a_param_0(0.02) 
I0704 07:45:34.202819 25348 net.cpp:1851] res4a_branch2b_param_0(0.02) 
I0704 07:45:34.202821 25348 net.cpp:1851] res5a_branch2a_param_0(0.02) 
I0704 07:45:34.202823 25348 net.cpp:1851] res5a_branch2b_param_0(0.02) 
I0704 07:45:34.202826 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (47061/2.3599e+06) 0.0199
I0704 07:45:34.202919 25348 solver.cpp:466] Iteration 6000, Testing net (#0)
I0704 07:45:35.852412 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9125
I0704 07:45:35.852430 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9968
I0704 07:45:35.852437 25348 solver.cpp:539]     Test net output #2: loss = 0.1561 (* 1 = 0.1561 loss)
I0704 07:45:35.872300 25348 solver.cpp:290] Iteration 6000 (26.808 iter/s, 3.73023s/100 iter), loss = 0
I0704 07:45:35.872318 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:35.872334 25348 sgd_solver.cpp:106] Iteration 6000, lr = 0.0090625
I0704 07:45:35.872882 25348 solver.cpp:375] Finding and applying sparsity: 0.04
I0704 07:45:36.135357 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:45:38.230183 25348 solver.cpp:290] Iteration 6100 (42.4126 iter/s, 2.35779s/100 iter), loss = 0
I0704 07:45:38.230208 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:38.230217 25348 sgd_solver.cpp:106] Iteration 6100, lr = 0.00904687
I0704 07:45:40.302433 25348 solver.cpp:290] Iteration 6200 (48.2588 iter/s, 2.07216s/100 iter), loss = 0
I0704 07:45:40.302458 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:40.302466 25348 sgd_solver.cpp:106] Iteration 6200, lr = 0.00903125
I0704 07:45:42.377840 25348 solver.cpp:290] Iteration 6300 (48.1854 iter/s, 2.07532s/100 iter), loss = 0
I0704 07:45:42.377864 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:42.377890 25348 sgd_solver.cpp:106] Iteration 6300, lr = 0.00901563
I0704 07:45:44.454025 25348 solver.cpp:290] Iteration 6400 (48.1672 iter/s, 2.0761s/100 iter), loss = 0
I0704 07:45:44.454049 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:44.454057 25348 sgd_solver.cpp:106] Iteration 6400, lr = 0.009
I0704 07:45:46.527431 25348 solver.cpp:290] Iteration 6500 (48.2318 iter/s, 2.07332s/100 iter), loss = 0
I0704 07:45:46.527453 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:46.527462 25348 sgd_solver.cpp:106] Iteration 6500, lr = 0.00898437
I0704 07:45:48.599530 25348 solver.cpp:290] Iteration 6600 (48.2623 iter/s, 2.07201s/100 iter), loss = 0
I0704 07:45:48.599553 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:48.599560 25348 sgd_solver.cpp:106] Iteration 6600, lr = 0.00896875
I0704 07:45:50.674157 25348 solver.cpp:290] Iteration 6700 (48.2034 iter/s, 2.07454s/100 iter), loss = 0
I0704 07:45:50.674180 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:50.674187 25348 sgd_solver.cpp:106] Iteration 6700, lr = 0.00895312
I0704 07:45:52.744110 25348 solver.cpp:290] Iteration 6800 (48.3123 iter/s, 2.06987s/100 iter), loss = 0
I0704 07:45:52.744173 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:52.744179 25348 sgd_solver.cpp:106] Iteration 6800, lr = 0.0089375
I0704 07:45:54.816160 25348 solver.cpp:290] Iteration 6900 (48.2643 iter/s, 2.07193s/100 iter), loss = 0
I0704 07:45:54.816182 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:54.816189 25348 sgd_solver.cpp:106] Iteration 6900, lr = 0.00892187
I0704 07:45:56.868746 25348 solver.cpp:354] Sparsity after update:
I0704 07:45:56.870124 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:45:56.870132 25348 net.cpp:1851] conv1a_param_0(0.02) 
I0704 07:45:56.870143 25348 net.cpp:1851] conv1b_param_0(0.0399) 
I0704 07:45:56.870149 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:45:56.870153 25348 net.cpp:1851] res2a_branch2a_param_0(0.04) 
I0704 07:45:56.870158 25348 net.cpp:1851] res2a_branch2b_param_0(0.0399) 
I0704 07:45:56.870162 25348 net.cpp:1851] res3a_branch2a_param_0(0.04) 
I0704 07:45:56.870167 25348 net.cpp:1851] res3a_branch2b_param_0(0.04) 
I0704 07:45:56.870170 25348 net.cpp:1851] res4a_branch2a_param_0(0.04) 
I0704 07:45:56.870174 25348 net.cpp:1851] res4a_branch2b_param_0(0.04) 
I0704 07:45:56.870178 25348 net.cpp:1851] res5a_branch2a_param_0(0.04) 
I0704 07:45:56.870182 25348 net.cpp:1851] res5a_branch2b_param_0(0.04) 
I0704 07:45:56.870187 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (94133/2.3599e+06) 0.0399
I0704 07:45:56.870277 25348 solver.cpp:466] Iteration 7000, Testing net (#0)
I0704 07:45:58.528292 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.906
I0704 07:45:58.528312 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9955
I0704 07:45:58.528318 25348 solver.cpp:539]     Test net output #2: loss = 0.186 (* 1 = 0.186 loss)
I0704 07:45:58.547979 25348 solver.cpp:290] Iteration 7000 (26.7975 iter/s, 3.73169s/100 iter), loss = 0
I0704 07:45:58.547996 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:58.548008 25348 sgd_solver.cpp:106] Iteration 7000, lr = 0.00890625
I0704 07:45:58.548506 25348 solver.cpp:375] Finding and applying sparsity: 0.06
I0704 07:45:58.824502 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:46:00.909068 25348 solver.cpp:290] Iteration 7100 (42.3549 iter/s, 2.361s/100 iter), loss = 0
I0704 07:46:00.909090 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:00.909096 25348 sgd_solver.cpp:106] Iteration 7100, lr = 0.00889063
I0704 07:46:02.984323 25348 solver.cpp:290] Iteration 7200 (48.1888 iter/s, 2.07517s/100 iter), loss = 0
I0704 07:46:02.984344 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:02.984351 25348 sgd_solver.cpp:106] Iteration 7200, lr = 0.008875
I0704 07:46:05.056545 25348 solver.cpp:290] Iteration 7300 (48.2594 iter/s, 2.07214s/100 iter), loss = 0
I0704 07:46:05.056565 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:05.056572 25348 sgd_solver.cpp:106] Iteration 7300, lr = 0.00885937
I0704 07:46:07.132844 25348 solver.cpp:290] Iteration 7400 (48.1646 iter/s, 2.07621s/100 iter), loss = 0
I0704 07:46:07.132874 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:07.132884 25348 sgd_solver.cpp:106] Iteration 7400, lr = 0.00884375
I0704 07:46:09.208468 25348 solver.cpp:290] Iteration 7500 (48.1804 iter/s, 2.07553s/100 iter), loss = 0
I0704 07:46:09.208489 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:09.208498 25348 sgd_solver.cpp:106] Iteration 7500, lr = 0.00882812
I0704 07:46:11.280071 25348 solver.cpp:290] Iteration 7600 (48.2738 iter/s, 2.07152s/100 iter), loss = 0
I0704 07:46:11.280092 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:11.280100 25348 sgd_solver.cpp:106] Iteration 7600, lr = 0.0088125
I0704 07:46:13.356956 25348 solver.cpp:290] Iteration 7700 (48.1511 iter/s, 2.0768s/100 iter), loss = 0
I0704 07:46:13.356982 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:13.357013 25348 sgd_solver.cpp:106] Iteration 7700, lr = 0.00879687
I0704 07:46:15.432950 25348 solver.cpp:290] Iteration 7800 (48.1718 iter/s, 2.07591s/100 iter), loss = 0
I0704 07:46:15.432972 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:15.432979 25348 sgd_solver.cpp:106] Iteration 7800, lr = 0.00878125
I0704 07:46:17.508100 25348 solver.cpp:290] Iteration 7900 (48.1913 iter/s, 2.07506s/100 iter), loss = 0
I0704 07:46:17.508121 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:17.508128 25348 sgd_solver.cpp:106] Iteration 7900, lr = 0.00876562
I0704 07:46:19.560109 25348 solver.cpp:354] Sparsity after update:
I0704 07:46:19.561491 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:46:19.561498 25348 net.cpp:1851] conv1a_param_0(0.03) 
I0704 07:46:19.561506 25348 net.cpp:1851] conv1b_param_0(0.0599) 
I0704 07:46:19.561508 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:46:19.561511 25348 net.cpp:1851] res2a_branch2a_param_0(0.06) 
I0704 07:46:19.561512 25348 net.cpp:1851] res2a_branch2b_param_0(0.0599) 
I0704 07:46:19.561514 25348 net.cpp:1851] res3a_branch2a_param_0(0.06) 
I0704 07:46:19.561517 25348 net.cpp:1851] res3a_branch2b_param_0(0.06) 
I0704 07:46:19.561518 25348 net.cpp:1851] res4a_branch2a_param_0(0.06) 
I0704 07:46:19.561520 25348 net.cpp:1851] res4a_branch2b_param_0(0.06) 
I0704 07:46:19.561522 25348 net.cpp:1851] res5a_branch2a_param_0(0.06) 
I0704 07:46:19.561524 25348 net.cpp:1851] res5a_branch2b_param_0(0.06) 
I0704 07:46:19.561527 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (141203/2.3599e+06) 0.0598
I0704 07:46:19.561612 25348 solver.cpp:466] Iteration 8000, Testing net (#0)
I0704 07:46:21.205999 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9029
I0704 07:46:21.206018 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9963
I0704 07:46:21.206024 25348 solver.cpp:539]     Test net output #2: loss = 0.1849 (* 1 = 0.1849 loss)
I0704 07:46:21.226096 25348 solver.cpp:290] Iteration 8000 (26.8971 iter/s, 3.71787s/100 iter), loss = 0
I0704 07:46:21.226114 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:21.226125 25348 sgd_solver.cpp:106] Iteration 8000, lr = 0.00875
I0704 07:46:21.226650 25348 solver.cpp:375] Finding and applying sparsity: 0.08
I0704 07:46:21.523664 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:46:23.611918 25348 solver.cpp:290] Iteration 8100 (41.9158 iter/s, 2.38573s/100 iter), loss = 0
I0704 07:46:23.611994 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:23.612002 25348 sgd_solver.cpp:106] Iteration 8100, lr = 0.00873438
I0704 07:46:25.685705 25348 solver.cpp:290] Iteration 8200 (48.2242 iter/s, 2.07365s/100 iter), loss = 0
I0704 07:46:25.685729 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:25.685736 25348 sgd_solver.cpp:106] Iteration 8200, lr = 0.00871875
I0704 07:46:27.761958 25348 solver.cpp:290] Iteration 8300 (48.1657 iter/s, 2.07617s/100 iter), loss = 0
I0704 07:46:27.761982 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:27.761988 25348 sgd_solver.cpp:106] Iteration 8300, lr = 0.00870312
I0704 07:46:29.835227 25348 solver.cpp:290] Iteration 8400 (48.2351 iter/s, 2.07318s/100 iter), loss = 0
I0704 07:46:29.835247 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:29.835254 25348 sgd_solver.cpp:106] Iteration 8400, lr = 0.0086875
I0704 07:46:31.905133 25348 solver.cpp:290] Iteration 8500 (48.3134 iter/s, 2.06982s/100 iter), loss = 0
I0704 07:46:31.905154 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:31.905161 25348 sgd_solver.cpp:106] Iteration 8500, lr = 0.00867188
I0704 07:46:34.003264 25348 solver.cpp:290] Iteration 8600 (47.6634 iter/s, 2.09805s/100 iter), loss = 0
I0704 07:46:34.003288 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:34.003298 25348 sgd_solver.cpp:106] Iteration 8600, lr = 0.00865625
I0704 07:46:36.074009 25348 solver.cpp:290] Iteration 8700 (48.2939 iter/s, 2.07066s/100 iter), loss = 0
I0704 07:46:36.074038 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:36.074045 25348 sgd_solver.cpp:106] Iteration 8700, lr = 0.00864062
I0704 07:46:38.146744 25348 solver.cpp:290] Iteration 8800 (48.2475 iter/s, 2.07264s/100 iter), loss = 0
I0704 07:46:38.146771 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:38.146780 25348 sgd_solver.cpp:106] Iteration 8800, lr = 0.008625
I0704 07:46:40.225381 25348 solver.cpp:290] Iteration 8900 (48.1106 iter/s, 2.07855s/100 iter), loss = 0
I0704 07:46:40.225407 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:40.225416 25348 sgd_solver.cpp:106] Iteration 8900, lr = 0.00860937
I0704 07:46:42.278522 25348 solver.cpp:354] Sparsity after update:
I0704 07:46:42.279929 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:46:42.279937 25348 net.cpp:1851] conv1a_param_0(0.0396) 
I0704 07:46:42.279947 25348 net.cpp:1851] conv1b_param_0(0.0799) 
I0704 07:46:42.279952 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:46:42.279955 25348 net.cpp:1851] res2a_branch2a_param_0(0.08) 
I0704 07:46:42.279960 25348 net.cpp:1851] res2a_branch2b_param_0(0.08) 
I0704 07:46:42.279965 25348 net.cpp:1851] res3a_branch2a_param_0(0.08) 
I0704 07:46:42.279969 25348 net.cpp:1851] res3a_branch2b_param_0(0.08) 
I0704 07:46:42.279973 25348 net.cpp:1851] res4a_branch2a_param_0(0.08) 
I0704 07:46:42.279978 25348 net.cpp:1851] res4a_branch2b_param_0(0.08) 
I0704 07:46:42.279983 25348 net.cpp:1851] res5a_branch2a_param_0(0.08) 
I0704 07:46:42.279986 25348 net.cpp:1851] res5a_branch2b_param_0(0.08) 
I0704 07:46:42.279991 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (188278/2.3599e+06) 0.0798
I0704 07:46:42.280091 25348 solver.cpp:466] Iteration 9000, Testing net (#0)
I0704 07:46:43.925376 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8996
I0704 07:46:43.925397 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9926
I0704 07:46:43.925402 25348 solver.cpp:539]     Test net output #2: loss = 0.2014 (* 1 = 0.2014 loss)
I0704 07:46:43.945425 25348 solver.cpp:290] Iteration 9000 (26.8824 iter/s, 3.71991s/100 iter), loss = 0
I0704 07:46:43.945446 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:43.945454 25348 sgd_solver.cpp:106] Iteration 9000, lr = 0.00859375
I0704 07:46:43.946218 25348 solver.cpp:375] Finding and applying sparsity: 0.1
I0704 07:46:44.238622 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:46:46.345155 25348 solver.cpp:290] Iteration 9100 (41.673 iter/s, 2.39963s/100 iter), loss = 0
I0704 07:46:46.345180 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:46.345188 25348 sgd_solver.cpp:106] Iteration 9100, lr = 0.00857813
I0704 07:46:48.420380 25348 solver.cpp:290] Iteration 9200 (48.1895 iter/s, 2.07514s/100 iter), loss = 0
I0704 07:46:48.420402 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:48.420408 25348 sgd_solver.cpp:106] Iteration 9200, lr = 0.0085625
I0704 07:46:50.499253 25348 solver.cpp:290] Iteration 9300 (48.1049 iter/s, 2.07879s/100 iter), loss = 0
I0704 07:46:50.499277 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:50.499286 25348 sgd_solver.cpp:106] Iteration 9300, lr = 0.00854687
I0704 07:46:52.572290 25348 solver.cpp:290] Iteration 9400 (48.2405 iter/s, 2.07295s/100 iter), loss = 0
I0704 07:46:52.572314 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:52.572321 25348 sgd_solver.cpp:106] Iteration 9400, lr = 0.00853125
I0704 07:46:54.648319 25348 solver.cpp:290] Iteration 9500 (48.1709 iter/s, 2.07594s/100 iter), loss = 0
I0704 07:46:54.648393 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:54.648401 25348 sgd_solver.cpp:106] Iteration 9500, lr = 0.00851563
I0704 07:46:56.722234 25348 solver.cpp:290] Iteration 9600 (48.2211 iter/s, 2.07378s/100 iter), loss = 0
I0704 07:46:56.722256 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:56.722263 25348 sgd_solver.cpp:106] Iteration 9600, lr = 0.0085
I0704 07:46:58.792049 25348 solver.cpp:290] Iteration 9700 (48.3155 iter/s, 2.06973s/100 iter), loss = 0.190476
I0704 07:46:58.792071 25348 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:46:58.792080 25348 sgd_solver.cpp:106] Iteration 9700, lr = 0.00848437
I0704 07:47:00.869410 25348 solver.cpp:290] Iteration 9800 (48.14 iter/s, 2.07728s/100 iter), loss = 0.047619
I0704 07:47:00.869431 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:00.869439 25348 sgd_solver.cpp:106] Iteration 9800, lr = 0.00846875
I0704 07:47:02.944416 25348 solver.cpp:290] Iteration 9900 (48.1946 iter/s, 2.07492s/100 iter), loss = -4.47035e-08
I0704 07:47:02.944438 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:02.944444 25348 sgd_solver.cpp:106] Iteration 9900, lr = 0.00845312
I0704 07:47:04.994676 25348 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_10000.caffemodel
I0704 07:47:05.018944 25348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_10000.solverstate
I0704 07:47:05.026366 25348 solver.cpp:354] Sparsity after update:
I0704 07:47:05.027302 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:47:05.027309 25348 net.cpp:1851] conv1a_param_0(0.0496) 
I0704 07:47:05.027318 25348 net.cpp:1851] conv1b_param_0(0.0998) 
I0704 07:47:05.027319 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:47:05.027323 25348 net.cpp:1851] res2a_branch2a_param_0(0.1) 
I0704 07:47:05.027324 25348 net.cpp:1851] res2a_branch2b_param_0(0.0999) 
I0704 07:47:05.027326 25348 net.cpp:1851] res3a_branch2a_param_0(0.1) 
I0704 07:47:05.027328 25348 net.cpp:1851] res3a_branch2b_param_0(0.1) 
I0704 07:47:05.027330 25348 net.cpp:1851] res4a_branch2a_param_0(0.1) 
I0704 07:47:05.027333 25348 net.cpp:1851] res4a_branch2b_param_0(0.1) 
I0704 07:47:05.027334 25348 net.cpp:1851] res5a_branch2a_param_0(0.1) 
I0704 07:47:05.027336 25348 net.cpp:1851] res5a_branch2b_param_0(0.1) 
I0704 07:47:05.027338 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (235342/2.3599e+06) 0.0997
I0704 07:47:05.027441 25348 solver.cpp:466] Iteration 10000, Testing net (#0)
I0704 07:47:06.671926 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8089
I0704 07:47:06.671944 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9867
I0704 07:47:06.671949 25348 solver.cpp:539]     Test net output #2: loss = 0.4254 (* 1 = 0.4254 loss)
I0704 07:47:06.692207 25348 solver.cpp:290] Iteration 10000 (26.6833 iter/s, 3.74766s/100 iter), loss = 0.047619
I0704 07:47:06.692224 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:06.692234 25348 sgd_solver.cpp:106] Iteration 10000, lr = 0.0084375
I0704 07:47:06.692782 25348 solver.cpp:375] Finding and applying sparsity: 0.12
I0704 07:47:06.946877 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:47:09.033807 25348 solver.cpp:290] Iteration 10100 (42.7074 iter/s, 2.34151s/100 iter), loss = 0.047619
I0704 07:47:09.033828 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:09.033834 25348 sgd_solver.cpp:106] Iteration 10100, lr = 0.00842187
I0704 07:47:11.116773 25348 solver.cpp:290] Iteration 10200 (48.0104 iter/s, 2.08288s/100 iter), loss = 0.047619
I0704 07:47:11.116794 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:11.116802 25348 sgd_solver.cpp:106] Iteration 10200, lr = 0.00840625
I0704 07:47:13.188920 25348 solver.cpp:290] Iteration 10300 (48.2611 iter/s, 2.07206s/100 iter), loss = -1.04308e-07
I0704 07:47:13.188942 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:13.188949 25348 sgd_solver.cpp:106] Iteration 10300, lr = 0.00839063
I0704 07:47:15.261929 25348 solver.cpp:290] Iteration 10400 (48.2411 iter/s, 2.07292s/100 iter), loss = 0.047619
I0704 07:47:15.261953 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:15.261962 25348 sgd_solver.cpp:106] Iteration 10400, lr = 0.008375
I0704 07:47:17.336333 25348 solver.cpp:290] Iteration 10500 (48.2086 iter/s, 2.07432s/100 iter), loss = -1.04308e-07
I0704 07:47:17.336356 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:17.336362 25348 sgd_solver.cpp:106] Iteration 10500, lr = 0.00835937
I0704 07:47:19.407399 25348 solver.cpp:290] Iteration 10600 (48.2863 iter/s, 2.07098s/100 iter), loss = -1.04308e-07
I0704 07:47:19.407421 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:19.407428 25348 sgd_solver.cpp:106] Iteration 10600, lr = 0.00834375
I0704 07:47:21.482646 25348 solver.cpp:290] Iteration 10700 (48.189 iter/s, 2.07516s/100 iter), loss = 0.047619
I0704 07:47:21.482669 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:21.482676 25348 sgd_solver.cpp:106] Iteration 10700, lr = 0.00832812
I0704 07:47:23.554105 25348 solver.cpp:290] Iteration 10800 (48.2772 iter/s, 2.07137s/100 iter), loss = -1.04308e-07
I0704 07:47:23.554127 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:23.554134 25348 sgd_solver.cpp:106] Iteration 10800, lr = 0.0083125
I0704 07:47:25.627172 25348 solver.cpp:290] Iteration 10900 (48.2397 iter/s, 2.07298s/100 iter), loss = 0.0476189
I0704 07:47:25.627244 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:25.627259 25348 sgd_solver.cpp:106] Iteration 10900, lr = 0.00829687
I0704 07:47:27.680713 25348 solver.cpp:354] Sparsity after update:
I0704 07:47:27.682126 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:47:27.682134 25348 net.cpp:1851] conv1a_param_0(0.0596) 
I0704 07:47:27.682144 25348 net.cpp:1851] conv1b_param_0(0.12) 
I0704 07:47:27.682149 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:47:27.682154 25348 net.cpp:1851] res2a_branch2a_param_0(0.12) 
I0704 07:47:27.682158 25348 net.cpp:1851] res2a_branch2b_param_0(0.12) 
I0704 07:47:27.682163 25348 net.cpp:1851] res3a_branch2a_param_0(0.12) 
I0704 07:47:27.682166 25348 net.cpp:1851] res3a_branch2b_param_0(0.12) 
I0704 07:47:27.682170 25348 net.cpp:1851] res4a_branch2a_param_0(0.12) 
I0704 07:47:27.682174 25348 net.cpp:1851] res4a_branch2b_param_0(0.12) 
I0704 07:47:27.682178 25348 net.cpp:1851] res5a_branch2a_param_0(0.12) 
I0704 07:47:27.682183 25348 net.cpp:1851] res5a_branch2b_param_0(0.12) 
I0704 07:47:27.682186 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (282421/2.3599e+06) 0.12
I0704 07:47:27.682276 25348 solver.cpp:466] Iteration 11000, Testing net (#0)
I0704 07:47:29.332545 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.751
I0704 07:47:29.332563 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9884
I0704 07:47:29.332571 25348 solver.cpp:539]     Test net output #2: loss = 0.725 (* 1 = 0.725 loss)
I0704 07:47:29.352393 25348 solver.cpp:290] Iteration 11000 (26.8453 iter/s, 3.72505s/100 iter), loss = 0.0476189
I0704 07:47:29.352412 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:29.352427 25348 sgd_solver.cpp:106] Iteration 11000, lr = 0.00828125
I0704 07:47:29.352993 25348 solver.cpp:375] Finding and applying sparsity: 0.14
I0704 07:47:29.587641 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:47:31.679364 25348 solver.cpp:290] Iteration 11100 (42.9759 iter/s, 2.32688s/100 iter), loss = 0.0476189
I0704 07:47:31.679385 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:31.679392 25348 sgd_solver.cpp:106] Iteration 11100, lr = 0.00826562
I0704 07:47:33.789124 25348 solver.cpp:290] Iteration 11200 (47.4007 iter/s, 2.10967s/100 iter), loss = 0.142857
I0704 07:47:33.789151 25348 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:47:33.789160 25348 sgd_solver.cpp:106] Iteration 11200, lr = 0.00825
I0704 07:47:35.859431 25348 solver.cpp:290] Iteration 11300 (48.3041 iter/s, 2.07022s/100 iter), loss = -1.37836e-07
I0704 07:47:35.859457 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:35.859467 25348 sgd_solver.cpp:106] Iteration 11300, lr = 0.00823438
I0704 07:47:37.936087 25348 solver.cpp:290] Iteration 11400 (48.1564 iter/s, 2.07657s/100 iter), loss = 0.095238
I0704 07:47:37.936113 25348 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:47:37.936122 25348 sgd_solver.cpp:106] Iteration 11400, lr = 0.00821875
I0704 07:47:40.009002 25348 solver.cpp:290] Iteration 11500 (48.2433 iter/s, 2.07283s/100 iter), loss = 0.0476189
I0704 07:47:40.009026 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:40.009032 25348 sgd_solver.cpp:106] Iteration 11500, lr = 0.00820312
I0704 07:47:42.084405 25348 solver.cpp:290] Iteration 11600 (48.1854 iter/s, 2.07532s/100 iter), loss = -1.3411e-07
I0704 07:47:42.084427 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:42.084434 25348 sgd_solver.cpp:106] Iteration 11600, lr = 0.0081875
I0704 07:47:44.156862 25348 solver.cpp:290] Iteration 11700 (48.2539 iter/s, 2.07237s/100 iter), loss = -1.3411e-07
I0704 07:47:44.156885 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:44.156891 25348 sgd_solver.cpp:106] Iteration 11700, lr = 0.00817188
I0704 07:47:46.236239 25348 solver.cpp:290] Iteration 11800 (48.0933 iter/s, 2.07929s/100 iter), loss = 0.0476189
I0704 07:47:46.236261 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:46.236268 25348 sgd_solver.cpp:106] Iteration 11800, lr = 0.00815625
I0704 07:47:48.310560 25348 solver.cpp:290] Iteration 11900 (48.2106 iter/s, 2.07423s/100 iter), loss = -1.49012e-07
I0704 07:47:48.310583 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:48.310590 25348 sgd_solver.cpp:106] Iteration 11900, lr = 0.00814062
I0704 07:47:50.365936 25348 solver.cpp:354] Sparsity after update:
I0704 07:47:50.367327 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:47:50.367336 25348 net.cpp:1851] conv1a_param_0(0.0696) 
I0704 07:47:50.367344 25348 net.cpp:1851] conv1b_param_0(0.14) 
I0704 07:47:50.367350 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:47:50.367354 25348 net.cpp:1851] res2a_branch2a_param_0(0.14) 
I0704 07:47:50.367358 25348 net.cpp:1851] res2a_branch2b_param_0(0.14) 
I0704 07:47:50.367363 25348 net.cpp:1851] res3a_branch2a_param_0(0.14) 
I0704 07:47:50.367368 25348 net.cpp:1851] res3a_branch2b_param_0(0.14) 
I0704 07:47:50.367372 25348 net.cpp:1851] res4a_branch2a_param_0(0.14) 
I0704 07:47:50.367377 25348 net.cpp:1851] res4a_branch2b_param_0(0.14) 
I0704 07:47:50.367380 25348 net.cpp:1851] res5a_branch2a_param_0(0.14) 
I0704 07:47:50.367385 25348 net.cpp:1851] res5a_branch2b_param_0(0.14) 
I0704 07:47:50.367388 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (329490/2.3599e+06) 0.14
I0704 07:47:50.367480 25348 solver.cpp:466] Iteration 12000, Testing net (#0)
I0704 07:47:52.025094 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8343
I0704 07:47:52.025112 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9907
I0704 07:47:52.025118 25348 solver.cpp:539]     Test net output #2: loss = 0.3233 (* 1 = 0.3233 loss)
I0704 07:47:52.045275 25348 solver.cpp:290] Iteration 12000 (26.7767 iter/s, 3.73459s/100 iter), loss = 0.0952379
I0704 07:47:52.045294 25348 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:47:52.045306 25348 sgd_solver.cpp:106] Iteration 12000, lr = 0.008125
I0704 07:47:52.045859 25348 solver.cpp:375] Finding and applying sparsity: 0.16
I0704 07:47:52.322541 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:47:54.399521 25348 solver.cpp:290] Iteration 12100 (42.4781 iter/s, 2.35415s/100 iter), loss = -1.49012e-07
I0704 07:47:54.399544 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:54.399550 25348 sgd_solver.cpp:106] Iteration 12100, lr = 0.00810937
I0704 07:47:56.476440 25348 solver.cpp:290] Iteration 12200 (48.1502 iter/s, 2.07683s/100 iter), loss = 0.0476189
I0704 07:47:56.476542 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:56.476549 25348 sgd_solver.cpp:106] Iteration 12200, lr = 0.00809375
I0704 07:47:58.548276 25348 solver.cpp:290] Iteration 12300 (48.2702 iter/s, 2.07167s/100 iter), loss = -1.63913e-07
I0704 07:47:58.548300 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:58.548305 25348 sgd_solver.cpp:106] Iteration 12300, lr = 0.00807813
I0704 07:48:00.622593 25348 solver.cpp:290] Iteration 12400 (48.2106 iter/s, 2.07423s/100 iter), loss = -1.63913e-07
I0704 07:48:00.622615 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:00.622622 25348 sgd_solver.cpp:106] Iteration 12400, lr = 0.0080625
I0704 07:48:02.703852 25348 solver.cpp:290] Iteration 12500 (48.0498 iter/s, 2.08117s/100 iter), loss = -1.49012e-07
I0704 07:48:02.703873 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:02.703881 25348 sgd_solver.cpp:106] Iteration 12500, lr = 0.00804687
I0704 07:48:04.781750 25348 solver.cpp:290] Iteration 12600 (48.1275 iter/s, 2.07781s/100 iter), loss = 0.0952379
I0704 07:48:04.781774 25348 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:48:04.781780 25348 sgd_solver.cpp:106] Iteration 12600, lr = 0.00803125
I0704 07:48:06.855885 25348 solver.cpp:290] Iteration 12700 (48.2149 iter/s, 2.07405s/100 iter), loss = -1.52737e-07
I0704 07:48:06.855906 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:06.855913 25348 sgd_solver.cpp:106] Iteration 12700, lr = 0.00801562
I0704 07:48:08.929366 25348 solver.cpp:290] Iteration 12800 (48.23 iter/s, 2.0734s/100 iter), loss = -1.63913e-07
I0704 07:48:08.929389 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:08.929396 25348 sgd_solver.cpp:106] Iteration 12800, lr = 0.008
I0704 07:48:11.001621 25348 solver.cpp:290] Iteration 12900 (48.2587 iter/s, 2.07217s/100 iter), loss = -1.63913e-07
I0704 07:48:11.001646 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:11.001654 25348 sgd_solver.cpp:106] Iteration 12900, lr = 0.00798437
I0704 07:48:13.062945 25348 solver.cpp:354] Sparsity after update:
I0704 07:48:13.064328 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:48:13.064337 25348 net.cpp:1851] conv1a_param_0(0.0796) 
I0704 07:48:13.064347 25348 net.cpp:1851] conv1b_param_0(0.16) 
I0704 07:48:13.064350 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:48:13.064354 25348 net.cpp:1851] res2a_branch2a_param_0(0.16) 
I0704 07:48:13.064359 25348 net.cpp:1851] res2a_branch2b_param_0(0.16) 
I0704 07:48:13.064364 25348 net.cpp:1851] res3a_branch2a_param_0(0.16) 
I0704 07:48:13.064368 25348 net.cpp:1851] res3a_branch2b_param_0(0.16) 
I0704 07:48:13.064373 25348 net.cpp:1851] res4a_branch2a_param_0(0.16) 
I0704 07:48:13.064378 25348 net.cpp:1851] res4a_branch2b_param_0(0.16) 
I0704 07:48:13.064381 25348 net.cpp:1851] res5a_branch2a_param_0(0.16) 
I0704 07:48:13.064386 25348 net.cpp:1851] res5a_branch2b_param_0(0.16) 
I0704 07:48:13.064390 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (376562/2.3599e+06) 0.16
I0704 07:48:13.064483 25348 solver.cpp:466] Iteration 13000, Testing net (#0)
I0704 07:48:14.708025 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8348
I0704 07:48:14.708045 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9875
I0704 07:48:14.708050 25348 solver.cpp:539]     Test net output #2: loss = 0.4223 (* 1 = 0.4223 loss)
I0704 07:48:14.727730 25348 solver.cpp:290] Iteration 13000 (26.8386 iter/s, 3.72598s/100 iter), loss = 0.0476189
I0704 07:48:14.727747 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:48:14.727759 25348 sgd_solver.cpp:106] Iteration 13000, lr = 0.00796875
I0704 07:48:14.728308 25348 solver.cpp:375] Finding and applying sparsity: 0.18
I0704 07:48:14.993615 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:48:17.085896 25348 solver.cpp:290] Iteration 13100 (42.4074 iter/s, 2.35808s/100 iter), loss = -1.63913e-07
I0704 07:48:17.085934 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:17.085942 25348 sgd_solver.cpp:106] Iteration 13100, lr = 0.00795313
I0704 07:48:19.156523 25348 solver.cpp:290] Iteration 13200 (48.2969 iter/s, 2.07053s/100 iter), loss = -1.67638e-07
I0704 07:48:19.156545 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:19.156553 25348 sgd_solver.cpp:106] Iteration 13200, lr = 0.0079375
I0704 07:48:21.231608 25348 solver.cpp:290] Iteration 13300 (48.1928 iter/s, 2.075s/100 iter), loss = 0.0476189
I0704 07:48:21.231631 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:48:21.231637 25348 sgd_solver.cpp:106] Iteration 13300, lr = 0.00792187
I0704 07:48:23.303719 25348 solver.cpp:290] Iteration 13400 (48.2621 iter/s, 2.07202s/100 iter), loss = -1.63913e-07
I0704 07:48:23.303743 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:23.303752 25348 sgd_solver.cpp:106] Iteration 13400, lr = 0.00790625
I0704 07:48:25.380127 25348 solver.cpp:290] Iteration 13500 (48.1621 iter/s, 2.07632s/100 iter), loss = -1.63913e-07
I0704 07:48:25.380149 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:25.380157 25348 sgd_solver.cpp:106] Iteration 13500, lr = 0.00789062
I0704 07:48:27.455042 25348 solver.cpp:290] Iteration 13600 (48.1968 iter/s, 2.07483s/100 iter), loss = -1.63913e-07
I0704 07:48:27.455106 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:27.455117 25348 sgd_solver.cpp:106] Iteration 13600, lr = 0.007875
I0704 07:48:29.530053 25348 solver.cpp:290] Iteration 13700 (48.1954 iter/s, 2.07488s/100 iter), loss = -1.71363e-07
I0704 07:48:29.530076 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:29.530082 25348 sgd_solver.cpp:106] Iteration 13700, lr = 0.00785937
I0704 07:48:31.606386 25348 solver.cpp:290] Iteration 13800 (48.1639 iter/s, 2.07624s/100 iter), loss = -1.78814e-07
I0704 07:48:31.606410 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:31.606416 25348 sgd_solver.cpp:106] Iteration 13800, lr = 0.00784375
I0704 07:48:33.695430 25348 solver.cpp:290] Iteration 13900 (47.8708 iter/s, 2.08895s/100 iter), loss = 0.0476189
I0704 07:48:33.695451 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:48:33.695458 25348 sgd_solver.cpp:106] Iteration 13900, lr = 0.00782812
I0704 07:48:35.751704 25348 solver.cpp:354] Sparsity after update:
I0704 07:48:35.753101 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:48:35.753108 25348 net.cpp:1851] conv1a_param_0(0.0896) 
I0704 07:48:35.753115 25348 net.cpp:1851] conv1b_param_0(0.18) 
I0704 07:48:35.753118 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:48:35.753121 25348 net.cpp:1851] res2a_branch2a_param_0(0.18) 
I0704 07:48:35.753124 25348 net.cpp:1851] res2a_branch2b_param_0(0.18) 
I0704 07:48:35.753126 25348 net.cpp:1851] res3a_branch2a_param_0(0.18) 
I0704 07:48:35.753129 25348 net.cpp:1851] res3a_branch2b_param_0(0.18) 
I0704 07:48:35.753131 25348 net.cpp:1851] res4a_branch2a_param_0(0.18) 
I0704 07:48:35.753134 25348 net.cpp:1851] res4a_branch2b_param_0(0.18) 
I0704 07:48:35.753135 25348 net.cpp:1851] res5a_branch2a_param_0(0.18) 
I0704 07:48:35.753139 25348 net.cpp:1851] res5a_branch2b_param_0(0.18) 
I0704 07:48:35.753141 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (423636/2.3599e+06) 0.18
I0704 07:48:35.753229 25348 solver.cpp:466] Iteration 14000, Testing net (#0)
I0704 07:48:37.396482 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8013
I0704 07:48:37.396502 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9882
I0704 07:48:37.396507 25348 solver.cpp:539]     Test net output #2: loss = 0.4968 (* 1 = 0.4968 loss)
I0704 07:48:37.416868 25348 solver.cpp:290] Iteration 14000 (26.8723 iter/s, 3.72131s/100 iter), loss = -1.86265e-07
I0704 07:48:37.416887 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:37.416900 25348 sgd_solver.cpp:106] Iteration 14000, lr = 0.0078125
I0704 07:48:37.417474 25348 solver.cpp:375] Finding and applying sparsity: 0.2
I0704 07:48:37.707029 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:48:39.803529 25348 solver.cpp:290] Iteration 14100 (41.9011 iter/s, 2.38657s/100 iter), loss = -2.12342e-07
I0704 07:48:39.803552 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:39.803560 25348 sgd_solver.cpp:106] Iteration 14100, lr = 0.00779688
I0704 07:48:41.880117 25348 solver.cpp:290] Iteration 14200 (48.1579 iter/s, 2.0765s/100 iter), loss = -2.30968e-07
I0704 07:48:41.880139 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:41.880146 25348 sgd_solver.cpp:106] Iteration 14200, lr = 0.00778125
I0704 07:48:43.955965 25348 solver.cpp:290] Iteration 14300 (48.1751 iter/s, 2.07576s/100 iter), loss = -2.23517e-07
I0704 07:48:43.955988 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:43.955996 25348 sgd_solver.cpp:106] Iteration 14300, lr = 0.00776563
I0704 07:48:46.037762 25348 solver.cpp:290] Iteration 14400 (48.0375 iter/s, 2.08171s/100 iter), loss = -2.23517e-07
I0704 07:48:46.037786 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:46.037792 25348 sgd_solver.cpp:106] Iteration 14400, lr = 0.00775
I0704 07:48:48.116003 25348 solver.cpp:290] Iteration 14500 (48.1196 iter/s, 2.07815s/100 iter), loss = 0.0476188
I0704 07:48:48.116039 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:48:48.116046 25348 sgd_solver.cpp:106] Iteration 14500, lr = 0.00773437
I0704 07:48:50.188387 25348 solver.cpp:290] Iteration 14600 (48.2559 iter/s, 2.07229s/100 iter), loss = -2.23517e-07
I0704 07:48:50.188410 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:50.188417 25348 sgd_solver.cpp:106] Iteration 14600, lr = 0.00771875
I0704 07:48:52.261140 25348 solver.cpp:290] Iteration 14700 (48.247 iter/s, 2.07267s/100 iter), loss = -2.42144e-07
I0704 07:48:52.261163 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:52.261170 25348 sgd_solver.cpp:106] Iteration 14700, lr = 0.00770312
I0704 07:48:54.335252 25348 solver.cpp:290] Iteration 14800 (48.2155 iter/s, 2.07402s/100 iter), loss = 0.0952379
I0704 07:48:54.335274 25348 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:48:54.335281 25348 sgd_solver.cpp:106] Iteration 14800, lr = 0.0076875
I0704 07:48:56.407382 25348 solver.cpp:290] Iteration 14900 (48.2615 iter/s, 2.07205s/100 iter), loss = 0.0952379
I0704 07:48:56.407404 25348 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:48:56.407411 25348 sgd_solver.cpp:106] Iteration 14900, lr = 0.00767187
I0704 07:48:58.462117 25348 solver.cpp:354] Sparsity after update:
I0704 07:48:58.463522 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:48:58.463531 25348 net.cpp:1851] conv1a_param_0(0.0996) 
I0704 07:48:58.463537 25348 net.cpp:1851] conv1b_param_0(0.2) 
I0704 07:48:58.463541 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:48:58.463543 25348 net.cpp:1851] res2a_branch2a_param_0(0.2) 
I0704 07:48:58.463546 25348 net.cpp:1851] res2a_branch2b_param_0(0.2) 
I0704 07:48:58.463547 25348 net.cpp:1851] res3a_branch2a_param_0(0.2) 
I0704 07:48:58.463551 25348 net.cpp:1851] res3a_branch2b_param_0(0.2) 
I0704 07:48:58.463552 25348 net.cpp:1851] res4a_branch2a_param_0(0.2) 
I0704 07:48:58.463554 25348 net.cpp:1851] res4a_branch2b_param_0(0.2) 
I0704 07:48:58.463557 25348 net.cpp:1851] res5a_branch2a_param_0(0.2) 
I0704 07:48:58.463558 25348 net.cpp:1851] res5a_branch2b_param_0(0.2) 
I0704 07:48:58.463560 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (470709/2.3599e+06) 0.199
I0704 07:48:58.463646 25348 solver.cpp:466] Iteration 15000, Testing net (#0)
I0704 07:49:00.106513 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8296
I0704 07:49:00.106533 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9896
I0704 07:49:00.106537 25348 solver.cpp:539]     Test net output #2: loss = 0.4104 (* 1 = 0.4104 loss)
I0704 07:49:00.127310 25348 solver.cpp:290] Iteration 15000 (26.8832 iter/s, 3.7198s/100 iter), loss = -2.38419e-07
I0704 07:49:00.127328 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:00.127338 25348 sgd_solver.cpp:106] Iteration 15000, lr = 0.00765625
I0704 07:49:00.127878 25348 solver.cpp:375] Finding and applying sparsity: 0.22
I0704 07:49:00.403573 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:49:02.500529 25348 solver.cpp:290] Iteration 15100 (42.1385 iter/s, 2.37313s/100 iter), loss = -2.38419e-07
I0704 07:49:02.500551 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:02.500558 25348 sgd_solver.cpp:106] Iteration 15100, lr = 0.00764062
I0704 07:49:04.571518 25348 solver.cpp:290] Iteration 15200 (48.2881 iter/s, 2.0709s/100 iter), loss = -2.38419e-07
I0704 07:49:04.571542 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:04.571548 25348 sgd_solver.cpp:106] Iteration 15200, lr = 0.007625
I0704 07:49:06.643508 25348 solver.cpp:290] Iteration 15300 (48.2648 iter/s, 2.0719s/100 iter), loss = -2.38419e-07
I0704 07:49:06.643529 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:06.643537 25348 sgd_solver.cpp:106] Iteration 15300, lr = 0.00760937
I0704 07:49:08.715504 25348 solver.cpp:290] Iteration 15400 (48.2646 iter/s, 2.07191s/100 iter), loss = -2.38419e-07
I0704 07:49:08.715525 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:08.715533 25348 sgd_solver.cpp:106] Iteration 15400, lr = 0.00759375
I0704 07:49:10.789386 25348 solver.cpp:290] Iteration 15500 (48.2207 iter/s, 2.0738s/100 iter), loss = -2.38419e-07
I0704 07:49:10.789408 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:10.789415 25348 sgd_solver.cpp:106] Iteration 15500, lr = 0.00757812
I0704 07:49:12.863679 25348 solver.cpp:290] Iteration 15600 (48.2112 iter/s, 2.07421s/100 iter), loss = -2.38419e-07
I0704 07:49:12.863701 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:12.863708 25348 sgd_solver.cpp:106] Iteration 15600, lr = 0.0075625
I0704 07:49:14.935485 25348 solver.cpp:290] Iteration 15700 (48.2691 iter/s, 2.07172s/100 iter), loss = -2.38419e-07
I0704 07:49:14.935506 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:14.935514 25348 sgd_solver.cpp:106] Iteration 15700, lr = 0.00754687
I0704 07:49:17.010493 25348 solver.cpp:290] Iteration 15800 (48.1945 iter/s, 2.07492s/100 iter), loss = 0.0952379
I0704 07:49:17.010514 25348 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:49:17.010521 25348 sgd_solver.cpp:106] Iteration 15800, lr = 0.00753125
I0704 07:49:19.085664 25348 solver.cpp:290] Iteration 15900 (48.1908 iter/s, 2.07509s/100 iter), loss = -2.38419e-07
I0704 07:49:19.085688 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:19.085697 25348 sgd_solver.cpp:106] Iteration 15900, lr = 0.00751562
I0704 07:49:21.138309 25348 solver.cpp:354] Sparsity after update:
I0704 07:49:21.139711 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:49:21.139719 25348 net.cpp:1851] conv1a_param_0(0.11) 
I0704 07:49:21.139727 25348 net.cpp:1851] conv1b_param_0(0.22) 
I0704 07:49:21.139730 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:49:21.139732 25348 net.cpp:1851] res2a_branch2a_param_0(0.22) 
I0704 07:49:21.139735 25348 net.cpp:1851] res2a_branch2b_param_0(0.22) 
I0704 07:49:21.139736 25348 net.cpp:1851] res3a_branch2a_param_0(0.22) 
I0704 07:49:21.139739 25348 net.cpp:1851] res3a_branch2b_param_0(0.22) 
I0704 07:49:21.139740 25348 net.cpp:1851] res4a_branch2a_param_0(0.22) 
I0704 07:49:21.139742 25348 net.cpp:1851] res4a_branch2b_param_0(0.22) 
I0704 07:49:21.139744 25348 net.cpp:1851] res5a_branch2a_param_0(0.22) 
I0704 07:49:21.139746 25348 net.cpp:1851] res5a_branch2b_param_0(0.22) 
I0704 07:49:21.139750 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (517780/2.3599e+06) 0.219
I0704 07:49:21.139835 25348 solver.cpp:466] Iteration 16000, Testing net (#0)
I0704 07:49:22.784217 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8087
I0704 07:49:22.784236 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9865
I0704 07:49:22.784242 25348 solver.cpp:539]     Test net output #2: loss = 0.4633 (* 1 = 0.4633 loss)
I0704 07:49:22.804522 25348 solver.cpp:290] Iteration 16000 (26.8909 iter/s, 3.71873s/100 iter), loss = -2.5332e-07
I0704 07:49:22.804540 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:22.804551 25348 sgd_solver.cpp:106] Iteration 16000, lr = 0.0075
I0704 07:49:22.805122 25348 solver.cpp:375] Finding and applying sparsity: 0.24
I0704 07:49:23.082512 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:49:25.177712 25348 solver.cpp:290] Iteration 16100 (42.139 iter/s, 2.3731s/100 iter), loss = 0.0476188
I0704 07:49:25.177734 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:49:25.177742 25348 sgd_solver.cpp:106] Iteration 16100, lr = 0.00748438
I0704 07:49:27.255579 25348 solver.cpp:290] Iteration 16200 (48.1283 iter/s, 2.07778s/100 iter), loss = 0.0476188
I0704 07:49:27.255604 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:49:27.255614 25348 sgd_solver.cpp:106] Iteration 16200, lr = 0.00746875
I0704 07:49:29.331255 25348 solver.cpp:290] Iteration 16300 (48.1791 iter/s, 2.07559s/100 iter), loss = -2.5332e-07
I0704 07:49:29.331326 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:29.331337 25348 sgd_solver.cpp:106] Iteration 16300, lr = 0.00745312
I0704 07:49:31.404942 25348 solver.cpp:290] Iteration 16400 (48.2264 iter/s, 2.07355s/100 iter), loss = 0.142857
I0704 07:49:31.404963 25348 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:49:31.404969 25348 sgd_solver.cpp:106] Iteration 16400, lr = 0.0074375
I0704 07:49:33.500304 25348 solver.cpp:290] Iteration 16500 (47.7264 iter/s, 2.09528s/100 iter), loss = -2.38419e-07
I0704 07:49:33.500325 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:33.500334 25348 sgd_solver.cpp:106] Iteration 16500, lr = 0.00742187
I0704 07:49:35.581120 25348 solver.cpp:290] Iteration 16600 (48.06 iter/s, 2.08073s/100 iter), loss = -2.5332e-07
I0704 07:49:35.581143 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:35.581151 25348 sgd_solver.cpp:106] Iteration 16600, lr = 0.00740625
I0704 07:49:37.652907 25348 solver.cpp:290] Iteration 16700 (48.2696 iter/s, 2.0717s/100 iter), loss = -2.6077e-07
I0704 07:49:37.652930 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:37.652936 25348 sgd_solver.cpp:106] Iteration 16700, lr = 0.00739062
I0704 07:49:39.729225 25348 solver.cpp:290] Iteration 16800 (48.1642 iter/s, 2.07623s/100 iter), loss = -2.6077e-07
I0704 07:49:39.729246 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:39.729254 25348 sgd_solver.cpp:106] Iteration 16800, lr = 0.007375
I0704 07:49:41.804369 25348 solver.cpp:290] Iteration 16900 (48.1914 iter/s, 2.07506s/100 iter), loss = -2.57045e-07
I0704 07:49:41.804395 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:41.804402 25348 sgd_solver.cpp:106] Iteration 16900, lr = 0.00735937
I0704 07:49:43.855830 25348 solver.cpp:354] Sparsity after update:
I0704 07:49:43.857110 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:49:43.857122 25348 net.cpp:1851] conv1a_param_0(0.12) 
I0704 07:49:43.857131 25348 net.cpp:1851] conv1b_param_0(0.24) 
I0704 07:49:43.857132 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:49:43.857136 25348 net.cpp:1851] res2a_branch2a_param_0(0.24) 
I0704 07:49:43.857138 25348 net.cpp:1851] res2a_branch2b_param_0(0.24) 
I0704 07:49:43.857141 25348 net.cpp:1851] res3a_branch2a_param_0(0.24) 
I0704 07:49:43.857143 25348 net.cpp:1851] res3a_branch2b_param_0(0.24) 
I0704 07:49:43.857146 25348 net.cpp:1851] res4a_branch2a_param_0(0.24) 
I0704 07:49:43.857148 25348 net.cpp:1851] res4a_branch2b_param_0(0.24) 
I0704 07:49:43.857151 25348 net.cpp:1851] res5a_branch2a_param_0(0.24) 
I0704 07:49:43.857152 25348 net.cpp:1851] res5a_branch2b_param_0(0.24) 
I0704 07:49:43.857154 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (564848/2.3599e+06) 0.239
I0704 07:49:43.857246 25348 solver.cpp:466] Iteration 17000, Testing net (#0)
I0704 07:49:45.512115 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8748
I0704 07:49:45.512135 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.995
I0704 07:49:45.512141 25348 solver.cpp:539]     Test net output #2: loss = 0.276 (* 1 = 0.276 loss)
I0704 07:49:45.532071 25348 solver.cpp:290] Iteration 17000 (26.8271 iter/s, 3.72757s/100 iter), loss = -2.5332e-07
I0704 07:49:45.532091 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:45.532100 25348 sgd_solver.cpp:106] Iteration 17000, lr = 0.00734375
I0704 07:49:45.532842 25348 solver.cpp:375] Finding and applying sparsity: 0.26
I0704 07:49:45.841781 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:49:47.930665 25348 solver.cpp:290] Iteration 17100 (41.6927 iter/s, 2.3985s/100 iter), loss = -2.6077e-07
I0704 07:49:47.930688 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:47.930696 25348 sgd_solver.cpp:106] Iteration 17100, lr = 0.00732813
I0704 07:49:50.008671 25348 solver.cpp:290] Iteration 17200 (48.1251 iter/s, 2.07792s/100 iter), loss = 0.142857
I0704 07:49:50.008708 25348 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:49:50.008714 25348 sgd_solver.cpp:106] Iteration 17200, lr = 0.0073125
I0704 07:49:52.078630 25348 solver.cpp:290] Iteration 17300 (48.3124 iter/s, 2.06986s/100 iter), loss = -2.5332e-07
I0704 07:49:52.078655 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:52.078663 25348 sgd_solver.cpp:106] Iteration 17300, lr = 0.00729688
I0704 07:49:54.151522 25348 solver.cpp:290] Iteration 17400 (48.2438 iter/s, 2.07281s/100 iter), loss = -2.5332e-07
I0704 07:49:54.151545 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:54.151551 25348 sgd_solver.cpp:106] Iteration 17400, lr = 0.00728125
I0704 07:49:56.225327 25348 solver.cpp:290] Iteration 17500 (48.2225 iter/s, 2.07372s/100 iter), loss = -2.5332e-07
I0704 07:49:56.225350 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:56.225359 25348 sgd_solver.cpp:106] Iteration 17500, lr = 0.00726563
I0704 07:49:58.296921 25348 solver.cpp:290] Iteration 17600 (48.274 iter/s, 2.07151s/100 iter), loss = -2.5332e-07
I0704 07:49:58.296944 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:58.296952 25348 sgd_solver.cpp:106] Iteration 17600, lr = 0.00725
I0704 07:50:00.367300 25348 solver.cpp:290] Iteration 17700 (48.3024 iter/s, 2.07029s/100 iter), loss = 0.0476188
I0704 07:50:00.367372 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:50:00.367379 25348 sgd_solver.cpp:106] Iteration 17700, lr = 0.00723437
I0704 07:50:02.441882 25348 solver.cpp:290] Iteration 17800 (48.2056 iter/s, 2.07445s/100 iter), loss = -2.5332e-07
I0704 07:50:02.441903 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:02.441910 25348 sgd_solver.cpp:106] Iteration 17800, lr = 0.00721875
I0704 07:50:04.519093 25348 solver.cpp:290] Iteration 17900 (48.1435 iter/s, 2.07712s/100 iter), loss = -2.5332e-07
I0704 07:50:04.519114 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:04.519121 25348 sgd_solver.cpp:106] Iteration 17900, lr = 0.00720312
I0704 07:50:06.569792 25348 solver.cpp:354] Sparsity after update:
I0704 07:50:06.571192 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:50:06.571199 25348 net.cpp:1851] conv1a_param_0(0.13) 
I0704 07:50:06.571208 25348 net.cpp:1851] conv1b_param_0(0.26) 
I0704 07:50:06.571209 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:50:06.571213 25348 net.cpp:1851] res2a_branch2a_param_0(0.26) 
I0704 07:50:06.571214 25348 net.cpp:1851] res2a_branch2b_param_0(0.26) 
I0704 07:50:06.571218 25348 net.cpp:1851] res3a_branch2a_param_0(0.26) 
I0704 07:50:06.571219 25348 net.cpp:1851] res3a_branch2b_param_0(0.26) 
I0704 07:50:06.571223 25348 net.cpp:1851] res4a_branch2a_param_0(0.26) 
I0704 07:50:06.571224 25348 net.cpp:1851] res4a_branch2b_param_0(0.26) 
I0704 07:50:06.571228 25348 net.cpp:1851] res5a_branch2a_param_0(0.26) 
I0704 07:50:06.571229 25348 net.cpp:1851] res5a_branch2b_param_0(0.26) 
I0704 07:50:06.571233 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (611920/2.3599e+06) 0.259
I0704 07:50:06.571326 25348 solver.cpp:466] Iteration 18000, Testing net (#0)
I0704 07:50:08.213593 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8536
I0704 07:50:08.213613 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9882
I0704 07:50:08.213620 25348 solver.cpp:539]     Test net output #2: loss = 0.3859 (* 1 = 0.3859 loss)
I0704 07:50:08.233319 25348 solver.cpp:290] Iteration 18000 (26.9244 iter/s, 3.7141s/100 iter), loss = 0.0476188
I0704 07:50:08.233337 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:50:08.233348 25348 sgd_solver.cpp:106] Iteration 18000, lr = 0.0071875
I0704 07:50:08.233906 25348 solver.cpp:375] Finding and applying sparsity: 0.28
I0704 07:50:08.577478 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:50:10.678467 25348 solver.cpp:290] Iteration 18100 (40.8989 iter/s, 2.44506s/100 iter), loss = -2.6077e-07
I0704 07:50:10.678493 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:10.678500 25348 sgd_solver.cpp:106] Iteration 18100, lr = 0.00717187
I0704 07:50:12.757352 25348 solver.cpp:290] Iteration 18200 (48.1047 iter/s, 2.0788s/100 iter), loss = -2.5332e-07
I0704 07:50:12.757375 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:12.757381 25348 sgd_solver.cpp:106] Iteration 18200, lr = 0.00715625
I0704 07:50:14.830209 25348 solver.cpp:290] Iteration 18300 (48.2446 iter/s, 2.07277s/100 iter), loss = -2.5332e-07
I0704 07:50:14.830234 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:14.830242 25348 sgd_solver.cpp:106] Iteration 18300, lr = 0.00714062
I0704 07:50:16.902987 25348 solver.cpp:290] Iteration 18400 (48.2465 iter/s, 2.07269s/100 iter), loss = -2.5332e-07
I0704 07:50:16.903010 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:16.903018 25348 sgd_solver.cpp:106] Iteration 18400, lr = 0.007125
I0704 07:50:18.978760 25348 solver.cpp:290] Iteration 18500 (48.1768 iter/s, 2.07569s/100 iter), loss = -2.5332e-07
I0704 07:50:18.978785 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:18.978793 25348 sgd_solver.cpp:106] Iteration 18500, lr = 0.00710937
I0704 07:50:21.050155 25348 solver.cpp:290] Iteration 18600 (48.2787 iter/s, 2.07131s/100 iter), loss = -2.5332e-07
I0704 07:50:21.050194 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:21.050200 25348 sgd_solver.cpp:106] Iteration 18600, lr = 0.00709375
I0704 07:50:23.125754 25348 solver.cpp:290] Iteration 18700 (48.1812 iter/s, 2.0755s/100 iter), loss = -2.5332e-07
I0704 07:50:23.125778 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:23.125787 25348 sgd_solver.cpp:106] Iteration 18700, lr = 0.00707812
I0704 07:50:25.200147 25348 solver.cpp:290] Iteration 18800 (48.2089 iter/s, 2.07431s/100 iter), loss = -2.5332e-07
I0704 07:50:25.200170 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:25.200177 25348 sgd_solver.cpp:106] Iteration 18800, lr = 0.0070625
I0704 07:50:27.274163 25348 solver.cpp:290] Iteration 18900 (48.2177 iter/s, 2.07393s/100 iter), loss = -2.5332e-07
I0704 07:50:27.274186 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:27.274194 25348 sgd_solver.cpp:106] Iteration 18900, lr = 0.00704687
I0704 07:50:29.331185 25348 solver.cpp:354] Sparsity after update:
I0704 07:50:29.332568 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:50:29.332576 25348 net.cpp:1851] conv1a_param_0(0.14) 
I0704 07:50:29.332583 25348 net.cpp:1851] conv1b_param_0(0.28) 
I0704 07:50:29.332588 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:50:29.332592 25348 net.cpp:1851] res2a_branch2a_param_0(0.28) 
I0704 07:50:29.332597 25348 net.cpp:1851] res2a_branch2b_param_0(0.28) 
I0704 07:50:29.332602 25348 net.cpp:1851] res3a_branch2a_param_0(0.28) 
I0704 07:50:29.332605 25348 net.cpp:1851] res3a_branch2b_param_0(0.28) 
I0704 07:50:29.332609 25348 net.cpp:1851] res4a_branch2a_param_0(0.28) 
I0704 07:50:29.332612 25348 net.cpp:1851] res4a_branch2b_param_0(0.28) 
I0704 07:50:29.332617 25348 net.cpp:1851] res5a_branch2a_param_0(0.28) 
I0704 07:50:29.332620 25348 net.cpp:1851] res5a_branch2b_param_0(0.28) 
I0704 07:50:29.332624 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (658995/2.3599e+06) 0.279
I0704 07:50:29.332761 25348 solver.cpp:466] Iteration 19000, Testing net (#0)
I0704 07:50:30.980499 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.7929
I0704 07:50:30.980593 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9901
I0704 07:50:30.980600 25348 solver.cpp:539]     Test net output #2: loss = 0.5299 (* 1 = 0.5299 loss)
I0704 07:50:31.000399 25348 solver.cpp:290] Iteration 19000 (26.8377 iter/s, 3.72611s/100 iter), loss = -2.5332e-07
I0704 07:50:31.000416 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:31.000429 25348 sgd_solver.cpp:106] Iteration 19000, lr = 0.00703125
I0704 07:50:31.000924 25348 solver.cpp:375] Finding and applying sparsity: 0.3
I0704 07:50:31.357607 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:50:33.476666 25348 solver.cpp:290] Iteration 19100 (40.3849 iter/s, 2.47617s/100 iter), loss = 0.0476188
I0704 07:50:33.476687 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:50:33.476694 25348 sgd_solver.cpp:106] Iteration 19100, lr = 0.00701563
I0704 07:50:35.556211 25348 solver.cpp:290] Iteration 19200 (48.0894 iter/s, 2.07946s/100 iter), loss = -2.5332e-07
I0704 07:50:35.556233 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:35.556241 25348 sgd_solver.cpp:106] Iteration 19200, lr = 0.007
I0704 07:50:37.626026 25348 solver.cpp:290] Iteration 19300 (48.3155 iter/s, 2.06973s/100 iter), loss = -2.5332e-07
I0704 07:50:37.626049 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:37.626055 25348 sgd_solver.cpp:106] Iteration 19300, lr = 0.00698437
I0704 07:50:39.697494 25348 solver.cpp:290] Iteration 19400 (48.277 iter/s, 2.07138s/100 iter), loss = 0.0476188
I0704 07:50:39.697515 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:50:39.697522 25348 sgd_solver.cpp:106] Iteration 19400, lr = 0.00696875
I0704 07:50:41.768862 25348 solver.cpp:290] Iteration 19500 (48.2793 iter/s, 2.07128s/100 iter), loss = -2.5332e-07
I0704 07:50:41.768885 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:41.768893 25348 sgd_solver.cpp:106] Iteration 19500, lr = 0.00695312
I0704 07:50:43.838079 25348 solver.cpp:290] Iteration 19600 (48.3295 iter/s, 2.06913s/100 iter), loss = -2.5332e-07
I0704 07:50:43.838102 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:43.838109 25348 sgd_solver.cpp:106] Iteration 19600, lr = 0.0069375
I0704 07:50:45.907618 25348 solver.cpp:290] Iteration 19700 (48.322 iter/s, 2.06945s/100 iter), loss = -2.5332e-07
I0704 07:50:45.907640 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:45.907646 25348 sgd_solver.cpp:106] Iteration 19700, lr = 0.00692187
I0704 07:50:47.977444 25348 solver.cpp:290] Iteration 19800 (48.3153 iter/s, 2.06974s/100 iter), loss = -2.5332e-07
I0704 07:50:47.977474 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:47.977484 25348 sgd_solver.cpp:106] Iteration 19800, lr = 0.00690625
I0704 07:50:50.058801 25348 solver.cpp:290] Iteration 19900 (48.0477 iter/s, 2.08126s/100 iter), loss = -2.5332e-07
I0704 07:50:50.058828 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:50.058837 25348 sgd_solver.cpp:106] Iteration 19900, lr = 0.00689062
I0704 07:50:52.108675 25348 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_20000.caffemodel
I0704 07:50:52.125344 25348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_20000.solverstate
I0704 07:50:52.133010 25348 solver.cpp:354] Sparsity after update:
I0704 07:50:52.133965 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:50:52.133975 25348 net.cpp:1851] conv1a_param_0(0.15) 
I0704 07:50:52.133985 25348 net.cpp:1851] conv1b_param_0(0.3) 
I0704 07:50:52.133991 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:50:52.133996 25348 net.cpp:1851] res2a_branch2a_param_0(0.3) 
I0704 07:50:52.134001 25348 net.cpp:1851] res2a_branch2b_param_0(0.3) 
I0704 07:50:52.134014 25348 net.cpp:1851] res3a_branch2a_param_0(0.3) 
I0704 07:50:52.134018 25348 net.cpp:1851] res3a_branch2b_param_0(0.3) 
I0704 07:50:52.134022 25348 net.cpp:1851] res4a_branch2a_param_0(0.3) 
I0704 07:50:52.134027 25348 net.cpp:1851] res4a_branch2b_param_0(0.3) 
I0704 07:50:52.134030 25348 net.cpp:1851] res5a_branch2a_param_0(0.3) 
I0704 07:50:52.134034 25348 net.cpp:1851] res5a_branch2b_param_0(0.3) 
I0704 07:50:52.134038 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (706069/2.3599e+06) 0.299
I0704 07:50:52.134140 25348 solver.cpp:466] Iteration 20000, Testing net (#0)
I0704 07:50:53.790336 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8362
I0704 07:50:53.790355 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9914
I0704 07:50:53.790362 25348 solver.cpp:539]     Test net output #2: loss = 0.4035 (* 1 = 0.4035 loss)
I0704 07:50:53.810799 25348 solver.cpp:290] Iteration 20000 (26.6534 iter/s, 3.75186s/100 iter), loss = -2.5332e-07
I0704 07:50:53.810818 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:53.810830 25348 sgd_solver.cpp:106] Iteration 20000, lr = 0.006875
I0704 07:50:53.811416 25348 solver.cpp:375] Finding and applying sparsity: 0.32
I0704 07:50:54.174671 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:50:56.270416 25348 solver.cpp:290] Iteration 20100 (40.6583 iter/s, 2.45953s/100 iter), loss = -2.5332e-07
I0704 07:50:56.270439 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:56.270445 25348 sgd_solver.cpp:106] Iteration 20100, lr = 0.00685938
I0704 07:50:58.343616 25348 solver.cpp:290] Iteration 20200 (48.2366 iter/s, 2.07311s/100 iter), loss = -2.5332e-07
I0704 07:50:58.343641 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:58.343647 25348 sgd_solver.cpp:106] Iteration 20200, lr = 0.00684375
I0704 07:51:00.421456 25348 solver.cpp:290] Iteration 20300 (48.1289 iter/s, 2.07775s/100 iter), loss = -2.5332e-07
I0704 07:51:00.421479 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:00.421486 25348 sgd_solver.cpp:106] Iteration 20300, lr = 0.00682813
I0704 07:51:02.500118 25348 solver.cpp:290] Iteration 20400 (48.1099 iter/s, 2.07857s/100 iter), loss = -2.5332e-07
I0704 07:51:02.500191 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:02.500200 25348 sgd_solver.cpp:106] Iteration 20400, lr = 0.0068125
I0704 07:51:04.571827 25348 solver.cpp:290] Iteration 20500 (48.2725 iter/s, 2.07157s/100 iter), loss = 0.0952378
I0704 07:51:04.571849 25348 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:51:04.571856 25348 sgd_solver.cpp:106] Iteration 20500, lr = 0.00679688
I0704 07:51:06.647766 25348 solver.cpp:290] Iteration 20600 (48.173 iter/s, 2.07585s/100 iter), loss = -2.5332e-07
I0704 07:51:06.647789 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:06.647795 25348 sgd_solver.cpp:106] Iteration 20600, lr = 0.00678125
I0704 07:51:08.720703 25348 solver.cpp:290] Iteration 20700 (48.2427 iter/s, 2.07285s/100 iter), loss = -2.5332e-07
I0704 07:51:08.720724 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:08.720731 25348 sgd_solver.cpp:106] Iteration 20700, lr = 0.00676562
I0704 07:51:10.795603 25348 solver.cpp:290] Iteration 20800 (48.1971 iter/s, 2.07481s/100 iter), loss = -2.5332e-07
I0704 07:51:10.795625 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:10.795631 25348 sgd_solver.cpp:106] Iteration 20800, lr = 0.00675
I0704 07:51:12.870414 25348 solver.cpp:290] Iteration 20900 (48.1992 iter/s, 2.07472s/100 iter), loss = -2.5332e-07
I0704 07:51:12.870435 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:12.870442 25348 sgd_solver.cpp:106] Iteration 20900, lr = 0.00673437
I0704 07:51:14.923179 25348 solver.cpp:354] Sparsity after update:
I0704 07:51:14.924594 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:51:14.924602 25348 net.cpp:1851] conv1a_param_0(0.16) 
I0704 07:51:14.924612 25348 net.cpp:1851] conv1b_param_0(0.32) 
I0704 07:51:14.924616 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:51:14.924621 25348 net.cpp:1851] res2a_branch2a_param_0(0.32) 
I0704 07:51:14.924625 25348 net.cpp:1851] res2a_branch2b_param_0(0.32) 
I0704 07:51:14.924629 25348 net.cpp:1851] res3a_branch2a_param_0(0.32) 
I0704 07:51:14.924633 25348 net.cpp:1851] res3a_branch2b_param_0(0.32) 
I0704 07:51:14.924638 25348 net.cpp:1851] res4a_branch2a_param_0(0.32) 
I0704 07:51:14.924641 25348 net.cpp:1851] res4a_branch2b_param_0(0.32) 
I0704 07:51:14.924645 25348 net.cpp:1851] res5a_branch2a_param_0(0.32) 
I0704 07:51:14.924649 25348 net.cpp:1851] res5a_branch2b_param_0(0.32) 
I0704 07:51:14.924654 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (753141/2.3599e+06) 0.319
I0704 07:51:14.924746 25348 solver.cpp:466] Iteration 21000, Testing net (#0)
I0704 07:51:16.568506 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8713
I0704 07:51:16.568526 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9932
I0704 07:51:16.568533 25348 solver.cpp:539]     Test net output #2: loss = 0.2795 (* 1 = 0.2795 loss)
I0704 07:51:16.589910 25348 solver.cpp:290] Iteration 21000 (26.8863 iter/s, 3.71937s/100 iter), loss = -2.6077e-07
I0704 07:51:16.589929 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:16.589939 25348 sgd_solver.cpp:106] Iteration 21000, lr = 0.00671875
I0704 07:51:16.590468 25348 solver.cpp:375] Finding and applying sparsity: 0.34
I0704 07:51:17.014266 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:51:19.103078 25348 solver.cpp:290] Iteration 21100 (39.7919 iter/s, 2.51308s/100 iter), loss = -2.6077e-07
I0704 07:51:19.103099 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:19.103107 25348 sgd_solver.cpp:106] Iteration 21100, lr = 0.00670313
I0704 07:51:21.175451 25348 solver.cpp:290] Iteration 21200 (48.2559 iter/s, 2.07229s/100 iter), loss = -2.6077e-07
I0704 07:51:21.175473 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:21.175480 25348 sgd_solver.cpp:106] Iteration 21200, lr = 0.0066875
I0704 07:51:23.252328 25348 solver.cpp:290] Iteration 21300 (48.1512 iter/s, 2.07679s/100 iter), loss = -2.6077e-07
I0704 07:51:23.252367 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:23.252374 25348 sgd_solver.cpp:106] Iteration 21300, lr = 0.00667187
I0704 07:51:25.322289 25348 solver.cpp:290] Iteration 21400 (48.3125 iter/s, 2.06986s/100 iter), loss = -2.5332e-07
I0704 07:51:25.322311 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:25.322319 25348 sgd_solver.cpp:106] Iteration 21400, lr = 0.00665625
I0704 07:51:27.394964 25348 solver.cpp:290] Iteration 21500 (48.2488 iter/s, 2.07259s/100 iter), loss = -2.5332e-07
I0704 07:51:27.394987 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:27.394994 25348 sgd_solver.cpp:106] Iteration 21500, lr = 0.00664062
I0704 07:51:29.467912 25348 solver.cpp:290] Iteration 21600 (48.2425 iter/s, 2.07286s/100 iter), loss = -2.5332e-07
I0704 07:51:29.467936 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:29.467941 25348 sgd_solver.cpp:106] Iteration 21600, lr = 0.006625
I0704 07:51:31.544471 25348 solver.cpp:290] Iteration 21700 (48.1586 iter/s, 2.07647s/100 iter), loss = -2.5332e-07
I0704 07:51:31.544492 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:31.544499 25348 sgd_solver.cpp:106] Iteration 21700, lr = 0.00660937
I0704 07:51:33.644775 25348 solver.cpp:290] Iteration 21800 (47.6141 iter/s, 2.10022s/100 iter), loss = -2.5332e-07
I0704 07:51:33.644847 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:33.644858 25348 sgd_solver.cpp:106] Iteration 21800, lr = 0.00659375
I0704 07:51:35.719146 25348 solver.cpp:290] Iteration 21900 (48.2105 iter/s, 2.07424s/100 iter), loss = -2.5332e-07
I0704 07:51:35.719167 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:35.719174 25348 sgd_solver.cpp:106] Iteration 21900, lr = 0.00657812
I0704 07:51:37.773211 25348 solver.cpp:354] Sparsity after update:
I0704 07:51:37.774622 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:51:37.774631 25348 net.cpp:1851] conv1a_param_0(0.17) 
I0704 07:51:37.774641 25348 net.cpp:1851] conv1b_param_0(0.34) 
I0704 07:51:37.774646 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:51:37.774649 25348 net.cpp:1851] res2a_branch2a_param_0(0.34) 
I0704 07:51:37.774654 25348 net.cpp:1851] res2a_branch2b_param_0(0.34) 
I0704 07:51:37.774658 25348 net.cpp:1851] res3a_branch2a_param_0(0.34) 
I0704 07:51:37.774662 25348 net.cpp:1851] res3a_branch2b_param_0(0.34) 
I0704 07:51:37.774667 25348 net.cpp:1851] res4a_branch2a_param_0(0.34) 
I0704 07:51:37.774672 25348 net.cpp:1851] res4a_branch2b_param_0(0.34) 
I0704 07:51:37.774677 25348 net.cpp:1851] res5a_branch2a_param_0(0.34) 
I0704 07:51:37.774680 25348 net.cpp:1851] res5a_branch2b_param_0(0.34) 
I0704 07:51:37.774685 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (800212/2.3599e+06) 0.339
I0704 07:51:37.774780 25348 solver.cpp:466] Iteration 22000, Testing net (#0)
I0704 07:51:39.432461 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8707
I0704 07:51:39.432482 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9875
I0704 07:51:39.432487 25348 solver.cpp:539]     Test net output #2: loss = 0.2971 (* 1 = 0.2971 loss)
I0704 07:51:39.452585 25348 solver.cpp:290] Iteration 22000 (26.7859 iter/s, 3.73331s/100 iter), loss = -2.5332e-07
I0704 07:51:39.452603 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:39.452616 25348 sgd_solver.cpp:106] Iteration 22000, lr = 0.0065625
I0704 07:51:39.453168 25348 solver.cpp:375] Finding and applying sparsity: 0.36
I0704 07:51:39.901208 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:51:41.995666 25348 solver.cpp:290] Iteration 22100 (39.3239 iter/s, 2.54299s/100 iter), loss = -2.5332e-07
I0704 07:51:41.995687 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:41.995693 25348 sgd_solver.cpp:106] Iteration 22100, lr = 0.00654687
I0704 07:51:44.068675 25348 solver.cpp:290] Iteration 22200 (48.2411 iter/s, 2.07292s/100 iter), loss = -2.5332e-07
I0704 07:51:44.068697 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:44.068706 25348 sgd_solver.cpp:106] Iteration 22200, lr = 0.00653125
I0704 07:51:46.147434 25348 solver.cpp:290] Iteration 22300 (48.1076 iter/s, 2.07867s/100 iter), loss = -2.5332e-07
I0704 07:51:46.147457 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:46.147465 25348 sgd_solver.cpp:106] Iteration 22300, lr = 0.00651562
I0704 07:51:48.219835 25348 solver.cpp:290] Iteration 22400 (48.2553 iter/s, 2.07231s/100 iter), loss = -2.5332e-07
I0704 07:51:48.219857 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:48.219866 25348 sgd_solver.cpp:106] Iteration 22400, lr = 0.0065
I0704 07:51:50.291261 25348 solver.cpp:290] Iteration 22500 (48.2779 iter/s, 2.07134s/100 iter), loss = -2.5332e-07
I0704 07:51:50.291285 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:50.291290 25348 sgd_solver.cpp:106] Iteration 22500, lr = 0.00648437
I0704 07:51:52.362565 25348 solver.cpp:290] Iteration 22600 (48.2808 iter/s, 2.07122s/100 iter), loss = -2.5332e-07
I0704 07:51:52.362588 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:52.362594 25348 sgd_solver.cpp:106] Iteration 22600, lr = 0.00646875
I0704 07:51:54.435712 25348 solver.cpp:290] Iteration 22700 (48.2378 iter/s, 2.07306s/100 iter), loss = 0.0476188
I0704 07:51:54.435752 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:51:54.435760 25348 sgd_solver.cpp:106] Iteration 22700, lr = 0.00645312
I0704 07:51:56.505913 25348 solver.cpp:290] Iteration 22800 (48.3069 iter/s, 2.0701s/100 iter), loss = -2.5332e-07
I0704 07:51:56.505935 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:56.505941 25348 sgd_solver.cpp:106] Iteration 22800, lr = 0.0064375
I0704 07:51:58.582919 25348 solver.cpp:290] Iteration 22900 (48.1482 iter/s, 2.07692s/100 iter), loss = -2.5332e-07
I0704 07:51:58.582942 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:58.582948 25348 sgd_solver.cpp:106] Iteration 22900, lr = 0.00642187
I0704 07:52:00.641165 25348 solver.cpp:354] Sparsity after update:
I0704 07:52:00.642571 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:52:00.642580 25348 net.cpp:1851] conv1a_param_0(0.18) 
I0704 07:52:00.642588 25348 net.cpp:1851] conv1b_param_0(0.36) 
I0704 07:52:00.642590 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:52:00.642593 25348 net.cpp:1851] res2a_branch2a_param_0(0.36) 
I0704 07:52:00.642596 25348 net.cpp:1851] res2a_branch2b_param_0(0.36) 
I0704 07:52:00.642597 25348 net.cpp:1851] res3a_branch2a_param_0(0.36) 
I0704 07:52:00.642599 25348 net.cpp:1851] res3a_branch2b_param_0(0.36) 
I0704 07:52:00.642602 25348 net.cpp:1851] res4a_branch2a_param_0(0.36) 
I0704 07:52:00.642604 25348 net.cpp:1851] res4a_branch2b_param_0(0.36) 
I0704 07:52:00.642606 25348 net.cpp:1851] res5a_branch2a_param_0(0.36) 
I0704 07:52:00.642608 25348 net.cpp:1851] res5a_branch2b_param_0(0.36) 
I0704 07:52:00.642611 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (847286/2.3599e+06) 0.359
I0704 07:52:00.642741 25348 solver.cpp:466] Iteration 23000, Testing net (#0)
I0704 07:52:02.284694 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8621
I0704 07:52:02.284713 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9948
I0704 07:52:02.284718 25348 solver.cpp:539]     Test net output #2: loss = 0.2992 (* 1 = 0.2992 loss)
I0704 07:52:02.304344 25348 solver.cpp:290] Iteration 23000 (26.8724 iter/s, 3.7213s/100 iter), loss = -2.6077e-07
I0704 07:52:02.304361 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:02.304376 25348 sgd_solver.cpp:106] Iteration 23000, lr = 0.00640625
I0704 07:52:02.304910 25348 solver.cpp:375] Finding and applying sparsity: 0.38
I0704 07:52:02.802547 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:52:04.901708 25348 solver.cpp:290] Iteration 23100 (38.502 iter/s, 2.59727s/100 iter), loss = -2.5332e-07
I0704 07:52:04.901783 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:04.901792 25348 sgd_solver.cpp:106] Iteration 23100, lr = 0.00639063
I0704 07:52:06.974179 25348 solver.cpp:290] Iteration 23200 (48.2548 iter/s, 2.07233s/100 iter), loss = -2.5332e-07
I0704 07:52:06.974203 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:06.974210 25348 sgd_solver.cpp:106] Iteration 23200, lr = 0.006375
I0704 07:52:09.048966 25348 solver.cpp:290] Iteration 23300 (48.1998 iter/s, 2.0747s/100 iter), loss = -2.5332e-07
I0704 07:52:09.048996 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:09.049005 25348 sgd_solver.cpp:106] Iteration 23300, lr = 0.00635938
I0704 07:52:11.121562 25348 solver.cpp:290] Iteration 23400 (48.2508 iter/s, 2.0725s/100 iter), loss = -2.5332e-07
I0704 07:52:11.121584 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:11.121593 25348 sgd_solver.cpp:106] Iteration 23400, lr = 0.00634375
I0704 07:52:13.193598 25348 solver.cpp:290] Iteration 23500 (48.2637 iter/s, 2.07195s/100 iter), loss = -2.5332e-07
I0704 07:52:13.193620 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:13.193629 25348 sgd_solver.cpp:106] Iteration 23500, lr = 0.00632813
I0704 07:52:15.271147 25348 solver.cpp:290] Iteration 23600 (48.1356 iter/s, 2.07746s/100 iter), loss = -2.5332e-07
I0704 07:52:15.271173 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:15.271180 25348 sgd_solver.cpp:106] Iteration 23600, lr = 0.0063125
I0704 07:52:17.345437 25348 solver.cpp:290] Iteration 23700 (48.2113 iter/s, 2.0742s/100 iter), loss = -2.5332e-07
I0704 07:52:17.345458 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:17.345465 25348 sgd_solver.cpp:106] Iteration 23700, lr = 0.00629687
I0704 07:52:19.423104 25348 solver.cpp:290] Iteration 23800 (48.1329 iter/s, 2.07758s/100 iter), loss = -2.5332e-07
I0704 07:52:19.423125 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:19.423132 25348 sgd_solver.cpp:106] Iteration 23800, lr = 0.00628125
I0704 07:52:21.494153 25348 solver.cpp:290] Iteration 23900 (48.2867 iter/s, 2.07096s/100 iter), loss = -2.5332e-07
I0704 07:52:21.494174 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:21.494181 25348 sgd_solver.cpp:106] Iteration 23900, lr = 0.00626562
I0704 07:52:23.546813 25348 solver.cpp:354] Sparsity after update:
I0704 07:52:23.548399 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:52:23.548408 25348 net.cpp:1851] conv1a_param_0(0.19) 
I0704 07:52:23.548415 25348 net.cpp:1851] conv1b_param_0(0.38) 
I0704 07:52:23.548418 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:52:23.548420 25348 net.cpp:1851] res2a_branch2a_param_0(0.38) 
I0704 07:52:23.548422 25348 net.cpp:1851] res2a_branch2b_param_0(0.38) 
I0704 07:52:23.548424 25348 net.cpp:1851] res3a_branch2a_param_0(0.38) 
I0704 07:52:23.548426 25348 net.cpp:1851] res3a_branch2b_param_0(0.38) 
I0704 07:52:23.548429 25348 net.cpp:1851] res4a_branch2a_param_0(0.38) 
I0704 07:52:23.548430 25348 net.cpp:1851] res4a_branch2b_param_0(0.38) 
I0704 07:52:23.548432 25348 net.cpp:1851] res5a_branch2a_param_0(0.38) 
I0704 07:52:23.548434 25348 net.cpp:1851] res5a_branch2b_param_0(0.38) 
I0704 07:52:23.548436 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (894352/2.3599e+06) 0.379
I0704 07:52:23.548537 25348 solver.cpp:466] Iteration 24000, Testing net (#0)
I0704 07:52:25.192459 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8765
I0704 07:52:25.192479 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9947
I0704 07:52:25.192484 25348 solver.cpp:539]     Test net output #2: loss = 0.2554 (* 1 = 0.2554 loss)
I0704 07:52:25.213939 25348 solver.cpp:290] Iteration 24000 (26.8842 iter/s, 3.71966s/100 iter), loss = -2.5332e-07
I0704 07:52:25.213958 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:25.213971 25348 sgd_solver.cpp:106] Iteration 24000, lr = 0.00625
I0704 07:52:25.214490 25348 solver.cpp:375] Finding and applying sparsity: 0.4
I0704 07:52:25.721491 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:52:27.807413 25348 solver.cpp:290] Iteration 24100 (38.5597 iter/s, 2.59338s/100 iter), loss = -2.5332e-07
I0704 07:52:27.807435 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:27.807445 25348 sgd_solver.cpp:106] Iteration 24100, lr = 0.00623438
I0704 07:52:29.879256 25348 solver.cpp:290] Iteration 24200 (48.2682 iter/s, 2.07176s/100 iter), loss = -2.5332e-07
I0704 07:52:29.879279 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:29.879286 25348 sgd_solver.cpp:106] Iteration 24200, lr = 0.00621875
I0704 07:52:31.954195 25348 solver.cpp:290] Iteration 24300 (48.1962 iter/s, 2.07485s/100 iter), loss = -2.5332e-07
I0704 07:52:31.954217 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:31.954224 25348 sgd_solver.cpp:106] Iteration 24300, lr = 0.00620312
I0704 07:52:34.032842 25348 solver.cpp:290] Iteration 24400 (48.1102 iter/s, 2.07856s/100 iter), loss = -2.5332e-07
I0704 07:52:34.032866 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:34.032874 25348 sgd_solver.cpp:106] Iteration 24400, lr = 0.0061875
I0704 07:52:36.103787 25348 solver.cpp:290] Iteration 24500 (48.2891 iter/s, 2.07086s/100 iter), loss = -2.5332e-07
I0704 07:52:36.103838 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:36.103847 25348 sgd_solver.cpp:106] Iteration 24500, lr = 0.00617187
I0704 07:52:38.177705 25348 solver.cpp:290] Iteration 24600 (48.2206 iter/s, 2.0738s/100 iter), loss = -2.5332e-07
I0704 07:52:38.177726 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:38.177733 25348 sgd_solver.cpp:106] Iteration 24600, lr = 0.00615625
I0704 07:52:40.248935 25348 solver.cpp:290] Iteration 24700 (48.2825 iter/s, 2.07115s/100 iter), loss = -2.5332e-07
I0704 07:52:40.248956 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:40.248963 25348 sgd_solver.cpp:106] Iteration 24700, lr = 0.00614062
I0704 07:52:42.326685 25348 solver.cpp:290] Iteration 24800 (48.1309 iter/s, 2.07767s/100 iter), loss = -2.5332e-07
I0704 07:52:42.326707 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:42.326714 25348 sgd_solver.cpp:106] Iteration 24800, lr = 0.006125
I0704 07:52:44.399396 25348 solver.cpp:290] Iteration 24900 (48.2479 iter/s, 2.07263s/100 iter), loss = -2.5332e-07
I0704 07:52:44.399420 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:44.399425 25348 sgd_solver.cpp:106] Iteration 24900, lr = 0.00610937
I0704 07:52:46.454818 25348 solver.cpp:354] Sparsity after update:
I0704 07:52:46.456205 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:52:46.456213 25348 net.cpp:1851] conv1a_param_0(0.2) 
I0704 07:52:46.456220 25348 net.cpp:1851] conv1b_param_0(0.4) 
I0704 07:52:46.456223 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:52:46.456224 25348 net.cpp:1851] res2a_branch2a_param_0(0.4) 
I0704 07:52:46.456226 25348 net.cpp:1851] res2a_branch2b_param_0(0.4) 
I0704 07:52:46.456228 25348 net.cpp:1851] res3a_branch2a_param_0(0.4) 
I0704 07:52:46.456230 25348 net.cpp:1851] res3a_branch2b_param_0(0.4) 
I0704 07:52:46.456233 25348 net.cpp:1851] res4a_branch2a_param_0(0.4) 
I0704 07:52:46.456234 25348 net.cpp:1851] res4a_branch2b_param_0(0.4) 
I0704 07:52:46.456236 25348 net.cpp:1851] res5a_branch2a_param_0(0.4) 
I0704 07:52:46.456238 25348 net.cpp:1851] res5a_branch2b_param_0(0.4) 
I0704 07:52:46.456240 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (941425/2.3599e+06) 0.399
I0704 07:52:46.456368 25348 solver.cpp:466] Iteration 25000, Testing net (#0)
I0704 07:52:48.101647 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8679
I0704 07:52:48.101666 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9886
I0704 07:52:48.101671 25348 solver.cpp:539]     Test net output #2: loss = 0.2986 (* 1 = 0.2986 loss)
I0704 07:52:48.122709 25348 solver.cpp:290] Iteration 25000 (26.8587 iter/s, 3.72318s/100 iter), loss = -2.5332e-07
I0704 07:52:48.122725 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:48.122735 25348 sgd_solver.cpp:106] Iteration 25000, lr = 0.00609375
I0704 07:52:48.123291 25348 solver.cpp:375] Finding and applying sparsity: 0.42
I0704 07:52:48.712899 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:52:50.812081 25348 solver.cpp:290] Iteration 25100 (37.1847 iter/s, 2.68927s/100 iter), loss = -2.5332e-07
I0704 07:52:50.812103 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:50.812111 25348 sgd_solver.cpp:106] Iteration 25100, lr = 0.00607812
I0704 07:52:52.884665 25348 solver.cpp:290] Iteration 25200 (48.251 iter/s, 2.0725s/100 iter), loss = -2.5332e-07
I0704 07:52:52.884687 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:52.884696 25348 sgd_solver.cpp:106] Iteration 25200, lr = 0.0060625
I0704 07:52:54.956020 25348 solver.cpp:290] Iteration 25300 (48.2795 iter/s, 2.07127s/100 iter), loss = -2.5332e-07
I0704 07:52:54.956043 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:54.956049 25348 sgd_solver.cpp:106] Iteration 25300, lr = 0.00604687
I0704 07:52:57.027595 25348 solver.cpp:290] Iteration 25400 (48.2745 iter/s, 2.07149s/100 iter), loss = -2.5332e-07
I0704 07:52:57.027631 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:57.027637 25348 sgd_solver.cpp:106] Iteration 25400, lr = 0.00603125
I0704 07:52:59.104912 25348 solver.cpp:290] Iteration 25500 (48.1413 iter/s, 2.07722s/100 iter), loss = -2.5332e-07
I0704 07:52:59.104935 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:59.104943 25348 sgd_solver.cpp:106] Iteration 25500, lr = 0.00601562
I0704 07:53:01.186434 25348 solver.cpp:290] Iteration 25600 (48.0438 iter/s, 2.08143s/100 iter), loss = -2.5332e-07
I0704 07:53:01.186455 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:01.186462 25348 sgd_solver.cpp:106] Iteration 25600, lr = 0.006
I0704 07:53:03.258388 25348 solver.cpp:290] Iteration 25700 (48.2657 iter/s, 2.07186s/100 iter), loss = -2.5332e-07
I0704 07:53:03.258409 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:03.258415 25348 sgd_solver.cpp:106] Iteration 25700, lr = 0.00598437
I0704 07:53:05.330869 25348 solver.cpp:290] Iteration 25800 (48.2533 iter/s, 2.0724s/100 iter), loss = -2.5332e-07
I0704 07:53:05.330893 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:05.330899 25348 sgd_solver.cpp:106] Iteration 25800, lr = 0.00596875
I0704 07:53:07.406355 25348 solver.cpp:290] Iteration 25900 (48.1835 iter/s, 2.0754s/100 iter), loss = -2.5332e-07
I0704 07:53:07.406422 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:07.406430 25348 sgd_solver.cpp:106] Iteration 25900, lr = 0.00595312
I0704 07:53:09.457937 25348 solver.cpp:354] Sparsity after update:
I0704 07:53:09.459336 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:53:09.459342 25348 net.cpp:1851] conv1a_param_0(0.21) 
I0704 07:53:09.459350 25348 net.cpp:1851] conv1b_param_0(0.42) 
I0704 07:53:09.459353 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:53:09.459355 25348 net.cpp:1851] res2a_branch2a_param_0(0.42) 
I0704 07:53:09.459358 25348 net.cpp:1851] res2a_branch2b_param_0(0.42) 
I0704 07:53:09.459360 25348 net.cpp:1851] res3a_branch2a_param_0(0.42) 
I0704 07:53:09.459362 25348 net.cpp:1851] res3a_branch2b_param_0(0.42) 
I0704 07:53:09.459364 25348 net.cpp:1851] res4a_branch2a_param_0(0.42) 
I0704 07:53:09.459367 25348 net.cpp:1851] res4a_branch2b_param_0(0.42) 
I0704 07:53:09.459368 25348 net.cpp:1851] res5a_branch2a_param_0(0.42) 
I0704 07:53:09.459370 25348 net.cpp:1851] res5a_branch2b_param_0(0.42) 
I0704 07:53:09.459372 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (988498/2.3599e+06) 0.419
I0704 07:53:09.459460 25348 solver.cpp:466] Iteration 26000, Testing net (#0)
I0704 07:53:11.102599 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.846
I0704 07:53:11.102618 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9886
I0704 07:53:11.102623 25348 solver.cpp:539]     Test net output #2: loss = 0.3719 (* 1 = 0.3719 loss)
I0704 07:53:11.123780 25348 solver.cpp:290] Iteration 26000 (26.9016 iter/s, 3.71725s/100 iter), loss = -2.5332e-07
I0704 07:53:11.123796 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:11.123808 25348 sgd_solver.cpp:106] Iteration 26000, lr = 0.0059375
I0704 07:53:11.124349 25348 solver.cpp:375] Finding and applying sparsity: 0.44
I0704 07:53:11.757253 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:53:13.857389 25348 solver.cpp:290] Iteration 26100 (36.583 iter/s, 2.73351s/100 iter), loss = -2.5332e-07
I0704 07:53:13.857412 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:13.857419 25348 sgd_solver.cpp:106] Iteration 26100, lr = 0.00592188
I0704 07:53:15.929682 25348 solver.cpp:290] Iteration 26200 (48.2578 iter/s, 2.07221s/100 iter), loss = -2.5332e-07
I0704 07:53:15.929703 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:15.929711 25348 sgd_solver.cpp:106] Iteration 26200, lr = 0.00590625
I0704 07:53:18.000599 25348 solver.cpp:290] Iteration 26300 (48.2898 iter/s, 2.07083s/100 iter), loss = -2.5332e-07
I0704 07:53:18.000623 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:18.000633 25348 sgd_solver.cpp:106] Iteration 26300, lr = 0.00589063
I0704 07:53:20.078788 25348 solver.cpp:290] Iteration 26400 (48.1208 iter/s, 2.0781s/100 iter), loss = -2.5332e-07
I0704 07:53:20.078810 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:20.078817 25348 sgd_solver.cpp:106] Iteration 26400, lr = 0.005875
I0704 07:53:22.158401 25348 solver.cpp:290] Iteration 26500 (48.0879 iter/s, 2.07953s/100 iter), loss = -2.5332e-07
I0704 07:53:22.158422 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:22.158428 25348 sgd_solver.cpp:106] Iteration 26500, lr = 0.00585938
I0704 07:53:24.237972 25348 solver.cpp:290] Iteration 26600 (48.0889 iter/s, 2.07948s/100 iter), loss = -2.5332e-07
I0704 07:53:24.237998 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:24.238006 25348 sgd_solver.cpp:106] Iteration 26600, lr = 0.00584375
I0704 07:53:26.314012 25348 solver.cpp:290] Iteration 26700 (48.1707 iter/s, 2.07595s/100 iter), loss = -2.5332e-07
I0704 07:53:26.314034 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:26.314041 25348 sgd_solver.cpp:106] Iteration 26700, lr = 0.00582812
I0704 07:53:28.388267 25348 solver.cpp:290] Iteration 26800 (48.2121 iter/s, 2.07417s/100 iter), loss = -2.5332e-07
I0704 07:53:28.388304 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:28.388311 25348 sgd_solver.cpp:106] Iteration 26800, lr = 0.0058125
I0704 07:53:30.460083 25348 solver.cpp:290] Iteration 26900 (48.2692 iter/s, 2.07172s/100 iter), loss = -2.5332e-07
I0704 07:53:30.460108 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:30.460115 25348 sgd_solver.cpp:106] Iteration 26900, lr = 0.00579687
I0704 07:53:32.511368 25348 solver.cpp:354] Sparsity after update:
I0704 07:53:32.512759 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:53:32.512768 25348 net.cpp:1851] conv1a_param_0(0.22) 
I0704 07:53:32.512776 25348 net.cpp:1851] conv1b_param_0(0.44) 
I0704 07:53:32.512780 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:53:32.512784 25348 net.cpp:1851] res2a_branch2a_param_0(0.44) 
I0704 07:53:32.512789 25348 net.cpp:1851] res2a_branch2b_param_0(0.44) 
I0704 07:53:32.512792 25348 net.cpp:1851] res3a_branch2a_param_0(0.44) 
I0704 07:53:32.512796 25348 net.cpp:1851] res3a_branch2b_param_0(0.44) 
I0704 07:53:32.512800 25348 net.cpp:1851] res4a_branch2a_param_0(0.44) 
I0704 07:53:32.512804 25348 net.cpp:1851] res4a_branch2b_param_0(0.44) 
I0704 07:53:32.512809 25348 net.cpp:1851] res5a_branch2a_param_0(0.44) 
I0704 07:53:32.512812 25348 net.cpp:1851] res5a_branch2b_param_0(0.44) 
I0704 07:53:32.512816 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.03557e+06/2.3599e+06) 0.439
I0704 07:53:32.512908 25348 solver.cpp:466] Iteration 27000, Testing net (#0)
I0704 07:53:34.156785 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8887
I0704 07:53:34.156805 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9937
I0704 07:53:34.156810 25348 solver.cpp:539]     Test net output #2: loss = 0.2318 (* 1 = 0.2318 loss)
I0704 07:53:34.176414 25348 solver.cpp:290] Iteration 27000 (26.9092 iter/s, 3.7162s/100 iter), loss = -2.5332e-07
I0704 07:53:34.176430 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:34.176442 25348 sgd_solver.cpp:106] Iteration 27000, lr = 0.00578125
I0704 07:53:34.176956 25348 solver.cpp:375] Finding and applying sparsity: 0.46
I0704 07:53:34.890564 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:53:36.988903 25348 solver.cpp:290] Iteration 27100 (35.557 iter/s, 2.81239s/100 iter), loss = -2.5332e-07
I0704 07:53:36.988924 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:36.988931 25348 sgd_solver.cpp:106] Iteration 27100, lr = 0.00576563
I0704 07:53:39.064262 25348 solver.cpp:290] Iteration 27200 (48.1864 iter/s, 2.07527s/100 iter), loss = -2.5332e-07
I0704 07:53:39.064343 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:39.064353 25348 sgd_solver.cpp:106] Iteration 27200, lr = 0.00575
I0704 07:53:41.137244 25348 solver.cpp:290] Iteration 27300 (48.243 iter/s, 2.07284s/100 iter), loss = -2.5332e-07
I0704 07:53:41.137269 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:41.137277 25348 sgd_solver.cpp:106] Iteration 27300, lr = 0.00573438
I0704 07:53:43.211098 25348 solver.cpp:290] Iteration 27400 (48.2214 iter/s, 2.07377s/100 iter), loss = -2.5332e-07
I0704 07:53:43.211120 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:43.211127 25348 sgd_solver.cpp:106] Iteration 27400, lr = 0.00571875
I0704 07:53:45.282551 25348 solver.cpp:290] Iteration 27500 (48.2773 iter/s, 2.07137s/100 iter), loss = -2.5332e-07
I0704 07:53:45.282572 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:45.282578 25348 sgd_solver.cpp:106] Iteration 27500, lr = 0.00570312
I0704 07:53:47.355146 25348 solver.cpp:290] Iteration 27600 (48.2506 iter/s, 2.07251s/100 iter), loss = -2.5332e-07
I0704 07:53:47.355171 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:47.355180 25348 sgd_solver.cpp:106] Iteration 27600, lr = 0.0056875
I0704 07:53:49.433187 25348 solver.cpp:290] Iteration 27700 (48.1243 iter/s, 2.07795s/100 iter), loss = -2.5332e-07
I0704 07:53:49.433213 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:49.433219 25348 sgd_solver.cpp:106] Iteration 27700, lr = 0.00567187
I0704 07:53:51.507174 25348 solver.cpp:290] Iteration 27800 (48.2184 iter/s, 2.07389s/100 iter), loss = -2.5332e-07
I0704 07:53:51.507200 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:51.507207 25348 sgd_solver.cpp:106] Iteration 27800, lr = 0.00565625
I0704 07:53:53.585644 25348 solver.cpp:290] Iteration 27900 (48.1144 iter/s, 2.07838s/100 iter), loss = -2.5332e-07
I0704 07:53:53.585675 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:53.585687 25348 sgd_solver.cpp:106] Iteration 27900, lr = 0.00564062
I0704 07:53:55.644053 25348 solver.cpp:354] Sparsity after update:
I0704 07:53:55.645606 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:53:55.645614 25348 net.cpp:1851] conv1a_param_0(0.23) 
I0704 07:53:55.645622 25348 net.cpp:1851] conv1b_param_0(0.46) 
I0704 07:53:55.645624 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:53:55.645627 25348 net.cpp:1851] res2a_branch2a_param_0(0.46) 
I0704 07:53:55.645628 25348 net.cpp:1851] res2a_branch2b_param_0(0.46) 
I0704 07:53:55.645632 25348 net.cpp:1851] res3a_branch2a_param_0(0.46) 
I0704 07:53:55.645633 25348 net.cpp:1851] res3a_branch2b_param_0(0.46) 
I0704 07:53:55.645637 25348 net.cpp:1851] res4a_branch2a_param_0(0.46) 
I0704 07:53:55.645638 25348 net.cpp:1851] res4a_branch2b_param_0(0.46) 
I0704 07:53:55.645642 25348 net.cpp:1851] res5a_branch2a_param_0(0.46) 
I0704 07:53:55.645643 25348 net.cpp:1851] res5a_branch2b_param_0(0.46) 
I0704 07:53:55.645645 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.08264e+06/2.3599e+06) 0.459
I0704 07:53:55.645735 25348 solver.cpp:466] Iteration 28000, Testing net (#0)
I0704 07:53:57.288645 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8791
I0704 07:53:57.288663 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9951
I0704 07:53:57.288668 25348 solver.cpp:539]     Test net output #2: loss = 0.2633 (* 1 = 0.2633 loss)
I0704 07:53:57.308953 25348 solver.cpp:290] Iteration 28000 (26.8588 iter/s, 3.72317s/100 iter), loss = -2.5332e-07
I0704 07:53:57.308972 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:57.308980 25348 sgd_solver.cpp:106] Iteration 28000, lr = 0.005625
I0704 07:53:57.309518 25348 solver.cpp:375] Finding and applying sparsity: 0.48
I0704 07:53:58.133551 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:54:00.225497 25348 solver.cpp:290] Iteration 28100 (34.2884 iter/s, 2.91644s/100 iter), loss = -2.5332e-07
I0704 07:54:00.225533 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:00.225540 25348 sgd_solver.cpp:106] Iteration 28100, lr = 0.00560937
I0704 07:54:02.297598 25348 solver.cpp:290] Iteration 28200 (48.2625 iter/s, 2.072s/100 iter), loss = -2.5332e-07
I0704 07:54:02.297621 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:02.297626 25348 sgd_solver.cpp:106] Iteration 28200, lr = 0.00559375
I0704 07:54:04.374299 25348 solver.cpp:290] Iteration 28300 (48.1554 iter/s, 2.07661s/100 iter), loss = -2.5332e-07
I0704 07:54:04.374326 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:04.374332 25348 sgd_solver.cpp:106] Iteration 28300, lr = 0.00557812
I0704 07:54:06.447976 25348 solver.cpp:290] Iteration 28400 (48.2256 iter/s, 2.07359s/100 iter), loss = -2.5332e-07
I0704 07:54:06.448000 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:06.448007 25348 sgd_solver.cpp:106] Iteration 28400, lr = 0.0055625
I0704 07:54:08.521772 25348 solver.cpp:290] Iteration 28500 (48.2227 iter/s, 2.07371s/100 iter), loss = -2.5332e-07
I0704 07:54:08.521795 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:08.521803 25348 sgd_solver.cpp:106] Iteration 28500, lr = 0.00554687
I0704 07:54:10.599828 25348 solver.cpp:290] Iteration 28600 (48.1239 iter/s, 2.07797s/100 iter), loss = -2.5332e-07
I0704 07:54:10.599869 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:10.599875 25348 sgd_solver.cpp:106] Iteration 28600, lr = 0.00553125
I0704 07:54:12.672439 25348 solver.cpp:290] Iteration 28700 (48.2507 iter/s, 2.07251s/100 iter), loss = -2.5332e-07
I0704 07:54:12.672463 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:12.672472 25348 sgd_solver.cpp:106] Iteration 28700, lr = 0.00551562
I0704 07:54:14.746520 25348 solver.cpp:290] Iteration 28800 (48.2161 iter/s, 2.074s/100 iter), loss = -2.5332e-07
I0704 07:54:14.746543 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:14.746552 25348 sgd_solver.cpp:106] Iteration 28800, lr = 0.0055
I0704 07:54:16.816256 25348 solver.cpp:290] Iteration 28900 (48.3173 iter/s, 2.06965s/100 iter), loss = -2.5332e-07
I0704 07:54:16.816278 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:16.816285 25348 sgd_solver.cpp:106] Iteration 28900, lr = 0.00548437
I0704 07:54:18.869832 25348 solver.cpp:354] Sparsity after update:
I0704 07:54:18.871220 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:54:18.871228 25348 net.cpp:1851] conv1a_param_0(0.24) 
I0704 07:54:18.871235 25348 net.cpp:1851] conv1b_param_0(0.479) 
I0704 07:54:18.871238 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:54:18.871242 25348 net.cpp:1851] res2a_branch2a_param_0(0.48) 
I0704 07:54:18.871243 25348 net.cpp:1851] res2a_branch2b_param_0(0.48) 
I0704 07:54:18.871245 25348 net.cpp:1851] res3a_branch2a_param_0(0.48) 
I0704 07:54:18.871248 25348 net.cpp:1851] res3a_branch2b_param_0(0.48) 
I0704 07:54:18.871249 25348 net.cpp:1851] res4a_branch2a_param_0(0.48) 
I0704 07:54:18.871253 25348 net.cpp:1851] res4a_branch2b_param_0(0.48) 
I0704 07:54:18.871254 25348 net.cpp:1851] res5a_branch2a_param_0(0.48) 
I0704 07:54:18.871258 25348 net.cpp:1851] res5a_branch2b_param_0(0.48) 
I0704 07:54:18.871259 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.12971e+06/2.3599e+06) 0.479
I0704 07:54:18.871346 25348 solver.cpp:466] Iteration 29000, Testing net (#0)
I0704 07:54:20.514633 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8845
I0704 07:54:20.514652 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9952
I0704 07:54:20.514658 25348 solver.cpp:539]     Test net output #2: loss = 0.2534 (* 1 = 0.2534 loss)
I0704 07:54:20.534303 25348 solver.cpp:290] Iteration 29000 (26.8968 iter/s, 3.71792s/100 iter), loss = -2.5332e-07
I0704 07:54:20.534322 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:20.534332 25348 sgd_solver.cpp:106] Iteration 29000, lr = 0.00546875
I0704 07:54:20.534890 25348 solver.cpp:375] Finding and applying sparsity: 0.5
I0704 07:54:21.456465 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:54:23.544469 25348 solver.cpp:290] Iteration 29100 (33.2219 iter/s, 3.01006s/100 iter), loss = -2.5332e-07
I0704 07:54:23.544494 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:23.544502 25348 sgd_solver.cpp:106] Iteration 29100, lr = 0.00545313
I0704 07:54:25.613539 25348 solver.cpp:290] Iteration 29200 (48.333 iter/s, 2.06898s/100 iter), loss = -2.5332e-07
I0704 07:54:25.613565 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:25.613572 25348 sgd_solver.cpp:106] Iteration 29200, lr = 0.0054375
I0704 07:54:27.690219 25348 solver.cpp:290] Iteration 29300 (48.1558 iter/s, 2.07659s/100 iter), loss = -2.5332e-07
I0704 07:54:27.690243 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:27.690249 25348 sgd_solver.cpp:106] Iteration 29300, lr = 0.00542188
I0704 07:54:29.761780 25348 solver.cpp:290] Iteration 29400 (48.2748 iter/s, 2.07147s/100 iter), loss = -2.5332e-07
I0704 07:54:29.761803 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:29.761811 25348 sgd_solver.cpp:106] Iteration 29400, lr = 0.00540625
I0704 07:54:31.839049 25348 solver.cpp:290] Iteration 29500 (48.1422 iter/s, 2.07718s/100 iter), loss = -2.5332e-07
I0704 07:54:31.839087 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:31.839094 25348 sgd_solver.cpp:106] Iteration 29500, lr = 0.00539062
I0704 07:54:33.917202 25348 solver.cpp:290] Iteration 29600 (48.122 iter/s, 2.07805s/100 iter), loss = -2.5332e-07
I0704 07:54:33.917224 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:33.917232 25348 sgd_solver.cpp:106] Iteration 29600, lr = 0.005375
I0704 07:54:35.996539 25348 solver.cpp:290] Iteration 29700 (48.0942 iter/s, 2.07925s/100 iter), loss = -2.5332e-07
I0704 07:54:35.996562 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:35.996569 25348 sgd_solver.cpp:106] Iteration 29700, lr = 0.00535937
I0704 07:54:38.068172 25348 solver.cpp:290] Iteration 29800 (48.2732 iter/s, 2.07154s/100 iter), loss = -2.5332e-07
I0704 07:54:38.068198 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:38.068207 25348 sgd_solver.cpp:106] Iteration 29800, lr = 0.00534375
I0704 07:54:40.140393 25348 solver.cpp:290] Iteration 29900 (48.2594 iter/s, 2.07213s/100 iter), loss = -2.5332e-07
I0704 07:54:40.140415 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:40.140424 25348 sgd_solver.cpp:106] Iteration 29900, lr = 0.00532812
I0704 07:54:42.192168 25348 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_30000.caffemodel
I0704 07:54:42.208760 25348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_30000.solverstate
I0704 07:54:42.216162 25348 solver.cpp:354] Sparsity after update:
I0704 07:54:42.217093 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:54:42.217102 25348 net.cpp:1851] conv1a_param_0(0.25) 
I0704 07:54:42.217109 25348 net.cpp:1851] conv1b_param_0(0.5) 
I0704 07:54:42.217111 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:54:42.217113 25348 net.cpp:1851] res2a_branch2a_param_0(0.5) 
I0704 07:54:42.217115 25348 net.cpp:1851] res2a_branch2b_param_0(0.5) 
I0704 07:54:42.217118 25348 net.cpp:1851] res3a_branch2a_param_0(0.5) 
I0704 07:54:42.217119 25348 net.cpp:1851] res3a_branch2b_param_0(0.5) 
I0704 07:54:42.217121 25348 net.cpp:1851] res4a_branch2a_param_0(0.5) 
I0704 07:54:42.217123 25348 net.cpp:1851] res4a_branch2b_param_0(0.5) 
I0704 07:54:42.217125 25348 net.cpp:1851] res5a_branch2a_param_0(0.5) 
I0704 07:54:42.217128 25348 net.cpp:1851] res5a_branch2b_param_0(0.5) 
I0704 07:54:42.217129 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.17679e+06/2.3599e+06) 0.499
I0704 07:54:42.217233 25348 solver.cpp:466] Iteration 30000, Testing net (#0)
I0704 07:54:43.862226 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8809
I0704 07:54:43.862244 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9934
I0704 07:54:43.862251 25348 solver.cpp:539]     Test net output #2: loss = 0.255 (* 1 = 0.255 loss)
I0704 07:54:43.882848 25348 solver.cpp:290] Iteration 30000 (26.7213 iter/s, 3.74233s/100 iter), loss = -2.5332e-07
I0704 07:54:43.882864 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:43.882877 25348 sgd_solver.cpp:106] Iteration 30000, lr = 0.0053125
I0704 07:54:43.883414 25348 solver.cpp:375] Finding and applying sparsity: 0.52
I0704 07:54:44.846622 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:54:46.933162 25348 solver.cpp:290] Iteration 30100 (32.7847 iter/s, 3.05021s/100 iter), loss = -2.5332e-07
I0704 07:54:46.933185 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:46.933192 25348 sgd_solver.cpp:106] Iteration 30100, lr = 0.00529688
I0704 07:54:49.004132 25348 solver.cpp:290] Iteration 30200 (48.2885 iter/s, 2.07089s/100 iter), loss = -2.5332e-07
I0704 07:54:49.004154 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:49.004161 25348 sgd_solver.cpp:106] Iteration 30200, lr = 0.00528125
I0704 07:54:51.075896 25348 solver.cpp:290] Iteration 30300 (48.27 iter/s, 2.07168s/100 iter), loss = -2.5332e-07
I0704 07:54:51.075918 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:51.075925 25348 sgd_solver.cpp:106] Iteration 30300, lr = 0.00526563
I0704 07:54:53.146976 25348 solver.cpp:290] Iteration 30400 (48.286 iter/s, 2.07099s/100 iter), loss = -2.5332e-07
I0704 07:54:53.146999 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:53.147006 25348 sgd_solver.cpp:106] Iteration 30400, lr = 0.00525
I0704 07:54:55.222872 25348 solver.cpp:290] Iteration 30500 (48.174 iter/s, 2.07581s/100 iter), loss = -2.5332e-07
I0704 07:54:55.222894 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:55.222903 25348 sgd_solver.cpp:106] Iteration 30500, lr = 0.00523437
I0704 07:54:57.293830 25348 solver.cpp:290] Iteration 30600 (48.2889 iter/s, 2.07087s/100 iter), loss = -2.5332e-07
I0704 07:54:57.293853 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:57.293861 25348 sgd_solver.cpp:106] Iteration 30600, lr = 0.00521875
I0704 07:54:59.367960 25348 solver.cpp:290] Iteration 30700 (48.215 iter/s, 2.07404s/100 iter), loss = -2.5332e-07
I0704 07:54:59.367983 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:59.367990 25348 sgd_solver.cpp:106] Iteration 30700, lr = 0.00520312
I0704 07:55:01.441632 25348 solver.cpp:290] Iteration 30800 (48.2257 iter/s, 2.07358s/100 iter), loss = -2.5332e-07
I0704 07:55:01.441670 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:01.441678 25348 sgd_solver.cpp:106] Iteration 30800, lr = 0.0051875
I0704 07:55:03.519050 25348 solver.cpp:290] Iteration 30900 (48.139 iter/s, 2.07732s/100 iter), loss = -2.5332e-07
I0704 07:55:03.519076 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:03.519086 25348 sgd_solver.cpp:106] Iteration 30900, lr = 0.00517187
I0704 07:55:05.572597 25348 solver.cpp:354] Sparsity after update:
I0704 07:55:05.574000 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:55:05.574007 25348 net.cpp:1851] conv1a_param_0(0.26) 
I0704 07:55:05.574014 25348 net.cpp:1851] conv1b_param_0(0.52) 
I0704 07:55:05.574017 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:55:05.574018 25348 net.cpp:1851] res2a_branch2a_param_0(0.52) 
I0704 07:55:05.574020 25348 net.cpp:1851] res2a_branch2b_param_0(0.52) 
I0704 07:55:05.574023 25348 net.cpp:1851] res3a_branch2a_param_0(0.52) 
I0704 07:55:05.574024 25348 net.cpp:1851] res3a_branch2b_param_0(0.52) 
I0704 07:55:05.574026 25348 net.cpp:1851] res4a_branch2a_param_0(0.52) 
I0704 07:55:05.574028 25348 net.cpp:1851] res4a_branch2b_param_0(0.52) 
I0704 07:55:05.574030 25348 net.cpp:1851] res5a_branch2a_param_0(0.52) 
I0704 07:55:05.574033 25348 net.cpp:1851] res5a_branch2b_param_0(0.52) 
I0704 07:55:05.574034 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.22386e+06/2.3599e+06) 0.519
I0704 07:55:05.574123 25348 solver.cpp:466] Iteration 31000, Testing net (#0)
I0704 07:55:07.217237 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8992
I0704 07:55:07.217255 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9936
I0704 07:55:07.217262 25348 solver.cpp:539]     Test net output #2: loss = 0.2147 (* 1 = 0.2147 loss)
I0704 07:55:07.237318 25348 solver.cpp:290] Iteration 31000 (26.8952 iter/s, 3.71814s/100 iter), loss = -2.5332e-07
I0704 07:55:07.237336 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:07.237344 25348 sgd_solver.cpp:106] Iteration 31000, lr = 0.00515625
I0704 07:55:07.238111 25348 solver.cpp:375] Finding and applying sparsity: 0.54
I0704 07:55:08.208667 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:55:10.310719 25348 solver.cpp:290] Iteration 31100 (32.5384 iter/s, 3.07329s/100 iter), loss = -2.5332e-07
I0704 07:55:10.310740 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:10.310748 25348 sgd_solver.cpp:106] Iteration 31100, lr = 0.00514062
I0704 07:55:12.381945 25348 solver.cpp:290] Iteration 31200 (48.2826 iter/s, 2.07114s/100 iter), loss = -2.5332e-07
I0704 07:55:12.382025 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:12.382036 25348 sgd_solver.cpp:106] Iteration 31200, lr = 0.005125
I0704 07:55:14.452172 25348 solver.cpp:290] Iteration 31300 (48.3072 iter/s, 2.07009s/100 iter), loss = -2.5332e-07
I0704 07:55:14.452194 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:14.452201 25348 sgd_solver.cpp:106] Iteration 31300, lr = 0.00510937
I0704 07:55:16.529171 25348 solver.cpp:290] Iteration 31400 (48.1484 iter/s, 2.07691s/100 iter), loss = -2.5332e-07
I0704 07:55:16.529194 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:16.529201 25348 sgd_solver.cpp:106] Iteration 31400, lr = 0.00509375
I0704 07:55:18.606731 25348 solver.cpp:290] Iteration 31500 (48.1354 iter/s, 2.07747s/100 iter), loss = -2.5332e-07
I0704 07:55:18.606752 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:18.606758 25348 sgd_solver.cpp:106] Iteration 31500, lr = 0.00507812
I0704 07:55:20.677386 25348 solver.cpp:290] Iteration 31600 (48.2959 iter/s, 2.07057s/100 iter), loss = -2.5332e-07
I0704 07:55:20.677408 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:20.677417 25348 sgd_solver.cpp:106] Iteration 31600, lr = 0.0050625
I0704 07:55:22.750090 25348 solver.cpp:290] Iteration 31700 (48.2482 iter/s, 2.07262s/100 iter), loss = -2.5332e-07
I0704 07:55:22.750116 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:22.750125 25348 sgd_solver.cpp:106] Iteration 31700, lr = 0.00504687
I0704 07:55:24.821933 25348 solver.cpp:290] Iteration 31800 (48.2683 iter/s, 2.07175s/100 iter), loss = -2.5332e-07
I0704 07:55:24.821959 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:24.821966 25348 sgd_solver.cpp:106] Iteration 31800, lr = 0.00503125
I0704 07:55:26.893606 25348 solver.cpp:290] Iteration 31900 (48.2722 iter/s, 2.07158s/100 iter), loss = -2.5332e-07
I0704 07:55:26.893630 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:26.893636 25348 sgd_solver.cpp:106] Iteration 31900, lr = 0.00501562
I0704 07:55:28.950845 25348 solver.cpp:354] Sparsity after update:
I0704 07:55:28.952277 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:55:28.952283 25348 net.cpp:1851] conv1a_param_0(0.27) 
I0704 07:55:28.952291 25348 net.cpp:1851] conv1b_param_0(0.54) 
I0704 07:55:28.952293 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:55:28.952296 25348 net.cpp:1851] res2a_branch2a_param_0(0.54) 
I0704 07:55:28.952297 25348 net.cpp:1851] res2a_branch2b_param_0(0.54) 
I0704 07:55:28.952299 25348 net.cpp:1851] res3a_branch2a_param_0(0.54) 
I0704 07:55:28.952301 25348 net.cpp:1851] res3a_branch2b_param_0(0.54) 
I0704 07:55:28.952303 25348 net.cpp:1851] res4a_branch2a_param_0(0.54) 
I0704 07:55:28.952306 25348 net.cpp:1851] res4a_branch2b_param_0(0.54) 
I0704 07:55:28.952307 25348 net.cpp:1851] res5a_branch2a_param_0(0.54) 
I0704 07:55:28.952309 25348 net.cpp:1851] res5a_branch2b_param_0(0.54) 
I0704 07:55:28.952311 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.27093e+06/2.3599e+06) 0.539
I0704 07:55:28.952399 25348 solver.cpp:466] Iteration 32000, Testing net (#0)
I0704 07:55:30.600853 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8717
I0704 07:55:30.600874 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9928
I0704 07:55:30.600879 25348 solver.cpp:539]     Test net output #2: loss = 0.287 (* 1 = 0.287 loss)
I0704 07:55:30.621198 25348 solver.cpp:290] Iteration 32000 (26.8279 iter/s, 3.72746s/100 iter), loss = -2.5332e-07
I0704 07:55:30.621217 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:30.621228 25348 sgd_solver.cpp:106] Iteration 32000, lr = 0.005
I0704 07:55:30.621772 25348 solver.cpp:375] Finding and applying sparsity: 0.56
I0704 07:55:31.617177 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:55:33.726516 25348 solver.cpp:290] Iteration 32100 (32.2039 iter/s, 3.10521s/100 iter), loss = 0.0476188
I0704 07:55:33.726547 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:55:33.726553 25348 sgd_solver.cpp:106] Iteration 32100, lr = 0.00498438
I0704 07:55:35.801352 25348 solver.cpp:290] Iteration 32200 (48.1987 iter/s, 2.07474s/100 iter), loss = -2.5332e-07
I0704 07:55:35.801376 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:35.801385 25348 sgd_solver.cpp:106] Iteration 32200, lr = 0.00496875
I0704 07:55:37.881471 25348 solver.cpp:290] Iteration 32300 (48.0762 iter/s, 2.08003s/100 iter), loss = -2.5332e-07
I0704 07:55:37.881494 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:37.881501 25348 sgd_solver.cpp:106] Iteration 32300, lr = 0.00495313
I0704 07:55:39.957617 25348 solver.cpp:290] Iteration 32400 (48.1681 iter/s, 2.07606s/100 iter), loss = -2.5332e-07
I0704 07:55:39.957639 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:39.957646 25348 sgd_solver.cpp:106] Iteration 32400, lr = 0.0049375
I0704 07:55:42.034205 25348 solver.cpp:290] Iteration 32500 (48.1579 iter/s, 2.0765s/100 iter), loss = -2.5332e-07
I0704 07:55:42.034227 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:42.034235 25348 sgd_solver.cpp:106] Iteration 32500, lr = 0.00492187
I0704 07:55:44.106542 25348 solver.cpp:290] Iteration 32600 (48.2567 iter/s, 2.07225s/100 iter), loss = -2.5332e-07
I0704 07:55:44.106609 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:44.106619 25348 sgd_solver.cpp:106] Iteration 32600, lr = 0.00490625
I0704 07:55:46.184480 25348 solver.cpp:290] Iteration 32700 (48.1276 iter/s, 2.07781s/100 iter), loss = -2.5332e-07
I0704 07:55:46.184502 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:46.184509 25348 sgd_solver.cpp:106] Iteration 32700, lr = 0.00489062
I0704 07:55:48.257105 25348 solver.cpp:290] Iteration 32800 (48.25 iter/s, 2.07254s/100 iter), loss = -2.5332e-07
I0704 07:55:48.257128 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:48.257135 25348 sgd_solver.cpp:106] Iteration 32800, lr = 0.004875
I0704 07:55:50.331696 25348 solver.cpp:290] Iteration 32900 (48.2043 iter/s, 2.0745s/100 iter), loss = -2.5332e-07
I0704 07:55:50.331718 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:50.331724 25348 sgd_solver.cpp:106] Iteration 32900, lr = 0.00485937
I0704 07:55:52.385601 25348 solver.cpp:354] Sparsity after update:
I0704 07:55:52.386989 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:55:52.386997 25348 net.cpp:1851] conv1a_param_0(0.28) 
I0704 07:55:52.387006 25348 net.cpp:1851] conv1b_param_0(0.56) 
I0704 07:55:52.387007 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:55:52.387009 25348 net.cpp:1851] res2a_branch2a_param_0(0.56) 
I0704 07:55:52.387012 25348 net.cpp:1851] res2a_branch2b_param_0(0.56) 
I0704 07:55:52.387013 25348 net.cpp:1851] res3a_branch2a_param_0(0.56) 
I0704 07:55:52.387015 25348 net.cpp:1851] res3a_branch2b_param_0(0.56) 
I0704 07:55:52.387017 25348 net.cpp:1851] res4a_branch2a_param_0(0.56) 
I0704 07:55:52.387019 25348 net.cpp:1851] res4a_branch2b_param_0(0.56) 
I0704 07:55:52.387022 25348 net.cpp:1851] res5a_branch2a_param_0(0.56) 
I0704 07:55:52.387024 25348 net.cpp:1851] res5a_branch2b_param_0(0.56) 
I0704 07:55:52.387027 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.318e+06/2.3599e+06) 0.558
I0704 07:55:52.387114 25348 solver.cpp:466] Iteration 33000, Testing net (#0)
I0704 07:55:54.030304 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.7612
I0704 07:55:54.030323 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9878
I0704 07:55:54.030328 25348 solver.cpp:539]     Test net output #2: loss = 0.684 (* 1 = 0.684 loss)
I0704 07:55:54.050001 25348 solver.cpp:290] Iteration 33000 (26.8949 iter/s, 3.71818s/100 iter), loss = -2.5332e-07
I0704 07:55:54.050019 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:54.050031 25348 sgd_solver.cpp:106] Iteration 33000, lr = 0.00484375
I0704 07:55:54.050592 25348 solver.cpp:375] Finding and applying sparsity: 0.58
I0704 07:55:54.944296 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:55:57.053640 25348 solver.cpp:290] Iteration 33100 (33.2941 iter/s, 3.00353s/100 iter), loss = -2.5332e-07
I0704 07:55:57.053661 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:57.053668 25348 sgd_solver.cpp:106] Iteration 33100, lr = 0.00482813
I0704 07:55:59.125533 25348 solver.cpp:290] Iteration 33200 (48.267 iter/s, 2.07181s/100 iter), loss = -2.5332e-07
I0704 07:55:59.125556 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:59.125564 25348 sgd_solver.cpp:106] Iteration 33200, lr = 0.0048125
I0704 07:56:01.207129 25348 solver.cpp:290] Iteration 33300 (48.042 iter/s, 2.08151s/100 iter), loss = -2.5332e-07
I0704 07:56:01.207152 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:01.207159 25348 sgd_solver.cpp:106] Iteration 33300, lr = 0.00479688
I0704 07:56:03.279147 25348 solver.cpp:290] Iteration 33400 (48.2642 iter/s, 2.07193s/100 iter), loss = -2.5332e-07
I0704 07:56:03.279170 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:03.279176 25348 sgd_solver.cpp:106] Iteration 33400, lr = 0.00478125
I0704 07:56:05.355294 25348 solver.cpp:290] Iteration 33500 (48.1681 iter/s, 2.07606s/100 iter), loss = 0.0476188
I0704 07:56:05.355332 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:56:05.355340 25348 sgd_solver.cpp:106] Iteration 33500, lr = 0.00476563
I0704 07:56:07.425720 25348 solver.cpp:290] Iteration 33600 (48.3015 iter/s, 2.07033s/100 iter), loss = 0.0476188
I0704 07:56:07.425743 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:56:07.425750 25348 sgd_solver.cpp:106] Iteration 33600, lr = 0.00475
I0704 07:56:09.498747 25348 solver.cpp:290] Iteration 33700 (48.2407 iter/s, 2.07294s/100 iter), loss = -2.5332e-07
I0704 07:56:09.498770 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:09.498777 25348 sgd_solver.cpp:106] Iteration 33700, lr = 0.00473437
I0704 07:56:11.583765 25348 solver.cpp:290] Iteration 33800 (47.9633 iter/s, 2.08493s/100 iter), loss = -2.6077e-07
I0704 07:56:11.583791 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:11.583797 25348 sgd_solver.cpp:106] Iteration 33800, lr = 0.00471875
I0704 07:56:13.676417 25348 solver.cpp:290] Iteration 33900 (47.7883 iter/s, 2.09256s/100 iter), loss = -2.5332e-07
I0704 07:56:13.676445 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:13.676455 25348 sgd_solver.cpp:106] Iteration 33900, lr = 0.00470312
I0704 07:56:15.752421 25348 solver.cpp:354] Sparsity after update:
I0704 07:56:15.753824 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:56:15.753834 25348 net.cpp:1851] conv1a_param_0(0.29) 
I0704 07:56:15.753844 25348 net.cpp:1851] conv1b_param_0(0.58) 
I0704 07:56:15.753849 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:56:15.753854 25348 net.cpp:1851] res2a_branch2a_param_0(0.58) 
I0704 07:56:15.753857 25348 net.cpp:1851] res2a_branch2b_param_0(0.58) 
I0704 07:56:15.753861 25348 net.cpp:1851] res3a_branch2a_param_0(0.58) 
I0704 07:56:15.753865 25348 net.cpp:1851] res3a_branch2b_param_0(0.58) 
I0704 07:56:15.753868 25348 net.cpp:1851] res4a_branch2a_param_0(0.58) 
I0704 07:56:15.753872 25348 net.cpp:1851] res4a_branch2b_param_0(0.58) 
I0704 07:56:15.753876 25348 net.cpp:1851] res5a_branch2a_param_0(0.58) 
I0704 07:56:15.753880 25348 net.cpp:1851] res5a_branch2b_param_0(0.58) 
I0704 07:56:15.753885 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.36507e+06/2.3599e+06) 0.578
I0704 07:56:15.753976 25348 solver.cpp:466] Iteration 34000, Testing net (#0)
I0704 07:56:17.406510 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8524
I0704 07:56:17.406530 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9881
I0704 07:56:17.406535 25348 solver.cpp:539]     Test net output #2: loss = 0.3518 (* 1 = 0.3518 loss)
I0704 07:56:17.426501 25348 solver.cpp:290] Iteration 34000 (26.667 iter/s, 3.74995s/100 iter), loss = -2.5332e-07
I0704 07:56:17.426518 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:17.426529 25348 sgd_solver.cpp:106] Iteration 34000, lr = 0.0046875
I0704 07:56:17.427050 25348 solver.cpp:375] Finding and applying sparsity: 0.6
I0704 07:56:18.359688 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:56:20.459000 25348 solver.cpp:290] Iteration 34100 (32.9773 iter/s, 3.03239s/100 iter), loss = -2.5332e-07
I0704 07:56:20.459022 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:20.459028 25348 sgd_solver.cpp:106] Iteration 34100, lr = 0.00467187
I0704 07:56:22.534564 25348 solver.cpp:290] Iteration 34200 (48.1816 iter/s, 2.07548s/100 iter), loss = -2.5332e-07
I0704 07:56:22.534587 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:22.534595 25348 sgd_solver.cpp:106] Iteration 34200, lr = 0.00465625
I0704 07:56:24.605175 25348 solver.cpp:290] Iteration 34300 (48.297 iter/s, 2.07052s/100 iter), loss = 0.0476188
I0704 07:56:24.605199 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:56:24.605208 25348 sgd_solver.cpp:106] Iteration 34300, lr = 0.00464062
I0704 07:56:26.685298 25348 solver.cpp:290] Iteration 34400 (48.0761 iter/s, 2.08004s/100 iter), loss = 0.0476188
I0704 07:56:26.685322 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:56:26.685330 25348 sgd_solver.cpp:106] Iteration 34400, lr = 0.004625
I0704 07:56:28.758674 25348 solver.cpp:290] Iteration 34500 (48.2325 iter/s, 2.07329s/100 iter), loss = -2.5332e-07
I0704 07:56:28.758695 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:28.758703 25348 sgd_solver.cpp:106] Iteration 34500, lr = 0.00460937
I0704 07:56:30.835386 25348 solver.cpp:290] Iteration 34600 (48.155 iter/s, 2.07663s/100 iter), loss = -2.5332e-07
I0704 07:56:30.835407 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:30.835413 25348 sgd_solver.cpp:106] Iteration 34600, lr = 0.00459375
I0704 07:56:32.911777 25348 solver.cpp:290] Iteration 34700 (48.1624 iter/s, 2.07631s/100 iter), loss = -2.5332e-07
I0704 07:56:32.911798 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:32.911805 25348 sgd_solver.cpp:106] Iteration 34700, lr = 0.00457812
I0704 07:56:35.024590 25348 solver.cpp:290] Iteration 34800 (47.3322 iter/s, 2.11273s/100 iter), loss = -2.5332e-07
I0704 07:56:35.024611 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:35.024618 25348 sgd_solver.cpp:106] Iteration 34800, lr = 0.0045625
I0704 07:56:37.099426 25348 solver.cpp:290] Iteration 34900 (48.1985 iter/s, 2.07475s/100 iter), loss = -2.5332e-07
I0704 07:56:37.099448 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:37.099454 25348 sgd_solver.cpp:106] Iteration 34900, lr = 0.00454687
I0704 07:56:39.151453 25348 solver.cpp:354] Sparsity after update:
I0704 07:56:39.152845 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:56:39.152853 25348 net.cpp:1851] conv1a_param_0(0.3) 
I0704 07:56:39.152860 25348 net.cpp:1851] conv1b_param_0(0.6) 
I0704 07:56:39.152864 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:56:39.152868 25348 net.cpp:1851] res2a_branch2a_param_0(0.6) 
I0704 07:56:39.152871 25348 net.cpp:1851] res2a_branch2b_param_0(0.6) 
I0704 07:56:39.152876 25348 net.cpp:1851] res3a_branch2a_param_0(0.6) 
I0704 07:56:39.152879 25348 net.cpp:1851] res3a_branch2b_param_0(0.6) 
I0704 07:56:39.152884 25348 net.cpp:1851] res4a_branch2a_param_0(0.6) 
I0704 07:56:39.152887 25348 net.cpp:1851] res4a_branch2b_param_0(0.6) 
I0704 07:56:39.152891 25348 net.cpp:1851] res5a_branch2a_param_0(0.6) 
I0704 07:56:39.152894 25348 net.cpp:1851] res5a_branch2b_param_0(0.6) 
I0704 07:56:39.152899 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.41214e+06/2.3599e+06) 0.598
I0704 07:56:39.153034 25348 solver.cpp:466] Iteration 35000, Testing net (#0)
I0704 07:56:40.793711 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8773
I0704 07:56:40.793730 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9926
I0704 07:56:40.793735 25348 solver.cpp:539]     Test net output #2: loss = 0.2769 (* 1 = 0.2769 loss)
I0704 07:56:40.814010 25348 solver.cpp:290] Iteration 35000 (26.9219 iter/s, 3.71445s/100 iter), loss = -2.5332e-07
I0704 07:56:40.814038 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:40.814043 25348 sgd_solver.cpp:106] Iteration 35000, lr = 0.00453125
I0704 07:56:40.814651 25348 solver.cpp:375] Finding and applying sparsity: 0.62
I0704 07:56:41.819806 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:56:43.932368 25348 solver.cpp:290] Iteration 35100 (32.0693 iter/s, 3.11824s/100 iter), loss = -2.5332e-07
I0704 07:56:43.932389 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:43.932397 25348 sgd_solver.cpp:106] Iteration 35100, lr = 0.00451563
I0704 07:56:46.010854 25348 solver.cpp:290] Iteration 35200 (48.1139 iter/s, 2.0784s/100 iter), loss = -2.5332e-07
I0704 07:56:46.010912 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:46.010921 25348 sgd_solver.cpp:106] Iteration 35200, lr = 0.0045
I0704 07:56:48.085492 25348 solver.cpp:290] Iteration 35300 (48.204 iter/s, 2.07452s/100 iter), loss = -2.5332e-07
I0704 07:56:48.085517 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:48.085526 25348 sgd_solver.cpp:106] Iteration 35300, lr = 0.00448438
I0704 07:56:50.170425 25348 solver.cpp:290] Iteration 35400 (47.9653 iter/s, 2.08484s/100 iter), loss = -2.5332e-07
I0704 07:56:50.170449 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:50.170455 25348 sgd_solver.cpp:106] Iteration 35400, lr = 0.00446875
I0704 07:56:52.250138 25348 solver.cpp:290] Iteration 35500 (48.0856 iter/s, 2.07963s/100 iter), loss = -2.5332e-07
I0704 07:56:52.250159 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:52.250165 25348 sgd_solver.cpp:106] Iteration 35500, lr = 0.00445312
I0704 07:56:54.322988 25348 solver.cpp:290] Iteration 35600 (48.2447 iter/s, 2.07277s/100 iter), loss = -2.5332e-07
I0704 07:56:54.323011 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:54.323017 25348 sgd_solver.cpp:106] Iteration 35600, lr = 0.0044375
I0704 07:56:56.408324 25348 solver.cpp:290] Iteration 35700 (47.9559 iter/s, 2.08525s/100 iter), loss = -2.5332e-07
I0704 07:56:56.408346 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:56.408352 25348 sgd_solver.cpp:106] Iteration 35700, lr = 0.00442187
I0704 07:56:58.485533 25348 solver.cpp:290] Iteration 35800 (48.1435 iter/s, 2.07712s/100 iter), loss = -2.5332e-07
I0704 07:56:58.485558 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:58.485564 25348 sgd_solver.cpp:106] Iteration 35800, lr = 0.00440625
I0704 07:57:00.557409 25348 solver.cpp:290] Iteration 35900 (48.2675 iter/s, 2.07179s/100 iter), loss = -2.5332e-07
I0704 07:57:00.557431 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:00.557437 25348 sgd_solver.cpp:106] Iteration 35900, lr = 0.00439062
I0704 07:57:02.615828 25348 solver.cpp:354] Sparsity after update:
I0704 07:57:02.617236 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:57:02.617244 25348 net.cpp:1851] conv1a_param_0(0.31) 
I0704 07:57:02.617251 25348 net.cpp:1851] conv1b_param_0(0.62) 
I0704 07:57:02.617254 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:57:02.617259 25348 net.cpp:1851] res2a_branch2a_param_0(0.62) 
I0704 07:57:02.617264 25348 net.cpp:1851] res2a_branch2b_param_0(0.62) 
I0704 07:57:02.617267 25348 net.cpp:1851] res3a_branch2a_param_0(0.62) 
I0704 07:57:02.617271 25348 net.cpp:1851] res3a_branch2b_param_0(0.62) 
I0704 07:57:02.617275 25348 net.cpp:1851] res4a_branch2a_param_0(0.62) 
I0704 07:57:02.617278 25348 net.cpp:1851] res4a_branch2b_param_0(0.62) 
I0704 07:57:02.617282 25348 net.cpp:1851] res5a_branch2a_param_0(0.62) 
I0704 07:57:02.617286 25348 net.cpp:1851] res5a_branch2b_param_0(0.62) 
I0704 07:57:02.617290 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.45921e+06/2.3599e+06) 0.618
I0704 07:57:02.617427 25348 solver.cpp:466] Iteration 36000, Testing net (#0)
I0704 07:57:04.260339 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8987
I0704 07:57:04.260359 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9954
I0704 07:57:04.260363 25348 solver.cpp:539]     Test net output #2: loss = 0.2169 (* 1 = 0.2169 loss)
I0704 07:57:04.280020 25348 solver.cpp:290] Iteration 36000 (26.8638 iter/s, 3.72248s/100 iter), loss = -2.5332e-07
I0704 07:57:04.280038 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:04.280051 25348 sgd_solver.cpp:106] Iteration 36000, lr = 0.004375
I0704 07:57:04.280585 25348 solver.cpp:375] Finding and applying sparsity: 0.64
I0704 07:57:05.510733 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:57:07.616991 25348 solver.cpp:290] Iteration 36100 (29.9683 iter/s, 3.33685s/100 iter), loss = -2.5332e-07
I0704 07:57:07.617025 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:07.617034 25348 sgd_solver.cpp:106] Iteration 36100, lr = 0.00435938
I0704 07:57:09.695698 25348 solver.cpp:290] Iteration 36200 (48.1091 iter/s, 2.07861s/100 iter), loss = -2.5332e-07
I0704 07:57:09.695727 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:09.695734 25348 sgd_solver.cpp:106] Iteration 36200, lr = 0.00434375
I0704 07:57:11.771351 25348 solver.cpp:290] Iteration 36300 (48.1798 iter/s, 2.07556s/100 iter), loss = -2.5332e-07
I0704 07:57:11.771374 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:11.771381 25348 sgd_solver.cpp:106] Iteration 36300, lr = 0.00432813
I0704 07:57:13.846949 25348 solver.cpp:290] Iteration 36400 (48.181 iter/s, 2.07551s/100 iter), loss = -2.5332e-07
I0704 07:57:13.846972 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:13.846979 25348 sgd_solver.cpp:106] Iteration 36400, lr = 0.0043125
I0704 07:57:15.917484 25348 solver.cpp:290] Iteration 36500 (48.2987 iter/s, 2.07045s/100 iter), loss = -2.5332e-07
I0704 07:57:15.917506 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:15.917515 25348 sgd_solver.cpp:106] Iteration 36500, lr = 0.00429688
I0704 07:57:17.988018 25348 solver.cpp:290] Iteration 36600 (48.2987 iter/s, 2.07045s/100 iter), loss = -2.5332e-07
I0704 07:57:17.988073 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:17.988081 25348 sgd_solver.cpp:106] Iteration 36600, lr = 0.00428125
I0704 07:57:20.060633 25348 solver.cpp:290] Iteration 36700 (48.251 iter/s, 2.0725s/100 iter), loss = -2.5332e-07
I0704 07:57:20.060655 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:20.060662 25348 sgd_solver.cpp:106] Iteration 36700, lr = 0.00426562
I0704 07:57:22.131702 25348 solver.cpp:290] Iteration 36800 (48.2862 iter/s, 2.07098s/100 iter), loss = -2.5332e-07
I0704 07:57:22.131724 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:22.131731 25348 sgd_solver.cpp:106] Iteration 36800, lr = 0.00425
I0704 07:57:24.203840 25348 solver.cpp:290] Iteration 36900 (48.2614 iter/s, 2.07205s/100 iter), loss = -2.5332e-07
I0704 07:57:24.203861 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:24.203867 25348 sgd_solver.cpp:106] Iteration 36900, lr = 0.00423437
I0704 07:57:26.255271 25348 solver.cpp:354] Sparsity after update:
I0704 07:57:26.256654 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:57:26.256661 25348 net.cpp:1851] conv1a_param_0(0.32) 
I0704 07:57:26.256669 25348 net.cpp:1851] conv1b_param_0(0.64) 
I0704 07:57:26.256671 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:57:26.256674 25348 net.cpp:1851] res2a_branch2a_param_0(0.64) 
I0704 07:57:26.256675 25348 net.cpp:1851] res2a_branch2b_param_0(0.64) 
I0704 07:57:26.256678 25348 net.cpp:1851] res3a_branch2a_param_0(0.64) 
I0704 07:57:26.256680 25348 net.cpp:1851] res3a_branch2b_param_0(0.64) 
I0704 07:57:26.256682 25348 net.cpp:1851] res4a_branch2a_param_0(0.64) 
I0704 07:57:26.256685 25348 net.cpp:1851] res4a_branch2b_param_0(0.64) 
I0704 07:57:26.256687 25348 net.cpp:1851] res5a_branch2a_param_0(0.64) 
I0704 07:57:26.256690 25348 net.cpp:1851] res5a_branch2b_param_0(0.64) 
I0704 07:57:26.256691 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.50629e+06/2.3599e+06) 0.638
I0704 07:57:26.256779 25348 solver.cpp:466] Iteration 37000, Testing net (#0)
I0704 07:57:27.900789 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8989
I0704 07:57:27.900806 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9954
I0704 07:57:27.900811 25348 solver.cpp:539]     Test net output #2: loss = 0.2078 (* 1 = 0.2078 loss)
I0704 07:57:27.920603 25348 solver.cpp:290] Iteration 37000 (26.9061 iter/s, 3.71663s/100 iter), loss = -2.5332e-07
I0704 07:57:27.920626 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:27.920632 25348 sgd_solver.cpp:106] Iteration 37000, lr = 0.00421875
I0704 07:57:27.921172 25348 solver.cpp:375] Finding and applying sparsity: 0.66
I0704 07:57:29.259896 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:57:31.373196 25348 solver.cpp:290] Iteration 37100 (28.9648 iter/s, 3.45247s/100 iter), loss = -2.5332e-07
I0704 07:57:31.373217 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:31.373225 25348 sgd_solver.cpp:106] Iteration 37100, lr = 0.00420313
I0704 07:57:33.463826 25348 solver.cpp:290] Iteration 37200 (47.8344 iter/s, 2.09055s/100 iter), loss = -2.5332e-07
I0704 07:57:33.463848 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:33.463855 25348 sgd_solver.cpp:106] Iteration 37200, lr = 0.0041875
I0704 07:57:35.534981 25348 solver.cpp:290] Iteration 37300 (48.2843 iter/s, 2.07107s/100 iter), loss = -2.5332e-07
I0704 07:57:35.535003 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:35.535010 25348 sgd_solver.cpp:106] Iteration 37300, lr = 0.00417187
I0704 07:57:37.612264 25348 solver.cpp:290] Iteration 37400 (48.1418 iter/s, 2.0772s/100 iter), loss = -2.5332e-07
I0704 07:57:37.612287 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:37.612293 25348 sgd_solver.cpp:106] Iteration 37400, lr = 0.00415625
I0704 07:57:39.694124 25348 solver.cpp:290] Iteration 37500 (48.0359 iter/s, 2.08178s/100 iter), loss = -2.5332e-07
I0704 07:57:39.694164 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:39.694170 25348 sgd_solver.cpp:106] Iteration 37500, lr = 0.00414062
I0704 07:57:41.767738 25348 solver.cpp:290] Iteration 37600 (48.2273 iter/s, 2.07351s/100 iter), loss = -2.5332e-07
I0704 07:57:41.767761 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:41.767767 25348 sgd_solver.cpp:106] Iteration 37600, lr = 0.004125
I0704 07:57:43.841697 25348 solver.cpp:290] Iteration 37700 (48.219 iter/s, 2.07387s/100 iter), loss = -2.5332e-07
I0704 07:57:43.841719 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:43.841727 25348 sgd_solver.cpp:106] Iteration 37700, lr = 0.00410937
I0704 07:57:45.911545 25348 solver.cpp:290] Iteration 37800 (48.3147 iter/s, 2.06976s/100 iter), loss = -2.5332e-07
I0704 07:57:45.911568 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:45.911577 25348 sgd_solver.cpp:106] Iteration 37800, lr = 0.00409375
I0704 07:57:47.981704 25348 solver.cpp:290] Iteration 37900 (48.3074 iter/s, 2.07007s/100 iter), loss = -2.5332e-07
I0704 07:57:47.981727 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:47.981734 25348 sgd_solver.cpp:106] Iteration 37900, lr = 0.00407812
I0704 07:57:50.035944 25348 solver.cpp:354] Sparsity after update:
I0704 07:57:50.037374 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:57:50.037382 25348 net.cpp:1851] conv1a_param_0(0.33) 
I0704 07:57:50.037389 25348 net.cpp:1851] conv1b_param_0(0.66) 
I0704 07:57:50.037391 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:57:50.037395 25348 net.cpp:1851] res2a_branch2a_param_0(0.66) 
I0704 07:57:50.037396 25348 net.cpp:1851] res2a_branch2b_param_0(0.66) 
I0704 07:57:50.037398 25348 net.cpp:1851] res3a_branch2a_param_0(0.66) 
I0704 07:57:50.037400 25348 net.cpp:1851] res3a_branch2b_param_0(0.66) 
I0704 07:57:50.037401 25348 net.cpp:1851] res4a_branch2a_param_0(0.66) 
I0704 07:57:50.037403 25348 net.cpp:1851] res4a_branch2b_param_0(0.66) 
I0704 07:57:50.037405 25348 net.cpp:1851] res5a_branch2a_param_0(0.66) 
I0704 07:57:50.037407 25348 net.cpp:1851] res5a_branch2b_param_0(0.66) 
I0704 07:57:50.037410 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.55336e+06/2.3599e+06) 0.658
I0704 07:57:50.037505 25348 solver.cpp:466] Iteration 38000, Testing net (#0)
I0704 07:57:51.680637 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9005
I0704 07:57:51.680655 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9962
I0704 07:57:51.680660 25348 solver.cpp:539]     Test net output #2: loss = 0.2165 (* 1 = 0.2165 loss)
I0704 07:57:51.700919 25348 solver.cpp:290] Iteration 38000 (26.8883 iter/s, 3.71908s/100 iter), loss = -2.5332e-07
I0704 07:57:51.700937 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:51.700951 25348 sgd_solver.cpp:106] Iteration 38000, lr = 0.0040625
I0704 07:57:51.701506 25348 solver.cpp:375] Finding and applying sparsity: 0.68
I0704 07:57:53.198724 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:57:55.321061 25348 solver.cpp:290] Iteration 38100 (27.6242 iter/s, 3.62002s/100 iter), loss = -2.5332e-07
I0704 07:57:55.321087 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:55.321095 25348 sgd_solver.cpp:106] Iteration 38100, lr = 0.00404688
I0704 07:57:57.397101 25348 solver.cpp:290] Iteration 38200 (48.1707 iter/s, 2.07595s/100 iter), loss = -2.5332e-07
I0704 07:57:57.397122 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:57.397130 25348 sgd_solver.cpp:106] Iteration 38200, lr = 0.00403125
I0704 07:57:59.477758 25348 solver.cpp:290] Iteration 38300 (48.0637 iter/s, 2.08057s/100 iter), loss = -2.5332e-07
I0704 07:57:59.477779 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:59.477787 25348 sgd_solver.cpp:106] Iteration 38300, lr = 0.00401562
I0704 07:58:01.549474 25348 solver.cpp:290] Iteration 38400 (48.2712 iter/s, 2.07163s/100 iter), loss = -2.5332e-07
I0704 07:58:01.549497 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:01.549505 25348 sgd_solver.cpp:106] Iteration 38400, lr = 0.004
I0704 07:58:03.632295 25348 solver.cpp:290] Iteration 38500 (48.0139 iter/s, 2.08273s/100 iter), loss = -2.5332e-07
I0704 07:58:03.632324 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:03.632334 25348 sgd_solver.cpp:106] Iteration 38500, lr = 0.00398437
I0704 07:58:05.707015 25348 solver.cpp:290] Iteration 38600 (48.2015 iter/s, 2.07463s/100 iter), loss = -2.5332e-07
I0704 07:58:05.707041 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:05.707051 25348 sgd_solver.cpp:106] Iteration 38600, lr = 0.00396875
I0704 07:58:07.780300 25348 solver.cpp:290] Iteration 38700 (48.2347 iter/s, 2.0732s/100 iter), loss = -2.5332e-07
I0704 07:58:07.780323 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:07.780329 25348 sgd_solver.cpp:106] Iteration 38700, lr = 0.00395312
I0704 07:58:09.852792 25348 solver.cpp:290] Iteration 38800 (48.2531 iter/s, 2.07241s/100 iter), loss = -2.5332e-07
I0704 07:58:09.852816 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:09.852823 25348 sgd_solver.cpp:106] Iteration 38800, lr = 0.0039375
I0704 07:58:11.921771 25348 solver.cpp:290] Iteration 38900 (48.3351 iter/s, 2.06889s/100 iter), loss = -2.5332e-07
I0704 07:58:11.921793 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:11.921800 25348 sgd_solver.cpp:106] Iteration 38900, lr = 0.00392187
I0704 07:58:13.972512 25348 solver.cpp:354] Sparsity after update:
I0704 07:58:13.973904 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:58:13.973912 25348 net.cpp:1851] conv1a_param_0(0.34) 
I0704 07:58:13.973920 25348 net.cpp:1851] conv1b_param_0(0.68) 
I0704 07:58:13.973922 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:58:13.973925 25348 net.cpp:1851] res2a_branch2a_param_0(0.68) 
I0704 07:58:13.973927 25348 net.cpp:1851] res2a_branch2b_param_0(0.68) 
I0704 07:58:13.973929 25348 net.cpp:1851] res3a_branch2a_param_0(0.68) 
I0704 07:58:13.973932 25348 net.cpp:1851] res3a_branch2b_param_0(0.68) 
I0704 07:58:13.973933 25348 net.cpp:1851] res4a_branch2a_param_0(0.68) 
I0704 07:58:13.973937 25348 net.cpp:1851] res4a_branch2b_param_0(0.68) 
I0704 07:58:13.973938 25348 net.cpp:1851] res5a_branch2a_param_0(0.68) 
I0704 07:58:13.973940 25348 net.cpp:1851] res5a_branch2b_param_0(0.68) 
I0704 07:58:13.973942 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.60043e+06/2.3599e+06) 0.678
I0704 07:58:13.974030 25348 solver.cpp:466] Iteration 39000, Testing net (#0)
I0704 07:58:15.616323 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9034
I0704 07:58:15.616340 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9963
I0704 07:58:15.616345 25348 solver.cpp:539]     Test net output #2: loss = 0.1934 (* 1 = 0.1934 loss)
I0704 07:58:15.636051 25348 solver.cpp:290] Iteration 39000 (26.9241 iter/s, 3.71415s/100 iter), loss = -2.5332e-07
I0704 07:58:15.636067 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:15.636080 25348 sgd_solver.cpp:106] Iteration 39000, lr = 0.00390625
I0704 07:58:15.636621 25348 solver.cpp:375] Finding and applying sparsity: 0.7
I0704 07:58:17.203591 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:58:19.311373 25348 solver.cpp:290] Iteration 39100 (27.2094 iter/s, 3.6752s/100 iter), loss = -2.5332e-07
I0704 07:58:19.311398 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:19.311406 25348 sgd_solver.cpp:106] Iteration 39100, lr = 0.00389063
I0704 07:58:21.381268 25348 solver.cpp:290] Iteration 39200 (48.3137 iter/s, 2.06981s/100 iter), loss = -2.5332e-07
I0704 07:58:21.381322 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:21.381330 25348 sgd_solver.cpp:106] Iteration 39200, lr = 0.003875
I0704 07:58:23.460855 25348 solver.cpp:290] Iteration 39300 (48.0892 iter/s, 2.07947s/100 iter), loss = -2.5332e-07
I0704 07:58:23.460888 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:23.460897 25348 sgd_solver.cpp:106] Iteration 39300, lr = 0.00385938
I0704 07:58:25.572104 25348 solver.cpp:290] Iteration 39400 (47.3675 iter/s, 2.11115s/100 iter), loss = -2.5332e-07
I0704 07:58:25.572134 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:25.572139 25348 sgd_solver.cpp:106] Iteration 39400, lr = 0.00384375
I0704 07:58:27.670536 25348 solver.cpp:290] Iteration 39500 (47.6567 iter/s, 2.09834s/100 iter), loss = -2.5332e-07
I0704 07:58:27.670559 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:27.670567 25348 sgd_solver.cpp:106] Iteration 39500, lr = 0.00382812
I0704 07:58:29.744562 25348 solver.cpp:290] Iteration 39600 (48.2174 iter/s, 2.07394s/100 iter), loss = -2.5332e-07
I0704 07:58:29.744586 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:29.744596 25348 sgd_solver.cpp:106] Iteration 39600, lr = 0.0038125
I0704 07:58:31.821180 25348 solver.cpp:290] Iteration 39700 (48.1572 iter/s, 2.07653s/100 iter), loss = -2.5332e-07
I0704 07:58:31.821202 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:31.821209 25348 sgd_solver.cpp:106] Iteration 39700, lr = 0.00379687
I0704 07:58:33.905793 25348 solver.cpp:290] Iteration 39800 (47.9725 iter/s, 2.08453s/100 iter), loss = -2.5332e-07
I0704 07:58:33.905815 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:33.905822 25348 sgd_solver.cpp:106] Iteration 39800, lr = 0.00378125
I0704 07:58:35.977787 25348 solver.cpp:290] Iteration 39900 (48.2647 iter/s, 2.07191s/100 iter), loss = -2.5332e-07
I0704 07:58:35.977808 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:35.977814 25348 sgd_solver.cpp:106] Iteration 39900, lr = 0.00376562
I0704 07:58:38.035850 25348 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_40000.caffemodel
I0704 07:58:38.053175 25348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_40000.solverstate
I0704 07:58:38.060578 25348 solver.cpp:354] Sparsity after update:
I0704 07:58:38.061509 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:58:38.061518 25348 net.cpp:1851] conv1a_param_0(0.35) 
I0704 07:58:38.061527 25348 net.cpp:1851] conv1b_param_0(0.7) 
I0704 07:58:38.061529 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:58:38.061532 25348 net.cpp:1851] res2a_branch2a_param_0(0.7) 
I0704 07:58:38.061534 25348 net.cpp:1851] res2a_branch2b_param_0(0.7) 
I0704 07:58:38.061537 25348 net.cpp:1851] res3a_branch2a_param_0(0.7) 
I0704 07:58:38.061540 25348 net.cpp:1851] res3a_branch2b_param_0(0.7) 
I0704 07:58:38.061542 25348 net.cpp:1851] res4a_branch2a_param_0(0.7) 
I0704 07:58:38.061544 25348 net.cpp:1851] res4a_branch2b_param_0(0.7) 
I0704 07:58:38.061547 25348 net.cpp:1851] res5a_branch2a_param_0(0.7) 
I0704 07:58:38.061549 25348 net.cpp:1851] res5a_branch2b_param_0(0.7) 
I0704 07:58:38.061553 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.6475e+06/2.3599e+06) 0.698
I0704 07:58:38.061655 25348 solver.cpp:466] Iteration 40000, Testing net (#0)
I0704 07:58:39.701607 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8875
I0704 07:58:39.701627 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.995
I0704 07:58:39.701632 25348 solver.cpp:539]     Test net output #2: loss = 0.2593 (* 1 = 0.2593 loss)
I0704 07:58:39.721287 25348 solver.cpp:290] Iteration 40000 (26.7139 iter/s, 3.74337s/100 iter), loss = -2.5332e-07
I0704 07:58:39.721307 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:39.721325 25348 sgd_solver.cpp:106] Iteration 40000, lr = 0.00375
I0704 07:58:39.721863 25348 solver.cpp:375] Finding and applying sparsity: 0.72
I0704 07:58:41.394453 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:58:43.517740 25348 solver.cpp:290] Iteration 40100 (26.3413 iter/s, 3.79632s/100 iter), loss = -2.5332e-07
I0704 07:58:43.517763 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:43.517771 25348 sgd_solver.cpp:106] Iteration 40100, lr = 0.00373438
I0704 07:58:45.595809 25348 solver.cpp:290] Iteration 40200 (48.1236 iter/s, 2.07798s/100 iter), loss = -2.5332e-07
I0704 07:58:45.595831 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:45.595839 25348 sgd_solver.cpp:106] Iteration 40200, lr = 0.00371875
I0704 07:58:47.667376 25348 solver.cpp:290] Iteration 40300 (48.2746 iter/s, 2.07148s/100 iter), loss = -2.5332e-07
I0704 07:58:47.667402 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:47.667409 25348 sgd_solver.cpp:106] Iteration 40300, lr = 0.00370313
I0704 07:58:49.747005 25348 solver.cpp:290] Iteration 40400 (48.0876 iter/s, 2.07954s/100 iter), loss = -2.5332e-07
I0704 07:58:49.747028 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:49.747035 25348 sgd_solver.cpp:106] Iteration 40400, lr = 0.0036875
I0704 07:58:51.834978 25348 solver.cpp:290] Iteration 40500 (47.8954 iter/s, 2.08788s/100 iter), loss = 0.0476188
I0704 07:58:51.835095 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:58:51.835104 25348 sgd_solver.cpp:106] Iteration 40500, lr = 0.00367187
I0704 07:58:53.912271 25348 solver.cpp:290] Iteration 40600 (48.1437 iter/s, 2.07712s/100 iter), loss = -2.5332e-07
I0704 07:58:53.912295 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:53.912304 25348 sgd_solver.cpp:106] Iteration 40600, lr = 0.00365625
I0704 07:58:55.983777 25348 solver.cpp:290] Iteration 40700 (48.2762 iter/s, 2.07142s/100 iter), loss = -2.5332e-07
I0704 07:58:55.983800 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:55.983808 25348 sgd_solver.cpp:106] Iteration 40700, lr = 0.00364062
I0704 07:58:58.055027 25348 solver.cpp:290] Iteration 40800 (48.2821 iter/s, 2.07116s/100 iter), loss = -2.5332e-07
I0704 07:58:58.055048 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:58.055055 25348 sgd_solver.cpp:106] Iteration 40800, lr = 0.003625
I0704 07:59:00.127351 25348 solver.cpp:290] Iteration 40900 (48.257 iter/s, 2.07224s/100 iter), loss = -2.5332e-07
I0704 07:59:00.127372 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:00.127380 25348 sgd_solver.cpp:106] Iteration 40900, lr = 0.00360937
I0704 07:59:02.177968 25348 solver.cpp:354] Sparsity after update:
I0704 07:59:02.179388 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:59:02.179396 25348 net.cpp:1851] conv1a_param_0(0.36) 
I0704 07:59:02.179406 25348 net.cpp:1851] conv1b_param_0(0.72) 
I0704 07:59:02.179410 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:59:02.179414 25348 net.cpp:1851] res2a_branch2a_param_0(0.72) 
I0704 07:59:02.179419 25348 net.cpp:1851] res2a_branch2b_param_0(0.72) 
I0704 07:59:02.179422 25348 net.cpp:1851] res3a_branch2a_param_0(0.72) 
I0704 07:59:02.179426 25348 net.cpp:1851] res3a_branch2b_param_0(0.72) 
I0704 07:59:02.179430 25348 net.cpp:1851] res4a_branch2a_param_0(0.72) 
I0704 07:59:02.179435 25348 net.cpp:1851] res4a_branch2b_param_0(0.72) 
I0704 07:59:02.179438 25348 net.cpp:1851] res5a_branch2a_param_0(0.72) 
I0704 07:59:02.179442 25348 net.cpp:1851] res5a_branch2b_param_0(0.72) 
I0704 07:59:02.179446 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.69458e+06/2.3599e+06) 0.718
I0704 07:59:02.179538 25348 solver.cpp:466] Iteration 41000, Testing net (#0)
I0704 07:59:03.838712 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8925
I0704 07:59:03.838733 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9958
I0704 07:59:03.838738 25348 solver.cpp:539]     Test net output #2: loss = 0.2211 (* 1 = 0.2211 loss)
I0704 07:59:03.858335 25348 solver.cpp:290] Iteration 41000 (26.8035 iter/s, 3.73086s/100 iter), loss = -2.5332e-07
I0704 07:59:03.858350 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:03.858366 25348 sgd_solver.cpp:106] Iteration 41000, lr = 0.00359375
I0704 07:59:03.858898 25348 solver.cpp:375] Finding and applying sparsity: 0.74
I0704 07:59:05.645359 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:59:07.756130 25348 solver.cpp:290] Iteration 41100 (25.6564 iter/s, 3.89767s/100 iter), loss = -2.5332e-07
I0704 07:59:07.756155 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:07.756162 25348 sgd_solver.cpp:106] Iteration 41100, lr = 0.00357813
I0704 07:59:09.847944 25348 solver.cpp:290] Iteration 41200 (47.8075 iter/s, 2.09172s/100 iter), loss = -2.5332e-07
I0704 07:59:09.847973 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:09.847983 25348 sgd_solver.cpp:106] Iteration 41200, lr = 0.0035625
I0704 07:59:11.927654 25348 solver.cpp:290] Iteration 41300 (48.0858 iter/s, 2.07962s/100 iter), loss = -2.5332e-07
I0704 07:59:11.927687 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:11.927698 25348 sgd_solver.cpp:106] Iteration 41300, lr = 0.00354687
I0704 07:59:14.001346 25348 solver.cpp:290] Iteration 41400 (48.2254 iter/s, 2.0736s/100 iter), loss = -2.5332e-07
I0704 07:59:14.001389 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:14.001399 25348 sgd_solver.cpp:106] Iteration 41400, lr = 0.00353125
I0704 07:59:16.087136 25348 solver.cpp:290] Iteration 41500 (47.9459 iter/s, 2.08569s/100 iter), loss = -2.5332e-07
I0704 07:59:16.087158 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:16.087165 25348 sgd_solver.cpp:106] Iteration 41500, lr = 0.00351562
I0704 07:59:18.178632 25348 solver.cpp:290] Iteration 41600 (47.8148 iter/s, 2.0914s/100 iter), loss = -2.5332e-07
I0704 07:59:18.178663 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:18.178673 25348 sgd_solver.cpp:106] Iteration 41600, lr = 0.0035
I0704 07:59:20.261375 25348 solver.cpp:290] Iteration 41700 (48.0158 iter/s, 2.08265s/100 iter), loss = -2.5332e-07
I0704 07:59:20.261401 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:20.261410 25348 sgd_solver.cpp:106] Iteration 41700, lr = 0.00348437
I0704 07:59:22.356503 25348 solver.cpp:290] Iteration 41800 (47.7318 iter/s, 2.09504s/100 iter), loss = -2.5332e-07
I0704 07:59:22.356593 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:22.356601 25348 sgd_solver.cpp:106] Iteration 41800, lr = 0.00346875
I0704 07:59:24.445863 25348 solver.cpp:290] Iteration 41900 (47.8651 iter/s, 2.0892s/100 iter), loss = -2.5332e-07
I0704 07:59:24.445888 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:24.445897 25348 sgd_solver.cpp:106] Iteration 41900, lr = 0.00345312
I0704 07:59:26.513860 25348 solver.cpp:354] Sparsity after update:
I0704 07:59:26.515285 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:59:26.515293 25348 net.cpp:1851] conv1a_param_0(0.37) 
I0704 07:59:26.515300 25348 net.cpp:1851] conv1b_param_0(0.74) 
I0704 07:59:26.515302 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:59:26.515305 25348 net.cpp:1851] res2a_branch2a_param_0(0.74) 
I0704 07:59:26.515306 25348 net.cpp:1851] res2a_branch2b_param_0(0.74) 
I0704 07:59:26.515308 25348 net.cpp:1851] res3a_branch2a_param_0(0.74) 
I0704 07:59:26.515311 25348 net.cpp:1851] res3a_branch2b_param_0(0.74) 
I0704 07:59:26.515312 25348 net.cpp:1851] res4a_branch2a_param_0(0.74) 
I0704 07:59:26.515314 25348 net.cpp:1851] res4a_branch2b_param_0(0.74) 
I0704 07:59:26.515316 25348 net.cpp:1851] res5a_branch2a_param_0(0.74) 
I0704 07:59:26.515318 25348 net.cpp:1851] res5a_branch2b_param_0(0.74) 
I0704 07:59:26.515321 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.74164e+06/2.3599e+06) 0.738
I0704 07:59:26.515408 25348 solver.cpp:466] Iteration 42000, Testing net (#0)
I0704 07:59:28.162900 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8879
I0704 07:59:28.162927 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9949
I0704 07:59:28.162935 25348 solver.cpp:539]     Test net output #2: loss = 0.2223 (* 1 = 0.2223 loss)
I0704 07:59:28.183058 25348 solver.cpp:290] Iteration 42000 (26.759 iter/s, 3.73706s/100 iter), loss = -2.5332e-07
I0704 07:59:28.183089 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:28.183099 25348 sgd_solver.cpp:106] Iteration 42000, lr = 0.0034375
I0704 07:59:28.183832 25348 solver.cpp:375] Finding and applying sparsity: 0.76
I0704 07:59:29.995735 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:59:32.105505 25348 solver.cpp:290] Iteration 42100 (25.4952 iter/s, 3.9223s/100 iter), loss = -2.5332e-07
I0704 07:59:32.105528 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:32.105535 25348 sgd_solver.cpp:106] Iteration 42100, lr = 0.00342188
I0704 07:59:34.224274 25348 solver.cpp:290] Iteration 42200 (47.1992 iter/s, 2.11868s/100 iter), loss = -2.5332e-07
I0704 07:59:34.224297 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:34.224303 25348 sgd_solver.cpp:106] Iteration 42200, lr = 0.00340625
I0704 07:59:36.300348 25348 solver.cpp:290] Iteration 42300 (48.1698 iter/s, 2.07599s/100 iter), loss = 0.0476188
I0704 07:59:36.300369 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:59:36.300376 25348 sgd_solver.cpp:106] Iteration 42300, lr = 0.00339063
I0704 07:59:38.371667 25348 solver.cpp:290] Iteration 42400 (48.2804 iter/s, 2.07123s/100 iter), loss = -2.5332e-07
I0704 07:59:38.371688 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:38.371695 25348 sgd_solver.cpp:106] Iteration 42400, lr = 0.003375
I0704 07:59:40.446701 25348 solver.cpp:290] Iteration 42500 (48.194 iter/s, 2.07495s/100 iter), loss = -2.5332e-07
I0704 07:59:40.446727 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:40.446735 25348 sgd_solver.cpp:106] Iteration 42500, lr = 0.00335937
I0704 07:59:42.530836 25348 solver.cpp:290] Iteration 42600 (47.9835 iter/s, 2.08405s/100 iter), loss = -2.5332e-07
I0704 07:59:42.530859 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:42.530865 25348 sgd_solver.cpp:106] Iteration 42600, lr = 0.00334375
I0704 07:59:44.615247 25348 solver.cpp:290] Iteration 42700 (47.9772 iter/s, 2.08432s/100 iter), loss = -2.5332e-07
I0704 07:59:44.615290 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:44.615299 25348 sgd_solver.cpp:106] Iteration 42700, lr = 0.00332812
I0704 07:59:46.694444 25348 solver.cpp:290] Iteration 42800 (48.0979 iter/s, 2.07909s/100 iter), loss = -2.5332e-07
I0704 07:59:46.694466 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:46.694473 25348 sgd_solver.cpp:106] Iteration 42800, lr = 0.0033125
I0704 07:59:48.783452 25348 solver.cpp:290] Iteration 42900 (47.8716 iter/s, 2.08892s/100 iter), loss = -2.5332e-07
I0704 07:59:48.783474 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:48.783481 25348 sgd_solver.cpp:106] Iteration 42900, lr = 0.00329687
I0704 07:59:50.858790 25348 solver.cpp:354] Sparsity after update:
I0704 07:59:50.860355 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:59:50.860364 25348 net.cpp:1851] conv1a_param_0(0.38) 
I0704 07:59:50.860375 25348 net.cpp:1851] conv1b_param_0(0.76) 
I0704 07:59:50.860380 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:59:50.860384 25348 net.cpp:1851] res2a_branch2a_param_0(0.76) 
I0704 07:59:50.860388 25348 net.cpp:1851] res2a_branch2b_param_0(0.76) 
I0704 07:59:50.860391 25348 net.cpp:1851] res3a_branch2a_param_0(0.76) 
I0704 07:59:50.860396 25348 net.cpp:1851] res3a_branch2b_param_0(0.76) 
I0704 07:59:50.860399 25348 net.cpp:1851] res4a_branch2a_param_0(0.76) 
I0704 07:59:50.860404 25348 net.cpp:1851] res4a_branch2b_param_0(0.76) 
I0704 07:59:50.860407 25348 net.cpp:1851] res5a_branch2a_param_0(0.76) 
I0704 07:59:50.860411 25348 net.cpp:1851] res5a_branch2b_param_0(0.76) 
I0704 07:59:50.860414 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.78872e+06/2.3599e+06) 0.758
I0704 07:59:50.860551 25348 solver.cpp:466] Iteration 43000, Testing net (#0)
I0704 07:59:52.508286 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8888
I0704 07:59:52.508338 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9941
I0704 07:59:52.508345 25348 solver.cpp:539]     Test net output #2: loss = 0.2339 (* 1 = 0.2339 loss)
I0704 07:59:52.528066 25348 solver.cpp:290] Iteration 43000 (26.7059 iter/s, 3.74448s/100 iter), loss = -2.5332e-07
I0704 07:59:52.528084 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:52.528096 25348 sgd_solver.cpp:106] Iteration 43000, lr = 0.00328125
I0704 07:59:52.528620 25348 solver.cpp:375] Finding and applying sparsity: 0.78
I0704 07:59:54.320488 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:59:56.438158 25348 solver.cpp:290] Iteration 43100 (25.5757 iter/s, 3.90996s/100 iter), loss = 0.0476188
I0704 07:59:56.438186 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:59:56.438194 25348 sgd_solver.cpp:106] Iteration 43100, lr = 0.00326563
I0704 07:59:58.522915 25348 solver.cpp:290] Iteration 43200 (47.9693 iter/s, 2.08467s/100 iter), loss = 0.0476188
I0704 07:59:58.522936 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:59:58.522943 25348 sgd_solver.cpp:106] Iteration 43200, lr = 0.00325
I0704 08:00:00.594065 25348 solver.cpp:290] Iteration 43300 (48.2844 iter/s, 2.07106s/100 iter), loss = -2.5332e-07
I0704 08:00:00.594087 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:00.594094 25348 sgd_solver.cpp:106] Iteration 43300, lr = 0.00323438
I0704 08:00:02.674525 25348 solver.cpp:290] Iteration 43400 (48.0685 iter/s, 2.08037s/100 iter), loss = -2.5332e-07
I0704 08:00:02.674561 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:02.674571 25348 sgd_solver.cpp:106] Iteration 43400, lr = 0.00321875
I0704 08:00:04.748981 25348 solver.cpp:290] Iteration 43500 (48.2077 iter/s, 2.07436s/100 iter), loss = -2.5332e-07
I0704 08:00:04.749004 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:04.749011 25348 sgd_solver.cpp:106] Iteration 43500, lr = 0.00320312
I0704 08:00:06.817955 25348 solver.cpp:290] Iteration 43600 (48.3352 iter/s, 2.06889s/100 iter), loss = -2.5332e-07
I0704 08:00:06.817980 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:06.817987 25348 sgd_solver.cpp:106] Iteration 43600, lr = 0.0031875
I0704 08:00:08.892122 25348 solver.cpp:290] Iteration 43700 (48.2142 iter/s, 2.07408s/100 iter), loss = -2.5332e-07
I0704 08:00:08.892145 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:08.892154 25348 sgd_solver.cpp:106] Iteration 43700, lr = 0.00317187
I0704 08:00:10.970543 25348 solver.cpp:290] Iteration 43800 (48.1155 iter/s, 2.07833s/100 iter), loss = -2.5332e-07
I0704 08:00:10.970568 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:10.970577 25348 sgd_solver.cpp:106] Iteration 43800, lr = 0.00315625
I0704 08:00:13.045102 25348 solver.cpp:290] Iteration 43900 (48.2051 iter/s, 2.07447s/100 iter), loss = -2.5332e-07
I0704 08:00:13.045125 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:13.045131 25348 sgd_solver.cpp:106] Iteration 43900, lr = 0.00314062
I0704 08:00:15.098747 25348 solver.cpp:354] Sparsity after update:
I0704 08:00:15.100158 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:00:15.100165 25348 net.cpp:1851] conv1a_param_0(0.39) 
I0704 08:00:15.100172 25348 net.cpp:1851] conv1b_param_0(0.78) 
I0704 08:00:15.100175 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:00:15.100178 25348 net.cpp:1851] res2a_branch2a_param_0(0.78) 
I0704 08:00:15.100179 25348 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0704 08:00:15.100181 25348 net.cpp:1851] res3a_branch2a_param_0(0.78) 
I0704 08:00:15.100183 25348 net.cpp:1851] res3a_branch2b_param_0(0.78) 
I0704 08:00:15.100185 25348 net.cpp:1851] res4a_branch2a_param_0(0.78) 
I0704 08:00:15.100186 25348 net.cpp:1851] res4a_branch2b_param_0(0.78) 
I0704 08:00:15.100188 25348 net.cpp:1851] res5a_branch2a_param_0(0.78) 
I0704 08:00:15.100201 25348 net.cpp:1851] res5a_branch2b_param_0(0.78) 
I0704 08:00:15.100205 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.83579e+06/2.3599e+06) 0.778
I0704 08:00:15.100296 25348 solver.cpp:466] Iteration 44000, Testing net (#0)
I0704 08:00:16.742854 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.882
I0704 08:00:16.742873 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9922
I0704 08:00:16.742879 25348 solver.cpp:539]     Test net output #2: loss = 0.2602 (* 1 = 0.2602 loss)
I0704 08:00:16.762612 25348 solver.cpp:290] Iteration 44000 (26.9007 iter/s, 3.71738s/100 iter), loss = -2.5332e-07
I0704 08:00:16.762630 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:16.762641 25348 sgd_solver.cpp:106] Iteration 44000, lr = 0.003125
I0704 08:00:16.763146 25348 solver.cpp:375] Finding and applying sparsity: 0.8
I0704 08:00:18.703826 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 08:00:20.803694 25348 solver.cpp:290] Iteration 44100 (24.7467 iter/s, 4.04095s/100 iter), loss = 0.142857
I0704 08:00:20.803716 25348 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 08:00:20.803725 25348 sgd_solver.cpp:106] Iteration 44100, lr = 0.00310938
I0704 08:00:22.874913 25348 solver.cpp:290] Iteration 44200 (48.2827 iter/s, 2.07113s/100 iter), loss = -2.5332e-07
I0704 08:00:22.874984 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:22.874992 25348 sgd_solver.cpp:106] Iteration 44200, lr = 0.00309375
I0704 08:00:24.949051 25348 solver.cpp:290] Iteration 44300 (48.2159 iter/s, 2.074s/100 iter), loss = -2.5332e-07
I0704 08:00:24.949076 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:24.949085 25348 sgd_solver.cpp:106] Iteration 44300, lr = 0.00307812
I0704 08:00:27.021338 25348 solver.cpp:290] Iteration 44400 (48.2579 iter/s, 2.0722s/100 iter), loss = -2.5332e-07
I0704 08:00:27.021365 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:27.021375 25348 sgd_solver.cpp:106] Iteration 44400, lr = 0.0030625
I0704 08:00:29.100705 25348 solver.cpp:290] Iteration 44500 (48.0936 iter/s, 2.07928s/100 iter), loss = -2.5332e-07
I0704 08:00:29.100726 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:29.100733 25348 sgd_solver.cpp:106] Iteration 44500, lr = 0.00304687
I0704 08:00:31.175432 25348 solver.cpp:290] Iteration 44600 (48.2011 iter/s, 2.07464s/100 iter), loss = -2.57045e-07
I0704 08:00:31.175454 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:31.175462 25348 sgd_solver.cpp:106] Iteration 44600, lr = 0.00303125
I0704 08:00:33.248586 25348 solver.cpp:290] Iteration 44700 (48.2377 iter/s, 2.07307s/100 iter), loss = -2.5332e-07
I0704 08:00:33.248620 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:33.248630 25348 sgd_solver.cpp:106] Iteration 44700, lr = 0.00301562
I0704 08:00:35.349755 25348 solver.cpp:290] Iteration 44800 (47.5948 iter/s, 2.10107s/100 iter), loss = -2.5332e-07
I0704 08:00:35.349787 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:35.349798 25348 sgd_solver.cpp:106] Iteration 44800, lr = 0.003
I0704 08:00:37.419860 25348 solver.cpp:290] Iteration 44900 (48.3089 iter/s, 2.07001s/100 iter), loss = -2.5332e-07
I0704 08:00:37.419886 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:37.419895 25348 sgd_solver.cpp:106] Iteration 44900, lr = 0.00298437
I0704 08:00:39.474380 25348 solver.cpp:354] Sparsity after update:
I0704 08:00:39.475797 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:00:39.475805 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:00:39.475813 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:00:39.475816 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:00:39.475817 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:00:39.475819 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:00:39.475821 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:00:39.475823 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:00:39.475826 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:00:39.475827 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:00:39.475829 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:00:39.475831 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:00:39.475833 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:00:39.475961 25348 solver.cpp:466] Iteration 45000, Testing net (#0)
I0704 08:00:41.118183 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8523
I0704 08:00:41.118202 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.991
I0704 08:00:41.118207 25348 solver.cpp:539]     Test net output #2: loss = 0.366 (* 1 = 0.366 loss)
I0704 08:00:41.138463 25348 solver.cpp:290] Iteration 45000 (26.8928 iter/s, 3.71847s/100 iter), loss = -2.5332e-07
I0704 08:00:41.138478 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:41.138494 25348 sgd_solver.cpp:106] Iteration 45000, lr = 0.00296875
I0704 08:00:43.210952 25348 solver.cpp:290] Iteration 45100 (48.253 iter/s, 2.07241s/100 iter), loss = -2.5332e-07
I0704 08:00:43.210975 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:43.210981 25348 sgd_solver.cpp:106] Iteration 45100, lr = 0.00295313
I0704 08:00:45.284284 25348 solver.cpp:290] Iteration 45200 (48.2336 iter/s, 2.07324s/100 iter), loss = -2.5332e-07
I0704 08:00:45.284307 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:45.284313 25348 sgd_solver.cpp:106] Iteration 45200, lr = 0.0029375
I0704 08:00:47.356322 25348 solver.cpp:290] Iteration 45300 (48.2637 iter/s, 2.07195s/100 iter), loss = -2.5332e-07
I0704 08:00:47.356343 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:47.356350 25348 sgd_solver.cpp:106] Iteration 45300, lr = 0.00292188
I0704 08:00:49.429651 25348 solver.cpp:290] Iteration 45400 (48.2336 iter/s, 2.07324s/100 iter), loss = -2.5332e-07
I0704 08:00:49.429673 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:49.429680 25348 sgd_solver.cpp:106] Iteration 45400, lr = 0.00290625
I0704 08:00:51.501461 25348 solver.cpp:290] Iteration 45500 (48.269 iter/s, 2.07172s/100 iter), loss = -2.5332e-07
I0704 08:00:51.501484 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:51.501492 25348 sgd_solver.cpp:106] Iteration 45500, lr = 0.00289063
I0704 08:00:53.574278 25348 solver.cpp:290] Iteration 45600 (48.2456 iter/s, 2.07273s/100 iter), loss = -2.5332e-07
I0704 08:00:53.574337 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:53.574347 25348 sgd_solver.cpp:106] Iteration 45600, lr = 0.002875
I0704 08:00:55.648495 25348 solver.cpp:290] Iteration 45700 (48.2137 iter/s, 2.0741s/100 iter), loss = -2.5332e-07
I0704 08:00:55.648517 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:55.648524 25348 sgd_solver.cpp:106] Iteration 45700, lr = 0.00285937
I0704 08:00:57.721370 25348 solver.cpp:290] Iteration 45800 (48.2442 iter/s, 2.07279s/100 iter), loss = -2.5332e-07
I0704 08:00:57.721393 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:57.721400 25348 sgd_solver.cpp:106] Iteration 45800, lr = 0.00284375
I0704 08:00:59.803424 25348 solver.cpp:290] Iteration 45900 (48.0315 iter/s, 2.08197s/100 iter), loss = 0.0476188
I0704 08:00:59.803450 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 08:00:59.803459 25348 sgd_solver.cpp:106] Iteration 45900, lr = 0.00282812
I0704 08:01:01.855618 25348 solver.cpp:354] Sparsity after update:
I0704 08:01:01.857007 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:01:01.857014 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:01:01.857022 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:01:01.857023 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:01:01.857025 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:01:01.857028 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:01:01.857029 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:01:01.857031 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:01:01.857033 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:01:01.857035 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:01:01.857038 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:01:01.857039 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:01:01.857041 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:01:01.857127 25348 solver.cpp:466] Iteration 46000, Testing net (#0)
I0704 08:01:03.498353 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8824
I0704 08:01:03.498373 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9959
I0704 08:01:03.498378 25348 solver.cpp:539]     Test net output #2: loss = 0.258 (* 1 = 0.258 loss)
I0704 08:01:03.518144 25348 solver.cpp:290] Iteration 46000 (26.9209 iter/s, 3.71459s/100 iter), loss = -2.5332e-07
I0704 08:01:03.518162 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:03.518174 25348 sgd_solver.cpp:106] Iteration 46000, lr = 0.0028125
I0704 08:01:05.590648 25348 solver.cpp:290] Iteration 46100 (48.2527 iter/s, 2.07242s/100 iter), loss = -2.5332e-07
I0704 08:01:05.590670 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:05.590677 25348 sgd_solver.cpp:106] Iteration 46100, lr = 0.00279688
I0704 08:01:07.671330 25348 solver.cpp:290] Iteration 46200 (48.0632 iter/s, 2.0806s/100 iter), loss = -2.5332e-07
I0704 08:01:07.671351 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:07.671358 25348 sgd_solver.cpp:106] Iteration 46200, lr = 0.00278125
I0704 08:01:09.747411 25348 solver.cpp:290] Iteration 46300 (48.1697 iter/s, 2.076s/100 iter), loss = -2.5332e-07
I0704 08:01:09.747434 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:09.747440 25348 sgd_solver.cpp:106] Iteration 46300, lr = 0.00276563
I0704 08:01:11.822172 25348 solver.cpp:290] Iteration 46400 (48.2003 iter/s, 2.07467s/100 iter), loss = -2.5332e-07
I0704 08:01:11.822193 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:11.822201 25348 sgd_solver.cpp:106] Iteration 46400, lr = 0.00275
I0704 08:01:13.894567 25348 solver.cpp:290] Iteration 46500 (48.2553 iter/s, 2.07231s/100 iter), loss = -2.5332e-07
I0704 08:01:13.894588 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:13.894596 25348 sgd_solver.cpp:106] Iteration 46500, lr = 0.00273437
I0704 08:01:15.968050 25348 solver.cpp:290] Iteration 46600 (48.2301 iter/s, 2.07339s/100 iter), loss = -2.5332e-07
I0704 08:01:15.968077 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:15.968086 25348 sgd_solver.cpp:106] Iteration 46600, lr = 0.00271875
I0704 08:01:18.040457 25348 solver.cpp:290] Iteration 46700 (48.2552 iter/s, 2.07232s/100 iter), loss = -2.5332e-07
I0704 08:01:18.040482 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:18.040489 25348 sgd_solver.cpp:106] Iteration 46700, lr = 0.00270312
I0704 08:01:20.117025 25348 solver.cpp:290] Iteration 46800 (48.1584 iter/s, 2.07648s/100 iter), loss = -2.5332e-07
I0704 08:01:20.117049 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:20.117055 25348 sgd_solver.cpp:106] Iteration 46800, lr = 0.0026875
I0704 08:01:22.193729 25348 solver.cpp:290] Iteration 46900 (48.1553 iter/s, 2.07662s/100 iter), loss = -2.5332e-07
I0704 08:01:22.193750 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:22.193758 25348 sgd_solver.cpp:106] Iteration 46900, lr = 0.00267187
I0704 08:01:24.247692 25348 solver.cpp:354] Sparsity after update:
I0704 08:01:24.249078 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:01:24.249086 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:01:24.249097 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:01:24.249101 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:01:24.249106 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:01:24.249110 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:01:24.249114 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:01:24.249119 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:01:24.249121 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:01:24.249125 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:01:24.249130 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:01:24.249132 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:01:24.249136 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:01:24.249228 25348 solver.cpp:466] Iteration 47000, Testing net (#0)
I0704 08:01:25.892215 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.904
I0704 08:01:25.892235 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9957
I0704 08:01:25.892241 25348 solver.cpp:539]     Test net output #2: loss = 0.1985 (* 1 = 0.1985 loss)
I0704 08:01:25.911939 25348 solver.cpp:290] Iteration 47000 (26.8956 iter/s, 3.71808s/100 iter), loss = -2.5332e-07
I0704 08:01:25.911957 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:25.911969 25348 sgd_solver.cpp:106] Iteration 47000, lr = 0.00265625
I0704 08:01:27.990439 25348 solver.cpp:290] Iteration 47100 (48.1136 iter/s, 2.07842s/100 iter), loss = -2.5332e-07
I0704 08:01:27.990463 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:27.990471 25348 sgd_solver.cpp:106] Iteration 47100, lr = 0.00264063
I0704 08:01:30.063395 25348 solver.cpp:290] Iteration 47200 (48.2423 iter/s, 2.07287s/100 iter), loss = -2.5332e-07
I0704 08:01:30.063416 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:30.063423 25348 sgd_solver.cpp:106] Iteration 47200, lr = 0.002625
I0704 08:01:32.134570 25348 solver.cpp:290] Iteration 47300 (48.2838 iter/s, 2.07109s/100 iter), loss = -2.5332e-07
I0704 08:01:32.134593 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:32.134600 25348 sgd_solver.cpp:106] Iteration 47300, lr = 0.00260938
I0704 08:01:34.215946 25348 solver.cpp:290] Iteration 47400 (48.0471 iter/s, 2.08129s/100 iter), loss = -2.5332e-07
I0704 08:01:34.215970 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:34.215978 25348 sgd_solver.cpp:106] Iteration 47400, lr = 0.00259375
I0704 08:01:36.295346 25348 solver.cpp:290] Iteration 47500 (48.0928 iter/s, 2.07931s/100 iter), loss = -2.5332e-07
I0704 08:01:36.295367 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:36.295375 25348 sgd_solver.cpp:106] Iteration 47500, lr = 0.00257812
I0704 08:01:38.368974 25348 solver.cpp:290] Iteration 47600 (48.2267 iter/s, 2.07354s/100 iter), loss = -2.5332e-07
I0704 08:01:38.369000 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:38.369009 25348 sgd_solver.cpp:106] Iteration 47600, lr = 0.0025625
I0704 08:01:40.440454 25348 solver.cpp:290] Iteration 47700 (48.2767 iter/s, 2.07139s/100 iter), loss = -2.5332e-07
I0704 08:01:40.440477 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:40.440485 25348 sgd_solver.cpp:106] Iteration 47700, lr = 0.00254687
I0704 08:01:42.516633 25348 solver.cpp:290] Iteration 47800 (48.1675 iter/s, 2.07609s/100 iter), loss = -2.5332e-07
I0704 08:01:42.516655 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:42.516661 25348 sgd_solver.cpp:106] Iteration 47800, lr = 0.00253125
I0704 08:01:44.586904 25348 solver.cpp:290] Iteration 47900 (48.3049 iter/s, 2.07018s/100 iter), loss = -2.5332e-07
I0704 08:01:44.586925 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:44.586956 25348 sgd_solver.cpp:106] Iteration 47900, lr = 0.00251562
I0704 08:01:46.638833 25348 solver.cpp:354] Sparsity after update:
I0704 08:01:46.640235 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:01:46.640242 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:01:46.640249 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:01:46.640251 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:01:46.640254 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:01:46.640255 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:01:46.640257 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:01:46.640259 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:01:46.640261 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:01:46.640264 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:01:46.640265 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:01:46.640267 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:01:46.640269 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:01:46.640359 25348 solver.cpp:466] Iteration 48000, Testing net (#0)
I0704 08:01:48.282627 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.904
I0704 08:01:48.282647 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9948
I0704 08:01:48.282654 25348 solver.cpp:539]     Test net output #2: loss = 0.2056 (* 1 = 0.2056 loss)
I0704 08:01:48.302356 25348 solver.cpp:290] Iteration 48000 (26.9155 iter/s, 3.71532s/100 iter), loss = -2.5332e-07
I0704 08:01:48.302373 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:48.302389 25348 sgd_solver.cpp:106] Iteration 48000, lr = 0.0025
I0704 08:01:50.379235 25348 solver.cpp:290] Iteration 48100 (48.1511 iter/s, 2.0768s/100 iter), loss = -2.5332e-07
I0704 08:01:50.379257 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:50.379264 25348 sgd_solver.cpp:106] Iteration 48100, lr = 0.00248438
I0704 08:01:52.458756 25348 solver.cpp:290] Iteration 48200 (48.09 iter/s, 2.07943s/100 iter), loss = -2.5332e-07
I0704 08:01:52.458783 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:52.458791 25348 sgd_solver.cpp:106] Iteration 48200, lr = 0.00246875
I0704 08:01:54.528787 25348 solver.cpp:290] Iteration 48300 (48.3105 iter/s, 2.06994s/100 iter), loss = -2.5332e-07
I0704 08:01:54.528863 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:54.528872 25348 sgd_solver.cpp:106] Iteration 48300, lr = 0.00245313
I0704 08:01:56.598598 25348 solver.cpp:290] Iteration 48400 (48.3168 iter/s, 2.06967s/100 iter), loss = -2.5332e-07
I0704 08:01:56.598620 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:56.598629 25348 sgd_solver.cpp:106] Iteration 48400, lr = 0.0024375
I0704 08:01:58.666834 25348 solver.cpp:290] Iteration 48500 (48.3524 iter/s, 2.06815s/100 iter), loss = -2.5332e-07
I0704 08:01:58.666860 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:58.666869 25348 sgd_solver.cpp:106] Iteration 48500, lr = 0.00242188
I0704 08:02:00.735390 25348 solver.cpp:290] Iteration 48600 (48.345 iter/s, 2.06847s/100 iter), loss = -2.5332e-07
I0704 08:02:00.735416 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:00.735425 25348 sgd_solver.cpp:106] Iteration 48600, lr = 0.00240625
I0704 08:02:02.811290 25348 solver.cpp:290] Iteration 48700 (48.1739 iter/s, 2.07581s/100 iter), loss = -2.5332e-07
I0704 08:02:02.811313 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:02.811321 25348 sgd_solver.cpp:106] Iteration 48700, lr = 0.00239062
I0704 08:02:04.884481 25348 solver.cpp:290] Iteration 48800 (48.2368 iter/s, 2.07311s/100 iter), loss = -2.5332e-07
I0704 08:02:04.884505 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:04.884510 25348 sgd_solver.cpp:106] Iteration 48800, lr = 0.002375
I0704 08:02:06.954874 25348 solver.cpp:290] Iteration 48900 (48.3021 iter/s, 2.0703s/100 iter), loss = -2.5332e-07
I0704 08:02:06.954895 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:06.954902 25348 sgd_solver.cpp:106] Iteration 48900, lr = 0.00235937
I0704 08:02:09.005672 25348 solver.cpp:354] Sparsity after update:
I0704 08:02:09.007086 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:02:09.007093 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:02:09.007100 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:02:09.007103 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:02:09.007105 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:02:09.007107 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:02:09.007109 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:02:09.007112 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:02:09.007113 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:02:09.007115 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:02:09.007117 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:02:09.007119 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:02:09.007120 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:02:09.007218 25348 solver.cpp:466] Iteration 49000, Testing net (#0)
I0704 08:02:10.649369 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9086
I0704 08:02:10.649387 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9967
I0704 08:02:10.649392 25348 solver.cpp:539]     Test net output #2: loss = 0.1895 (* 1 = 0.1895 loss)
I0704 08:02:10.669046 25348 solver.cpp:290] Iteration 49000 (26.9248 iter/s, 3.71404s/100 iter), loss = -2.5332e-07
I0704 08:02:10.669064 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:10.669076 25348 sgd_solver.cpp:106] Iteration 49000, lr = 0.00234375
I0704 08:02:12.740764 25348 solver.cpp:290] Iteration 49100 (48.271 iter/s, 2.07164s/100 iter), loss = -2.5332e-07
I0704 08:02:12.740787 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:12.740794 25348 sgd_solver.cpp:106] Iteration 49100, lr = 0.00232813
I0704 08:02:14.811699 25348 solver.cpp:290] Iteration 49200 (48.2894 iter/s, 2.07085s/100 iter), loss = -2.5332e-07
I0704 08:02:14.811722 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:14.811728 25348 sgd_solver.cpp:106] Iteration 49200, lr = 0.0023125
I0704 08:02:16.881594 25348 solver.cpp:290] Iteration 49300 (48.3136 iter/s, 2.06981s/100 iter), loss = -2.5332e-07
I0704 08:02:16.881616 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:16.881623 25348 sgd_solver.cpp:106] Iteration 49300, lr = 0.00229687
I0704 08:02:18.956161 25348 solver.cpp:290] Iteration 49400 (48.2049 iter/s, 2.07448s/100 iter), loss = -2.5332e-07
I0704 08:02:18.956182 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:18.956188 25348 sgd_solver.cpp:106] Iteration 49400, lr = 0.00228125
I0704 08:02:21.030202 25348 solver.cpp:290] Iteration 49500 (48.217 iter/s, 2.07396s/100 iter), loss = -2.5332e-07
I0704 08:02:21.030226 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:21.030236 25348 sgd_solver.cpp:106] Iteration 49500, lr = 0.00226562
I0704 08:02:23.102231 25348 solver.cpp:290] Iteration 49600 (48.2639 iter/s, 2.07194s/100 iter), loss = -2.5332e-07
I0704 08:02:23.102253 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:23.102259 25348 sgd_solver.cpp:106] Iteration 49600, lr = 0.00225
I0704 08:02:25.171476 25348 solver.cpp:290] Iteration 49700 (48.3289 iter/s, 2.06916s/100 iter), loss = -2.5332e-07
I0704 08:02:25.171527 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:25.171535 25348 sgd_solver.cpp:106] Iteration 49700, lr = 0.00223437
I0704 08:02:27.245054 25348 solver.cpp:290] Iteration 49800 (48.2285 iter/s, 2.07346s/100 iter), loss = -2.5332e-07
I0704 08:02:27.245079 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:27.245086 25348 sgd_solver.cpp:106] Iteration 49800, lr = 0.00221875
I0704 08:02:29.316233 25348 solver.cpp:290] Iteration 49900 (48.2837 iter/s, 2.07109s/100 iter), loss = -2.5332e-07
I0704 08:02:29.316256 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:29.316263 25348 sgd_solver.cpp:106] Iteration 49900, lr = 0.00220312
I0704 08:02:31.367466 25348 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_50000.caffemodel
I0704 08:02:31.384235 25348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_50000.solverstate
I0704 08:02:31.391520 25348 solver.cpp:354] Sparsity after update:
I0704 08:02:31.392475 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:02:31.392483 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:02:31.392491 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:02:31.392493 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:02:31.392496 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:02:31.392498 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:02:31.392499 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:02:31.392501 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:02:31.392503 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:02:31.392505 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:02:31.392508 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:02:31.392509 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:02:31.392511 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:02:31.392607 25348 solver.cpp:466] Iteration 50000, Testing net (#0)
I0704 08:02:33.034262 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.909
I0704 08:02:33.034282 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9967
I0704 08:02:33.034287 25348 solver.cpp:539]     Test net output #2: loss = 0.1869 (* 1 = 0.1869 loss)
I0704 08:02:33.054738 25348 solver.cpp:290] Iteration 50000 (26.7496 iter/s, 3.73837s/100 iter), loss = -2.5332e-07
I0704 08:02:33.054755 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:33.054766 25348 sgd_solver.cpp:106] Iteration 50000, lr = 0.0021875
I0704 08:02:35.150411 25348 solver.cpp:290] Iteration 50100 (47.7192 iter/s, 2.09559s/100 iter), loss = -2.5332e-07
I0704 08:02:35.150434 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:35.150440 25348 sgd_solver.cpp:106] Iteration 50100, lr = 0.00217188
I0704 08:02:37.219923 25348 solver.cpp:290] Iteration 50200 (48.3227 iter/s, 2.06942s/100 iter), loss = -2.5332e-07
I0704 08:02:37.219947 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:37.219955 25348 sgd_solver.cpp:106] Iteration 50200, lr = 0.00215625
I0704 08:02:39.293373 25348 solver.cpp:290] Iteration 50300 (48.2308 iter/s, 2.07336s/100 iter), loss = -2.5332e-07
I0704 08:02:39.293395 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:39.293402 25348 sgd_solver.cpp:106] Iteration 50300, lr = 0.00214063
I0704 08:02:41.362026 25348 solver.cpp:290] Iteration 50400 (48.3427 iter/s, 2.06857s/100 iter), loss = -2.5332e-07
I0704 08:02:41.362061 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:41.362071 25348 sgd_solver.cpp:106] Iteration 50400, lr = 0.002125
I0704 08:02:43.434051 25348 solver.cpp:290] Iteration 50500 (48.2643 iter/s, 2.07193s/100 iter), loss = -2.5332e-07
I0704 08:02:43.434078 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:43.434103 25348 sgd_solver.cpp:106] Iteration 50500, lr = 0.00210937
I0704 08:02:45.504045 25348 solver.cpp:290] Iteration 50600 (48.3114 iter/s, 2.0699s/100 iter), loss = -2.5332e-07
I0704 08:02:45.504068 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:45.504075 25348 sgd_solver.cpp:106] Iteration 50600, lr = 0.00209375
I0704 08:02:47.576961 25348 solver.cpp:290] Iteration 50700 (48.2432 iter/s, 2.07283s/100 iter), loss = -2.5332e-07
I0704 08:02:47.576983 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:47.576992 25348 sgd_solver.cpp:106] Iteration 50700, lr = 0.00207812
I0704 08:02:49.649227 25348 solver.cpp:290] Iteration 50800 (48.2584 iter/s, 2.07218s/100 iter), loss = -2.5332e-07
I0704 08:02:49.649250 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:49.649256 25348 sgd_solver.cpp:106] Iteration 50800, lr = 0.0020625
I0704 08:02:51.720217 25348 solver.cpp:290] Iteration 50900 (48.2881 iter/s, 2.0709s/100 iter), loss = -2.5332e-07
I0704 08:02:51.720240 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:51.720247 25348 sgd_solver.cpp:106] Iteration 50900, lr = 0.00204687
I0704 08:02:53.770952 25348 solver.cpp:354] Sparsity after update:
I0704 08:02:53.772341 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:02:53.772347 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:02:53.772354 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:02:53.772356 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:02:53.772359 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:02:53.772361 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:02:53.772364 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:02:53.772367 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:02:53.772368 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:02:53.772370 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:02:53.772373 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:02:53.772375 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:02:53.772378 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:02:53.772466 25348 solver.cpp:466] Iteration 51000, Testing net (#0)
I0704 08:02:55.414664 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.908
I0704 08:02:55.414722 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9963
I0704 08:02:55.414729 25348 solver.cpp:539]     Test net output #2: loss = 0.1874 (* 1 = 0.1874 loss)
I0704 08:02:55.434356 25348 solver.cpp:290] Iteration 51000 (26.9251 iter/s, 3.71401s/100 iter), loss = -2.5332e-07
I0704 08:02:55.434373 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:55.434389 25348 sgd_solver.cpp:106] Iteration 51000, lr = 0.00203125
I0704 08:02:57.504086 25348 solver.cpp:290] Iteration 51100 (48.3174 iter/s, 2.06965s/100 iter), loss = -2.5332e-07
I0704 08:02:57.504108 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:57.504115 25348 sgd_solver.cpp:106] Iteration 51100, lr = 0.00201563
I0704 08:02:59.577030 25348 solver.cpp:290] Iteration 51200 (48.2426 iter/s, 2.07286s/100 iter), loss = -2.5332e-07
I0704 08:02:59.577054 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:59.577064 25348 sgd_solver.cpp:106] Iteration 51200, lr = 0.002
I0704 08:03:01.647611 25348 solver.cpp:290] Iteration 51300 (48.2977 iter/s, 2.07049s/100 iter), loss = -2.5332e-07
I0704 08:03:01.647634 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:01.647639 25348 sgd_solver.cpp:106] Iteration 51300, lr = 0.00198438
I0704 08:03:03.717363 25348 solver.cpp:290] Iteration 51400 (48.317 iter/s, 2.06967s/100 iter), loss = -2.5332e-07
I0704 08:03:03.717386 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:03.717393 25348 sgd_solver.cpp:106] Iteration 51400, lr = 0.00196875
I0704 08:03:05.789881 25348 solver.cpp:290] Iteration 51500 (48.2525 iter/s, 2.07243s/100 iter), loss = -2.5332e-07
I0704 08:03:05.789904 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:05.789911 25348 sgd_solver.cpp:106] Iteration 51500, lr = 0.00195312
I0704 08:03:07.866638 25348 solver.cpp:290] Iteration 51600 (48.1541 iter/s, 2.07667s/100 iter), loss = -2.5332e-07
I0704 08:03:07.866662 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:07.866669 25348 sgd_solver.cpp:106] Iteration 51600, lr = 0.0019375
I0704 08:03:09.942589 25348 solver.cpp:290] Iteration 51700 (48.1727 iter/s, 2.07586s/100 iter), loss = -2.5332e-07
I0704 08:03:09.942613 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:09.942621 25348 sgd_solver.cpp:106] Iteration 51700, lr = 0.00192187
I0704 08:03:12.018702 25348 solver.cpp:290] Iteration 51800 (48.1689 iter/s, 2.07603s/100 iter), loss = -2.5332e-07
I0704 08:03:12.018725 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:12.018731 25348 sgd_solver.cpp:106] Iteration 51800, lr = 0.00190625
I0704 08:03:14.090252 25348 solver.cpp:290] Iteration 51900 (48.2751 iter/s, 2.07146s/100 iter), loss = -2.5332e-07
I0704 08:03:14.090276 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:14.090281 25348 sgd_solver.cpp:106] Iteration 51900, lr = 0.00189062
I0704 08:03:16.140334 25348 solver.cpp:354] Sparsity after update:
I0704 08:03:16.141726 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:03:16.141732 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:03:16.141739 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:03:16.141742 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:03:16.141744 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:03:16.141746 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:03:16.141748 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:03:16.141751 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:03:16.141752 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:03:16.141754 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:03:16.141757 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:03:16.141758 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:03:16.141760 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:03:16.141860 25348 solver.cpp:466] Iteration 52000, Testing net (#0)
I0704 08:03:17.783646 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.911
I0704 08:03:17.783665 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9964
I0704 08:03:17.783670 25348 solver.cpp:539]     Test net output #2: loss = 0.1871 (* 1 = 0.1871 loss)
I0704 08:03:17.803330 25348 solver.cpp:290] Iteration 52000 (26.9328 iter/s, 3.71295s/100 iter), loss = -2.5332e-07
I0704 08:03:17.803349 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:17.803359 25348 sgd_solver.cpp:106] Iteration 52000, lr = 0.001875
I0704 08:03:19.874171 25348 solver.cpp:290] Iteration 52100 (48.2915 iter/s, 2.07076s/100 iter), loss = -2.5332e-07
I0704 08:03:19.874193 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:19.874200 25348 sgd_solver.cpp:106] Iteration 52100, lr = 0.00185938
I0704 08:03:21.944897 25348 solver.cpp:290] Iteration 52200 (48.2943 iter/s, 2.07064s/100 iter), loss = -2.5332e-07
I0704 08:03:21.944922 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:21.944931 25348 sgd_solver.cpp:106] Iteration 52200, lr = 0.00184375
I0704 08:03:24.015745 25348 solver.cpp:290] Iteration 52300 (48.2915 iter/s, 2.07076s/100 iter), loss = -2.5332e-07
I0704 08:03:24.015779 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:24.015789 25348 sgd_solver.cpp:106] Iteration 52300, lr = 0.00182813
I0704 08:03:26.088711 25348 solver.cpp:290] Iteration 52400 (48.2423 iter/s, 2.07287s/100 iter), loss = -2.5332e-07
I0704 08:03:26.088783 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:26.088795 25348 sgd_solver.cpp:106] Iteration 52400, lr = 0.0018125
I0704 08:03:28.157580 25348 solver.cpp:290] Iteration 52500 (48.3386 iter/s, 2.06874s/100 iter), loss = -2.5332e-07
I0704 08:03:28.157605 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:28.157613 25348 sgd_solver.cpp:106] Iteration 52500, lr = 0.00179687
I0704 08:03:30.230128 25348 solver.cpp:290] Iteration 52600 (48.2519 iter/s, 2.07246s/100 iter), loss = -2.5332e-07
I0704 08:03:30.230159 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:30.230168 25348 sgd_solver.cpp:106] Iteration 52600, lr = 0.00178125
I0704 08:03:32.305192 25348 solver.cpp:290] Iteration 52700 (48.1934 iter/s, 2.07497s/100 iter), loss = -2.5332e-07
I0704 08:03:32.305214 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:32.305222 25348 sgd_solver.cpp:106] Iteration 52700, lr = 0.00176562
I0704 08:03:34.391844 25348 solver.cpp:290] Iteration 52800 (47.9257 iter/s, 2.08657s/100 iter), loss = -2.5332e-07
I0704 08:03:34.391865 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:34.391872 25348 sgd_solver.cpp:106] Iteration 52800, lr = 0.00175
I0704 08:03:36.463367 25348 solver.cpp:290] Iteration 52900 (48.2757 iter/s, 2.07144s/100 iter), loss = -2.5332e-07
I0704 08:03:36.463389 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:36.463397 25348 sgd_solver.cpp:106] Iteration 52900, lr = 0.00173437
I0704 08:03:38.513762 25348 solver.cpp:354] Sparsity after update:
I0704 08:03:38.515188 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:03:38.515195 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:03:38.515203 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:03:38.515206 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:03:38.515209 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:03:38.515213 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:03:38.515215 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:03:38.515218 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:03:38.515220 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:03:38.515223 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:03:38.515225 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:03:38.515228 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:03:38.515229 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:03:38.515316 25348 solver.cpp:466] Iteration 53000, Testing net (#0)
I0704 08:03:40.156697 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9098
I0704 08:03:40.156715 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9971
I0704 08:03:40.156720 25348 solver.cpp:539]     Test net output #2: loss = 0.1755 (* 1 = 0.1755 loss)
I0704 08:03:40.176373 25348 solver.cpp:290] Iteration 53000 (26.9333 iter/s, 3.71288s/100 iter), loss = -2.5332e-07
I0704 08:03:40.176388 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:40.176403 25348 sgd_solver.cpp:106] Iteration 53000, lr = 0.00171875
I0704 08:03:42.249794 25348 solver.cpp:290] Iteration 53100 (48.2314 iter/s, 2.07334s/100 iter), loss = -2.5332e-07
I0704 08:03:42.249821 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:42.249830 25348 sgd_solver.cpp:106] Iteration 53100, lr = 0.00170313
I0704 08:03:44.319524 25348 solver.cpp:290] Iteration 53200 (48.3176 iter/s, 2.06964s/100 iter), loss = -2.5332e-07
I0704 08:03:44.319545 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:44.319552 25348 sgd_solver.cpp:106] Iteration 53200, lr = 0.0016875
I0704 08:03:46.391937 25348 solver.cpp:290] Iteration 53300 (48.255 iter/s, 2.07233s/100 iter), loss = -2.5332e-07
I0704 08:03:46.391959 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:46.391966 25348 sgd_solver.cpp:106] Iteration 53300, lr = 0.00167188
I0704 08:03:48.468461 25348 solver.cpp:290] Iteration 53400 (48.1594 iter/s, 2.07644s/100 iter), loss = -2.5332e-07
I0704 08:03:48.468483 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:48.468490 25348 sgd_solver.cpp:106] Iteration 53400, lr = 0.00165625
I0704 08:03:50.539525 25348 solver.cpp:290] Iteration 53500 (48.2864 iter/s, 2.07098s/100 iter), loss = -2.5332e-07
I0704 08:03:50.539548 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:50.539556 25348 sgd_solver.cpp:106] Iteration 53500, lr = 0.00164062
I0704 08:03:52.615279 25348 solver.cpp:290] Iteration 53600 (48.1772 iter/s, 2.07567s/100 iter), loss = -2.5332e-07
I0704 08:03:52.615303 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:52.615309 25348 sgd_solver.cpp:106] Iteration 53600, lr = 0.001625
I0704 08:03:54.684483 25348 solver.cpp:290] Iteration 53700 (48.3298 iter/s, 2.06912s/100 iter), loss = -2.5332e-07
I0704 08:03:54.684505 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:54.684512 25348 sgd_solver.cpp:106] Iteration 53700, lr = 0.00160937
I0704 08:03:56.757211 25348 solver.cpp:290] Iteration 53800 (48.2476 iter/s, 2.07264s/100 iter), loss = -2.5332e-07
I0704 08:03:56.757290 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:56.757298 25348 sgd_solver.cpp:106] Iteration 53800, lr = 0.00159375
I0704 08:03:58.828912 25348 solver.cpp:290] Iteration 53900 (48.2728 iter/s, 2.07156s/100 iter), loss = -2.5332e-07
I0704 08:03:58.828935 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:58.828943 25348 sgd_solver.cpp:106] Iteration 53900, lr = 0.00157812
I0704 08:04:00.880380 25348 solver.cpp:354] Sparsity after update:
I0704 08:04:00.881773 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:04:00.881780 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:04:00.881790 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:04:00.881794 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:04:00.881799 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:04:00.881804 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:04:00.881808 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:04:00.881813 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:04:00.881816 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:04:00.881821 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:04:00.881825 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:04:00.881830 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:04:00.881834 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:04:00.881927 25348 solver.cpp:466] Iteration 54000, Testing net (#0)
I0704 08:04:02.522730 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.911
I0704 08:04:02.522749 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9959
I0704 08:04:02.522754 25348 solver.cpp:539]     Test net output #2: loss = 0.1867 (* 1 = 0.1867 loss)
I0704 08:04:02.542512 25348 solver.cpp:290] Iteration 54000 (26.929 iter/s, 3.71347s/100 iter), loss = -2.5332e-07
I0704 08:04:02.542529 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:02.542542 25348 sgd_solver.cpp:106] Iteration 54000, lr = 0.0015625
I0704 08:04:04.620020 25348 solver.cpp:290] Iteration 54100 (48.1365 iter/s, 2.07743s/100 iter), loss = -2.5332e-07
I0704 08:04:04.620043 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:04.620049 25348 sgd_solver.cpp:106] Iteration 54100, lr = 0.00154688
I0704 08:04:06.694241 25348 solver.cpp:290] Iteration 54200 (48.2129 iter/s, 2.07413s/100 iter), loss = -2.5332e-07
I0704 08:04:06.694272 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:06.694281 25348 sgd_solver.cpp:106] Iteration 54200, lr = 0.00153125
I0704 08:04:08.769978 25348 solver.cpp:290] Iteration 54300 (48.1779 iter/s, 2.07564s/100 iter), loss = -2.5332e-07
I0704 08:04:08.770006 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:08.770016 25348 sgd_solver.cpp:106] Iteration 54300, lr = 0.00151563
I0704 08:04:10.839845 25348 solver.cpp:290] Iteration 54400 (48.3144 iter/s, 2.06978s/100 iter), loss = -2.5332e-07
I0704 08:04:10.839874 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:10.839881 25348 sgd_solver.cpp:106] Iteration 54400, lr = 0.0015
I0704 08:04:12.912994 25348 solver.cpp:290] Iteration 54500 (48.2379 iter/s, 2.07306s/100 iter), loss = -2.5332e-07
I0704 08:04:12.913017 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:12.913024 25348 sgd_solver.cpp:106] Iteration 54500, lr = 0.00148437
I0704 08:04:14.986187 25348 solver.cpp:290] Iteration 54600 (48.2368 iter/s, 2.07311s/100 iter), loss = -2.5332e-07
I0704 08:04:14.986212 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:14.986222 25348 sgd_solver.cpp:106] Iteration 54600, lr = 0.00146875
I0704 08:04:17.056661 25348 solver.cpp:290] Iteration 54700 (48.3002 iter/s, 2.07039s/100 iter), loss = -2.5332e-07
I0704 08:04:17.056685 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:17.056694 25348 sgd_solver.cpp:106] Iteration 54700, lr = 0.00145312
I0704 08:04:19.131032 25348 solver.cpp:290] Iteration 54800 (48.2094 iter/s, 2.07428s/100 iter), loss = -2.5332e-07
I0704 08:04:19.131057 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:19.131065 25348 sgd_solver.cpp:106] Iteration 54800, lr = 0.0014375
I0704 08:04:21.201319 25348 solver.cpp:290] Iteration 54900 (48.3045 iter/s, 2.0702s/100 iter), loss = -2.5332e-07
I0704 08:04:21.201342 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:21.201350 25348 sgd_solver.cpp:106] Iteration 54900, lr = 0.00142187
I0704 08:04:23.255657 25348 solver.cpp:354] Sparsity after update:
I0704 08:04:23.257051 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:04:23.257058 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:04:23.257068 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:04:23.257073 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:04:23.257078 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:04:23.257082 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:04:23.257086 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:04:23.257089 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:04:23.257094 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:04:23.257098 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:04:23.257102 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:04:23.257105 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:04:23.257110 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:04:23.257201 25348 solver.cpp:466] Iteration 55000, Testing net (#0)
I0704 08:04:24.899339 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9125
I0704 08:04:24.899358 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9966
I0704 08:04:24.899364 25348 solver.cpp:539]     Test net output #2: loss = 0.1754 (* 1 = 0.1754 loss)
I0704 08:04:24.919103 25348 solver.cpp:290] Iteration 55000 (26.8987 iter/s, 3.71765s/100 iter), loss = -2.5332e-07
I0704 08:04:24.919123 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:24.919134 25348 sgd_solver.cpp:106] Iteration 55000, lr = 0.00140625
I0704 08:04:26.997964 25348 solver.cpp:290] Iteration 55100 (48.1052 iter/s, 2.07878s/100 iter), loss = -2.5332e-07
I0704 08:04:26.998051 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:26.998061 25348 sgd_solver.cpp:106] Iteration 55100, lr = 0.00139063
I0704 08:04:29.068749 25348 solver.cpp:290] Iteration 55200 (48.2943 iter/s, 2.07064s/100 iter), loss = -2.5332e-07
I0704 08:04:29.068773 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:29.068779 25348 sgd_solver.cpp:106] Iteration 55200, lr = 0.001375
I0704 08:04:31.139755 25348 solver.cpp:290] Iteration 55300 (48.2878 iter/s, 2.07092s/100 iter), loss = -2.5332e-07
I0704 08:04:31.139778 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:31.139785 25348 sgd_solver.cpp:106] Iteration 55300, lr = 0.00135938
I0704 08:04:33.217241 25348 solver.cpp:290] Iteration 55400 (48.1371 iter/s, 2.0774s/100 iter), loss = -2.5332e-07
I0704 08:04:33.217265 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:33.217274 25348 sgd_solver.cpp:106] Iteration 55400, lr = 0.00134375
I0704 08:04:35.330760 25348 solver.cpp:290] Iteration 55500 (47.3164 iter/s, 2.11343s/100 iter), loss = -2.5332e-07
I0704 08:04:35.330780 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:35.330790 25348 sgd_solver.cpp:106] Iteration 55500, lr = 0.00132813
I0704 08:04:37.400964 25348 solver.cpp:290] Iteration 55600 (48.3064 iter/s, 2.07012s/100 iter), loss = -2.5332e-07
I0704 08:04:37.400986 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:37.400992 25348 sgd_solver.cpp:106] Iteration 55600, lr = 0.0013125
I0704 08:04:39.473687 25348 solver.cpp:290] Iteration 55700 (48.2478 iter/s, 2.07264s/100 iter), loss = -2.5332e-07
I0704 08:04:39.473711 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:39.473721 25348 sgd_solver.cpp:106] Iteration 55700, lr = 0.00129687
I0704 08:04:41.549100 25348 solver.cpp:290] Iteration 55800 (48.1852 iter/s, 2.07533s/100 iter), loss = -2.5332e-07
I0704 08:04:41.549124 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:41.549129 25348 sgd_solver.cpp:106] Iteration 55800, lr = 0.00128125
I0704 08:04:43.622618 25348 solver.cpp:290] Iteration 55900 (48.2292 iter/s, 2.07343s/100 iter), loss = -2.5332e-07
I0704 08:04:43.622640 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:43.622648 25348 sgd_solver.cpp:106] Iteration 55900, lr = 0.00126562
I0704 08:04:45.677562 25348 solver.cpp:354] Sparsity after update:
I0704 08:04:45.678825 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:04:45.678833 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:04:45.678839 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:04:45.678843 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:04:45.678844 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:04:45.678846 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:04:45.678848 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:04:45.678850 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:04:45.678853 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:04:45.678854 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:04:45.678856 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:04:45.678858 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:04:45.678860 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:04:45.678946 25348 solver.cpp:466] Iteration 56000, Testing net (#0)
I0704 08:04:47.320372 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9119
I0704 08:04:47.320392 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9968
I0704 08:04:47.320397 25348 solver.cpp:539]     Test net output #2: loss = 0.1705 (* 1 = 0.1705 loss)
I0704 08:04:47.340554 25348 solver.cpp:290] Iteration 56000 (26.8976 iter/s, 3.71781s/100 iter), loss = -2.5332e-07
I0704 08:04:47.340572 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:47.340584 25348 sgd_solver.cpp:106] Iteration 56000, lr = 0.00125
I0704 08:04:49.414075 25348 solver.cpp:290] Iteration 56100 (48.229 iter/s, 2.07344s/100 iter), loss = -2.5332e-07
I0704 08:04:49.414098 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:49.414104 25348 sgd_solver.cpp:106] Iteration 56100, lr = 0.00123438
I0704 08:04:51.487831 25348 solver.cpp:290] Iteration 56200 (48.2237 iter/s, 2.07367s/100 iter), loss = -2.5332e-07
I0704 08:04:51.487857 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:51.487864 25348 sgd_solver.cpp:106] Iteration 56200, lr = 0.00121875
I0704 08:04:53.559255 25348 solver.cpp:290] Iteration 56300 (48.2781 iter/s, 2.07133s/100 iter), loss = -2.5332e-07
I0704 08:04:53.559283 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:53.559293 25348 sgd_solver.cpp:106] Iteration 56300, lr = 0.00120313
I0704 08:04:55.631796 25348 solver.cpp:290] Iteration 56400 (48.2521 iter/s, 2.07245s/100 iter), loss = -2.5332e-07
I0704 08:04:55.631820 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:55.631829 25348 sgd_solver.cpp:106] Iteration 56400, lr = 0.0011875
I0704 08:04:57.702633 25348 solver.cpp:290] Iteration 56500 (48.2917 iter/s, 2.07075s/100 iter), loss = -2.5332e-07
I0704 08:04:57.702713 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:57.702724 25348 sgd_solver.cpp:106] Iteration 56500, lr = 0.00117187
I0704 08:04:59.777993 25348 solver.cpp:290] Iteration 56600 (48.1877 iter/s, 2.07522s/100 iter), loss = -2.5332e-07
I0704 08:04:59.778015 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:59.778023 25348 sgd_solver.cpp:106] Iteration 56600, lr = 0.00115625
I0704 08:05:01.846787 25348 solver.cpp:290] Iteration 56700 (48.3393 iter/s, 2.06871s/100 iter), loss = -2.5332e-07
I0704 08:05:01.846812 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:01.846817 25348 sgd_solver.cpp:106] Iteration 56700, lr = 0.00114062
I0704 08:05:03.918018 25348 solver.cpp:290] Iteration 56800 (48.2825 iter/s, 2.07114s/100 iter), loss = -2.5332e-07
I0704 08:05:03.918040 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:03.918046 25348 sgd_solver.cpp:106] Iteration 56800, lr = 0.001125
I0704 08:05:05.988574 25348 solver.cpp:290] Iteration 56900 (48.2982 iter/s, 2.07047s/100 iter), loss = -2.5332e-07
I0704 08:05:05.988596 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:05.988603 25348 sgd_solver.cpp:106] Iteration 56900, lr = 0.00110937
I0704 08:05:08.044821 25348 solver.cpp:354] Sparsity after update:
I0704 08:05:08.046197 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:05:08.046205 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:05:08.046214 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:05:08.046219 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:05:08.046224 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:05:08.046228 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:05:08.046233 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:05:08.046238 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:05:08.046242 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:05:08.046247 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:05:08.046252 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:05:08.046255 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:05:08.046259 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:05:08.046351 25348 solver.cpp:466] Iteration 57000, Testing net (#0)
I0704 08:05:09.689340 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9119
I0704 08:05:09.689359 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9961
I0704 08:05:09.689365 25348 solver.cpp:539]     Test net output #2: loss = 0.1757 (* 1 = 0.1757 loss)
I0704 08:05:09.709202 25348 solver.cpp:290] Iteration 57000 (26.8781 iter/s, 3.7205s/100 iter), loss = -2.5332e-07
I0704 08:05:09.709218 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:09.709233 25348 sgd_solver.cpp:106] Iteration 57000, lr = 0.00109375
I0704 08:05:11.779510 25348 solver.cpp:290] Iteration 57100 (48.3039 iter/s, 2.07023s/100 iter), loss = -2.5332e-07
I0704 08:05:11.779533 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:11.779541 25348 sgd_solver.cpp:106] Iteration 57100, lr = 0.00107813
I0704 08:05:13.853374 25348 solver.cpp:290] Iteration 57200 (48.2211 iter/s, 2.07378s/100 iter), loss = -2.5332e-07
I0704 08:05:13.853397 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:13.853405 25348 sgd_solver.cpp:106] Iteration 57200, lr = 0.0010625
I0704 08:05:15.922807 25348 solver.cpp:290] Iteration 57300 (48.3245 iter/s, 2.06935s/100 iter), loss = -2.5332e-07
I0704 08:05:15.922829 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:15.922837 25348 sgd_solver.cpp:106] Iteration 57300, lr = 0.00104688
I0704 08:05:17.994293 25348 solver.cpp:290] Iteration 57400 (48.2765 iter/s, 2.0714s/100 iter), loss = -2.5332e-07
I0704 08:05:17.994315 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:17.994323 25348 sgd_solver.cpp:106] Iteration 57400, lr = 0.00103125
I0704 08:05:20.065922 25348 solver.cpp:290] Iteration 57500 (48.2732 iter/s, 2.07154s/100 iter), loss = -2.5332e-07
I0704 08:05:20.065943 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:20.065950 25348 sgd_solver.cpp:106] Iteration 57500, lr = 0.00101562
I0704 08:05:22.142550 25348 solver.cpp:290] Iteration 57600 (48.157 iter/s, 2.07654s/100 iter), loss = -2.5332e-07
I0704 08:05:22.142573 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:22.142580 25348 sgd_solver.cpp:106] Iteration 57600, lr = 0.001
I0704 08:05:24.218351 25348 solver.cpp:290] Iteration 57700 (48.1761 iter/s, 2.07572s/100 iter), loss = -2.5332e-07
I0704 08:05:24.218375 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:24.218381 25348 sgd_solver.cpp:106] Iteration 57700, lr = 0.000984375
I0704 08:05:26.287920 25348 solver.cpp:290] Iteration 57800 (48.3213 iter/s, 2.06948s/100 iter), loss = -2.5332e-07
I0704 08:05:26.287943 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:26.287952 25348 sgd_solver.cpp:106] Iteration 57800, lr = 0.00096875
I0704 08:05:28.367661 25348 solver.cpp:290] Iteration 57900 (48.085 iter/s, 2.07965s/100 iter), loss = -2.5332e-07
I0704 08:05:28.367770 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:28.367792 25348 sgd_solver.cpp:106] Iteration 57900, lr = 0.000953125
I0704 08:05:30.417819 25348 solver.cpp:354] Sparsity after update:
I0704 08:05:30.419212 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:05:30.419219 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:05:30.419226 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:05:30.419229 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:05:30.419230 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:05:30.419232 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:05:30.419234 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:05:30.419236 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:05:30.419239 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:05:30.419240 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:05:30.419242 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:05:30.419245 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:05:30.419246 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:05:30.419333 25348 solver.cpp:466] Iteration 58000, Testing net (#0)
I0704 08:05:32.059862 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9129
I0704 08:05:32.059883 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9961
I0704 08:05:32.059888 25348 solver.cpp:539]     Test net output #2: loss = 0.1754 (* 1 = 0.1754 loss)
I0704 08:05:32.081885 25348 solver.cpp:290] Iteration 58000 (26.9251 iter/s, 3.71401s/100 iter), loss = -2.5332e-07
I0704 08:05:32.081914 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:32.081924 25348 sgd_solver.cpp:106] Iteration 58000, lr = 0.0009375
I0704 08:05:34.196832 25348 solver.cpp:290] Iteration 58100 (47.2846 iter/s, 2.11485s/100 iter), loss = -2.5332e-07
I0704 08:05:34.196861 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:34.196869 25348 sgd_solver.cpp:106] Iteration 58100, lr = 0.000921875
I0704 08:05:36.265882 25348 solver.cpp:290] Iteration 58200 (48.3335 iter/s, 2.06896s/100 iter), loss = -2.5332e-07
I0704 08:05:36.265908 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:36.265918 25348 sgd_solver.cpp:106] Iteration 58200, lr = 0.00090625
I0704 08:05:38.338917 25348 solver.cpp:290] Iteration 58300 (48.2405 iter/s, 2.07295s/100 iter), loss = -2.5332e-07
I0704 08:05:38.338940 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:38.338946 25348 sgd_solver.cpp:106] Iteration 58300, lr = 0.000890625
I0704 08:05:40.411526 25348 solver.cpp:290] Iteration 58400 (48.2504 iter/s, 2.07252s/100 iter), loss = -2.5332e-07
I0704 08:05:40.411547 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:40.411556 25348 sgd_solver.cpp:106] Iteration 58400, lr = 0.000875
I0704 08:05:42.481514 25348 solver.cpp:290] Iteration 58500 (48.3114 iter/s, 2.0699s/100 iter), loss = -2.5332e-07
I0704 08:05:42.481537 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:42.481544 25348 sgd_solver.cpp:106] Iteration 58500, lr = 0.000859375
I0704 08:05:44.555968 25348 solver.cpp:290] Iteration 58600 (48.2075 iter/s, 2.07436s/100 iter), loss = -2.5332e-07
I0704 08:05:44.555994 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:44.556004 25348 sgd_solver.cpp:106] Iteration 58600, lr = 0.00084375
I0704 08:05:46.629209 25348 solver.cpp:290] Iteration 58700 (48.2357 iter/s, 2.07315s/100 iter), loss = -2.5332e-07
I0704 08:05:46.629232 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:46.629238 25348 sgd_solver.cpp:106] Iteration 58700, lr = 0.000828125
I0704 08:05:48.698076 25348 solver.cpp:290] Iteration 58800 (48.3377 iter/s, 2.06878s/100 iter), loss = -2.5332e-07
I0704 08:05:48.698097 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:48.698106 25348 sgd_solver.cpp:106] Iteration 58800, lr = 0.0008125
I0704 08:05:50.767051 25348 solver.cpp:290] Iteration 58900 (48.3351 iter/s, 2.06889s/100 iter), loss = -2.5332e-07
I0704 08:05:50.767074 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:50.767081 25348 sgd_solver.cpp:106] Iteration 58900, lr = 0.000796875
I0704 08:05:52.817042 25348 solver.cpp:354] Sparsity after update:
I0704 08:05:52.818452 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:05:52.818460 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:05:52.818470 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:05:52.818475 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:05:52.818480 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:05:52.818485 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:05:52.818488 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:05:52.818492 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:05:52.818497 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:05:52.818501 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:05:52.818506 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:05:52.818511 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:05:52.818514 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:05:52.818606 25348 solver.cpp:466] Iteration 59000, Testing net (#0)
I0704 08:05:54.460489 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9146
I0704 08:05:54.460507 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.997
I0704 08:05:54.460512 25348 solver.cpp:539]     Test net output #2: loss = 0.1667 (* 1 = 0.1667 loss)
I0704 08:05:54.480088 25348 solver.cpp:290] Iteration 59000 (26.9331 iter/s, 3.71291s/100 iter), loss = -2.5332e-07
I0704 08:05:54.480105 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:54.480119 25348 sgd_solver.cpp:106] Iteration 59000, lr = 0.00078125
I0704 08:05:56.551357 25348 solver.cpp:290] Iteration 59100 (48.2815 iter/s, 2.07119s/100 iter), loss = -2.5332e-07
I0704 08:05:56.551383 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:56.551393 25348 sgd_solver.cpp:106] Iteration 59100, lr = 0.000765625
I0704 08:05:58.622308 25348 solver.cpp:290] Iteration 59200 (48.2891 iter/s, 2.07086s/100 iter), loss = -2.5332e-07
I0704 08:05:58.622376 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:58.622391 25348 sgd_solver.cpp:106] Iteration 59200, lr = 0.00075
I0704 08:06:00.697468 25348 solver.cpp:290] Iteration 59300 (48.1921 iter/s, 2.07503s/100 iter), loss = -2.5332e-07
I0704 08:06:00.697490 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:00.697496 25348 sgd_solver.cpp:106] Iteration 59300, lr = 0.000734375
I0704 08:06:02.769433 25348 solver.cpp:290] Iteration 59400 (48.2654 iter/s, 2.07188s/100 iter), loss = -2.5332e-07
I0704 08:06:02.769456 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:02.769462 25348 sgd_solver.cpp:106] Iteration 59400, lr = 0.00071875
I0704 08:06:04.843685 25348 solver.cpp:290] Iteration 59500 (48.2122 iter/s, 2.07417s/100 iter), loss = -2.5332e-07
I0704 08:06:04.843708 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:04.843715 25348 sgd_solver.cpp:106] Iteration 59500, lr = 0.000703125
I0704 08:06:06.920531 25348 solver.cpp:290] Iteration 59600 (48.152 iter/s, 2.07676s/100 iter), loss = -2.5332e-07
I0704 08:06:06.920553 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:06.920559 25348 sgd_solver.cpp:106] Iteration 59600, lr = 0.0006875
I0704 08:06:08.994865 25348 solver.cpp:290] Iteration 59700 (48.2103 iter/s, 2.07425s/100 iter), loss = -2.5332e-07
I0704 08:06:08.994887 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:08.994894 25348 sgd_solver.cpp:106] Iteration 59700, lr = 0.000671875
I0704 08:06:11.065526 25348 solver.cpp:290] Iteration 59800 (48.2958 iter/s, 2.07057s/100 iter), loss = -2.5332e-07
I0704 08:06:11.065548 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:11.065556 25348 sgd_solver.cpp:106] Iteration 59800, lr = 0.00065625
I0704 08:06:13.139819 25348 solver.cpp:290] Iteration 59900 (48.2112 iter/s, 2.07421s/100 iter), loss = -2.5332e-07
I0704 08:06:13.139842 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:13.139848 25348 sgd_solver.cpp:106] Iteration 59900, lr = 0.000640625
I0704 08:06:15.195750 25348 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_60000.caffemodel
I0704 08:06:15.212489 25348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_60000.solverstate
I0704 08:06:15.219972 25348 solver.cpp:354] Sparsity after update:
I0704 08:06:15.220912 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:06:15.220919 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:06:15.220927 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:06:15.220929 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:06:15.220932 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:06:15.220933 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:06:15.220935 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:06:15.220937 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:06:15.220939 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:06:15.220942 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:06:15.220943 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:06:15.220945 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:06:15.220947 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:06:15.221040 25348 solver.cpp:466] Iteration 60000, Testing net (#0)
I0704 08:06:16.864121 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.915
I0704 08:06:16.864141 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9972
I0704 08:06:16.864147 25348 solver.cpp:539]     Test net output #2: loss = 0.1674 (* 1 = 0.1674 loss)
I0704 08:06:16.883782 25348 solver.cpp:290] Iteration 60000 (26.7106 iter/s, 3.74383s/100 iter), loss = -2.5332e-07
I0704 08:06:16.883807 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:16.883824 25348 sgd_solver.cpp:106] Iteration 60000, lr = 0.000625
I0704 08:06:18.954923 25348 solver.cpp:290] Iteration 60100 (48.2846 iter/s, 2.07105s/100 iter), loss = -2.5332e-07
I0704 08:06:18.954949 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:18.954957 25348 sgd_solver.cpp:106] Iteration 60100, lr = 0.000609375
I0704 08:06:21.025010 25348 solver.cpp:290] Iteration 60200 (48.3093 iter/s, 2.07s/100 iter), loss = -2.5332e-07
I0704 08:06:21.025035 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:21.025041 25348 sgd_solver.cpp:106] Iteration 60200, lr = 0.00059375
I0704 08:06:23.098119 25348 solver.cpp:290] Iteration 60300 (48.2388 iter/s, 2.07302s/100 iter), loss = -2.5332e-07
I0704 08:06:23.098143 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:23.098151 25348 sgd_solver.cpp:106] Iteration 60300, lr = 0.000578125
I0704 08:06:25.171129 25348 solver.cpp:290] Iteration 60400 (48.2411 iter/s, 2.07292s/100 iter), loss = -2.5332e-07
I0704 08:06:25.171154 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:25.171162 25348 sgd_solver.cpp:106] Iteration 60400, lr = 0.0005625
I0704 08:06:27.253141 25348 solver.cpp:290] Iteration 60500 (48.0325 iter/s, 2.08192s/100 iter), loss = -2.5332e-07
I0704 08:06:27.253163 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:27.253171 25348 sgd_solver.cpp:106] Iteration 60500, lr = 0.000546875
I0704 08:06:29.327792 25348 solver.cpp:290] Iteration 60600 (48.2029 iter/s, 2.07456s/100 iter), loss = -2.5332e-07
I0704 08:06:29.327867 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:29.327875 25348 sgd_solver.cpp:106] Iteration 60600, lr = 0.00053125
I0704 08:06:31.399304 25348 solver.cpp:290] Iteration 60700 (48.2771 iter/s, 2.07137s/100 iter), loss = -2.5332e-07
I0704 08:06:31.399327 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:31.399333 25348 sgd_solver.cpp:106] Iteration 60700, lr = 0.000515625
I0704 08:06:33.474877 25348 solver.cpp:290] Iteration 60800 (48.1815 iter/s, 2.07549s/100 iter), loss = -2.5332e-07
I0704 08:06:33.474907 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:33.474917 25348 sgd_solver.cpp:106] Iteration 60800, lr = 0.0005
I0704 08:06:35.552253 25348 solver.cpp:290] Iteration 60900 (48.1398 iter/s, 2.07728s/100 iter), loss = -2.5332e-07
I0704 08:06:35.552274 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:35.552283 25348 sgd_solver.cpp:106] Iteration 60900, lr = 0.000484375
I0704 08:06:37.605747 25348 solver.cpp:354] Sparsity after update:
I0704 08:06:37.607130 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:06:37.607136 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:06:37.607143 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:06:37.607146 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:06:37.607148 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:06:37.607151 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:06:37.607152 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:06:37.607154 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:06:37.607156 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:06:37.607158 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:06:37.607161 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:06:37.607162 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:06:37.607164 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:06:37.607252 25348 solver.cpp:466] Iteration 61000, Testing net (#0)
I0704 08:06:39.251441 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.914
I0704 08:06:39.251459 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9966
I0704 08:06:39.251466 25348 solver.cpp:539]     Test net output #2: loss = 0.1693 (* 1 = 0.1693 loss)
I0704 08:06:39.271239 25348 solver.cpp:290] Iteration 61000 (26.89 iter/s, 3.71886s/100 iter), loss = -2.5332e-07
I0704 08:06:39.271258 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:39.271267 25348 sgd_solver.cpp:106] Iteration 61000, lr = 0.00046875
I0704 08:06:41.343469 25348 solver.cpp:290] Iteration 61100 (48.2591 iter/s, 2.07215s/100 iter), loss = -2.5332e-07
I0704 08:06:41.343492 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:41.343500 25348 sgd_solver.cpp:106] Iteration 61100, lr = 0.000453125
I0704 08:06:43.418529 25348 solver.cpp:290] Iteration 61200 (48.1934 iter/s, 2.07497s/100 iter), loss = -2.5332e-07
I0704 08:06:43.418550 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:43.418556 25348 sgd_solver.cpp:106] Iteration 61200, lr = 0.0004375
I0704 08:06:45.491545 25348 solver.cpp:290] Iteration 61300 (48.241 iter/s, 2.07293s/100 iter), loss = -2.5332e-07
I0704 08:06:45.491569 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:45.491577 25348 sgd_solver.cpp:106] Iteration 61300, lr = 0.000421875
I0704 08:06:47.566174 25348 solver.cpp:290] Iteration 61400 (48.2034 iter/s, 2.07454s/100 iter), loss = -2.5332e-07
I0704 08:06:47.566196 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:47.566205 25348 sgd_solver.cpp:106] Iteration 61400, lr = 0.00040625
I0704 08:06:49.637663 25348 solver.cpp:290] Iteration 61500 (48.2765 iter/s, 2.0714s/100 iter), loss = -2.5332e-07
I0704 08:06:49.637686 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:49.637693 25348 sgd_solver.cpp:106] Iteration 61500, lr = 0.000390625
I0704 08:06:51.708540 25348 solver.cpp:290] Iteration 61600 (48.2907 iter/s, 2.07079s/100 iter), loss = -2.5332e-07
I0704 08:06:51.708562 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:51.708570 25348 sgd_solver.cpp:106] Iteration 61600, lr = 0.000375
I0704 08:06:53.784720 25348 solver.cpp:290] Iteration 61700 (48.1674 iter/s, 2.07609s/100 iter), loss = -2.5332e-07
I0704 08:06:53.784744 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:53.784750 25348 sgd_solver.cpp:106] Iteration 61700, lr = 0.000359375
I0704 08:06:55.858744 25348 solver.cpp:290] Iteration 61800 (48.2175 iter/s, 2.07394s/100 iter), loss = -2.5332e-07
I0704 08:06:55.858767 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:55.858773 25348 sgd_solver.cpp:106] Iteration 61800, lr = 0.00034375
I0704 08:06:57.931777 25348 solver.cpp:290] Iteration 61900 (48.2405 iter/s, 2.07295s/100 iter), loss = -2.5332e-07
I0704 08:06:57.931798 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:57.931807 25348 sgd_solver.cpp:106] Iteration 61900, lr = 0.000328125
I0704 08:06:59.986757 25348 solver.cpp:354] Sparsity after update:
I0704 08:06:59.988160 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:06:59.988168 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:06:59.988174 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:06:59.988178 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:06:59.988179 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:06:59.988181 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:06:59.988183 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:06:59.988185 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:06:59.988188 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:06:59.988189 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:06:59.988191 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:06:59.988193 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:06:59.988194 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:06:59.988278 25348 solver.cpp:466] Iteration 62000, Testing net (#0)
I0704 08:07:01.628139 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9157
I0704 08:07:01.628156 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9965
I0704 08:07:01.628161 25348 solver.cpp:539]     Test net output #2: loss = 0.1676 (* 1 = 0.1676 loss)
I0704 08:07:01.648098 25348 solver.cpp:290] Iteration 62000 (26.9093 iter/s, 3.71619s/100 iter), loss = -2.5332e-07
I0704 08:07:01.648123 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:01.648129 25348 sgd_solver.cpp:106] Iteration 62000, lr = 0.0003125
I0704 08:07:03.725587 25348 solver.cpp:290] Iteration 62100 (48.137 iter/s, 2.0774s/100 iter), loss = -2.5332e-07
I0704 08:07:03.725608 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:03.725615 25348 sgd_solver.cpp:106] Iteration 62100, lr = 0.000296875
I0704 08:07:05.800490 25348 solver.cpp:290] Iteration 62200 (48.197 iter/s, 2.07482s/100 iter), loss = -2.5332e-07
I0704 08:07:05.800513 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:05.800520 25348 sgd_solver.cpp:106] Iteration 62200, lr = 0.00028125
I0704 08:07:07.875686 25348 solver.cpp:290] Iteration 62300 (48.1902 iter/s, 2.07511s/100 iter), loss = -2.5332e-07
I0704 08:07:07.875710 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:07.875716 25348 sgd_solver.cpp:106] Iteration 62300, lr = 0.000265625
I0704 08:07:09.947206 25348 solver.cpp:290] Iteration 62400 (48.2758 iter/s, 2.07143s/100 iter), loss = -2.5332e-07
I0704 08:07:09.947229 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:09.947235 25348 sgd_solver.cpp:106] Iteration 62400, lr = 0.00025
I0704 08:07:12.016764 25348 solver.cpp:290] Iteration 62500 (48.3215 iter/s, 2.06947s/100 iter), loss = -2.5332e-07
I0704 08:07:12.016786 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:12.016794 25348 sgd_solver.cpp:106] Iteration 62500, lr = 0.000234375
I0704 08:07:14.091687 25348 solver.cpp:290] Iteration 62600 (48.1966 iter/s, 2.07484s/100 iter), loss = -2.5332e-07
I0704 08:07:14.091709 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:14.091717 25348 sgd_solver.cpp:106] Iteration 62600, lr = 0.00021875
I0704 08:07:16.161609 25348 solver.cpp:290] Iteration 62700 (48.313 iter/s, 2.06983s/100 iter), loss = -2.5332e-07
I0704 08:07:16.161634 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:16.161643 25348 sgd_solver.cpp:106] Iteration 62700, lr = 0.000203125
I0704 08:07:18.233100 25348 solver.cpp:290] Iteration 62800 (48.2765 iter/s, 2.0714s/100 iter), loss = -2.5332e-07
I0704 08:07:18.233124 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:18.233129 25348 sgd_solver.cpp:106] Iteration 62800, lr = 0.0001875
I0704 08:07:20.301028 25348 solver.cpp:290] Iteration 62900 (48.3596 iter/s, 2.06784s/100 iter), loss = -2.5332e-07
I0704 08:07:20.301049 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:20.301075 25348 sgd_solver.cpp:106] Iteration 62900, lr = 0.000171875
I0704 08:07:22.350026 25348 solver.cpp:354] Sparsity after update:
I0704 08:07:22.351424 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:07:22.351431 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:07:22.351442 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:07:22.351446 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:07:22.351451 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:07:22.351455 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:07:22.351459 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:07:22.351464 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:07:22.351469 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:07:22.351472 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:07:22.351476 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:07:22.351481 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:07:22.351485 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:07:22.351577 25348 solver.cpp:466] Iteration 63000, Testing net (#0)
I0704 08:07:23.991703 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.914
I0704 08:07:23.991722 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9971
I0704 08:07:23.991727 25348 solver.cpp:539]     Test net output #2: loss = 0.1664 (* 1 = 0.1664 loss)
I0704 08:07:24.011265 25348 solver.cpp:290] Iteration 63000 (26.9534 iter/s, 3.71011s/100 iter), loss = -2.5332e-07
I0704 08:07:24.011282 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:24.011294 25348 sgd_solver.cpp:106] Iteration 63000, lr = 0.00015625
I0704 08:07:26.083037 25348 solver.cpp:290] Iteration 63100 (48.2697 iter/s, 2.07169s/100 iter), loss = -2.5332e-07
I0704 08:07:26.083060 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:26.083067 25348 sgd_solver.cpp:106] Iteration 63100, lr = 0.000140625
I0704 08:07:28.152573 25348 solver.cpp:290] Iteration 63200 (48.3221 iter/s, 2.06945s/100 iter), loss = -2.5332e-07
I0704 08:07:28.152600 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:28.152607 25348 sgd_solver.cpp:106] Iteration 63200, lr = 0.000125
I0704 08:07:30.224687 25348 solver.cpp:290] Iteration 63300 (48.262 iter/s, 2.07203s/100 iter), loss = -2.5332e-07
I0704 08:07:30.224763 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:30.224771 25348 sgd_solver.cpp:106] Iteration 63300, lr = 0.000109375
I0704 08:07:32.302664 25348 solver.cpp:290] Iteration 63400 (48.1269 iter/s, 2.07784s/100 iter), loss = -2.5332e-07
I0704 08:07:32.302687 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:32.302693 25348 sgd_solver.cpp:106] Iteration 63400, lr = 9.37498e-05
I0704 08:07:34.374064 25348 solver.cpp:290] Iteration 63500 (48.2785 iter/s, 2.07131s/100 iter), loss = -2.5332e-07
I0704 08:07:34.374086 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:34.374094 25348 sgd_solver.cpp:106] Iteration 63500, lr = 7.8125e-05
I0704 08:07:36.445134 25348 solver.cpp:290] Iteration 63600 (48.2863 iter/s, 2.07098s/100 iter), loss = -2.5332e-07
I0704 08:07:36.445163 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:36.445168 25348 sgd_solver.cpp:106] Iteration 63600, lr = 6.25002e-05
I0704 08:07:38.519754 25348 solver.cpp:290] Iteration 63700 (48.2038 iter/s, 2.07452s/100 iter), loss = -2.5332e-07
I0704 08:07:38.519778 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:38.519784 25348 sgd_solver.cpp:106] Iteration 63700, lr = 4.68749e-05
I0704 08:07:40.596948 25348 solver.cpp:290] Iteration 63800 (48.1439 iter/s, 2.07711s/100 iter), loss = -2.5332e-07
I0704 08:07:40.596971 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:40.596977 25348 sgd_solver.cpp:106] Iteration 63800, lr = 3.12501e-05
I0704 08:07:42.669821 25348 solver.cpp:290] Iteration 63900 (48.2442 iter/s, 2.07279s/100 iter), loss = -2.5332e-07
I0704 08:07:42.669842 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:42.669849 25348 sgd_solver.cpp:106] Iteration 63900, lr = 1.56248e-05
I0704 08:07:44.722931 25348 solver.cpp:354] Sparsity after update:
I0704 08:07:44.724344 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:07:44.724351 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:07:44.724360 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:07:44.724365 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:07:44.724370 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:07:44.724375 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:07:44.724380 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:07:44.724383 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:07:44.724388 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:07:44.724392 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:07:44.724397 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:07:44.724400 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:07:44.724406 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:07:44.724417 25348 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_64000.caffemodel
I0704 08:07:44.740816 25348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_64000.solverstate
I0704 08:07:44.752998 25348 solver.cpp:446] Iteration 64000, loss = -2.5332e-07
I0704 08:07:44.753016 25348 solver.cpp:466] Iteration 64000, Testing net (#0)
I0704 08:07:46.392413 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9154
I0704 08:07:46.392434 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9967
I0704 08:07:46.392439 25348 solver.cpp:539]     Test net output #2: loss = 0.1661 (* 1 = 0.1661 loss)
I0704 08:07:46.392442 25348 solver.cpp:451] Optimization Done.
I0704 08:07:46.440244 25348 caffe.cpp:246] Optimization Done.
