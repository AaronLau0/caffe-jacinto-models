I0628 19:16:22.766230 32034 caffe.cpp:608] This is NVCaffe 0.16.2 started at Wed Jun 28 19:16:22 2017
I0628 19:16:22.766347 32034 caffe.cpp:611] CuDNN version: 6.0.21
I0628 19:16:22.766351 32034 caffe.cpp:612] CuBLAS version: 8000
I0628 19:16:22.766353 32034 caffe.cpp:613] CUDA version: 8000
I0628 19:16:22.766355 32034 caffe.cpp:614] CUDA driver version: 8000
I0628 19:16:22.885723 32034 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0628 19:16:22.886231 32034 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8277393408, dev_info[0]: total=8506769408 free=8277393408
I0628 19:16:22.886687 32034 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8277393408, dev_info[1]: total=8508145664 free=8379236352
I0628 19:16:22.886698 32034 caffe.cpp:208] Using GPUs 0, 1
I0628 19:16:22.886965 32034 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0628 19:16:22.887228 32034 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0628 19:16:22.887264 32034 solver.cpp:42] Solver data type: FLOAT
I0628 19:16:22.887300 32034 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: true
iter_size: 1
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.02
sparsity_step_iter: 1000
sparsity_start_iter: 0
sparsity_start_factor: 0
I0628 19:16:22.893164 32034 solver.cpp:77] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/train.prototxt
I0628 19:16:22.893627 32034 net.cpp:442] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0628 19:16:22.893635 32034 net.cpp:442] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0628 19:16:22.893822 32034 net.cpp:77] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 32
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0628 19:16:22.893920 32034 net.cpp:108] Using FLOAT as default forward math type
I0628 19:16:22.893924 32034 net.cpp:114] Using FLOAT as default backward math type
I0628 19:16:22.893928 32034 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0628 19:16:22.893930 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:22.893982 32034 net.cpp:183] Created Layer data (0)
I0628 19:16:22.893990 32034 net.cpp:529] data -> data
I0628 19:16:22.894001 32034 net.cpp:529] data -> label
I0628 19:16:22.894023 32034 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 32
I0628 19:16:22.894042 32034 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0628 19:16:22.894714 32050 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0628 19:16:22.895442 32034 data_layer.cpp:188] ReshapePrefetch 32, 3, 32, 32
I0628 19:16:22.895496 32034 data_layer.cpp:206] Output data size: 32, 3, 32, 32
I0628 19:16:22.895503 32034 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0628 19:16:22.895521 32034 net.cpp:244] Setting up data
I0628 19:16:22.895532 32034 net.cpp:251] TRAIN Top shape for layer 0 'data' 32 3 32 32 (98304)
I0628 19:16:22.895542 32034 net.cpp:251] TRAIN Top shape for layer 0 'data' 32 (32)
I0628 19:16:22.895547 32034 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0628 19:16:22.895551 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:22.895560 32034 net.cpp:183] Created Layer data/bias (1)
I0628 19:16:22.895563 32034 net.cpp:560] data/bias <- data
I0628 19:16:22.895570 32034 net.cpp:529] data/bias -> data/bias
I0628 19:16:22.897207 32034 net.cpp:244] Setting up data/bias
I0628 19:16:22.897217 32034 net.cpp:251] TRAIN Top shape for layer 1 'data/bias' 32 3 32 32 (98304)
I0628 19:16:22.897225 32034 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0628 19:16:22.897228 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:22.897243 32034 net.cpp:183] Created Layer conv1a (2)
I0628 19:16:22.897245 32034 net.cpp:560] conv1a <- data/bias
I0628 19:16:22.897248 32034 net.cpp:529] conv1a -> conv1a
I0628 19:16:23.180197 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.15G, req 0G)
I0628 19:16:23.180227 32034 net.cpp:244] Setting up conv1a
I0628 19:16:23.180236 32034 net.cpp:251] TRAIN Top shape for layer 2 'conv1a' 32 32 32 32 (1048576)
I0628 19:16:23.180248 32034 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0628 19:16:23.180255 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.180269 32034 net.cpp:183] Created Layer conv1a/bn (3)
I0628 19:16:23.180272 32034 net.cpp:560] conv1a/bn <- conv1a
I0628 19:16:23.180276 32034 net.cpp:512] conv1a/bn -> conv1a (in-place)
I0628 19:16:23.180835 32034 net.cpp:244] Setting up conv1a/bn
I0628 19:16:23.180842 32034 net.cpp:251] TRAIN Top shape for layer 3 'conv1a/bn' 32 32 32 32 (1048576)
I0628 19:16:23.180850 32034 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0628 19:16:23.180852 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.180856 32034 net.cpp:183] Created Layer conv1a/relu (4)
I0628 19:16:23.180860 32034 net.cpp:560] conv1a/relu <- conv1a
I0628 19:16:23.180861 32034 net.cpp:512] conv1a/relu -> conv1a (in-place)
I0628 19:16:23.180876 32034 net.cpp:244] Setting up conv1a/relu
I0628 19:16:23.180881 32034 net.cpp:251] TRAIN Top shape for layer 4 'conv1a/relu' 32 32 32 32 (1048576)
I0628 19:16:23.180882 32034 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0628 19:16:23.180886 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.180896 32034 net.cpp:183] Created Layer conv1b (5)
I0628 19:16:23.180898 32034 net.cpp:560] conv1b <- conv1a
I0628 19:16:23.180902 32034 net.cpp:529] conv1b -> conv1b
I0628 19:16:23.187296 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.13G, req 0G)
I0628 19:16:23.187315 32034 net.cpp:244] Setting up conv1b
I0628 19:16:23.187324 32034 net.cpp:251] TRAIN Top shape for layer 5 'conv1b' 32 32 32 32 (1048576)
I0628 19:16:23.187335 32034 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0628 19:16:23.187341 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.187350 32034 net.cpp:183] Created Layer conv1b/bn (6)
I0628 19:16:23.187355 32034 net.cpp:560] conv1b/bn <- conv1b
I0628 19:16:23.187360 32034 net.cpp:512] conv1b/bn -> conv1b (in-place)
I0628 19:16:23.187897 32034 net.cpp:244] Setting up conv1b/bn
I0628 19:16:23.187906 32034 net.cpp:251] TRAIN Top shape for layer 6 'conv1b/bn' 32 32 32 32 (1048576)
I0628 19:16:23.187924 32034 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0628 19:16:23.187929 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.187935 32034 net.cpp:183] Created Layer conv1b/relu (7)
I0628 19:16:23.187940 32034 net.cpp:560] conv1b/relu <- conv1b
I0628 19:16:23.187947 32034 net.cpp:512] conv1b/relu -> conv1b (in-place)
I0628 19:16:23.187952 32034 net.cpp:244] Setting up conv1b/relu
I0628 19:16:23.187958 32034 net.cpp:251] TRAIN Top shape for layer 7 'conv1b/relu' 32 32 32 32 (1048576)
I0628 19:16:23.187963 32034 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0628 19:16:23.187968 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.187976 32034 net.cpp:183] Created Layer pool1 (8)
I0628 19:16:23.187980 32034 net.cpp:560] pool1 <- conv1b
I0628 19:16:23.187984 32034 net.cpp:529] pool1 -> pool1
I0628 19:16:23.188047 32034 net.cpp:244] Setting up pool1
I0628 19:16:23.188053 32034 net.cpp:251] TRAIN Top shape for layer 8 'pool1' 32 32 32 32 (1048576)
I0628 19:16:23.188057 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0628 19:16:23.188062 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.188073 32034 net.cpp:183] Created Layer res2a_branch2a (9)
I0628 19:16:23.188077 32034 net.cpp:560] res2a_branch2a <- pool1
I0628 19:16:23.188081 32034 net.cpp:529] res2a_branch2a -> res2a_branch2a
I0628 19:16:23.197912 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.11G, req 0G)
I0628 19:16:23.197935 32034 net.cpp:244] Setting up res2a_branch2a
I0628 19:16:23.197945 32034 net.cpp:251] TRAIN Top shape for layer 9 'res2a_branch2a' 32 64 32 32 (2097152)
I0628 19:16:23.197958 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0628 19:16:23.197964 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.197974 32034 net.cpp:183] Created Layer res2a_branch2a/bn (10)
I0628 19:16:23.197979 32034 net.cpp:560] res2a_branch2a/bn <- res2a_branch2a
I0628 19:16:23.197983 32034 net.cpp:512] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0628 19:16:23.198557 32034 net.cpp:244] Setting up res2a_branch2a/bn
I0628 19:16:23.198565 32034 net.cpp:251] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 32 64 32 32 (2097152)
I0628 19:16:23.198573 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0628 19:16:23.198580 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.198585 32034 net.cpp:183] Created Layer res2a_branch2a/relu (11)
I0628 19:16:23.198588 32034 net.cpp:560] res2a_branch2a/relu <- res2a_branch2a
I0628 19:16:23.198592 32034 net.cpp:512] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0628 19:16:23.198598 32034 net.cpp:244] Setting up res2a_branch2a/relu
I0628 19:16:23.198604 32034 net.cpp:251] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 32 64 32 32 (2097152)
I0628 19:16:23.198608 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0628 19:16:23.198612 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.198623 32034 net.cpp:183] Created Layer res2a_branch2b (12)
I0628 19:16:23.198626 32034 net.cpp:560] res2a_branch2b <- res2a_branch2a
I0628 19:16:23.198629 32034 net.cpp:529] res2a_branch2b -> res2a_branch2b
I0628 19:16:23.204071 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.09G, req 0G)
I0628 19:16:23.204092 32034 net.cpp:244] Setting up res2a_branch2b
I0628 19:16:23.204098 32034 net.cpp:251] TRAIN Top shape for layer 12 'res2a_branch2b' 32 64 32 32 (2097152)
I0628 19:16:23.204107 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0628 19:16:23.204121 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.204131 32034 net.cpp:183] Created Layer res2a_branch2b/bn (13)
I0628 19:16:23.204135 32034 net.cpp:560] res2a_branch2b/bn <- res2a_branch2b
I0628 19:16:23.204139 32034 net.cpp:512] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0628 19:16:23.204690 32034 net.cpp:244] Setting up res2a_branch2b/bn
I0628 19:16:23.204699 32034 net.cpp:251] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 32 64 32 32 (2097152)
I0628 19:16:23.204706 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0628 19:16:23.204711 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.204717 32034 net.cpp:183] Created Layer res2a_branch2b/relu (14)
I0628 19:16:23.204721 32034 net.cpp:560] res2a_branch2b/relu <- res2a_branch2b
I0628 19:16:23.204726 32034 net.cpp:512] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0628 19:16:23.204733 32034 net.cpp:244] Setting up res2a_branch2b/relu
I0628 19:16:23.204738 32034 net.cpp:251] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 32 64 32 32 (2097152)
I0628 19:16:23.204743 32034 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0628 19:16:23.204747 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.204754 32034 net.cpp:183] Created Layer pool2 (15)
I0628 19:16:23.204758 32034 net.cpp:560] pool2 <- res2a_branch2b
I0628 19:16:23.204761 32034 net.cpp:529] pool2 -> pool2
I0628 19:16:23.204818 32034 net.cpp:244] Setting up pool2
I0628 19:16:23.204823 32034 net.cpp:251] TRAIN Top shape for layer 15 'pool2' 32 64 16 16 (524288)
I0628 19:16:23.204825 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0628 19:16:23.204830 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.204840 32034 net.cpp:183] Created Layer res3a_branch2a (16)
I0628 19:16:23.204844 32034 net.cpp:560] res3a_branch2a <- pool2
I0628 19:16:23.204848 32034 net.cpp:529] res3a_branch2a -> res3a_branch2a
I0628 19:16:23.215445 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.08G, req 0.02G)
I0628 19:16:23.215476 32034 net.cpp:244] Setting up res3a_branch2a
I0628 19:16:23.215487 32034 net.cpp:251] TRAIN Top shape for layer 16 'res3a_branch2a' 32 128 16 16 (1048576)
I0628 19:16:23.215498 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0628 19:16:23.215504 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.215520 32034 net.cpp:183] Created Layer res3a_branch2a/bn (17)
I0628 19:16:23.215525 32034 net.cpp:560] res3a_branch2a/bn <- res3a_branch2a
I0628 19:16:23.215530 32034 net.cpp:512] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0628 19:16:23.216265 32034 net.cpp:244] Setting up res3a_branch2a/bn
I0628 19:16:23.216274 32034 net.cpp:251] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 32 128 16 16 (1048576)
I0628 19:16:23.216296 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0628 19:16:23.216302 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.216307 32034 net.cpp:183] Created Layer res3a_branch2a/relu (18)
I0628 19:16:23.216311 32034 net.cpp:560] res3a_branch2a/relu <- res3a_branch2a
I0628 19:16:23.216315 32034 net.cpp:512] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0628 19:16:23.216322 32034 net.cpp:244] Setting up res3a_branch2a/relu
I0628 19:16:23.216326 32034 net.cpp:251] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 32 128 16 16 (1048576)
I0628 19:16:23.216331 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0628 19:16:23.216334 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.216351 32034 net.cpp:183] Created Layer res3a_branch2b (19)
I0628 19:16:23.216363 32034 net.cpp:560] res3a_branch2b <- res3a_branch2a
I0628 19:16:23.216367 32034 net.cpp:529] res3a_branch2b -> res3a_branch2b
I0628 19:16:23.221616 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.07G, req 0.02G)
I0628 19:16:23.221634 32034 net.cpp:244] Setting up res3a_branch2b
I0628 19:16:23.221640 32034 net.cpp:251] TRAIN Top shape for layer 19 'res3a_branch2b' 32 128 16 16 (1048576)
I0628 19:16:23.221645 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0628 19:16:23.221649 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.221657 32034 net.cpp:183] Created Layer res3a_branch2b/bn (20)
I0628 19:16:23.221659 32034 net.cpp:560] res3a_branch2b/bn <- res3a_branch2b
I0628 19:16:23.221662 32034 net.cpp:512] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0628 19:16:23.222177 32034 net.cpp:244] Setting up res3a_branch2b/bn
I0628 19:16:23.222184 32034 net.cpp:251] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 32 128 16 16 (1048576)
I0628 19:16:23.222190 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0628 19:16:23.222193 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.222195 32034 net.cpp:183] Created Layer res3a_branch2b/relu (21)
I0628 19:16:23.222198 32034 net.cpp:560] res3a_branch2b/relu <- res3a_branch2b
I0628 19:16:23.222201 32034 net.cpp:512] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0628 19:16:23.222204 32034 net.cpp:244] Setting up res3a_branch2b/relu
I0628 19:16:23.222208 32034 net.cpp:251] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 32 128 16 16 (1048576)
I0628 19:16:23.222209 32034 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0628 19:16:23.222211 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.222215 32034 net.cpp:183] Created Layer pool3 (22)
I0628 19:16:23.222218 32034 net.cpp:560] pool3 <- res3a_branch2b
I0628 19:16:23.222220 32034 net.cpp:529] pool3 -> pool3
I0628 19:16:23.222266 32034 net.cpp:244] Setting up pool3
I0628 19:16:23.222270 32034 net.cpp:251] TRAIN Top shape for layer 22 'pool3' 32 128 16 16 (1048576)
I0628 19:16:23.222272 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0628 19:16:23.222275 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.222281 32034 net.cpp:183] Created Layer res4a_branch2a (23)
I0628 19:16:23.222285 32034 net.cpp:560] res4a_branch2a <- pool3
I0628 19:16:23.222286 32034 net.cpp:529] res4a_branch2a -> res4a_branch2a
I0628 19:16:23.243301 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.04G, req 0.02G)
I0628 19:16:23.243335 32034 net.cpp:244] Setting up res4a_branch2a
I0628 19:16:23.243346 32034 net.cpp:251] TRAIN Top shape for layer 23 'res4a_branch2a' 32 256 16 16 (2097152)
I0628 19:16:23.243357 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0628 19:16:23.243363 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.243376 32034 net.cpp:183] Created Layer res4a_branch2a/bn (24)
I0628 19:16:23.243381 32034 net.cpp:560] res4a_branch2a/bn <- res4a_branch2a
I0628 19:16:23.243386 32034 net.cpp:512] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0628 19:16:23.244150 32034 net.cpp:244] Setting up res4a_branch2a/bn
I0628 19:16:23.244161 32034 net.cpp:251] TRAIN Top shape for layer 24 'res4a_branch2a/bn' 32 256 16 16 (2097152)
I0628 19:16:23.244169 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0628 19:16:23.244174 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.244179 32034 net.cpp:183] Created Layer res4a_branch2a/relu (25)
I0628 19:16:23.244194 32034 net.cpp:560] res4a_branch2a/relu <- res4a_branch2a
I0628 19:16:23.244199 32034 net.cpp:512] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0628 19:16:23.244206 32034 net.cpp:244] Setting up res4a_branch2a/relu
I0628 19:16:23.244210 32034 net.cpp:251] TRAIN Top shape for layer 25 'res4a_branch2a/relu' 32 256 16 16 (2097152)
I0628 19:16:23.244215 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0628 19:16:23.244218 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.244230 32034 net.cpp:183] Created Layer res4a_branch2b (26)
I0628 19:16:23.244232 32034 net.cpp:560] res4a_branch2b <- res4a_branch2a
I0628 19:16:23.244237 32034 net.cpp:529] res4a_branch2b -> res4a_branch2b
I0628 19:16:23.254741 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.02G, req 0.02G)
I0628 19:16:23.254770 32034 net.cpp:244] Setting up res4a_branch2b
I0628 19:16:23.254778 32034 net.cpp:251] TRAIN Top shape for layer 26 'res4a_branch2b' 32 256 16 16 (2097152)
I0628 19:16:23.254786 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0628 19:16:23.254791 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.254799 32034 net.cpp:183] Created Layer res4a_branch2b/bn (27)
I0628 19:16:23.254803 32034 net.cpp:560] res4a_branch2b/bn <- res4a_branch2b
I0628 19:16:23.254806 32034 net.cpp:512] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0628 19:16:23.255373 32034 net.cpp:244] Setting up res4a_branch2b/bn
I0628 19:16:23.255379 32034 net.cpp:251] TRAIN Top shape for layer 27 'res4a_branch2b/bn' 32 256 16 16 (2097152)
I0628 19:16:23.255385 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0628 19:16:23.255388 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.255393 32034 net.cpp:183] Created Layer res4a_branch2b/relu (28)
I0628 19:16:23.255394 32034 net.cpp:560] res4a_branch2b/relu <- res4a_branch2b
I0628 19:16:23.255398 32034 net.cpp:512] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0628 19:16:23.255401 32034 net.cpp:244] Setting up res4a_branch2b/relu
I0628 19:16:23.255404 32034 net.cpp:251] TRAIN Top shape for layer 28 'res4a_branch2b/relu' 32 256 16 16 (2097152)
I0628 19:16:23.255408 32034 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0628 19:16:23.255409 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.255414 32034 net.cpp:183] Created Layer pool4 (29)
I0628 19:16:23.255417 32034 net.cpp:560] pool4 <- res4a_branch2b
I0628 19:16:23.255419 32034 net.cpp:529] pool4 -> pool4
I0628 19:16:23.255465 32034 net.cpp:244] Setting up pool4
I0628 19:16:23.255468 32034 net.cpp:251] TRAIN Top shape for layer 29 'pool4' 32 256 8 8 (524288)
I0628 19:16:23.255471 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0628 19:16:23.255475 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.255482 32034 net.cpp:183] Created Layer res5a_branch2a (30)
I0628 19:16:23.255484 32034 net.cpp:560] res5a_branch2a <- pool4
I0628 19:16:23.255487 32034 net.cpp:529] res5a_branch2a -> res5a_branch2a
I0628 19:16:23.302940 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 3  (limit 8G, req 0.02G)
I0628 19:16:23.302964 32034 net.cpp:244] Setting up res5a_branch2a
I0628 19:16:23.302973 32034 net.cpp:251] TRAIN Top shape for layer 30 'res5a_branch2a' 32 512 8 8 (1048576)
I0628 19:16:23.302980 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0628 19:16:23.302983 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.302992 32034 net.cpp:183] Created Layer res5a_branch2a/bn (31)
I0628 19:16:23.302995 32034 net.cpp:560] res5a_branch2a/bn <- res5a_branch2a
I0628 19:16:23.303009 32034 net.cpp:512] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0628 19:16:23.303586 32034 net.cpp:244] Setting up res5a_branch2a/bn
I0628 19:16:23.303592 32034 net.cpp:251] TRAIN Top shape for layer 31 'res5a_branch2a/bn' 32 512 8 8 (1048576)
I0628 19:16:23.303598 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0628 19:16:23.303601 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.303606 32034 net.cpp:183] Created Layer res5a_branch2a/relu (32)
I0628 19:16:23.303607 32034 net.cpp:560] res5a_branch2a/relu <- res5a_branch2a
I0628 19:16:23.303609 32034 net.cpp:512] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0628 19:16:23.303613 32034 net.cpp:244] Setting up res5a_branch2a/relu
I0628 19:16:23.303617 32034 net.cpp:251] TRAIN Top shape for layer 32 'res5a_branch2a/relu' 32 512 8 8 (1048576)
I0628 19:16:23.303618 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0628 19:16:23.303620 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.303627 32034 net.cpp:183] Created Layer res5a_branch2b (33)
I0628 19:16:23.303630 32034 net.cpp:560] res5a_branch2b <- res5a_branch2a
I0628 19:16:23.303632 32034 net.cpp:529] res5a_branch2b -> res5a_branch2b
I0628 19:16:23.322635 32034 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 5 5  (limit 7.98G, req 0.02G)
I0628 19:16:23.322652 32034 net.cpp:244] Setting up res5a_branch2b
I0628 19:16:23.322657 32034 net.cpp:251] TRAIN Top shape for layer 33 'res5a_branch2b' 32 512 8 8 (1048576)
I0628 19:16:23.322665 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0628 19:16:23.322667 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.322674 32034 net.cpp:183] Created Layer res5a_branch2b/bn (34)
I0628 19:16:23.322677 32034 net.cpp:560] res5a_branch2b/bn <- res5a_branch2b
I0628 19:16:23.322679 32034 net.cpp:512] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0628 19:16:23.323210 32034 net.cpp:244] Setting up res5a_branch2b/bn
I0628 19:16:23.323216 32034 net.cpp:251] TRAIN Top shape for layer 34 'res5a_branch2b/bn' 32 512 8 8 (1048576)
I0628 19:16:23.323221 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0628 19:16:23.323225 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.323227 32034 net.cpp:183] Created Layer res5a_branch2b/relu (35)
I0628 19:16:23.323230 32034 net.cpp:560] res5a_branch2b/relu <- res5a_branch2b
I0628 19:16:23.323231 32034 net.cpp:512] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0628 19:16:23.323235 32034 net.cpp:244] Setting up res5a_branch2b/relu
I0628 19:16:23.323237 32034 net.cpp:251] TRAIN Top shape for layer 35 'res5a_branch2b/relu' 32 512 8 8 (1048576)
I0628 19:16:23.323240 32034 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0628 19:16:23.323241 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.323246 32034 net.cpp:183] Created Layer pool5 (36)
I0628 19:16:23.323247 32034 net.cpp:560] pool5 <- res5a_branch2b
I0628 19:16:23.323249 32034 net.cpp:529] pool5 -> pool5
I0628 19:16:23.323271 32034 net.cpp:244] Setting up pool5
I0628 19:16:23.323276 32034 net.cpp:251] TRAIN Top shape for layer 36 'pool5' 32 512 1 1 (16384)
I0628 19:16:23.323277 32034 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0628 19:16:23.323279 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.323284 32034 net.cpp:183] Created Layer fc10 (37)
I0628 19:16:23.323287 32034 net.cpp:560] fc10 <- pool5
I0628 19:16:23.323289 32034 net.cpp:529] fc10 -> fc10
I0628 19:16:23.323508 32034 net.cpp:244] Setting up fc10
I0628 19:16:23.323514 32034 net.cpp:251] TRAIN Top shape for layer 37 'fc10' 32 10 (320)
I0628 19:16:23.323528 32034 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0628 19:16:23.323530 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.323542 32034 net.cpp:183] Created Layer loss (38)
I0628 19:16:23.323545 32034 net.cpp:560] loss <- fc10
I0628 19:16:23.323547 32034 net.cpp:560] loss <- label
I0628 19:16:23.323551 32034 net.cpp:529] loss -> loss
I0628 19:16:23.323678 32034 net.cpp:244] Setting up loss
I0628 19:16:23.323684 32034 net.cpp:251] TRAIN Top shape for layer 38 'loss' (1)
I0628 19:16:23.323688 32034 net.cpp:255]     with loss weight 1
I0628 19:16:23.323691 32034 net.cpp:322] loss needs backward computation.
I0628 19:16:23.323694 32034 net.cpp:322] fc10 needs backward computation.
I0628 19:16:23.323696 32034 net.cpp:322] pool5 needs backward computation.
I0628 19:16:23.323699 32034 net.cpp:322] res5a_branch2b/relu needs backward computation.
I0628 19:16:23.323701 32034 net.cpp:322] res5a_branch2b/bn needs backward computation.
I0628 19:16:23.323704 32034 net.cpp:322] res5a_branch2b needs backward computation.
I0628 19:16:23.323705 32034 net.cpp:322] res5a_branch2a/relu needs backward computation.
I0628 19:16:23.323707 32034 net.cpp:322] res5a_branch2a/bn needs backward computation.
I0628 19:16:23.323709 32034 net.cpp:322] res5a_branch2a needs backward computation.
I0628 19:16:23.323712 32034 net.cpp:322] pool4 needs backward computation.
I0628 19:16:23.323714 32034 net.cpp:322] res4a_branch2b/relu needs backward computation.
I0628 19:16:23.323717 32034 net.cpp:322] res4a_branch2b/bn needs backward computation.
I0628 19:16:23.323719 32034 net.cpp:322] res4a_branch2b needs backward computation.
I0628 19:16:23.323721 32034 net.cpp:322] res4a_branch2a/relu needs backward computation.
I0628 19:16:23.323724 32034 net.cpp:322] res4a_branch2a/bn needs backward computation.
I0628 19:16:23.323726 32034 net.cpp:322] res4a_branch2a needs backward computation.
I0628 19:16:23.323729 32034 net.cpp:322] pool3 needs backward computation.
I0628 19:16:23.323731 32034 net.cpp:322] res3a_branch2b/relu needs backward computation.
I0628 19:16:23.323734 32034 net.cpp:322] res3a_branch2b/bn needs backward computation.
I0628 19:16:23.323735 32034 net.cpp:322] res3a_branch2b needs backward computation.
I0628 19:16:23.323737 32034 net.cpp:322] res3a_branch2a/relu needs backward computation.
I0628 19:16:23.323740 32034 net.cpp:322] res3a_branch2a/bn needs backward computation.
I0628 19:16:23.323741 32034 net.cpp:322] res3a_branch2a needs backward computation.
I0628 19:16:23.323743 32034 net.cpp:322] pool2 needs backward computation.
I0628 19:16:23.323745 32034 net.cpp:322] res2a_branch2b/relu needs backward computation.
I0628 19:16:23.323747 32034 net.cpp:322] res2a_branch2b/bn needs backward computation.
I0628 19:16:23.323750 32034 net.cpp:322] res2a_branch2b needs backward computation.
I0628 19:16:23.323751 32034 net.cpp:322] res2a_branch2a/relu needs backward computation.
I0628 19:16:23.323755 32034 net.cpp:322] res2a_branch2a/bn needs backward computation.
I0628 19:16:23.323755 32034 net.cpp:322] res2a_branch2a needs backward computation.
I0628 19:16:23.323757 32034 net.cpp:322] pool1 needs backward computation.
I0628 19:16:23.323760 32034 net.cpp:322] conv1b/relu needs backward computation.
I0628 19:16:23.323762 32034 net.cpp:322] conv1b/bn needs backward computation.
I0628 19:16:23.323765 32034 net.cpp:322] conv1b needs backward computation.
I0628 19:16:23.323767 32034 net.cpp:322] conv1a/relu needs backward computation.
I0628 19:16:23.323770 32034 net.cpp:322] conv1a/bn needs backward computation.
I0628 19:16:23.323771 32034 net.cpp:322] conv1a needs backward computation.
I0628 19:16:23.323773 32034 net.cpp:324] data/bias does not need backward computation.
I0628 19:16:23.323776 32034 net.cpp:324] data does not need backward computation.
I0628 19:16:23.323779 32034 net.cpp:366] This network produces output loss
I0628 19:16:23.323806 32034 net.cpp:388] Top memory (TRAIN) required for data: 176160768 diff: 176160776
I0628 19:16:23.323808 32034 net.cpp:391] Bottom memory (TRAIN) required for data: 176160768 diff: 176160768
I0628 19:16:23.323815 32034 net.cpp:394] Shared (in-place) memory (TRAIN) by data: 117440512 diff: 117440512
I0628 19:16:23.323818 32034 net.cpp:397] Parameters memory (TRAIN) required for data: 9450960 diff: 9450960
I0628 19:16:23.323820 32034 net.cpp:400] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0628 19:16:23.323822 32034 net.cpp:406] Network initialization done.
I0628 19:16:23.324174 32034 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/test.prototxt
I0628 19:16:23.324328 32034 net.cpp:77] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 25
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0628 19:16:23.324414 32034 net.cpp:108] Using FLOAT as default forward math type
I0628 19:16:23.324417 32034 net.cpp:114] Using FLOAT as default backward math type
I0628 19:16:23.324419 32034 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0628 19:16:23.324422 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.324432 32034 net.cpp:183] Created Layer data (0)
I0628 19:16:23.324435 32034 net.cpp:529] data -> data
I0628 19:16:23.324439 32034 net.cpp:529] data -> label
I0628 19:16:23.324445 32034 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 25
I0628 19:16:23.324452 32034 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0628 19:16:23.325189 32080 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0628 19:16:23.325268 32034 data_layer.cpp:188] ReshapePrefetch 25, 3, 32, 32
I0628 19:16:23.325321 32034 data_layer.cpp:206] Output data size: 25, 3, 32, 32
I0628 19:16:23.325325 32034 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0628 19:16:23.325422 32034 net.cpp:244] Setting up data
I0628 19:16:23.325428 32034 net.cpp:251] TEST Top shape for layer 0 'data' 25 3 32 32 (76800)
I0628 19:16:23.325431 32034 net.cpp:251] TEST Top shape for layer 0 'data' 25 (25)
I0628 19:16:23.325434 32034 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0628 19:16:23.325438 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.325443 32034 net.cpp:183] Created Layer label_data_1_split (1)
I0628 19:16:23.325446 32034 net.cpp:560] label_data_1_split <- label
I0628 19:16:23.325449 32034 net.cpp:529] label_data_1_split -> label_data_1_split_0
I0628 19:16:23.325453 32034 net.cpp:529] label_data_1_split -> label_data_1_split_1
I0628 19:16:23.325456 32034 net.cpp:529] label_data_1_split -> label_data_1_split_2
I0628 19:16:23.325505 32034 net.cpp:244] Setting up label_data_1_split
I0628 19:16:23.325510 32034 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 25 (25)
I0628 19:16:23.325520 32034 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 25 (25)
I0628 19:16:23.325522 32034 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 25 (25)
I0628 19:16:23.325525 32034 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0628 19:16:23.325527 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.325532 32034 net.cpp:183] Created Layer data/bias (2)
I0628 19:16:23.325534 32034 net.cpp:560] data/bias <- data
I0628 19:16:23.325537 32034 net.cpp:529] data/bias -> data/bias
I0628 19:16:23.325665 32034 net.cpp:244] Setting up data/bias
I0628 19:16:23.325672 32034 net.cpp:251] TEST Top shape for layer 2 'data/bias' 25 3 32 32 (76800)
I0628 19:16:23.325676 32034 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0628 19:16:23.325680 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.325686 32034 net.cpp:183] Created Layer conv1a (3)
I0628 19:16:23.325690 32034 net.cpp:560] conv1a <- data/bias
I0628 19:16:23.325692 32034 net.cpp:529] conv1a -> conv1a
I0628 19:16:23.326020 32081 data_layer.cpp:188] ReshapePrefetch 25, 3, 32, 32
I0628 19:16:23.326027 32081 data_layer.cpp:206] Output data size: 25, 3, 32, 32
I0628 19:16:23.326833 32081 data_layer.cpp:110] [0] Parser threads: 1
I0628 19:16:23.326839 32081 data_layer.cpp:112] [0] Transformer threads: 1
I0628 19:16:23.328567 32034 net.cpp:244] Setting up conv1a
I0628 19:16:23.328577 32034 net.cpp:251] TEST Top shape for layer 3 'conv1a' 25 32 32 32 (819200)
I0628 19:16:23.328583 32034 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0628 19:16:23.328585 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.328591 32034 net.cpp:183] Created Layer conv1a/bn (4)
I0628 19:16:23.328593 32034 net.cpp:560] conv1a/bn <- conv1a
I0628 19:16:23.328596 32034 net.cpp:512] conv1a/bn -> conv1a (in-place)
I0628 19:16:23.329527 32034 net.cpp:244] Setting up conv1a/bn
I0628 19:16:23.329535 32034 net.cpp:251] TEST Top shape for layer 4 'conv1a/bn' 25 32 32 32 (819200)
I0628 19:16:23.329542 32034 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0628 19:16:23.329545 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.329556 32034 net.cpp:183] Created Layer conv1a/relu (5)
I0628 19:16:23.329560 32034 net.cpp:560] conv1a/relu <- conv1a
I0628 19:16:23.329561 32034 net.cpp:512] conv1a/relu -> conv1a (in-place)
I0628 19:16:23.329566 32034 net.cpp:244] Setting up conv1a/relu
I0628 19:16:23.329568 32034 net.cpp:251] TEST Top shape for layer 5 'conv1a/relu' 25 32 32 32 (819200)
I0628 19:16:23.329571 32034 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0628 19:16:23.329573 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.329581 32034 net.cpp:183] Created Layer conv1b (6)
I0628 19:16:23.329584 32034 net.cpp:560] conv1b <- conv1a
I0628 19:16:23.329586 32034 net.cpp:529] conv1b -> conv1b
I0628 19:16:23.331485 32034 net.cpp:244] Setting up conv1b
I0628 19:16:23.331495 32034 net.cpp:251] TEST Top shape for layer 6 'conv1b' 25 32 32 32 (819200)
I0628 19:16:23.331501 32034 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0628 19:16:23.331503 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.331508 32034 net.cpp:183] Created Layer conv1b/bn (7)
I0628 19:16:23.331511 32034 net.cpp:560] conv1b/bn <- conv1b
I0628 19:16:23.331513 32034 net.cpp:512] conv1b/bn -> conv1b (in-place)
I0628 19:16:23.332476 32034 net.cpp:244] Setting up conv1b/bn
I0628 19:16:23.332485 32034 net.cpp:251] TEST Top shape for layer 7 'conv1b/bn' 25 32 32 32 (819200)
I0628 19:16:23.332492 32034 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0628 19:16:23.332495 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.332510 32034 net.cpp:183] Created Layer conv1b/relu (8)
I0628 19:16:23.332514 32034 net.cpp:560] conv1b/relu <- conv1b
I0628 19:16:23.332516 32034 net.cpp:512] conv1b/relu -> conv1b (in-place)
I0628 19:16:23.332520 32034 net.cpp:244] Setting up conv1b/relu
I0628 19:16:23.332525 32034 net.cpp:251] TEST Top shape for layer 8 'conv1b/relu' 25 32 32 32 (819200)
I0628 19:16:23.332527 32034 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0628 19:16:23.332532 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.332538 32034 net.cpp:183] Created Layer pool1 (9)
I0628 19:16:23.332542 32034 net.cpp:560] pool1 <- conv1b
I0628 19:16:23.332546 32034 net.cpp:529] pool1 -> pool1
I0628 19:16:23.332665 32034 net.cpp:244] Setting up pool1
I0628 19:16:23.332675 32034 net.cpp:251] TEST Top shape for layer 9 'pool1' 25 32 32 32 (819200)
I0628 19:16:23.332682 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0628 19:16:23.332689 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.332708 32034 net.cpp:183] Created Layer res2a_branch2a (10)
I0628 19:16:23.332715 32034 net.cpp:560] res2a_branch2a <- pool1
I0628 19:16:23.332722 32034 net.cpp:529] res2a_branch2a -> res2a_branch2a
I0628 19:16:23.336020 32034 net.cpp:244] Setting up res2a_branch2a
I0628 19:16:23.336032 32034 net.cpp:251] TEST Top shape for layer 10 'res2a_branch2a' 25 64 32 32 (1638400)
I0628 19:16:23.336042 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0628 19:16:23.336047 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.336056 32034 net.cpp:183] Created Layer res2a_branch2a/bn (11)
I0628 19:16:23.336061 32034 net.cpp:560] res2a_branch2a/bn <- res2a_branch2a
I0628 19:16:23.336064 32034 net.cpp:512] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0628 19:16:23.337332 32034 net.cpp:244] Setting up res2a_branch2a/bn
I0628 19:16:23.337343 32034 net.cpp:251] TEST Top shape for layer 11 'res2a_branch2a/bn' 25 64 32 32 (1638400)
I0628 19:16:23.337353 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0628 19:16:23.337357 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.337363 32034 net.cpp:183] Created Layer res2a_branch2a/relu (12)
I0628 19:16:23.337368 32034 net.cpp:560] res2a_branch2a/relu <- res2a_branch2a
I0628 19:16:23.337373 32034 net.cpp:512] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0628 19:16:23.337378 32034 net.cpp:244] Setting up res2a_branch2a/relu
I0628 19:16:23.337383 32034 net.cpp:251] TEST Top shape for layer 12 'res2a_branch2a/relu' 25 64 32 32 (1638400)
I0628 19:16:23.337388 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0628 19:16:23.337391 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.337402 32034 net.cpp:183] Created Layer res2a_branch2b (13)
I0628 19:16:23.337406 32034 net.cpp:560] res2a_branch2b <- res2a_branch2a
I0628 19:16:23.337410 32034 net.cpp:529] res2a_branch2b -> res2a_branch2b
I0628 19:16:23.339617 32034 net.cpp:244] Setting up res2a_branch2b
I0628 19:16:23.339630 32034 net.cpp:251] TEST Top shape for layer 13 'res2a_branch2b' 25 64 32 32 (1638400)
I0628 19:16:23.339639 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0628 19:16:23.339645 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.339656 32034 net.cpp:183] Created Layer res2a_branch2b/bn (14)
I0628 19:16:23.339663 32034 net.cpp:560] res2a_branch2b/bn <- res2a_branch2b
I0628 19:16:23.339668 32034 net.cpp:512] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0628 19:16:23.340932 32034 net.cpp:244] Setting up res2a_branch2b/bn
I0628 19:16:23.340940 32034 net.cpp:251] TEST Top shape for layer 14 'res2a_branch2b/bn' 25 64 32 32 (1638400)
I0628 19:16:23.340955 32034 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0628 19:16:23.340960 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.340963 32034 net.cpp:183] Created Layer res2a_branch2b/relu (15)
I0628 19:16:23.340966 32034 net.cpp:560] res2a_branch2b/relu <- res2a_branch2b
I0628 19:16:23.340970 32034 net.cpp:512] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0628 19:16:23.340973 32034 net.cpp:244] Setting up res2a_branch2b/relu
I0628 19:16:23.340977 32034 net.cpp:251] TEST Top shape for layer 15 'res2a_branch2b/relu' 25 64 32 32 (1638400)
I0628 19:16:23.340979 32034 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0628 19:16:23.340981 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.340986 32034 net.cpp:183] Created Layer pool2 (16)
I0628 19:16:23.340989 32034 net.cpp:560] pool2 <- res2a_branch2b
I0628 19:16:23.340991 32034 net.cpp:529] pool2 -> pool2
I0628 19:16:23.341047 32034 net.cpp:244] Setting up pool2
I0628 19:16:23.341050 32034 net.cpp:251] TEST Top shape for layer 16 'pool2' 25 64 16 16 (409600)
I0628 19:16:23.341053 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0628 19:16:23.341055 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.341063 32034 net.cpp:183] Created Layer res3a_branch2a (17)
I0628 19:16:23.341066 32034 net.cpp:560] res3a_branch2a <- pool2
I0628 19:16:23.341068 32034 net.cpp:529] res3a_branch2a -> res3a_branch2a
I0628 19:16:23.345630 32034 net.cpp:244] Setting up res3a_branch2a
I0628 19:16:23.345641 32034 net.cpp:251] TEST Top shape for layer 17 'res3a_branch2a' 25 128 16 16 (819200)
I0628 19:16:23.345646 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0628 19:16:23.345649 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.345654 32034 net.cpp:183] Created Layer res3a_branch2a/bn (18)
I0628 19:16:23.345657 32034 net.cpp:560] res3a_branch2a/bn <- res3a_branch2a
I0628 19:16:23.345660 32034 net.cpp:512] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0628 19:16:23.346604 32034 net.cpp:244] Setting up res3a_branch2a/bn
I0628 19:16:23.346612 32034 net.cpp:251] TEST Top shape for layer 18 'res3a_branch2a/bn' 25 128 16 16 (819200)
I0628 19:16:23.346621 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0628 19:16:23.346624 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.346627 32034 net.cpp:183] Created Layer res3a_branch2a/relu (19)
I0628 19:16:23.346631 32034 net.cpp:560] res3a_branch2a/relu <- res3a_branch2a
I0628 19:16:23.346633 32034 net.cpp:512] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0628 19:16:23.346638 32034 net.cpp:244] Setting up res3a_branch2a/relu
I0628 19:16:23.346642 32034 net.cpp:251] TEST Top shape for layer 19 'res3a_branch2a/relu' 25 128 16 16 (819200)
I0628 19:16:23.346644 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0628 19:16:23.346647 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.346652 32034 net.cpp:183] Created Layer res3a_branch2b (20)
I0628 19:16:23.346654 32034 net.cpp:560] res3a_branch2b <- res3a_branch2a
I0628 19:16:23.346657 32034 net.cpp:529] res3a_branch2b -> res3a_branch2b
I0628 19:16:23.348682 32034 net.cpp:244] Setting up res3a_branch2b
I0628 19:16:23.348690 32034 net.cpp:251] TEST Top shape for layer 20 'res3a_branch2b' 25 128 16 16 (819200)
I0628 19:16:23.348695 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0628 19:16:23.348698 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.348703 32034 net.cpp:183] Created Layer res3a_branch2b/bn (21)
I0628 19:16:23.348713 32034 net.cpp:560] res3a_branch2b/bn <- res3a_branch2b
I0628 19:16:23.348717 32034 net.cpp:512] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0628 19:16:23.349653 32034 net.cpp:244] Setting up res3a_branch2b/bn
I0628 19:16:23.349660 32034 net.cpp:251] TEST Top shape for layer 21 'res3a_branch2b/bn' 25 128 16 16 (819200)
I0628 19:16:23.349668 32034 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0628 19:16:23.349670 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.349674 32034 net.cpp:183] Created Layer res3a_branch2b/relu (22)
I0628 19:16:23.349678 32034 net.cpp:560] res3a_branch2b/relu <- res3a_branch2b
I0628 19:16:23.349680 32034 net.cpp:512] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0628 19:16:23.349684 32034 net.cpp:244] Setting up res3a_branch2b/relu
I0628 19:16:23.349687 32034 net.cpp:251] TEST Top shape for layer 22 'res3a_branch2b/relu' 25 128 16 16 (819200)
I0628 19:16:23.349690 32034 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0628 19:16:23.349694 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.349697 32034 net.cpp:183] Created Layer pool3 (23)
I0628 19:16:23.349699 32034 net.cpp:560] pool3 <- res3a_branch2b
I0628 19:16:23.349701 32034 net.cpp:529] pool3 -> pool3
I0628 19:16:23.349751 32034 net.cpp:244] Setting up pool3
I0628 19:16:23.349756 32034 net.cpp:251] TEST Top shape for layer 23 'pool3' 25 128 16 16 (819200)
I0628 19:16:23.349758 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0628 19:16:23.349761 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.349767 32034 net.cpp:183] Created Layer res4a_branch2a (24)
I0628 19:16:23.349769 32034 net.cpp:560] res4a_branch2a <- pool3
I0628 19:16:23.349772 32034 net.cpp:529] res4a_branch2a -> res4a_branch2a
I0628 19:16:23.359735 32034 net.cpp:244] Setting up res4a_branch2a
I0628 19:16:23.359742 32034 net.cpp:251] TEST Top shape for layer 24 'res4a_branch2a' 25 256 16 16 (1638400)
I0628 19:16:23.359747 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0628 19:16:23.359750 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.359755 32034 net.cpp:183] Created Layer res4a_branch2a/bn (25)
I0628 19:16:23.359757 32034 net.cpp:560] res4a_branch2a/bn <- res4a_branch2a
I0628 19:16:23.359760 32034 net.cpp:512] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0628 19:16:23.360709 32034 net.cpp:244] Setting up res4a_branch2a/bn
I0628 19:16:23.360718 32034 net.cpp:251] TEST Top shape for layer 25 'res4a_branch2a/bn' 25 256 16 16 (1638400)
I0628 19:16:23.360723 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0628 19:16:23.360725 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.360728 32034 net.cpp:183] Created Layer res4a_branch2a/relu (26)
I0628 19:16:23.360730 32034 net.cpp:560] res4a_branch2a/relu <- res4a_branch2a
I0628 19:16:23.360733 32034 net.cpp:512] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0628 19:16:23.360736 32034 net.cpp:244] Setting up res4a_branch2a/relu
I0628 19:16:23.360739 32034 net.cpp:251] TEST Top shape for layer 26 'res4a_branch2a/relu' 25 256 16 16 (1638400)
I0628 19:16:23.360741 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0628 19:16:23.360744 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.360749 32034 net.cpp:183] Created Layer res4a_branch2b (27)
I0628 19:16:23.360751 32034 net.cpp:560] res4a_branch2b <- res4a_branch2a
I0628 19:16:23.360754 32034 net.cpp:529] res4a_branch2b -> res4a_branch2b
I0628 19:16:23.366919 32034 net.cpp:244] Setting up res4a_branch2b
I0628 19:16:23.366940 32034 net.cpp:251] TEST Top shape for layer 27 'res4a_branch2b' 25 256 16 16 (1638400)
I0628 19:16:23.366966 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0628 19:16:23.366971 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.366982 32034 net.cpp:183] Created Layer res4a_branch2b/bn (28)
I0628 19:16:23.366988 32034 net.cpp:560] res4a_branch2b/bn <- res4a_branch2b
I0628 19:16:23.366992 32034 net.cpp:512] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0628 19:16:23.368288 32034 net.cpp:244] Setting up res4a_branch2b/bn
I0628 19:16:23.368299 32034 net.cpp:251] TEST Top shape for layer 28 'res4a_branch2b/bn' 25 256 16 16 (1638400)
I0628 19:16:23.368307 32034 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0628 19:16:23.368311 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.368316 32034 net.cpp:183] Created Layer res4a_branch2b/relu (29)
I0628 19:16:23.368320 32034 net.cpp:560] res4a_branch2b/relu <- res4a_branch2b
I0628 19:16:23.368324 32034 net.cpp:512] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0628 19:16:23.368331 32034 net.cpp:244] Setting up res4a_branch2b/relu
I0628 19:16:23.368336 32034 net.cpp:251] TEST Top shape for layer 29 'res4a_branch2b/relu' 25 256 16 16 (1638400)
I0628 19:16:23.368340 32034 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0628 19:16:23.368345 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.368351 32034 net.cpp:183] Created Layer pool4 (30)
I0628 19:16:23.368355 32034 net.cpp:560] pool4 <- res4a_branch2b
I0628 19:16:23.368360 32034 net.cpp:529] pool4 -> pool4
I0628 19:16:23.368433 32034 net.cpp:244] Setting up pool4
I0628 19:16:23.368439 32034 net.cpp:251] TEST Top shape for layer 30 'pool4' 25 256 8 8 (409600)
I0628 19:16:23.368443 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0628 19:16:23.368448 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.368465 32034 net.cpp:183] Created Layer res5a_branch2a (31)
I0628 19:16:23.368470 32034 net.cpp:560] res5a_branch2a <- pool4
I0628 19:16:23.368474 32034 net.cpp:529] res5a_branch2a -> res5a_branch2a
I0628 19:16:23.401430 32034 net.cpp:244] Setting up res5a_branch2a
I0628 19:16:23.401461 32034 net.cpp:251] TEST Top shape for layer 31 'res5a_branch2a' 25 512 8 8 (819200)
I0628 19:16:23.401473 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0628 19:16:23.401482 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.401499 32034 net.cpp:183] Created Layer res5a_branch2a/bn (32)
I0628 19:16:23.401509 32034 net.cpp:560] res5a_branch2a/bn <- res5a_branch2a
I0628 19:16:23.401515 32034 net.cpp:512] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0628 19:16:23.402819 32034 net.cpp:244] Setting up res5a_branch2a/bn
I0628 19:16:23.402828 32034 net.cpp:251] TEST Top shape for layer 32 'res5a_branch2a/bn' 25 512 8 8 (819200)
I0628 19:16:23.402835 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0628 19:16:23.402838 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.402842 32034 net.cpp:183] Created Layer res5a_branch2a/relu (33)
I0628 19:16:23.402843 32034 net.cpp:560] res5a_branch2a/relu <- res5a_branch2a
I0628 19:16:23.402846 32034 net.cpp:512] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0628 19:16:23.402851 32034 net.cpp:244] Setting up res5a_branch2a/relu
I0628 19:16:23.402853 32034 net.cpp:251] TEST Top shape for layer 33 'res5a_branch2a/relu' 25 512 8 8 (819200)
I0628 19:16:23.402855 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0628 19:16:23.402858 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.402864 32034 net.cpp:183] Created Layer res5a_branch2b (34)
I0628 19:16:23.402866 32034 net.cpp:560] res5a_branch2b <- res5a_branch2a
I0628 19:16:23.402878 32034 net.cpp:529] res5a_branch2b -> res5a_branch2b
I0628 19:16:23.418716 32034 net.cpp:244] Setting up res5a_branch2b
I0628 19:16:23.418727 32034 net.cpp:251] TEST Top shape for layer 34 'res5a_branch2b' 25 512 8 8 (819200)
I0628 19:16:23.418740 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0628 19:16:23.418743 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.418748 32034 net.cpp:183] Created Layer res5a_branch2b/bn (35)
I0628 19:16:23.418751 32034 net.cpp:560] res5a_branch2b/bn <- res5a_branch2b
I0628 19:16:23.418754 32034 net.cpp:512] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0628 19:16:23.419737 32034 net.cpp:244] Setting up res5a_branch2b/bn
I0628 19:16:23.419745 32034 net.cpp:251] TEST Top shape for layer 35 'res5a_branch2b/bn' 25 512 8 8 (819200)
I0628 19:16:23.419751 32034 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0628 19:16:23.419754 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.419757 32034 net.cpp:183] Created Layer res5a_branch2b/relu (36)
I0628 19:16:23.419759 32034 net.cpp:560] res5a_branch2b/relu <- res5a_branch2b
I0628 19:16:23.419761 32034 net.cpp:512] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0628 19:16:23.419765 32034 net.cpp:244] Setting up res5a_branch2b/relu
I0628 19:16:23.419769 32034 net.cpp:251] TEST Top shape for layer 36 'res5a_branch2b/relu' 25 512 8 8 (819200)
I0628 19:16:23.419770 32034 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0628 19:16:23.419772 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.419783 32034 net.cpp:183] Created Layer pool5 (37)
I0628 19:16:23.419785 32034 net.cpp:560] pool5 <- res5a_branch2b
I0628 19:16:23.419788 32034 net.cpp:529] pool5 -> pool5
I0628 19:16:23.419816 32034 net.cpp:244] Setting up pool5
I0628 19:16:23.419821 32034 net.cpp:251] TEST Top shape for layer 37 'pool5' 25 512 1 1 (12800)
I0628 19:16:23.419822 32034 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0628 19:16:23.419824 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.419828 32034 net.cpp:183] Created Layer fc10 (38)
I0628 19:16:23.419831 32034 net.cpp:560] fc10 <- pool5
I0628 19:16:23.419832 32034 net.cpp:529] fc10 -> fc10
I0628 19:16:23.420070 32034 net.cpp:244] Setting up fc10
I0628 19:16:23.420078 32034 net.cpp:251] TEST Top shape for layer 38 'fc10' 25 10 (250)
I0628 19:16:23.420081 32034 layer_factory.hpp:136] Creating layer 'fc10_fc10_0_split' of type 'Split'
I0628 19:16:23.420084 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.420087 32034 net.cpp:183] Created Layer fc10_fc10_0_split (39)
I0628 19:16:23.420089 32034 net.cpp:560] fc10_fc10_0_split <- fc10
I0628 19:16:23.420091 32034 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0628 19:16:23.420094 32034 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0628 19:16:23.420096 32034 net.cpp:529] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0628 19:16:23.420147 32034 net.cpp:244] Setting up fc10_fc10_0_split
I0628 19:16:23.420151 32034 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 25 10 (250)
I0628 19:16:23.420155 32034 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 25 10 (250)
I0628 19:16:23.420156 32034 net.cpp:251] TEST Top shape for layer 39 'fc10_fc10_0_split' 25 10 (250)
I0628 19:16:23.420158 32034 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0628 19:16:23.420161 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.420164 32034 net.cpp:183] Created Layer loss (40)
I0628 19:16:23.420166 32034 net.cpp:560] loss <- fc10_fc10_0_split_0
I0628 19:16:23.420168 32034 net.cpp:560] loss <- label_data_1_split_0
I0628 19:16:23.420178 32034 net.cpp:529] loss -> loss
I0628 19:16:23.420298 32034 net.cpp:244] Setting up loss
I0628 19:16:23.420303 32034 net.cpp:251] TEST Top shape for layer 40 'loss' (1)
I0628 19:16:23.420306 32034 net.cpp:255]     with loss weight 1
I0628 19:16:23.420311 32034 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0628 19:16:23.420313 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.420320 32034 net.cpp:183] Created Layer accuracy/top1 (41)
I0628 19:16:23.420321 32034 net.cpp:560] accuracy/top1 <- fc10_fc10_0_split_1
I0628 19:16:23.420325 32034 net.cpp:560] accuracy/top1 <- label_data_1_split_1
I0628 19:16:23.420326 32034 net.cpp:529] accuracy/top1 -> accuracy/top1
I0628 19:16:23.420331 32034 net.cpp:244] Setting up accuracy/top1
I0628 19:16:23.420332 32034 net.cpp:251] TEST Top shape for layer 41 'accuracy/top1' (1)
I0628 19:16:23.420334 32034 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0628 19:16:23.420338 32034 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0628 19:16:23.420342 32034 net.cpp:183] Created Layer accuracy/top5 (42)
I0628 19:16:23.420344 32034 net.cpp:560] accuracy/top5 <- fc10_fc10_0_split_2
I0628 19:16:23.420347 32034 net.cpp:560] accuracy/top5 <- label_data_1_split_2
I0628 19:16:23.420349 32034 net.cpp:529] accuracy/top5 -> accuracy/top5
I0628 19:16:23.420353 32034 net.cpp:244] Setting up accuracy/top5
I0628 19:16:23.420356 32034 net.cpp:251] TEST Top shape for layer 42 'accuracy/top5' (1)
I0628 19:16:23.420358 32034 net.cpp:324] accuracy/top5 does not need backward computation.
I0628 19:16:23.420361 32034 net.cpp:324] accuracy/top1 does not need backward computation.
I0628 19:16:23.420362 32034 net.cpp:322] loss needs backward computation.
I0628 19:16:23.420366 32034 net.cpp:322] fc10_fc10_0_split needs backward computation.
I0628 19:16:23.420367 32034 net.cpp:322] fc10 needs backward computation.
I0628 19:16:23.420369 32034 net.cpp:322] pool5 needs backward computation.
I0628 19:16:23.420372 32034 net.cpp:322] res5a_branch2b/relu needs backward computation.
I0628 19:16:23.420373 32034 net.cpp:322] res5a_branch2b/bn needs backward computation.
I0628 19:16:23.420375 32034 net.cpp:322] res5a_branch2b needs backward computation.
I0628 19:16:23.420377 32034 net.cpp:322] res5a_branch2a/relu needs backward computation.
I0628 19:16:23.420379 32034 net.cpp:322] res5a_branch2a/bn needs backward computation.
I0628 19:16:23.420382 32034 net.cpp:322] res5a_branch2a needs backward computation.
I0628 19:16:23.420384 32034 net.cpp:322] pool4 needs backward computation.
I0628 19:16:23.420387 32034 net.cpp:322] res4a_branch2b/relu needs backward computation.
I0628 19:16:23.420388 32034 net.cpp:322] res4a_branch2b/bn needs backward computation.
I0628 19:16:23.420390 32034 net.cpp:322] res4a_branch2b needs backward computation.
I0628 19:16:23.420392 32034 net.cpp:322] res4a_branch2a/relu needs backward computation.
I0628 19:16:23.420394 32034 net.cpp:322] res4a_branch2a/bn needs backward computation.
I0628 19:16:23.420397 32034 net.cpp:322] res4a_branch2a needs backward computation.
I0628 19:16:23.420399 32034 net.cpp:322] pool3 needs backward computation.
I0628 19:16:23.420402 32034 net.cpp:322] res3a_branch2b/relu needs backward computation.
I0628 19:16:23.420403 32034 net.cpp:322] res3a_branch2b/bn needs backward computation.
I0628 19:16:23.420405 32034 net.cpp:322] res3a_branch2b needs backward computation.
I0628 19:16:23.420408 32034 net.cpp:322] res3a_branch2a/relu needs backward computation.
I0628 19:16:23.420410 32034 net.cpp:322] res3a_branch2a/bn needs backward computation.
I0628 19:16:23.420413 32034 net.cpp:322] res3a_branch2a needs backward computation.
I0628 19:16:23.420414 32034 net.cpp:322] pool2 needs backward computation.
I0628 19:16:23.420416 32034 net.cpp:322] res2a_branch2b/relu needs backward computation.
I0628 19:16:23.420418 32034 net.cpp:322] res2a_branch2b/bn needs backward computation.
I0628 19:16:23.420420 32034 net.cpp:322] res2a_branch2b needs backward computation.
I0628 19:16:23.420428 32034 net.cpp:322] res2a_branch2a/relu needs backward computation.
I0628 19:16:23.420429 32034 net.cpp:322] res2a_branch2a/bn needs backward computation.
I0628 19:16:23.420433 32034 net.cpp:322] res2a_branch2a needs backward computation.
I0628 19:16:23.420434 32034 net.cpp:322] pool1 needs backward computation.
I0628 19:16:23.420436 32034 net.cpp:322] conv1b/relu needs backward computation.
I0628 19:16:23.420439 32034 net.cpp:322] conv1b/bn needs backward computation.
I0628 19:16:23.420441 32034 net.cpp:322] conv1b needs backward computation.
I0628 19:16:23.420442 32034 net.cpp:322] conv1a/relu needs backward computation.
I0628 19:16:23.420445 32034 net.cpp:322] conv1a/bn needs backward computation.
I0628 19:16:23.420447 32034 net.cpp:322] conv1a needs backward computation.
I0628 19:16:23.420450 32034 net.cpp:324] data/bias does not need backward computation.
I0628 19:16:23.420454 32034 net.cpp:324] label_data_1_split does not need backward computation.
I0628 19:16:23.420457 32034 net.cpp:324] data does not need backward computation.
I0628 19:16:23.420459 32034 net.cpp:366] This network produces output accuracy/top1
I0628 19:16:23.420462 32034 net.cpp:366] This network produces output accuracy/top5
I0628 19:16:23.420464 32034 net.cpp:366] This network produces output loss
I0628 19:16:23.420490 32034 net.cpp:388] Top memory (TEST) required for data: 137625600 diff: 91750408
I0628 19:16:23.420492 32034 net.cpp:391] Bottom memory (TEST) required for data: 137625600 diff: 137625600
I0628 19:16:23.420495 32034 net.cpp:394] Shared (in-place) memory (TEST) by data: 91750400 diff: 91750400
I0628 19:16:23.420497 32034 net.cpp:397] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0628 19:16:23.420498 32034 net.cpp:400] Parameters shared memory (TEST) by data: 0 diff: 0
I0628 19:16:23.420501 32034 net.cpp:406] Network initialization done.
I0628 19:16:23.420552 32034 solver.cpp:56] Solver scaffolding done.
I0628 19:16:23.424154 32034 caffe.cpp:137] Finetuning from training/cifar10_jacintonet11v2_2017-06-28_18-56-45/initial/cifar10_jacintonet11v2_iter_64000.caffemodel
I0628 19:16:23.428874 32034 net.cpp:1087] Copying source layer data Type:Data #blobs=0
I0628 19:16:23.428899 32034 net.cpp:1087] Copying source layer data/bias Type:Bias #blobs=1
I0628 19:16:23.428936 32034 net.cpp:1087] Copying source layer conv1a Type:Convolution #blobs=2
I0628 19:16:23.428953 32034 net.cpp:1087] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.429221 32034 net.cpp:1087] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0628 19:16:23.429229 32034 net.cpp:1087] Copying source layer conv1b Type:Convolution #blobs=2
I0628 19:16:23.429240 32034 net.cpp:1087] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.429402 32034 net.cpp:1087] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0628 19:16:23.429407 32034 net.cpp:1087] Copying source layer pool1 Type:Pooling #blobs=0
I0628 19:16:23.429411 32034 net.cpp:1087] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0628 19:16:23.429430 32034 net.cpp:1087] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.429600 32034 net.cpp:1087] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0628 19:16:23.429606 32034 net.cpp:1087] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0628 19:16:23.429621 32034 net.cpp:1087] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.429775 32034 net.cpp:1087] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0628 19:16:23.429781 32034 net.cpp:1087] Copying source layer pool2 Type:Pooling #blobs=0
I0628 19:16:23.429785 32034 net.cpp:1087] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0628 19:16:23.429827 32034 net.cpp:1087] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.429970 32034 net.cpp:1087] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0628 19:16:23.429976 32034 net.cpp:1087] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0628 19:16:23.430016 32034 net.cpp:1087] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.430148 32034 net.cpp:1087] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0628 19:16:23.430155 32034 net.cpp:1087] Copying source layer pool3 Type:Pooling #blobs=0
I0628 19:16:23.430157 32034 net.cpp:1087] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0628 19:16:23.430279 32034 net.cpp:1087] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.430418 32034 net.cpp:1087] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0628 19:16:23.430424 32034 net.cpp:1087] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0628 19:16:23.430490 32034 net.cpp:1087] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.430629 32034 net.cpp:1087] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0628 19:16:23.430634 32034 net.cpp:1087] Copying source layer pool4 Type:Pooling #blobs=0
I0628 19:16:23.430637 32034 net.cpp:1087] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0628 19:16:23.431035 32034 net.cpp:1087] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.431182 32034 net.cpp:1087] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0628 19:16:23.431188 32034 net.cpp:1087] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0628 19:16:23.431370 32034 net.cpp:1087] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.431514 32034 net.cpp:1087] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0628 19:16:23.431519 32034 net.cpp:1087] Copying source layer pool5 Type:Pooling #blobs=0
I0628 19:16:23.431522 32034 net.cpp:1087] Copying source layer fc10 Type:InnerProduct #blobs=2
I0628 19:16:23.431535 32034 net.cpp:1087] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0628 19:16:23.435065 32034 net.cpp:1087] Copying source layer data Type:Data #blobs=0
I0628 19:16:23.435081 32034 net.cpp:1087] Copying source layer data/bias Type:Bias #blobs=1
I0628 19:16:23.435104 32034 net.cpp:1087] Copying source layer conv1a Type:Convolution #blobs=2
I0628 19:16:23.435115 32034 net.cpp:1087] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.435307 32034 net.cpp:1087] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0628 19:16:23.435312 32034 net.cpp:1087] Copying source layer conv1b Type:Convolution #blobs=2
I0628 19:16:23.435319 32034 net.cpp:1087] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.435436 32034 net.cpp:1087] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0628 19:16:23.435439 32034 net.cpp:1087] Copying source layer pool1 Type:Pooling #blobs=0
I0628 19:16:23.435441 32034 net.cpp:1087] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0628 19:16:23.435456 32034 net.cpp:1087] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.435582 32034 net.cpp:1087] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0628 19:16:23.435586 32034 net.cpp:1087] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0628 19:16:23.435598 32034 net.cpp:1087] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.435719 32034 net.cpp:1087] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0628 19:16:23.435722 32034 net.cpp:1087] Copying source layer pool2 Type:Pooling #blobs=0
I0628 19:16:23.435724 32034 net.cpp:1087] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0628 19:16:23.435761 32034 net.cpp:1087] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.435866 32034 net.cpp:1087] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0628 19:16:23.435870 32034 net.cpp:1087] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0628 19:16:23.435891 32034 net.cpp:1087] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.435991 32034 net.cpp:1087] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0628 19:16:23.436002 32034 net.cpp:1087] Copying source layer pool3 Type:Pooling #blobs=0
I0628 19:16:23.436004 32034 net.cpp:1087] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0628 19:16:23.436115 32034 net.cpp:1087] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.436214 32034 net.cpp:1087] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0628 19:16:23.436218 32034 net.cpp:1087] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0628 19:16:23.436271 32034 net.cpp:1087] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.436372 32034 net.cpp:1087] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0628 19:16:23.436377 32034 net.cpp:1087] Copying source layer pool4 Type:Pooling #blobs=0
I0628 19:16:23.436379 32034 net.cpp:1087] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0628 19:16:23.436739 32034 net.cpp:1087] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0628 19:16:23.436846 32034 net.cpp:1087] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0628 19:16:23.436849 32034 net.cpp:1087] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0628 19:16:23.437006 32034 net.cpp:1087] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0628 19:16:23.437120 32034 net.cpp:1087] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0628 19:16:23.437125 32034 net.cpp:1087] Copying source layer pool5 Type:Pooling #blobs=0
I0628 19:16:23.437129 32034 net.cpp:1087] Copying source layer fc10 Type:InnerProduct #blobs=2
I0628 19:16:23.437137 32034 net.cpp:1087] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0628 19:16:23.437203 32034 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0628 19:16:23.437207 32034 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0628 19:16:23.437209 32034 parallel.cpp:59] Starting Optimization
I0628 19:16:23.437211 32034 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0628 19:16:23.437237 32034 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0628 19:16:23.437836 32082 device_alternate.hpp:116] NVML initialized on thread 140590727554816
I0628 19:16:23.449897 32082 common.cpp:563] NVML succeeded to set CPU affinity on device 0
I0628 19:16:23.449942 32083 device_alternate.hpp:116] NVML initialized on thread 140590719162112
I0628 19:16:23.451162 32083 common.cpp:563] NVML succeeded to set CPU affinity on device 1
I0628 19:16:23.455222 32083 solver.cpp:42] Solver data type: FLOAT
I0628 19:16:23.455693 32083 net.cpp:108] Using FLOAT as default forward math type
I0628 19:16:23.455699 32083 net.cpp:114] Using FLOAT as default backward math type
I0628 19:16:23.455724 32083 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 32
I0628 19:16:23.455732 32083 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0628 19:16:23.456372 32084 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0628 19:16:23.457113 32083 data_layer.cpp:188] ReshapePrefetch 32, 3, 32, 32
I0628 19:16:23.457176 32083 data_layer.cpp:206] Output data size: 32, 3, 32, 32
I0628 19:16:23.457181 32083 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0628 19:16:23.736929 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 0  (limit 8.25G, req 0G)
I0628 19:16:23.744374 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 0  (limit 8.23G, req 0G)
I0628 19:16:23.757426 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.2G, req 0G)
I0628 19:16:23.764397 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.19G, req 0G)
I0628 19:16:23.777598 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.17G, req 0.02G)
I0628 19:16:23.783074 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0.02G)
I0628 19:16:23.805760 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.14G, req 0.02G)
I0628 19:16:23.815860 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.12G, req 0.02G)
I0628 19:16:23.862053 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 3  (limit 8.09G, req 0.02G)
I0628 19:16:23.882817 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 5 5  (limit 8.08G, req 0.02G)
I0628 19:16:23.884151 32083 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/test.prototxt
I0628 19:16:23.884270 32083 net.cpp:108] Using FLOAT as default forward math type
I0628 19:16:23.884275 32083 net.cpp:114] Using FLOAT as default backward math type
I0628 19:16:23.884292 32083 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 25
I0628 19:16:23.884299 32083 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0628 19:16:23.884932 32086 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0628 19:16:23.885007 32083 data_layer.cpp:188] ReshapePrefetch 25, 3, 32, 32
I0628 19:16:23.885066 32083 data_layer.cpp:206] Output data size: 25, 3, 32, 32
I0628 19:16:23.885069 32083 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0628 19:16:23.886028 32087 data_layer.cpp:188] ReshapePrefetch 25, 3, 32, 32
I0628 19:16:23.886034 32087 data_layer.cpp:206] Output data size: 25, 3, 32, 32
I0628 19:16:23.886831 32087 data_layer.cpp:110] [1] Parser threads: 1
I0628 19:16:23.886838 32087 data_layer.cpp:112] [1] Transformer threads: 1
I0628 19:16:23.976263 32083 solver.cpp:56] Solver scaffolding done.
I0628 19:16:23.992076 32083 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0628 19:16:23.992076 32082 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0628 19:16:24.092077 32083 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:16:24.092100 32082 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:16:24.094200 32083 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:16:24.094259 32083 solver.cpp:474] Solving jacintonet11v2_train
I0628 19:16:24.094264 32083 solver.cpp:475] Learning Rate Policy: poly
I0628 19:16:24.103812 32082 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:16:24.103890 32082 solver.cpp:474] Solving jacintonet11v2_train
I0628 19:16:24.103895 32082 solver.cpp:475] Learning Rate Policy: poly
I0628 19:16:24.108479 32082 solver.cpp:268] Starting Optimization on GPU 0
I0628 19:16:24.108484 32083 solver.cpp:268] Starting Optimization on GPU 1
I0628 19:16:24.108516 32082 solver.cpp:545] Iteration 0, Testing net (#0)
I0628 19:16:24.108552 32089 device_alternate.hpp:116] NVML initialized on thread 140590326802176
I0628 19:16:24.108570 32089 common.cpp:563] NVML succeeded to set CPU affinity on device 1
I0628 19:16:24.109127 32088 device_alternate.hpp:116] NVML initialized on thread 140590335194880
I0628 19:16:24.109138 32088 common.cpp:563] NVML succeeded to set CPU affinity on device 0
I0628 19:16:24.156026 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.92
I0628 19:16:24.156042 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 1
I0628 19:16:24.156047 32082 solver.cpp:630]     Test net output #2: loss = 0.230969 (* 1 = 0.230969 loss)
I0628 19:16:24.156050 32082 solver.cpp:295] [MultiGPU] Initial Test completed
I0628 19:16:24.156061 32082 blocking_queue.cpp:40] Data layer prefetch queue empty
I0628 19:16:24.164827 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.76G, req 0.02G)
I0628 19:16:24.165887 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.69G, req 0.01G)
I0628 19:16:24.170982 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.76G, req 0.02G)
I0628 19:16:24.171963 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.68G, req 0.01G)
I0628 19:16:24.180423 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 1  (limit 7.74G, req 0.02G)
I0628 19:16:24.181746 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.67G, req 0.01G)
I0628 19:16:24.186123 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.73G, req 0.02G)
I0628 19:16:24.187412 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.65G, req 0.01G)
I0628 19:16:24.194908 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.71G, req 0.02G)
I0628 19:16:24.197582 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.64G, req 0.02G)
I0628 19:16:24.199347 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.7G, req 0.02G)
I0628 19:16:24.202584 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.63G, req 0.02G)
I0628 19:16:24.214046 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.69G, req 0.02G)
I0628 19:16:24.218883 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.61G, req 0.02G)
I0628 19:16:24.220908 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.67G, req 0.02G)
I0628 19:16:24.226111 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.6G, req 0.02G)
I0628 19:16:24.241638 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 3  (limit 7.65G, req 0.02G)
I0628 19:16:24.247148 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 3  (limit 7.57G, req 0.02G)
I0628 19:16:24.248780 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 7 5 5  (limit 7.64G, req 0.02G)
I0628 19:16:24.254791 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 7 5 5  (limit 7.56G, req 0.02G)
I0628 19:16:24.271425 32085 data_layer.cpp:188] ReshapePrefetch 32, 3, 32, 32
I0628 19:16:24.271440 32085 data_layer.cpp:206] Output data size: 32, 3, 32, 32
I0628 19:16:24.271509 32085 data_layer.cpp:110] [1] Parser threads: 1
I0628 19:16:24.271517 32085 data_layer.cpp:112] [1] Transformer threads: 1
I0628 19:16:24.276278 32051 data_layer.cpp:188] ReshapePrefetch 32, 3, 32, 32
I0628 19:16:24.276290 32051 data_layer.cpp:206] Output data size: 32, 3, 32, 32
I0628 19:16:24.276361 32051 data_layer.cpp:110] [0] Parser threads: 1
I0628 19:16:24.276368 32051 data_layer.cpp:112] [0] Transformer threads: 1
I0628 19:16:24.276680 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.02
I0628 19:16:24.282459 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 0  (limit 7.56G, req 0.02G)
I0628 19:16:24.287362 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.56G, req 0.02G)
I0628 19:16:24.295169 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 1  (limit 7.56G, req 0.02G)
I0628 19:16:24.299315 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.56G, req 0.02G)
I0628 19:16:24.305445 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.56G, req 0.02G)
I0628 19:16:24.308485 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.56G, req 0.02G)
I0628 19:16:24.320910 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0.02G)
I0628 19:16:24.327947 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.56G, req 0.02G)
I0628 19:16:24.344398 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.56G, req 0.02G)
I0628 19:16:24.350427 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 7 5 5  (limit 7.56G, req 0.02G)
I0628 19:16:24.436935 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:16:24.440271 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:16:24.440646 32082 solver.cpp:354] Iteration 0 (0.284538 s), loss = 0.00127412
I0628 19:16:24.440663 32082 solver.cpp:371]     Train net output #0: loss = 0.00127412 (* 1 = 0.00127412 loss)
I0628 19:16:24.440670 32082 sgd_solver.cpp:137] Iteration 0, lr = 0.01, m = 0.9
I0628 19:16:24.446297 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.45G, req 0.02G)
I0628 19:16:24.451243 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.45G, req 0.02G)
I0628 19:16:24.458339 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.45G, req 0.02G)
I0628 19:16:24.463274 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.45G, req 0.02G)
I0628 19:16:24.470731 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.45G, req 0.02G)
I0628 19:16:24.474318 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.45G, req 0.02G)
I0628 19:16:24.492055 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.45G, req 0.02G)
I0628 19:16:24.501844 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.45G, req 0.02G)
I0628 19:16:24.524276 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 3  (limit 7.45G, req 0.02G)
I0628 19:16:24.533179 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 7 5 5  (limit 7.45G, req 0.02G)
I0628 19:16:24.546957 32082 solver.cpp:354] Iteration 1 (0.106255 s), loss = 0.00123006
I0628 19:16:24.546990 32082 solver.cpp:371]     Train net output #0: loss = 0.00123006 (* 1 = 0.00123006 loss)
I0628 19:16:24.555524 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.68G/1 1 0 0  (limit 6.9G, req 0.02G)
I0628 19:16:24.557387 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.68G/1 1 0 0  (limit 6.79G, req 0.02G)
I0628 19:16:24.563413 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.35G/2 1 1 3  (limit 6.22G, req 0.02G)
I0628 19:16:24.564997 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.35G/2 1 1 3  (limit 6.11G, req 0.02G)
I0628 19:16:24.578878 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.35G/1 6 4 3  (limit 6.22G, req 0.02G)
I0628 19:16:24.581668 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.35G/1 6 4 3  (limit 6.11G, req 0.02G)
I0628 19:16:24.586210 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.35G/2 6 4 3  (limit 6.22G, req 0.02G)
I0628 19:16:24.589797 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.35G/2 6 4 3  (limit 6.11G, req 0.02G)
I0628 19:16:24.597297 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.35G/1 6 4 5  (limit 6.22G, req 0.02G)
I0628 19:16:24.601249 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.35G/1 6 4 5  (limit 6.11G, req 0.02G)
I0628 19:16:24.602174 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.35G/2 6 4 0  (limit 6.22G, req 0.02G)
I0628 19:16:24.606168 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.35G/2 6 4 0  (limit 6.11G, req 0.02G)
I0628 19:16:24.630295 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.35G/1 6 4 5  (limit 6.22G, req 0.03G)
I0628 19:16:24.635841 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.35G/1 6 4 5  (limit 6.11G, req 0.03G)
I0628 19:16:24.640148 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.35G/2 6 4 3  (limit 6.22G, req 0.03G)
I0628 19:16:24.644145 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.35G/2 6 4 3  (limit 6.11G, req 0.03G)
I0628 19:16:24.681107 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.35G/1 7 5 5  (limit 6.22G, req 0.03G)
I0628 19:16:24.687614 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.35G/1 7 5 5  (limit 6.11G, req 0.03G)
I0628 19:16:24.692947 32083 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.35G/2 7 5 5  (limit 6.22G, req 0.03G)
I0628 19:16:24.696846 32082 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.35G/2 7 5 5  (limit 6.11G, req 0.03G)
I0628 19:16:24.708863 32082 solver.cpp:349] Iteration 2 (6.17939 iter/s, 0.161828s/100 iter), loss = 0.000986893
I0628 19:16:24.708884 32082 solver.cpp:371]     Train net output #0: loss = 0.000986893 (* 1 = 0.000986893 loss)
I0628 19:16:24.743780 32083 cudnn_conv_layer.cpp:283] [1] Layer 'conv1a' reallocating workspace: 1.35G -> 0.07G
I0628 19:16:24.743875 32082 cudnn_conv_layer.cpp:283] [0] Layer 'conv1a' reallocating workspace: 1.35G -> 0.07G
I0628 19:16:26.401309 32082 solver.cpp:349] Iteration 100 (57.9145 iter/s, 1.69215s/100 iter), loss = 0.000659005
I0628 19:16:26.401331 32082 solver.cpp:371]     Train net output #0: loss = 0.000659005 (* 1 = 0.000659005 loss)
I0628 19:16:26.401335 32082 sgd_solver.cpp:137] Iteration 100, lr = 0.00998437, m = 0.9
I0628 19:16:28.120751 32082 solver.cpp:349] Iteration 200 (58.1687 iter/s, 1.71914s/100 iter), loss = 0.00237289
I0628 19:16:28.120779 32082 solver.cpp:371]     Train net output #0: loss = 0.00237289 (* 1 = 0.00237289 loss)
I0628 19:16:28.120784 32082 sgd_solver.cpp:137] Iteration 200, lr = 0.00996875, m = 0.9
I0628 19:16:29.838142 32082 solver.cpp:349] Iteration 300 (58.2384 iter/s, 1.71708s/100 iter), loss = 0.00174145
I0628 19:16:29.838169 32082 solver.cpp:371]     Train net output #0: loss = 0.00174145 (* 1 = 0.00174145 loss)
I0628 19:16:29.838176 32082 sgd_solver.cpp:137] Iteration 300, lr = 0.00995312, m = 0.9
I0628 19:16:31.555893 32082 solver.cpp:349] Iteration 400 (58.2262 iter/s, 1.71744s/100 iter), loss = 0.00140638
I0628 19:16:31.555919 32082 solver.cpp:371]     Train net output #0: loss = 0.00140638 (* 1 = 0.00140638 loss)
I0628 19:16:31.555925 32082 sgd_solver.cpp:137] Iteration 400, lr = 0.0099375, m = 0.9
I0628 19:16:33.273975 32082 solver.cpp:349] Iteration 500 (58.2149 iter/s, 1.71777s/100 iter), loss = 0.000838197
I0628 19:16:33.273998 32082 solver.cpp:371]     Train net output #0: loss = 0.000838196 (* 1 = 0.000838196 loss)
I0628 19:16:33.274003 32082 sgd_solver.cpp:137] Iteration 500, lr = 0.00992187, m = 0.9
I0628 19:16:34.993888 32082 solver.cpp:349] Iteration 600 (58.1528 iter/s, 1.71961s/100 iter), loss = 0.00101999
I0628 19:16:34.993916 32082 solver.cpp:371]     Train net output #0: loss = 0.00101999 (* 1 = 0.00101999 loss)
I0628 19:16:34.993921 32082 sgd_solver.cpp:137] Iteration 600, lr = 0.00990625, m = 0.9
I0628 19:16:36.715065 32082 solver.cpp:349] Iteration 700 (58.1103 iter/s, 1.72087s/100 iter), loss = 0.00143992
I0628 19:16:36.715090 32082 solver.cpp:371]     Train net output #0: loss = 0.00143992 (* 1 = 0.00143992 loss)
I0628 19:16:36.715113 32082 sgd_solver.cpp:137] Iteration 700, lr = 0.00989062, m = 0.9
I0628 19:16:38.054155 32050 data_reader.cpp:262] Starting prefetch of epoch 1
I0628 19:16:38.432346 32082 solver.cpp:349] Iteration 800 (58.2427 iter/s, 1.71695s/100 iter), loss = 0.000767142
I0628 19:16:38.432371 32082 solver.cpp:371]     Train net output #0: loss = 0.000767141 (* 1 = 0.000767141 loss)
I0628 19:16:38.432377 32082 sgd_solver.cpp:137] Iteration 800, lr = 0.009875, m = 0.9
I0628 19:16:40.153707 32082 solver.cpp:349] Iteration 900 (58.1041 iter/s, 1.72105s/100 iter), loss = 0.000887107
I0628 19:16:40.153733 32082 solver.cpp:371]     Train net output #0: loss = 0.000887106 (* 1 = 0.000887106 loss)
I0628 19:16:40.153739 32082 sgd_solver.cpp:137] Iteration 900, lr = 0.00985937, m = 0.9
I0628 19:16:41.860955 32082 solver.cpp:401] Sparsity after update:
I0628 19:16:41.862035 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:16:41.862043 32082 net.cpp:2170] conv1a_param_0(0.01) 
I0628 19:16:41.862056 32082 net.cpp:2170] conv1b_param_0(0.02) 
I0628 19:16:41.862062 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:16:41.862066 32082 net.cpp:2170] res2a_branch2a_param_0(0.02) 
I0628 19:16:41.862069 32082 net.cpp:2170] res2a_branch2b_param_0(0.02) 
I0628 19:16:41.862074 32082 net.cpp:2170] res3a_branch2a_param_0(0.02) 
I0628 19:16:41.862078 32082 net.cpp:2170] res3a_branch2b_param_0(0.02) 
I0628 19:16:41.862083 32082 net.cpp:2170] res4a_branch2a_param_0(0.02) 
I0628 19:16:41.862087 32082 net.cpp:2170] res4a_branch2b_param_0(0.02) 
I0628 19:16:41.862092 32082 net.cpp:2170] res5a_branch2a_param_0(0.02) 
I0628 19:16:41.862097 32082 net.cpp:2170] res5a_branch2b_param_0(0.02) 
I0628 19:16:41.862100 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (47060/2.3599e+06) 0.0199
I0628 19:16:41.862112 32082 solver.cpp:545] Iteration 1000, Testing net (#0)
I0628 19:16:42.860646 32080 data_reader.cpp:262] Starting prefetch of epoch 1
I0628 19:16:42.882050 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9168
I0628 19:16:42.882063 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9964
I0628 19:16:42.882068 32082 solver.cpp:630]     Test net output #2: loss = 0.299202 (* 1 = 0.299202 loss)
I0628 19:16:42.882081 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01981s
I0628 19:16:42.899294 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.04
I0628 19:16:43.024837 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:16:43.026312 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:16:43.026671 32082 solver.cpp:349] Iteration 1000 (34.8131 iter/s, 2.87248s/100 iter), loss = 0.00151083
I0628 19:16:43.026690 32082 solver.cpp:371]     Train net output #0: loss = 0.00151083 (* 1 = 0.00151083 loss)
I0628 19:16:43.026695 32082 sgd_solver.cpp:137] Iteration 1000, lr = 0.00984375, m = 0.9
I0628 19:16:44.751830 32082 solver.cpp:349] Iteration 1100 (57.9758 iter/s, 1.72486s/100 iter), loss = 0.00245544
I0628 19:16:44.751857 32082 solver.cpp:371]     Train net output #0: loss = 0.00245544 (* 1 = 0.00245544 loss)
I0628 19:16:44.751863 32082 sgd_solver.cpp:137] Iteration 1100, lr = 0.00982813, m = 0.9
I0628 19:16:46.470062 32082 solver.cpp:349] Iteration 1200 (58.2099 iter/s, 1.71792s/100 iter), loss = 0.00142794
I0628 19:16:46.470088 32082 solver.cpp:371]     Train net output #0: loss = 0.00142794 (* 1 = 0.00142794 loss)
I0628 19:16:46.470093 32082 sgd_solver.cpp:137] Iteration 1200, lr = 0.0098125, m = 0.9
I0628 19:16:48.191891 32082 solver.cpp:349] Iteration 1300 (58.0882 iter/s, 1.72152s/100 iter), loss = 0.00138737
I0628 19:16:48.191913 32082 solver.cpp:371]     Train net output #0: loss = 0.00138737 (* 1 = 0.00138737 loss)
I0628 19:16:48.191917 32082 sgd_solver.cpp:137] Iteration 1300, lr = 0.00979687, m = 0.9
I0628 19:16:49.909384 32082 solver.cpp:349] Iteration 1400 (58.2347 iter/s, 1.71719s/100 iter), loss = 0.00105391
I0628 19:16:49.909409 32082 solver.cpp:371]     Train net output #0: loss = 0.00105391 (* 1 = 0.00105391 loss)
I0628 19:16:49.909430 32082 sgd_solver.cpp:137] Iteration 1400, lr = 0.00978125, m = 0.9
I0628 19:16:51.629667 32082 solver.cpp:349] Iteration 1500 (58.141 iter/s, 1.71996s/100 iter), loss = 0.000944691
I0628 19:16:51.629690 32082 solver.cpp:371]     Train net output #0: loss = 0.00094469 (* 1 = 0.00094469 loss)
I0628 19:16:51.629696 32082 sgd_solver.cpp:137] Iteration 1500, lr = 0.00976562, m = 0.9
I0628 19:16:52.644906 32050 data_reader.cpp:262] Starting prefetch of epoch 2
I0628 19:16:53.350920 32082 solver.cpp:349] Iteration 1600 (58.1077 iter/s, 1.72094s/100 iter), loss = 0.00200254
I0628 19:16:53.351018 32082 solver.cpp:371]     Train net output #0: loss = 0.00200254 (* 1 = 0.00200254 loss)
I0628 19:16:53.351025 32082 sgd_solver.cpp:137] Iteration 1600, lr = 0.00975, m = 0.9
I0628 19:16:55.076735 32082 solver.cpp:349] Iteration 1700 (57.9582 iter/s, 1.72538s/100 iter), loss = 0.00134221
I0628 19:16:55.076757 32082 solver.cpp:371]     Train net output #0: loss = 0.00134221 (* 1 = 0.00134221 loss)
I0628 19:16:55.076761 32082 sgd_solver.cpp:137] Iteration 1700, lr = 0.00973437, m = 0.9
I0628 19:16:56.795634 32082 solver.cpp:349] Iteration 1800 (58.2067 iter/s, 1.71802s/100 iter), loss = 0.00154625
I0628 19:16:56.795660 32082 solver.cpp:371]     Train net output #0: loss = 0.00154625 (* 1 = 0.00154625 loss)
I0628 19:16:56.795665 32082 sgd_solver.cpp:137] Iteration 1800, lr = 0.00971875, m = 0.9
I0628 19:16:58.517601 32082 solver.cpp:349] Iteration 1900 (58.103 iter/s, 1.72108s/100 iter), loss = 0.000624003
I0628 19:16:58.517623 32082 solver.cpp:371]     Train net output #0: loss = 0.000624001 (* 1 = 0.000624001 loss)
I0628 19:16:58.517628 32082 sgd_solver.cpp:137] Iteration 1900, lr = 0.00970312, m = 0.9
I0628 19:17:00.221585 32082 solver.cpp:401] Sparsity after update:
I0628 19:17:00.222738 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:17:00.222748 32082 net.cpp:2170] conv1a_param_0(0.02) 
I0628 19:17:00.222754 32082 net.cpp:2170] conv1b_param_0(0.0399) 
I0628 19:17:00.222755 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:17:00.222765 32082 net.cpp:2170] res2a_branch2a_param_0(0.04) 
I0628 19:17:00.222767 32082 net.cpp:2170] res2a_branch2b_param_0(0.0399) 
I0628 19:17:00.222769 32082 net.cpp:2170] res3a_branch2a_param_0(0.04) 
I0628 19:17:00.222771 32082 net.cpp:2170] res3a_branch2b_param_0(0.04) 
I0628 19:17:00.222772 32082 net.cpp:2170] res4a_branch2a_param_0(0.04) 
I0628 19:17:00.222774 32082 net.cpp:2170] res4a_branch2b_param_0(0.04) 
I0628 19:17:00.222776 32082 net.cpp:2170] res5a_branch2a_param_0(0.04) 
I0628 19:17:00.222779 32082 net.cpp:2170] res5a_branch2b_param_0(0.04) 
I0628 19:17:00.222780 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (94130/2.3599e+06) 0.0399
I0628 19:17:00.222789 32082 solver.cpp:545] Iteration 2000, Testing net (#0)
I0628 19:17:01.223199 32080 data_reader.cpp:262] Starting prefetch of epoch 2
I0628 19:17:01.243849 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9172
I0628 19:17:01.243862 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:17:01.243867 32082 solver.cpp:630]     Test net output #2: loss = 0.299738 (* 1 = 0.299738 loss)
I0628 19:17:01.243881 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02059s
I0628 19:17:01.261152 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.06
I0628 19:17:01.408627 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:17:01.410100 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:17:01.410457 32082 solver.cpp:349] Iteration 2000 (34.5851 iter/s, 2.89141s/100 iter), loss = 0.00200341
I0628 19:17:01.410476 32082 solver.cpp:371]     Train net output #0: loss = 0.00200341 (* 1 = 0.00200341 loss)
I0628 19:17:01.410485 32082 sgd_solver.cpp:137] Iteration 2000, lr = 0.0096875, m = 0.9
I0628 19:17:03.129894 32082 solver.cpp:349] Iteration 2100 (58.1881 iter/s, 1.71856s/100 iter), loss = 0.000749538
I0628 19:17:03.129920 32082 solver.cpp:371]     Train net output #0: loss = 0.000749537 (* 1 = 0.000749537 loss)
I0628 19:17:03.129925 32082 sgd_solver.cpp:137] Iteration 2100, lr = 0.00967188, m = 0.9
I0628 19:17:04.851223 32082 solver.cpp:349] Iteration 2200 (58.1241 iter/s, 1.72046s/100 iter), loss = 0.000698627
I0628 19:17:04.851248 32082 solver.cpp:371]     Train net output #0: loss = 0.000698626 (* 1 = 0.000698626 loss)
I0628 19:17:04.851254 32082 sgd_solver.cpp:137] Iteration 2200, lr = 0.00965625, m = 0.9
I0628 19:17:06.580906 32082 solver.cpp:349] Iteration 2300 (57.8433 iter/s, 1.72881s/100 iter), loss = 0.00200384
I0628 19:17:06.580934 32082 solver.cpp:371]     Train net output #0: loss = 0.00200384 (* 1 = 0.00200384 loss)
I0628 19:17:06.580955 32082 sgd_solver.cpp:137] Iteration 2300, lr = 0.00964062, m = 0.9
I0628 19:17:07.268123 32050 data_reader.cpp:262] Starting prefetch of epoch 3
I0628 19:17:08.316471 32082 solver.cpp:349] Iteration 2400 (57.6477 iter/s, 1.73467s/100 iter), loss = 0.0009961
I0628 19:17:08.316495 32082 solver.cpp:371]     Train net output #0: loss = 0.000996098 (* 1 = 0.000996098 loss)
I0628 19:17:08.316499 32082 sgd_solver.cpp:137] Iteration 2400, lr = 0.009625, m = 0.9
I0628 19:17:10.035967 32082 solver.cpp:349] Iteration 2500 (58.1857 iter/s, 1.71864s/100 iter), loss = 0.00135639
I0628 19:17:10.035993 32082 solver.cpp:371]     Train net output #0: loss = 0.00135639 (* 1 = 0.00135639 loss)
I0628 19:17:10.036000 32082 sgd_solver.cpp:137] Iteration 2500, lr = 0.00960938, m = 0.9
I0628 19:17:11.754714 32082 solver.cpp:349] Iteration 2600 (58.2111 iter/s, 1.71789s/100 iter), loss = 0.00163494
I0628 19:17:11.754740 32082 solver.cpp:371]     Train net output #0: loss = 0.00163493 (* 1 = 0.00163493 loss)
I0628 19:17:11.754746 32082 sgd_solver.cpp:137] Iteration 2600, lr = 0.00959375, m = 0.9
I0628 19:17:13.476811 32082 solver.cpp:349] Iteration 2700 (58.0977 iter/s, 1.72124s/100 iter), loss = 0.000746667
I0628 19:17:13.476833 32082 solver.cpp:371]     Train net output #0: loss = 0.000746666 (* 1 = 0.000746666 loss)
I0628 19:17:13.476837 32082 sgd_solver.cpp:137] Iteration 2700, lr = 0.00957812, m = 0.9
I0628 19:17:15.201035 32082 solver.cpp:349] Iteration 2800 (58.0256 iter/s, 1.72338s/100 iter), loss = 0.00188829
I0628 19:17:15.201062 32082 solver.cpp:371]     Train net output #0: loss = 0.00188829 (* 1 = 0.00188829 loss)
I0628 19:17:15.201068 32082 sgd_solver.cpp:137] Iteration 2800, lr = 0.0095625, m = 0.9
I0628 19:17:16.922005 32082 solver.cpp:349] Iteration 2900 (58.1356 iter/s, 1.72012s/100 iter), loss = 0.00103684
I0628 19:17:16.922030 32082 solver.cpp:371]     Train net output #0: loss = 0.00103684 (* 1 = 0.00103684 loss)
I0628 19:17:16.922036 32082 sgd_solver.cpp:137] Iteration 2900, lr = 0.00954687, m = 0.9
I0628 19:17:18.632292 32082 solver.cpp:401] Sparsity after update:
I0628 19:17:18.633371 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:17:18.633379 32082 net.cpp:2170] conv1a_param_0(0.03) 
I0628 19:17:18.633388 32082 net.cpp:2170] conv1b_param_0(0.0599) 
I0628 19:17:18.633393 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:17:18.633396 32082 net.cpp:2170] res2a_branch2a_param_0(0.06) 
I0628 19:17:18.633401 32082 net.cpp:2170] res2a_branch2b_param_0(0.0599) 
I0628 19:17:18.633405 32082 net.cpp:2170] res3a_branch2a_param_0(0.06) 
I0628 19:17:18.633409 32082 net.cpp:2170] res3a_branch2b_param_0(0.06) 
I0628 19:17:18.633414 32082 net.cpp:2170] res4a_branch2a_param_0(0.06) 
I0628 19:17:18.633419 32082 net.cpp:2170] res4a_branch2b_param_0(0.06) 
I0628 19:17:18.633424 32082 net.cpp:2170] res5a_branch2a_param_0(0.06) 
I0628 19:17:18.633426 32082 net.cpp:2170] res5a_branch2b_param_0(0.06) 
I0628 19:17:18.633430 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (141202/2.3599e+06) 0.0598
I0628 19:17:18.633440 32082 solver.cpp:545] Iteration 3000, Testing net (#0)
I0628 19:17:19.631693 32080 data_reader.cpp:262] Starting prefetch of epoch 3
I0628 19:17:19.654022 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9158
I0628 19:17:19.654034 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:17:19.654039 32082 solver.cpp:630]     Test net output #2: loss = 0.299341 (* 1 = 0.299341 loss)
I0628 19:17:19.654053 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02013s
I0628 19:17:19.671375 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.08
I0628 19:17:19.833674 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:17:19.835153 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:17:19.835510 32082 solver.cpp:349] Iteration 3000 (34.3394 iter/s, 2.91211s/100 iter), loss = 0.000913233
I0628 19:17:19.835526 32082 solver.cpp:371]     Train net output #0: loss = 0.000913232 (* 1 = 0.000913232 loss)
I0628 19:17:19.835546 32082 sgd_solver.cpp:137] Iteration 3000, lr = 0.00953125, m = 0.9
I0628 19:17:21.557530 32082 solver.cpp:349] Iteration 3100 (58.0999 iter/s, 1.72117s/100 iter), loss = 0.00112214
I0628 19:17:21.557557 32082 solver.cpp:371]     Train net output #0: loss = 0.00112214 (* 1 = 0.00112214 loss)
I0628 19:17:21.557562 32082 sgd_solver.cpp:137] Iteration 3100, lr = 0.00951563, m = 0.9
I0628 19:17:21.919602 32050 data_reader.cpp:262] Starting prefetch of epoch 4
I0628 19:17:23.277473 32082 solver.cpp:349] Iteration 3200 (58.1698 iter/s, 1.71911s/100 iter), loss = 0.00111836
I0628 19:17:23.277501 32082 solver.cpp:371]     Train net output #0: loss = 0.00111835 (* 1 = 0.00111835 loss)
I0628 19:17:23.277508 32082 sgd_solver.cpp:137] Iteration 3200, lr = 0.0095, m = 0.9
I0628 19:17:24.996561 32082 solver.cpp:349] Iteration 3300 (58.1988 iter/s, 1.71825s/100 iter), loss = 0.000879399
I0628 19:17:24.996649 32082 solver.cpp:371]     Train net output #0: loss = 0.000879398 (* 1 = 0.000879398 loss)
I0628 19:17:24.996664 32082 sgd_solver.cpp:137] Iteration 3300, lr = 0.00948437, m = 0.9
I0628 19:17:26.714577 32082 solver.cpp:349] Iteration 3400 (58.2375 iter/s, 1.71711s/100 iter), loss = 0.000845185
I0628 19:17:26.714601 32082 solver.cpp:371]     Train net output #0: loss = 0.000845184 (* 1 = 0.000845184 loss)
I0628 19:17:26.714607 32082 sgd_solver.cpp:137] Iteration 3400, lr = 0.00946875, m = 0.9
I0628 19:17:28.436924 32082 solver.cpp:349] Iteration 3500 (58.0882 iter/s, 1.72152s/100 iter), loss = 0.00106532
I0628 19:17:28.436946 32082 solver.cpp:371]     Train net output #0: loss = 0.00106532 (* 1 = 0.00106532 loss)
I0628 19:17:28.436951 32082 sgd_solver.cpp:137] Iteration 3500, lr = 0.00945312, m = 0.9
I0628 19:17:30.154984 32082 solver.cpp:349] Iteration 3600 (58.2329 iter/s, 1.71724s/100 iter), loss = 0.00163842
I0628 19:17:30.155011 32082 solver.cpp:371]     Train net output #0: loss = 0.00163842 (* 1 = 0.00163842 loss)
I0628 19:17:30.155017 32082 sgd_solver.cpp:137] Iteration 3600, lr = 0.0094375, m = 0.9
I0628 19:17:31.874857 32082 solver.cpp:349] Iteration 3700 (58.1717 iter/s, 1.71905s/100 iter), loss = 0.00162913
I0628 19:17:31.874881 32082 solver.cpp:371]     Train net output #0: loss = 0.00162913 (* 1 = 0.00162913 loss)
I0628 19:17:31.874884 32082 sgd_solver.cpp:137] Iteration 3700, lr = 0.00942187, m = 0.9
I0628 19:17:33.596530 32082 solver.cpp:349] Iteration 3800 (58.1105 iter/s, 1.72086s/100 iter), loss = 0.00116374
I0628 19:17:33.596556 32082 solver.cpp:371]     Train net output #0: loss = 0.00116374 (* 1 = 0.00116374 loss)
I0628 19:17:33.596562 32082 sgd_solver.cpp:137] Iteration 3800, lr = 0.00940625, m = 0.9
I0628 19:17:35.319780 32082 solver.cpp:349] Iteration 3900 (58.0575 iter/s, 1.72243s/100 iter), loss = 0.00173968
I0628 19:17:35.319802 32082 solver.cpp:371]     Train net output #0: loss = 0.00173968 (* 1 = 0.00173968 loss)
I0628 19:17:35.319806 32082 sgd_solver.cpp:137] Iteration 3900, lr = 0.00939062, m = 0.9
I0628 19:17:35.373925 32050 data_reader.cpp:262] Starting prefetch of epoch 5
I0628 19:17:37.025796 32082 solver.cpp:401] Sparsity after update:
I0628 19:17:37.026919 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:17:37.026927 32082 net.cpp:2170] conv1a_param_0(0.04) 
I0628 19:17:37.026934 32082 net.cpp:2170] conv1b_param_0(0.0799) 
I0628 19:17:37.026937 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:17:37.026942 32082 net.cpp:2170] res2a_branch2a_param_0(0.08) 
I0628 19:17:37.026945 32082 net.cpp:2170] res2a_branch2b_param_0(0.08) 
I0628 19:17:37.026948 32082 net.cpp:2170] res3a_branch2a_param_0(0.08) 
I0628 19:17:37.026953 32082 net.cpp:2170] res3a_branch2b_param_0(0.08) 
I0628 19:17:37.026957 32082 net.cpp:2170] res4a_branch2a_param_0(0.08) 
I0628 19:17:37.026962 32082 net.cpp:2170] res4a_branch2b_param_0(0.08) 
I0628 19:17:37.026965 32082 net.cpp:2170] res5a_branch2a_param_0(0.08) 
I0628 19:17:37.026969 32082 net.cpp:2170] res5a_branch2b_param_0(0.08) 
I0628 19:17:37.026973 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (188271/2.3599e+06) 0.0798
I0628 19:17:37.026985 32082 solver.cpp:545] Iteration 4000, Testing net (#0)
I0628 19:17:38.025363 32080 data_reader.cpp:262] Starting prefetch of epoch 4
I0628 19:17:38.047377 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9176
I0628 19:17:38.047390 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:17:38.047395 32082 solver.cpp:630]     Test net output #2: loss = 0.299908 (* 1 = 0.299908 loss)
I0628 19:17:38.047410 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01997s
I0628 19:17:38.064651 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.1
I0628 19:17:38.254528 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:17:38.255998 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:17:38.256356 32082 solver.cpp:349] Iteration 4000 (34.0689 iter/s, 2.93523s/100 iter), loss = 0.00120807
I0628 19:17:38.256384 32082 solver.cpp:371]     Train net output #0: loss = 0.00120807 (* 1 = 0.00120807 loss)
I0628 19:17:38.256391 32082 sgd_solver.cpp:137] Iteration 4000, lr = 0.009375, m = 0.9
I0628 19:17:39.980475 32082 solver.cpp:349] Iteration 4100 (58.028 iter/s, 1.72331s/100 iter), loss = 0.000839306
I0628 19:17:39.980501 32082 solver.cpp:371]     Train net output #0: loss = 0.000839304 (* 1 = 0.000839304 loss)
I0628 19:17:39.980507 32082 sgd_solver.cpp:137] Iteration 4100, lr = 0.00935937, m = 0.9
I0628 19:17:41.698829 32082 solver.cpp:349] Iteration 4200 (58.2225 iter/s, 1.71755s/100 iter), loss = 0.000959956
I0628 19:17:41.698856 32082 solver.cpp:371]     Train net output #0: loss = 0.000959954 (* 1 = 0.000959954 loss)
I0628 19:17:41.698863 32082 sgd_solver.cpp:137] Iteration 4200, lr = 0.00934375, m = 0.9
I0628 19:17:43.419827 32082 solver.cpp:349] Iteration 4300 (58.1329 iter/s, 1.7202s/100 iter), loss = 0.00184238
I0628 19:17:43.419849 32082 solver.cpp:371]     Train net output #0: loss = 0.00184238 (* 1 = 0.00184238 loss)
I0628 19:17:43.419853 32082 sgd_solver.cpp:137] Iteration 4300, lr = 0.00932813, m = 0.9
I0628 19:17:45.140874 32082 solver.cpp:349] Iteration 4400 (58.131 iter/s, 1.72025s/100 iter), loss = 0.000961925
I0628 19:17:45.140898 32082 solver.cpp:371]     Train net output #0: loss = 0.000961923 (* 1 = 0.000961923 loss)
I0628 19:17:45.140905 32082 sgd_solver.cpp:137] Iteration 4400, lr = 0.0093125, m = 0.9
I0628 19:17:46.864351 32082 solver.cpp:349] Iteration 4500 (58.0491 iter/s, 1.72268s/100 iter), loss = 0.00203632
I0628 19:17:46.864378 32082 solver.cpp:371]     Train net output #0: loss = 0.00203632 (* 1 = 0.00203632 loss)
I0628 19:17:46.864385 32082 sgd_solver.cpp:137] Iteration 4500, lr = 0.00929687, m = 0.9
I0628 19:17:48.587512 32082 solver.cpp:349] Iteration 4600 (58.0598 iter/s, 1.72236s/100 iter), loss = 0.00190979
I0628 19:17:48.587539 32082 solver.cpp:371]     Train net output #0: loss = 0.00190979 (* 1 = 0.00190979 loss)
I0628 19:17:48.587545 32082 sgd_solver.cpp:137] Iteration 4600, lr = 0.00928125, m = 0.9
I0628 19:17:50.032589 32050 data_reader.cpp:262] Starting prefetch of epoch 6
I0628 19:17:50.307451 32082 solver.cpp:349] Iteration 4700 (58.1684 iter/s, 1.71915s/100 iter), loss = 0.000908224
I0628 19:17:50.307479 32082 solver.cpp:371]     Train net output #0: loss = 0.000908222 (* 1 = 0.000908222 loss)
I0628 19:17:50.307485 32082 sgd_solver.cpp:137] Iteration 4700, lr = 0.00926562, m = 0.9
I0628 19:17:52.026530 32082 solver.cpp:349] Iteration 4800 (58.1974 iter/s, 1.71829s/100 iter), loss = 0.00193136
I0628 19:17:52.026556 32082 solver.cpp:371]     Train net output #0: loss = 0.00193135 (* 1 = 0.00193135 loss)
I0628 19:17:52.026561 32082 sgd_solver.cpp:137] Iteration 4800, lr = 0.00925, m = 0.9
I0628 19:17:53.750279 32082 solver.cpp:349] Iteration 4900 (58.0396 iter/s, 1.72296s/100 iter), loss = 0.000980431
I0628 19:17:53.750305 32082 solver.cpp:371]     Train net output #0: loss = 0.000980429 (* 1 = 0.000980429 loss)
I0628 19:17:53.750311 32082 sgd_solver.cpp:137] Iteration 4900, lr = 0.00923437, m = 0.9
I0628 19:17:55.459545 32082 solver.cpp:401] Sparsity after update:
I0628 19:17:55.460713 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:17:55.460721 32082 net.cpp:2170] conv1a_param_0(0.0496) 
I0628 19:17:55.460729 32082 net.cpp:2170] conv1b_param_0(0.0998) 
I0628 19:17:55.460731 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:17:55.460733 32082 net.cpp:2170] res2a_branch2a_param_0(0.1) 
I0628 19:17:55.460736 32082 net.cpp:2170] res2a_branch2b_param_0(0.0999) 
I0628 19:17:55.460737 32082 net.cpp:2170] res3a_branch2a_param_0(0.1) 
I0628 19:17:55.460741 32082 net.cpp:2170] res3a_branch2b_param_0(0.1) 
I0628 19:17:55.460742 32082 net.cpp:2170] res4a_branch2a_param_0(0.1) 
I0628 19:17:55.460743 32082 net.cpp:2170] res4a_branch2b_param_0(0.1) 
I0628 19:17:55.460747 32082 net.cpp:2170] res5a_branch2a_param_0(0.1) 
I0628 19:17:55.460748 32082 net.cpp:2170] res5a_branch2b_param_0(0.1) 
I0628 19:17:55.460749 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (235344/2.3599e+06) 0.0997
I0628 19:17:55.460757 32082 solver.cpp:545] Iteration 5000, Testing net (#0)
I0628 19:17:56.459312 32080 data_reader.cpp:262] Starting prefetch of epoch 5
I0628 19:17:56.479486 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9168
I0628 19:17:56.479498 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:17:56.479503 32082 solver.cpp:630]     Test net output #2: loss = 0.299623 (* 1 = 0.299623 loss)
I0628 19:17:56.479517 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01832s
I0628 19:17:56.496773 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.12
I0628 19:17:56.688431 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:17:56.689913 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:17:56.690274 32082 solver.cpp:349] Iteration 5000 (34.0287 iter/s, 2.93869s/100 iter), loss = 0.000907788
I0628 19:17:56.690291 32082 solver.cpp:371]     Train net output #0: loss = 0.000907785 (* 1 = 0.000907785 loss)
I0628 19:17:56.690296 32082 sgd_solver.cpp:137] Iteration 5000, lr = 0.00921875, m = 0.9
I0628 19:17:58.414589 32082 solver.cpp:349] Iteration 5100 (58.0199 iter/s, 1.72355s/100 iter), loss = 0.0018882
I0628 19:17:58.414609 32082 solver.cpp:371]     Train net output #0: loss = 0.0018882 (* 1 = 0.0018882 loss)
I0628 19:17:58.414613 32082 sgd_solver.cpp:137] Iteration 5100, lr = 0.00920312, m = 0.9
I0628 19:18:00.136987 32082 solver.cpp:349] Iteration 5200 (58.0844 iter/s, 1.72163s/100 iter), loss = 0.00126935
I0628 19:18:00.137009 32082 solver.cpp:371]     Train net output #0: loss = 0.00126935 (* 1 = 0.00126935 loss)
I0628 19:18:00.137014 32082 sgd_solver.cpp:137] Iteration 5200, lr = 0.0091875, m = 0.9
I0628 19:18:01.862867 32082 solver.cpp:349] Iteration 5300 (57.9673 iter/s, 1.72511s/100 iter), loss = 0.00183895
I0628 19:18:01.862888 32082 solver.cpp:371]     Train net output #0: loss = 0.00183895 (* 1 = 0.00183895 loss)
I0628 19:18:01.862893 32082 sgd_solver.cpp:137] Iteration 5300, lr = 0.00917188, m = 0.9
I0628 19:18:03.586518 32082 solver.cpp:349] Iteration 5400 (58.0422 iter/s, 1.72288s/100 iter), loss = 0.000883252
I0628 19:18:03.586544 32082 solver.cpp:371]     Train net output #0: loss = 0.000883251 (* 1 = 0.000883251 loss)
I0628 19:18:03.586549 32082 sgd_solver.cpp:137] Iteration 5400, lr = 0.00915625, m = 0.9
I0628 19:18:04.702170 32050 data_reader.cpp:262] Starting prefetch of epoch 7
I0628 19:18:05.303620 32082 solver.cpp:349] Iteration 5500 (58.2636 iter/s, 1.71634s/100 iter), loss = 0.00131171
I0628 19:18:05.303647 32082 solver.cpp:371]     Train net output #0: loss = 0.00131171 (* 1 = 0.00131171 loss)
I0628 19:18:05.303653 32082 sgd_solver.cpp:137] Iteration 5500, lr = 0.00914062, m = 0.9
I0628 19:18:07.025302 32082 solver.cpp:349] Iteration 5600 (58.1086 iter/s, 1.72091s/100 iter), loss = 0.00159786
I0628 19:18:07.025326 32082 solver.cpp:371]     Train net output #0: loss = 0.00159785 (* 1 = 0.00159785 loss)
I0628 19:18:07.025333 32082 sgd_solver.cpp:137] Iteration 5600, lr = 0.009125, m = 0.9
I0628 19:18:08.745064 32082 solver.cpp:349] Iteration 5700 (58.1734 iter/s, 1.719s/100 iter), loss = 0.00153627
I0628 19:18:08.745112 32082 solver.cpp:371]     Train net output #0: loss = 0.00153627 (* 1 = 0.00153627 loss)
I0628 19:18:08.745120 32082 sgd_solver.cpp:137] Iteration 5700, lr = 0.00910938, m = 0.9
I0628 19:18:10.467553 32082 solver.cpp:349] Iteration 5800 (58.082 iter/s, 1.7217s/100 iter), loss = 0.00149291
I0628 19:18:10.467578 32082 solver.cpp:371]     Train net output #0: loss = 0.0014929 (* 1 = 0.0014929 loss)
I0628 19:18:10.467584 32082 sgd_solver.cpp:137] Iteration 5800, lr = 0.00909375, m = 0.9
I0628 19:18:12.189560 32082 solver.cpp:349] Iteration 5900 (58.0973 iter/s, 1.72125s/100 iter), loss = 0.00114046
I0628 19:18:12.189587 32082 solver.cpp:371]     Train net output #0: loss = 0.00114045 (* 1 = 0.00114045 loss)
I0628 19:18:12.189592 32082 sgd_solver.cpp:137] Iteration 5900, lr = 0.00907812, m = 0.9
I0628 19:18:13.893934 32082 solver.cpp:401] Sparsity after update:
I0628 19:18:13.895041 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:18:13.895051 32082 net.cpp:2170] conv1a_param_0(0.0596) 
I0628 19:18:13.895056 32082 net.cpp:2170] conv1b_param_0(0.12) 
I0628 19:18:13.895058 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:18:13.895061 32082 net.cpp:2170] res2a_branch2a_param_0(0.12) 
I0628 19:18:13.895062 32082 net.cpp:2170] res2a_branch2b_param_0(0.12) 
I0628 19:18:13.895066 32082 net.cpp:2170] res3a_branch2a_param_0(0.12) 
I0628 19:18:13.895067 32082 net.cpp:2170] res3a_branch2b_param_0(0.12) 
I0628 19:18:13.895071 32082 net.cpp:2170] res4a_branch2a_param_0(0.12) 
I0628 19:18:13.895072 32082 net.cpp:2170] res4a_branch2b_param_0(0.12) 
I0628 19:18:13.895074 32082 net.cpp:2170] res5a_branch2a_param_0(0.12) 
I0628 19:18:13.895076 32082 net.cpp:2170] res5a_branch2b_param_0(0.12) 
I0628 19:18:13.895078 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (282417/2.3599e+06) 0.12
I0628 19:18:13.895087 32082 solver.cpp:545] Iteration 6000, Testing net (#0)
I0628 19:18:14.895823 32080 data_reader.cpp:262] Starting prefetch of epoch 6
I0628 19:18:14.916054 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9162
I0628 19:18:14.916066 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:18:14.916074 32082 solver.cpp:630]     Test net output #2: loss = 0.297778 (* 1 = 0.297778 loss)
I0628 19:18:14.916090 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02058s
I0628 19:18:14.933434 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.14
I0628 19:18:15.158463 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:18:15.159934 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:18:15.160293 32082 solver.cpp:349] Iteration 6000 (33.6761 iter/s, 2.96947s/100 iter), loss = 0.00198984
I0628 19:18:15.160313 32082 solver.cpp:371]     Train net output #0: loss = 0.00198984 (* 1 = 0.00198984 loss)
I0628 19:18:15.160322 32082 sgd_solver.cpp:137] Iteration 6000, lr = 0.0090625, m = 0.9
I0628 19:18:16.883004 32082 solver.cpp:349] Iteration 6100 (58.0733 iter/s, 1.72196s/100 iter), loss = 0.00134146
I0628 19:18:16.883026 32082 solver.cpp:371]     Train net output #0: loss = 0.00134146 (* 1 = 0.00134146 loss)
I0628 19:18:16.883031 32082 sgd_solver.cpp:137] Iteration 6100, lr = 0.00904687, m = 0.9
I0628 19:18:18.604333 32082 solver.cpp:349] Iteration 6200 (58.1196 iter/s, 1.72059s/100 iter), loss = 0.00151677
I0628 19:18:18.604357 32082 solver.cpp:371]     Train net output #0: loss = 0.00151677 (* 1 = 0.00151677 loss)
I0628 19:18:18.604360 32082 sgd_solver.cpp:137] Iteration 6200, lr = 0.00903125, m = 0.9
I0628 19:18:19.395951 32050 data_reader.cpp:262] Starting prefetch of epoch 8
I0628 19:18:20.325747 32082 solver.cpp:349] Iteration 6300 (58.1167 iter/s, 1.72068s/100 iter), loss = 0.000867034
I0628 19:18:20.325773 32082 solver.cpp:371]     Train net output #0: loss = 0.000867032 (* 1 = 0.000867032 loss)
I0628 19:18:20.325778 32082 sgd_solver.cpp:137] Iteration 6300, lr = 0.00901563, m = 0.9
I0628 19:18:22.046751 32082 solver.cpp:349] Iteration 6400 (58.1307 iter/s, 1.72026s/100 iter), loss = 0.00102577
I0628 19:18:22.046794 32082 solver.cpp:371]     Train net output #0: loss = 0.00102576 (* 1 = 0.00102576 loss)
I0628 19:18:22.046800 32082 sgd_solver.cpp:137] Iteration 6400, lr = 0.009, m = 0.9
I0628 19:18:23.770651 32082 solver.cpp:349] Iteration 6500 (58.0335 iter/s, 1.72314s/100 iter), loss = 0.00104501
I0628 19:18:23.770678 32082 solver.cpp:371]     Train net output #0: loss = 0.00104501 (* 1 = 0.00104501 loss)
I0628 19:18:23.770684 32082 sgd_solver.cpp:137] Iteration 6500, lr = 0.00898437, m = 0.9
I0628 19:18:25.491950 32082 solver.cpp:349] Iteration 6600 (58.1206 iter/s, 1.72056s/100 iter), loss = 0.00224834
I0628 19:18:25.492019 32082 solver.cpp:371]     Train net output #0: loss = 0.00224834 (* 1 = 0.00224834 loss)
I0628 19:18:25.492027 32082 sgd_solver.cpp:137] Iteration 6600, lr = 0.00896875, m = 0.9
I0628 19:18:27.212291 32082 solver.cpp:349] Iteration 6700 (58.1544 iter/s, 1.71956s/100 iter), loss = 0.000830301
I0628 19:18:27.212319 32082 solver.cpp:371]     Train net output #0: loss = 0.000830299 (* 1 = 0.000830299 loss)
I0628 19:18:27.212326 32082 sgd_solver.cpp:137] Iteration 6700, lr = 0.00895312, m = 0.9
I0628 19:18:28.932799 32082 solver.cpp:349] Iteration 6800 (58.1472 iter/s, 1.71977s/100 iter), loss = 0.000736836
I0628 19:18:28.932822 32082 solver.cpp:371]     Train net output #0: loss = 0.000736834 (* 1 = 0.000736834 loss)
I0628 19:18:28.932828 32082 sgd_solver.cpp:137] Iteration 6800, lr = 0.0089375, m = 0.9
I0628 19:18:30.652338 32082 solver.cpp:349] Iteration 6900 (58.1798 iter/s, 1.71881s/100 iter), loss = 0.000978627
I0628 19:18:30.652364 32082 solver.cpp:371]     Train net output #0: loss = 0.000978625 (* 1 = 0.000978625 loss)
I0628 19:18:30.652370 32082 sgd_solver.cpp:137] Iteration 6900, lr = 0.00892187, m = 0.9
I0628 19:18:32.356318 32082 solver.cpp:401] Sparsity after update:
I0628 19:18:32.357453 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:18:32.357461 32082 net.cpp:2170] conv1a_param_0(0.0696) 
I0628 19:18:32.357468 32082 net.cpp:2170] conv1b_param_0(0.14) 
I0628 19:18:32.357472 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:18:32.357476 32082 net.cpp:2170] res2a_branch2a_param_0(0.14) 
I0628 19:18:32.357481 32082 net.cpp:2170] res2a_branch2b_param_0(0.14) 
I0628 19:18:32.357484 32082 net.cpp:2170] res3a_branch2a_param_0(0.14) 
I0628 19:18:32.357488 32082 net.cpp:2170] res3a_branch2b_param_0(0.14) 
I0628 19:18:32.357492 32082 net.cpp:2170] res4a_branch2a_param_0(0.14) 
I0628 19:18:32.357496 32082 net.cpp:2170] res4a_branch2b_param_0(0.14) 
I0628 19:18:32.357501 32082 net.cpp:2170] res5a_branch2a_param_0(0.14) 
I0628 19:18:32.357504 32082 net.cpp:2170] res5a_branch2b_param_0(0.14) 
I0628 19:18:32.357508 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (329487/2.3599e+06) 0.14
I0628 19:18:32.357520 32082 solver.cpp:545] Iteration 7000, Testing net (#0)
I0628 19:18:33.357944 32080 data_reader.cpp:262] Starting prefetch of epoch 7
I0628 19:18:33.378206 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9164
I0628 19:18:33.378219 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:18:33.378224 32082 solver.cpp:630]     Test net output #2: loss = 0.29925 (* 1 = 0.29925 loss)
I0628 19:18:33.378237 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02031s
I0628 19:18:33.395573 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.16
I0628 19:18:33.631631 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:18:33.633095 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:18:33.633455 32082 solver.cpp:349] Iteration 7000 (33.5583 iter/s, 2.97989s/100 iter), loss = 0.000955429
I0628 19:18:33.633472 32082 solver.cpp:371]     Train net output #0: loss = 0.000955427 (* 1 = 0.000955427 loss)
I0628 19:18:33.633478 32082 sgd_solver.cpp:137] Iteration 7000, lr = 0.00890625, m = 0.9
I0628 19:18:34.114519 32050 data_reader.cpp:262] Starting prefetch of epoch 9
I0628 19:18:35.355132 32082 solver.cpp:349] Iteration 7100 (58.107 iter/s, 1.72096s/100 iter), loss = 0.000786382
I0628 19:18:35.355159 32082 solver.cpp:371]     Train net output #0: loss = 0.00078638 (* 1 = 0.00078638 loss)
I0628 19:18:35.355165 32082 sgd_solver.cpp:137] Iteration 7100, lr = 0.00889063, m = 0.9
I0628 19:18:37.076681 32082 solver.cpp:349] Iteration 7200 (58.1117 iter/s, 1.72082s/100 iter), loss = 0.000819024
I0628 19:18:37.076707 32082 solver.cpp:371]     Train net output #0: loss = 0.000819022 (* 1 = 0.000819022 loss)
I0628 19:18:37.076714 32082 sgd_solver.cpp:137] Iteration 7200, lr = 0.008875, m = 0.9
I0628 19:18:38.804913 32082 solver.cpp:349] Iteration 7300 (57.8868 iter/s, 1.72751s/100 iter), loss = 0.00110974
I0628 19:18:38.804952 32082 solver.cpp:371]     Train net output #0: loss = 0.00110974 (* 1 = 0.00110974 loss)
I0628 19:18:38.804956 32082 sgd_solver.cpp:137] Iteration 7300, lr = 0.00885937, m = 0.9
I0628 19:18:40.528326 32082 solver.cpp:349] Iteration 7400 (58.0489 iter/s, 1.72269s/100 iter), loss = 0.00114124
I0628 19:18:40.528349 32082 solver.cpp:371]     Train net output #0: loss = 0.00114124 (* 1 = 0.00114124 loss)
I0628 19:18:40.528355 32082 sgd_solver.cpp:137] Iteration 7400, lr = 0.00884375, m = 0.9
I0628 19:18:42.247586 32082 solver.cpp:349] Iteration 7500 (58.1885 iter/s, 1.71855s/100 iter), loss = 0.0010139
I0628 19:18:42.247611 32082 solver.cpp:371]     Train net output #0: loss = 0.0010139 (* 1 = 0.0010139 loss)
I0628 19:18:42.247615 32082 sgd_solver.cpp:137] Iteration 7500, lr = 0.00882812, m = 0.9
I0628 19:18:43.968003 32082 solver.cpp:349] Iteration 7600 (58.1494 iter/s, 1.71971s/100 iter), loss = 0.00130514
I0628 19:18:43.968029 32082 solver.cpp:371]     Train net output #0: loss = 0.00130513 (* 1 = 0.00130513 loss)
I0628 19:18:43.968035 32082 sgd_solver.cpp:137] Iteration 7600, lr = 0.0088125, m = 0.9
I0628 19:18:45.688783 32082 solver.cpp:349] Iteration 7700 (58.1371 iter/s, 1.72007s/100 iter), loss = 0.0018141
I0628 19:18:45.688805 32082 solver.cpp:371]     Train net output #0: loss = 0.0018141 (* 1 = 0.0018141 loss)
I0628 19:18:45.688810 32082 sgd_solver.cpp:137] Iteration 7700, lr = 0.00879687, m = 0.9
I0628 19:18:47.417119 32082 solver.cpp:349] Iteration 7800 (57.8827 iter/s, 1.72763s/100 iter), loss = 0.00176653
I0628 19:18:47.417146 32082 solver.cpp:371]     Train net output #0: loss = 0.00176653 (* 1 = 0.00176653 loss)
I0628 19:18:47.417152 32082 sgd_solver.cpp:137] Iteration 7800, lr = 0.00878125, m = 0.9
I0628 19:18:47.574573 32050 data_reader.cpp:262] Starting prefetch of epoch 10
I0628 19:18:49.138737 32082 solver.cpp:349] Iteration 7900 (58.1087 iter/s, 1.72091s/100 iter), loss = 0.000712459
I0628 19:18:49.138763 32082 solver.cpp:371]     Train net output #0: loss = 0.000712456 (* 1 = 0.000712456 loss)
I0628 19:18:49.138768 32082 sgd_solver.cpp:137] Iteration 7900, lr = 0.00876562, m = 0.9
I0628 19:18:50.846983 32082 solver.cpp:401] Sparsity after update:
I0628 19:18:50.848057 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:18:50.848064 32082 net.cpp:2170] conv1a_param_0(0.0796) 
I0628 19:18:50.848071 32082 net.cpp:2170] conv1b_param_0(0.16) 
I0628 19:18:50.848074 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:18:50.848076 32082 net.cpp:2170] res2a_branch2a_param_0(0.16) 
I0628 19:18:50.848078 32082 net.cpp:2170] res2a_branch2b_param_0(0.16) 
I0628 19:18:50.848080 32082 net.cpp:2170] res3a_branch2a_param_0(0.16) 
I0628 19:18:50.848083 32082 net.cpp:2170] res3a_branch2b_param_0(0.16) 
I0628 19:18:50.848084 32082 net.cpp:2170] res4a_branch2a_param_0(0.16) 
I0628 19:18:50.848085 32082 net.cpp:2170] res4a_branch2b_param_0(0.16) 
I0628 19:18:50.848088 32082 net.cpp:2170] res5a_branch2a_param_0(0.16) 
I0628 19:18:50.848090 32082 net.cpp:2170] res5a_branch2b_param_0(0.16) 
I0628 19:18:50.848098 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (376561/2.3599e+06) 0.16
I0628 19:18:50.848105 32082 solver.cpp:545] Iteration 8000, Testing net (#0)
I0628 19:18:51.846923 32080 data_reader.cpp:262] Starting prefetch of epoch 8
I0628 19:18:51.867439 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9174
I0628 19:18:51.867451 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9964
I0628 19:18:51.867456 32082 solver.cpp:630]     Test net output #2: loss = 0.298238 (* 1 = 0.298238 loss)
I0628 19:18:51.867470 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01897s
I0628 19:18:51.884699 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.18
I0628 19:18:52.142195 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:18:52.143671 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:18:52.144024 32082 solver.cpp:349] Iteration 8000 (33.2878 iter/s, 3.0041s/100 iter), loss = 0.00093761
I0628 19:18:52.144052 32082 solver.cpp:371]     Train net output #0: loss = 0.000937607 (* 1 = 0.000937607 loss)
I0628 19:18:52.144057 32082 sgd_solver.cpp:137] Iteration 8000, lr = 0.00875, m = 0.9
I0628 19:18:53.865178 32082 solver.cpp:349] Iteration 8100 (58.1241 iter/s, 1.72046s/100 iter), loss = 0.000754828
I0628 19:18:53.865203 32082 solver.cpp:371]     Train net output #0: loss = 0.000754826 (* 1 = 0.000754826 loss)
I0628 19:18:53.865209 32082 sgd_solver.cpp:137] Iteration 8100, lr = 0.00873438, m = 0.9
I0628 19:18:55.582878 32082 solver.cpp:349] Iteration 8200 (58.2409 iter/s, 1.71701s/100 iter), loss = 0.0015611
I0628 19:18:55.582962 32082 solver.cpp:371]     Train net output #0: loss = 0.0015611 (* 1 = 0.0015611 loss)
I0628 19:18:55.582968 32082 sgd_solver.cpp:137] Iteration 8200, lr = 0.00871875, m = 0.9
I0628 19:18:57.303390 32082 solver.cpp:349] Iteration 8300 (58.1477 iter/s, 1.71976s/100 iter), loss = 0.000840607
I0628 19:18:57.303413 32082 solver.cpp:371]     Train net output #0: loss = 0.000840604 (* 1 = 0.000840604 loss)
I0628 19:18:57.303418 32082 sgd_solver.cpp:137] Iteration 8300, lr = 0.00870312, m = 0.9
I0628 19:18:59.023788 32082 solver.cpp:349] Iteration 8400 (58.1493 iter/s, 1.71971s/100 iter), loss = 0.00125962
I0628 19:18:59.023811 32082 solver.cpp:371]     Train net output #0: loss = 0.00125962 (* 1 = 0.00125962 loss)
I0628 19:18:59.023818 32082 sgd_solver.cpp:137] Iteration 8400, lr = 0.0086875, m = 0.9
I0628 19:19:00.742475 32082 solver.cpp:349] Iteration 8500 (58.2073 iter/s, 1.718s/100 iter), loss = 0.00143807
I0628 19:19:00.742523 32082 solver.cpp:371]     Train net output #0: loss = 0.00143807 (* 1 = 0.00143807 loss)
I0628 19:19:00.742532 32082 sgd_solver.cpp:137] Iteration 8500, lr = 0.00867188, m = 0.9
I0628 19:19:02.288385 32050 data_reader.cpp:262] Starting prefetch of epoch 11
I0628 19:19:02.460824 32082 solver.cpp:349] Iteration 8600 (58.2195 iter/s, 1.71764s/100 iter), loss = 0.00103549
I0628 19:19:02.460847 32082 solver.cpp:371]     Train net output #0: loss = 0.00103549 (* 1 = 0.00103549 loss)
I0628 19:19:02.460853 32082 sgd_solver.cpp:137] Iteration 8600, lr = 0.00865625, m = 0.9
I0628 19:19:04.179535 32082 solver.cpp:349] Iteration 8700 (58.2063 iter/s, 1.71803s/100 iter), loss = 0.00161488
I0628 19:19:04.179563 32082 solver.cpp:371]     Train net output #0: loss = 0.00161488 (* 1 = 0.00161488 loss)
I0628 19:19:04.179569 32082 sgd_solver.cpp:137] Iteration 8700, lr = 0.00864062, m = 0.9
I0628 19:19:05.901384 32082 solver.cpp:349] Iteration 8800 (58.1002 iter/s, 1.72117s/100 iter), loss = 0.00142391
I0628 19:19:05.901407 32082 solver.cpp:371]     Train net output #0: loss = 0.00142391 (* 1 = 0.00142391 loss)
I0628 19:19:05.901412 32082 sgd_solver.cpp:137] Iteration 8800, lr = 0.008625, m = 0.9
I0628 19:19:07.621522 32082 solver.cpp:349] Iteration 8900 (58.1577 iter/s, 1.71946s/100 iter), loss = 0.000880841
I0628 19:19:07.621552 32082 solver.cpp:371]     Train net output #0: loss = 0.000880838 (* 1 = 0.000880838 loss)
I0628 19:19:07.621559 32082 sgd_solver.cpp:137] Iteration 8900, lr = 0.00860937, m = 0.9
I0628 19:19:09.324519 32082 solver.cpp:401] Sparsity after update:
I0628 19:19:09.325896 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:19:09.325903 32082 net.cpp:2170] conv1a_param_0(0.0896) 
I0628 19:19:09.325912 32082 net.cpp:2170] conv1b_param_0(0.18) 
I0628 19:19:09.325917 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:19:09.325920 32082 net.cpp:2170] res2a_branch2a_param_0(0.18) 
I0628 19:19:09.325925 32082 net.cpp:2170] res2a_branch2b_param_0(0.18) 
I0628 19:19:09.325928 32082 net.cpp:2170] res3a_branch2a_param_0(0.18) 
I0628 19:19:09.325932 32082 net.cpp:2170] res3a_branch2b_param_0(0.18) 
I0628 19:19:09.325937 32082 net.cpp:2170] res4a_branch2a_param_0(0.18) 
I0628 19:19:09.325940 32082 net.cpp:2170] res4a_branch2b_param_0(0.18) 
I0628 19:19:09.325944 32082 net.cpp:2170] res5a_branch2a_param_0(0.18) 
I0628 19:19:09.325948 32082 net.cpp:2170] res5a_branch2b_param_0(0.18) 
I0628 19:19:09.325953 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (423633/2.3599e+06) 0.18
I0628 19:19:09.325963 32082 solver.cpp:545] Iteration 9000, Testing net (#0)
I0628 19:19:10.323429 32080 data_reader.cpp:262] Starting prefetch of epoch 9
I0628 19:19:10.344887 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9178
I0628 19:19:10.344899 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:19:10.344907 32082 solver.cpp:630]     Test net output #2: loss = 0.299471 (* 1 = 0.299471 loss)
I0628 19:19:10.344923 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01858s
I0628 19:19:10.362179 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.2
I0628 19:19:10.634727 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:19:10.636188 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:19:10.636544 32082 solver.cpp:349] Iteration 9000 (33.18 iter/s, 3.01387s/100 iter), loss = 0.00136166
I0628 19:19:10.636564 32082 solver.cpp:371]     Train net output #0: loss = 0.00136165 (* 1 = 0.00136165 loss)
I0628 19:19:10.636572 32082 sgd_solver.cpp:137] Iteration 9000, lr = 0.00859375, m = 0.9
I0628 19:19:12.358628 32082 solver.cpp:349] Iteration 9100 (58.0919 iter/s, 1.72141s/100 iter), loss = 0.000980862
I0628 19:19:12.358656 32082 solver.cpp:371]     Train net output #0: loss = 0.000980859 (* 1 = 0.000980859 loss)
I0628 19:19:12.358662 32082 sgd_solver.cpp:137] Iteration 9100, lr = 0.00857813, m = 0.9
I0628 19:19:14.080646 32082 solver.cpp:349] Iteration 9200 (58.0943 iter/s, 1.72134s/100 iter), loss = 0.000576059
I0628 19:19:14.080672 32082 solver.cpp:371]     Train net output #0: loss = 0.000576056 (* 1 = 0.000576056 loss)
I0628 19:19:14.080677 32082 sgd_solver.cpp:137] Iteration 9200, lr = 0.0085625, m = 0.9
I0628 19:19:15.801635 32082 solver.cpp:349] Iteration 9300 (58.1288 iter/s, 1.72032s/100 iter), loss = 0.002242
I0628 19:19:15.801661 32082 solver.cpp:371]     Train net output #0: loss = 0.002242 (* 1 = 0.002242 loss)
I0628 19:19:15.801667 32082 sgd_solver.cpp:137] Iteration 9300, lr = 0.00854687, m = 0.9
I0628 19:19:17.021692 32050 data_reader.cpp:262] Starting prefetch of epoch 12
I0628 19:19:17.520140 32082 solver.cpp:349] Iteration 9400 (58.2128 iter/s, 1.71784s/100 iter), loss = 0.00107085
I0628 19:19:17.520165 32082 solver.cpp:371]     Train net output #0: loss = 0.00107085 (* 1 = 0.00107085 loss)
I0628 19:19:17.520171 32082 sgd_solver.cpp:137] Iteration 9400, lr = 0.00853125, m = 0.9
I0628 19:19:19.243333 32082 solver.cpp:349] Iteration 9500 (58.0542 iter/s, 1.72253s/100 iter), loss = 0.00131783
I0628 19:19:19.243355 32082 solver.cpp:371]     Train net output #0: loss = 0.00131783 (* 1 = 0.00131783 loss)
I0628 19:19:19.243358 32082 sgd_solver.cpp:137] Iteration 9500, lr = 0.00851563, m = 0.9
I0628 19:19:20.968773 32082 solver.cpp:349] Iteration 9600 (57.9784 iter/s, 1.72478s/100 iter), loss = 0.00106778
I0628 19:19:20.968799 32082 solver.cpp:371]     Train net output #0: loss = 0.00106778 (* 1 = 0.00106778 loss)
I0628 19:19:20.968806 32082 sgd_solver.cpp:137] Iteration 9600, lr = 0.0085, m = 0.9
I0628 19:19:22.689841 32082 solver.cpp:349] Iteration 9700 (58.1259 iter/s, 1.7204s/100 iter), loss = 0.000689816
I0628 19:19:22.689862 32082 solver.cpp:371]     Train net output #0: loss = 0.000689813 (* 1 = 0.000689813 loss)
I0628 19:19:22.689867 32082 sgd_solver.cpp:137] Iteration 9700, lr = 0.00848437, m = 0.9
I0628 19:19:24.408540 32082 solver.cpp:349] Iteration 9800 (58.2056 iter/s, 1.71805s/100 iter), loss = 0.000979654
I0628 19:19:24.408563 32082 solver.cpp:371]     Train net output #0: loss = 0.000979651 (* 1 = 0.000979651 loss)
I0628 19:19:24.408567 32082 sgd_solver.cpp:137] Iteration 9800, lr = 0.00846875, m = 0.9
I0628 19:19:26.126735 32082 solver.cpp:349] Iteration 9900 (58.2227 iter/s, 1.71754s/100 iter), loss = 0.002617
I0628 19:19:26.126906 32082 solver.cpp:371]     Train net output #0: loss = 0.002617 (* 1 = 0.002617 loss)
I0628 19:19:26.126914 32082 sgd_solver.cpp:137] Iteration 9900, lr = 0.00845312, m = 0.9
I0628 19:19:27.831676 32082 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_10000.caffemodel
I0628 19:19:27.843545 32082 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_10000.solverstate
I0628 19:19:27.846946 32082 solver.cpp:401] Sparsity after update:
I0628 19:19:27.848012 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:19:27.848021 32082 net.cpp:2170] conv1a_param_0(0.0996) 
I0628 19:19:27.848029 32082 net.cpp:2170] conv1b_param_0(0.2) 
I0628 19:19:27.848034 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:19:27.848038 32082 net.cpp:2170] res2a_branch2a_param_0(0.2) 
I0628 19:19:27.848042 32082 net.cpp:2170] res2a_branch2b_param_0(0.2) 
I0628 19:19:27.848047 32082 net.cpp:2170] res3a_branch2a_param_0(0.2) 
I0628 19:19:27.848052 32082 net.cpp:2170] res3a_branch2b_param_0(0.2) 
I0628 19:19:27.848057 32082 net.cpp:2170] res4a_branch2a_param_0(0.2) 
I0628 19:19:27.848060 32082 net.cpp:2170] res4a_branch2b_param_0(0.2) 
I0628 19:19:27.848064 32082 net.cpp:2170] res5a_branch2a_param_0(0.2) 
I0628 19:19:27.848069 32082 net.cpp:2170] res5a_branch2b_param_0(0.2) 
I0628 19:19:27.848073 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (470708/2.3599e+06) 0.199
I0628 19:19:27.848085 32082 solver.cpp:545] Iteration 10000, Testing net (#0)
I0628 19:19:28.848776 32080 data_reader.cpp:262] Starting prefetch of epoch 10
I0628 19:19:28.868971 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9176
I0628 19:19:28.868983 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:19:28.868988 32082 solver.cpp:630]     Test net output #2: loss = 0.297735 (* 1 = 0.297735 loss)
I0628 19:19:28.869000 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02055s
I0628 19:19:28.886283 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.22
I0628 19:19:29.187264 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:19:29.188745 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:19:29.189101 32082 solver.cpp:349] Iteration 10000 (32.6681 iter/s, 3.06109s/100 iter), loss = 0.000613237
I0628 19:19:29.189121 32082 solver.cpp:371]     Train net output #0: loss = 0.000613234 (* 1 = 0.000613234 loss)
I0628 19:19:29.189126 32082 sgd_solver.cpp:137] Iteration 10000, lr = 0.0084375, m = 0.9
I0628 19:19:30.911495 32082 solver.cpp:349] Iteration 10100 (58.0805 iter/s, 1.72175s/100 iter), loss = 0.0014592
I0628 19:19:30.911523 32082 solver.cpp:371]     Train net output #0: loss = 0.0014592 (* 1 = 0.0014592 loss)
I0628 19:19:30.911530 32082 sgd_solver.cpp:137] Iteration 10100, lr = 0.00842187, m = 0.9
I0628 19:19:31.823320 32050 data_reader.cpp:262] Starting prefetch of epoch 13
I0628 19:19:32.632098 32082 solver.cpp:349] Iteration 10200 (58.1413 iter/s, 1.71995s/100 iter), loss = 0.000780241
I0628 19:19:32.632122 32082 solver.cpp:371]     Train net output #0: loss = 0.000780238 (* 1 = 0.000780238 loss)
I0628 19:19:32.632128 32082 sgd_solver.cpp:137] Iteration 10200, lr = 0.00840625, m = 0.9
I0628 19:19:34.349612 32082 solver.cpp:349] Iteration 10300 (58.2456 iter/s, 1.71687s/100 iter), loss = 0.000969737
I0628 19:19:34.349638 32082 solver.cpp:371]     Train net output #0: loss = 0.000969734 (* 1 = 0.000969734 loss)
I0628 19:19:34.349644 32082 sgd_solver.cpp:137] Iteration 10300, lr = 0.00839063, m = 0.9
I0628 19:19:36.072269 32082 solver.cpp:349] Iteration 10400 (58.0717 iter/s, 1.72201s/100 iter), loss = 0.00253104
I0628 19:19:36.072294 32082 solver.cpp:371]     Train net output #0: loss = 0.00253104 (* 1 = 0.00253104 loss)
I0628 19:19:36.072301 32082 sgd_solver.cpp:137] Iteration 10400, lr = 0.008375, m = 0.9
I0628 19:19:37.795069 32082 solver.cpp:349] Iteration 10500 (58.0668 iter/s, 1.72215s/100 iter), loss = 0.00152859
I0628 19:19:37.795109 32082 solver.cpp:371]     Train net output #0: loss = 0.00152858 (* 1 = 0.00152858 loss)
I0628 19:19:37.795115 32082 sgd_solver.cpp:137] Iteration 10500, lr = 0.00835937, m = 0.9
I0628 19:19:39.516350 32082 solver.cpp:349] Iteration 10600 (58.1185 iter/s, 1.72062s/100 iter), loss = 0.00187302
I0628 19:19:39.516372 32082 solver.cpp:371]     Train net output #0: loss = 0.00187301 (* 1 = 0.00187301 loss)
I0628 19:19:39.516378 32082 sgd_solver.cpp:137] Iteration 10600, lr = 0.00834375, m = 0.9
I0628 19:19:41.234783 32082 solver.cpp:349] Iteration 10700 (58.2141 iter/s, 1.7178s/100 iter), loss = 0.000964664
I0628 19:19:41.234812 32082 solver.cpp:371]     Train net output #0: loss = 0.000964661 (* 1 = 0.000964661 loss)
I0628 19:19:41.234817 32082 sgd_solver.cpp:137] Iteration 10700, lr = 0.00832812, m = 0.9
I0628 19:19:42.953799 32082 solver.cpp:349] Iteration 10800 (58.1945 iter/s, 1.71837s/100 iter), loss = 0.00154089
I0628 19:19:42.953824 32082 solver.cpp:371]     Train net output #0: loss = 0.00154089 (* 1 = 0.00154089 loss)
I0628 19:19:42.953830 32082 sgd_solver.cpp:137] Iteration 10800, lr = 0.0083125, m = 0.9
I0628 19:19:44.675196 32082 solver.cpp:349] Iteration 10900 (58.1139 iter/s, 1.72076s/100 iter), loss = 0.00134306
I0628 19:19:44.675218 32082 solver.cpp:371]     Train net output #0: loss = 0.00134306 (* 1 = 0.00134306 loss)
I0628 19:19:44.675225 32082 sgd_solver.cpp:137] Iteration 10900, lr = 0.00829687, m = 0.9
I0628 19:19:45.258944 32050 data_reader.cpp:262] Starting prefetch of epoch 14
I0628 19:19:46.377391 32082 solver.cpp:401] Sparsity after update:
I0628 19:19:46.378463 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:19:46.378470 32082 net.cpp:2170] conv1a_param_0(0.11) 
I0628 19:19:46.378479 32082 net.cpp:2170] conv1b_param_0(0.22) 
I0628 19:19:46.378484 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:19:46.378487 32082 net.cpp:2170] res2a_branch2a_param_0(0.22) 
I0628 19:19:46.378492 32082 net.cpp:2170] res2a_branch2b_param_0(0.22) 
I0628 19:19:46.378496 32082 net.cpp:2170] res3a_branch2a_param_0(0.22) 
I0628 19:19:46.378501 32082 net.cpp:2170] res3a_branch2b_param_0(0.22) 
I0628 19:19:46.378505 32082 net.cpp:2170] res4a_branch2a_param_0(0.22) 
I0628 19:19:46.378510 32082 net.cpp:2170] res4a_branch2b_param_0(0.22) 
I0628 19:19:46.378515 32082 net.cpp:2170] res5a_branch2a_param_0(0.22) 
I0628 19:19:46.378518 32082 net.cpp:2170] res5a_branch2b_param_0(0.22) 
I0628 19:19:46.378523 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (517781/2.3599e+06) 0.219
I0628 19:19:46.378535 32082 solver.cpp:545] Iteration 11000, Testing net (#0)
I0628 19:19:47.378394 32080 data_reader.cpp:262] Starting prefetch of epoch 11
I0628 19:19:47.399004 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9182
I0628 19:19:47.399019 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:19:47.399024 32082 solver.cpp:630]     Test net output #2: loss = 0.298564 (* 1 = 0.298564 loss)
I0628 19:19:47.399037 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02015s
I0628 19:19:47.416344 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.24
I0628 19:19:47.737447 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:19:47.738919 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:19:47.739274 32082 solver.cpp:349] Iteration 11000 (32.6479 iter/s, 3.06299s/100 iter), loss = 0.00173886
I0628 19:19:47.739291 32082 solver.cpp:371]     Train net output #0: loss = 0.00173885 (* 1 = 0.00173885 loss)
I0628 19:19:47.739297 32082 sgd_solver.cpp:137] Iteration 11000, lr = 0.00828125, m = 0.9
I0628 19:19:49.458783 32082 solver.cpp:349] Iteration 11100 (58.1772 iter/s, 1.71889s/100 iter), loss = 0.000907674
I0628 19:19:49.458806 32082 solver.cpp:371]     Train net output #0: loss = 0.000907671 (* 1 = 0.000907671 loss)
I0628 19:19:49.458812 32082 sgd_solver.cpp:137] Iteration 11100, lr = 0.00826562, m = 0.9
I0628 19:19:51.180548 32082 solver.cpp:349] Iteration 11200 (58.1017 iter/s, 1.72112s/100 iter), loss = 0.00130449
I0628 19:19:51.180574 32082 solver.cpp:371]     Train net output #0: loss = 0.00130449 (* 1 = 0.00130449 loss)
I0628 19:19:51.180580 32082 sgd_solver.cpp:137] Iteration 11200, lr = 0.00825, m = 0.9
I0628 19:19:52.902299 32082 solver.cpp:349] Iteration 11300 (58.1017 iter/s, 1.72112s/100 iter), loss = 0.000824856
I0628 19:19:52.902326 32082 solver.cpp:371]     Train net output #0: loss = 0.000824853 (* 1 = 0.000824853 loss)
I0628 19:19:52.902333 32082 sgd_solver.cpp:137] Iteration 11300, lr = 0.00823438, m = 0.9
I0628 19:19:54.625238 32082 solver.cpp:349] Iteration 11400 (58.0616 iter/s, 1.72231s/100 iter), loss = 0.00111397
I0628 19:19:54.625259 32082 solver.cpp:371]     Train net output #0: loss = 0.00111397 (* 1 = 0.00111397 loss)
I0628 19:19:54.625263 32082 sgd_solver.cpp:137] Iteration 11400, lr = 0.00821875, m = 0.9
I0628 19:19:56.346091 32082 solver.cpp:349] Iteration 11500 (58.1316 iter/s, 1.72024s/100 iter), loss = 0.00137205
I0628 19:19:56.346154 32082 solver.cpp:371]     Train net output #0: loss = 0.00137205 (* 1 = 0.00137205 loss)
I0628 19:19:56.346161 32082 sgd_solver.cpp:137] Iteration 11500, lr = 0.00820312, m = 0.9
I0628 19:19:58.069077 32082 solver.cpp:349] Iteration 11600 (58.0612 iter/s, 1.72232s/100 iter), loss = 0.0014088
I0628 19:19:58.069123 32082 solver.cpp:371]     Train net output #0: loss = 0.0014088 (* 1 = 0.0014088 loss)
I0628 19:19:58.069136 32082 sgd_solver.cpp:137] Iteration 11600, lr = 0.0081875, m = 0.9
I0628 19:19:59.787880 32082 solver.cpp:349] Iteration 11700 (58.2023 iter/s, 1.71815s/100 iter), loss = 0.00168468
I0628 19:19:59.787906 32082 solver.cpp:371]     Train net output #0: loss = 0.00168468 (* 1 = 0.00168468 loss)
I0628 19:19:59.787912 32082 sgd_solver.cpp:137] Iteration 11700, lr = 0.00817188, m = 0.9
I0628 19:20:00.045784 32050 data_reader.cpp:262] Starting prefetch of epoch 15
I0628 19:20:01.506086 32082 solver.cpp:349] Iteration 11800 (58.2215 iter/s, 1.71758s/100 iter), loss = 0.00105685
I0628 19:20:01.506144 32082 solver.cpp:371]     Train net output #0: loss = 0.00105685 (* 1 = 0.00105685 loss)
I0628 19:20:01.506153 32082 sgd_solver.cpp:137] Iteration 11800, lr = 0.00815625, m = 0.9
I0628 19:20:03.226028 32082 solver.cpp:349] Iteration 11900 (58.1638 iter/s, 1.71928s/100 iter), loss = 0.00101841
I0628 19:20:03.226054 32082 solver.cpp:371]     Train net output #0: loss = 0.0010184 (* 1 = 0.0010184 loss)
I0628 19:20:03.226060 32082 sgd_solver.cpp:137] Iteration 11900, lr = 0.00814062, m = 0.9
I0628 19:20:04.932199 32082 solver.cpp:401] Sparsity after update:
I0628 19:20:04.933286 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:20:04.933295 32082 net.cpp:2170] conv1a_param_0(0.12) 
I0628 19:20:04.933300 32082 net.cpp:2170] conv1b_param_0(0.24) 
I0628 19:20:04.933301 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:20:04.933303 32082 net.cpp:2170] res2a_branch2a_param_0(0.24) 
I0628 19:20:04.933305 32082 net.cpp:2170] res2a_branch2b_param_0(0.24) 
I0628 19:20:04.933308 32082 net.cpp:2170] res3a_branch2a_param_0(0.24) 
I0628 19:20:04.933310 32082 net.cpp:2170] res3a_branch2b_param_0(0.24) 
I0628 19:20:04.933312 32082 net.cpp:2170] res4a_branch2a_param_0(0.24) 
I0628 19:20:04.933315 32082 net.cpp:2170] res4a_branch2b_param_0(0.24) 
I0628 19:20:04.933316 32082 net.cpp:2170] res5a_branch2a_param_0(0.24) 
I0628 19:20:04.933317 32082 net.cpp:2170] res5a_branch2b_param_0(0.24) 
I0628 19:20:04.933320 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (564843/2.3599e+06) 0.239
I0628 19:20:04.933328 32082 solver.cpp:545] Iteration 12000, Testing net (#0)
I0628 19:20:05.931288 32080 data_reader.cpp:262] Starting prefetch of epoch 12
I0628 19:20:05.954638 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9182
I0628 19:20:05.954651 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:20:05.954656 32082 solver.cpp:630]     Test net output #2: loss = 0.297628 (* 1 = 0.297628 loss)
I0628 19:20:05.954670 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.021s
I0628 19:20:05.971977 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.26
I0628 19:20:06.299933 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:20:06.301401 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:20:06.301760 32082 solver.cpp:349] Iteration 12000 (32.5238 iter/s, 3.07467s/100 iter), loss = 0.00180383
I0628 19:20:06.301779 32082 solver.cpp:371]     Train net output #0: loss = 0.00180382 (* 1 = 0.00180382 loss)
I0628 19:20:06.301785 32082 sgd_solver.cpp:137] Iteration 12000, lr = 0.008125, m = 0.9
I0628 19:20:08.025883 32082 solver.cpp:349] Iteration 12100 (58.021 iter/s, 1.72351s/100 iter), loss = 0.00162371
I0628 19:20:08.025907 32082 solver.cpp:371]     Train net output #0: loss = 0.0016237 (* 1 = 0.0016237 loss)
I0628 19:20:08.025913 32082 sgd_solver.cpp:137] Iteration 12100, lr = 0.00810937, m = 0.9
I0628 19:20:09.747210 32082 solver.cpp:349] Iteration 12200 (58.1154 iter/s, 1.72071s/100 iter), loss = 0.00101637
I0628 19:20:09.747251 32082 solver.cpp:371]     Train net output #0: loss = 0.00101637 (* 1 = 0.00101637 loss)
I0628 19:20:09.747256 32082 sgd_solver.cpp:137] Iteration 12200, lr = 0.00809375, m = 0.9
I0628 19:20:11.465023 32082 solver.cpp:349] Iteration 12300 (58.2348 iter/s, 1.71719s/100 iter), loss = 0.00271192
I0628 19:20:11.465049 32082 solver.cpp:371]     Train net output #0: loss = 0.00271192 (* 1 = 0.00271192 loss)
I0628 19:20:11.465054 32082 sgd_solver.cpp:137] Iteration 12300, lr = 0.00807813, m = 0.9
I0628 19:20:13.189365 32082 solver.cpp:349] Iteration 12400 (58.0137 iter/s, 1.72373s/100 iter), loss = 0.000604328
I0628 19:20:13.189393 32082 solver.cpp:371]     Train net output #0: loss = 0.000604324 (* 1 = 0.000604324 loss)
I0628 19:20:13.189399 32082 sgd_solver.cpp:137] Iteration 12400, lr = 0.0080625, m = 0.9
I0628 19:20:14.840718 32050 data_reader.cpp:262] Starting prefetch of epoch 16
I0628 19:20:14.909363 32082 solver.cpp:349] Iteration 12500 (58.1602 iter/s, 1.71939s/100 iter), loss = 0.00110945
I0628 19:20:14.909390 32082 solver.cpp:371]     Train net output #0: loss = 0.00110944 (* 1 = 0.00110944 loss)
I0628 19:20:14.909396 32082 sgd_solver.cpp:137] Iteration 12500, lr = 0.00804687, m = 0.9
I0628 19:20:16.630746 32082 solver.cpp:349] Iteration 12600 (58.1134 iter/s, 1.72077s/100 iter), loss = 0.000995209
I0628 19:20:16.630769 32082 solver.cpp:371]     Train net output #0: loss = 0.000995206 (* 1 = 0.000995206 loss)
I0628 19:20:16.630775 32082 sgd_solver.cpp:137] Iteration 12600, lr = 0.00803125, m = 0.9
I0628 19:20:18.350015 32082 solver.cpp:349] Iteration 12700 (58.1846 iter/s, 1.71867s/100 iter), loss = 0.00212596
I0628 19:20:18.350040 32082 solver.cpp:371]     Train net output #0: loss = 0.00212596 (* 1 = 0.00212596 loss)
I0628 19:20:18.350046 32082 sgd_solver.cpp:137] Iteration 12700, lr = 0.00801562, m = 0.9
I0628 19:20:20.068261 32082 solver.cpp:349] Iteration 12800 (58.2193 iter/s, 1.71764s/100 iter), loss = 0.00214719
I0628 19:20:20.068287 32082 solver.cpp:371]     Train net output #0: loss = 0.00214718 (* 1 = 0.00214718 loss)
I0628 19:20:20.068294 32082 sgd_solver.cpp:137] Iteration 12800, lr = 0.008, m = 0.9
I0628 19:20:21.790199 32082 solver.cpp:349] Iteration 12900 (58.0945 iter/s, 1.72133s/100 iter), loss = 0.00180843
I0628 19:20:21.790225 32082 solver.cpp:371]     Train net output #0: loss = 0.00180843 (* 1 = 0.00180843 loss)
I0628 19:20:21.790230 32082 sgd_solver.cpp:137] Iteration 12900, lr = 0.00798437, m = 0.9
I0628 19:20:23.490283 32082 solver.cpp:401] Sparsity after update:
I0628 19:20:23.491392 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:20:23.491400 32082 net.cpp:2170] conv1a_param_0(0.13) 
I0628 19:20:23.491406 32082 net.cpp:2170] conv1b_param_0(0.26) 
I0628 19:20:23.491408 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:20:23.491411 32082 net.cpp:2170] res2a_branch2a_param_0(0.26) 
I0628 19:20:23.491413 32082 net.cpp:2170] res2a_branch2b_param_0(0.26) 
I0628 19:20:23.491415 32082 net.cpp:2170] res3a_branch2a_param_0(0.26) 
I0628 19:20:23.491417 32082 net.cpp:2170] res3a_branch2b_param_0(0.26) 
I0628 19:20:23.491420 32082 net.cpp:2170] res4a_branch2a_param_0(0.26) 
I0628 19:20:23.491421 32082 net.cpp:2170] res4a_branch2b_param_0(0.26) 
I0628 19:20:23.491423 32082 net.cpp:2170] res5a_branch2a_param_0(0.26) 
I0628 19:20:23.491425 32082 net.cpp:2170] res5a_branch2b_param_0(0.26) 
I0628 19:20:23.491427 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (611917/2.3599e+06) 0.259
I0628 19:20:23.491435 32082 solver.cpp:545] Iteration 13000, Testing net (#0)
I0628 19:20:24.491652 32080 data_reader.cpp:262] Starting prefetch of epoch 13
I0628 19:20:24.511811 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9176
I0628 19:20:24.511824 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:20:24.511829 32082 solver.cpp:630]     Test net output #2: loss = 0.297317 (* 1 = 0.297317 loss)
I0628 19:20:24.511842 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02008s
I0628 19:20:24.529108 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.28
I0628 19:20:24.891427 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:20:24.892896 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:20:24.893254 32082 solver.cpp:349] Iteration 13000 (32.2371 iter/s, 3.10201s/100 iter), loss = 0.000840855
I0628 19:20:24.893271 32082 solver.cpp:371]     Train net output #0: loss = 0.000840851 (* 1 = 0.000840851 loss)
I0628 19:20:24.893280 32082 sgd_solver.cpp:137] Iteration 13000, lr = 0.00796875, m = 0.9
I0628 19:20:26.619350 32082 solver.cpp:349] Iteration 13100 (57.9542 iter/s, 1.7255s/100 iter), loss = 0.00142433
I0628 19:20:26.619418 32082 solver.cpp:371]     Train net output #0: loss = 0.00142432 (* 1 = 0.00142432 loss)
I0628 19:20:26.619426 32082 sgd_solver.cpp:137] Iteration 13100, lr = 0.00795313, m = 0.9
I0628 19:20:28.341728 32082 solver.cpp:349] Iteration 13200 (58.0811 iter/s, 1.72173s/100 iter), loss = 0.00166087
I0628 19:20:28.341753 32082 solver.cpp:371]     Train net output #0: loss = 0.00166087 (* 1 = 0.00166087 loss)
I0628 19:20:28.341759 32082 sgd_solver.cpp:137] Iteration 13200, lr = 0.0079375, m = 0.9
I0628 19:20:29.684077 32050 data_reader.cpp:262] Starting prefetch of epoch 17
I0628 19:20:30.062101 32082 solver.cpp:349] Iteration 13300 (58.1471 iter/s, 1.71978s/100 iter), loss = 0.000912278
I0628 19:20:30.062125 32082 solver.cpp:371]     Train net output #0: loss = 0.000912274 (* 1 = 0.000912274 loss)
I0628 19:20:30.062131 32082 sgd_solver.cpp:137] Iteration 13300, lr = 0.00792187, m = 0.9
I0628 19:20:31.781476 32082 solver.cpp:349] Iteration 13400 (58.1807 iter/s, 1.71878s/100 iter), loss = 0.00118087
I0628 19:20:31.781500 32082 solver.cpp:371]     Train net output #0: loss = 0.00118086 (* 1 = 0.00118086 loss)
I0628 19:20:31.781507 32082 sgd_solver.cpp:137] Iteration 13400, lr = 0.00790625, m = 0.9
I0628 19:20:33.503947 32082 solver.cpp:349] Iteration 13500 (58.076 iter/s, 1.72188s/100 iter), loss = 0.00181193
I0628 19:20:33.503973 32082 solver.cpp:371]     Train net output #0: loss = 0.00181193 (* 1 = 0.00181193 loss)
I0628 19:20:33.503978 32082 sgd_solver.cpp:137] Iteration 13500, lr = 0.00789062, m = 0.9
I0628 19:20:35.224673 32082 solver.cpp:349] Iteration 13600 (58.1349 iter/s, 1.72014s/100 iter), loss = 0.002209
I0628 19:20:35.224695 32082 solver.cpp:371]     Train net output #0: loss = 0.002209 (* 1 = 0.002209 loss)
I0628 19:20:35.224699 32082 sgd_solver.cpp:137] Iteration 13600, lr = 0.007875, m = 0.9
I0628 19:20:36.947293 32082 solver.cpp:349] Iteration 13700 (58.0707 iter/s, 1.72204s/100 iter), loss = 0.00297785
I0628 19:20:36.947314 32082 solver.cpp:371]     Train net output #0: loss = 0.00297785 (* 1 = 0.00297785 loss)
I0628 19:20:36.947319 32082 sgd_solver.cpp:137] Iteration 13700, lr = 0.00785937, m = 0.9
I0628 19:20:38.674298 32082 solver.cpp:349] Iteration 13800 (57.9232 iter/s, 1.72642s/100 iter), loss = 0.00194699
I0628 19:20:38.674324 32082 solver.cpp:371]     Train net output #0: loss = 0.00194698 (* 1 = 0.00194698 loss)
I0628 19:20:38.674330 32082 sgd_solver.cpp:137] Iteration 13800, lr = 0.00784375, m = 0.9
I0628 19:20:40.393911 32082 solver.cpp:349] Iteration 13900 (58.1724 iter/s, 1.71903s/100 iter), loss = 0.00165088
I0628 19:20:40.393939 32082 solver.cpp:371]     Train net output #0: loss = 0.00165087 (* 1 = 0.00165087 loss)
I0628 19:20:40.393944 32082 sgd_solver.cpp:137] Iteration 13900, lr = 0.00782812, m = 0.9
I0628 19:20:42.096611 32082 solver.cpp:401] Sparsity after update:
I0628 19:20:42.097736 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:20:42.097745 32082 net.cpp:2170] conv1a_param_0(0.14) 
I0628 19:20:42.097751 32082 net.cpp:2170] conv1b_param_0(0.28) 
I0628 19:20:42.097754 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:20:42.097755 32082 net.cpp:2170] res2a_branch2a_param_0(0.28) 
I0628 19:20:42.097757 32082 net.cpp:2170] res2a_branch2b_param_0(0.28) 
I0628 19:20:42.097759 32082 net.cpp:2170] res3a_branch2a_param_0(0.28) 
I0628 19:20:42.097761 32082 net.cpp:2170] res3a_branch2b_param_0(0.28) 
I0628 19:20:42.097762 32082 net.cpp:2170] res4a_branch2a_param_0(0.28) 
I0628 19:20:42.097764 32082 net.cpp:2170] res4a_branch2b_param_0(0.28) 
I0628 19:20:42.097766 32082 net.cpp:2170] res5a_branch2a_param_0(0.28) 
I0628 19:20:42.097769 32082 net.cpp:2170] res5a_branch2b_param_0(0.28) 
I0628 19:20:42.097770 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (658990/2.3599e+06) 0.279
I0628 19:20:42.097776 32082 solver.cpp:545] Iteration 14000, Testing net (#0)
I0628 19:20:43.097185 32080 data_reader.cpp:262] Starting prefetch of epoch 14
I0628 19:20:43.121727 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9182
I0628 19:20:43.121740 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:20:43.121760 32082 solver.cpp:630]     Test net output #2: loss = 0.297584 (* 1 = 0.297584 loss)
I0628 19:20:43.121775 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02367s
I0628 19:20:43.139058 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.3
I0628 19:20:43.508545 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:20:43.510035 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:20:43.510392 32082 solver.cpp:349] Iteration 14000 (32.098 iter/s, 3.11546s/100 iter), loss = 0.00085541
I0628 19:20:43.510411 32082 solver.cpp:371]     Train net output #0: loss = 0.000855407 (* 1 = 0.000855407 loss)
I0628 19:20:43.510417 32082 sgd_solver.cpp:137] Iteration 14000, lr = 0.0078125, m = 0.9
I0628 19:20:44.524830 32050 data_reader.cpp:262] Starting prefetch of epoch 18
I0628 19:20:45.233043 32082 solver.cpp:349] Iteration 14100 (58.0695 iter/s, 1.72208s/100 iter), loss = 0.00177509
I0628 19:20:45.233067 32082 solver.cpp:371]     Train net output #0: loss = 0.00177508 (* 1 = 0.00177508 loss)
I0628 19:20:45.233074 32082 sgd_solver.cpp:137] Iteration 14100, lr = 0.00779688, m = 0.9
I0628 19:20:46.954504 32082 solver.cpp:349] Iteration 14200 (58.1098 iter/s, 1.72088s/100 iter), loss = 0.00186602
I0628 19:20:46.954530 32082 solver.cpp:371]     Train net output #0: loss = 0.00186602 (* 1 = 0.00186602 loss)
I0628 19:20:46.954535 32082 sgd_solver.cpp:137] Iteration 14200, lr = 0.00778125, m = 0.9
I0628 19:20:48.674077 32082 solver.cpp:349] Iteration 14300 (58.1735 iter/s, 1.71899s/100 iter), loss = 0.00137069
I0628 19:20:48.674101 32082 solver.cpp:371]     Train net output #0: loss = 0.00137069 (* 1 = 0.00137069 loss)
I0628 19:20:48.674106 32082 sgd_solver.cpp:137] Iteration 14300, lr = 0.00776563, m = 0.9
I0628 19:20:50.399368 32082 solver.cpp:349] Iteration 14400 (57.9806 iter/s, 1.72471s/100 iter), loss = 0.000982747
I0628 19:20:50.399389 32082 solver.cpp:371]     Train net output #0: loss = 0.000982743 (* 1 = 0.000982743 loss)
I0628 19:20:50.399394 32082 sgd_solver.cpp:137] Iteration 14400, lr = 0.00775, m = 0.9
I0628 19:20:52.116974 32082 solver.cpp:349] Iteration 14500 (58.2398 iter/s, 1.71704s/100 iter), loss = 0.00206265
I0628 19:20:52.117000 32082 solver.cpp:371]     Train net output #0: loss = 0.00206265 (* 1 = 0.00206265 loss)
I0628 19:20:52.117005 32082 sgd_solver.cpp:137] Iteration 14500, lr = 0.00773437, m = 0.9
I0628 19:20:53.840986 32082 solver.cpp:349] Iteration 14600 (58.0236 iter/s, 1.72344s/100 iter), loss = 0.000885526
I0628 19:20:53.841011 32082 solver.cpp:371]     Train net output #0: loss = 0.000885522 (* 1 = 0.000885522 loss)
I0628 19:20:53.841015 32082 sgd_solver.cpp:137] Iteration 14600, lr = 0.00771875, m = 0.9
I0628 19:20:55.558038 32082 solver.cpp:349] Iteration 14700 (58.2587 iter/s, 1.71648s/100 iter), loss = 0.00121897
I0628 19:20:55.558065 32082 solver.cpp:371]     Train net output #0: loss = 0.00121896 (* 1 = 0.00121896 loss)
I0628 19:20:55.558071 32082 sgd_solver.cpp:137] Iteration 14700, lr = 0.00770312, m = 0.9
I0628 19:20:57.283500 32082 solver.cpp:349] Iteration 14800 (57.9748 iter/s, 1.72489s/100 iter), loss = 0.00199677
I0628 19:20:57.283588 32082 solver.cpp:371]     Train net output #0: loss = 0.00199677 (* 1 = 0.00199677 loss)
I0628 19:20:57.283596 32082 sgd_solver.cpp:137] Iteration 14800, lr = 0.0076875, m = 0.9
I0628 19:20:57.974362 32050 data_reader.cpp:262] Starting prefetch of epoch 19
I0628 19:20:59.005138 32082 solver.cpp:349] Iteration 14900 (58.1056 iter/s, 1.721s/100 iter), loss = 0.000870252
I0628 19:20:59.005161 32082 solver.cpp:371]     Train net output #0: loss = 0.000870248 (* 1 = 0.000870248 loss)
I0628 19:20:59.005164 32082 sgd_solver.cpp:137] Iteration 14900, lr = 0.00767187, m = 0.9
I0628 19:21:00.714540 32082 solver.cpp:401] Sparsity after update:
I0628 19:21:00.715612 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:21:00.715621 32082 net.cpp:2170] conv1a_param_0(0.15) 
I0628 19:21:00.715627 32082 net.cpp:2170] conv1b_param_0(0.3) 
I0628 19:21:00.715631 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:21:00.715639 32082 net.cpp:2170] res2a_branch2a_param_0(0.3) 
I0628 19:21:00.715642 32082 net.cpp:2170] res2a_branch2b_param_0(0.3) 
I0628 19:21:00.715644 32082 net.cpp:2170] res3a_branch2a_param_0(0.3) 
I0628 19:21:00.715646 32082 net.cpp:2170] res3a_branch2b_param_0(0.3) 
I0628 19:21:00.715648 32082 net.cpp:2170] res4a_branch2a_param_0(0.3) 
I0628 19:21:00.715651 32082 net.cpp:2170] res4a_branch2b_param_0(0.3) 
I0628 19:21:00.715654 32082 net.cpp:2170] res5a_branch2a_param_0(0.3) 
I0628 19:21:00.715657 32082 net.cpp:2170] res5a_branch2b_param_0(0.3) 
I0628 19:21:00.715662 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (706067/2.3599e+06) 0.299
I0628 19:21:00.715670 32082 solver.cpp:545] Iteration 15000, Testing net (#0)
I0628 19:21:01.712234 32080 data_reader.cpp:262] Starting prefetch of epoch 15
I0628 19:21:01.735826 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9172
I0628 19:21:01.735837 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9964
I0628 19:21:01.735842 32082 solver.cpp:630]     Test net output #2: loss = 0.296975 (* 1 = 0.296975 loss)
I0628 19:21:01.735857 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01987s
I0628 19:21:01.753106 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.32
I0628 19:21:02.145108 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:21:02.146592 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:21:02.146950 32082 solver.cpp:349] Iteration 15000 (31.8388 iter/s, 3.14082s/100 iter), loss = 0.00143987
I0628 19:21:02.146968 32082 solver.cpp:371]     Train net output #0: loss = 0.00143987 (* 1 = 0.00143987 loss)
I0628 19:21:02.146973 32082 sgd_solver.cpp:137] Iteration 15000, lr = 0.00765625, m = 0.9
I0628 19:21:03.867398 32082 solver.cpp:349] Iteration 15100 (58.1432 iter/s, 1.71989s/100 iter), loss = 0.00275538
I0628 19:21:03.867418 32082 solver.cpp:371]     Train net output #0: loss = 0.00275538 (* 1 = 0.00275538 loss)
I0628 19:21:03.867421 32082 sgd_solver.cpp:137] Iteration 15100, lr = 0.00764062, m = 0.9
I0628 19:21:05.586139 32082 solver.cpp:349] Iteration 15200 (58.2009 iter/s, 1.71819s/100 iter), loss = 0.000795216
I0628 19:21:05.586163 32082 solver.cpp:371]     Train net output #0: loss = 0.000795213 (* 1 = 0.000795213 loss)
I0628 19:21:05.586169 32082 sgd_solver.cpp:137] Iteration 15200, lr = 0.007625, m = 0.9
I0628 19:21:07.306494 32082 solver.cpp:349] Iteration 15300 (58.1466 iter/s, 1.71979s/100 iter), loss = 0.00182629
I0628 19:21:07.306520 32082 solver.cpp:371]     Train net output #0: loss = 0.00182629 (* 1 = 0.00182629 loss)
I0628 19:21:07.306526 32082 sgd_solver.cpp:137] Iteration 15300, lr = 0.00760937, m = 0.9
I0628 19:21:09.026090 32082 solver.cpp:349] Iteration 15400 (58.1722 iter/s, 1.71903s/100 iter), loss = 0.00124094
I0628 19:21:09.026115 32082 solver.cpp:371]     Train net output #0: loss = 0.00124094 (* 1 = 0.00124094 loss)
I0628 19:21:09.026121 32082 sgd_solver.cpp:137] Iteration 15400, lr = 0.00759375, m = 0.9
I0628 19:21:10.747356 32082 solver.cpp:349] Iteration 15500 (58.1157 iter/s, 1.72071s/100 iter), loss = 0.0026744
I0628 19:21:10.747397 32082 solver.cpp:371]     Train net output #0: loss = 0.0026744 (* 1 = 0.0026744 loss)
I0628 19:21:10.747403 32082 sgd_solver.cpp:137] Iteration 15500, lr = 0.00757812, m = 0.9
I0628 19:21:12.463558 32082 solver.cpp:349] Iteration 15600 (58.2877 iter/s, 1.71563s/100 iter), loss = 0.00154356
I0628 19:21:12.463583 32082 solver.cpp:371]     Train net output #0: loss = 0.00154356 (* 1 = 0.00154356 loss)
I0628 19:21:12.463589 32082 sgd_solver.cpp:137] Iteration 15600, lr = 0.0075625, m = 0.9
I0628 19:21:12.827086 32050 data_reader.cpp:262] Starting prefetch of epoch 20
I0628 19:21:14.183971 32082 solver.cpp:349] Iteration 15700 (58.1445 iter/s, 1.71985s/100 iter), loss = 0.00130775
I0628 19:21:14.183997 32082 solver.cpp:371]     Train net output #0: loss = 0.00130774 (* 1 = 0.00130774 loss)
I0628 19:21:14.184005 32082 sgd_solver.cpp:137] Iteration 15700, lr = 0.00754687, m = 0.9
I0628 19:21:15.903528 32082 solver.cpp:349] Iteration 15800 (58.1733 iter/s, 1.719s/100 iter), loss = 0.00106151
I0628 19:21:15.903551 32082 solver.cpp:371]     Train net output #0: loss = 0.00106151 (* 1 = 0.00106151 loss)
I0628 19:21:15.903555 32082 sgd_solver.cpp:137] Iteration 15800, lr = 0.00753125, m = 0.9
I0628 19:21:17.630542 32082 solver.cpp:349] Iteration 15900 (57.9219 iter/s, 1.72646s/100 iter), loss = 0.00107149
I0628 19:21:17.630566 32082 solver.cpp:371]     Train net output #0: loss = 0.00107149 (* 1 = 0.00107149 loss)
I0628 19:21:17.630573 32082 sgd_solver.cpp:137] Iteration 15900, lr = 0.00751562, m = 0.9
I0628 19:21:19.334621 32082 solver.cpp:401] Sparsity after update:
I0628 19:21:19.335716 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:21:19.335726 32082 net.cpp:2170] conv1a_param_0(0.16) 
I0628 19:21:19.335733 32082 net.cpp:2170] conv1b_param_0(0.32) 
I0628 19:21:19.335738 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:21:19.335742 32082 net.cpp:2170] res2a_branch2a_param_0(0.32) 
I0628 19:21:19.335747 32082 net.cpp:2170] res2a_branch2b_param_0(0.32) 
I0628 19:21:19.335750 32082 net.cpp:2170] res3a_branch2a_param_0(0.32) 
I0628 19:21:19.335755 32082 net.cpp:2170] res3a_branch2b_param_0(0.32) 
I0628 19:21:19.335759 32082 net.cpp:2170] res4a_branch2a_param_0(0.32) 
I0628 19:21:19.335764 32082 net.cpp:2170] res4a_branch2b_param_0(0.32) 
I0628 19:21:19.335768 32082 net.cpp:2170] res5a_branch2a_param_0(0.32) 
I0628 19:21:19.335772 32082 net.cpp:2170] res5a_branch2b_param_0(0.32) 
I0628 19:21:19.335777 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (753133/2.3599e+06) 0.319
I0628 19:21:19.335788 32082 solver.cpp:545] Iteration 16000, Testing net (#0)
I0628 19:21:20.332100 32080 data_reader.cpp:262] Starting prefetch of epoch 16
I0628 19:21:20.355062 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.917199
I0628 19:21:20.355074 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9956
I0628 19:21:20.355080 32082 solver.cpp:630]     Test net output #2: loss = 0.299341 (* 1 = 0.299341 loss)
I0628 19:21:20.355098 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01901s
I0628 19:21:20.372404 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.34
I0628 19:21:20.779772 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:21:20.781250 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:21:20.781608 32082 solver.cpp:349] Iteration 16000 (31.7451 iter/s, 3.15009s/100 iter), loss = 0.00156999
I0628 19:21:20.781625 32082 solver.cpp:371]     Train net output #0: loss = 0.00156999 (* 1 = 0.00156999 loss)
I0628 19:21:20.781631 32082 sgd_solver.cpp:137] Iteration 16000, lr = 0.0075, m = 0.9
I0628 19:21:22.503127 32082 solver.cpp:349] Iteration 16100 (58.1065 iter/s, 1.72098s/100 iter), loss = 0.00238028
I0628 19:21:22.503151 32082 solver.cpp:371]     Train net output #0: loss = 0.00238028 (* 1 = 0.00238028 loss)
I0628 19:21:22.503155 32082 sgd_solver.cpp:137] Iteration 16100, lr = 0.00748438, m = 0.9
I0628 19:21:24.224289 32082 solver.cpp:349] Iteration 16200 (58.1187 iter/s, 1.72062s/100 iter), loss = 0.00130366
I0628 19:21:24.224334 32082 solver.cpp:371]     Train net output #0: loss = 0.00130366 (* 1 = 0.00130366 loss)
I0628 19:21:24.224339 32082 sgd_solver.cpp:137] Iteration 16200, lr = 0.00746875, m = 0.9
I0628 19:21:25.945211 32082 solver.cpp:349] Iteration 16300 (58.1276 iter/s, 1.72035s/100 iter), loss = 0.00132841
I0628 19:21:25.945233 32082 solver.cpp:371]     Train net output #0: loss = 0.00132841 (* 1 = 0.00132841 loss)
I0628 19:21:25.945238 32082 sgd_solver.cpp:137] Iteration 16300, lr = 0.00745312, m = 0.9
I0628 19:21:27.668085 32082 solver.cpp:349] Iteration 16400 (58.0608 iter/s, 1.72233s/100 iter), loss = 0.00167868
I0628 19:21:27.668157 32082 solver.cpp:371]     Train net output #0: loss = 0.00167867 (* 1 = 0.00167867 loss)
I0628 19:21:27.668165 32082 sgd_solver.cpp:137] Iteration 16400, lr = 0.0074375, m = 0.9
I0628 19:21:27.719925 32050 data_reader.cpp:262] Starting prefetch of epoch 21
I0628 19:21:29.387053 32082 solver.cpp:349] Iteration 16500 (58.1947 iter/s, 1.71837s/100 iter), loss = 0.00176654
I0628 19:21:29.387079 32082 solver.cpp:371]     Train net output #0: loss = 0.00176654 (* 1 = 0.00176654 loss)
I0628 19:21:29.387084 32082 sgd_solver.cpp:137] Iteration 16500, lr = 0.00742187, m = 0.9
I0628 19:21:31.105729 32082 solver.cpp:349] Iteration 16600 (58.2027 iter/s, 1.71813s/100 iter), loss = 0.000747273
I0628 19:21:31.105752 32082 solver.cpp:371]     Train net output #0: loss = 0.000747269 (* 1 = 0.000747269 loss)
I0628 19:21:31.105756 32082 sgd_solver.cpp:137] Iteration 16600, lr = 0.00740625, m = 0.9
I0628 19:21:32.826076 32082 solver.cpp:349] Iteration 16700 (58.1461 iter/s, 1.71981s/100 iter), loss = 0.00120975
I0628 19:21:32.826102 32082 solver.cpp:371]     Train net output #0: loss = 0.00120974 (* 1 = 0.00120974 loss)
I0628 19:21:32.826107 32082 sgd_solver.cpp:137] Iteration 16700, lr = 0.00739062, m = 0.9
I0628 19:21:34.549110 32082 solver.cpp:349] Iteration 16800 (58.0555 iter/s, 1.72249s/100 iter), loss = 0.00288433
I0628 19:21:34.549137 32082 solver.cpp:371]     Train net output #0: loss = 0.00288433 (* 1 = 0.00288433 loss)
I0628 19:21:34.549144 32082 sgd_solver.cpp:137] Iteration 16800, lr = 0.007375, m = 0.9
I0628 19:21:36.270468 32082 solver.cpp:349] Iteration 16900 (58.112 iter/s, 1.72081s/100 iter), loss = 0.00170128
I0628 19:21:36.270495 32082 solver.cpp:371]     Train net output #0: loss = 0.00170128 (* 1 = 0.00170128 loss)
I0628 19:21:36.270501 32082 sgd_solver.cpp:137] Iteration 16900, lr = 0.00735937, m = 0.9
I0628 19:21:37.969740 32082 solver.cpp:401] Sparsity after update:
I0628 19:21:37.970813 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:21:37.970823 32082 net.cpp:2170] conv1a_param_0(0.17) 
I0628 19:21:37.970829 32082 net.cpp:2170] conv1b_param_0(0.34) 
I0628 19:21:37.970831 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:21:37.970834 32082 net.cpp:2170] res2a_branch2a_param_0(0.34) 
I0628 19:21:37.970837 32082 net.cpp:2170] res2a_branch2b_param_0(0.34) 
I0628 19:21:37.970839 32082 net.cpp:2170] res3a_branch2a_param_0(0.34) 
I0628 19:21:37.970841 32082 net.cpp:2170] res3a_branch2b_param_0(0.34) 
I0628 19:21:37.970844 32082 net.cpp:2170] res4a_branch2a_param_0(0.34) 
I0628 19:21:37.970845 32082 net.cpp:2170] res4a_branch2b_param_0(0.34) 
I0628 19:21:37.970847 32082 net.cpp:2170] res5a_branch2a_param_0(0.34) 
I0628 19:21:37.970849 32082 net.cpp:2170] res5a_branch2b_param_0(0.34) 
I0628 19:21:37.970851 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (800206/2.3599e+06) 0.339
I0628 19:21:37.970859 32082 solver.cpp:545] Iteration 17000, Testing net (#0)
I0628 19:21:38.971030 32080 data_reader.cpp:262] Starting prefetch of epoch 17
I0628 19:21:38.991153 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9188
I0628 19:21:38.991165 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:21:38.991169 32082 solver.cpp:630]     Test net output #2: loss = 0.298422 (* 1 = 0.298422 loss)
I0628 19:21:38.991183 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02003s
I0628 19:21:39.008482 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.36
I0628 19:21:39.438681 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:21:39.440184 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:21:39.440562 32082 solver.cpp:349] Iteration 17000 (31.5543 iter/s, 3.16914s/100 iter), loss = 0.00145359
I0628 19:21:39.440585 32082 solver.cpp:371]     Train net output #0: loss = 0.00145359 (* 1 = 0.00145359 loss)
I0628 19:21:39.440603 32082 sgd_solver.cpp:137] Iteration 17000, lr = 0.00734375, m = 0.9
I0628 19:21:41.161120 32082 solver.cpp:349] Iteration 17100 (58.1392 iter/s, 1.72001s/100 iter), loss = 0.00232401
I0628 19:21:41.161159 32082 solver.cpp:371]     Train net output #0: loss = 0.00232401 (* 1 = 0.00232401 loss)
I0628 19:21:41.161166 32082 sgd_solver.cpp:137] Iteration 17100, lr = 0.00732813, m = 0.9
I0628 19:21:42.605798 32050 data_reader.cpp:262] Starting prefetch of epoch 22
I0628 19:21:42.880795 32082 solver.cpp:349] Iteration 17200 (58.1694 iter/s, 1.71912s/100 iter), loss = 0.00116345
I0628 19:21:42.880821 32082 solver.cpp:371]     Train net output #0: loss = 0.00116345 (* 1 = 0.00116345 loss)
I0628 19:21:42.880828 32082 sgd_solver.cpp:137] Iteration 17200, lr = 0.0073125, m = 0.9
I0628 19:21:44.607269 32082 solver.cpp:349] Iteration 17300 (57.9397 iter/s, 1.72593s/100 iter), loss = 0.00166173
I0628 19:21:44.607291 32082 solver.cpp:371]     Train net output #0: loss = 0.00166173 (* 1 = 0.00166173 loss)
I0628 19:21:44.607295 32082 sgd_solver.cpp:137] Iteration 17300, lr = 0.00729688, m = 0.9
I0628 19:21:46.327725 32082 solver.cpp:349] Iteration 17400 (58.142 iter/s, 1.71993s/100 iter), loss = 0.00112594
I0628 19:21:46.327747 32082 solver.cpp:371]     Train net output #0: loss = 0.00112594 (* 1 = 0.00112594 loss)
I0628 19:21:46.327751 32082 sgd_solver.cpp:137] Iteration 17400, lr = 0.00728125, m = 0.9
I0628 19:21:48.047842 32082 solver.cpp:349] Iteration 17500 (58.1534 iter/s, 1.71959s/100 iter), loss = 0.00133067
I0628 19:21:48.047868 32082 solver.cpp:371]     Train net output #0: loss = 0.00133066 (* 1 = 0.00133066 loss)
I0628 19:21:48.047873 32082 sgd_solver.cpp:137] Iteration 17500, lr = 0.00726563, m = 0.9
I0628 19:21:49.770807 32082 solver.cpp:349] Iteration 17600 (58.0575 iter/s, 1.72243s/100 iter), loss = 0.00323801
I0628 19:21:49.770833 32082 solver.cpp:371]     Train net output #0: loss = 0.003238 (* 1 = 0.003238 loss)
I0628 19:21:49.770839 32082 sgd_solver.cpp:137] Iteration 17600, lr = 0.00725, m = 0.9
I0628 19:21:51.490188 32082 solver.cpp:349] Iteration 17700 (58.1785 iter/s, 1.71885s/100 iter), loss = 0.00175501
I0628 19:21:51.490214 32082 solver.cpp:371]     Train net output #0: loss = 0.00175501 (* 1 = 0.00175501 loss)
I0628 19:21:51.490221 32082 sgd_solver.cpp:137] Iteration 17700, lr = 0.00723437, m = 0.9
I0628 19:21:53.210973 32082 solver.cpp:349] Iteration 17800 (58.131 iter/s, 1.72025s/100 iter), loss = 0.00158824
I0628 19:21:53.210994 32082 solver.cpp:371]     Train net output #0: loss = 0.00158824 (* 1 = 0.00158824 loss)
I0628 19:21:53.210999 32082 sgd_solver.cpp:137] Iteration 17800, lr = 0.00721875, m = 0.9
I0628 19:21:54.931502 32082 solver.cpp:349] Iteration 17900 (58.1393 iter/s, 1.72001s/100 iter), loss = 0.00123675
I0628 19:21:54.931529 32082 solver.cpp:371]     Train net output #0: loss = 0.00123674 (* 1 = 0.00123674 loss)
I0628 19:21:54.931535 32082 sgd_solver.cpp:137] Iteration 17900, lr = 0.00720312, m = 0.9
I0628 19:21:56.050997 32050 data_reader.cpp:262] Starting prefetch of epoch 23
I0628 19:21:56.634529 32082 solver.cpp:401] Sparsity after update:
I0628 19:21:56.635679 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:21:56.635687 32082 net.cpp:2170] conv1a_param_0(0.18) 
I0628 19:21:56.635694 32082 net.cpp:2170] conv1b_param_0(0.36) 
I0628 19:21:56.635699 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:21:56.635700 32082 net.cpp:2170] res2a_branch2a_param_0(0.36) 
I0628 19:21:56.635702 32082 net.cpp:2170] res2a_branch2b_param_0(0.36) 
I0628 19:21:56.635705 32082 net.cpp:2170] res3a_branch2a_param_0(0.36) 
I0628 19:21:56.635706 32082 net.cpp:2170] res3a_branch2b_param_0(0.36) 
I0628 19:21:56.635709 32082 net.cpp:2170] res4a_branch2a_param_0(0.36) 
I0628 19:21:56.635710 32082 net.cpp:2170] res4a_branch2b_param_0(0.36) 
I0628 19:21:56.635713 32082 net.cpp:2170] res5a_branch2a_param_0(0.36) 
I0628 19:21:56.635715 32082 net.cpp:2170] res5a_branch2b_param_0(0.36) 
I0628 19:21:56.635717 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (847279/2.3599e+06) 0.359
I0628 19:21:56.635725 32082 solver.cpp:545] Iteration 18000, Testing net (#0)
I0628 19:21:57.638653 32080 data_reader.cpp:262] Starting prefetch of epoch 18
I0628 19:21:57.659021 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9188
I0628 19:21:57.659054 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:21:57.659059 32082 solver.cpp:630]     Test net output #2: loss = 0.297917 (* 1 = 0.297917 loss)
I0628 19:21:57.659072 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02306s
I0628 19:21:57.676357 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.38
I0628 19:21:58.129647 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:21:58.131119 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:21:58.131477 32082 solver.cpp:349] Iteration 18000 (31.2594 iter/s, 3.19903s/100 iter), loss = 0.00127674
I0628 19:21:58.131496 32082 solver.cpp:371]     Train net output #0: loss = 0.00127674 (* 1 = 0.00127674 loss)
I0628 19:21:58.131501 32082 sgd_solver.cpp:137] Iteration 18000, lr = 0.0071875, m = 0.9
I0628 19:21:59.854558 32082 solver.cpp:349] Iteration 18100 (58.0531 iter/s, 1.72256s/100 iter), loss = 0.00228488
I0628 19:21:59.854584 32082 solver.cpp:371]     Train net output #0: loss = 0.00228487 (* 1 = 0.00228487 loss)
I0628 19:21:59.854590 32082 sgd_solver.cpp:137] Iteration 18100, lr = 0.00717187, m = 0.9
I0628 19:22:01.572015 32082 solver.cpp:349] Iteration 18200 (58.2435 iter/s, 1.71693s/100 iter), loss = 0.000693781
I0628 19:22:01.572041 32082 solver.cpp:371]     Train net output #0: loss = 0.000693776 (* 1 = 0.000693776 loss)
I0628 19:22:01.572046 32082 sgd_solver.cpp:137] Iteration 18200, lr = 0.00715625, m = 0.9
I0628 19:22:03.292608 32082 solver.cpp:349] Iteration 18300 (58.1373 iter/s, 1.72007s/100 iter), loss = 0.00129666
I0628 19:22:03.292635 32082 solver.cpp:371]     Train net output #0: loss = 0.00129666 (* 1 = 0.00129666 loss)
I0628 19:22:03.292641 32082 sgd_solver.cpp:137] Iteration 18300, lr = 0.00714062, m = 0.9
I0628 19:22:05.012122 32082 solver.cpp:349] Iteration 18400 (58.1737 iter/s, 1.71899s/100 iter), loss = 0.00102875
I0628 19:22:05.012148 32082 solver.cpp:371]     Train net output #0: loss = 0.00102875 (* 1 = 0.00102875 loss)
I0628 19:22:05.012154 32082 sgd_solver.cpp:137] Iteration 18400, lr = 0.007125, m = 0.9
I0628 19:22:06.732118 32082 solver.cpp:349] Iteration 18500 (58.1573 iter/s, 1.71947s/100 iter), loss = 0.00305622
I0628 19:22:06.732141 32082 solver.cpp:371]     Train net output #0: loss = 0.00305621 (* 1 = 0.00305621 loss)
I0628 19:22:06.732146 32082 sgd_solver.cpp:137] Iteration 18500, lr = 0.00710937, m = 0.9
I0628 19:22:08.455132 32082 solver.cpp:349] Iteration 18600 (58.0552 iter/s, 1.7225s/100 iter), loss = 0.000981332
I0628 19:22:08.455155 32082 solver.cpp:371]     Train net output #0: loss = 0.000981327 (* 1 = 0.000981327 loss)
I0628 19:22:08.455158 32082 sgd_solver.cpp:137] Iteration 18600, lr = 0.00709375, m = 0.9
I0628 19:22:10.175518 32082 solver.cpp:349] Iteration 18700 (58.1439 iter/s, 1.71987s/100 iter), loss = 0.00153814
I0628 19:22:10.175544 32082 solver.cpp:371]     Train net output #0: loss = 0.00153814 (* 1 = 0.00153814 loss)
I0628 19:22:10.175550 32082 sgd_solver.cpp:137] Iteration 18700, lr = 0.00707812, m = 0.9
I0628 19:22:10.967597 32050 data_reader.cpp:262] Starting prefetch of epoch 24
I0628 19:22:11.894840 32082 solver.cpp:349] Iteration 18800 (58.18 iter/s, 1.7188s/100 iter), loss = 0.00173058
I0628 19:22:11.894861 32082 solver.cpp:371]     Train net output #0: loss = 0.00173057 (* 1 = 0.00173057 loss)
I0628 19:22:11.894865 32082 sgd_solver.cpp:137] Iteration 18800, lr = 0.0070625, m = 0.9
I0628 19:22:13.615367 32082 solver.cpp:349] Iteration 18900 (58.1389 iter/s, 1.72002s/100 iter), loss = 0.00184476
I0628 19:22:13.615391 32082 solver.cpp:371]     Train net output #0: loss = 0.00184476 (* 1 = 0.00184476 loss)
I0628 19:22:13.615394 32082 sgd_solver.cpp:137] Iteration 18900, lr = 0.00704687, m = 0.9
I0628 19:22:15.321308 32082 solver.cpp:401] Sparsity after update:
I0628 19:22:15.322412 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:22:15.322419 32082 net.cpp:2170] conv1a_param_0(0.19) 
I0628 19:22:15.322425 32082 net.cpp:2170] conv1b_param_0(0.38) 
I0628 19:22:15.322428 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:22:15.322430 32082 net.cpp:2170] res2a_branch2a_param_0(0.38) 
I0628 19:22:15.322432 32082 net.cpp:2170] res2a_branch2b_param_0(0.38) 
I0628 19:22:15.322433 32082 net.cpp:2170] res3a_branch2a_param_0(0.38) 
I0628 19:22:15.322435 32082 net.cpp:2170] res3a_branch2b_param_0(0.38) 
I0628 19:22:15.322438 32082 net.cpp:2170] res4a_branch2a_param_0(0.38) 
I0628 19:22:15.322449 32082 net.cpp:2170] res4a_branch2b_param_0(0.38) 
I0628 19:22:15.322451 32082 net.cpp:2170] res5a_branch2a_param_0(0.38) 
I0628 19:22:15.322453 32082 net.cpp:2170] res5a_branch2b_param_0(0.38) 
I0628 19:22:15.322455 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (894353/2.3599e+06) 0.379
I0628 19:22:15.322463 32082 solver.cpp:545] Iteration 19000, Testing net (#0)
I0628 19:22:16.325639 32080 data_reader.cpp:262] Starting prefetch of epoch 19
I0628 19:22:16.345877 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9176
I0628 19:22:16.345890 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:22:16.345894 32082 solver.cpp:630]     Test net output #2: loss = 0.297331 (* 1 = 0.297331 loss)
I0628 19:22:16.345908 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02316s
I0628 19:22:16.363210 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.4
I0628 19:22:16.836954 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:22:16.838429 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:22:16.838789 32082 solver.cpp:349] Iteration 19000 (31.0318 iter/s, 3.2225s/100 iter), loss = 0.00127647
I0628 19:22:16.838809 32082 solver.cpp:371]     Train net output #0: loss = 0.00127646 (* 1 = 0.00127646 loss)
I0628 19:22:16.838817 32082 sgd_solver.cpp:137] Iteration 19000, lr = 0.00703125, m = 0.9
I0628 19:22:18.559056 32082 solver.cpp:349] Iteration 19100 (58.1479 iter/s, 1.71975s/100 iter), loss = 0.00159837
I0628 19:22:18.559077 32082 solver.cpp:371]     Train net output #0: loss = 0.00159836 (* 1 = 0.00159836 loss)
I0628 19:22:18.559083 32082 sgd_solver.cpp:137] Iteration 19100, lr = 0.00701563, m = 0.9
I0628 19:22:20.285709 32082 solver.cpp:349] Iteration 19200 (57.9327 iter/s, 1.72614s/100 iter), loss = 0.000955108
I0628 19:22:20.285732 32082 solver.cpp:371]     Train net output #0: loss = 0.000955104 (* 1 = 0.000955104 loss)
I0628 19:22:20.285737 32082 sgd_solver.cpp:137] Iteration 19200, lr = 0.007, m = 0.9
I0628 19:22:22.006700 32082 solver.cpp:349] Iteration 19300 (58.1232 iter/s, 1.72048s/100 iter), loss = 0.0011965
I0628 19:22:22.006722 32082 solver.cpp:371]     Train net output #0: loss = 0.00119649 (* 1 = 0.00119649 loss)
I0628 19:22:22.006726 32082 sgd_solver.cpp:137] Iteration 19300, lr = 0.00698437, m = 0.9
I0628 19:22:23.729660 32082 solver.cpp:349] Iteration 19400 (58.0567 iter/s, 1.72245s/100 iter), loss = 0.0013953
I0628 19:22:23.729686 32082 solver.cpp:371]     Train net output #0: loss = 0.0013953 (* 1 = 0.0013953 loss)
I0628 19:22:23.729691 32082 sgd_solver.cpp:137] Iteration 19400, lr = 0.00696875, m = 0.9
I0628 19:22:25.451745 32082 solver.cpp:349] Iteration 19500 (58.0864 iter/s, 1.72157s/100 iter), loss = 0.00127724
I0628 19:22:25.451771 32082 solver.cpp:371]     Train net output #0: loss = 0.00127723 (* 1 = 0.00127723 loss)
I0628 19:22:25.451776 32082 sgd_solver.cpp:137] Iteration 19500, lr = 0.00695312, m = 0.9
I0628 19:22:25.933691 32050 data_reader.cpp:262] Starting prefetch of epoch 25
I0628 19:22:27.172125 32082 solver.cpp:349] Iteration 19600 (58.1439 iter/s, 1.71987s/100 iter), loss = 0.00126146
I0628 19:22:27.172150 32082 solver.cpp:371]     Train net output #0: loss = 0.00126146 (* 1 = 0.00126146 loss)
I0628 19:22:27.172155 32082 sgd_solver.cpp:137] Iteration 19600, lr = 0.0069375, m = 0.9
I0628 19:22:28.893153 32082 solver.cpp:349] Iteration 19700 (58.122 iter/s, 1.72052s/100 iter), loss = 0.00206663
I0628 19:22:28.893230 32082 solver.cpp:371]     Train net output #0: loss = 0.00206662 (* 1 = 0.00206662 loss)
I0628 19:22:28.893235 32082 sgd_solver.cpp:137] Iteration 19700, lr = 0.00692187, m = 0.9
I0628 19:22:30.612263 32082 solver.cpp:349] Iteration 19800 (58.1885 iter/s, 1.71855s/100 iter), loss = 0.00336939
I0628 19:22:30.612289 32082 solver.cpp:371]     Train net output #0: loss = 0.00336938 (* 1 = 0.00336938 loss)
I0628 19:22:30.612296 32082 sgd_solver.cpp:137] Iteration 19800, lr = 0.00690625, m = 0.9
I0628 19:22:32.335132 32082 solver.cpp:349] Iteration 19900 (58.0599 iter/s, 1.72236s/100 iter), loss = 0.00151099
I0628 19:22:32.335155 32082 solver.cpp:371]     Train net output #0: loss = 0.00151099 (* 1 = 0.00151099 loss)
I0628 19:22:32.335160 32082 sgd_solver.cpp:137] Iteration 19900, lr = 0.00689062, m = 0.9
I0628 19:22:34.043210 32082 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_20000.caffemodel
I0628 19:22:34.051419 32082 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_20000.solverstate
I0628 19:22:34.054909 32082 solver.cpp:401] Sparsity after update:
I0628 19:22:34.055934 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:22:34.055941 32082 net.cpp:2170] conv1a_param_0(0.2) 
I0628 19:22:34.055948 32082 net.cpp:2170] conv1b_param_0(0.4) 
I0628 19:22:34.055950 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:22:34.055953 32082 net.cpp:2170] res2a_branch2a_param_0(0.4) 
I0628 19:22:34.055955 32082 net.cpp:2170] res2a_branch2b_param_0(0.4) 
I0628 19:22:34.055958 32082 net.cpp:2170] res3a_branch2a_param_0(0.4) 
I0628 19:22:34.055959 32082 net.cpp:2170] res3a_branch2b_param_0(0.4) 
I0628 19:22:34.055961 32082 net.cpp:2170] res4a_branch2a_param_0(0.4) 
I0628 19:22:34.055963 32082 net.cpp:2170] res4a_branch2b_param_0(0.4) 
I0628 19:22:34.055965 32082 net.cpp:2170] res5a_branch2a_param_0(0.4) 
I0628 19:22:34.055968 32082 net.cpp:2170] res5a_branch2b_param_0(0.4) 
I0628 19:22:34.055970 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (941426/2.3599e+06) 0.399
I0628 19:22:34.055979 32082 solver.cpp:545] Iteration 20000, Testing net (#0)
I0628 19:22:35.055912 32080 data_reader.cpp:262] Starting prefetch of epoch 20
I0628 19:22:35.076084 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9182
I0628 19:22:35.076095 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:22:35.076099 32082 solver.cpp:630]     Test net output #2: loss = 0.298101 (* 1 = 0.298101 loss)
I0628 19:22:35.076113 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01986s
I0628 19:22:35.093415 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.42
I0628 19:22:35.579188 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:22:35.580651 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:22:35.581009 32082 solver.cpp:349] Iteration 20000 (30.8169 iter/s, 3.24497s/100 iter), loss = 0.00138409
I0628 19:22:35.581028 32082 solver.cpp:371]     Train net output #0: loss = 0.00138408 (* 1 = 0.00138408 loss)
I0628 19:22:35.581033 32082 sgd_solver.cpp:137] Iteration 20000, lr = 0.006875, m = 0.9
I0628 19:22:37.302196 32082 solver.cpp:349] Iteration 20100 (58.1162 iter/s, 1.72069s/100 iter), loss = 0.00123437
I0628 19:22:37.302222 32082 solver.cpp:371]     Train net output #0: loss = 0.00123437 (* 1 = 0.00123437 loss)
I0628 19:22:37.302228 32082 sgd_solver.cpp:137] Iteration 20100, lr = 0.00685938, m = 0.9
I0628 19:22:39.025413 32082 solver.cpp:349] Iteration 20200 (58.0481 iter/s, 1.72271s/100 iter), loss = 0.00327714
I0628 19:22:39.025435 32082 solver.cpp:371]     Train net output #0: loss = 0.00327714 (* 1 = 0.00327714 loss)
I0628 19:22:39.025441 32082 sgd_solver.cpp:137] Iteration 20200, lr = 0.00684375, m = 0.9
I0628 19:22:40.745877 32082 solver.cpp:349] Iteration 20300 (58.1408 iter/s, 1.71996s/100 iter), loss = 0.00313952
I0628 19:22:40.745916 32082 solver.cpp:371]     Train net output #0: loss = 0.00313952 (* 1 = 0.00313952 loss)
I0628 19:22:40.745923 32082 sgd_solver.cpp:137] Iteration 20300, lr = 0.00682813, m = 0.9
I0628 19:22:40.900382 32050 data_reader.cpp:262] Starting prefetch of epoch 26
I0628 19:22:42.471127 32082 solver.cpp:349] Iteration 20400 (57.98 iter/s, 1.72473s/100 iter), loss = 0.00106967
I0628 19:22:42.471151 32082 solver.cpp:371]     Train net output #0: loss = 0.00106966 (* 1 = 0.00106966 loss)
I0628 19:22:42.471155 32082 sgd_solver.cpp:137] Iteration 20400, lr = 0.0068125, m = 0.9
I0628 19:22:44.193503 32082 solver.cpp:349] Iteration 20500 (58.0761 iter/s, 1.72188s/100 iter), loss = 0.00188611
I0628 19:22:44.193531 32082 solver.cpp:371]     Train net output #0: loss = 0.00188611 (* 1 = 0.00188611 loss)
I0628 19:22:44.193536 32082 sgd_solver.cpp:137] Iteration 20500, lr = 0.00679688, m = 0.9
I0628 19:22:45.910614 32082 solver.cpp:349] Iteration 20600 (58.2544 iter/s, 1.71661s/100 iter), loss = 0.00155148
I0628 19:22:45.910640 32082 solver.cpp:371]     Train net output #0: loss = 0.00155148 (* 1 = 0.00155148 loss)
I0628 19:22:45.910645 32082 sgd_solver.cpp:137] Iteration 20600, lr = 0.00678125, m = 0.9
I0628 19:22:47.631467 32082 solver.cpp:349] Iteration 20700 (58.1276 iter/s, 1.72035s/100 iter), loss = 0.00201201
I0628 19:22:47.631494 32082 solver.cpp:371]     Train net output #0: loss = 0.00201201 (* 1 = 0.00201201 loss)
I0628 19:22:47.631500 32082 sgd_solver.cpp:137] Iteration 20700, lr = 0.00676562, m = 0.9
I0628 19:22:49.353922 32082 solver.cpp:349] Iteration 20800 (58.0736 iter/s, 1.72195s/100 iter), loss = 0.00121136
I0628 19:22:49.353948 32082 solver.cpp:371]     Train net output #0: loss = 0.00121136 (* 1 = 0.00121136 loss)
I0628 19:22:49.353955 32082 sgd_solver.cpp:137] Iteration 20800, lr = 0.00675, m = 0.9
I0628 19:22:51.075182 32082 solver.cpp:349] Iteration 20900 (58.1138 iter/s, 1.72076s/100 iter), loss = 0.00136441
I0628 19:22:51.075206 32082 solver.cpp:371]     Train net output #0: loss = 0.00136441 (* 1 = 0.00136441 loss)
I0628 19:22:51.075212 32082 sgd_solver.cpp:137] Iteration 20900, lr = 0.00673437, m = 0.9
I0628 19:22:52.779145 32082 solver.cpp:401] Sparsity after update:
I0628 19:22:52.780202 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:22:52.780210 32082 net.cpp:2170] conv1a_param_0(0.21) 
I0628 19:22:52.780216 32082 net.cpp:2170] conv1b_param_0(0.42) 
I0628 19:22:52.780218 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:22:52.780220 32082 net.cpp:2170] res2a_branch2a_param_0(0.42) 
I0628 19:22:52.780223 32082 net.cpp:2170] res2a_branch2b_param_0(0.42) 
I0628 19:22:52.780225 32082 net.cpp:2170] res3a_branch2a_param_0(0.42) 
I0628 19:22:52.780227 32082 net.cpp:2170] res3a_branch2b_param_0(0.42) 
I0628 19:22:52.780230 32082 net.cpp:2170] res4a_branch2a_param_0(0.42) 
I0628 19:22:52.780231 32082 net.cpp:2170] res4a_branch2b_param_0(0.42) 
I0628 19:22:52.780233 32082 net.cpp:2170] res5a_branch2a_param_0(0.42) 
I0628 19:22:52.780236 32082 net.cpp:2170] res5a_branch2b_param_0(0.42) 
I0628 19:22:52.780238 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (988492/2.3599e+06) 0.419
I0628 19:22:52.780246 32082 solver.cpp:545] Iteration 21000, Testing net (#0)
I0628 19:22:53.780596 32080 data_reader.cpp:262] Starting prefetch of epoch 21
I0628 19:22:53.801076 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9184
I0628 19:22:53.801090 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9956
I0628 19:22:53.801095 32082 solver.cpp:630]     Test net output #2: loss = 0.299408 (* 1 = 0.299408 loss)
I0628 19:22:53.801108 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02059s
I0628 19:22:53.818778 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.44
I0628 19:22:54.331900 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:22:54.333371 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:22:54.333734 32082 solver.cpp:349] Iteration 21000 (30.6969 iter/s, 3.25766s/100 iter), loss = 0.00173958
I0628 19:22:54.333762 32082 solver.cpp:371]     Train net output #0: loss = 0.00173958 (* 1 = 0.00173958 loss)
I0628 19:22:54.333767 32082 sgd_solver.cpp:137] Iteration 21000, lr = 0.00671875, m = 0.9
I0628 19:22:55.884397 32050 data_reader.cpp:262] Starting prefetch of epoch 27
I0628 19:22:56.056288 32082 solver.cpp:349] Iteration 21100 (58.07 iter/s, 1.72206s/100 iter), loss = 0.00127419
I0628 19:22:56.056311 32082 solver.cpp:371]     Train net output #0: loss = 0.00127419 (* 1 = 0.00127419 loss)
I0628 19:22:56.056315 32082 sgd_solver.cpp:137] Iteration 21100, lr = 0.00670313, m = 0.9
I0628 19:22:57.776768 32082 solver.cpp:349] Iteration 21200 (58.1398 iter/s, 1.71999s/100 iter), loss = 0.00314684
I0628 19:22:57.776789 32082 solver.cpp:371]     Train net output #0: loss = 0.00314684 (* 1 = 0.00314684 loss)
I0628 19:22:57.776794 32082 sgd_solver.cpp:137] Iteration 21200, lr = 0.0066875, m = 0.9
I0628 19:22:59.493788 32082 solver.cpp:349] Iteration 21300 (58.2569 iter/s, 1.71653s/100 iter), loss = 0.0014739
I0628 19:22:59.493862 32082 solver.cpp:371]     Train net output #0: loss = 0.0014739 (* 1 = 0.0014739 loss)
I0628 19:22:59.493868 32082 sgd_solver.cpp:137] Iteration 21300, lr = 0.00667187, m = 0.9
I0628 19:23:01.216753 32082 solver.cpp:349] Iteration 21400 (58.0577 iter/s, 1.72242s/100 iter), loss = 0.00122436
I0628 19:23:01.216778 32082 solver.cpp:371]     Train net output #0: loss = 0.00122436 (* 1 = 0.00122436 loss)
I0628 19:23:01.216784 32082 sgd_solver.cpp:137] Iteration 21400, lr = 0.00665625, m = 0.9
I0628 19:23:02.935565 32082 solver.cpp:349] Iteration 21500 (58.1964 iter/s, 1.71832s/100 iter), loss = 0.00208552
I0628 19:23:02.935591 32082 solver.cpp:371]     Train net output #0: loss = 0.00208552 (* 1 = 0.00208552 loss)
I0628 19:23:02.935596 32082 sgd_solver.cpp:137] Iteration 21500, lr = 0.00664062, m = 0.9
I0628 19:23:04.653000 32082 solver.cpp:349] Iteration 21600 (58.243 iter/s, 1.71694s/100 iter), loss = 0.00211512
I0628 19:23:04.653028 32082 solver.cpp:371]     Train net output #0: loss = 0.00211512 (* 1 = 0.00211512 loss)
I0628 19:23:04.653033 32082 sgd_solver.cpp:137] Iteration 21600, lr = 0.006625, m = 0.9
I0628 19:23:06.371117 32082 solver.cpp:349] Iteration 21700 (58.2199 iter/s, 1.71763s/100 iter), loss = 0.000734824
I0628 19:23:06.371141 32082 solver.cpp:371]     Train net output #0: loss = 0.00073482 (* 1 = 0.00073482 loss)
I0628 19:23:06.371147 32082 sgd_solver.cpp:137] Iteration 21700, lr = 0.00660937, m = 0.9
I0628 19:23:08.089682 32082 solver.cpp:349] Iteration 21800 (58.2046 iter/s, 1.71808s/100 iter), loss = 0.00353842
I0628 19:23:08.089709 32082 solver.cpp:371]     Train net output #0: loss = 0.00353842 (* 1 = 0.00353842 loss)
I0628 19:23:08.089715 32082 sgd_solver.cpp:137] Iteration 21800, lr = 0.00659375, m = 0.9
I0628 19:23:09.310729 32050 data_reader.cpp:262] Starting prefetch of epoch 28
I0628 19:23:09.813076 32082 solver.cpp:349] Iteration 21900 (58.0415 iter/s, 1.7229s/100 iter), loss = 0.00208937
I0628 19:23:09.813098 32082 solver.cpp:371]     Train net output #0: loss = 0.00208937 (* 1 = 0.00208937 loss)
I0628 19:23:09.813102 32082 sgd_solver.cpp:137] Iteration 21900, lr = 0.00657812, m = 0.9
I0628 19:23:11.513779 32082 solver.cpp:401] Sparsity after update:
I0628 19:23:11.514871 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:23:11.514879 32082 net.cpp:2170] conv1a_param_0(0.22) 
I0628 19:23:11.514885 32082 net.cpp:2170] conv1b_param_0(0.44) 
I0628 19:23:11.514888 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:23:11.514890 32082 net.cpp:2170] res2a_branch2a_param_0(0.44) 
I0628 19:23:11.514892 32082 net.cpp:2170] res2a_branch2b_param_0(0.44) 
I0628 19:23:11.514894 32082 net.cpp:2170] res3a_branch2a_param_0(0.44) 
I0628 19:23:11.514896 32082 net.cpp:2170] res3a_branch2b_param_0(0.44) 
I0628 19:23:11.514899 32082 net.cpp:2170] res4a_branch2a_param_0(0.44) 
I0628 19:23:11.514900 32082 net.cpp:2170] res4a_branch2b_param_0(0.44) 
I0628 19:23:11.514902 32082 net.cpp:2170] res5a_branch2a_param_0(0.44) 
I0628 19:23:11.514904 32082 net.cpp:2170] res5a_branch2b_param_0(0.44) 
I0628 19:23:11.514906 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.03556e+06/2.3599e+06) 0.439
I0628 19:23:11.514914 32082 solver.cpp:545] Iteration 22000, Testing net (#0)
I0628 19:23:12.512642 32080 data_reader.cpp:262] Starting prefetch of epoch 22
I0628 19:23:12.535820 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9174
I0628 19:23:12.535833 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:23:12.535840 32082 solver.cpp:630]     Test net output #2: loss = 0.301279 (* 1 = 0.301279 loss)
I0628 19:23:12.535858 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02068s
I0628 19:23:12.553115 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.46
I0628 19:23:13.083266 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:23:13.084722 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:23:13.085078 32082 solver.cpp:349] Iteration 22000 (30.5705 iter/s, 3.27113s/100 iter), loss = 0.00241128
I0628 19:23:13.085103 32082 solver.cpp:371]     Train net output #0: loss = 0.00241128 (* 1 = 0.00241128 loss)
I0628 19:23:13.085108 32082 sgd_solver.cpp:137] Iteration 22000, lr = 0.0065625, m = 0.9
I0628 19:23:14.805814 32082 solver.cpp:349] Iteration 22100 (58.1309 iter/s, 1.72026s/100 iter), loss = 0.00162286
I0628 19:23:14.805835 32082 solver.cpp:371]     Train net output #0: loss = 0.00162286 (* 1 = 0.00162286 loss)
I0628 19:23:14.805840 32082 sgd_solver.cpp:137] Iteration 22100, lr = 0.00654687, m = 0.9
I0628 19:23:16.526540 32082 solver.cpp:349] Iteration 22200 (58.1311 iter/s, 1.72025s/100 iter), loss = 0.00185903
I0628 19:23:16.526561 32082 solver.cpp:371]     Train net output #0: loss = 0.00185903 (* 1 = 0.00185903 loss)
I0628 19:23:16.526564 32082 sgd_solver.cpp:137] Iteration 22200, lr = 0.00653125, m = 0.9
I0628 19:23:18.252411 32082 solver.cpp:349] Iteration 22300 (57.9578 iter/s, 1.72539s/100 iter), loss = 0.00149192
I0628 19:23:18.252434 32082 solver.cpp:371]     Train net output #0: loss = 0.00149192 (* 1 = 0.00149192 loss)
I0628 19:23:18.252439 32082 sgd_solver.cpp:137] Iteration 22300, lr = 0.00651562, m = 0.9
I0628 19:23:19.969911 32082 solver.cpp:349] Iteration 22400 (58.2403 iter/s, 1.71702s/100 iter), loss = 0.0029787
I0628 19:23:19.969934 32082 solver.cpp:371]     Train net output #0: loss = 0.00297869 (* 1 = 0.00297869 loss)
I0628 19:23:19.969940 32082 sgd_solver.cpp:137] Iteration 22400, lr = 0.0065, m = 0.9
I0628 19:23:21.694207 32082 solver.cpp:349] Iteration 22500 (58.0109 iter/s, 1.72381s/100 iter), loss = 0.00134489
I0628 19:23:21.694232 32082 solver.cpp:371]     Train net output #0: loss = 0.00134488 (* 1 = 0.00134488 loss)
I0628 19:23:21.694238 32082 sgd_solver.cpp:137] Iteration 22500, lr = 0.00648437, m = 0.9
I0628 19:23:23.417632 32082 solver.cpp:349] Iteration 22600 (58.0403 iter/s, 1.72294s/100 iter), loss = 0.00138944
I0628 19:23:23.417659 32082 solver.cpp:371]     Train net output #0: loss = 0.00138943 (* 1 = 0.00138943 loss)
I0628 19:23:23.417665 32082 sgd_solver.cpp:137] Iteration 22600, lr = 0.00646875, m = 0.9
I0628 19:23:24.327414 32050 data_reader.cpp:262] Starting prefetch of epoch 29
I0628 19:23:25.139910 32082 solver.cpp:349] Iteration 22700 (58.0789 iter/s, 1.72179s/100 iter), loss = 0.00114649
I0628 19:23:25.139935 32082 solver.cpp:371]     Train net output #0: loss = 0.00114648 (* 1 = 0.00114648 loss)
I0628 19:23:25.139941 32082 sgd_solver.cpp:137] Iteration 22700, lr = 0.00645312, m = 0.9
I0628 19:23:26.862645 32082 solver.cpp:349] Iteration 22800 (58.0635 iter/s, 1.72225s/100 iter), loss = 0.00198938
I0628 19:23:26.862673 32082 solver.cpp:371]     Train net output #0: loss = 0.00198938 (* 1 = 0.00198938 loss)
I0628 19:23:26.862679 32082 sgd_solver.cpp:137] Iteration 22800, lr = 0.0064375, m = 0.9
I0628 19:23:28.581779 32082 solver.cpp:349] Iteration 22900 (58.1851 iter/s, 1.71865s/100 iter), loss = 0.00340618
I0628 19:23:28.581802 32082 solver.cpp:371]     Train net output #0: loss = 0.00340617 (* 1 = 0.00340617 loss)
I0628 19:23:28.581807 32082 sgd_solver.cpp:137] Iteration 22900, lr = 0.00642187, m = 0.9
I0628 19:23:30.285939 32082 solver.cpp:401] Sparsity after update:
I0628 19:23:30.287101 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:23:30.287108 32082 net.cpp:2170] conv1a_param_0(0.23) 
I0628 19:23:30.287118 32082 net.cpp:2170] conv1b_param_0(0.46) 
I0628 19:23:30.287123 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:23:30.287127 32082 net.cpp:2170] res2a_branch2a_param_0(0.46) 
I0628 19:23:30.287132 32082 net.cpp:2170] res2a_branch2b_param_0(0.46) 
I0628 19:23:30.287134 32082 net.cpp:2170] res3a_branch2a_param_0(0.46) 
I0628 19:23:30.287138 32082 net.cpp:2170] res3a_branch2b_param_0(0.46) 
I0628 19:23:30.287143 32082 net.cpp:2170] res4a_branch2a_param_0(0.46) 
I0628 19:23:30.287147 32082 net.cpp:2170] res4a_branch2b_param_0(0.46) 
I0628 19:23:30.287150 32082 net.cpp:2170] res5a_branch2a_param_0(0.46) 
I0628 19:23:30.287154 32082 net.cpp:2170] res5a_branch2b_param_0(0.46) 
I0628 19:23:30.287158 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.08264e+06/2.3599e+06) 0.459
I0628 19:23:30.287168 32082 solver.cpp:545] Iteration 23000, Testing net (#0)
I0628 19:23:31.284610 32080 data_reader.cpp:262] Starting prefetch of epoch 23
I0628 19:23:31.307639 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9186
I0628 19:23:31.307652 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:23:31.307659 32082 solver.cpp:630]     Test net output #2: loss = 0.302122 (* 1 = 0.302122 loss)
I0628 19:23:31.307675 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02015s
I0628 19:23:31.324957 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.48
I0628 19:23:31.873148 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:23:31.874608 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:23:31.874966 32082 solver.cpp:349] Iteration 23000 (30.3764 iter/s, 3.29203s/100 iter), loss = 0.00149413
I0628 19:23:31.874985 32082 solver.cpp:371]     Train net output #0: loss = 0.00149412 (* 1 = 0.00149412 loss)
I0628 19:23:31.874994 32082 sgd_solver.cpp:137] Iteration 23000, lr = 0.00640625, m = 0.9
I0628 19:23:33.594794 32082 solver.cpp:349] Iteration 23100 (58.1671 iter/s, 1.71918s/100 iter), loss = 0.0021158
I0628 19:23:33.594822 32082 solver.cpp:371]     Train net output #0: loss = 0.00211579 (* 1 = 0.00211579 loss)
I0628 19:23:33.594828 32082 sgd_solver.cpp:137] Iteration 23100, lr = 0.00639063, m = 0.9
I0628 19:23:35.316073 32082 solver.cpp:349] Iteration 23200 (58.1182 iter/s, 1.72063s/100 iter), loss = 0.00132608
I0628 19:23:35.316102 32082 solver.cpp:371]     Train net output #0: loss = 0.00132607 (* 1 = 0.00132607 loss)
I0628 19:23:35.316107 32082 sgd_solver.cpp:137] Iteration 23200, lr = 0.006375, m = 0.9
I0628 19:23:37.045815 32082 solver.cpp:349] Iteration 23300 (57.8338 iter/s, 1.72909s/100 iter), loss = 0.0022326
I0628 19:23:37.045837 32082 solver.cpp:371]     Train net output #0: loss = 0.0022326 (* 1 = 0.0022326 loss)
I0628 19:23:37.045842 32082 sgd_solver.cpp:137] Iteration 23300, lr = 0.00635938, m = 0.9
I0628 19:23:38.769493 32082 solver.cpp:349] Iteration 23400 (58.0369 iter/s, 1.72304s/100 iter), loss = 0.00292
I0628 19:23:38.769516 32082 solver.cpp:371]     Train net output #0: loss = 0.00292 (* 1 = 0.00292 loss)
I0628 19:23:38.769521 32082 sgd_solver.cpp:137] Iteration 23400, lr = 0.00634375, m = 0.9
I0628 19:23:39.355033 32050 data_reader.cpp:262] Starting prefetch of epoch 30
I0628 19:23:40.489640 32082 solver.cpp:349] Iteration 23500 (58.1561 iter/s, 1.71951s/100 iter), loss = 0.00406069
I0628 19:23:40.489666 32082 solver.cpp:371]     Train net output #0: loss = 0.00406069 (* 1 = 0.00406069 loss)
I0628 19:23:40.489672 32082 sgd_solver.cpp:137] Iteration 23500, lr = 0.00632813, m = 0.9
I0628 19:23:42.211535 32082 solver.cpp:349] Iteration 23600 (58.0971 iter/s, 1.72126s/100 iter), loss = 0.00136618
I0628 19:23:42.211557 32082 solver.cpp:371]     Train net output #0: loss = 0.00136618 (* 1 = 0.00136618 loss)
I0628 19:23:42.211561 32082 sgd_solver.cpp:137] Iteration 23600, lr = 0.0063125, m = 0.9
I0628 19:23:43.932713 32082 solver.cpp:349] Iteration 23700 (58.121 iter/s, 1.72055s/100 iter), loss = 0.00280409
I0628 19:23:43.932749 32082 solver.cpp:371]     Train net output #0: loss = 0.00280409 (* 1 = 0.00280409 loss)
I0628 19:23:43.932752 32082 sgd_solver.cpp:137] Iteration 23700, lr = 0.00629687, m = 0.9
I0628 19:23:45.653357 32082 solver.cpp:349] Iteration 23800 (58.1394 iter/s, 1.72s/100 iter), loss = 0.00110968
I0628 19:23:45.653378 32082 solver.cpp:371]     Train net output #0: loss = 0.00110967 (* 1 = 0.00110967 loss)
I0628 19:23:45.653383 32082 sgd_solver.cpp:137] Iteration 23800, lr = 0.00628125, m = 0.9
I0628 19:23:47.376703 32082 solver.cpp:349] Iteration 23900 (58.0477 iter/s, 1.72272s/100 iter), loss = 0.00161635
I0628 19:23:47.376725 32082 solver.cpp:371]     Train net output #0: loss = 0.00161635 (* 1 = 0.00161635 loss)
I0628 19:23:47.376729 32082 sgd_solver.cpp:137] Iteration 23900, lr = 0.00626562, m = 0.9
I0628 19:23:49.081579 32082 solver.cpp:401] Sparsity after update:
I0628 19:23:49.082677 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:23:49.082686 32082 net.cpp:2170] conv1a_param_0(0.24) 
I0628 19:23:49.082692 32082 net.cpp:2170] conv1b_param_0(0.48) 
I0628 19:23:49.082695 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:23:49.082700 32082 net.cpp:2170] res2a_branch2a_param_0(0.48) 
I0628 19:23:49.082705 32082 net.cpp:2170] res2a_branch2b_param_0(0.48) 
I0628 19:23:49.082708 32082 net.cpp:2170] res3a_branch2a_param_0(0.48) 
I0628 19:23:49.082713 32082 net.cpp:2170] res3a_branch2b_param_0(0.48) 
I0628 19:23:49.082717 32082 net.cpp:2170] res4a_branch2a_param_0(0.48) 
I0628 19:23:49.082721 32082 net.cpp:2170] res4a_branch2b_param_0(0.48) 
I0628 19:23:49.082726 32082 net.cpp:2170] res5a_branch2a_param_0(0.48) 
I0628 19:23:49.082729 32082 net.cpp:2170] res5a_branch2b_param_0(0.48) 
I0628 19:23:49.082733 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.12971e+06/2.3599e+06) 0.479
I0628 19:23:49.082746 32082 solver.cpp:545] Iteration 24000, Testing net (#0)
I0628 19:23:50.080083 32080 data_reader.cpp:262] Starting prefetch of epoch 24
I0628 19:23:50.101380 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.917
I0628 19:23:50.101393 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:23:50.101398 32082 solver.cpp:630]     Test net output #2: loss = 0.301958 (* 1 = 0.301958 loss)
I0628 19:23:50.101411 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01832s
I0628 19:23:50.121851 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.5
I0628 19:23:50.693084 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:23:50.694553 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:23:50.694911 32082 solver.cpp:349] Iteration 24000 (30.1473 iter/s, 3.31704s/100 iter), loss = 0.00116391
I0628 19:23:50.694928 32082 solver.cpp:371]     Train net output #0: loss = 0.00116391 (* 1 = 0.00116391 loss)
I0628 19:23:50.694934 32082 sgd_solver.cpp:137] Iteration 24000, lr = 0.00625, m = 0.9
I0628 19:23:52.418093 32082 solver.cpp:349] Iteration 24100 (58.053 iter/s, 1.72256s/100 iter), loss = 0.00326255
I0628 19:23:52.418115 32082 solver.cpp:371]     Train net output #0: loss = 0.00326255 (* 1 = 0.00326255 loss)
I0628 19:23:52.418119 32082 sgd_solver.cpp:137] Iteration 24100, lr = 0.00623438, m = 0.9
I0628 19:23:54.136739 32082 solver.cpp:349] Iteration 24200 (58.2063 iter/s, 1.71803s/100 iter), loss = 0.0046329
I0628 19:23:54.136764 32082 solver.cpp:371]     Train net output #0: loss = 0.0046329 (* 1 = 0.0046329 loss)
I0628 19:23:54.136770 32082 sgd_solver.cpp:137] Iteration 24200, lr = 0.00621875, m = 0.9
I0628 19:23:54.394824 32050 data_reader.cpp:262] Starting prefetch of epoch 31
I0628 19:23:55.857755 32082 solver.cpp:349] Iteration 24300 (58.1262 iter/s, 1.72039s/100 iter), loss = 0.00367827
I0628 19:23:55.857775 32082 solver.cpp:371]     Train net output #0: loss = 0.00367826 (* 1 = 0.00367826 loss)
I0628 19:23:55.857779 32082 sgd_solver.cpp:137] Iteration 24300, lr = 0.00620312, m = 0.9
I0628 19:23:57.581557 32082 solver.cpp:349] Iteration 24400 (58.032 iter/s, 1.72319s/100 iter), loss = 0.00171259
I0628 19:23:57.581593 32082 solver.cpp:371]     Train net output #0: loss = 0.00171259 (* 1 = 0.00171259 loss)
I0628 19:23:57.581598 32082 sgd_solver.cpp:137] Iteration 24400, lr = 0.0061875, m = 0.9
I0628 19:23:59.306432 32082 solver.cpp:349] Iteration 24500 (57.9965 iter/s, 1.72424s/100 iter), loss = 0.00254918
I0628 19:23:59.306457 32082 solver.cpp:371]     Train net output #0: loss = 0.00254918 (* 1 = 0.00254918 loss)
I0628 19:23:59.306463 32082 sgd_solver.cpp:137] Iteration 24500, lr = 0.00617187, m = 0.9
I0628 19:24:01.030421 32082 solver.cpp:349] Iteration 24600 (58.0259 iter/s, 1.72337s/100 iter), loss = 0.00365565
I0628 19:24:01.030486 32082 solver.cpp:371]     Train net output #0: loss = 0.00365565 (* 1 = 0.00365565 loss)
I0628 19:24:01.030491 32082 sgd_solver.cpp:137] Iteration 24600, lr = 0.00615625, m = 0.9
I0628 19:24:02.759022 32082 solver.cpp:349] Iteration 24700 (57.8724 iter/s, 1.72794s/100 iter), loss = 0.00223862
I0628 19:24:02.759084 32082 solver.cpp:371]     Train net output #0: loss = 0.00223861 (* 1 = 0.00223861 loss)
I0628 19:24:02.759099 32082 sgd_solver.cpp:137] Iteration 24700, lr = 0.00614062, m = 0.9
I0628 19:24:04.495790 32082 solver.cpp:349] Iteration 24800 (57.6006 iter/s, 1.73609s/100 iter), loss = 0.00418949
I0628 19:24:04.495815 32082 solver.cpp:371]     Train net output #0: loss = 0.00418949 (* 1 = 0.00418949 loss)
I0628 19:24:04.495821 32082 sgd_solver.cpp:137] Iteration 24800, lr = 0.006125, m = 0.9
I0628 19:24:06.214879 32082 solver.cpp:349] Iteration 24900 (58.1912 iter/s, 1.71847s/100 iter), loss = 0.000711182
I0628 19:24:06.214905 32082 solver.cpp:371]     Train net output #0: loss = 0.00071118 (* 1 = 0.00071118 loss)
I0628 19:24:06.214910 32082 sgd_solver.cpp:137] Iteration 24900, lr = 0.00610937, m = 0.9
I0628 19:24:07.866806 32050 data_reader.cpp:262] Starting prefetch of epoch 32
I0628 19:24:07.918223 32082 solver.cpp:401] Sparsity after update:
I0628 19:24:07.919277 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:24:07.919283 32082 net.cpp:2170] conv1a_param_0(0.25) 
I0628 19:24:07.919291 32082 net.cpp:2170] conv1b_param_0(0.5) 
I0628 19:24:07.919293 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:24:07.919296 32082 net.cpp:2170] res2a_branch2a_param_0(0.5) 
I0628 19:24:07.919298 32082 net.cpp:2170] res2a_branch2b_param_0(0.5) 
I0628 19:24:07.919301 32082 net.cpp:2170] res3a_branch2a_param_0(0.5) 
I0628 19:24:07.919303 32082 net.cpp:2170] res3a_branch2b_param_0(0.5) 
I0628 19:24:07.919306 32082 net.cpp:2170] res4a_branch2a_param_0(0.5) 
I0628 19:24:07.919307 32082 net.cpp:2170] res4a_branch2b_param_0(0.5) 
I0628 19:24:07.919309 32082 net.cpp:2170] res5a_branch2a_param_0(0.5) 
I0628 19:24:07.919312 32082 net.cpp:2170] res5a_branch2b_param_0(0.5) 
I0628 19:24:07.919313 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.17678e+06/2.3599e+06) 0.499
I0628 19:24:07.919322 32082 solver.cpp:545] Iteration 25000, Testing net (#0)
I0628 19:24:08.915524 32080 data_reader.cpp:262] Starting prefetch of epoch 25
I0628 19:24:08.941742 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9178
I0628 19:24:08.941771 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9956
I0628 19:24:08.941779 32082 solver.cpp:630]     Test net output #2: loss = 0.30503 (* 1 = 0.30503 loss)
I0628 19:24:08.941800 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02214s
I0628 19:24:08.959055 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.52
I0628 19:24:09.560417 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:24:09.561887 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:24:09.562248 32082 solver.cpp:349] Iteration 25000 (29.8845 iter/s, 3.34622s/100 iter), loss = 0.00126559
I0628 19:24:09.562268 32082 solver.cpp:371]     Train net output #0: loss = 0.00126558 (* 1 = 0.00126558 loss)
I0628 19:24:09.562273 32082 sgd_solver.cpp:137] Iteration 25000, lr = 0.00609375, m = 0.9
I0628 19:24:11.282735 32082 solver.cpp:349] Iteration 25100 (58.1435 iter/s, 1.71988s/100 iter), loss = 0.00135276
I0628 19:24:11.282760 32082 solver.cpp:371]     Train net output #0: loss = 0.00135276 (* 1 = 0.00135276 loss)
I0628 19:24:11.282766 32082 sgd_solver.cpp:137] Iteration 25100, lr = 0.00607812, m = 0.9
I0628 19:24:13.004425 32082 solver.cpp:349] Iteration 25200 (58.1031 iter/s, 1.72108s/100 iter), loss = 0.00335391
I0628 19:24:13.004446 32082 solver.cpp:371]     Train net output #0: loss = 0.00335391 (* 1 = 0.00335391 loss)
I0628 19:24:13.004449 32082 sgd_solver.cpp:137] Iteration 25200, lr = 0.0060625, m = 0.9
I0628 19:24:14.722285 32082 solver.cpp:349] Iteration 25300 (58.2323 iter/s, 1.71726s/100 iter), loss = 0.00304155
I0628 19:24:14.722327 32082 solver.cpp:371]     Train net output #0: loss = 0.00304155 (* 1 = 0.00304155 loss)
I0628 19:24:14.722333 32082 sgd_solver.cpp:137] Iteration 25300, lr = 0.00604687, m = 0.9
I0628 19:24:16.443972 32082 solver.cpp:349] Iteration 25400 (58.1036 iter/s, 1.72106s/100 iter), loss = 0.00229869
I0628 19:24:16.443995 32082 solver.cpp:371]     Train net output #0: loss = 0.00229869 (* 1 = 0.00229869 loss)
I0628 19:24:16.444000 32082 sgd_solver.cpp:137] Iteration 25400, lr = 0.00603125, m = 0.9
I0628 19:24:18.170461 32082 solver.cpp:349] Iteration 25500 (57.9412 iter/s, 1.72589s/100 iter), loss = 0.00128942
I0628 19:24:18.170487 32082 solver.cpp:371]     Train net output #0: loss = 0.00128942 (* 1 = 0.00128942 loss)
I0628 19:24:18.170493 32082 sgd_solver.cpp:137] Iteration 25500, lr = 0.00601562, m = 0.9
I0628 19:24:19.891433 32082 solver.cpp:349] Iteration 25600 (58.1271 iter/s, 1.72037s/100 iter), loss = 0.00202967
I0628 19:24:19.891460 32082 solver.cpp:371]     Train net output #0: loss = 0.00202967 (* 1 = 0.00202967 loss)
I0628 19:24:19.891466 32082 sgd_solver.cpp:137] Iteration 25600, lr = 0.006, m = 0.9
I0628 19:24:21.609705 32082 solver.cpp:349] Iteration 25700 (58.2185 iter/s, 1.71767s/100 iter), loss = 0.00251289
I0628 19:24:21.609732 32082 solver.cpp:371]     Train net output #0: loss = 0.00251289 (* 1 = 0.00251289 loss)
I0628 19:24:21.609738 32082 sgd_solver.cpp:137] Iteration 25700, lr = 0.00598437, m = 0.9
I0628 19:24:22.956204 32050 data_reader.cpp:262] Starting prefetch of epoch 33
I0628 19:24:23.333916 32082 solver.cpp:349] Iteration 25800 (58.0179 iter/s, 1.72361s/100 iter), loss = 0.00166926
I0628 19:24:23.333942 32082 solver.cpp:371]     Train net output #0: loss = 0.00166926 (* 1 = 0.00166926 loss)
I0628 19:24:23.333948 32082 sgd_solver.cpp:137] Iteration 25800, lr = 0.00596875, m = 0.9
I0628 19:24:25.054853 32082 solver.cpp:349] Iteration 25900 (58.1281 iter/s, 1.72034s/100 iter), loss = 0.00302531
I0628 19:24:25.054873 32082 solver.cpp:371]     Train net output #0: loss = 0.0030253 (* 1 = 0.0030253 loss)
I0628 19:24:25.054877 32082 sgd_solver.cpp:137] Iteration 25900, lr = 0.00595312, m = 0.9
I0628 19:24:26.758683 32082 solver.cpp:401] Sparsity after update:
I0628 19:24:26.759793 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:24:26.759800 32082 net.cpp:2170] conv1a_param_0(0.26) 
I0628 19:24:26.759807 32082 net.cpp:2170] conv1b_param_0(0.52) 
I0628 19:24:26.759809 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:24:26.759812 32082 net.cpp:2170] res2a_branch2a_param_0(0.52) 
I0628 19:24:26.759814 32082 net.cpp:2170] res2a_branch2b_param_0(0.52) 
I0628 19:24:26.759816 32082 net.cpp:2170] res3a_branch2a_param_0(0.52) 
I0628 19:24:26.759819 32082 net.cpp:2170] res3a_branch2b_param_0(0.52) 
I0628 19:24:26.759821 32082 net.cpp:2170] res4a_branch2a_param_0(0.52) 
I0628 19:24:26.759824 32082 net.cpp:2170] res4a_branch2b_param_0(0.52) 
I0628 19:24:26.759827 32082 net.cpp:2170] res5a_branch2a_param_0(0.52) 
I0628 19:24:26.759830 32082 net.cpp:2170] res5a_branch2b_param_0(0.52) 
I0628 19:24:26.759834 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.22386e+06/2.3599e+06) 0.519
I0628 19:24:26.759850 32082 solver.cpp:545] Iteration 26000, Testing net (#0)
I0628 19:24:27.756052 32080 data_reader.cpp:262] Starting prefetch of epoch 26
I0628 19:24:27.778436 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9172
I0628 19:24:27.778457 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9954
I0628 19:24:27.778465 32082 solver.cpp:630]     Test net output #2: loss = 0.304759 (* 1 = 0.304759 loss)
I0628 19:24:27.778484 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.0183s
I0628 19:24:27.796644 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.54
I0628 19:24:28.409518 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:24:28.410989 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:24:28.411347 32082 solver.cpp:349] Iteration 26000 (29.8029 iter/s, 3.35538s/100 iter), loss = 0.00356309
I0628 19:24:28.411375 32082 solver.cpp:371]     Train net output #0: loss = 0.00356309 (* 1 = 0.00356309 loss)
I0628 19:24:28.411383 32082 sgd_solver.cpp:137] Iteration 26000, lr = 0.0059375, m = 0.9
I0628 19:24:30.132568 32082 solver.cpp:349] Iteration 26100 (58.1186 iter/s, 1.72062s/100 iter), loss = 0.0077015
I0628 19:24:30.132596 32082 solver.cpp:371]     Train net output #0: loss = 0.0077015 (* 1 = 0.0077015 loss)
I0628 19:24:30.132601 32082 sgd_solver.cpp:137] Iteration 26100, lr = 0.00592188, m = 0.9
I0628 19:24:31.860738 32082 solver.cpp:349] Iteration 26200 (57.8848 iter/s, 1.72757s/100 iter), loss = 0.00370453
I0628 19:24:31.860815 32082 solver.cpp:371]     Train net output #0: loss = 0.00370453 (* 1 = 0.00370453 loss)
I0628 19:24:31.860822 32082 sgd_solver.cpp:137] Iteration 26200, lr = 0.00590625, m = 0.9
I0628 19:24:33.582290 32082 solver.cpp:349] Iteration 26300 (58.109 iter/s, 1.7209s/100 iter), loss = 0.00333424
I0628 19:24:33.582316 32082 solver.cpp:371]     Train net output #0: loss = 0.00333424 (* 1 = 0.00333424 loss)
I0628 19:24:33.582322 32082 sgd_solver.cpp:137] Iteration 26300, lr = 0.00589063, m = 0.9
I0628 19:24:35.307708 32082 solver.cpp:349] Iteration 26400 (57.977 iter/s, 1.72482s/100 iter), loss = 0.0023658
I0628 19:24:35.307734 32082 solver.cpp:371]     Train net output #0: loss = 0.0023658 (* 1 = 0.0023658 loss)
I0628 19:24:35.307739 32082 sgd_solver.cpp:137] Iteration 26400, lr = 0.005875, m = 0.9
I0628 19:24:37.027324 32082 solver.cpp:349] Iteration 26500 (58.1725 iter/s, 1.71903s/100 iter), loss = 0.00298276
I0628 19:24:37.027349 32082 solver.cpp:371]     Train net output #0: loss = 0.00298275 (* 1 = 0.00298275 loss)
I0628 19:24:37.027355 32082 sgd_solver.cpp:137] Iteration 26500, lr = 0.00585938, m = 0.9
I0628 19:24:38.041836 32050 data_reader.cpp:262] Starting prefetch of epoch 34
I0628 19:24:38.744817 32082 solver.cpp:349] Iteration 26600 (58.2444 iter/s, 1.7169s/100 iter), loss = 0.00327829
I0628 19:24:38.744843 32082 solver.cpp:371]     Train net output #0: loss = 0.00327829 (* 1 = 0.00327829 loss)
I0628 19:24:38.744849 32082 sgd_solver.cpp:137] Iteration 26600, lr = 0.00584375, m = 0.9
I0628 19:24:40.465169 32082 solver.cpp:349] Iteration 26700 (58.1475 iter/s, 1.71976s/100 iter), loss = 0.00235766
I0628 19:24:40.465194 32082 solver.cpp:371]     Train net output #0: loss = 0.00235765 (* 1 = 0.00235765 loss)
I0628 19:24:40.465200 32082 sgd_solver.cpp:137] Iteration 26700, lr = 0.00582812, m = 0.9
I0628 19:24:42.188380 32082 solver.cpp:349] Iteration 26800 (58.051 iter/s, 1.72262s/100 iter), loss = 0.00331774
I0628 19:24:42.188405 32082 solver.cpp:371]     Train net output #0: loss = 0.00331773 (* 1 = 0.00331773 loss)
I0628 19:24:42.188411 32082 sgd_solver.cpp:137] Iteration 26800, lr = 0.0058125, m = 0.9
I0628 19:24:43.908401 32082 solver.cpp:349] Iteration 26900 (58.1586 iter/s, 1.71944s/100 iter), loss = 0.00156465
I0628 19:24:43.908427 32082 solver.cpp:371]     Train net output #0: loss = 0.00156464 (* 1 = 0.00156464 loss)
I0628 19:24:43.908433 32082 sgd_solver.cpp:137] Iteration 26900, lr = 0.00579687, m = 0.9
I0628 19:24:45.609601 32082 solver.cpp:401] Sparsity after update:
I0628 19:24:45.610652 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:24:45.610659 32082 net.cpp:2170] conv1a_param_0(0.27) 
I0628 19:24:45.610666 32082 net.cpp:2170] conv1b_param_0(0.54) 
I0628 19:24:45.610667 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:24:45.610669 32082 net.cpp:2170] res2a_branch2a_param_0(0.54) 
I0628 19:24:45.610671 32082 net.cpp:2170] res2a_branch2b_param_0(0.54) 
I0628 19:24:45.610676 32082 net.cpp:2170] res3a_branch2a_param_0(0.54) 
I0628 19:24:45.610677 32082 net.cpp:2170] res3a_branch2b_param_0(0.54) 
I0628 19:24:45.610678 32082 net.cpp:2170] res4a_branch2a_param_0(0.54) 
I0628 19:24:45.610680 32082 net.cpp:2170] res4a_branch2b_param_0(0.54) 
I0628 19:24:45.610682 32082 net.cpp:2170] res5a_branch2a_param_0(0.54) 
I0628 19:24:45.610684 32082 net.cpp:2170] res5a_branch2b_param_0(0.54) 
I0628 19:24:45.610687 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.27093e+06/2.3599e+06) 0.539
I0628 19:24:45.610693 32082 solver.cpp:545] Iteration 27000, Testing net (#0)
I0628 19:24:46.606994 32080 data_reader.cpp:262] Starting prefetch of epoch 27
I0628 19:24:46.629393 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.915
I0628 19:24:46.629406 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9954
I0628 19:24:46.629411 32082 solver.cpp:630]     Test net output #2: loss = 0.306885 (* 1 = 0.306885 loss)
I0628 19:24:46.629425 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01841s
I0628 19:24:46.646632 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.56
I0628 19:24:47.283013 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:24:47.284489 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:24:47.284848 32082 solver.cpp:349] Iteration 27000 (29.6266 iter/s, 3.37535s/100 iter), loss = 0.00466532
I0628 19:24:47.284865 32082 solver.cpp:371]     Train net output #0: loss = 0.00466531 (* 1 = 0.00466531 loss)
I0628 19:24:47.284871 32082 sgd_solver.cpp:137] Iteration 27000, lr = 0.00578125, m = 0.9
I0628 19:24:49.011363 32082 solver.cpp:349] Iteration 27100 (57.9394 iter/s, 1.72594s/100 iter), loss = 0.0018782
I0628 19:24:49.011389 32082 solver.cpp:371]     Train net output #0: loss = 0.00187819 (* 1 = 0.00187819 loss)
I0628 19:24:49.011395 32082 sgd_solver.cpp:137] Iteration 27100, lr = 0.00576563, m = 0.9
I0628 19:24:50.729307 32082 solver.cpp:349] Iteration 27200 (58.2287 iter/s, 1.71737s/100 iter), loss = 0.00198111
I0628 19:24:50.729329 32082 solver.cpp:371]     Train net output #0: loss = 0.00198111 (* 1 = 0.00198111 loss)
I0628 19:24:50.729336 32082 sgd_solver.cpp:137] Iteration 27200, lr = 0.00575, m = 0.9
I0628 19:24:52.452019 32082 solver.cpp:349] Iteration 27300 (58.0674 iter/s, 1.72214s/100 iter), loss = 0.00366353
I0628 19:24:52.452046 32082 solver.cpp:371]     Train net output #0: loss = 0.00366352 (* 1 = 0.00366352 loss)
I0628 19:24:52.452052 32082 sgd_solver.cpp:137] Iteration 27300, lr = 0.00573438, m = 0.9
I0628 19:24:53.138563 32050 data_reader.cpp:262] Starting prefetch of epoch 35
I0628 19:24:54.171617 32082 solver.cpp:349] Iteration 27400 (58.1728 iter/s, 1.71902s/100 iter), loss = 0.00373996
I0628 19:24:54.171643 32082 solver.cpp:371]     Train net output #0: loss = 0.00373995 (* 1 = 0.00373995 loss)
I0628 19:24:54.171648 32082 sgd_solver.cpp:137] Iteration 27400, lr = 0.00571875, m = 0.9
I0628 19:24:55.890153 32082 solver.cpp:349] Iteration 27500 (58.2085 iter/s, 1.71796s/100 iter), loss = 0.00387776
I0628 19:24:55.890174 32082 solver.cpp:371]     Train net output #0: loss = 0.00387776 (* 1 = 0.00387776 loss)
I0628 19:24:55.890178 32082 sgd_solver.cpp:137] Iteration 27500, lr = 0.00570312, m = 0.9
I0628 19:24:57.609365 32082 solver.cpp:349] Iteration 27600 (58.1853 iter/s, 1.71865s/100 iter), loss = 0.00659175
I0628 19:24:57.609390 32082 solver.cpp:371]     Train net output #0: loss = 0.00659174 (* 1 = 0.00659174 loss)
I0628 19:24:57.609395 32082 sgd_solver.cpp:137] Iteration 27600, lr = 0.0056875, m = 0.9
I0628 19:24:59.330711 32082 solver.cpp:349] Iteration 27700 (58.1134 iter/s, 1.72077s/100 iter), loss = 0.00105971
I0628 19:24:59.330737 32082 solver.cpp:371]     Train net output #0: loss = 0.0010597 (* 1 = 0.0010597 loss)
I0628 19:24:59.330744 32082 sgd_solver.cpp:137] Iteration 27700, lr = 0.00567187, m = 0.9
I0628 19:25:01.051568 32082 solver.cpp:349] Iteration 27800 (58.13 iter/s, 1.72028s/100 iter), loss = 0.00382418
I0628 19:25:01.051589 32082 solver.cpp:371]     Train net output #0: loss = 0.00382417 (* 1 = 0.00382417 loss)
I0628 19:25:01.051594 32082 sgd_solver.cpp:137] Iteration 27800, lr = 0.00565625, m = 0.9
I0628 19:25:02.771811 32082 solver.cpp:349] Iteration 27900 (58.1505 iter/s, 1.71968s/100 iter), loss = 0.00423872
I0628 19:25:02.771880 32082 solver.cpp:371]     Train net output #0: loss = 0.00423872 (* 1 = 0.00423872 loss)
I0628 19:25:02.771888 32082 sgd_solver.cpp:137] Iteration 27900, lr = 0.00564062, m = 0.9
I0628 19:25:04.472621 32082 solver.cpp:401] Sparsity after update:
I0628 19:25:04.473685 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:25:04.473695 32082 net.cpp:2170] conv1a_param_0(0.28) 
I0628 19:25:04.473701 32082 net.cpp:2170] conv1b_param_0(0.56) 
I0628 19:25:04.473703 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:25:04.473706 32082 net.cpp:2170] res2a_branch2a_param_0(0.56) 
I0628 19:25:04.473707 32082 net.cpp:2170] res2a_branch2b_param_0(0.56) 
I0628 19:25:04.473709 32082 net.cpp:2170] res3a_branch2a_param_0(0.56) 
I0628 19:25:04.473711 32082 net.cpp:2170] res3a_branch2b_param_0(0.56) 
I0628 19:25:04.473714 32082 net.cpp:2170] res4a_branch2a_param_0(0.56) 
I0628 19:25:04.473716 32082 net.cpp:2170] res4a_branch2b_param_0(0.56) 
I0628 19:25:04.473718 32082 net.cpp:2170] res5a_branch2a_param_0(0.56) 
I0628 19:25:04.473721 32082 net.cpp:2170] res5a_branch2b_param_0(0.56) 
I0628 19:25:04.473723 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.318e+06/2.3599e+06) 0.558
I0628 19:25:04.473731 32082 solver.cpp:545] Iteration 28000, Testing net (#0)
I0628 19:25:05.471539 32080 data_reader.cpp:262] Starting prefetch of epoch 28
I0628 19:25:05.493854 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.915
I0628 19:25:05.493870 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:25:05.493875 32082 solver.cpp:630]     Test net output #2: loss = 0.308403 (* 1 = 0.308403 loss)
I0628 19:25:05.493890 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01984s
I0628 19:25:05.511186 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.58
I0628 19:25:06.183776 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:25:06.185250 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:25:06.185614 32082 solver.cpp:349] Iteration 28000 (29.3026 iter/s, 3.41267s/100 iter), loss = 0.00758583
I0628 19:25:06.185632 32082 solver.cpp:371]     Train net output #0: loss = 0.00758583 (* 1 = 0.00758583 loss)
I0628 19:25:06.185637 32082 sgd_solver.cpp:137] Iteration 28000, lr = 0.005625, m = 0.9
I0628 19:25:07.913631 32082 solver.cpp:349] Iteration 28100 (57.8886 iter/s, 1.72746s/100 iter), loss = 0.0050168
I0628 19:25:07.913652 32082 solver.cpp:371]     Train net output #0: loss = 0.00501679 (* 1 = 0.00501679 loss)
I0628 19:25:07.913657 32082 sgd_solver.cpp:137] Iteration 28100, lr = 0.00560937, m = 0.9
I0628 19:25:08.274459 32050 data_reader.cpp:262] Starting prefetch of epoch 36
I0628 19:25:09.633776 32082 solver.cpp:349] Iteration 28200 (58.1535 iter/s, 1.71959s/100 iter), loss = 0.00274666
I0628 19:25:09.633802 32082 solver.cpp:371]     Train net output #0: loss = 0.00274665 (* 1 = 0.00274665 loss)
I0628 19:25:09.633808 32082 sgd_solver.cpp:137] Iteration 28200, lr = 0.00559375, m = 0.9
I0628 19:25:11.352881 32082 solver.cpp:349] Iteration 28300 (58.189 iter/s, 1.71854s/100 iter), loss = 0.00279583
I0628 19:25:11.352908 32082 solver.cpp:371]     Train net output #0: loss = 0.00279582 (* 1 = 0.00279582 loss)
I0628 19:25:11.352915 32082 sgd_solver.cpp:137] Iteration 28300, lr = 0.00557812, m = 0.9
I0628 19:25:13.071796 32082 solver.cpp:349] Iteration 28400 (58.1954 iter/s, 1.71835s/100 iter), loss = 0.00220348
I0628 19:25:13.071821 32082 solver.cpp:371]     Train net output #0: loss = 0.00220347 (* 1 = 0.00220347 loss)
I0628 19:25:13.071828 32082 sgd_solver.cpp:137] Iteration 28400, lr = 0.0055625, m = 0.9
I0628 19:25:14.794862 32082 solver.cpp:349] Iteration 28500 (58.0551 iter/s, 1.7225s/100 iter), loss = 0.00616774
I0628 19:25:14.794885 32082 solver.cpp:371]     Train net output #0: loss = 0.00616773 (* 1 = 0.00616773 loss)
I0628 19:25:14.794889 32082 sgd_solver.cpp:137] Iteration 28500, lr = 0.00554687, m = 0.9
I0628 19:25:16.518028 32082 solver.cpp:349] Iteration 28600 (58.0515 iter/s, 1.72261s/100 iter), loss = 0.00408518
I0628 19:25:16.518071 32082 solver.cpp:371]     Train net output #0: loss = 0.00408517 (* 1 = 0.00408517 loss)
I0628 19:25:16.518077 32082 sgd_solver.cpp:137] Iteration 28600, lr = 0.00553125, m = 0.9
I0628 19:25:18.238307 32082 solver.cpp:349] Iteration 28700 (58.1497 iter/s, 1.7197s/100 iter), loss = 0.00249629
I0628 19:25:18.238333 32082 solver.cpp:371]     Train net output #0: loss = 0.00249628 (* 1 = 0.00249628 loss)
I0628 19:25:18.238339 32082 sgd_solver.cpp:137] Iteration 28700, lr = 0.00551562, m = 0.9
I0628 19:25:19.956948 32082 solver.cpp:349] Iteration 28800 (58.2045 iter/s, 1.71808s/100 iter), loss = 0.0042915
I0628 19:25:19.956970 32082 solver.cpp:371]     Train net output #0: loss = 0.00429149 (* 1 = 0.00429149 loss)
I0628 19:25:19.956976 32082 sgd_solver.cpp:137] Iteration 28800, lr = 0.0055, m = 0.9
I0628 19:25:21.679132 32082 solver.cpp:349] Iteration 28900 (58.0846 iter/s, 1.72163s/100 iter), loss = 0.00242194
I0628 19:25:21.679158 32082 solver.cpp:371]     Train net output #0: loss = 0.00242193 (* 1 = 0.00242193 loss)
I0628 19:25:21.679164 32082 sgd_solver.cpp:137] Iteration 28900, lr = 0.00548437, m = 0.9
I0628 19:25:21.731163 32050 data_reader.cpp:262] Starting prefetch of epoch 37
I0628 19:25:23.383343 32082 solver.cpp:401] Sparsity after update:
I0628 19:25:23.384413 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:25:23.384421 32082 net.cpp:2170] conv1a_param_0(0.29) 
I0628 19:25:23.384428 32082 net.cpp:2170] conv1b_param_0(0.58) 
I0628 19:25:23.384430 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:25:23.384433 32082 net.cpp:2170] res2a_branch2a_param_0(0.58) 
I0628 19:25:23.384435 32082 net.cpp:2170] res2a_branch2b_param_0(0.58) 
I0628 19:25:23.384438 32082 net.cpp:2170] res3a_branch2a_param_0(0.58) 
I0628 19:25:23.384439 32082 net.cpp:2170] res3a_branch2b_param_0(0.58) 
I0628 19:25:23.384441 32082 net.cpp:2170] res4a_branch2a_param_0(0.58) 
I0628 19:25:23.384443 32082 net.cpp:2170] res4a_branch2b_param_0(0.58) 
I0628 19:25:23.384445 32082 net.cpp:2170] res5a_branch2a_param_0(0.58) 
I0628 19:25:23.384447 32082 net.cpp:2170] res5a_branch2b_param_0(0.58) 
I0628 19:25:23.384449 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.36507e+06/2.3599e+06) 0.578
I0628 19:25:23.384457 32082 solver.cpp:545] Iteration 29000, Testing net (#0)
I0628 19:25:24.386255 32080 data_reader.cpp:262] Starting prefetch of epoch 29
I0628 19:25:24.406369 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9138
I0628 19:25:24.406383 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:25:24.406386 32082 solver.cpp:630]     Test net output #2: loss = 0.308685 (* 1 = 0.308685 loss)
I0628 19:25:24.406400 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02164s
I0628 19:25:24.423640 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.6
I0628 19:25:25.122592 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:25:25.124065 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:25:25.124423 32082 solver.cpp:349] Iteration 29000 (29.0341 iter/s, 3.44422s/100 iter), loss = 0.00597963
I0628 19:25:25.124440 32082 solver.cpp:371]     Train net output #0: loss = 0.00597962 (* 1 = 0.00597962 loss)
I0628 19:25:25.124445 32082 sgd_solver.cpp:137] Iteration 29000, lr = 0.00546875, m = 0.9
I0628 19:25:26.844480 32082 solver.cpp:349] Iteration 29100 (58.1561 iter/s, 1.71951s/100 iter), loss = 0.00287913
I0628 19:25:26.844506 32082 solver.cpp:371]     Train net output #0: loss = 0.00287913 (* 1 = 0.00287913 loss)
I0628 19:25:26.844512 32082 sgd_solver.cpp:137] Iteration 29100, lr = 0.00545313, m = 0.9
I0628 19:25:28.565696 32082 solver.cpp:349] Iteration 29200 (58.1172 iter/s, 1.72066s/100 iter), loss = 0.00285409
I0628 19:25:28.565723 32082 solver.cpp:371]     Train net output #0: loss = 0.00285408 (* 1 = 0.00285408 loss)
I0628 19:25:28.565728 32082 sgd_solver.cpp:137] Iteration 29200, lr = 0.0054375, m = 0.9
I0628 19:25:30.283984 32082 solver.cpp:349] Iteration 29300 (58.2162 iter/s, 1.71773s/100 iter), loss = 0.00719778
I0628 19:25:30.284024 32082 solver.cpp:371]     Train net output #0: loss = 0.00719777 (* 1 = 0.00719777 loss)
I0628 19:25:30.284031 32082 sgd_solver.cpp:137] Iteration 29300, lr = 0.00542188, m = 0.9
I0628 19:25:32.001339 32082 solver.cpp:349] Iteration 29400 (58.2483 iter/s, 1.71679s/100 iter), loss = 0.00347338
I0628 19:25:32.001365 32082 solver.cpp:371]     Train net output #0: loss = 0.00347338 (* 1 = 0.00347338 loss)
I0628 19:25:32.001371 32082 sgd_solver.cpp:137] Iteration 29400, lr = 0.00540625, m = 0.9
I0628 19:25:33.729456 32082 solver.cpp:349] Iteration 29500 (57.885 iter/s, 1.72756s/100 iter), loss = 0.00272149
I0628 19:25:33.729542 32082 solver.cpp:371]     Train net output #0: loss = 0.00272149 (* 1 = 0.00272149 loss)
I0628 19:25:33.729554 32082 sgd_solver.cpp:137] Iteration 29500, lr = 0.00539062, m = 0.9
I0628 19:25:35.449051 32082 solver.cpp:349] Iteration 29600 (58.1742 iter/s, 1.71898s/100 iter), loss = 0.00742239
I0628 19:25:35.449076 32082 solver.cpp:371]     Train net output #0: loss = 0.00742239 (* 1 = 0.00742239 loss)
I0628 19:25:35.449082 32082 sgd_solver.cpp:137] Iteration 29600, lr = 0.005375, m = 0.9
I0628 19:25:36.896714 32050 data_reader.cpp:262] Starting prefetch of epoch 38
I0628 19:25:37.176542 32082 solver.cpp:349] Iteration 29700 (57.9059 iter/s, 1.72694s/100 iter), loss = 0.00369038
I0628 19:25:37.176570 32082 solver.cpp:371]     Train net output #0: loss = 0.00369037 (* 1 = 0.00369037 loss)
I0628 19:25:37.176578 32082 sgd_solver.cpp:137] Iteration 29700, lr = 0.00535937, m = 0.9
I0628 19:25:38.902904 32082 solver.cpp:349] Iteration 29800 (57.9439 iter/s, 1.72581s/100 iter), loss = 0.00276335
I0628 19:25:38.902930 32082 solver.cpp:371]     Train net output #0: loss = 0.00276334 (* 1 = 0.00276334 loss)
I0628 19:25:38.902935 32082 sgd_solver.cpp:137] Iteration 29800, lr = 0.00534375, m = 0.9
I0628 19:25:40.622119 32082 solver.cpp:349] Iteration 29900 (58.1845 iter/s, 1.71867s/100 iter), loss = 0.00225052
I0628 19:25:40.622143 32082 solver.cpp:371]     Train net output #0: loss = 0.00225051 (* 1 = 0.00225051 loss)
I0628 19:25:40.622146 32082 sgd_solver.cpp:137] Iteration 29900, lr = 0.00532812, m = 0.9
I0628 19:25:42.330240 32082 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_30000.caffemodel
I0628 19:25:42.338052 32082 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_30000.solverstate
I0628 19:25:42.341480 32082 solver.cpp:401] Sparsity after update:
I0628 19:25:42.342700 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:25:42.342707 32082 net.cpp:2170] conv1a_param_0(0.3) 
I0628 19:25:42.342715 32082 net.cpp:2170] conv1b_param_0(0.6) 
I0628 19:25:42.342716 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:25:42.342718 32082 net.cpp:2170] res2a_branch2a_param_0(0.6) 
I0628 19:25:42.342720 32082 net.cpp:2170] res2a_branch2b_param_0(0.6) 
I0628 19:25:42.342722 32082 net.cpp:2170] res3a_branch2a_param_0(0.6) 
I0628 19:25:42.342725 32082 net.cpp:2170] res3a_branch2b_param_0(0.6) 
I0628 19:25:42.342726 32082 net.cpp:2170] res4a_branch2a_param_0(0.6) 
I0628 19:25:42.342728 32082 net.cpp:2170] res4a_branch2b_param_0(0.6) 
I0628 19:25:42.342730 32082 net.cpp:2170] res5a_branch2a_param_0(0.6) 
I0628 19:25:42.342731 32082 net.cpp:2170] res5a_branch2b_param_0(0.6) 
I0628 19:25:42.342733 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.41214e+06/2.3599e+06) 0.598
I0628 19:25:42.342741 32082 solver.cpp:545] Iteration 30000, Testing net (#0)
I0628 19:25:43.339903 32080 data_reader.cpp:262] Starting prefetch of epoch 30
I0628 19:25:43.359998 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9146
I0628 19:25:43.360010 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.996
I0628 19:25:43.360015 32082 solver.cpp:630]     Test net output #2: loss = 0.311607 (* 1 = 0.311607 loss)
I0628 19:25:43.360029 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01699s
I0628 19:25:43.377415 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.62
I0628 19:25:44.105124 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:25:44.106595 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:25:44.106950 32082 solver.cpp:349] Iteration 30000 (28.7044 iter/s, 3.48378s/100 iter), loss = 0.00882763
I0628 19:25:44.106968 32082 solver.cpp:371]     Train net output #0: loss = 0.00882763 (* 1 = 0.00882763 loss)
I0628 19:25:44.106973 32082 sgd_solver.cpp:137] Iteration 30000, lr = 0.0053125, m = 0.9
I0628 19:25:45.827836 32082 solver.cpp:349] Iteration 30100 (58.1276 iter/s, 1.72035s/100 iter), loss = 0.0119758
I0628 19:25:45.827878 32082 solver.cpp:371]     Train net output #0: loss = 0.0119758 (* 1 = 0.0119758 loss)
I0628 19:25:45.827885 32082 sgd_solver.cpp:137] Iteration 30100, lr = 0.00529688, m = 0.9
I0628 19:25:47.549576 32082 solver.cpp:349] Iteration 30200 (58.0997 iter/s, 1.72118s/100 iter), loss = 0.00440972
I0628 19:25:47.549598 32082 solver.cpp:371]     Train net output #0: loss = 0.00440972 (* 1 = 0.00440972 loss)
I0628 19:25:47.549602 32082 sgd_solver.cpp:137] Iteration 30200, lr = 0.00528125, m = 0.9
I0628 19:25:49.271919 32082 solver.cpp:349] Iteration 30300 (58.0785 iter/s, 1.72181s/100 iter), loss = 0.00678877
I0628 19:25:49.271946 32082 solver.cpp:371]     Train net output #0: loss = 0.00678877 (* 1 = 0.00678877 loss)
I0628 19:25:49.271952 32082 sgd_solver.cpp:137] Iteration 30300, lr = 0.00526563, m = 0.9
I0628 19:25:50.991188 32082 solver.cpp:349] Iteration 30400 (58.1827 iter/s, 1.71872s/100 iter), loss = 0.00486152
I0628 19:25:50.991214 32082 solver.cpp:371]     Train net output #0: loss = 0.00486151 (* 1 = 0.00486151 loss)
I0628 19:25:50.991219 32082 sgd_solver.cpp:137] Iteration 30400, lr = 0.00525, m = 0.9
I0628 19:25:52.112284 32050 data_reader.cpp:262] Starting prefetch of epoch 39
I0628 19:25:52.713943 32082 solver.cpp:349] Iteration 30500 (58.0648 iter/s, 1.72221s/100 iter), loss = 0.00344304
I0628 19:25:52.713968 32082 solver.cpp:371]     Train net output #0: loss = 0.00344304 (* 1 = 0.00344304 loss)
I0628 19:25:52.713974 32082 sgd_solver.cpp:137] Iteration 30500, lr = 0.00523437, m = 0.9
I0628 19:25:54.435698 32082 solver.cpp:349] Iteration 30600 (58.0984 iter/s, 1.72122s/100 iter), loss = 0.0039297
I0628 19:25:54.435725 32082 solver.cpp:371]     Train net output #0: loss = 0.0039297 (* 1 = 0.0039297 loss)
I0628 19:25:54.435731 32082 sgd_solver.cpp:137] Iteration 30600, lr = 0.00521875, m = 0.9
I0628 19:25:56.160006 32082 solver.cpp:349] Iteration 30700 (58.0125 iter/s, 1.72377s/100 iter), loss = 0.002698
I0628 19:25:56.160032 32082 solver.cpp:371]     Train net output #0: loss = 0.002698 (* 1 = 0.002698 loss)
I0628 19:25:56.160037 32082 sgd_solver.cpp:137] Iteration 30700, lr = 0.00520312, m = 0.9
I0628 19:25:57.883898 32082 solver.cpp:349] Iteration 30800 (58.0264 iter/s, 1.72335s/100 iter), loss = 0.00174224
I0628 19:25:57.883924 32082 solver.cpp:371]     Train net output #0: loss = 0.00174223 (* 1 = 0.00174223 loss)
I0628 19:25:57.883930 32082 sgd_solver.cpp:137] Iteration 30800, lr = 0.0051875, m = 0.9
I0628 19:25:59.608002 32082 solver.cpp:349] Iteration 30900 (58.0192 iter/s, 1.72357s/100 iter), loss = 0.0039794
I0628 19:25:59.608026 32082 solver.cpp:371]     Train net output #0: loss = 0.0039794 (* 1 = 0.0039794 loss)
I0628 19:25:59.608029 32082 sgd_solver.cpp:137] Iteration 30900, lr = 0.00517187, m = 0.9
I0628 19:26:01.311663 32082 solver.cpp:401] Sparsity after update:
I0628 19:26:01.312723 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:26:01.312731 32082 net.cpp:2170] conv1a_param_0(0.31) 
I0628 19:26:01.312739 32082 net.cpp:2170] conv1b_param_0(0.62) 
I0628 19:26:01.312741 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:26:01.312744 32082 net.cpp:2170] res2a_branch2a_param_0(0.62) 
I0628 19:26:01.312747 32082 net.cpp:2170] res2a_branch2b_param_0(0.62) 
I0628 19:26:01.312748 32082 net.cpp:2170] res3a_branch2a_param_0(0.62) 
I0628 19:26:01.312750 32082 net.cpp:2170] res3a_branch2b_param_0(0.62) 
I0628 19:26:01.312752 32082 net.cpp:2170] res4a_branch2a_param_0(0.62) 
I0628 19:26:01.312754 32082 net.cpp:2170] res4a_branch2b_param_0(0.62) 
I0628 19:26:01.312757 32082 net.cpp:2170] res5a_branch2a_param_0(0.62) 
I0628 19:26:01.312758 32082 net.cpp:2170] res5a_branch2b_param_0(0.62) 
I0628 19:26:01.312760 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.45921e+06/2.3599e+06) 0.618
I0628 19:26:01.312768 32082 solver.cpp:545] Iteration 31000, Testing net (#0)
I0628 19:26:02.310564 32080 data_reader.cpp:262] Starting prefetch of epoch 31
I0628 19:26:02.332324 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9126
I0628 19:26:02.332347 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9962
I0628 19:26:02.332352 32082 solver.cpp:630]     Test net output #2: loss = 0.316316 (* 1 = 0.316316 loss)
I0628 19:26:02.332366 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.0193s
I0628 19:26:02.349519 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.64
I0628 19:26:03.105046 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:26:03.106521 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:26:03.106880 32082 solver.cpp:349] Iteration 31000 (28.589 iter/s, 3.49785s/100 iter), loss = 0.00982035
I0628 19:26:03.106899 32082 solver.cpp:371]     Train net output #0: loss = 0.00982035 (* 1 = 0.00982035 loss)
I0628 19:26:03.106904 32082 sgd_solver.cpp:137] Iteration 31000, lr = 0.00515625, m = 0.9
I0628 19:26:04.825475 32082 solver.cpp:349] Iteration 31100 (58.2048 iter/s, 1.71807s/100 iter), loss = 0.002375
I0628 19:26:04.825568 32082 solver.cpp:371]     Train net output #0: loss = 0.002375 (* 1 = 0.002375 loss)
I0628 19:26:04.825577 32082 sgd_solver.cpp:137] Iteration 31100, lr = 0.00514062, m = 0.9
I0628 19:26:06.546826 32082 solver.cpp:349] Iteration 31200 (58.1143 iter/s, 1.72075s/100 iter), loss = 0.00279212
I0628 19:26:06.546849 32082 solver.cpp:371]     Train net output #0: loss = 0.00279212 (* 1 = 0.00279212 loss)
I0628 19:26:06.546852 32082 sgd_solver.cpp:137] Iteration 31200, lr = 0.005125, m = 0.9
I0628 19:26:07.341758 32050 data_reader.cpp:262] Starting prefetch of epoch 40
I0628 19:26:08.269062 32082 solver.cpp:349] Iteration 31300 (58.0818 iter/s, 1.72171s/100 iter), loss = 0.0036589
I0628 19:26:08.269089 32082 solver.cpp:371]     Train net output #0: loss = 0.0036589 (* 1 = 0.0036589 loss)
I0628 19:26:08.269095 32082 sgd_solver.cpp:137] Iteration 31300, lr = 0.00510937, m = 0.9
I0628 19:26:09.990872 32082 solver.cpp:349] Iteration 31400 (58.0963 iter/s, 1.72128s/100 iter), loss = 0.0049309
I0628 19:26:09.990895 32082 solver.cpp:371]     Train net output #0: loss = 0.0049309 (* 1 = 0.0049309 loss)
I0628 19:26:09.990898 32082 sgd_solver.cpp:137] Iteration 31400, lr = 0.00509375, m = 0.9
I0628 19:26:11.709177 32082 solver.cpp:349] Iteration 31500 (58.2146 iter/s, 1.71778s/100 iter), loss = 0.00273789
I0628 19:26:11.709203 32082 solver.cpp:371]     Train net output #0: loss = 0.00273789 (* 1 = 0.00273789 loss)
I0628 19:26:11.709208 32082 sgd_solver.cpp:137] Iteration 31500, lr = 0.00507812, m = 0.9
I0628 19:26:13.426754 32082 solver.cpp:349] Iteration 31600 (58.2394 iter/s, 1.71705s/100 iter), loss = 0.00570961
I0628 19:26:13.426781 32082 solver.cpp:371]     Train net output #0: loss = 0.00570961 (* 1 = 0.00570961 loss)
I0628 19:26:13.426787 32082 sgd_solver.cpp:137] Iteration 31600, lr = 0.0050625, m = 0.9
I0628 19:26:15.153353 32082 solver.cpp:349] Iteration 31700 (57.9351 iter/s, 1.72607s/100 iter), loss = 0.00224975
I0628 19:26:15.153375 32082 solver.cpp:371]     Train net output #0: loss = 0.00224975 (* 1 = 0.00224975 loss)
I0628 19:26:15.153379 32082 sgd_solver.cpp:137] Iteration 31700, lr = 0.00504687, m = 0.9
I0628 19:26:16.873749 32082 solver.cpp:349] Iteration 31800 (58.1437 iter/s, 1.71988s/100 iter), loss = 0.00276529
I0628 19:26:16.873778 32082 solver.cpp:371]     Train net output #0: loss = 0.00276529 (* 1 = 0.00276529 loss)
I0628 19:26:16.873785 32082 sgd_solver.cpp:137] Iteration 31800, lr = 0.00503125, m = 0.9
I0628 19:26:18.595824 32082 solver.cpp:349] Iteration 31900 (58.0873 iter/s, 1.72155s/100 iter), loss = 0.00211174
I0628 19:26:18.595846 32082 solver.cpp:371]     Train net output #0: loss = 0.00211174 (* 1 = 0.00211174 loss)
I0628 19:26:18.595850 32082 sgd_solver.cpp:137] Iteration 31900, lr = 0.00501562, m = 0.9
I0628 19:26:20.299163 32082 solver.cpp:401] Sparsity after update:
I0628 19:26:20.300285 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:26:20.300293 32082 net.cpp:2170] conv1a_param_0(0.32) 
I0628 19:26:20.300300 32082 net.cpp:2170] conv1b_param_0(0.64) 
I0628 19:26:20.300303 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:26:20.300307 32082 net.cpp:2170] res2a_branch2a_param_0(0.64) 
I0628 19:26:20.300312 32082 net.cpp:2170] res2a_branch2b_param_0(0.64) 
I0628 19:26:20.300314 32082 net.cpp:2170] res3a_branch2a_param_0(0.64) 
I0628 19:26:20.300318 32082 net.cpp:2170] res3a_branch2b_param_0(0.64) 
I0628 19:26:20.300321 32082 net.cpp:2170] res4a_branch2a_param_0(0.64) 
I0628 19:26:20.300325 32082 net.cpp:2170] res4a_branch2b_param_0(0.64) 
I0628 19:26:20.300329 32082 net.cpp:2170] res5a_branch2a_param_0(0.64) 
I0628 19:26:20.300333 32082 net.cpp:2170] res5a_branch2b_param_0(0.64) 
I0628 19:26:20.300338 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.50628e+06/2.3599e+06) 0.638
I0628 19:26:20.300348 32082 solver.cpp:545] Iteration 32000, Testing net (#0)
I0628 19:26:21.301542 32080 data_reader.cpp:262] Starting prefetch of epoch 32
I0628 19:26:21.321763 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.911
I0628 19:26:21.321774 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9954
I0628 19:26:21.321789 32082 solver.cpp:630]     Test net output #2: loss = 0.323528 (* 1 = 0.323528 loss)
I0628 19:26:21.321802 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02117s
I0628 19:26:21.338984 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.66
I0628 19:26:22.107825 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:26:22.109290 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:26:22.109648 32082 solver.cpp:349] Iteration 32000 (28.4672 iter/s, 3.51281s/100 iter), loss = 0.00327806
I0628 19:26:22.109665 32082 solver.cpp:371]     Train net output #0: loss = 0.00327806 (* 1 = 0.00327806 loss)
I0628 19:26:22.109670 32082 sgd_solver.cpp:137] Iteration 32000, lr = 0.005, m = 0.9
I0628 19:26:22.592028 32050 data_reader.cpp:262] Starting prefetch of epoch 41
I0628 19:26:23.829520 32082 solver.cpp:349] Iteration 32100 (58.1611 iter/s, 1.71936s/100 iter), loss = 0.0045468
I0628 19:26:23.829542 32082 solver.cpp:371]     Train net output #0: loss = 0.0045468 (* 1 = 0.0045468 loss)
I0628 19:26:23.829550 32082 sgd_solver.cpp:137] Iteration 32100, lr = 0.00498438, m = 0.9
I0628 19:26:25.552453 32082 solver.cpp:349] Iteration 32200 (58.0581 iter/s, 1.72241s/100 iter), loss = 0.0082751
I0628 19:26:25.552479 32082 solver.cpp:371]     Train net output #0: loss = 0.0082751 (* 1 = 0.0082751 loss)
I0628 19:26:25.552485 32082 sgd_solver.cpp:137] Iteration 32200, lr = 0.00496875, m = 0.9
I0628 19:26:27.276376 32082 solver.cpp:349] Iteration 32300 (58.0248 iter/s, 1.7234s/100 iter), loss = 0.0175805
I0628 19:26:27.276402 32082 solver.cpp:371]     Train net output #0: loss = 0.0175805 (* 1 = 0.0175805 loss)
I0628 19:26:27.276408 32082 sgd_solver.cpp:137] Iteration 32300, lr = 0.00495313, m = 0.9
I0628 19:26:28.996325 32082 solver.cpp:349] Iteration 32400 (58.159 iter/s, 1.71943s/100 iter), loss = 0.00649336
I0628 19:26:28.996350 32082 solver.cpp:371]     Train net output #0: loss = 0.00649336 (* 1 = 0.00649336 loss)
I0628 19:26:28.996356 32082 sgd_solver.cpp:137] Iteration 32400, lr = 0.0049375, m = 0.9
I0628 19:26:30.712353 32082 solver.cpp:349] Iteration 32500 (58.2917 iter/s, 1.71551s/100 iter), loss = 0.00696994
I0628 19:26:30.712379 32082 solver.cpp:371]     Train net output #0: loss = 0.00696995 (* 1 = 0.00696995 loss)
I0628 19:26:30.712385 32082 sgd_solver.cpp:137] Iteration 32500, lr = 0.00492187, m = 0.9
I0628 19:26:32.431192 32082 solver.cpp:349] Iteration 32600 (58.1964 iter/s, 1.71832s/100 iter), loss = 0.00337892
I0628 19:26:32.431218 32082 solver.cpp:371]     Train net output #0: loss = 0.00337893 (* 1 = 0.00337893 loss)
I0628 19:26:32.431224 32082 sgd_solver.cpp:137] Iteration 32600, lr = 0.00490625, m = 0.9
I0628 19:26:34.148564 32082 solver.cpp:349] Iteration 32700 (58.2461 iter/s, 1.71685s/100 iter), loss = 0.00794096
I0628 19:26:34.148591 32082 solver.cpp:371]     Train net output #0: loss = 0.00794097 (* 1 = 0.00794097 loss)
I0628 19:26:34.148597 32082 sgd_solver.cpp:137] Iteration 32700, lr = 0.00489062, m = 0.9
I0628 19:26:35.864140 32082 solver.cpp:349] Iteration 32800 (58.307 iter/s, 1.71506s/100 iter), loss = 0.00529499
I0628 19:26:35.864225 32082 solver.cpp:371]     Train net output #0: loss = 0.005295 (* 1 = 0.005295 loss)
I0628 19:26:35.864233 32082 sgd_solver.cpp:137] Iteration 32800, lr = 0.004875, m = 0.9
I0628 19:26:36.019273 32050 data_reader.cpp:262] Starting prefetch of epoch 42
I0628 19:26:37.588546 32082 solver.cpp:349] Iteration 32900 (58.0105 iter/s, 1.72383s/100 iter), loss = 0.00389099
I0628 19:26:37.588569 32082 solver.cpp:371]     Train net output #0: loss = 0.00389099 (* 1 = 0.00389099 loss)
I0628 19:26:37.588573 32082 sgd_solver.cpp:137] Iteration 32900, lr = 0.00485937, m = 0.9
I0628 19:26:39.287981 32082 solver.cpp:401] Sparsity after update:
I0628 19:26:39.289034 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:26:39.289043 32082 net.cpp:2170] conv1a_param_0(0.33) 
I0628 19:26:39.289052 32082 net.cpp:2170] conv1b_param_0(0.66) 
I0628 19:26:39.289057 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:26:39.289060 32082 net.cpp:2170] res2a_branch2a_param_0(0.66) 
I0628 19:26:39.289064 32082 net.cpp:2170] res2a_branch2b_param_0(0.647) 
I0628 19:26:39.289068 32082 net.cpp:2170] res3a_branch2a_param_0(0.66) 
I0628 19:26:39.289072 32082 net.cpp:2170] res3a_branch2b_param_0(0.66) 
I0628 19:26:39.289077 32082 net.cpp:2170] res4a_branch2a_param_0(0.66) 
I0628 19:26:39.289080 32082 net.cpp:2170] res4a_branch2b_param_0(0.66) 
I0628 19:26:39.289083 32082 net.cpp:2170] res5a_branch2a_param_0(0.66) 
I0628 19:26:39.289088 32082 net.cpp:2170] res5a_branch2b_param_0(0.66) 
I0628 19:26:39.289091 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.55324e+06/2.3599e+06) 0.658
I0628 19:26:39.289103 32082 solver.cpp:545] Iteration 33000, Testing net (#0)
I0628 19:26:40.284121 32080 data_reader.cpp:262] Starting prefetch of epoch 33
I0628 19:26:40.307348 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.908
I0628 19:26:40.307364 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9954
I0628 19:26:40.307371 32082 solver.cpp:630]     Test net output #2: loss = 0.329499 (* 1 = 0.329499 loss)
I0628 19:26:40.307389 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.018s
I0628 19:26:40.324646 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.68
I0628 19:26:41.133483 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:26:41.134955 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:26:41.135313 32082 solver.cpp:349] Iteration 33000 (28.2027 iter/s, 3.54576s/100 iter), loss = 0.00446094
I0628 19:26:41.135332 32082 solver.cpp:371]     Train net output #0: loss = 0.00446095 (* 1 = 0.00446095 loss)
I0628 19:26:41.135340 32082 sgd_solver.cpp:137] Iteration 33000, lr = 0.00484375, m = 0.9
I0628 19:26:42.854140 32082 solver.cpp:349] Iteration 33100 (58.1964 iter/s, 1.71832s/100 iter), loss = 0.00299629
I0628 19:26:42.854162 32082 solver.cpp:371]     Train net output #0: loss = 0.0029963 (* 1 = 0.0029963 loss)
I0628 19:26:42.854166 32082 sgd_solver.cpp:137] Iteration 33100, lr = 0.00482813, m = 0.9
I0628 19:26:44.571943 32082 solver.cpp:349] Iteration 33200 (58.231 iter/s, 1.7173s/100 iter), loss = 0.00456729
I0628 19:26:44.571990 32082 solver.cpp:371]     Train net output #0: loss = 0.0045673 (* 1 = 0.0045673 loss)
I0628 19:26:44.571997 32082 sgd_solver.cpp:137] Iteration 33200, lr = 0.0048125, m = 0.9
I0628 19:26:46.293244 32082 solver.cpp:349] Iteration 33300 (58.1137 iter/s, 1.72076s/100 iter), loss = 0.00215493
I0628 19:26:46.293269 32082 solver.cpp:371]     Train net output #0: loss = 0.00215493 (* 1 = 0.00215493 loss)
I0628 19:26:46.293275 32082 sgd_solver.cpp:137] Iteration 33300, lr = 0.00479688, m = 0.9
I0628 19:26:48.011871 32082 solver.cpp:349] Iteration 33400 (58.2032 iter/s, 1.71812s/100 iter), loss = 0.00868004
I0628 19:26:48.011893 32082 solver.cpp:371]     Train net output #0: loss = 0.00868005 (* 1 = 0.00868005 loss)
I0628 19:26:48.011898 32082 sgd_solver.cpp:137] Iteration 33400, lr = 0.00478125, m = 0.9
I0628 19:26:49.732007 32082 solver.cpp:349] Iteration 33500 (58.1519 iter/s, 1.71963s/100 iter), loss = 0.0053904
I0628 19:26:49.732044 32082 solver.cpp:371]     Train net output #0: loss = 0.00539041 (* 1 = 0.00539041 loss)
I0628 19:26:49.732050 32082 sgd_solver.cpp:137] Iteration 33500, lr = 0.00476563, m = 0.9
I0628 19:26:51.276684 32050 data_reader.cpp:262] Starting prefetch of epoch 43
I0628 19:26:51.448038 32082 solver.cpp:349] Iteration 33600 (58.2916 iter/s, 1.71551s/100 iter), loss = 0.00551467
I0628 19:26:51.448062 32082 solver.cpp:371]     Train net output #0: loss = 0.00551468 (* 1 = 0.00551468 loss)
I0628 19:26:51.448066 32082 sgd_solver.cpp:137] Iteration 33600, lr = 0.00475, m = 0.9
I0628 19:26:53.169694 32082 solver.cpp:349] Iteration 33700 (58.1006 iter/s, 1.72115s/100 iter), loss = 0.0162314
I0628 19:26:53.169719 32082 solver.cpp:371]     Train net output #0: loss = 0.0162314 (* 1 = 0.0162314 loss)
I0628 19:26:53.169725 32082 sgd_solver.cpp:137] Iteration 33700, lr = 0.00473437, m = 0.9
I0628 19:26:54.887886 32082 solver.cpp:349] Iteration 33800 (58.2179 iter/s, 1.71768s/100 iter), loss = 0.00340678
I0628 19:26:54.887912 32082 solver.cpp:371]     Train net output #0: loss = 0.00340679 (* 1 = 0.00340679 loss)
I0628 19:26:54.887917 32082 sgd_solver.cpp:137] Iteration 33800, lr = 0.00471875, m = 0.9
I0628 19:26:56.610631 32082 solver.cpp:349] Iteration 33900 (58.064 iter/s, 1.72224s/100 iter), loss = 0.00347558
I0628 19:26:56.610652 32082 solver.cpp:371]     Train net output #0: loss = 0.00347559 (* 1 = 0.00347559 loss)
I0628 19:26:56.610657 32082 sgd_solver.cpp:137] Iteration 33900, lr = 0.00470312, m = 0.9
I0628 19:26:58.313221 32082 solver.cpp:401] Sparsity after update:
I0628 19:26:58.314347 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:26:58.314354 32082 net.cpp:2170] conv1a_param_0(0.34) 
I0628 19:26:58.314362 32082 net.cpp:2170] conv1b_param_0(0.68) 
I0628 19:26:58.314363 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:26:58.314366 32082 net.cpp:2170] res2a_branch2a_param_0(0.68) 
I0628 19:26:58.314368 32082 net.cpp:2170] res2a_branch2b_param_0(0.652) 
I0628 19:26:58.314371 32082 net.cpp:2170] res3a_branch2a_param_0(0.68) 
I0628 19:26:58.314373 32082 net.cpp:2170] res3a_branch2b_param_0(0.68) 
I0628 19:26:58.314374 32082 net.cpp:2170] res4a_branch2a_param_0(0.68) 
I0628 19:26:58.314378 32082 net.cpp:2170] res4a_branch2b_param_0(0.68) 
I0628 19:26:58.314379 32082 net.cpp:2170] res5a_branch2a_param_0(0.68) 
I0628 19:26:58.314381 32082 net.cpp:2170] res5a_branch2b_param_0(0.68) 
I0628 19:26:58.314383 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.60018e+06/2.3599e+06) 0.678
I0628 19:26:58.314390 32082 solver.cpp:545] Iteration 34000, Testing net (#0)
I0628 19:26:59.313334 32080 data_reader.cpp:262] Starting prefetch of epoch 34
I0628 19:26:59.333452 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9088
I0628 19:26:59.333464 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9952
I0628 19:26:59.333469 32082 solver.cpp:630]     Test net output #2: loss = 0.329585 (* 1 = 0.329585 loss)
I0628 19:26:59.333482 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01882s
I0628 19:26:59.350714 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.7
I0628 19:27:00.167042 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:27:00.168521 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:27:00.168893 32082 solver.cpp:349] Iteration 34000 (28.1114 iter/s, 3.55728s/100 iter), loss = 0.00725057
I0628 19:27:00.168911 32082 solver.cpp:371]     Train net output #0: loss = 0.00725057 (* 1 = 0.00725057 loss)
I0628 19:27:00.168917 32082 sgd_solver.cpp:137] Iteration 34000, lr = 0.0046875, m = 0.9
I0628 19:27:01.888778 32082 solver.cpp:349] Iteration 34100 (58.1602 iter/s, 1.71939s/100 iter), loss = 0.0120626
I0628 19:27:01.888801 32082 solver.cpp:371]     Train net output #0: loss = 0.0120626 (* 1 = 0.0120626 loss)
I0628 19:27:01.888808 32082 sgd_solver.cpp:137] Iteration 34100, lr = 0.00467187, m = 0.9
I0628 19:27:03.607036 32082 solver.cpp:349] Iteration 34200 (58.2154 iter/s, 1.71776s/100 iter), loss = 0.00361765
I0628 19:27:03.607075 32082 solver.cpp:371]     Train net output #0: loss = 0.00361765 (* 1 = 0.00361765 loss)
I0628 19:27:03.607080 32082 sgd_solver.cpp:137] Iteration 34200, lr = 0.00465625, m = 0.9
I0628 19:27:05.329390 32082 solver.cpp:349] Iteration 34300 (58.0775 iter/s, 1.72184s/100 iter), loss = 0.00612782
I0628 19:27:05.329416 32082 solver.cpp:371]     Train net output #0: loss = 0.00612782 (* 1 = 0.00612782 loss)
I0628 19:27:05.329421 32082 sgd_solver.cpp:137] Iteration 34300, lr = 0.00464062, m = 0.9
I0628 19:27:06.550937 32050 data_reader.cpp:262] Starting prefetch of epoch 44
I0628 19:27:07.051458 32082 solver.cpp:349] Iteration 34400 (58.0866 iter/s, 1.72157s/100 iter), loss = 0.00793967
I0628 19:27:07.051481 32082 solver.cpp:371]     Train net output #0: loss = 0.00793967 (* 1 = 0.00793967 loss)
I0628 19:27:07.051487 32082 sgd_solver.cpp:137] Iteration 34400, lr = 0.004625, m = 0.9
I0628 19:27:08.781559 32082 solver.cpp:349] Iteration 34500 (57.8169 iter/s, 1.7296s/100 iter), loss = 0.004718
I0628 19:27:08.781584 32082 solver.cpp:371]     Train net output #0: loss = 0.004718 (* 1 = 0.004718 loss)
I0628 19:27:08.781589 32082 sgd_solver.cpp:137] Iteration 34500, lr = 0.00460937, m = 0.9
I0628 19:27:10.501622 32082 solver.cpp:349] Iteration 34600 (58.1543 iter/s, 1.71956s/100 iter), loss = 0.0100171
I0628 19:27:10.501648 32082 solver.cpp:371]     Train net output #0: loss = 0.0100171 (* 1 = 0.0100171 loss)
I0628 19:27:10.501654 32082 sgd_solver.cpp:137] Iteration 34600, lr = 0.00459375, m = 0.9
I0628 19:27:12.221623 32082 solver.cpp:349] Iteration 34700 (58.1565 iter/s, 1.7195s/100 iter), loss = 0.0059734
I0628 19:27:12.221650 32082 solver.cpp:371]     Train net output #0: loss = 0.0059734 (* 1 = 0.0059734 loss)
I0628 19:27:12.221657 32082 sgd_solver.cpp:137] Iteration 34700, lr = 0.00457812, m = 0.9
I0628 19:27:13.945973 32082 solver.cpp:349] Iteration 34800 (58.0099 iter/s, 1.72385s/100 iter), loss = 0.00990715
I0628 19:27:13.946028 32082 solver.cpp:371]     Train net output #0: loss = 0.00990715 (* 1 = 0.00990715 loss)
I0628 19:27:13.946045 32082 sgd_solver.cpp:137] Iteration 34800, lr = 0.0045625, m = 0.9
I0628 19:27:15.666646 32082 solver.cpp:349] Iteration 34900 (58.1353 iter/s, 1.72013s/100 iter), loss = 0.0101621
I0628 19:27:15.666672 32082 solver.cpp:371]     Train net output #0: loss = 0.0101621 (* 1 = 0.0101621 loss)
I0628 19:27:15.666678 32082 sgd_solver.cpp:137] Iteration 34900, lr = 0.00454687, m = 0.9
I0628 19:27:17.368623 32082 solver.cpp:401] Sparsity after update:
I0628 19:27:17.369701 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:27:17.369709 32082 net.cpp:2170] conv1a_param_0(0.35) 
I0628 19:27:17.369715 32082 net.cpp:2170] conv1b_param_0(0.7) 
I0628 19:27:17.369717 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:27:17.369720 32082 net.cpp:2170] res2a_branch2a_param_0(0.7) 
I0628 19:27:17.369725 32082 net.cpp:2170] res2a_branch2b_param_0(0.654) 
I0628 19:27:17.369727 32082 net.cpp:2170] res3a_branch2a_param_0(0.7) 
I0628 19:27:17.369729 32082 net.cpp:2170] res3a_branch2b_param_0(0.686) 
I0628 19:27:17.369732 32082 net.cpp:2170] res4a_branch2a_param_0(0.7) 
I0628 19:27:17.369735 32082 net.cpp:2170] res4a_branch2b_param_0(0.7) 
I0628 19:27:17.369737 32082 net.cpp:2170] res5a_branch2a_param_0(0.7) 
I0628 19:27:17.369740 32082 net.cpp:2170] res5a_branch2b_param_0(0.7) 
I0628 19:27:17.369741 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.64658e+06/2.3599e+06) 0.698
I0628 19:27:17.369748 32082 solver.cpp:545] Iteration 35000, Testing net (#0)
I0628 19:27:18.367383 32080 data_reader.cpp:262] Starting prefetch of epoch 35
I0628 19:27:18.389145 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9094
I0628 19:27:18.389159 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9948
I0628 19:27:18.389163 32082 solver.cpp:630]     Test net output #2: loss = 0.326457 (* 1 = 0.326457 loss)
I0628 19:27:18.389179 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01916s
I0628 19:27:18.406579 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.72
I0628 19:27:19.252629 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:27:19.254091 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:27:19.254451 32082 solver.cpp:349] Iteration 35000 (27.8799 iter/s, 3.58682s/100 iter), loss = 0.00427119
I0628 19:27:19.254468 32082 solver.cpp:371]     Train net output #0: loss = 0.00427119 (* 1 = 0.00427119 loss)
I0628 19:27:19.254477 32082 sgd_solver.cpp:137] Iteration 35000, lr = 0.00453125, m = 0.9
I0628 19:27:20.975774 32082 solver.cpp:349] Iteration 35100 (58.1114 iter/s, 1.72083s/100 iter), loss = 0.0176839
I0628 19:27:20.975808 32082 solver.cpp:371]     Train net output #0: loss = 0.0176839 (* 1 = 0.0176839 loss)
I0628 19:27:20.975812 32082 sgd_solver.cpp:137] Iteration 35100, lr = 0.00451563, m = 0.9
I0628 19:27:21.887156 32050 data_reader.cpp:262] Starting prefetch of epoch 45
I0628 19:27:22.697123 32082 solver.cpp:349] Iteration 35200 (58.1109 iter/s, 1.72085s/100 iter), loss = 0.00321784
I0628 19:27:22.697149 32082 solver.cpp:371]     Train net output #0: loss = 0.00321784 (* 1 = 0.00321784 loss)
I0628 19:27:22.697154 32082 sgd_solver.cpp:137] Iteration 35200, lr = 0.0045, m = 0.9
I0628 19:27:24.416890 32082 solver.cpp:349] Iteration 35300 (58.1642 iter/s, 1.71927s/100 iter), loss = 0.00864619
I0628 19:27:24.416916 32082 solver.cpp:371]     Train net output #0: loss = 0.00864619 (* 1 = 0.00864619 loss)
I0628 19:27:24.416923 32082 sgd_solver.cpp:137] Iteration 35300, lr = 0.00448438, m = 0.9
I0628 19:27:26.137315 32082 solver.cpp:349] Iteration 35400 (58.1419 iter/s, 1.71993s/100 iter), loss = 0.00812749
I0628 19:27:26.137338 32082 solver.cpp:371]     Train net output #0: loss = 0.00812748 (* 1 = 0.00812748 loss)
I0628 19:27:26.137344 32082 sgd_solver.cpp:137] Iteration 35400, lr = 0.00446875, m = 0.9
I0628 19:27:27.864836 32082 solver.cpp:349] Iteration 35500 (57.9029 iter/s, 1.72703s/100 iter), loss = 0.00765098
I0628 19:27:27.864863 32082 solver.cpp:371]     Train net output #0: loss = 0.00765098 (* 1 = 0.00765098 loss)
I0628 19:27:27.864871 32082 sgd_solver.cpp:137] Iteration 35500, lr = 0.00445312, m = 0.9
I0628 19:27:29.586872 32082 solver.cpp:349] Iteration 35600 (58.0876 iter/s, 1.72154s/100 iter), loss = 0.00669238
I0628 19:27:29.586899 32082 solver.cpp:371]     Train net output #0: loss = 0.00669238 (* 1 = 0.00669238 loss)
I0628 19:27:29.586905 32082 sgd_solver.cpp:137] Iteration 35600, lr = 0.0044375, m = 0.9
I0628 19:27:31.305361 32082 solver.cpp:349] Iteration 35700 (58.2073 iter/s, 1.718s/100 iter), loss = 0.00395535
I0628 19:27:31.305388 32082 solver.cpp:371]     Train net output #0: loss = 0.00395534 (* 1 = 0.00395534 loss)
I0628 19:27:31.305394 32082 sgd_solver.cpp:137] Iteration 35700, lr = 0.00442187, m = 0.9
I0628 19:27:33.022294 32082 solver.cpp:349] Iteration 35800 (58.2601 iter/s, 1.71644s/100 iter), loss = 0.00449342
I0628 19:27:33.022316 32082 solver.cpp:371]     Train net output #0: loss = 0.00449342 (* 1 = 0.00449342 loss)
I0628 19:27:33.022322 32082 sgd_solver.cpp:137] Iteration 35800, lr = 0.00440625, m = 0.9
I0628 19:27:34.742115 32082 solver.cpp:349] Iteration 35900 (58.1621 iter/s, 1.71933s/100 iter), loss = 0.00591834
I0628 19:27:34.742141 32082 solver.cpp:371]     Train net output #0: loss = 0.00591834 (* 1 = 0.00591834 loss)
I0628 19:27:34.742146 32082 sgd_solver.cpp:137] Iteration 35900, lr = 0.00439062, m = 0.9
I0628 19:27:35.327350 32050 data_reader.cpp:262] Starting prefetch of epoch 46
I0628 19:27:36.443222 32082 solver.cpp:401] Sparsity after update:
I0628 19:27:36.444237 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:27:36.444244 32082 net.cpp:2170] conv1a_param_0(0.36) 
I0628 19:27:36.444252 32082 net.cpp:2170] conv1b_param_0(0.72) 
I0628 19:27:36.444253 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:27:36.444257 32082 net.cpp:2170] res2a_branch2a_param_0(0.72) 
I0628 19:27:36.444258 32082 net.cpp:2170] res2a_branch2b_param_0(0.655) 
I0628 19:27:36.444262 32082 net.cpp:2170] res3a_branch2a_param_0(0.72) 
I0628 19:27:36.444263 32082 net.cpp:2170] res3a_branch2b_param_0(0.689) 
I0628 19:27:36.444265 32082 net.cpp:2170] res4a_branch2a_param_0(0.72) 
I0628 19:27:36.444267 32082 net.cpp:2170] res4a_branch2b_param_0(0.72) 
I0628 19:27:36.444269 32082 net.cpp:2170] res5a_branch2a_param_0(0.72) 
I0628 19:27:36.444272 32082 net.cpp:2170] res5a_branch2b_param_0(0.72) 
I0628 19:27:36.444274 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.69284e+06/2.3599e+06) 0.717
I0628 19:27:36.444281 32082 solver.cpp:545] Iteration 36000, Testing net (#0)
I0628 19:27:37.438995 32080 data_reader.cpp:262] Starting prefetch of epoch 36
I0628 19:27:37.462795 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9078
I0628 19:27:37.462806 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9948
I0628 19:27:37.462811 32082 solver.cpp:630]     Test net output #2: loss = 0.335488 (* 1 = 0.335488 loss)
I0628 19:27:37.462823 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01828s
I0628 19:27:37.480068 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.74
I0628 19:27:38.353996 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:27:38.355464 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:27:38.355820 32082 solver.cpp:349] Iteration 36000 (27.6799 iter/s, 3.61273s/100 iter), loss = 0.0107648
I0628 19:27:38.355839 32082 solver.cpp:371]     Train net output #0: loss = 0.0107648 (* 1 = 0.0107648 loss)
I0628 19:27:38.355844 32082 sgd_solver.cpp:137] Iteration 36000, lr = 0.004375, m = 0.9
I0628 19:27:40.082172 32082 solver.cpp:349] Iteration 36100 (57.9417 iter/s, 1.72587s/100 iter), loss = 0.0116852
I0628 19:27:40.082193 32082 solver.cpp:371]     Train net output #0: loss = 0.0116852 (* 1 = 0.0116852 loss)
I0628 19:27:40.082197 32082 sgd_solver.cpp:137] Iteration 36100, lr = 0.00435938, m = 0.9
I0628 19:27:41.798470 32082 solver.cpp:349] Iteration 36200 (58.2813 iter/s, 1.71582s/100 iter), loss = 0.00544455
I0628 19:27:41.798496 32082 solver.cpp:371]     Train net output #0: loss = 0.00544455 (* 1 = 0.00544455 loss)
I0628 19:27:41.798501 32082 sgd_solver.cpp:137] Iteration 36200, lr = 0.00434375, m = 0.9
I0628 19:27:43.518506 32082 solver.cpp:349] Iteration 36300 (58.1548 iter/s, 1.71955s/100 iter), loss = 0.00510266
I0628 19:27:43.518527 32082 solver.cpp:371]     Train net output #0: loss = 0.00510266 (* 1 = 0.00510266 loss)
I0628 19:27:43.518532 32082 sgd_solver.cpp:137] Iteration 36300, lr = 0.00432813, m = 0.9
I0628 19:27:45.239980 32082 solver.cpp:349] Iteration 36400 (58.106 iter/s, 1.72099s/100 iter), loss = 0.00581676
I0628 19:27:45.240006 32082 solver.cpp:371]     Train net output #0: loss = 0.00581676 (* 1 = 0.00581676 loss)
I0628 19:27:45.240012 32082 sgd_solver.cpp:137] Iteration 36400, lr = 0.0043125, m = 0.9
I0628 19:27:46.961422 32082 solver.cpp:349] Iteration 36500 (58.1073 iter/s, 1.72095s/100 iter), loss = 0.00692566
I0628 19:27:46.961448 32082 solver.cpp:371]     Train net output #0: loss = 0.00692566 (* 1 = 0.00692566 loss)
I0628 19:27:46.961454 32082 sgd_solver.cpp:137] Iteration 36500, lr = 0.00429688, m = 0.9
I0628 19:27:48.683187 32082 solver.cpp:349] Iteration 36600 (58.0964 iter/s, 1.72128s/100 iter), loss = 0.00646818
I0628 19:27:48.683212 32082 solver.cpp:371]     Train net output #0: loss = 0.00646819 (* 1 = 0.00646819 loss)
I0628 19:27:48.683218 32082 sgd_solver.cpp:137] Iteration 36600, lr = 0.00428125, m = 0.9
I0628 19:27:50.400107 32082 solver.cpp:349] Iteration 36700 (58.2603 iter/s, 1.71644s/100 iter), loss = 0.00795696
I0628 19:27:50.400135 32082 solver.cpp:371]     Train net output #0: loss = 0.00795696 (* 1 = 0.00795696 loss)
I0628 19:27:50.400141 32082 sgd_solver.cpp:137] Iteration 36700, lr = 0.00426562, m = 0.9
I0628 19:27:50.658504 32050 data_reader.cpp:262] Starting prefetch of epoch 47
I0628 19:27:52.128213 32082 solver.cpp:349] Iteration 36800 (57.8832 iter/s, 1.72762s/100 iter), loss = 0.00791638
I0628 19:27:52.128240 32082 solver.cpp:371]     Train net output #0: loss = 0.00791638 (* 1 = 0.00791638 loss)
I0628 19:27:52.128247 32082 sgd_solver.cpp:137] Iteration 36800, lr = 0.00425, m = 0.9
I0628 19:27:53.850055 32082 solver.cpp:349] Iteration 36900 (58.0938 iter/s, 1.72136s/100 iter), loss = 0.00439089
I0628 19:27:53.850082 32082 solver.cpp:371]     Train net output #0: loss = 0.00439089 (* 1 = 0.00439089 loss)
I0628 19:27:53.850088 32082 sgd_solver.cpp:137] Iteration 36900, lr = 0.00423437, m = 0.9
I0628 19:27:55.551638 32082 solver.cpp:401] Sparsity after update:
I0628 19:27:55.552758 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:27:55.552767 32082 net.cpp:2170] conv1a_param_0(0.37) 
I0628 19:27:55.552788 32082 net.cpp:2170] conv1b_param_0(0.74) 
I0628 19:27:55.552791 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:27:55.552794 32082 net.cpp:2170] res2a_branch2a_param_0(0.74) 
I0628 19:27:55.552799 32082 net.cpp:2170] res2a_branch2b_param_0(0.659) 
I0628 19:27:55.552803 32082 net.cpp:2170] res3a_branch2a_param_0(0.727) 
I0628 19:27:55.552809 32082 net.cpp:2170] res3a_branch2b_param_0(0.692) 
I0628 19:27:55.552812 32082 net.cpp:2170] res4a_branch2a_param_0(0.74) 
I0628 19:27:55.552815 32082 net.cpp:2170] res4a_branch2b_param_0(0.74) 
I0628 19:27:55.552820 32082 net.cpp:2170] res5a_branch2a_param_0(0.74) 
I0628 19:27:55.552824 32082 net.cpp:2170] res5a_branch2b_param_0(0.74) 
I0628 19:27:55.552826 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.73816e+06/2.3599e+06) 0.737
I0628 19:27:55.552840 32082 solver.cpp:545] Iteration 37000, Testing net (#0)
I0628 19:27:56.547806 32080 data_reader.cpp:262] Starting prefetch of epoch 37
I0628 19:27:56.571777 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9058
I0628 19:27:56.571790 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.995
I0628 19:27:56.571795 32082 solver.cpp:630]     Test net output #2: loss = 0.337024 (* 1 = 0.337024 loss)
I0628 19:27:56.571808 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01871s
I0628 19:27:56.589052 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.76
I0628 19:27:57.483656 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:27:57.485136 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:27:57.485492 32082 solver.cpp:349] Iteration 37000 (27.5143 iter/s, 3.63447s/100 iter), loss = 0.0100698
I0628 19:27:57.485512 32082 solver.cpp:371]     Train net output #0: loss = 0.0100698 (* 1 = 0.0100698 loss)
I0628 19:27:57.485518 32082 sgd_solver.cpp:137] Iteration 37000, lr = 0.00421875, m = 0.9
I0628 19:27:59.204883 32082 solver.cpp:349] Iteration 37100 (58.1763 iter/s, 1.71891s/100 iter), loss = 0.0133468
I0628 19:27:59.204902 32082 solver.cpp:371]     Train net output #0: loss = 0.0133468 (* 1 = 0.0133468 loss)
I0628 19:27:59.204906 32082 sgd_solver.cpp:137] Iteration 37100, lr = 0.00420313, m = 0.9
I0628 19:28:00.925164 32082 solver.cpp:349] Iteration 37200 (58.1459 iter/s, 1.71981s/100 iter), loss = 0.0173099
I0628 19:28:00.925186 32082 solver.cpp:371]     Train net output #0: loss = 0.0173099 (* 1 = 0.0173099 loss)
I0628 19:28:00.925190 32082 sgd_solver.cpp:137] Iteration 37200, lr = 0.0041875, m = 0.9
I0628 19:28:02.646306 32082 solver.cpp:349] Iteration 37300 (58.117 iter/s, 1.72067s/100 iter), loss = 0.0309862
I0628 19:28:02.646327 32082 solver.cpp:371]     Train net output #0: loss = 0.0309862 (* 1 = 0.0309862 loss)
I0628 19:28:02.646330 32082 sgd_solver.cpp:137] Iteration 37300, lr = 0.00417187, m = 0.9
I0628 19:28:04.362717 32082 solver.cpp:349] Iteration 37400 (58.277 iter/s, 1.71594s/100 iter), loss = 0.00203535
I0628 19:28:04.362742 32082 solver.cpp:371]     Train net output #0: loss = 0.00203535 (* 1 = 0.00203535 loss)
I0628 19:28:04.362747 32082 sgd_solver.cpp:137] Iteration 37400, lr = 0.00415625, m = 0.9
I0628 19:28:06.012223 32050 data_reader.cpp:262] Starting prefetch of epoch 48
I0628 19:28:06.080940 32082 solver.cpp:349] Iteration 37500 (58.2158 iter/s, 1.71775s/100 iter), loss = 0.00507345
I0628 19:28:06.080967 32082 solver.cpp:371]     Train net output #0: loss = 0.00507345 (* 1 = 0.00507345 loss)
I0628 19:28:06.080973 32082 sgd_solver.cpp:137] Iteration 37500, lr = 0.00414062, m = 0.9
I0628 19:28:07.800045 32082 solver.cpp:349] Iteration 37600 (58.1861 iter/s, 1.71862s/100 iter), loss = 0.0239773
I0628 19:28:07.800114 32082 solver.cpp:371]     Train net output #0: loss = 0.0239773 (* 1 = 0.0239773 loss)
I0628 19:28:07.800122 32082 sgd_solver.cpp:137] Iteration 37600, lr = 0.004125, m = 0.9
I0628 19:28:09.521589 32082 solver.cpp:349] Iteration 37700 (58.1051 iter/s, 1.72102s/100 iter), loss = 0.0157856
I0628 19:28:09.521613 32082 solver.cpp:371]     Train net output #0: loss = 0.0157856 (* 1 = 0.0157856 loss)
I0628 19:28:09.521620 32082 sgd_solver.cpp:137] Iteration 37700, lr = 0.00410937, m = 0.9
I0628 19:28:11.242199 32082 solver.cpp:349] Iteration 37800 (58.135 iter/s, 1.72013s/100 iter), loss = 0.0107801
I0628 19:28:11.242221 32082 solver.cpp:371]     Train net output #0: loss = 0.0107801 (* 1 = 0.0107801 loss)
I0628 19:28:11.242224 32082 sgd_solver.cpp:137] Iteration 37800, lr = 0.00409375, m = 0.9
I0628 19:28:12.963493 32082 solver.cpp:349] Iteration 37900 (58.1117 iter/s, 1.72082s/100 iter), loss = 0.0176397
I0628 19:28:12.963520 32082 solver.cpp:371]     Train net output #0: loss = 0.0176397 (* 1 = 0.0176397 loss)
I0628 19:28:12.963524 32082 sgd_solver.cpp:137] Iteration 37900, lr = 0.00407812, m = 0.9
I0628 19:28:14.663517 32082 solver.cpp:401] Sparsity after update:
I0628 19:28:14.664553 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:28:14.664561 32082 net.cpp:2170] conv1a_param_0(0.38) 
I0628 19:28:14.664567 32082 net.cpp:2170] conv1b_param_0(0.76) 
I0628 19:28:14.664571 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:28:14.664573 32082 net.cpp:2170] res2a_branch2a_param_0(0.76) 
I0628 19:28:14.664575 32082 net.cpp:2170] res2a_branch2b_param_0(0.66) 
I0628 19:28:14.664578 32082 net.cpp:2170] res3a_branch2a_param_0(0.732) 
I0628 19:28:14.664580 32082 net.cpp:2170] res3a_branch2b_param_0(0.696) 
I0628 19:28:14.664582 32082 net.cpp:2170] res4a_branch2a_param_0(0.76) 
I0628 19:28:14.664584 32082 net.cpp:2170] res4a_branch2b_param_0(0.76) 
I0628 19:28:14.664587 32082 net.cpp:2170] res5a_branch2a_param_0(0.76) 
I0628 19:28:14.664589 32082 net.cpp:2170] res5a_branch2b_param_0(0.76) 
I0628 19:28:14.664592 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.78336e+06/2.3599e+06) 0.756
I0628 19:28:14.664598 32082 solver.cpp:545] Iteration 38000, Testing net (#0)
I0628 19:28:15.659682 32080 data_reader.cpp:262] Starting prefetch of epoch 38
I0628 19:28:15.685371 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9088
I0628 19:28:15.685384 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9956
I0628 19:28:15.685389 32082 solver.cpp:630]     Test net output #2: loss = 0.325561 (* 1 = 0.325561 loss)
I0628 19:28:15.685403 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02054s
I0628 19:28:15.702728 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.78
I0628 19:28:16.614776 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:28:16.616246 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:28:16.616603 32082 solver.cpp:349] Iteration 38000 (27.3811 iter/s, 3.65215s/100 iter), loss = 0.0085587
I0628 19:28:16.616622 32082 solver.cpp:371]     Train net output #0: loss = 0.00855871 (* 1 = 0.00855871 loss)
I0628 19:28:16.616627 32082 sgd_solver.cpp:137] Iteration 38000, lr = 0.0040625, m = 0.9
I0628 19:28:18.337291 32082 solver.cpp:349] Iteration 38100 (58.132 iter/s, 1.72022s/100 iter), loss = 0.00451715
I0628 19:28:18.337316 32082 solver.cpp:371]     Train net output #0: loss = 0.00451716 (* 1 = 0.00451716 loss)
I0628 19:28:18.337321 32082 sgd_solver.cpp:137] Iteration 38100, lr = 0.00404688, m = 0.9
I0628 19:28:20.055465 32082 solver.cpp:349] Iteration 38200 (58.2173 iter/s, 1.7177s/100 iter), loss = 0.0135893
I0628 19:28:20.055490 32082 solver.cpp:371]     Train net output #0: loss = 0.0135893 (* 1 = 0.0135893 loss)
I0628 19:28:20.055496 32082 sgd_solver.cpp:137] Iteration 38200, lr = 0.00403125, m = 0.9
I0628 19:28:21.395459 32050 data_reader.cpp:262] Starting prefetch of epoch 49
I0628 19:28:21.774197 32082 solver.cpp:349] Iteration 38300 (58.1984 iter/s, 1.71826s/100 iter), loss = 0.00881385
I0628 19:28:21.774233 32082 solver.cpp:371]     Train net output #0: loss = 0.00881385 (* 1 = 0.00881385 loss)
I0628 19:28:21.774237 32082 sgd_solver.cpp:137] Iteration 38300, lr = 0.00401562, m = 0.9
I0628 19:28:23.493762 32082 solver.cpp:349] Iteration 38400 (58.1706 iter/s, 1.71908s/100 iter), loss = 0.00543489
I0628 19:28:23.493784 32082 solver.cpp:371]     Train net output #0: loss = 0.00543489 (* 1 = 0.00543489 loss)
I0628 19:28:23.493790 32082 sgd_solver.cpp:137] Iteration 38400, lr = 0.004, m = 0.9
I0628 19:28:25.218673 32082 solver.cpp:349] Iteration 38500 (57.9898 iter/s, 1.72444s/100 iter), loss = 0.010471
I0628 19:28:25.218694 32082 solver.cpp:371]     Train net output #0: loss = 0.010471 (* 1 = 0.010471 loss)
I0628 19:28:25.218698 32082 sgd_solver.cpp:137] Iteration 38500, lr = 0.00398437, m = 0.9
I0628 19:28:26.937132 32082 solver.cpp:349] Iteration 38600 (58.2074 iter/s, 1.718s/100 iter), loss = 0.0326741
I0628 19:28:26.937153 32082 solver.cpp:371]     Train net output #0: loss = 0.0326741 (* 1 = 0.0326741 loss)
I0628 19:28:26.937156 32082 sgd_solver.cpp:137] Iteration 38600, lr = 0.00396875, m = 0.9
I0628 19:28:28.656319 32082 solver.cpp:349] Iteration 38700 (58.1826 iter/s, 1.71873s/100 iter), loss = 0.0100293
I0628 19:28:28.656342 32082 solver.cpp:371]     Train net output #0: loss = 0.0100293 (* 1 = 0.0100293 loss)
I0628 19:28:28.656347 32082 sgd_solver.cpp:137] Iteration 38700, lr = 0.00395312, m = 0.9
I0628 19:28:30.374647 32082 solver.cpp:349] Iteration 38800 (58.2118 iter/s, 1.71786s/100 iter), loss = 0.0128048
I0628 19:28:30.374670 32082 solver.cpp:371]     Train net output #0: loss = 0.0128048 (* 1 = 0.0128048 loss)
I0628 19:28:30.374675 32082 sgd_solver.cpp:137] Iteration 38800, lr = 0.0039375, m = 0.9
I0628 19:28:32.093144 32082 solver.cpp:349] Iteration 38900 (58.2061 iter/s, 1.71803s/100 iter), loss = 0.0151223
I0628 19:28:32.093164 32082 solver.cpp:371]     Train net output #0: loss = 0.0151223 (* 1 = 0.0151223 loss)
I0628 19:28:32.093168 32082 sgd_solver.cpp:137] Iteration 38900, lr = 0.00392187, m = 0.9
I0628 19:28:33.793215 32082 solver.cpp:401] Sparsity after update:
I0628 19:28:33.794261 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:28:33.794270 32082 net.cpp:2170] conv1a_param_0(0.39) 
I0628 19:28:33.794276 32082 net.cpp:2170] conv1b_param_0(0.78) 
I0628 19:28:33.794278 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:28:33.794281 32082 net.cpp:2170] res2a_branch2a_param_0(0.78) 
I0628 19:28:33.794282 32082 net.cpp:2170] res2a_branch2b_param_0(0.661) 
I0628 19:28:33.794286 32082 net.cpp:2170] res3a_branch2a_param_0(0.736) 
I0628 19:28:33.794291 32082 net.cpp:2170] res3a_branch2b_param_0(0.697) 
I0628 19:28:33.794296 32082 net.cpp:2170] res4a_branch2a_param_0(0.779) 
I0628 19:28:33.794297 32082 net.cpp:2170] res4a_branch2b_param_0(0.78) 
I0628 19:28:33.794301 32082 net.cpp:2170] res5a_branch2a_param_0(0.78) 
I0628 19:28:33.794304 32082 net.cpp:2170] res5a_branch2b_param_0(0.78) 
I0628 19:28:33.794308 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.82806e+06/2.3599e+06) 0.775
I0628 19:28:33.794318 32082 solver.cpp:545] Iteration 39000, Testing net (#0)
I0628 19:28:34.790611 32080 data_reader.cpp:262] Starting prefetch of epoch 39
I0628 19:28:34.812432 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.904
I0628 19:28:34.812443 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9936
I0628 19:28:34.812448 32082 solver.cpp:630]     Test net output #2: loss = 0.339945 (* 1 = 0.339945 loss)
I0628 19:28:34.812463 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01789s
I0628 19:28:34.829643 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.8
I0628 19:28:35.772253 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:28:35.773727 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:28:35.774091 32082 solver.cpp:349] Iteration 39000 (27.1739 iter/s, 3.68s/100 iter), loss = 0.0125196
I0628 19:28:35.774108 32082 solver.cpp:371]     Train net output #0: loss = 0.0125196 (* 1 = 0.0125196 loss)
I0628 19:28:35.774123 32082 sgd_solver.cpp:137] Iteration 39000, lr = 0.00390625, m = 0.9
I0628 19:28:36.803174 32050 data_reader.cpp:262] Starting prefetch of epoch 50
I0628 19:28:37.507954 32082 solver.cpp:349] Iteration 39100 (57.6904 iter/s, 1.73339s/100 iter), loss = 0.0163694
I0628 19:28:37.507998 32082 solver.cpp:371]     Train net output #0: loss = 0.0163694 (* 1 = 0.0163694 loss)
I0628 19:28:37.508005 32082 sgd_solver.cpp:137] Iteration 39100, lr = 0.00389063, m = 0.9
I0628 19:28:39.225128 32082 solver.cpp:349] Iteration 39200 (58.2519 iter/s, 1.71668s/100 iter), loss = 0.0129611
I0628 19:28:39.225208 32082 solver.cpp:371]     Train net output #0: loss = 0.0129611 (* 1 = 0.0129611 loss)
I0628 19:28:39.225214 32082 sgd_solver.cpp:137] Iteration 39200, lr = 0.003875, m = 0.9
I0628 19:28:40.942929 32082 solver.cpp:349] Iteration 39300 (58.2317 iter/s, 1.71728s/100 iter), loss = 0.0142701
I0628 19:28:40.942950 32082 solver.cpp:371]     Train net output #0: loss = 0.0142701 (* 1 = 0.0142701 loss)
I0628 19:28:40.942955 32082 sgd_solver.cpp:137] Iteration 39300, lr = 0.00385938, m = 0.9
I0628 19:28:42.660531 32082 solver.cpp:349] Iteration 39400 (58.2363 iter/s, 1.71714s/100 iter), loss = 0.0127994
I0628 19:28:42.660557 32082 solver.cpp:371]     Train net output #0: loss = 0.0127995 (* 1 = 0.0127995 loss)
I0628 19:28:42.660563 32082 sgd_solver.cpp:137] Iteration 39400, lr = 0.00384375, m = 0.9
I0628 19:28:44.380776 32082 solver.cpp:349] Iteration 39500 (58.147 iter/s, 1.71978s/100 iter), loss = 0.035984
I0628 19:28:44.380798 32082 solver.cpp:371]     Train net output #0: loss = 0.035984 (* 1 = 0.035984 loss)
I0628 19:28:44.380802 32082 sgd_solver.cpp:137] Iteration 39500, lr = 0.00382812, m = 0.9
I0628 19:28:46.101076 32082 solver.cpp:349] Iteration 39600 (58.1449 iter/s, 1.71984s/100 iter), loss = 0.00729266
I0628 19:28:46.101099 32082 solver.cpp:371]     Train net output #0: loss = 0.00729268 (* 1 = 0.00729268 loss)
I0628 19:28:46.101101 32082 sgd_solver.cpp:137] Iteration 39600, lr = 0.0038125, m = 0.9
I0628 19:28:47.819993 32082 solver.cpp:349] Iteration 39700 (58.1917 iter/s, 1.71846s/100 iter), loss = 0.00837137
I0628 19:28:47.820014 32082 solver.cpp:371]     Train net output #0: loss = 0.00837139 (* 1 = 0.00837139 loss)
I0628 19:28:47.820019 32082 sgd_solver.cpp:137] Iteration 39700, lr = 0.00379687, m = 0.9
I0628 19:28:49.542105 32082 solver.cpp:349] Iteration 39800 (58.0834 iter/s, 1.72166s/100 iter), loss = 0.0125428
I0628 19:28:49.542132 32082 solver.cpp:371]     Train net output #0: loss = 0.0125428 (* 1 = 0.0125428 loss)
I0628 19:28:49.542138 32082 sgd_solver.cpp:137] Iteration 39800, lr = 0.00378125, m = 0.9
I0628 19:28:50.230829 32050 data_reader.cpp:262] Starting prefetch of epoch 51
I0628 19:28:51.262450 32082 solver.cpp:349] Iteration 39900 (58.1203 iter/s, 1.72057s/100 iter), loss = 0.00869081
I0628 19:28:51.262472 32082 solver.cpp:371]     Train net output #0: loss = 0.00869083 (* 1 = 0.00869083 loss)
I0628 19:28:51.262476 32082 sgd_solver.cpp:137] Iteration 39900, lr = 0.00376562, m = 0.9
I0628 19:28:52.964654 32082 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_40000.caffemodel
I0628 19:28:52.972439 32082 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_40000.solverstate
I0628 19:28:52.975863 32082 solver.cpp:401] Sparsity after update:
I0628 19:28:52.976987 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:28:52.976994 32082 net.cpp:2170] conv1a_param_0(0.4) 
I0628 19:28:52.977001 32082 net.cpp:2170] conv1b_param_0(0.8) 
I0628 19:28:52.977004 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:28:52.977005 32082 net.cpp:2170] res2a_branch2a_param_0(0.8) 
I0628 19:28:52.977007 32082 net.cpp:2170] res2a_branch2b_param_0(0.663) 
I0628 19:28:52.977010 32082 net.cpp:2170] res3a_branch2a_param_0(0.74) 
I0628 19:28:52.977011 32082 net.cpp:2170] res3a_branch2b_param_0(0.699) 
I0628 19:28:52.977013 32082 net.cpp:2170] res4a_branch2a_param_0(0.785) 
I0628 19:28:52.977015 32082 net.cpp:2170] res4a_branch2b_param_0(0.799) 
I0628 19:28:52.977016 32082 net.cpp:2170] res5a_branch2a_param_0(0.8) 
I0628 19:28:52.977018 32082 net.cpp:2170] res5a_branch2b_param_0(0.8) 
I0628 19:28:52.977020 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.86882e+06/2.3599e+06) 0.792
I0628 19:28:52.977027 32082 solver.cpp:545] Iteration 40000, Testing net (#0)
I0628 19:28:53.971825 32080 data_reader.cpp:262] Starting prefetch of epoch 40
I0628 19:28:53.991928 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.906
I0628 19:28:53.991951 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9942
I0628 19:28:53.991957 32082 solver.cpp:630]     Test net output #2: loss = 0.331833 (* 1 = 0.331833 loss)
I0628 19:28:53.991971 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01523s
I0628 19:28:54.009136 32088 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.82
I0628 19:28:54.977824 32088 net.cpp:2177] All zero weights of convolution layers are frozen
I0628 19:28:54.979295 32088 inner_product_layer.cpp:14] all zero weights of fc10 are frozen
I0628 19:28:54.979653 32082 solver.cpp:349] Iteration 40000 (26.8946 iter/s, 3.71822s/100 iter), loss = 0.017854
I0628 19:28:54.979671 32082 solver.cpp:371]     Train net output #0: loss = 0.017854 (* 1 = 0.017854 loss)
I0628 19:28:54.979676 32082 sgd_solver.cpp:137] Iteration 40000, lr = 0.00375, m = 0.9
I0628 19:28:56.696931 32082 solver.cpp:349] Iteration 40100 (58.2167 iter/s, 1.71772s/100 iter), loss = 0.0257802
I0628 19:28:56.696959 32082 solver.cpp:371]     Train net output #0: loss = 0.0257802 (* 1 = 0.0257802 loss)
I0628 19:28:56.696965 32082 sgd_solver.cpp:137] Iteration 40100, lr = 0.00373438, m = 0.9
I0628 19:28:58.414034 32082 solver.cpp:349] Iteration 40200 (58.2232 iter/s, 1.71753s/100 iter), loss = 0.00336482
I0628 19:28:58.414057 32082 solver.cpp:371]     Train net output #0: loss = 0.00336484 (* 1 = 0.00336484 loss)
I0628 19:28:58.414060 32082 sgd_solver.cpp:137] Iteration 40200, lr = 0.00371875, m = 0.9
I0628 19:29:00.132112 32082 solver.cpp:349] Iteration 40300 (58.1901 iter/s, 1.71851s/100 iter), loss = 0.0131555
I0628 19:29:00.132136 32082 solver.cpp:371]     Train net output #0: loss = 0.0131555 (* 1 = 0.0131555 loss)
I0628 19:29:00.132143 32082 sgd_solver.cpp:137] Iteration 40300, lr = 0.00370313, m = 0.9
I0628 19:29:01.848062 32082 solver.cpp:349] Iteration 40400 (58.2626 iter/s, 1.71637s/100 iter), loss = 0.0187084
I0628 19:29:01.848088 32082 solver.cpp:371]     Train net output #0: loss = 0.0187084 (* 1 = 0.0187084 loss)
I0628 19:29:01.848094 32082 sgd_solver.cpp:137] Iteration 40400, lr = 0.0036875, m = 0.9
I0628 19:29:03.567414 32082 solver.cpp:349] Iteration 40500 (58.1475 iter/s, 1.71976s/100 iter), loss = 0.0372087
I0628 19:29:03.567441 32082 solver.cpp:371]     Train net output #0: loss = 0.0372087 (* 1 = 0.0372087 loss)
I0628 19:29:03.567447 32082 sgd_solver.cpp:137] Iteration 40500, lr = 0.00367187, m = 0.9
I0628 19:29:05.287065 32082 solver.cpp:349] Iteration 40600 (58.1376 iter/s, 1.72006s/100 iter), loss = 0.014595
I0628 19:29:05.287088 32082 solver.cpp:371]     Train net output #0: loss = 0.014595 (* 1 = 0.014595 loss)
I0628 19:29:05.287091 32082 sgd_solver.cpp:137] Iteration 40600, lr = 0.00365625, m = 0.9
I0628 19:29:05.648756 32050 data_reader.cpp:262] Starting prefetch of epoch 52
I0628 19:29:07.005853 32082 solver.cpp:349] Iteration 40700 (58.1666 iter/s, 1.7192s/100 iter), loss = 0.0100605
I0628 19:29:07.005879 32082 solver.cpp:371]     Train net output #0: loss = 0.0100606 (* 1 = 0.0100606 loss)
I0628 19:29:07.005885 32082 sgd_solver.cpp:137] Iteration 40700, lr = 0.00364062, m = 0.9
I0628 19:29:08.723845 32082 solver.cpp:349] Iteration 40800 (58.194 iter/s, 1.71839s/100 iter), loss = 0.0127572
I0628 19:29:08.723871 32082 solver.cpp:371]     Train net output #0: loss = 0.0127572 (* 1 = 0.0127572 loss)
I0628 19:29:08.723877 32082 sgd_solver.cpp:137] Iteration 40800, lr = 0.003625, m = 0.9
I0628 19:29:10.442031 32082 solver.cpp:349] Iteration 40900 (58.1876 iter/s, 1.71858s/100 iter), loss = 0.0090329
I0628 19:29:10.442098 32082 solver.cpp:371]     Train net output #0: loss = 0.00903292 (* 1 = 0.00903292 loss)
I0628 19:29:10.442106 32082 sgd_solver.cpp:137] Iteration 40900, lr = 0.00360937, m = 0.9
I0628 19:29:12.145032 32082 solver.cpp:401] Sparsity after update:
I0628 19:29:12.146100 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:29:12.146107 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:29:12.146113 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:29:12.146116 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:29:12.146118 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:29:12.146121 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:29:12.146122 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:29:12.146124 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:29:12.146127 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:29:12.146129 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:29:12.146132 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:29:12.146134 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:29:12.146136 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:29:12.146144 32082 solver.cpp:545] Iteration 41000, Testing net (#0)
I0628 19:29:13.143688 32080 data_reader.cpp:262] Starting prefetch of epoch 41
I0628 19:29:13.165563 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9062
I0628 19:29:13.165575 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9948
I0628 19:29:13.165580 32082 solver.cpp:630]     Test net output #2: loss = 0.321509 (* 1 = 0.321509 loss)
I0628 19:29:13.165594 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.0197s
I0628 19:29:13.182957 32082 solver.cpp:349] Iteration 41000 (36.476 iter/s, 2.74153s/100 iter), loss = 0.00918568
I0628 19:29:13.182981 32082 solver.cpp:371]     Train net output #0: loss = 0.0091857 (* 1 = 0.0091857 loss)
I0628 19:29:13.182984 32082 sgd_solver.cpp:137] Iteration 41000, lr = 0.00359375, m = 0.9
I0628 19:29:14.907570 32082 solver.cpp:349] Iteration 41100 (57.9709 iter/s, 1.725s/100 iter), loss = 0.0451678
I0628 19:29:14.907598 32082 solver.cpp:371]     Train net output #0: loss = 0.0451678 (* 1 = 0.0451678 loss)
I0628 19:29:14.907604 32082 sgd_solver.cpp:137] Iteration 41100, lr = 0.00357813, m = 0.9
I0628 19:29:16.630242 32082 solver.cpp:349] Iteration 41200 (58.0367 iter/s, 1.72305s/100 iter), loss = 0.00724304
I0628 19:29:16.630270 32082 solver.cpp:371]     Train net output #0: loss = 0.00724307 (* 1 = 0.00724307 loss)
I0628 19:29:16.630275 32082 sgd_solver.cpp:137] Iteration 41200, lr = 0.0035625, m = 0.9
I0628 19:29:18.349261 32082 solver.cpp:349] Iteration 41300 (58.1603 iter/s, 1.71939s/100 iter), loss = 0.00720539
I0628 19:29:18.349285 32082 solver.cpp:371]     Train net output #0: loss = 0.00720543 (* 1 = 0.00720543 loss)
I0628 19:29:18.349292 32082 sgd_solver.cpp:137] Iteration 41300, lr = 0.00354687, m = 0.9
I0628 19:29:20.064653 32082 solver.cpp:349] Iteration 41400 (58.2832 iter/s, 1.71576s/100 iter), loss = 0.00917817
I0628 19:29:20.064680 32082 solver.cpp:371]     Train net output #0: loss = 0.00917821 (* 1 = 0.00917821 loss)
I0628 19:29:20.064687 32082 sgd_solver.cpp:137] Iteration 41400, lr = 0.00353125, m = 0.9
I0628 19:29:20.119894 32050 data_reader.cpp:262] Starting prefetch of epoch 53
I0628 19:29:21.791862 32082 solver.cpp:349] Iteration 41500 (57.8847 iter/s, 1.72757s/100 iter), loss = 0.0126066
I0628 19:29:21.791887 32082 solver.cpp:371]     Train net output #0: loss = 0.0126067 (* 1 = 0.0126067 loss)
I0628 19:29:21.791893 32082 sgd_solver.cpp:137] Iteration 41500, lr = 0.00351562, m = 0.9
I0628 19:29:23.512588 32082 solver.cpp:349] Iteration 41600 (58.1029 iter/s, 1.72109s/100 iter), loss = 0.0109971
I0628 19:29:23.512609 32082 solver.cpp:371]     Train net output #0: loss = 0.0109971 (* 1 = 0.0109971 loss)
I0628 19:29:23.512614 32082 sgd_solver.cpp:137] Iteration 41600, lr = 0.0035, m = 0.9
I0628 19:29:25.231779 32082 solver.cpp:349] Iteration 41700 (58.1547 iter/s, 1.71955s/100 iter), loss = 0.00551805
I0628 19:29:25.231843 32082 solver.cpp:371]     Train net output #0: loss = 0.00551808 (* 1 = 0.00551808 loss)
I0628 19:29:25.231850 32082 sgd_solver.cpp:137] Iteration 41700, lr = 0.00348437, m = 0.9
I0628 19:29:26.949162 32082 solver.cpp:349] Iteration 41800 (58.2177 iter/s, 1.71769s/100 iter), loss = 0.0135826
I0628 19:29:26.949188 32082 solver.cpp:371]     Train net output #0: loss = 0.0135826 (* 1 = 0.0135826 loss)
I0628 19:29:26.949194 32082 sgd_solver.cpp:137] Iteration 41800, lr = 0.00346875, m = 0.9
I0628 19:29:28.668037 32082 solver.cpp:349] Iteration 41900 (58.166 iter/s, 1.71922s/100 iter), loss = 0.0109861
I0628 19:29:28.668063 32082 solver.cpp:371]     Train net output #0: loss = 0.0109862 (* 1 = 0.0109862 loss)
I0628 19:29:28.668069 32082 sgd_solver.cpp:137] Iteration 41900, lr = 0.00345312, m = 0.9
I0628 19:29:30.373868 32082 solver.cpp:401] Sparsity after update:
I0628 19:29:30.374994 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:29:30.375001 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:29:30.375008 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:29:30.375010 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:29:30.375012 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:29:30.375015 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:29:30.375017 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:29:30.375020 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:29:30.375022 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:29:30.375025 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:29:30.375025 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:29:30.375027 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:29:30.375030 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:29:30.375037 32082 solver.cpp:545] Iteration 42000, Testing net (#0)
I0628 19:29:31.374879 32080 data_reader.cpp:262] Starting prefetch of epoch 42
I0628 19:29:31.395395 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9072
I0628 19:29:31.395407 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9958
I0628 19:29:31.395412 32082 solver.cpp:630]     Test net output #2: loss = 0.326982 (* 1 = 0.326982 loss)
I0628 19:29:31.395426 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02061s
I0628 19:29:31.413235 32082 solver.cpp:349] Iteration 42000 (36.4197 iter/s, 2.74577s/100 iter), loss = 0.0130987
I0628 19:29:31.413260 32082 solver.cpp:371]     Train net output #0: loss = 0.0130987 (* 1 = 0.0130987 loss)
I0628 19:29:31.413266 32082 sgd_solver.cpp:137] Iteration 42000, lr = 0.0034375, m = 0.9
I0628 19:29:33.133997 32082 solver.cpp:349] Iteration 42100 (58.1025 iter/s, 1.7211s/100 iter), loss = 0.0179411
I0628 19:29:33.134016 32082 solver.cpp:371]     Train net output #0: loss = 0.0179411 (* 1 = 0.0179411 loss)
I0628 19:29:33.134021 32082 sgd_solver.cpp:137] Iteration 42100, lr = 0.00342188, m = 0.9
I0628 19:29:34.579499 32050 data_reader.cpp:262] Starting prefetch of epoch 54
I0628 19:29:34.854001 32082 solver.cpp:349] Iteration 42200 (58.128 iter/s, 1.72034s/100 iter), loss = 0.00732109
I0628 19:29:34.854029 32082 solver.cpp:371]     Train net output #0: loss = 0.00732114 (* 1 = 0.00732114 loss)
I0628 19:29:34.854035 32082 sgd_solver.cpp:137] Iteration 42200, lr = 0.00340625, m = 0.9
I0628 19:29:36.575470 32082 solver.cpp:349] Iteration 42300 (58.0791 iter/s, 1.72179s/100 iter), loss = 0.0098538
I0628 19:29:36.575497 32082 solver.cpp:371]     Train net output #0: loss = 0.00985384 (* 1 = 0.00985384 loss)
I0628 19:29:36.575503 32082 sgd_solver.cpp:137] Iteration 42300, lr = 0.00339063, m = 0.9
I0628 19:29:38.294020 32082 solver.cpp:349] Iteration 42400 (58.1779 iter/s, 1.71887s/100 iter), loss = 0.00521113
I0628 19:29:38.294044 32082 solver.cpp:371]     Train net output #0: loss = 0.00521117 (* 1 = 0.00521117 loss)
I0628 19:29:38.294049 32082 sgd_solver.cpp:137] Iteration 42400, lr = 0.003375, m = 0.9
I0628 19:29:40.016149 32082 solver.cpp:349] Iteration 42500 (58.0569 iter/s, 1.72245s/100 iter), loss = 0.00789747
I0628 19:29:40.016187 32082 solver.cpp:371]     Train net output #0: loss = 0.00789751 (* 1 = 0.00789751 loss)
I0628 19:29:40.016191 32082 sgd_solver.cpp:137] Iteration 42500, lr = 0.00335937, m = 0.9
I0628 19:29:41.735357 32082 solver.cpp:349] Iteration 42600 (58.1561 iter/s, 1.71951s/100 iter), loss = 0.0287423
I0628 19:29:41.735414 32082 solver.cpp:371]     Train net output #0: loss = 0.0287423 (* 1 = 0.0287423 loss)
I0628 19:29:41.735419 32082 sgd_solver.cpp:137] Iteration 42600, lr = 0.00334375, m = 0.9
I0628 19:29:43.453660 32082 solver.cpp:349] Iteration 42700 (58.1877 iter/s, 1.71858s/100 iter), loss = 0.0108914
I0628 19:29:43.453683 32082 solver.cpp:371]     Train net output #0: loss = 0.0108915 (* 1 = 0.0108915 loss)
I0628 19:29:43.453689 32082 sgd_solver.cpp:137] Iteration 42700, lr = 0.00332812, m = 0.9
I0628 19:29:45.174269 32082 solver.cpp:349] Iteration 42800 (58.1086 iter/s, 1.72092s/100 iter), loss = 0.0085382
I0628 19:29:45.174290 32082 solver.cpp:371]     Train net output #0: loss = 0.00853825 (* 1 = 0.00853825 loss)
I0628 19:29:45.174294 32082 sgd_solver.cpp:137] Iteration 42800, lr = 0.0033125, m = 0.9
I0628 19:29:46.891454 32082 solver.cpp:349] Iteration 42900 (58.2245 iter/s, 1.71749s/100 iter), loss = 0.0180744
I0628 19:29:46.891479 32082 solver.cpp:371]     Train net output #0: loss = 0.0180744 (* 1 = 0.0180744 loss)
I0628 19:29:46.891482 32082 sgd_solver.cpp:137] Iteration 42900, lr = 0.00329687, m = 0.9
I0628 19:29:48.010766 32050 data_reader.cpp:262] Starting prefetch of epoch 55
I0628 19:29:48.594993 32082 solver.cpp:401] Sparsity after update:
I0628 19:29:48.596055 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:29:48.596061 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:29:48.596068 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:29:48.596072 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:29:48.596076 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:29:48.596081 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:29:48.596084 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:29:48.596088 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:29:48.596091 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:29:48.596096 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:29:48.596101 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:29:48.596104 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:29:48.596107 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:29:48.596117 32082 solver.cpp:545] Iteration 43000, Testing net (#0)
I0628 19:29:49.594431 32080 data_reader.cpp:262] Starting prefetch of epoch 43
I0628 19:29:49.614636 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9072
I0628 19:29:49.614650 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9952
I0628 19:29:49.614655 32082 solver.cpp:630]     Test net output #2: loss = 0.32724 (* 1 = 0.32724 loss)
I0628 19:29:49.614668 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01875s
I0628 19:29:49.635653 32082 solver.cpp:349] Iteration 43000 (36.4339 iter/s, 2.7447s/100 iter), loss = 0.01558
I0628 19:29:49.635679 32082 solver.cpp:371]     Train net output #0: loss = 0.01558 (* 1 = 0.01558 loss)
I0628 19:29:49.635685 32082 sgd_solver.cpp:137] Iteration 43000, lr = 0.00328125, m = 0.9
I0628 19:29:51.355626 32082 solver.cpp:349] Iteration 43100 (58.1308 iter/s, 1.72026s/100 iter), loss = 0.00520991
I0628 19:29:51.355654 32082 solver.cpp:371]     Train net output #0: loss = 0.00520995 (* 1 = 0.00520995 loss)
I0628 19:29:51.355659 32082 sgd_solver.cpp:137] Iteration 43100, lr = 0.00326563, m = 0.9
I0628 19:29:53.077832 32082 solver.cpp:349] Iteration 43200 (58.0556 iter/s, 1.72249s/100 iter), loss = 0.0088419
I0628 19:29:53.077858 32082 solver.cpp:371]     Train net output #0: loss = 0.00884195 (* 1 = 0.00884195 loss)
I0628 19:29:53.077865 32082 sgd_solver.cpp:137] Iteration 43200, lr = 0.00325, m = 0.9
I0628 19:29:54.799535 32082 solver.cpp:349] Iteration 43300 (58.0727 iter/s, 1.72198s/100 iter), loss = 0.00762213
I0628 19:29:54.799561 32082 solver.cpp:371]     Train net output #0: loss = 0.00762218 (* 1 = 0.00762218 loss)
I0628 19:29:54.799567 32082 sgd_solver.cpp:137] Iteration 43300, lr = 0.00323438, m = 0.9
I0628 19:29:56.516005 32082 solver.cpp:349] Iteration 43400 (58.2499 iter/s, 1.71674s/100 iter), loss = 0.00712828
I0628 19:29:56.516047 32082 solver.cpp:371]     Train net output #0: loss = 0.00712833 (* 1 = 0.00712833 loss)
I0628 19:29:56.516052 32082 sgd_solver.cpp:137] Iteration 43400, lr = 0.00321875, m = 0.9
I0628 19:29:58.234392 32082 solver.cpp:349] Iteration 43500 (58.1856 iter/s, 1.71864s/100 iter), loss = 0.010086
I0628 19:29:58.234416 32082 solver.cpp:371]     Train net output #0: loss = 0.0100861 (* 1 = 0.0100861 loss)
I0628 19:29:58.234421 32082 sgd_solver.cpp:137] Iteration 43500, lr = 0.00320312, m = 0.9
I0628 19:29:59.951349 32082 solver.cpp:349] Iteration 43600 (58.2335 iter/s, 1.71722s/100 iter), loss = 0.00716173
I0628 19:29:59.951375 32082 solver.cpp:371]     Train net output #0: loss = 0.00716178 (* 1 = 0.00716178 loss)
I0628 19:29:59.951381 32082 sgd_solver.cpp:137] Iteration 43600, lr = 0.0031875, m = 0.9
I0628 19:30:01.672484 32082 solver.cpp:349] Iteration 43700 (58.0923 iter/s, 1.7214s/100 iter), loss = 0.00528269
I0628 19:30:01.672511 32082 solver.cpp:371]     Train net output #0: loss = 0.00528274 (* 1 = 0.00528274 loss)
I0628 19:30:01.672518 32082 sgd_solver.cpp:137] Iteration 43700, lr = 0.00317187, m = 0.9
I0628 19:30:02.462254 32050 data_reader.cpp:262] Starting prefetch of epoch 56
I0628 19:30:03.391921 32082 solver.cpp:349] Iteration 43800 (58.1499 iter/s, 1.71969s/100 iter), loss = 0.0104525
I0628 19:30:03.391947 32082 solver.cpp:371]     Train net output #0: loss = 0.0104526 (* 1 = 0.0104526 loss)
I0628 19:30:03.391952 32082 sgd_solver.cpp:137] Iteration 43800, lr = 0.00315625, m = 0.9
I0628 19:30:05.112218 32082 solver.cpp:349] Iteration 43900 (58.1209 iter/s, 1.72055s/100 iter), loss = 0.005248
I0628 19:30:05.112243 32082 solver.cpp:371]     Train net output #0: loss = 0.00524806 (* 1 = 0.00524806 loss)
I0628 19:30:05.112249 32082 sgd_solver.cpp:137] Iteration 43900, lr = 0.00314062, m = 0.9
I0628 19:30:06.816552 32082 solver.cpp:401] Sparsity after update:
I0628 19:30:06.817664 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:30:06.817672 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:30:06.817677 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:30:06.817679 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:30:06.817682 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:30:06.817683 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:30:06.817685 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:30:06.817689 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:30:06.817692 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:30:06.817692 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:30:06.817694 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:30:06.817697 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:30:06.817698 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:30:06.817705 32082 solver.cpp:545] Iteration 44000, Testing net (#0)
I0628 19:30:07.816700 32080 data_reader.cpp:262] Starting prefetch of epoch 44
I0628 19:30:07.841500 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9078
I0628 19:30:07.841513 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9952
I0628 19:30:07.841518 32082 solver.cpp:630]     Test net output #2: loss = 0.327823 (* 1 = 0.327823 loss)
I0628 19:30:07.841531 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.024s
I0628 19:30:07.858922 32082 solver.cpp:349] Iteration 44000 (36.4016 iter/s, 2.74713s/100 iter), loss = 0.0145247
I0628 19:30:07.858945 32082 solver.cpp:371]     Train net output #0: loss = 0.0145247 (* 1 = 0.0145247 loss)
I0628 19:30:07.858948 32082 sgd_solver.cpp:137] Iteration 44000, lr = 0.003125, m = 0.9
I0628 19:30:09.577044 32082 solver.cpp:349] Iteration 44100 (58.1945 iter/s, 1.71837s/100 iter), loss = 0.013836
I0628 19:30:09.577066 32082 solver.cpp:371]     Train net output #0: loss = 0.013836 (* 1 = 0.013836 loss)
I0628 19:30:09.577070 32082 sgd_solver.cpp:137] Iteration 44100, lr = 0.00310938, m = 0.9
I0628 19:30:11.296895 32082 solver.cpp:349] Iteration 44200 (58.1363 iter/s, 1.7201s/100 iter), loss = 0.00296077
I0628 19:30:11.296936 32082 solver.cpp:371]     Train net output #0: loss = 0.00296084 (* 1 = 0.00296084 loss)
I0628 19:30:11.296943 32082 sgd_solver.cpp:137] Iteration 44200, lr = 0.00309375, m = 0.9
I0628 19:30:13.015236 32082 solver.cpp:349] Iteration 44300 (58.1882 iter/s, 1.71856s/100 iter), loss = 0.00231409
I0628 19:30:13.015303 32082 solver.cpp:371]     Train net output #0: loss = 0.00231415 (* 1 = 0.00231415 loss)
I0628 19:30:13.015308 32082 sgd_solver.cpp:137] Iteration 44300, lr = 0.00307812, m = 0.9
I0628 19:30:14.743685 32082 solver.cpp:349] Iteration 44400 (57.8488 iter/s, 1.72864s/100 iter), loss = 0.00645771
I0628 19:30:14.743706 32082 solver.cpp:371]     Train net output #0: loss = 0.00645777 (* 1 = 0.00645777 loss)
I0628 19:30:14.743710 32082 sgd_solver.cpp:137] Iteration 44400, lr = 0.0030625, m = 0.9
I0628 19:30:16.466238 32082 solver.cpp:349] Iteration 44500 (58.0454 iter/s, 1.72279s/100 iter), loss = 0.00843079
I0628 19:30:16.466260 32082 solver.cpp:371]     Train net output #0: loss = 0.00843085 (* 1 = 0.00843085 loss)
I0628 19:30:16.466266 32082 sgd_solver.cpp:137] Iteration 44500, lr = 0.00304687, m = 0.9
I0628 19:30:16.947729 32050 data_reader.cpp:262] Starting prefetch of epoch 57
I0628 19:30:18.186101 32082 solver.cpp:349] Iteration 44600 (58.1364 iter/s, 1.72009s/100 iter), loss = 0.00357081
I0628 19:30:18.186121 32082 solver.cpp:371]     Train net output #0: loss = 0.00357087 (* 1 = 0.00357087 loss)
I0628 19:30:18.186125 32082 sgd_solver.cpp:137] Iteration 44600, lr = 0.00303125, m = 0.9
I0628 19:30:19.904723 32082 solver.cpp:349] Iteration 44700 (58.1783 iter/s, 1.71885s/100 iter), loss = 0.00318138
I0628 19:30:19.904749 32082 solver.cpp:371]     Train net output #0: loss = 0.00318144 (* 1 = 0.00318144 loss)
I0628 19:30:19.904754 32082 sgd_solver.cpp:137] Iteration 44700, lr = 0.00301562, m = 0.9
I0628 19:30:21.623790 32082 solver.cpp:349] Iteration 44800 (58.1637 iter/s, 1.71929s/100 iter), loss = 0.0193758
I0628 19:30:21.623811 32082 solver.cpp:371]     Train net output #0: loss = 0.0193759 (* 1 = 0.0193759 loss)
I0628 19:30:21.623814 32082 sgd_solver.cpp:137] Iteration 44800, lr = 0.003, m = 0.9
I0628 19:30:23.345237 32082 solver.cpp:349] Iteration 44900 (58.0831 iter/s, 1.72167s/100 iter), loss = 0.00643501
I0628 19:30:23.345259 32082 solver.cpp:371]     Train net output #0: loss = 0.00643507 (* 1 = 0.00643507 loss)
I0628 19:30:23.345263 32082 sgd_solver.cpp:137] Iteration 44900, lr = 0.00298437, m = 0.9
I0628 19:30:25.044358 32082 solver.cpp:401] Sparsity after update:
I0628 19:30:25.045480 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:30:25.045486 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:30:25.045492 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:30:25.045495 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:30:25.045497 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:30:25.045498 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:30:25.045500 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:30:25.045502 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:30:25.045506 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:30:25.045507 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:30:25.045511 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:30:25.045512 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:30:25.045514 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:30:25.045521 32082 solver.cpp:545] Iteration 45000, Testing net (#0)
I0628 19:30:26.044845 32080 data_reader.cpp:262] Starting prefetch of epoch 45
I0628 19:30:26.065284 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.909
I0628 19:30:26.065300 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.995
I0628 19:30:26.065305 32082 solver.cpp:630]     Test net output #2: loss = 0.326119 (* 1 = 0.326119 loss)
I0628 19:30:26.065321 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01995s
I0628 19:30:26.082873 32082 solver.cpp:349] Iteration 45000 (36.5229 iter/s, 2.73801s/100 iter), loss = 0.00913076
I0628 19:30:26.082896 32082 solver.cpp:371]     Train net output #0: loss = 0.00913083 (* 1 = 0.00913083 loss)
I0628 19:30:26.082902 32082 sgd_solver.cpp:137] Iteration 45000, lr = 0.00296875, m = 0.9
I0628 19:30:27.801398 32082 solver.cpp:349] Iteration 45100 (58.1824 iter/s, 1.71873s/100 iter), loss = 0.0139378
I0628 19:30:27.801434 32082 solver.cpp:371]     Train net output #0: loss = 0.0139378 (* 1 = 0.0139378 loss)
I0628 19:30:27.801439 32082 sgd_solver.cpp:137] Iteration 45100, lr = 0.00295313, m = 0.9
I0628 19:30:29.526639 32082 solver.cpp:349] Iteration 45200 (57.9564 iter/s, 1.72544s/100 iter), loss = 0.0170134
I0628 19:30:29.526661 32082 solver.cpp:371]     Train net output #0: loss = 0.0170135 (* 1 = 0.0170135 loss)
I0628 19:30:29.526665 32082 sgd_solver.cpp:137] Iteration 45200, lr = 0.0029375, m = 0.9
I0628 19:30:31.245883 32082 solver.cpp:349] Iteration 45300 (58.1582 iter/s, 1.71945s/100 iter), loss = 0.00509872
I0628 19:30:31.245905 32082 solver.cpp:371]     Train net output #0: loss = 0.00509879 (* 1 = 0.00509879 loss)
I0628 19:30:31.245909 32082 sgd_solver.cpp:137] Iteration 45300, lr = 0.00292188, m = 0.9
I0628 19:30:31.400969 32050 data_reader.cpp:262] Starting prefetch of epoch 58
I0628 19:30:32.968406 32082 solver.cpp:349] Iteration 45400 (58.0476 iter/s, 1.72273s/100 iter), loss = 0.00380326
I0628 19:30:32.968430 32082 solver.cpp:371]     Train net output #0: loss = 0.00380332 (* 1 = 0.00380332 loss)
I0628 19:30:32.968435 32082 sgd_solver.cpp:137] Iteration 45400, lr = 0.00290625, m = 0.9
I0628 19:30:34.688326 32082 solver.cpp:349] Iteration 45500 (58.1357 iter/s, 1.72011s/100 iter), loss = 0.00527788
I0628 19:30:34.688352 32082 solver.cpp:371]     Train net output #0: loss = 0.00527794 (* 1 = 0.00527794 loss)
I0628 19:30:34.688359 32082 sgd_solver.cpp:137] Iteration 45500, lr = 0.00289063, m = 0.9
I0628 19:30:36.412231 32082 solver.cpp:349] Iteration 45600 (58.0015 iter/s, 1.72409s/100 iter), loss = 0.00687773
I0628 19:30:36.412253 32082 solver.cpp:371]     Train net output #0: loss = 0.00687779 (* 1 = 0.00687779 loss)
I0628 19:30:36.412258 32082 sgd_solver.cpp:137] Iteration 45600, lr = 0.002875, m = 0.9
I0628 19:30:38.131016 32082 solver.cpp:349] Iteration 45700 (58.1742 iter/s, 1.71898s/100 iter), loss = 0.0133891
I0628 19:30:38.131043 32082 solver.cpp:371]     Train net output #0: loss = 0.0133892 (* 1 = 0.0133892 loss)
I0628 19:30:38.131048 32082 sgd_solver.cpp:137] Iteration 45700, lr = 0.00285937, m = 0.9
I0628 19:30:39.849946 32082 solver.cpp:349] Iteration 45800 (58.1697 iter/s, 1.71911s/100 iter), loss = 0.00459244
I0628 19:30:39.849972 32082 solver.cpp:371]     Train net output #0: loss = 0.0045925 (* 1 = 0.0045925 loss)
I0628 19:30:39.849978 32082 sgd_solver.cpp:137] Iteration 45800, lr = 0.00284375, m = 0.9
I0628 19:30:41.568409 32082 solver.cpp:349] Iteration 45900 (58.1856 iter/s, 1.71864s/100 iter), loss = 0.00529667
I0628 19:30:41.568436 32082 solver.cpp:371]     Train net output #0: loss = 0.00529673 (* 1 = 0.00529673 loss)
I0628 19:30:41.568442 32082 sgd_solver.cpp:137] Iteration 45900, lr = 0.00282812, m = 0.9
I0628 19:30:43.268610 32082 solver.cpp:401] Sparsity after update:
I0628 19:30:43.269696 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:30:43.269704 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:30:43.269713 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:30:43.269717 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:30:43.269721 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:30:43.269726 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:30:43.269728 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:30:43.269733 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:30:43.269737 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:30:43.269740 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:30:43.269744 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:30:43.269748 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:30:43.269752 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:30:43.269762 32082 solver.cpp:545] Iteration 46000, Testing net (#0)
I0628 19:30:44.271152 32080 data_reader.cpp:262] Starting prefetch of epoch 46
I0628 19:30:44.291303 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9092
I0628 19:30:44.291316 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9948
I0628 19:30:44.291322 32082 solver.cpp:630]     Test net output #2: loss = 0.32822 (* 1 = 0.32822 loss)
I0628 19:30:44.291338 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.0217s
I0628 19:30:44.309070 32082 solver.cpp:349] Iteration 46000 (36.4835 iter/s, 2.74096s/100 iter), loss = 0.00551848
I0628 19:30:44.309098 32082 solver.cpp:371]     Train net output #0: loss = 0.00551854 (* 1 = 0.00551854 loss)
I0628 19:30:44.309103 32082 sgd_solver.cpp:137] Iteration 46000, lr = 0.0028125, m = 0.9
I0628 19:30:45.858000 32050 data_reader.cpp:262] Starting prefetch of epoch 59
I0628 19:30:46.029335 32082 solver.cpp:349] Iteration 46100 (58.1249 iter/s, 1.72043s/100 iter), loss = 0.00680064
I0628 19:30:46.029353 32082 solver.cpp:371]     Train net output #0: loss = 0.0068007 (* 1 = 0.0068007 loss)
I0628 19:30:46.029357 32082 sgd_solver.cpp:137] Iteration 46100, lr = 0.00279688, m = 0.9
I0628 19:30:47.747095 32082 solver.cpp:349] Iteration 46200 (58.2094 iter/s, 1.71793s/100 iter), loss = 0.0103332
I0628 19:30:47.747122 32082 solver.cpp:371]     Train net output #0: loss = 0.0103333 (* 1 = 0.0103333 loss)
I0628 19:30:47.747128 32082 sgd_solver.cpp:137] Iteration 46200, lr = 0.00278125, m = 0.9
I0628 19:30:49.467548 32082 solver.cpp:349] Iteration 46300 (58.1188 iter/s, 1.72061s/100 iter), loss = 0.00277061
I0628 19:30:49.467574 32082 solver.cpp:371]     Train net output #0: loss = 0.00277067 (* 1 = 0.00277067 loss)
I0628 19:30:49.467581 32082 sgd_solver.cpp:137] Iteration 46300, lr = 0.00276563, m = 0.9
I0628 19:30:51.189524 32082 solver.cpp:349] Iteration 46400 (58.0675 iter/s, 1.72213s/100 iter), loss = 0.0107746
I0628 19:30:51.189550 32082 solver.cpp:371]     Train net output #0: loss = 0.0107747 (* 1 = 0.0107747 loss)
I0628 19:30:51.189558 32082 sgd_solver.cpp:137] Iteration 46400, lr = 0.00275, m = 0.9
I0628 19:30:52.907680 32082 solver.cpp:349] Iteration 46500 (58.1968 iter/s, 1.71831s/100 iter), loss = 0.00777216
I0628 19:30:52.907702 32082 solver.cpp:371]     Train net output #0: loss = 0.00777223 (* 1 = 0.00777223 loss)
I0628 19:30:52.907706 32082 sgd_solver.cpp:137] Iteration 46500, lr = 0.00273437, m = 0.9
I0628 19:30:54.626277 32082 solver.cpp:349] Iteration 46600 (58.1816 iter/s, 1.71876s/100 iter), loss = 0.00644095
I0628 19:30:54.626302 32082 solver.cpp:371]     Train net output #0: loss = 0.00644101 (* 1 = 0.00644101 loss)
I0628 19:30:54.626308 32082 sgd_solver.cpp:137] Iteration 46600, lr = 0.00271875, m = 0.9
I0628 19:30:56.346366 32082 solver.cpp:349] Iteration 46700 (58.1315 iter/s, 1.72024s/100 iter), loss = 0.00683091
I0628 19:30:56.346387 32082 solver.cpp:371]     Train net output #0: loss = 0.00683097 (* 1 = 0.00683097 loss)
I0628 19:30:56.346391 32082 sgd_solver.cpp:137] Iteration 46700, lr = 0.00270312, m = 0.9
I0628 19:30:58.065832 32082 solver.cpp:349] Iteration 46800 (58.1525 iter/s, 1.71962s/100 iter), loss = 0.0041775
I0628 19:30:58.065872 32082 solver.cpp:371]     Train net output #0: loss = 0.00417756 (* 1 = 0.00417756 loss)
I0628 19:30:58.065878 32082 sgd_solver.cpp:137] Iteration 46800, lr = 0.0026875, m = 0.9
I0628 19:30:59.284381 32050 data_reader.cpp:262] Starting prefetch of epoch 60
I0628 19:30:59.781443 32082 solver.cpp:349] Iteration 46900 (58.2839 iter/s, 1.71574s/100 iter), loss = 0.0119092
I0628 19:30:59.781467 32082 solver.cpp:371]     Train net output #0: loss = 0.0119093 (* 1 = 0.0119093 loss)
I0628 19:30:59.781471 32082 sgd_solver.cpp:137] Iteration 46900, lr = 0.00267187, m = 0.9
I0628 19:31:01.484293 32082 solver.cpp:401] Sparsity after update:
I0628 19:31:01.485360 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:31:01.485368 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:31:01.485375 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:31:01.485378 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:31:01.485379 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:31:01.485381 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:31:01.485384 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:31:01.485386 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:31:01.485388 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:31:01.485391 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:31:01.485394 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:31:01.485395 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:31:01.485397 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:31:01.485404 32082 solver.cpp:545] Iteration 47000, Testing net (#0)
I0628 19:31:02.481218 32080 data_reader.cpp:262] Starting prefetch of epoch 47
I0628 19:31:02.503768 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9082
I0628 19:31:02.503782 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.995
I0628 19:31:02.503787 32082 solver.cpp:630]     Test net output #2: loss = 0.33079 (* 1 = 0.33079 loss)
I0628 19:31:02.503801 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.0185s
I0628 19:31:02.521303 32082 solver.cpp:349] Iteration 47000 (36.4948 iter/s, 2.74011s/100 iter), loss = 0.00554575
I0628 19:31:02.521327 32082 solver.cpp:371]     Train net output #0: loss = 0.0055458 (* 1 = 0.0055458 loss)
I0628 19:31:02.521330 32082 sgd_solver.cpp:137] Iteration 47000, lr = 0.00265625, m = 0.9
I0628 19:31:04.243083 32082 solver.cpp:349] Iteration 47100 (58.0747 iter/s, 1.72192s/100 iter), loss = 0.00557048
I0628 19:31:04.243103 32082 solver.cpp:371]     Train net output #0: loss = 0.00557053 (* 1 = 0.00557053 loss)
I0628 19:31:04.243108 32082 sgd_solver.cpp:137] Iteration 47100, lr = 0.00264063, m = 0.9
I0628 19:31:05.964433 32082 solver.cpp:349] Iteration 47200 (58.0892 iter/s, 1.72149s/100 iter), loss = 0.00727172
I0628 19:31:05.964458 32082 solver.cpp:371]     Train net output #0: loss = 0.00727178 (* 1 = 0.00727178 loss)
I0628 19:31:05.964463 32082 sgd_solver.cpp:137] Iteration 47200, lr = 0.002625, m = 0.9
I0628 19:31:07.690390 32082 solver.cpp:349] Iteration 47300 (57.9345 iter/s, 1.72609s/100 iter), loss = 0.00347837
I0628 19:31:07.690413 32082 solver.cpp:371]     Train net output #0: loss = 0.00347843 (* 1 = 0.00347843 loss)
I0628 19:31:07.690418 32082 sgd_solver.cpp:137] Iteration 47300, lr = 0.00260938, m = 0.9
I0628 19:31:09.412642 32082 solver.cpp:349] Iteration 47400 (58.0591 iter/s, 1.72238s/100 iter), loss = 0.00957891
I0628 19:31:09.412669 32082 solver.cpp:371]     Train net output #0: loss = 0.00957897 (* 1 = 0.00957897 loss)
I0628 19:31:09.412675 32082 sgd_solver.cpp:137] Iteration 47400, lr = 0.00259375, m = 0.9
I0628 19:31:11.134905 32082 solver.cpp:349] Iteration 47500 (58.0591 iter/s, 1.72238s/100 iter), loss = 0.00404905
I0628 19:31:11.134932 32082 solver.cpp:371]     Train net output #0: loss = 0.0040491 (* 1 = 0.0040491 loss)
I0628 19:31:11.134938 32082 sgd_solver.cpp:137] Iteration 47500, lr = 0.00257812, m = 0.9
I0628 19:31:12.858428 32082 solver.cpp:349] Iteration 47600 (58.0201 iter/s, 1.72354s/100 iter), loss = 0.00737942
I0628 19:31:12.858453 32082 solver.cpp:371]     Train net output #0: loss = 0.00737948 (* 1 = 0.00737948 loss)
I0628 19:31:12.858459 32082 sgd_solver.cpp:137] Iteration 47600, lr = 0.0025625, m = 0.9
I0628 19:31:13.770386 32050 data_reader.cpp:262] Starting prefetch of epoch 61
I0628 19:31:14.576947 32082 solver.cpp:349] Iteration 47700 (58.1857 iter/s, 1.71863s/100 iter), loss = 0.00309651
I0628 19:31:14.576972 32082 solver.cpp:371]     Train net output #0: loss = 0.00309657 (* 1 = 0.00309657 loss)
I0628 19:31:14.576977 32082 sgd_solver.cpp:137] Iteration 47700, lr = 0.00254687, m = 0.9
I0628 19:31:16.295563 32082 solver.cpp:349] Iteration 47800 (58.1825 iter/s, 1.71873s/100 iter), loss = 0.00687449
I0628 19:31:16.295588 32082 solver.cpp:371]     Train net output #0: loss = 0.00687455 (* 1 = 0.00687455 loss)
I0628 19:31:16.295593 32082 sgd_solver.cpp:137] Iteration 47800, lr = 0.00253125, m = 0.9
I0628 19:31:18.012522 32082 solver.cpp:349] Iteration 47900 (58.2387 iter/s, 1.71707s/100 iter), loss = 0.0064046
I0628 19:31:18.012544 32082 solver.cpp:371]     Train net output #0: loss = 0.00640465 (* 1 = 0.00640465 loss)
I0628 19:31:18.012548 32082 sgd_solver.cpp:137] Iteration 47900, lr = 0.00251562, m = 0.9
I0628 19:31:19.724723 32082 solver.cpp:401] Sparsity after update:
I0628 19:31:19.725774 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:31:19.725781 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:31:19.725788 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:31:19.725790 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:31:19.725793 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:31:19.725795 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:31:19.725797 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:31:19.725800 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:31:19.725802 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:31:19.725805 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:31:19.725806 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:31:19.725808 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:31:19.725811 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:31:19.725818 32082 solver.cpp:545] Iteration 48000, Testing net (#0)
I0628 19:31:20.720028 32080 data_reader.cpp:262] Starting prefetch of epoch 48
I0628 19:31:20.746903 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.909
I0628 19:31:20.746927 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9944
I0628 19:31:20.746932 32082 solver.cpp:630]     Test net output #2: loss = 0.330493 (* 1 = 0.330493 loss)
I0628 19:31:20.746948 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02121s
I0628 19:31:20.764407 32082 solver.cpp:349] Iteration 48000 (36.336 iter/s, 2.75209s/100 iter), loss = 0.0110237
I0628 19:31:20.764425 32082 solver.cpp:371]     Train net output #0: loss = 0.0110238 (* 1 = 0.0110238 loss)
I0628 19:31:20.764430 32082 sgd_solver.cpp:137] Iteration 48000, lr = 0.0025, m = 0.9
I0628 19:31:22.486985 32082 solver.cpp:349] Iteration 48100 (58.0488 iter/s, 1.72269s/100 iter), loss = 0.012843
I0628 19:31:22.487010 32082 solver.cpp:371]     Train net output #0: loss = 0.012843 (* 1 = 0.012843 loss)
I0628 19:31:22.487016 32082 sgd_solver.cpp:137] Iteration 48100, lr = 0.00248438, m = 0.9
I0628 19:31:24.203696 32082 solver.cpp:349] Iteration 48200 (58.2477 iter/s, 1.71681s/100 iter), loss = 0.0037552
I0628 19:31:24.203719 32082 solver.cpp:371]     Train net output #0: loss = 0.00375525 (* 1 = 0.00375525 loss)
I0628 19:31:24.203725 32082 sgd_solver.cpp:137] Iteration 48200, lr = 0.00246875, m = 0.9
I0628 19:31:25.921236 32082 solver.cpp:349] Iteration 48300 (58.2196 iter/s, 1.71764s/100 iter), loss = 0.00390803
I0628 19:31:25.921262 32082 solver.cpp:371]     Train net output #0: loss = 0.00390808 (* 1 = 0.00390808 loss)
I0628 19:31:25.921267 32082 sgd_solver.cpp:137] Iteration 48300, lr = 0.00245313, m = 0.9
I0628 19:31:27.646749 32082 solver.cpp:349] Iteration 48400 (57.9506 iter/s, 1.72561s/100 iter), loss = 0.00473092
I0628 19:31:27.646775 32082 solver.cpp:371]     Train net output #0: loss = 0.00473096 (* 1 = 0.00473096 loss)
I0628 19:31:27.646781 32082 sgd_solver.cpp:137] Iteration 48400, lr = 0.0024375, m = 0.9
I0628 19:31:28.232884 32050 data_reader.cpp:262] Starting prefetch of epoch 62
I0628 19:31:29.365824 32082 solver.cpp:349] Iteration 48500 (58.1678 iter/s, 1.71916s/100 iter), loss = 0.00767589
I0628 19:31:29.365849 32082 solver.cpp:371]     Train net output #0: loss = 0.00767594 (* 1 = 0.00767594 loss)
I0628 19:31:29.365854 32082 sgd_solver.cpp:137] Iteration 48500, lr = 0.00242188, m = 0.9
I0628 19:31:31.085402 32082 solver.cpp:349] Iteration 48600 (58.1508 iter/s, 1.71967s/100 iter), loss = 0.0031787
I0628 19:31:31.085427 32082 solver.cpp:371]     Train net output #0: loss = 0.00317875 (* 1 = 0.00317875 loss)
I0628 19:31:31.085433 32082 sgd_solver.cpp:137] Iteration 48600, lr = 0.00240625, m = 0.9
I0628 19:31:32.806442 32082 solver.cpp:349] Iteration 48700 (58.1015 iter/s, 1.72113s/100 iter), loss = 0.00741158
I0628 19:31:32.806469 32082 solver.cpp:371]     Train net output #0: loss = 0.00741163 (* 1 = 0.00741163 loss)
I0628 19:31:32.806475 32082 sgd_solver.cpp:137] Iteration 48700, lr = 0.00239062, m = 0.9
I0628 19:31:34.526782 32082 solver.cpp:349] Iteration 48800 (58.1253 iter/s, 1.72042s/100 iter), loss = 0.00253744
I0628 19:31:34.526803 32082 solver.cpp:371]     Train net output #0: loss = 0.00253749 (* 1 = 0.00253749 loss)
I0628 19:31:34.526808 32082 sgd_solver.cpp:137] Iteration 48800, lr = 0.002375, m = 0.9
I0628 19:31:36.245029 32082 solver.cpp:349] Iteration 48900 (58.1959 iter/s, 1.71833s/100 iter), loss = 0.0036672
I0628 19:31:36.245052 32082 solver.cpp:371]     Train net output #0: loss = 0.00366725 (* 1 = 0.00366725 loss)
I0628 19:31:36.245056 32082 sgd_solver.cpp:137] Iteration 48900, lr = 0.00235937, m = 0.9
I0628 19:31:37.945616 32082 solver.cpp:401] Sparsity after update:
I0628 19:31:37.946679 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:31:37.946687 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:31:37.946696 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:31:37.946701 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:31:37.946704 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:31:37.946708 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:31:37.946712 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:31:37.946717 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:31:37.946720 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:31:37.946724 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:31:37.946727 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:31:37.946732 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:31:37.946735 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:31:37.946745 32082 solver.cpp:545] Iteration 49000, Testing net (#0)
I0628 19:31:38.944047 32080 data_reader.cpp:262] Starting prefetch of epoch 49
I0628 19:31:38.965816 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9084
I0628 19:31:38.965827 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9954
I0628 19:31:38.965832 32082 solver.cpp:630]     Test net output #2: loss = 0.329053 (* 1 = 0.329053 loss)
I0628 19:31:38.965847 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01917s
I0628 19:31:38.983292 32082 solver.cpp:349] Iteration 49000 (36.5174 iter/s, 2.73842s/100 iter), loss = 0.00571343
I0628 19:31:38.983319 32082 solver.cpp:371]     Train net output #0: loss = 0.00571348 (* 1 = 0.00571348 loss)
I0628 19:31:38.983325 32082 sgd_solver.cpp:137] Iteration 49000, lr = 0.00234375, m = 0.9
I0628 19:31:40.704135 32082 solver.cpp:349] Iteration 49100 (58.1087 iter/s, 1.72091s/100 iter), loss = 0.00505917
I0628 19:31:40.704161 32082 solver.cpp:371]     Train net output #0: loss = 0.00505921 (* 1 = 0.00505921 loss)
I0628 19:31:40.704167 32082 sgd_solver.cpp:137] Iteration 49100, lr = 0.00232813, m = 0.9
I0628 19:31:42.421161 32082 solver.cpp:349] Iteration 49200 (58.2379 iter/s, 1.71709s/100 iter), loss = 0.00428611
I0628 19:31:42.421186 32082 solver.cpp:371]     Train net output #0: loss = 0.00428616 (* 1 = 0.00428616 loss)
I0628 19:31:42.421192 32082 sgd_solver.cpp:137] Iteration 49200, lr = 0.0023125, m = 0.9
I0628 19:31:42.679154 32050 data_reader.cpp:262] Starting prefetch of epoch 63
I0628 19:31:44.139909 32082 solver.cpp:349] Iteration 49300 (58.1803 iter/s, 1.71879s/100 iter), loss = 0.00565032
I0628 19:31:44.139995 32082 solver.cpp:371]     Train net output #0: loss = 0.00565036 (* 1 = 0.00565036 loss)
I0628 19:31:44.140002 32082 sgd_solver.cpp:137] Iteration 49300, lr = 0.00229687, m = 0.9
I0628 19:31:45.858863 32082 solver.cpp:349] Iteration 49400 (58.1749 iter/s, 1.71896s/100 iter), loss = 0.00440927
I0628 19:31:45.858891 32082 solver.cpp:371]     Train net output #0: loss = 0.00440932 (* 1 = 0.00440932 loss)
I0628 19:31:45.858896 32082 sgd_solver.cpp:137] Iteration 49400, lr = 0.00228125, m = 0.9
I0628 19:31:47.576354 32082 solver.cpp:349] Iteration 49500 (58.2224 iter/s, 1.71755s/100 iter), loss = 0.0094743
I0628 19:31:47.576380 32082 solver.cpp:371]     Train net output #0: loss = 0.00947434 (* 1 = 0.00947434 loss)
I0628 19:31:47.576385 32082 sgd_solver.cpp:137] Iteration 49500, lr = 0.00226562, m = 0.9
I0628 19:31:49.298944 32082 solver.cpp:349] Iteration 49600 (58.0501 iter/s, 1.72265s/100 iter), loss = 0.0118733
I0628 19:31:49.298966 32082 solver.cpp:371]     Train net output #0: loss = 0.0118734 (* 1 = 0.0118734 loss)
I0628 19:31:49.298970 32082 sgd_solver.cpp:137] Iteration 49600, lr = 0.00225, m = 0.9
I0628 19:31:51.018218 32082 solver.cpp:349] Iteration 49700 (58.162 iter/s, 1.71934s/100 iter), loss = 0.0119982
I0628 19:31:51.018244 32082 solver.cpp:371]     Train net output #0: loss = 0.0119982 (* 1 = 0.0119982 loss)
I0628 19:31:51.018249 32082 sgd_solver.cpp:137] Iteration 49700, lr = 0.00223437, m = 0.9
I0628 19:31:52.740928 32082 solver.cpp:349] Iteration 49800 (58.0462 iter/s, 1.72276s/100 iter), loss = 0.00718952
I0628 19:31:52.740952 32082 solver.cpp:371]     Train net output #0: loss = 0.00718956 (* 1 = 0.00718956 loss)
I0628 19:31:52.740957 32082 sgd_solver.cpp:137] Iteration 49800, lr = 0.00221875, m = 0.9
I0628 19:31:54.463337 32082 solver.cpp:349] Iteration 49900 (58.0563 iter/s, 1.72247s/100 iter), loss = 0.00158542
I0628 19:31:54.463362 32082 solver.cpp:371]     Train net output #0: loss = 0.00158547 (* 1 = 0.00158547 loss)
I0628 19:31:54.463368 32082 sgd_solver.cpp:137] Iteration 49900, lr = 0.00220312, m = 0.9
I0628 19:31:56.112381 32050 data_reader.cpp:262] Starting prefetch of epoch 64
I0628 19:31:56.163964 32082 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_50000.caffemodel
I0628 19:31:56.172559 32082 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_50000.solverstate
I0628 19:31:56.176095 32082 solver.cpp:401] Sparsity after update:
I0628 19:31:56.177222 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:31:56.177229 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:31:56.177235 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:31:56.177237 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:31:56.177239 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:31:56.177242 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:31:56.177243 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:31:56.177245 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:31:56.177248 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:31:56.177249 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:31:56.177250 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:31:56.177253 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:31:56.177254 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:31:56.177263 32082 solver.cpp:545] Iteration 50000, Testing net (#0)
I0628 19:31:57.175150 32080 data_reader.cpp:262] Starting prefetch of epoch 50
I0628 19:31:57.195279 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9078
I0628 19:31:57.195291 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9944
I0628 19:31:57.195297 32082 solver.cpp:630]     Test net output #2: loss = 0.337476 (* 1 = 0.337476 loss)
I0628 19:31:57.195310 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.0181s
I0628 19:31:57.212707 32082 solver.cpp:349] Iteration 50000 (36.3706 iter/s, 2.74947s/100 iter), loss = 0.00362595
I0628 19:31:57.212733 32082 solver.cpp:371]     Train net output #0: loss = 0.003626 (* 1 = 0.003626 loss)
I0628 19:31:57.212739 32082 sgd_solver.cpp:137] Iteration 50000, lr = 0.0021875, m = 0.9
I0628 19:31:58.930752 32082 solver.cpp:349] Iteration 50100 (58.2042 iter/s, 1.71809s/100 iter), loss = 0.0023176
I0628 19:31:58.930778 32082 solver.cpp:371]     Train net output #0: loss = 0.00231765 (* 1 = 0.00231765 loss)
I0628 19:31:58.930783 32082 sgd_solver.cpp:137] Iteration 50100, lr = 0.00217188, m = 0.9
I0628 19:32:00.649725 32082 solver.cpp:349] Iteration 50200 (58.1729 iter/s, 1.71901s/100 iter), loss = 0.0106033
I0628 19:32:00.649751 32082 solver.cpp:371]     Train net output #0: loss = 0.0106033 (* 1 = 0.0106033 loss)
I0628 19:32:00.649756 32082 sgd_solver.cpp:137] Iteration 50200, lr = 0.00215625, m = 0.9
I0628 19:32:02.368999 32082 solver.cpp:349] Iteration 50300 (58.1628 iter/s, 1.71931s/100 iter), loss = 0.0102234
I0628 19:32:02.369019 32082 solver.cpp:371]     Train net output #0: loss = 0.0102235 (* 1 = 0.0102235 loss)
I0628 19:32:02.369022 32082 sgd_solver.cpp:137] Iteration 50300, lr = 0.00214063, m = 0.9
I0628 19:32:04.086170 32082 solver.cpp:349] Iteration 50400 (58.2337 iter/s, 1.71722s/100 iter), loss = 0.00746123
I0628 19:32:04.086195 32082 solver.cpp:371]     Train net output #0: loss = 0.00746127 (* 1 = 0.00746127 loss)
I0628 19:32:04.086200 32082 sgd_solver.cpp:137] Iteration 50400, lr = 0.002125, m = 0.9
I0628 19:32:05.804507 32082 solver.cpp:349] Iteration 50500 (58.1946 iter/s, 1.71837s/100 iter), loss = 0.00221503
I0628 19:32:05.804530 32082 solver.cpp:371]     Train net output #0: loss = 0.00221508 (* 1 = 0.00221508 loss)
I0628 19:32:05.804535 32082 sgd_solver.cpp:137] Iteration 50500, lr = 0.00210937, m = 0.9
I0628 19:32:07.524612 32082 solver.cpp:349] Iteration 50600 (58.1348 iter/s, 1.72014s/100 iter), loss = 0.00134335
I0628 19:32:07.524636 32082 solver.cpp:371]     Train net output #0: loss = 0.00134339 (* 1 = 0.00134339 loss)
I0628 19:32:07.524639 32082 sgd_solver.cpp:137] Iteration 50600, lr = 0.00209375, m = 0.9
I0628 19:32:09.249861 32082 solver.cpp:349] Iteration 50700 (57.9616 iter/s, 1.72528s/100 iter), loss = 0.00947948
I0628 19:32:09.249917 32082 solver.cpp:371]     Train net output #0: loss = 0.00947953 (* 1 = 0.00947953 loss)
I0628 19:32:09.249935 32082 sgd_solver.cpp:137] Iteration 50700, lr = 0.00207812, m = 0.9
I0628 19:32:10.591418 32050 data_reader.cpp:262] Starting prefetch of epoch 65
I0628 19:32:10.968454 32082 solver.cpp:349] Iteration 50800 (58.188 iter/s, 1.71857s/100 iter), loss = 0.00238978
I0628 19:32:10.968482 32082 solver.cpp:371]     Train net output #0: loss = 0.00238982 (* 1 = 0.00238982 loss)
I0628 19:32:10.968487 32082 sgd_solver.cpp:137] Iteration 50800, lr = 0.0020625, m = 0.9
I0628 19:32:12.685891 32082 solver.cpp:349] Iteration 50900 (58.2255 iter/s, 1.71746s/100 iter), loss = 0.00464554
I0628 19:32:12.685919 32082 solver.cpp:371]     Train net output #0: loss = 0.00464558 (* 1 = 0.00464558 loss)
I0628 19:32:12.685925 32082 sgd_solver.cpp:137] Iteration 50900, lr = 0.00204687, m = 0.9
I0628 19:32:14.384325 32082 solver.cpp:401] Sparsity after update:
I0628 19:32:14.385426 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:32:14.385433 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:32:14.385439 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:32:14.385442 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:32:14.385443 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:32:14.385445 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:32:14.385447 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:32:14.385449 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:32:14.385452 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:32:14.385453 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:32:14.385455 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:32:14.385457 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:32:14.385458 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:32:14.385466 32082 solver.cpp:545] Iteration 51000, Testing net (#0)
I0628 19:32:15.381454 32080 data_reader.cpp:262] Starting prefetch of epoch 51
I0628 19:32:15.404444 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.91
I0628 19:32:15.404458 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.995
I0628 19:32:15.404461 32082 solver.cpp:630]     Test net output #2: loss = 0.330124 (* 1 = 0.330124 loss)
I0628 19:32:15.404475 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01904s
I0628 19:32:15.421845 32082 solver.cpp:349] Iteration 51000 (36.5495 iter/s, 2.73601s/100 iter), loss = 0.00721269
I0628 19:32:15.421871 32082 solver.cpp:371]     Train net output #0: loss = 0.00721274 (* 1 = 0.00721274 loss)
I0628 19:32:15.421876 32082 sgd_solver.cpp:137] Iteration 51000, lr = 0.00203125, m = 0.9
I0628 19:32:17.137773 32082 solver.cpp:349] Iteration 51100 (58.2768 iter/s, 1.71595s/100 iter), loss = 0.0102852
I0628 19:32:17.137796 32082 solver.cpp:371]     Train net output #0: loss = 0.0102853 (* 1 = 0.0102853 loss)
I0628 19:32:17.137801 32082 sgd_solver.cpp:137] Iteration 51100, lr = 0.00201563, m = 0.9
I0628 19:32:18.862146 32082 solver.cpp:349] Iteration 51200 (57.9914 iter/s, 1.72439s/100 iter), loss = 0.00549418
I0628 19:32:18.862174 32082 solver.cpp:371]     Train net output #0: loss = 0.00549423 (* 1 = 0.00549423 loss)
I0628 19:32:18.862180 32082 sgd_solver.cpp:137] Iteration 51200, lr = 0.002, m = 0.9
I0628 19:32:20.580976 32082 solver.cpp:349] Iteration 51300 (58.1787 iter/s, 1.71884s/100 iter), loss = 0.0055683
I0628 19:32:20.580998 32082 solver.cpp:371]     Train net output #0: loss = 0.00556834 (* 1 = 0.00556834 loss)
I0628 19:32:20.581002 32082 sgd_solver.cpp:137] Iteration 51300, lr = 0.00198438, m = 0.9
I0628 19:32:22.303722 32082 solver.cpp:349] Iteration 51400 (58.0462 iter/s, 1.72277s/100 iter), loss = 0.00289521
I0628 19:32:22.303743 32082 solver.cpp:371]     Train net output #0: loss = 0.00289525 (* 1 = 0.00289525 loss)
I0628 19:32:22.303747 32082 sgd_solver.cpp:137] Iteration 51400, lr = 0.00196875, m = 0.9
I0628 19:32:24.022572 32082 solver.cpp:349] Iteration 51500 (58.1779 iter/s, 1.71887s/100 iter), loss = 0.0050806
I0628 19:32:24.022625 32082 solver.cpp:371]     Train net output #0: loss = 0.00508065 (* 1 = 0.00508065 loss)
I0628 19:32:24.022639 32082 sgd_solver.cpp:137] Iteration 51500, lr = 0.00195312, m = 0.9
I0628 19:32:25.036737 32050 data_reader.cpp:262] Starting prefetch of epoch 66
I0628 19:32:25.744645 32082 solver.cpp:349] Iteration 51600 (58.0706 iter/s, 1.72204s/100 iter), loss = 0.00779561
I0628 19:32:25.744670 32082 solver.cpp:371]     Train net output #0: loss = 0.00779566 (* 1 = 0.00779566 loss)
I0628 19:32:25.744675 32082 sgd_solver.cpp:137] Iteration 51600, lr = 0.0019375, m = 0.9
I0628 19:32:27.464205 32082 solver.cpp:349] Iteration 51700 (58.1542 iter/s, 1.71957s/100 iter), loss = 0.00430424
I0628 19:32:27.464231 32082 solver.cpp:371]     Train net output #0: loss = 0.00430428 (* 1 = 0.00430428 loss)
I0628 19:32:27.464236 32082 sgd_solver.cpp:137] Iteration 51700, lr = 0.00192187, m = 0.9
I0628 19:32:29.181900 32082 solver.cpp:349] Iteration 51800 (58.2175 iter/s, 1.7177s/100 iter), loss = 0.00764516
I0628 19:32:29.181943 32082 solver.cpp:371]     Train net output #0: loss = 0.0076452 (* 1 = 0.0076452 loss)
I0628 19:32:29.181949 32082 sgd_solver.cpp:137] Iteration 51800, lr = 0.00190625, m = 0.9
I0628 19:32:30.900447 32082 solver.cpp:349] Iteration 51900 (58.1893 iter/s, 1.71853s/100 iter), loss = 0.00733223
I0628 19:32:30.900475 32082 solver.cpp:371]     Train net output #0: loss = 0.00733227 (* 1 = 0.00733227 loss)
I0628 19:32:30.900481 32082 sgd_solver.cpp:137] Iteration 51900, lr = 0.00189062, m = 0.9
I0628 19:32:32.601670 32082 solver.cpp:401] Sparsity after update:
I0628 19:32:32.602761 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:32:32.602768 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:32:32.602777 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:32:32.602782 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:32:32.602785 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:32:32.602790 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:32:32.602793 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:32:32.602797 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:32:32.602800 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:32:32.602804 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:32:32.602808 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:32:32.602811 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:32:32.602815 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:32:32.602828 32082 solver.cpp:545] Iteration 52000, Testing net (#0)
I0628 19:32:33.600270 32080 data_reader.cpp:262] Starting prefetch of epoch 52
I0628 19:32:33.620359 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.91
I0628 19:32:33.620371 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.995
I0628 19:32:33.620380 32082 solver.cpp:630]     Test net output #2: loss = 0.331645 (* 1 = 0.331645 loss)
I0628 19:32:33.620398 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01787s
I0628 19:32:33.637749 32082 solver.cpp:349] Iteration 52000 (36.5258 iter/s, 2.73779s/100 iter), loss = 0.00607719
I0628 19:32:33.637775 32082 solver.cpp:371]     Train net output #0: loss = 0.00607723 (* 1 = 0.00607723 loss)
I0628 19:32:33.637781 32082 sgd_solver.cpp:137] Iteration 52000, lr = 0.001875, m = 0.9
I0628 19:32:35.355564 32082 solver.cpp:349] Iteration 52100 (58.1978 iter/s, 1.71828s/100 iter), loss = 0.00339066
I0628 19:32:35.355587 32082 solver.cpp:371]     Train net output #0: loss = 0.00339071 (* 1 = 0.00339071 loss)
I0628 19:32:35.355593 32082 sgd_solver.cpp:137] Iteration 52100, lr = 0.00185938, m = 0.9
I0628 19:32:37.072065 32082 solver.cpp:349] Iteration 52200 (58.2424 iter/s, 1.71696s/100 iter), loss = 0.00347627
I0628 19:32:37.072089 32082 solver.cpp:371]     Train net output #0: loss = 0.00347632 (* 1 = 0.00347632 loss)
I0628 19:32:37.072095 32082 sgd_solver.cpp:137] Iteration 52200, lr = 0.00184375, m = 0.9
I0628 19:32:38.788353 32082 solver.cpp:349] Iteration 52300 (58.2498 iter/s, 1.71674s/100 iter), loss = 0.00600969
I0628 19:32:38.788378 32082 solver.cpp:371]     Train net output #0: loss = 0.00600974 (* 1 = 0.00600974 loss)
I0628 19:32:38.788384 32082 sgd_solver.cpp:137] Iteration 52300, lr = 0.00182813, m = 0.9
I0628 19:32:39.477370 32050 data_reader.cpp:262] Starting prefetch of epoch 67
I0628 19:32:40.509044 32082 solver.cpp:349] Iteration 52400 (58.1009 iter/s, 1.72114s/100 iter), loss = 0.00371
I0628 19:32:40.509071 32082 solver.cpp:371]     Train net output #0: loss = 0.00371004 (* 1 = 0.00371004 loss)
I0628 19:32:40.509076 32082 sgd_solver.cpp:137] Iteration 52400, lr = 0.0018125, m = 0.9
I0628 19:32:42.230979 32082 solver.cpp:349] Iteration 52500 (58.0592 iter/s, 1.72238s/100 iter), loss = 0.010918
I0628 19:32:42.231003 32082 solver.cpp:371]     Train net output #0: loss = 0.010918 (* 1 = 0.010918 loss)
I0628 19:32:42.231006 32082 sgd_solver.cpp:137] Iteration 52500, lr = 0.00179687, m = 0.9
I0628 19:32:43.954047 32082 solver.cpp:349] Iteration 52600 (58.0214 iter/s, 1.7235s/100 iter), loss = 0.0080129
I0628 19:32:43.954072 32082 solver.cpp:371]     Train net output #0: loss = 0.00801295 (* 1 = 0.00801295 loss)
I0628 19:32:43.954079 32082 sgd_solver.cpp:137] Iteration 52600, lr = 0.00178125, m = 0.9
I0628 19:32:45.674789 32082 solver.cpp:349] Iteration 52700 (58.0997 iter/s, 1.72118s/100 iter), loss = 0.001916
I0628 19:32:45.674852 32082 solver.cpp:371]     Train net output #0: loss = 0.00191604 (* 1 = 0.00191604 loss)
I0628 19:32:45.674857 32082 sgd_solver.cpp:137] Iteration 52700, lr = 0.00176562, m = 0.9
I0628 19:32:47.397868 32082 solver.cpp:349] Iteration 52800 (58.0223 iter/s, 1.72347s/100 iter), loss = 0.00435365
I0628 19:32:47.397893 32082 solver.cpp:371]     Train net output #0: loss = 0.00435369 (* 1 = 0.00435369 loss)
I0628 19:32:47.397898 32082 sgd_solver.cpp:137] Iteration 52800, lr = 0.00175, m = 0.9
I0628 19:32:49.116621 32082 solver.cpp:349] Iteration 52900 (58.1672 iter/s, 1.71918s/100 iter), loss = 0.00373482
I0628 19:32:49.116647 32082 solver.cpp:371]     Train net output #0: loss = 0.00373487 (* 1 = 0.00373487 loss)
I0628 19:32:49.116652 32082 sgd_solver.cpp:137] Iteration 52900, lr = 0.00173437, m = 0.9
I0628 19:32:50.816567 32082 solver.cpp:401] Sparsity after update:
I0628 19:32:50.817733 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:32:50.817740 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:32:50.817745 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:32:50.817747 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:32:50.817749 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:32:50.817751 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:32:50.817754 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:32:50.817756 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:32:50.817759 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:32:50.817760 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:32:50.817762 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:32:50.817764 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:32:50.817766 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:32:50.817775 32082 solver.cpp:545] Iteration 53000, Testing net (#0)
I0628 19:32:51.813743 32080 data_reader.cpp:262] Starting prefetch of epoch 53
I0628 19:32:51.837363 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9106
I0628 19:32:51.837376 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.995
I0628 19:32:51.837381 32082 solver.cpp:630]     Test net output #2: loss = 0.331414 (* 1 = 0.331414 loss)
I0628 19:32:51.837396 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01989s
I0628 19:32:51.855115 32082 solver.cpp:349] Iteration 53000 (36.5071 iter/s, 2.73919s/100 iter), loss = 0.00817138
I0628 19:32:51.855140 32082 solver.cpp:371]     Train net output #0: loss = 0.00817143 (* 1 = 0.00817143 loss)
I0628 19:32:51.855146 32082 sgd_solver.cpp:137] Iteration 53000, lr = 0.00171875, m = 0.9
I0628 19:32:53.575655 32082 solver.cpp:349] Iteration 53100 (58.1072 iter/s, 1.72096s/100 iter), loss = 0.00802852
I0628 19:32:53.575677 32082 solver.cpp:371]     Train net output #0: loss = 0.00802857 (* 1 = 0.00802857 loss)
I0628 19:32:53.575681 32082 sgd_solver.cpp:137] Iteration 53100, lr = 0.00170313, m = 0.9
I0628 19:32:53.936532 32050 data_reader.cpp:262] Starting prefetch of epoch 68
I0628 19:32:55.293876 32082 solver.cpp:349] Iteration 53200 (58.1857 iter/s, 1.71864s/100 iter), loss = 0.00424734
I0628 19:32:55.293901 32082 solver.cpp:371]     Train net output #0: loss = 0.00424739 (* 1 = 0.00424739 loss)
I0628 19:32:55.293907 32082 sgd_solver.cpp:137] Iteration 53200, lr = 0.0016875, m = 0.9
I0628 19:32:57.018399 32082 solver.cpp:349] Iteration 53300 (57.9734 iter/s, 1.72493s/100 iter), loss = 0.00652922
I0628 19:32:57.018420 32082 solver.cpp:371]     Train net output #0: loss = 0.00652927 (* 1 = 0.00652927 loss)
I0628 19:32:57.018424 32082 sgd_solver.cpp:137] Iteration 53300, lr = 0.00167188, m = 0.9
I0628 19:32:58.737228 32082 solver.cpp:349] Iteration 53400 (58.1653 iter/s, 1.71924s/100 iter), loss = 0.00406245
I0628 19:32:58.737254 32082 solver.cpp:371]     Train net output #0: loss = 0.00406249 (* 1 = 0.00406249 loss)
I0628 19:32:58.737260 32082 sgd_solver.cpp:137] Iteration 53400, lr = 0.00165625, m = 0.9
I0628 19:33:00.453879 32082 solver.cpp:349] Iteration 53500 (58.2396 iter/s, 1.71704s/100 iter), loss = 0.00591737
I0628 19:33:00.453920 32082 solver.cpp:371]     Train net output #0: loss = 0.00591742 (* 1 = 0.00591742 loss)
I0628 19:33:00.453927 32082 sgd_solver.cpp:137] Iteration 53500, lr = 0.00164062, m = 0.9
I0628 19:33:02.174006 32082 solver.cpp:349] Iteration 53600 (58.1226 iter/s, 1.7205s/100 iter), loss = 0.00766969
I0628 19:33:02.174032 32082 solver.cpp:371]     Train net output #0: loss = 0.00766973 (* 1 = 0.00766973 loss)
I0628 19:33:02.174038 32082 sgd_solver.cpp:137] Iteration 53600, lr = 0.001625, m = 0.9
I0628 19:33:03.890817 32082 solver.cpp:349] Iteration 53700 (58.2345 iter/s, 1.7172s/100 iter), loss = 0.00600879
I0628 19:33:03.890843 32082 solver.cpp:371]     Train net output #0: loss = 0.00600883 (* 1 = 0.00600883 loss)
I0628 19:33:03.890848 32082 sgd_solver.cpp:137] Iteration 53700, lr = 0.00160937, m = 0.9
I0628 19:33:05.610847 32082 solver.cpp:349] Iteration 53800 (58.1256 iter/s, 1.72041s/100 iter), loss = 0.00573171
I0628 19:33:05.610872 32082 solver.cpp:371]     Train net output #0: loss = 0.00573176 (* 1 = 0.00573176 loss)
I0628 19:33:05.610877 32082 sgd_solver.cpp:137] Iteration 53800, lr = 0.00159375, m = 0.9
I0628 19:33:07.328786 32082 solver.cpp:349] Iteration 53900 (58.1965 iter/s, 1.71832s/100 iter), loss = 0.00416038
I0628 19:33:07.328809 32082 solver.cpp:371]     Train net output #0: loss = 0.00416043 (* 1 = 0.00416043 loss)
I0628 19:33:07.328814 32082 sgd_solver.cpp:137] Iteration 53900, lr = 0.00157812, m = 0.9
I0628 19:33:07.380633 32050 data_reader.cpp:262] Starting prefetch of epoch 69
I0628 19:33:09.030648 32082 solver.cpp:401] Sparsity after update:
I0628 19:33:09.031774 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:33:09.031781 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:33:09.031790 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:33:09.031791 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:33:09.031793 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:33:09.031796 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:33:09.031798 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:33:09.031800 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:33:09.031802 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:33:09.031805 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:33:09.031807 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:33:09.031810 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:33:09.031811 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:33:09.031818 32082 solver.cpp:545] Iteration 54000, Testing net (#0)
I0628 19:33:10.032057 32080 data_reader.cpp:262] Starting prefetch of epoch 54
I0628 19:33:10.052670 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9084
I0628 19:33:10.052682 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9952
I0628 19:33:10.052687 32082 solver.cpp:630]     Test net output #2: loss = 0.331596 (* 1 = 0.331596 loss)
I0628 19:33:10.052700 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02112s
I0628 19:33:10.070014 32082 solver.cpp:349] Iteration 54000 (36.4717 iter/s, 2.74185s/100 iter), loss = 0.00854644
I0628 19:33:10.070040 32082 solver.cpp:371]     Train net output #0: loss = 0.00854648 (* 1 = 0.00854648 loss)
I0628 19:33:10.070046 32082 sgd_solver.cpp:137] Iteration 54000, lr = 0.0015625, m = 0.9
I0628 19:33:11.791560 32082 solver.cpp:349] Iteration 54100 (58.075 iter/s, 1.72191s/100 iter), loss = 0.00860527
I0628 19:33:11.791585 32082 solver.cpp:371]     Train net output #0: loss = 0.00860532 (* 1 = 0.00860532 loss)
I0628 19:33:11.791591 32082 sgd_solver.cpp:137] Iteration 54100, lr = 0.00154688, m = 0.9
I0628 19:33:13.508347 32082 solver.cpp:349] Iteration 54200 (58.2361 iter/s, 1.71715s/100 iter), loss = 0.0046466
I0628 19:33:13.508368 32082 solver.cpp:371]     Train net output #0: loss = 0.00464664 (* 1 = 0.00464664 loss)
I0628 19:33:13.508373 32082 sgd_solver.cpp:137] Iteration 54200, lr = 0.00153125, m = 0.9
I0628 19:33:15.224827 32082 solver.cpp:349] Iteration 54300 (58.2472 iter/s, 1.71682s/100 iter), loss = 0.00634444
I0628 19:33:15.224848 32082 solver.cpp:371]     Train net output #0: loss = 0.00634448 (* 1 = 0.00634448 loss)
I0628 19:33:15.224854 32082 sgd_solver.cpp:137] Iteration 54300, lr = 0.00151563, m = 0.9
I0628 19:33:16.949244 32082 solver.cpp:349] Iteration 54400 (57.9786 iter/s, 1.72478s/100 iter), loss = 0.00482436
I0628 19:33:16.949334 32082 solver.cpp:371]     Train net output #0: loss = 0.0048244 (* 1 = 0.0048244 loss)
I0628 19:33:16.949340 32082 sgd_solver.cpp:137] Iteration 54400, lr = 0.0015, m = 0.9
I0628 19:33:18.667095 32082 solver.cpp:349] Iteration 54500 (58.2027 iter/s, 1.71813s/100 iter), loss = 0.00403198
I0628 19:33:18.667122 32082 solver.cpp:371]     Train net output #0: loss = 0.00403203 (* 1 = 0.00403203 loss)
I0628 19:33:18.667129 32082 sgd_solver.cpp:137] Iteration 54500, lr = 0.00148437, m = 0.9
I0628 19:33:20.386566 32082 solver.cpp:349] Iteration 54600 (58.1459 iter/s, 1.71981s/100 iter), loss = 0.00438608
I0628 19:33:20.386590 32082 solver.cpp:371]     Train net output #0: loss = 0.00438613 (* 1 = 0.00438613 loss)
I0628 19:33:20.386595 32082 sgd_solver.cpp:137] Iteration 54600, lr = 0.00146875, m = 0.9
I0628 19:33:21.831696 32050 data_reader.cpp:262] Starting prefetch of epoch 70
I0628 19:33:22.106022 32082 solver.cpp:349] Iteration 54700 (58.1464 iter/s, 1.7198s/100 iter), loss = 0.00291828
I0628 19:33:22.106048 32082 solver.cpp:371]     Train net output #0: loss = 0.00291832 (* 1 = 0.00291832 loss)
I0628 19:33:22.106055 32082 sgd_solver.cpp:137] Iteration 54700, lr = 0.00145312, m = 0.9
I0628 19:33:23.828219 32082 solver.cpp:349] Iteration 54800 (58.054 iter/s, 1.72253s/100 iter), loss = 0.0102088
I0628 19:33:23.828243 32082 solver.cpp:371]     Train net output #0: loss = 0.0102089 (* 1 = 0.0102089 loss)
I0628 19:33:23.828246 32082 sgd_solver.cpp:137] Iteration 54800, lr = 0.0014375, m = 0.9
I0628 19:33:25.544909 32082 solver.cpp:349] Iteration 54900 (58.2402 iter/s, 1.71703s/100 iter), loss = 0.00365792
I0628 19:33:25.544935 32082 solver.cpp:371]     Train net output #0: loss = 0.00365796 (* 1 = 0.00365796 loss)
I0628 19:33:25.544941 32082 sgd_solver.cpp:137] Iteration 54900, lr = 0.00142187, m = 0.9
I0628 19:33:27.246131 32082 solver.cpp:401] Sparsity after update:
I0628 19:33:27.247216 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:33:27.247223 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:33:27.247229 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:33:27.247231 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:33:27.247234 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:33:27.247236 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:33:27.247238 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:33:27.247241 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:33:27.247242 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:33:27.247244 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:33:27.247246 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:33:27.247248 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:33:27.247251 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:33:27.247256 32082 solver.cpp:545] Iteration 55000, Testing net (#0)
I0628 19:33:28.242290 32080 data_reader.cpp:262] Starting prefetch of epoch 55
I0628 19:33:28.266381 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.908
I0628 19:33:28.266394 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9956
I0628 19:33:28.266398 32082 solver.cpp:630]     Test net output #2: loss = 0.333397 (* 1 = 0.333397 loss)
I0628 19:33:28.266413 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01937s
I0628 19:33:28.283821 32082 solver.cpp:349] Iteration 55000 (36.5035 iter/s, 2.73946s/100 iter), loss = 0.00335355
I0628 19:33:28.283844 32082 solver.cpp:371]     Train net output #0: loss = 0.0033536 (* 1 = 0.0033536 loss)
I0628 19:33:28.283850 32082 sgd_solver.cpp:137] Iteration 55000, lr = 0.00140625, m = 0.9
I0628 19:33:30.004431 32082 solver.cpp:349] Iteration 55100 (58.1079 iter/s, 1.72094s/100 iter), loss = 0.00423073
I0628 19:33:30.004453 32082 solver.cpp:371]     Train net output #0: loss = 0.00423077 (* 1 = 0.00423077 loss)
I0628 19:33:30.004457 32082 sgd_solver.cpp:137] Iteration 55100, lr = 0.00139063, m = 0.9
I0628 19:33:31.726378 32082 solver.cpp:349] Iteration 55200 (58.0629 iter/s, 1.72227s/100 iter), loss = 0.0089226
I0628 19:33:31.726420 32082 solver.cpp:371]     Train net output #0: loss = 0.00892264 (* 1 = 0.00892264 loss)
I0628 19:33:31.726426 32082 sgd_solver.cpp:137] Iteration 55200, lr = 0.001375, m = 0.9
I0628 19:33:33.446234 32082 solver.cpp:349] Iteration 55300 (58.1345 iter/s, 1.72015s/100 iter), loss = 0.00865376
I0628 19:33:33.446260 32082 solver.cpp:371]     Train net output #0: loss = 0.0086538 (* 1 = 0.0086538 loss)
I0628 19:33:33.446267 32082 sgd_solver.cpp:137] Iteration 55300, lr = 0.00135938, m = 0.9
I0628 19:33:35.160712 32082 solver.cpp:349] Iteration 55400 (58.3165 iter/s, 1.71478s/100 iter), loss = 0.0122648
I0628 19:33:35.160737 32082 solver.cpp:371]     Train net output #0: loss = 0.0122648 (* 1 = 0.0122648 loss)
I0628 19:33:35.160742 32082 sgd_solver.cpp:137] Iteration 55400, lr = 0.00134375, m = 0.9
I0628 19:33:36.277487 32050 data_reader.cpp:262] Starting prefetch of epoch 71
I0628 19:33:36.878252 32082 solver.cpp:349] Iteration 55500 (58.2125 iter/s, 1.71784s/100 iter), loss = 0.00849211
I0628 19:33:36.878276 32082 solver.cpp:371]     Train net output #0: loss = 0.00849216 (* 1 = 0.00849216 loss)
I0628 19:33:36.878281 32082 sgd_solver.cpp:137] Iteration 55500, lr = 0.00132813, m = 0.9
I0628 19:33:38.597296 32082 solver.cpp:349] Iteration 55600 (58.1616 iter/s, 1.71935s/100 iter), loss = 0.00394615
I0628 19:33:38.597321 32082 solver.cpp:371]     Train net output #0: loss = 0.0039462 (* 1 = 0.0039462 loss)
I0628 19:33:38.597326 32082 sgd_solver.cpp:137] Iteration 55600, lr = 0.0013125, m = 0.9
I0628 19:33:40.315601 32082 solver.cpp:349] Iteration 55700 (58.1861 iter/s, 1.71862s/100 iter), loss = 0.00622475
I0628 19:33:40.315625 32082 solver.cpp:371]     Train net output #0: loss = 0.00622479 (* 1 = 0.00622479 loss)
I0628 19:33:40.315630 32082 sgd_solver.cpp:137] Iteration 55700, lr = 0.00129687, m = 0.9
I0628 19:33:42.046097 32082 solver.cpp:349] Iteration 55800 (57.7762 iter/s, 1.73082s/100 iter), loss = 0.00557415
I0628 19:33:42.046120 32082 solver.cpp:371]     Train net output #0: loss = 0.0055742 (* 1 = 0.0055742 loss)
I0628 19:33:42.046128 32082 sgd_solver.cpp:137] Iteration 55800, lr = 0.00128125, m = 0.9
I0628 19:33:43.763483 32082 solver.cpp:349] Iteration 55900 (58.2175 iter/s, 1.7177s/100 iter), loss = 0.00502246
I0628 19:33:43.763509 32082 solver.cpp:371]     Train net output #0: loss = 0.00502251 (* 1 = 0.00502251 loss)
I0628 19:33:43.763515 32082 sgd_solver.cpp:137] Iteration 55900, lr = 0.00126562, m = 0.9
I0628 19:33:45.467501 32082 solver.cpp:401] Sparsity after update:
I0628 19:33:45.468544 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:33:45.468551 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:33:45.468559 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:33:45.468564 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:33:45.468566 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:33:45.468569 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:33:45.468574 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:33:45.468577 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:33:45.468581 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:33:45.468585 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:33:45.468588 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:33:45.468592 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:33:45.468597 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:33:45.468607 32082 solver.cpp:545] Iteration 56000, Testing net (#0)
I0628 19:33:46.463043 32080 data_reader.cpp:262] Starting prefetch of epoch 56
I0628 19:33:46.489207 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9108
I0628 19:33:46.489223 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9954
I0628 19:33:46.489230 32082 solver.cpp:630]     Test net output #2: loss = 0.331827 (* 1 = 0.331827 loss)
I0628 19:33:46.489248 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02084s
I0628 19:33:46.506772 32082 solver.cpp:349] Iteration 56000 (36.4457 iter/s, 2.74381s/100 iter), loss = 0.00610609
I0628 19:33:46.506791 32082 solver.cpp:371]     Train net output #0: loss = 0.00610613 (* 1 = 0.00610613 loss)
I0628 19:33:46.506796 32082 sgd_solver.cpp:137] Iteration 56000, lr = 0.00125, m = 0.9
I0628 19:33:48.225307 32082 solver.cpp:349] Iteration 56100 (58.1786 iter/s, 1.71884s/100 iter), loss = 0.0059128
I0628 19:33:48.225378 32082 solver.cpp:371]     Train net output #0: loss = 0.00591285 (* 1 = 0.00591285 loss)
I0628 19:33:48.225391 32082 sgd_solver.cpp:137] Iteration 56100, lr = 0.00123438, m = 0.9
I0628 19:33:49.941817 32082 solver.cpp:349] Iteration 56200 (58.2497 iter/s, 1.71675s/100 iter), loss = 0.00310108
I0628 19:33:49.941843 32082 solver.cpp:371]     Train net output #0: loss = 0.00310112 (* 1 = 0.00310112 loss)
I0628 19:33:49.941849 32082 sgd_solver.cpp:137] Iteration 56200, lr = 0.00121875, m = 0.9
I0628 19:33:50.733929 32050 data_reader.cpp:262] Starting prefetch of epoch 72
I0628 19:33:51.661631 32082 solver.cpp:349] Iteration 56300 (58.1359 iter/s, 1.72011s/100 iter), loss = 0.00477077
I0628 19:33:51.661658 32082 solver.cpp:371]     Train net output #0: loss = 0.00477082 (* 1 = 0.00477082 loss)
I0628 19:33:51.661664 32082 sgd_solver.cpp:137] Iteration 56300, lr = 0.00120313, m = 0.9
I0628 19:33:53.388430 32082 solver.cpp:349] Iteration 56400 (57.9009 iter/s, 1.72709s/100 iter), loss = 0.00270256
I0628 19:33:53.388451 32082 solver.cpp:371]     Train net output #0: loss = 0.00270261 (* 1 = 0.00270261 loss)
I0628 19:33:53.388458 32082 sgd_solver.cpp:137] Iteration 56400, lr = 0.0011875, m = 0.9
I0628 19:33:55.108036 32082 solver.cpp:349] Iteration 56500 (58.143 iter/s, 1.7199s/100 iter), loss = 0.00875451
I0628 19:33:55.108062 32082 solver.cpp:371]     Train net output #0: loss = 0.00875456 (* 1 = 0.00875456 loss)
I0628 19:33:55.108067 32082 sgd_solver.cpp:137] Iteration 56500, lr = 0.00117187, m = 0.9
I0628 19:33:56.824056 32082 solver.cpp:349] Iteration 56600 (58.2648 iter/s, 1.7163s/100 iter), loss = 0.00605158
I0628 19:33:56.824082 32082 solver.cpp:371]     Train net output #0: loss = 0.00605163 (* 1 = 0.00605163 loss)
I0628 19:33:56.824087 32082 sgd_solver.cpp:137] Iteration 56600, lr = 0.00115625, m = 0.9
I0628 19:33:58.541224 32082 solver.cpp:349] Iteration 56700 (58.2261 iter/s, 1.71744s/100 iter), loss = 0.00180055
I0628 19:33:58.541249 32082 solver.cpp:371]     Train net output #0: loss = 0.0018006 (* 1 = 0.0018006 loss)
I0628 19:33:58.541255 32082 sgd_solver.cpp:137] Iteration 56700, lr = 0.00114062, m = 0.9
I0628 19:34:00.263839 32082 solver.cpp:349] Iteration 56800 (58.042 iter/s, 1.72289s/100 iter), loss = 0.00327673
I0628 19:34:00.263861 32082 solver.cpp:371]     Train net output #0: loss = 0.00327678 (* 1 = 0.00327678 loss)
I0628 19:34:00.263865 32082 sgd_solver.cpp:137] Iteration 56800, lr = 0.001125, m = 0.9
I0628 19:34:01.981704 32082 solver.cpp:349] Iteration 56900 (58.2023 iter/s, 1.71814s/100 iter), loss = 0.0072868
I0628 19:34:01.981725 32082 solver.cpp:371]     Train net output #0: loss = 0.00728685 (* 1 = 0.00728685 loss)
I0628 19:34:01.981729 32082 sgd_solver.cpp:137] Iteration 56900, lr = 0.00110937, m = 0.9
I0628 19:34:03.681535 32082 solver.cpp:401] Sparsity after update:
I0628 19:34:03.682577 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:34:03.682585 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:34:03.682595 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:34:03.682600 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:34:03.682603 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:34:03.682607 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:34:03.682611 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:34:03.682615 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:34:03.682618 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:34:03.682622 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:34:03.682626 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:34:03.682631 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:34:03.682633 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:34:03.682644 32082 solver.cpp:545] Iteration 57000, Testing net (#0)
I0628 19:34:04.676272 32080 data_reader.cpp:262] Starting prefetch of epoch 57
I0628 19:34:04.703379 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.909
I0628 19:34:04.703397 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9954
I0628 19:34:04.703418 32082 solver.cpp:630]     Test net output #2: loss = 0.332614 (* 1 = 0.332614 loss)
I0628 19:34:04.703436 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02097s
I0628 19:34:04.721128 32082 solver.cpp:349] Iteration 57000 (36.4979 iter/s, 2.73988s/100 iter), loss = 0.00524302
I0628 19:34:04.721148 32082 solver.cpp:371]     Train net output #0: loss = 0.00524307 (* 1 = 0.00524307 loss)
I0628 19:34:04.721153 32082 sgd_solver.cpp:137] Iteration 57000, lr = 0.00109375, m = 0.9
I0628 19:34:05.202630 32050 data_reader.cpp:262] Starting prefetch of epoch 73
I0628 19:34:06.440837 32082 solver.cpp:349] Iteration 57100 (58.1403 iter/s, 1.71998s/100 iter), loss = 0.00144296
I0628 19:34:06.440860 32082 solver.cpp:371]     Train net output #0: loss = 0.001443 (* 1 = 0.001443 loss)
I0628 19:34:06.440865 32082 sgd_solver.cpp:137] Iteration 57100, lr = 0.00107813, m = 0.9
I0628 19:34:08.161502 32082 solver.cpp:349] Iteration 57200 (58.1082 iter/s, 1.72093s/100 iter), loss = 0.00379741
I0628 19:34:08.161523 32082 solver.cpp:371]     Train net output #0: loss = 0.00379746 (* 1 = 0.00379746 loss)
I0628 19:34:08.161527 32082 sgd_solver.cpp:137] Iteration 57200, lr = 0.0010625, m = 0.9
I0628 19:34:09.879668 32082 solver.cpp:349] Iteration 57300 (58.1927 iter/s, 1.71843s/100 iter), loss = 0.0110087
I0628 19:34:09.879695 32082 solver.cpp:371]     Train net output #0: loss = 0.0110087 (* 1 = 0.0110087 loss)
I0628 19:34:09.879701 32082 sgd_solver.cpp:137] Iteration 57300, lr = 0.00104688, m = 0.9
I0628 19:34:11.597204 32082 solver.cpp:349] Iteration 57400 (58.2145 iter/s, 1.71779s/100 iter), loss = 0.00756564
I0628 19:34:11.597229 32082 solver.cpp:371]     Train net output #0: loss = 0.00756569 (* 1 = 0.00756569 loss)
I0628 19:34:11.597235 32082 sgd_solver.cpp:137] Iteration 57400, lr = 0.00103125, m = 0.9
I0628 19:34:13.313872 32082 solver.cpp:349] Iteration 57500 (58.244 iter/s, 1.71692s/100 iter), loss = 0.0108518
I0628 19:34:13.313899 32082 solver.cpp:371]     Train net output #0: loss = 0.0108518 (* 1 = 0.0108518 loss)
I0628 19:34:13.313905 32082 sgd_solver.cpp:137] Iteration 57500, lr = 0.00101562, m = 0.9
I0628 19:34:15.032825 32082 solver.cpp:349] Iteration 57600 (58.1667 iter/s, 1.7192s/100 iter), loss = 0.00660956
I0628 19:34:15.032845 32082 solver.cpp:371]     Train net output #0: loss = 0.0066096 (* 1 = 0.0066096 loss)
I0628 19:34:15.032850 32082 sgd_solver.cpp:137] Iteration 57600, lr = 0.001, m = 0.9
I0628 19:34:16.750895 32082 solver.cpp:349] Iteration 57700 (58.1964 iter/s, 1.71832s/100 iter), loss = 0.00581368
I0628 19:34:16.750917 32082 solver.cpp:371]     Train net output #0: loss = 0.00581373 (* 1 = 0.00581373 loss)
I0628 19:34:16.750922 32082 sgd_solver.cpp:137] Iteration 57700, lr = 0.000984375, m = 0.9
I0628 19:34:18.468333 32082 solver.cpp:349] Iteration 57800 (58.2181 iter/s, 1.71768s/100 iter), loss = 0.00566295
I0628 19:34:18.468406 32082 solver.cpp:371]     Train net output #0: loss = 0.00566299 (* 1 = 0.00566299 loss)
I0628 19:34:18.468413 32082 sgd_solver.cpp:137] Iteration 57800, lr = 0.00096875, m = 0.9
I0628 19:34:18.623225 32050 data_reader.cpp:262] Starting prefetch of epoch 74
I0628 19:34:20.184119 32082 solver.cpp:349] Iteration 57900 (58.2762 iter/s, 1.71597s/100 iter), loss = 0.003471
I0628 19:34:20.184146 32082 solver.cpp:371]     Train net output #0: loss = 0.00347105 (* 1 = 0.00347105 loss)
I0628 19:34:20.184152 32082 sgd_solver.cpp:137] Iteration 57900, lr = 0.000953125, m = 0.9
I0628 19:34:21.885618 32082 solver.cpp:401] Sparsity after update:
I0628 19:34:21.886695 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:34:21.886703 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:34:21.886710 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:34:21.886713 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:34:21.886715 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:34:21.886718 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:34:21.886720 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:34:21.886723 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:34:21.886724 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:34:21.886726 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:34:21.886729 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:34:21.886730 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:34:21.886732 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:34:21.886739 32082 solver.cpp:545] Iteration 58000, Testing net (#0)
I0628 19:34:22.883785 32080 data_reader.cpp:262] Starting prefetch of epoch 58
I0628 19:34:22.905979 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9084
I0628 19:34:22.905992 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9956
I0628 19:34:22.905997 32082 solver.cpp:630]     Test net output #2: loss = 0.332114 (* 1 = 0.332114 loss)
I0628 19:34:22.906010 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01943s
I0628 19:34:22.923408 32082 solver.cpp:349] Iteration 58000 (36.5006 iter/s, 2.73968s/100 iter), loss = 0.00294096
I0628 19:34:22.923430 32082 solver.cpp:371]     Train net output #0: loss = 0.00294101 (* 1 = 0.00294101 loss)
I0628 19:34:22.923434 32082 sgd_solver.cpp:137] Iteration 58000, lr = 0.0009375, m = 0.9
I0628 19:34:24.639655 32082 solver.cpp:349] Iteration 58100 (58.2588 iter/s, 1.71648s/100 iter), loss = 0.00501284
I0628 19:34:24.639680 32082 solver.cpp:371]     Train net output #0: loss = 0.00501289 (* 1 = 0.00501289 loss)
I0628 19:34:24.639685 32082 sgd_solver.cpp:137] Iteration 58100, lr = 0.000921875, m = 0.9
I0628 19:34:26.354631 32082 solver.cpp:349] Iteration 58200 (58.3023 iter/s, 1.7152s/100 iter), loss = 0.00332411
I0628 19:34:26.354656 32082 solver.cpp:371]     Train net output #0: loss = 0.00332416 (* 1 = 0.00332416 loss)
I0628 19:34:26.354662 32082 sgd_solver.cpp:137] Iteration 58200, lr = 0.00090625, m = 0.9
I0628 19:34:28.075840 32082 solver.cpp:349] Iteration 58300 (58.0913 iter/s, 1.72143s/100 iter), loss = 0.0042586
I0628 19:34:28.075865 32082 solver.cpp:371]     Train net output #0: loss = 0.00425865 (* 1 = 0.00425865 loss)
I0628 19:34:28.075870 32082 sgd_solver.cpp:137] Iteration 58300, lr = 0.000890625, m = 0.9
I0628 19:34:29.794306 32082 solver.cpp:349] Iteration 58400 (58.1842 iter/s, 1.71868s/100 iter), loss = 0.00437587
I0628 19:34:29.794328 32082 solver.cpp:371]     Train net output #0: loss = 0.00437593 (* 1 = 0.00437593 loss)
I0628 19:34:29.794332 32082 sgd_solver.cpp:137] Iteration 58400, lr = 0.000875, m = 0.9
I0628 19:34:31.516248 32082 solver.cpp:349] Iteration 58500 (58.0667 iter/s, 1.72216s/100 iter), loss = 0.00467773
I0628 19:34:31.516273 32082 solver.cpp:371]     Train net output #0: loss = 0.00467778 (* 1 = 0.00467778 loss)
I0628 19:34:31.516278 32082 sgd_solver.cpp:137] Iteration 58500, lr = 0.000859375, m = 0.9
I0628 19:34:33.060557 32050 data_reader.cpp:262] Starting prefetch of epoch 75
I0628 19:34:33.232120 32082 solver.cpp:349] Iteration 58600 (58.2723 iter/s, 1.71608s/100 iter), loss = 0.00333279
I0628 19:34:33.232143 32082 solver.cpp:371]     Train net output #0: loss = 0.00333284 (* 1 = 0.00333284 loss)
I0628 19:34:33.232148 32082 sgd_solver.cpp:137] Iteration 58600, lr = 0.00084375, m = 0.9
I0628 19:34:34.947851 32082 solver.cpp:349] Iteration 58700 (58.2771 iter/s, 1.71594s/100 iter), loss = 0.0075526
I0628 19:34:34.947872 32082 solver.cpp:371]     Train net output #0: loss = 0.00755265 (* 1 = 0.00755265 loss)
I0628 19:34:34.947876 32082 sgd_solver.cpp:137] Iteration 58700, lr = 0.000828125, m = 0.9
I0628 19:34:36.665683 32082 solver.cpp:349] Iteration 58800 (58.2058 iter/s, 1.71804s/100 iter), loss = 0.00238037
I0628 19:34:36.665709 32082 solver.cpp:371]     Train net output #0: loss = 0.00238043 (* 1 = 0.00238043 loss)
I0628 19:34:36.665714 32082 sgd_solver.cpp:137] Iteration 58800, lr = 0.0008125, m = 0.9
I0628 19:34:38.383692 32082 solver.cpp:349] Iteration 58900 (58.2002 iter/s, 1.71821s/100 iter), loss = 0.0049039
I0628 19:34:38.383718 32082 solver.cpp:371]     Train net output #0: loss = 0.00490396 (* 1 = 0.00490396 loss)
I0628 19:34:38.383723 32082 sgd_solver.cpp:137] Iteration 58900, lr = 0.000796875, m = 0.9
I0628 19:34:40.086629 32082 solver.cpp:401] Sparsity after update:
I0628 19:34:40.087741 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:34:40.087749 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:34:40.087759 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:34:40.087764 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:34:40.087767 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:34:40.087771 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:34:40.087775 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:34:40.087779 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:34:40.087783 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:34:40.087787 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:34:40.087791 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:34:40.087795 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:34:40.087800 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:34:40.087810 32082 solver.cpp:545] Iteration 59000, Testing net (#0)
I0628 19:34:41.081414 32080 data_reader.cpp:262] Starting prefetch of epoch 59
I0628 19:34:41.105538 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9104
I0628 19:34:41.105557 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9952
I0628 19:34:41.105564 32082 solver.cpp:630]     Test net output #2: loss = 0.333245 (* 1 = 0.333245 loss)
I0628 19:34:41.105581 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01791s
I0628 19:34:41.123003 32082 solver.cpp:349] Iteration 59000 (36.5011 iter/s, 2.73965s/100 iter), loss = 0.0050142
I0628 19:34:41.123029 32082 solver.cpp:371]     Train net output #0: loss = 0.00501425 (* 1 = 0.00501425 loss)
I0628 19:34:41.123034 32082 sgd_solver.cpp:137] Iteration 59000, lr = 0.00078125, m = 0.9
I0628 19:34:42.846156 32082 solver.cpp:349] Iteration 59100 (58.0267 iter/s, 1.72334s/100 iter), loss = 0.00376047
I0628 19:34:42.846179 32082 solver.cpp:371]     Train net output #0: loss = 0.00376052 (* 1 = 0.00376052 loss)
I0628 19:34:42.846184 32082 sgd_solver.cpp:137] Iteration 59100, lr = 0.000765625, m = 0.9
I0628 19:34:44.566047 32082 solver.cpp:349] Iteration 59200 (58.1367 iter/s, 1.72008s/100 iter), loss = 0.00368143
I0628 19:34:44.566072 32082 solver.cpp:371]     Train net output #0: loss = 0.00368148 (* 1 = 0.00368148 loss)
I0628 19:34:44.566077 32082 sgd_solver.cpp:137] Iteration 59200, lr = 0.00075, m = 0.9
I0628 19:34:46.286880 32082 solver.cpp:349] Iteration 59300 (58.1052 iter/s, 1.72102s/100 iter), loss = 0.00295771
I0628 19:34:46.286906 32082 solver.cpp:371]     Train net output #0: loss = 0.00295776 (* 1 = 0.00295776 loss)
I0628 19:34:46.286911 32082 sgd_solver.cpp:137] Iteration 59300, lr = 0.000734375, m = 0.9
I0628 19:34:47.506872 32050 data_reader.cpp:262] Starting prefetch of epoch 76
I0628 19:34:48.005471 32082 solver.cpp:349] Iteration 59400 (58.1816 iter/s, 1.71876s/100 iter), loss = 0.0074082
I0628 19:34:48.005496 32082 solver.cpp:371]     Train net output #0: loss = 0.00740826 (* 1 = 0.00740826 loss)
I0628 19:34:48.005501 32082 sgd_solver.cpp:137] Iteration 59400, lr = 0.00071875, m = 0.9
I0628 19:34:49.723048 32082 solver.cpp:349] Iteration 59500 (58.2155 iter/s, 1.71776s/100 iter), loss = 0.00655531
I0628 19:34:49.723120 32082 solver.cpp:371]     Train net output #0: loss = 0.00655537 (* 1 = 0.00655537 loss)
I0628 19:34:49.723126 32082 sgd_solver.cpp:137] Iteration 59500, lr = 0.000703125, m = 0.9
I0628 19:34:51.440656 32082 solver.cpp:349] Iteration 59600 (58.2162 iter/s, 1.71773s/100 iter), loss = 0.00403642
I0628 19:34:51.440677 32082 solver.cpp:371]     Train net output #0: loss = 0.00403647 (* 1 = 0.00403647 loss)
I0628 19:34:51.440683 32082 sgd_solver.cpp:137] Iteration 59600, lr = 0.0006875, m = 0.9
I0628 19:34:53.161468 32082 solver.cpp:349] Iteration 59700 (58.1062 iter/s, 1.72099s/100 iter), loss = 0.0052365
I0628 19:34:53.161492 32082 solver.cpp:371]     Train net output #0: loss = 0.00523656 (* 1 = 0.00523656 loss)
I0628 19:34:53.161497 32082 sgd_solver.cpp:137] Iteration 59700, lr = 0.000671875, m = 0.9
I0628 19:34:54.881597 32082 solver.cpp:349] Iteration 59800 (58.1294 iter/s, 1.7203s/100 iter), loss = 0.00456959
I0628 19:34:54.881619 32082 solver.cpp:371]     Train net output #0: loss = 0.00456964 (* 1 = 0.00456964 loss)
I0628 19:34:54.881623 32082 sgd_solver.cpp:137] Iteration 59800, lr = 0.00065625, m = 0.9
I0628 19:34:56.611656 32082 solver.cpp:349] Iteration 59900 (57.7957 iter/s, 1.73023s/100 iter), loss = 0.00508308
I0628 19:34:56.611680 32082 solver.cpp:371]     Train net output #0: loss = 0.00508314 (* 1 = 0.00508314 loss)
I0628 19:34:56.611686 32082 sgd_solver.cpp:137] Iteration 59900, lr = 0.000640625, m = 0.9
I0628 19:34:58.311800 32082 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_60000.caffemodel
I0628 19:34:58.319614 32082 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_60000.solverstate
I0628 19:34:58.323061 32082 solver.cpp:401] Sparsity after update:
I0628 19:34:58.324194 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:34:58.324203 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:34:58.324209 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:34:58.324213 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:34:58.324214 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:34:58.324216 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:34:58.324219 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:34:58.324221 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:34:58.324223 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:34:58.324225 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:34:58.324228 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:34:58.324229 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:34:58.324232 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:34:58.324240 32082 solver.cpp:545] Iteration 60000, Testing net (#0)
I0628 19:34:59.325350 32080 data_reader.cpp:262] Starting prefetch of epoch 60
I0628 19:34:59.345502 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9096
I0628 19:34:59.345515 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9956
I0628 19:34:59.345520 32082 solver.cpp:630]     Test net output #2: loss = 0.332563 (* 1 = 0.332563 loss)
I0628 19:34:59.345532 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02141s
I0628 19:34:59.362996 32082 solver.cpp:349] Iteration 60000 (36.3421 iter/s, 2.75163s/100 iter), loss = 0.00193262
I0628 19:34:59.363020 32082 solver.cpp:371]     Train net output #0: loss = 0.00193268 (* 1 = 0.00193268 loss)
I0628 19:34:59.363024 32082 sgd_solver.cpp:137] Iteration 60000, lr = 0.000625, m = 0.9
I0628 19:35:01.079515 32082 solver.cpp:349] Iteration 60100 (58.252 iter/s, 1.71668s/100 iter), loss = 0.00833237
I0628 19:35:01.079540 32082 solver.cpp:371]     Train net output #0: loss = 0.00833242 (* 1 = 0.00833242 loss)
I0628 19:35:01.079546 32082 sgd_solver.cpp:137] Iteration 60100, lr = 0.000609375, m = 0.9
I0628 19:35:01.989837 32050 data_reader.cpp:262] Starting prefetch of epoch 77
I0628 19:35:02.797485 32082 solver.cpp:349] Iteration 60200 (58.203 iter/s, 1.71812s/100 iter), loss = 0.00399826
I0628 19:35:02.797513 32082 solver.cpp:371]     Train net output #0: loss = 0.00399831 (* 1 = 0.00399831 loss)
I0628 19:35:02.797518 32082 sgd_solver.cpp:137] Iteration 60200, lr = 0.00059375, m = 0.9
I0628 19:35:04.516842 32082 solver.cpp:349] Iteration 60300 (58.1562 iter/s, 1.71951s/100 iter), loss = 0.0044278
I0628 19:35:04.516866 32082 solver.cpp:371]     Train net output #0: loss = 0.00442785 (* 1 = 0.00442785 loss)
I0628 19:35:04.516872 32082 sgd_solver.cpp:137] Iteration 60300, lr = 0.000578125, m = 0.9
I0628 19:35:06.240447 32082 solver.cpp:349] Iteration 60400 (58.0129 iter/s, 1.72375s/100 iter), loss = 0.00541626
I0628 19:35:06.240473 32082 solver.cpp:371]     Train net output #0: loss = 0.00541632 (* 1 = 0.00541632 loss)
I0628 19:35:06.240479 32082 sgd_solver.cpp:137] Iteration 60400, lr = 0.0005625, m = 0.9
I0628 19:35:07.963264 32082 solver.cpp:349] Iteration 60500 (58.0395 iter/s, 1.72296s/100 iter), loss = 0.00467051
I0628 19:35:07.963286 32082 solver.cpp:371]     Train net output #0: loss = 0.00467056 (* 1 = 0.00467056 loss)
I0628 19:35:07.963291 32082 sgd_solver.cpp:137] Iteration 60500, lr = 0.000546875, m = 0.9
I0628 19:35:09.686712 32082 solver.cpp:349] Iteration 60600 (58.0182 iter/s, 1.7236s/100 iter), loss = 0.00577596
I0628 19:35:09.686735 32082 solver.cpp:371]     Train net output #0: loss = 0.00577602 (* 1 = 0.00577602 loss)
I0628 19:35:09.686740 32082 sgd_solver.cpp:137] Iteration 60600, lr = 0.00053125, m = 0.9
I0628 19:35:11.405675 32082 solver.cpp:349] Iteration 60700 (58.1698 iter/s, 1.71911s/100 iter), loss = 0.00359151
I0628 19:35:11.405701 32082 solver.cpp:371]     Train net output #0: loss = 0.00359157 (* 1 = 0.00359157 loss)
I0628 19:35:11.405707 32082 sgd_solver.cpp:137] Iteration 60700, lr = 0.000515625, m = 0.9
I0628 19:35:13.125854 32082 solver.cpp:349] Iteration 60800 (58.1289 iter/s, 1.72032s/100 iter), loss = 0.00308878
I0628 19:35:13.125880 32082 solver.cpp:371]     Train net output #0: loss = 0.00308884 (* 1 = 0.00308884 loss)
I0628 19:35:13.125885 32082 sgd_solver.cpp:137] Iteration 60800, lr = 0.0005, m = 0.9
I0628 19:35:14.845942 32082 solver.cpp:349] Iteration 60900 (58.132 iter/s, 1.72022s/100 iter), loss = 0.00377965
I0628 19:35:14.845966 32082 solver.cpp:371]     Train net output #0: loss = 0.00377971 (* 1 = 0.00377971 loss)
I0628 19:35:14.845973 32082 sgd_solver.cpp:137] Iteration 60900, lr = 0.000484375, m = 0.9
I0628 19:35:15.430310 32050 data_reader.cpp:262] Starting prefetch of epoch 78
I0628 19:35:16.545991 32082 solver.cpp:401] Sparsity after update:
I0628 19:35:16.547024 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:35:16.547030 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:35:16.547037 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:35:16.547039 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:35:16.547041 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:35:16.547044 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:35:16.547045 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:35:16.547047 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:35:16.547049 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:35:16.547050 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:35:16.547052 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:35:16.547055 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:35:16.547057 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:35:16.547065 32082 solver.cpp:545] Iteration 61000, Testing net (#0)
I0628 19:35:17.546247 32080 data_reader.cpp:262] Starting prefetch of epoch 61
I0628 19:35:17.567265 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9096
I0628 19:35:17.567276 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.995
I0628 19:35:17.567282 32082 solver.cpp:630]     Test net output #2: loss = 0.332143 (* 1 = 0.332143 loss)
I0628 19:35:17.567308 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.02034s
I0628 19:35:17.585119 32082 solver.cpp:349] Iteration 61000 (36.5042 iter/s, 2.73941s/100 iter), loss = 0.00957494
I0628 19:35:17.585146 32082 solver.cpp:371]     Train net output #0: loss = 0.00957499 (* 1 = 0.00957499 loss)
I0628 19:35:17.585152 32082 sgd_solver.cpp:137] Iteration 61000, lr = 0.00046875, m = 0.9
I0628 19:35:19.301733 32082 solver.cpp:349] Iteration 61100 (58.25 iter/s, 1.71674s/100 iter), loss = 0.00251508
I0628 19:35:19.301753 32082 solver.cpp:371]     Train net output #0: loss = 0.00251514 (* 1 = 0.00251514 loss)
I0628 19:35:19.301758 32082 sgd_solver.cpp:137] Iteration 61100, lr = 0.000453125, m = 0.9
I0628 19:35:21.021795 32082 solver.cpp:349] Iteration 61200 (58.1329 iter/s, 1.7202s/100 iter), loss = 0.00508208
I0628 19:35:21.021852 32082 solver.cpp:371]     Train net output #0: loss = 0.00508214 (* 1 = 0.00508214 loss)
I0628 19:35:21.021857 32082 sgd_solver.cpp:137] Iteration 61200, lr = 0.0004375, m = 0.9
I0628 19:35:22.741119 32082 solver.cpp:349] Iteration 61300 (58.1593 iter/s, 1.71942s/100 iter), loss = 0.00181793
I0628 19:35:22.741145 32082 solver.cpp:371]     Train net output #0: loss = 0.00181799 (* 1 = 0.00181799 loss)
I0628 19:35:22.741152 32082 sgd_solver.cpp:137] Iteration 61300, lr = 0.000421875, m = 0.9
I0628 19:35:24.459820 32082 solver.cpp:349] Iteration 61400 (58.1795 iter/s, 1.71882s/100 iter), loss = 0.00492441
I0628 19:35:24.459844 32082 solver.cpp:371]     Train net output #0: loss = 0.00492447 (* 1 = 0.00492447 loss)
I0628 19:35:24.459851 32082 sgd_solver.cpp:137] Iteration 61400, lr = 0.00040625, m = 0.9
I0628 19:35:26.183339 32082 solver.cpp:349] Iteration 61500 (58.0169 iter/s, 1.72364s/100 iter), loss = 0.00449921
I0628 19:35:26.183387 32082 solver.cpp:371]     Train net output #0: loss = 0.00449927 (* 1 = 0.00449927 loss)
I0628 19:35:26.183394 32082 sgd_solver.cpp:137] Iteration 61500, lr = 0.000390625, m = 0.9
I0628 19:35:27.904253 32082 solver.cpp:349] Iteration 61600 (58.1057 iter/s, 1.721s/100 iter), loss = 0.00539983
I0628 19:35:27.904279 32082 solver.cpp:371]     Train net output #0: loss = 0.00539989 (* 1 = 0.00539989 loss)
I0628 19:35:27.904285 32082 sgd_solver.cpp:137] Iteration 61600, lr = 0.000375, m = 0.9
I0628 19:35:29.626201 32082 solver.cpp:349] Iteration 61700 (58.07 iter/s, 1.72206s/100 iter), loss = 0.00357078
I0628 19:35:29.626227 32082 solver.cpp:371]     Train net output #0: loss = 0.00357084 (* 1 = 0.00357084 loss)
I0628 19:35:29.626232 32082 sgd_solver.cpp:137] Iteration 61700, lr = 0.000359375, m = 0.9
I0628 19:35:29.884251 32050 data_reader.cpp:262] Starting prefetch of epoch 79
I0628 19:35:31.349999 32082 solver.cpp:349] Iteration 61800 (58.0078 iter/s, 1.72391s/100 iter), loss = 0.00486238
I0628 19:35:31.350024 32082 solver.cpp:371]     Train net output #0: loss = 0.00486243 (* 1 = 0.00486243 loss)
I0628 19:35:31.350030 32082 sgd_solver.cpp:137] Iteration 61800, lr = 0.00034375, m = 0.9
I0628 19:35:33.067368 32082 solver.cpp:349] Iteration 61900 (58.225 iter/s, 1.71747s/100 iter), loss = 0.00267896
I0628 19:35:33.067392 32082 solver.cpp:371]     Train net output #0: loss = 0.00267902 (* 1 = 0.00267902 loss)
I0628 19:35:33.067399 32082 sgd_solver.cpp:137] Iteration 61900, lr = 0.000328125, m = 0.9
I0628 19:35:34.767946 32082 solver.cpp:401] Sparsity after update:
I0628 19:35:34.769017 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:35:34.769026 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:35:34.769031 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:35:34.769032 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:35:34.769034 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:35:34.769037 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:35:34.769038 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:35:34.769040 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:35:34.769042 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:35:34.769044 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:35:34.769048 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:35:34.769050 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:35:34.769052 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:35:34.769063 32082 solver.cpp:545] Iteration 62000, Testing net (#0)
I0628 19:35:35.766456 32080 data_reader.cpp:262] Starting prefetch of epoch 62
I0628 19:35:35.787864 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9094
I0628 19:35:35.787878 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9956
I0628 19:35:35.787883 32082 solver.cpp:630]     Test net output #2: loss = 0.332727 (* 1 = 0.332727 loss)
I0628 19:35:35.787897 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01892s
I0628 19:35:35.805236 32082 solver.cpp:349] Iteration 62000 (36.5222 iter/s, 2.73806s/100 iter), loss = 0.00664685
I0628 19:35:35.805285 32082 solver.cpp:371]     Train net output #0: loss = 0.00664691 (* 1 = 0.00664691 loss)
I0628 19:35:35.805289 32082 sgd_solver.cpp:137] Iteration 62000, lr = 0.0003125, m = 0.9
I0628 19:35:37.525030 32082 solver.cpp:349] Iteration 62100 (58.1438 iter/s, 1.71987s/100 iter), loss = 0.00701395
I0628 19:35:37.525054 32082 solver.cpp:371]     Train net output #0: loss = 0.00701401 (* 1 = 0.00701401 loss)
I0628 19:35:37.525060 32082 sgd_solver.cpp:137] Iteration 62100, lr = 0.000296875, m = 0.9
I0628 19:35:39.245071 32082 solver.cpp:349] Iteration 62200 (58.1348 iter/s, 1.72014s/100 iter), loss = 0.006533
I0628 19:35:39.245098 32082 solver.cpp:371]     Train net output #0: loss = 0.00653306 (* 1 = 0.00653306 loss)
I0628 19:35:39.245103 32082 sgd_solver.cpp:137] Iteration 62200, lr = 0.00028125, m = 0.9
I0628 19:35:40.962679 32082 solver.cpp:349] Iteration 62300 (58.2173 iter/s, 1.7177s/100 iter), loss = 0.00577083
I0628 19:35:40.962705 32082 solver.cpp:371]     Train net output #0: loss = 0.00577089 (* 1 = 0.00577089 loss)
I0628 19:35:40.962710 32082 sgd_solver.cpp:137] Iteration 62300, lr = 0.000265625, m = 0.9
I0628 19:35:42.682744 32082 solver.cpp:349] Iteration 62400 (58.1342 iter/s, 1.72016s/100 iter), loss = 0.000930602
I0628 19:35:42.682766 32082 solver.cpp:371]     Train net output #0: loss = 0.00093066 (* 1 = 0.00093066 loss)
I0628 19:35:42.682770 32082 sgd_solver.cpp:137] Iteration 62400, lr = 0.00025, m = 0.9
I0628 19:35:44.334462 32050 data_reader.cpp:262] Starting prefetch of epoch 80
I0628 19:35:44.403084 32082 solver.cpp:349] Iteration 62500 (58.1248 iter/s, 1.72044s/100 iter), loss = 0.00294777
I0628 19:35:44.403106 32082 solver.cpp:371]     Train net output #0: loss = 0.00294783 (* 1 = 0.00294783 loss)
I0628 19:35:44.403110 32082 sgd_solver.cpp:137] Iteration 62500, lr = 0.000234375, m = 0.9
I0628 19:35:46.125000 32082 solver.cpp:349] Iteration 62600 (58.0717 iter/s, 1.72201s/100 iter), loss = 0.00396825
I0628 19:35:46.125021 32082 solver.cpp:371]     Train net output #0: loss = 0.0039683 (* 1 = 0.0039683 loss)
I0628 19:35:46.125025 32082 sgd_solver.cpp:137] Iteration 62600, lr = 0.00021875, m = 0.9
I0628 19:35:47.849781 32082 solver.cpp:349] Iteration 62700 (57.9754 iter/s, 1.72487s/100 iter), loss = 0.00879229
I0628 19:35:47.849807 32082 solver.cpp:371]     Train net output #0: loss = 0.00879235 (* 1 = 0.00879235 loss)
I0628 19:35:47.849812 32082 sgd_solver.cpp:137] Iteration 62700, lr = 0.000203125, m = 0.9
I0628 19:35:49.567936 32082 solver.cpp:349] Iteration 62800 (58.1992 iter/s, 1.71824s/100 iter), loss = 0.00772831
I0628 19:35:49.567963 32082 solver.cpp:371]     Train net output #0: loss = 0.00772837 (* 1 = 0.00772837 loss)
I0628 19:35:49.567968 32082 sgd_solver.cpp:137] Iteration 62800, lr = 0.0001875, m = 0.9
I0628 19:35:51.285089 32082 solver.cpp:349] Iteration 62900 (58.2333 iter/s, 1.71723s/100 iter), loss = 0.00477999
I0628 19:35:51.285166 32082 solver.cpp:371]     Train net output #0: loss = 0.00478005 (* 1 = 0.00478005 loss)
I0628 19:35:51.285171 32082 sgd_solver.cpp:137] Iteration 62900, lr = 0.000171875, m = 0.9
I0628 19:35:52.986681 32082 solver.cpp:401] Sparsity after update:
I0628 19:35:52.987745 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:35:52.987751 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:35:52.987758 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:35:52.987761 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:35:52.987762 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:35:52.987766 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:35:52.987767 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:35:52.987769 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:35:52.987771 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:35:52.987773 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:35:52.987776 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:35:52.987778 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:35:52.987781 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:35:52.987787 32082 solver.cpp:545] Iteration 63000, Testing net (#0)
I0628 19:35:53.987442 32080 data_reader.cpp:262] Starting prefetch of epoch 63
I0628 19:35:54.007498 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9096
I0628 19:35:54.007508 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9954
I0628 19:35:54.007513 32082 solver.cpp:630]     Test net output #2: loss = 0.334117 (* 1 = 0.334117 loss)
I0628 19:35:54.007529 32082 solver.cpp:305] [MultiGPU] Tests completed in 1.01981s
I0628 19:35:54.026218 32082 solver.cpp:349] Iteration 63000 (36.48 iter/s, 2.74123s/100 iter), loss = 0.00271028
I0628 19:35:54.026239 32082 solver.cpp:371]     Train net output #0: loss = 0.00271035 (* 1 = 0.00271035 loss)
I0628 19:35:54.026243 32082 sgd_solver.cpp:137] Iteration 63000, lr = 0.00015625, m = 0.9
I0628 19:35:55.744562 32082 solver.cpp:349] Iteration 63100 (58.1928 iter/s, 1.71843s/100 iter), loss = 0.00179535
I0628 19:35:55.744587 32082 solver.cpp:371]     Train net output #0: loss = 0.00179541 (* 1 = 0.00179541 loss)
I0628 19:35:55.744592 32082 sgd_solver.cpp:137] Iteration 63100, lr = 0.000140625, m = 0.9
I0628 19:35:57.462111 32082 solver.cpp:349] Iteration 63200 (58.22 iter/s, 1.71762s/100 iter), loss = 0.00727016
I0628 19:35:57.462134 32082 solver.cpp:371]     Train net output #0: loss = 0.00727022 (* 1 = 0.00727022 loss)
I0628 19:35:57.462138 32082 sgd_solver.cpp:137] Iteration 63200, lr = 0.000125, m = 0.9
I0628 19:35:58.805685 32050 data_reader.cpp:262] Starting prefetch of epoch 81
I0628 19:35:59.183284 32082 solver.cpp:349] Iteration 63300 (58.0974 iter/s, 1.72125s/100 iter), loss = 0.00304053
I0628 19:35:59.183310 32082 solver.cpp:371]     Train net output #0: loss = 0.0030406 (* 1 = 0.0030406 loss)
I0628 19:35:59.183316 32082 sgd_solver.cpp:137] Iteration 63300, lr = 0.000109375, m = 0.9
I0628 19:36:00.902621 32082 solver.cpp:349] Iteration 63400 (58.1598 iter/s, 1.7194s/100 iter), loss = 0.00524929
I0628 19:36:00.902647 32082 solver.cpp:371]     Train net output #0: loss = 0.00524936 (* 1 = 0.00524936 loss)
I0628 19:36:00.902652 32082 sgd_solver.cpp:137] Iteration 63400, lr = 9.37498e-05, m = 0.9
I0628 19:36:02.630903 32082 solver.cpp:349] Iteration 63500 (57.8588 iter/s, 1.72835s/100 iter), loss = 0.00755569
I0628 19:36:02.630928 32082 solver.cpp:371]     Train net output #0: loss = 0.00755576 (* 1 = 0.00755576 loss)
I0628 19:36:02.630934 32082 sgd_solver.cpp:137] Iteration 63500, lr = 7.8125e-05, m = 0.9
I0628 19:36:04.350314 32082 solver.cpp:349] Iteration 63600 (58.1573 iter/s, 1.71947s/100 iter), loss = 0.00824207
I0628 19:36:04.350340 32082 solver.cpp:371]     Train net output #0: loss = 0.00824213 (* 1 = 0.00824213 loss)
I0628 19:36:04.350347 32082 sgd_solver.cpp:137] Iteration 63600, lr = 6.25002e-05, m = 0.9
I0628 19:36:06.071424 32082 solver.cpp:349] Iteration 63700 (58.1001 iter/s, 1.72117s/100 iter), loss = 0.00297086
I0628 19:36:06.071465 32082 solver.cpp:371]     Train net output #0: loss = 0.00297092 (* 1 = 0.00297092 loss)
I0628 19:36:06.071471 32082 sgd_solver.cpp:137] Iteration 63700, lr = 4.68749e-05, m = 0.9
I0628 19:36:07.792111 32082 solver.cpp:349] Iteration 63800 (58.115 iter/s, 1.72073s/100 iter), loss = 0.00408494
I0628 19:36:07.792138 32082 solver.cpp:371]     Train net output #0: loss = 0.004085 (* 1 = 0.004085 loss)
I0628 19:36:07.792145 32082 sgd_solver.cpp:137] Iteration 63800, lr = 3.12501e-05, m = 0.9
I0628 19:36:09.511823 32082 solver.cpp:349] Iteration 63900 (58.1475 iter/s, 1.71976s/100 iter), loss = 0.00561901
I0628 19:36:09.511845 32082 solver.cpp:371]     Train net output #0: loss = 0.00561907 (* 1 = 0.00561907 loss)
I0628 19:36:09.511852 32082 sgd_solver.cpp:137] Iteration 63900, lr = 1.56248e-05, m = 0.9
I0628 19:36:11.212572 32082 solver.cpp:349] Iteration 63999 (58.2077 iter/s, 1.7008s/100 iter), loss = 0.00344106
I0628 19:36:11.212595 32082 solver.cpp:371]     Train net output #0: loss = 0.00344112 (* 1 = 0.00344112 loss)
I0628 19:36:11.212601 32082 solver.cpp:401] Sparsity after update:
I0628 19:36:11.213904 32082 net.cpp:2161] Num Params(11), Sparsity (zero_weights/count): 
I0628 19:36:11.213912 32082 net.cpp:2170] conv1a_param_0(0.41) 
I0628 19:36:11.213914 32082 net.cpp:2170] conv1b_param_0(0.82) 
I0628 19:36:11.213917 32082 net.cpp:2170] fc10_param_0(0) 
I0628 19:36:11.213918 32082 net.cpp:2170] res2a_branch2a_param_0(0.82) 
I0628 19:36:11.213922 32082 net.cpp:2170] res2a_branch2b_param_0(0.664) 
I0628 19:36:11.213923 32082 net.cpp:2170] res3a_branch2a_param_0(0.742) 
I0628 19:36:11.213927 32082 net.cpp:2170] res3a_branch2b_param_0(0.701) 
I0628 19:36:11.213928 32082 net.cpp:2170] res4a_branch2a_param_0(0.79) 
I0628 19:36:11.213930 32082 net.cpp:2170] res4a_branch2b_param_0(0.804) 
I0628 19:36:11.213932 32082 net.cpp:2170] res5a_branch2a_param_0(0.82) 
I0628 19:36:11.213934 32082 net.cpp:2170] res5a_branch2b_param_0(0.82) 
I0628 19:36:11.213935 32082 net.cpp:2172] Total Sparsity (zero_weights/count) =  (1.90705e+06/2.3599e+06) 0.808
I0628 19:36:11.213971 32082 solver.cpp:675] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_64000.caffemodel
I0628 19:36:11.221719 32082 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-28_18-56-45/sparse/cifar10_jacintonet11v2_iter_64000.solverstate
I0628 19:36:11.231536 32082 solver.cpp:522] Iteration 64000, loss = 0.00742856
I0628 19:36:11.231556 32082 solver.cpp:545] Iteration 64000, Testing net (#0)
I0628 19:36:12.229812 32080 data_reader.cpp:262] Starting prefetch of epoch 64
I0628 19:36:12.250552 32082 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.9088
I0628 19:36:12.250569 32082 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.9952
I0628 19:36:12.250576 32082 solver.cpp:630]     Test net output #2: loss = 0.332688 (* 1 = 0.332688 loss)
I0628 19:36:12.255136 32034 parallel.cpp:71] Root Solver performance on device 0: 53.94 * 32 = 1726 img/sec
I0628 19:36:12.255147 32034 parallel.cpp:76]      Solver performance on device 1: 53.94 * 32 = 1726 img/sec
I0628 19:36:12.255149 32034 parallel.cpp:79] Overall multi-GPU performance: 3452.12 img/sec
I0628 19:36:12.287798 32034 caffe.cpp:247] Optimization Done in 19m 50s
