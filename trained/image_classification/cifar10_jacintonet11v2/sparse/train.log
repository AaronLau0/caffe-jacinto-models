WARNING: gnome-keyring:: couldn't connect to: /run/user/30409/keyring-KJvviu/pkcs11: Connection refused
p11-kit: skipping module 'gnome-keyring' whose initialization failed: An error occurred on the device
I0630 01:43:36.961720 29015 caffe.cpp:209] Using GPUs 0, 1, 2
I0630 01:43:36.962589 29015 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0630 01:43:36.962942 29015 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0630 01:43:36.963276 29015 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0630 01:43:37.352587 29015 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: true
iter_size: 1
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.02
sparsity_step_iter: 1000
sparsity_start_iter: 0
sparsity_start_factor: 0
I0630 01:43:37.352681 29015 solver.cpp:82] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/train.prototxt
I0630 01:43:37.353327 29015 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0630 01:43:37.353333 29015 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0630 01:43:37.353543 29015 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 21
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0630 01:43:37.353634 29015 layer_factory.hpp:77] Creating layer data
I0630 01:43:37.353730 29015 net.cpp:98] Creating Layer data
I0630 01:43:37.353739 29015 net.cpp:413] data -> data
I0630 01:43:37.353757 29015 net.cpp:413] data -> label
I0630 01:43:37.354579 29042 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0630 01:43:37.355602 29015 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0630 01:43:37.355654 29015 data_layer.cpp:83] output data size: 21,3,32,32
I0630 01:43:37.357301 29015 net.cpp:148] Setting up data
I0630 01:43:37.357312 29015 net.cpp:155] Top shape: 21 3 32 32 (64512)
I0630 01:43:37.357316 29015 net.cpp:155] Top shape: 21 (21)
I0630 01:43:37.357317 29015 net.cpp:163] Memory required for data: 258132
I0630 01:43:37.357323 29015 layer_factory.hpp:77] Creating layer data/bias
I0630 01:43:37.357331 29015 net.cpp:98] Creating Layer data/bias
I0630 01:43:37.357336 29015 net.cpp:439] data/bias <- data
I0630 01:43:37.357342 29015 net.cpp:413] data/bias -> data/bias
I0630 01:43:37.358391 29015 net.cpp:148] Setting up data/bias
I0630 01:43:37.358399 29015 net.cpp:155] Top shape: 21 3 32 32 (64512)
I0630 01:43:37.358402 29015 net.cpp:163] Memory required for data: 516180
I0630 01:43:37.358410 29015 layer_factory.hpp:77] Creating layer conv1a
I0630 01:43:37.358419 29015 net.cpp:98] Creating Layer conv1a
I0630 01:43:37.358422 29015 net.cpp:439] conv1a <- data/bias
I0630 01:43:37.358424 29015 net.cpp:413] conv1a -> conv1a
I0630 01:43:37.359338 29044 blocking_queue.cpp:50] Waiting for data
I0630 01:43:37.359760 29015 net.cpp:148] Setting up conv1a
I0630 01:43:37.359769 29015 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:43:37.359771 29015 net.cpp:163] Memory required for data: 3268692
I0630 01:43:37.359776 29015 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 01:43:37.359783 29015 net.cpp:98] Creating Layer conv1a/bn
I0630 01:43:37.359786 29015 net.cpp:439] conv1a/bn <- conv1a
I0630 01:43:37.359788 29015 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 01:43:37.360522 29015 net.cpp:148] Setting up conv1a/bn
I0630 01:43:37.360532 29015 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:43:37.360534 29015 net.cpp:163] Memory required for data: 6021204
I0630 01:43:37.360541 29015 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 01:43:37.360545 29015 net.cpp:98] Creating Layer conv1a/relu
I0630 01:43:37.360548 29015 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 01:43:37.360550 29015 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 01:43:37.360559 29015 net.cpp:148] Setting up conv1a/relu
I0630 01:43:37.360563 29015 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:43:37.360572 29015 net.cpp:163] Memory required for data: 8773716
I0630 01:43:37.360575 29015 layer_factory.hpp:77] Creating layer conv1b
I0630 01:43:37.360579 29015 net.cpp:98] Creating Layer conv1b
I0630 01:43:37.360582 29015 net.cpp:439] conv1b <- conv1a/bn
I0630 01:43:37.360585 29015 net.cpp:413] conv1b -> conv1b
I0630 01:43:37.361001 29015 net.cpp:148] Setting up conv1b
I0630 01:43:37.361007 29015 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:43:37.361009 29015 net.cpp:163] Memory required for data: 11526228
I0630 01:43:37.361013 29015 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 01:43:37.361017 29015 net.cpp:98] Creating Layer conv1b/bn
I0630 01:43:37.361019 29015 net.cpp:439] conv1b/bn <- conv1b
I0630 01:43:37.361022 29015 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 01:43:37.361739 29015 net.cpp:148] Setting up conv1b/bn
I0630 01:43:37.361747 29015 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:43:37.361749 29015 net.cpp:163] Memory required for data: 14278740
I0630 01:43:37.361754 29015 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 01:43:37.361757 29015 net.cpp:98] Creating Layer conv1b/relu
I0630 01:43:37.361759 29015 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 01:43:37.361762 29015 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 01:43:37.361765 29015 net.cpp:148] Setting up conv1b/relu
I0630 01:43:37.361768 29015 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:43:37.361769 29015 net.cpp:163] Memory required for data: 17031252
I0630 01:43:37.361771 29015 layer_factory.hpp:77] Creating layer pool1
I0630 01:43:37.361776 29015 net.cpp:98] Creating Layer pool1
I0630 01:43:37.361778 29015 net.cpp:439] pool1 <- conv1b/bn
I0630 01:43:37.361780 29015 net.cpp:413] pool1 -> pool1
I0630 01:43:37.361825 29015 net.cpp:148] Setting up pool1
I0630 01:43:37.361829 29015 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0630 01:43:37.361831 29015 net.cpp:163] Memory required for data: 19783764
I0630 01:43:37.361835 29015 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 01:43:37.361840 29015 net.cpp:98] Creating Layer res2a_branch2a
I0630 01:43:37.361845 29015 net.cpp:439] res2a_branch2a <- pool1
I0630 01:43:37.361850 29015 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 01:43:37.362607 29015 net.cpp:148] Setting up res2a_branch2a
I0630 01:43:37.362614 29015 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:43:37.362617 29015 net.cpp:163] Memory required for data: 25288788
I0630 01:43:37.362620 29015 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 01:43:37.362624 29015 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 01:43:37.362627 29015 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 01:43:37.362629 29015 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 01:43:37.363348 29015 net.cpp:148] Setting up res2a_branch2a/bn
I0630 01:43:37.363354 29015 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:43:37.363356 29015 net.cpp:163] Memory required for data: 30793812
I0630 01:43:37.363361 29015 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 01:43:37.363364 29015 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 01:43:37.363366 29015 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 01:43:37.363369 29015 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 01:43:37.363373 29015 net.cpp:148] Setting up res2a_branch2a/relu
I0630 01:43:37.363375 29015 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:43:37.363378 29015 net.cpp:163] Memory required for data: 36298836
I0630 01:43:37.363379 29015 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 01:43:37.363384 29015 net.cpp:98] Creating Layer res2a_branch2b
I0630 01:43:37.363385 29015 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 01:43:37.363387 29015 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 01:43:37.364852 29015 net.cpp:148] Setting up res2a_branch2b
I0630 01:43:37.364861 29015 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:43:37.364863 29015 net.cpp:163] Memory required for data: 41803860
I0630 01:43:37.364873 29015 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 01:43:37.364878 29015 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 01:43:37.364881 29015 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 01:43:37.364883 29015 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 01:43:37.365602 29015 net.cpp:148] Setting up res2a_branch2b/bn
I0630 01:43:37.365610 29015 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:43:37.365612 29015 net.cpp:163] Memory required for data: 47308884
I0630 01:43:37.365617 29015 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 01:43:37.365622 29015 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 01:43:37.365623 29015 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 01:43:37.365627 29015 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 01:43:37.365633 29015 net.cpp:148] Setting up res2a_branch2b/relu
I0630 01:43:37.365635 29015 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0630 01:43:37.365638 29015 net.cpp:163] Memory required for data: 52813908
I0630 01:43:37.365639 29015 layer_factory.hpp:77] Creating layer pool2
I0630 01:43:37.365643 29015 net.cpp:98] Creating Layer pool2
I0630 01:43:37.365644 29015 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 01:43:37.365646 29015 net.cpp:413] pool2 -> pool2
I0630 01:43:37.365687 29015 net.cpp:148] Setting up pool2
I0630 01:43:37.365694 29015 net.cpp:155] Top shape: 21 64 16 16 (344064)
I0630 01:43:37.365697 29015 net.cpp:163] Memory required for data: 54190164
I0630 01:43:37.365700 29015 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 01:43:37.365710 29015 net.cpp:98] Creating Layer res3a_branch2a
I0630 01:43:37.365713 29015 net.cpp:439] res3a_branch2a <- pool2
I0630 01:43:37.365718 29015 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 01:43:37.368469 29015 net.cpp:148] Setting up res3a_branch2a
I0630 01:43:37.368479 29015 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:43:37.368481 29015 net.cpp:163] Memory required for data: 56942676
I0630 01:43:37.368485 29015 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 01:43:37.368489 29015 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 01:43:37.368491 29015 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 01:43:37.368495 29015 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 01:43:37.369231 29015 net.cpp:148] Setting up res3a_branch2a/bn
I0630 01:43:37.369240 29015 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:43:37.369241 29015 net.cpp:163] Memory required for data: 59695188
I0630 01:43:37.369252 29015 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 01:43:37.369256 29015 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 01:43:37.369258 29015 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 01:43:37.369261 29015 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 01:43:37.369264 29015 net.cpp:148] Setting up res3a_branch2a/relu
I0630 01:43:37.369267 29015 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:43:37.369268 29015 net.cpp:163] Memory required for data: 62447700
I0630 01:43:37.369271 29015 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 01:43:37.369277 29015 net.cpp:98] Creating Layer res3a_branch2b
I0630 01:43:37.369278 29015 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 01:43:37.369282 29015 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 01:43:37.370342 29015 net.cpp:148] Setting up res3a_branch2b
I0630 01:43:37.370349 29015 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:43:37.370352 29015 net.cpp:163] Memory required for data: 65200212
I0630 01:43:37.370355 29015 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 01:43:37.370360 29015 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 01:43:37.370363 29015 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 01:43:37.370367 29015 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 01:43:37.371006 29015 net.cpp:148] Setting up res3a_branch2b/bn
I0630 01:43:37.371012 29015 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:43:37.371021 29015 net.cpp:163] Memory required for data: 67952724
I0630 01:43:37.371026 29015 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 01:43:37.371028 29015 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 01:43:37.371031 29015 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 01:43:37.371033 29015 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 01:43:37.371037 29015 net.cpp:148] Setting up res3a_branch2b/relu
I0630 01:43:37.371039 29015 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:43:37.371042 29015 net.cpp:163] Memory required for data: 70705236
I0630 01:43:37.371043 29015 layer_factory.hpp:77] Creating layer pool3
I0630 01:43:37.371047 29015 net.cpp:98] Creating Layer pool3
I0630 01:43:37.371048 29015 net.cpp:439] pool3 <- res3a_branch2b/bn
I0630 01:43:37.371052 29015 net.cpp:413] pool3 -> pool3
I0630 01:43:37.371085 29015 net.cpp:148] Setting up pool3
I0630 01:43:37.371089 29015 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0630 01:43:37.371090 29015 net.cpp:163] Memory required for data: 73457748
I0630 01:43:37.371093 29015 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 01:43:37.371096 29015 net.cpp:98] Creating Layer res4a_branch2a
I0630 01:43:37.371098 29015 net.cpp:439] res4a_branch2a <- pool3
I0630 01:43:37.371101 29015 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 01:43:37.377280 29015 net.cpp:148] Setting up res4a_branch2a
I0630 01:43:37.377288 29015 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:43:37.377290 29015 net.cpp:163] Memory required for data: 78962772
I0630 01:43:37.377293 29015 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 01:43:37.377297 29015 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 01:43:37.377300 29015 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 01:43:37.377302 29015 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 01:43:37.377969 29015 net.cpp:148] Setting up res4a_branch2a/bn
I0630 01:43:37.377976 29015 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:43:37.377979 29015 net.cpp:163] Memory required for data: 84467796
I0630 01:43:37.377984 29015 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 01:43:37.377986 29015 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 01:43:37.377990 29015 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 01:43:37.377991 29015 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 01:43:37.377995 29015 net.cpp:148] Setting up res4a_branch2a/relu
I0630 01:43:37.377997 29015 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:43:37.378000 29015 net.cpp:163] Memory required for data: 89972820
I0630 01:43:37.378001 29015 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 01:43:37.378005 29015 net.cpp:98] Creating Layer res4a_branch2b
I0630 01:43:37.378006 29015 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 01:43:37.378010 29015 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 01:43:37.381299 29015 net.cpp:148] Setting up res4a_branch2b
I0630 01:43:37.381307 29015 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:43:37.381309 29015 net.cpp:163] Memory required for data: 95477844
I0630 01:43:37.381312 29015 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 01:43:37.381316 29015 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 01:43:37.381319 29015 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 01:43:37.381321 29015 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 01:43:37.381984 29015 net.cpp:148] Setting up res4a_branch2b/bn
I0630 01:43:37.381991 29015 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:43:37.381994 29015 net.cpp:163] Memory required for data: 100982868
I0630 01:43:37.381999 29015 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 01:43:37.382000 29015 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 01:43:37.382004 29015 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 01:43:37.382005 29015 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 01:43:37.382015 29015 net.cpp:148] Setting up res4a_branch2b/relu
I0630 01:43:37.382019 29015 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0630 01:43:37.382020 29015 net.cpp:163] Memory required for data: 106487892
I0630 01:43:37.382021 29015 layer_factory.hpp:77] Creating layer pool4
I0630 01:43:37.382025 29015 net.cpp:98] Creating Layer pool4
I0630 01:43:37.382027 29015 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 01:43:37.382030 29015 net.cpp:413] pool4 -> pool4
I0630 01:43:37.382064 29015 net.cpp:148] Setting up pool4
I0630 01:43:37.382068 29015 net.cpp:155] Top shape: 21 256 8 8 (344064)
I0630 01:43:37.382071 29015 net.cpp:163] Memory required for data: 107864148
I0630 01:43:37.382072 29015 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 01:43:37.382076 29015 net.cpp:98] Creating Layer res5a_branch2a
I0630 01:43:37.382078 29015 net.cpp:439] res5a_branch2a <- pool4
I0630 01:43:37.382081 29015 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 01:43:37.407178 29015 net.cpp:148] Setting up res5a_branch2a
I0630 01:43:37.407196 29015 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:43:37.407199 29015 net.cpp:163] Memory required for data: 110616660
I0630 01:43:37.407205 29015 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 01:43:37.407212 29015 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 01:43:37.407215 29015 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 01:43:37.407219 29015 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 01:43:37.407946 29015 net.cpp:148] Setting up res5a_branch2a/bn
I0630 01:43:37.407953 29015 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:43:37.407955 29015 net.cpp:163] Memory required for data: 113369172
I0630 01:43:37.407961 29015 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 01:43:37.407965 29015 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 01:43:37.407968 29015 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 01:43:37.407970 29015 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 01:43:37.407974 29015 net.cpp:148] Setting up res5a_branch2a/relu
I0630 01:43:37.407977 29015 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:43:37.407979 29015 net.cpp:163] Memory required for data: 116121684
I0630 01:43:37.407980 29015 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 01:43:37.407992 29015 net.cpp:98] Creating Layer res5a_branch2b
I0630 01:43:37.407995 29015 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 01:43:37.407999 29015 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 01:43:37.421036 29015 net.cpp:148] Setting up res5a_branch2b
I0630 01:43:37.421057 29015 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:43:37.421059 29015 net.cpp:163] Memory required for data: 118874196
I0630 01:43:37.421072 29015 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 01:43:37.421079 29015 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 01:43:37.421083 29015 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 01:43:37.421087 29015 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 01:43:37.421905 29015 net.cpp:148] Setting up res5a_branch2b/bn
I0630 01:43:37.421913 29015 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:43:37.421916 29015 net.cpp:163] Memory required for data: 121626708
I0630 01:43:37.421921 29015 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 01:43:37.421926 29015 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 01:43:37.421927 29015 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 01:43:37.421931 29015 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 01:43:37.421934 29015 net.cpp:148] Setting up res5a_branch2b/relu
I0630 01:43:37.421937 29015 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0630 01:43:37.421938 29015 net.cpp:163] Memory required for data: 124379220
I0630 01:43:37.421941 29015 layer_factory.hpp:77] Creating layer pool5
I0630 01:43:37.421947 29015 net.cpp:98] Creating Layer pool5
I0630 01:43:37.421948 29015 net.cpp:439] pool5 <- res5a_branch2b/bn
I0630 01:43:37.421962 29015 net.cpp:413] pool5 -> pool5
I0630 01:43:37.421998 29015 net.cpp:148] Setting up pool5
I0630 01:43:37.422004 29015 net.cpp:155] Top shape: 21 512 1 1 (10752)
I0630 01:43:37.422008 29015 net.cpp:163] Memory required for data: 124422228
I0630 01:43:37.422011 29015 layer_factory.hpp:77] Creating layer fc10
I0630 01:43:37.422026 29015 net.cpp:98] Creating Layer fc10
I0630 01:43:37.422030 29015 net.cpp:439] fc10 <- pool5
I0630 01:43:37.422035 29015 net.cpp:413] fc10 -> fc10
I0630 01:43:37.422368 29015 net.cpp:148] Setting up fc10
I0630 01:43:37.422375 29015 net.cpp:155] Top shape: 21 10 (210)
I0630 01:43:37.422379 29015 net.cpp:163] Memory required for data: 124423068
I0630 01:43:37.422385 29015 layer_factory.hpp:77] Creating layer loss
I0630 01:43:37.422391 29015 net.cpp:98] Creating Layer loss
I0630 01:43:37.422395 29015 net.cpp:439] loss <- fc10
I0630 01:43:37.422399 29015 net.cpp:439] loss <- label
I0630 01:43:37.422405 29015 net.cpp:413] loss -> loss
I0630 01:43:37.422426 29015 layer_factory.hpp:77] Creating layer loss
I0630 01:43:37.422600 29015 net.cpp:148] Setting up loss
I0630 01:43:37.422606 29015 net.cpp:155] Top shape: (1)
I0630 01:43:37.422610 29015 net.cpp:158]     with loss weight 1
I0630 01:43:37.422624 29015 net.cpp:163] Memory required for data: 124423072
I0630 01:43:37.422629 29015 net.cpp:224] loss needs backward computation.
I0630 01:43:37.422631 29015 net.cpp:224] fc10 needs backward computation.
I0630 01:43:37.422633 29015 net.cpp:224] pool5 needs backward computation.
I0630 01:43:37.422636 29015 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 01:43:37.422641 29015 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 01:43:37.422646 29015 net.cpp:224] res5a_branch2b needs backward computation.
I0630 01:43:37.422649 29015 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 01:43:37.422653 29015 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 01:43:37.422657 29015 net.cpp:224] res5a_branch2a needs backward computation.
I0630 01:43:37.422662 29015 net.cpp:224] pool4 needs backward computation.
I0630 01:43:37.422667 29015 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 01:43:37.422670 29015 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 01:43:37.422674 29015 net.cpp:224] res4a_branch2b needs backward computation.
I0630 01:43:37.422678 29015 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 01:43:37.422683 29015 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 01:43:37.422686 29015 net.cpp:224] res4a_branch2a needs backward computation.
I0630 01:43:37.422690 29015 net.cpp:224] pool3 needs backward computation.
I0630 01:43:37.422694 29015 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 01:43:37.422698 29015 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 01:43:37.422703 29015 net.cpp:224] res3a_branch2b needs backward computation.
I0630 01:43:37.422706 29015 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 01:43:37.422710 29015 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 01:43:37.422714 29015 net.cpp:224] res3a_branch2a needs backward computation.
I0630 01:43:37.422719 29015 net.cpp:224] pool2 needs backward computation.
I0630 01:43:37.422722 29015 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 01:43:37.422726 29015 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 01:43:37.422730 29015 net.cpp:224] res2a_branch2b needs backward computation.
I0630 01:43:37.422734 29015 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 01:43:37.422739 29015 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 01:43:37.422742 29015 net.cpp:224] res2a_branch2a needs backward computation.
I0630 01:43:37.422746 29015 net.cpp:224] pool1 needs backward computation.
I0630 01:43:37.422749 29015 net.cpp:224] conv1b/relu needs backward computation.
I0630 01:43:37.422754 29015 net.cpp:224] conv1b/bn needs backward computation.
I0630 01:43:37.422757 29015 net.cpp:224] conv1b needs backward computation.
I0630 01:43:37.422766 29015 net.cpp:224] conv1a/relu needs backward computation.
I0630 01:43:37.422771 29015 net.cpp:224] conv1a/bn needs backward computation.
I0630 01:43:37.422775 29015 net.cpp:224] conv1a needs backward computation.
I0630 01:43:37.422780 29015 net.cpp:226] data/bias does not need backward computation.
I0630 01:43:37.422785 29015 net.cpp:226] data does not need backward computation.
I0630 01:43:37.422790 29015 net.cpp:268] This network produces output loss
I0630 01:43:37.422813 29015 net.cpp:288] Network initialization done.
I0630 01:43:37.423477 29015 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/test.prototxt
I0630 01:43:37.423676 29015 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0630 01:43:37.423799 29015 layer_factory.hpp:77] Creating layer data
I0630 01:43:37.423871 29015 net.cpp:98] Creating Layer data
I0630 01:43:37.423877 29015 net.cpp:413] data -> data
I0630 01:43:37.423883 29015 net.cpp:413] data -> label
I0630 01:43:37.438899 29045 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0630 01:43:37.439009 29015 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0630 01:43:37.439069 29015 data_layer.cpp:83] output data size: 50,3,32,32
I0630 01:43:37.441308 29015 net.cpp:148] Setting up data
I0630 01:43:37.441316 29015 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0630 01:43:37.441319 29015 net.cpp:155] Top shape: 50 (50)
I0630 01:43:37.441321 29015 net.cpp:163] Memory required for data: 614600
I0630 01:43:37.441323 29015 layer_factory.hpp:77] Creating layer label_data_1_split
I0630 01:43:37.441329 29015 net.cpp:98] Creating Layer label_data_1_split
I0630 01:43:37.441331 29015 net.cpp:439] label_data_1_split <- label
I0630 01:43:37.441334 29015 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0630 01:43:37.441339 29015 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0630 01:43:37.441341 29015 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0630 01:43:37.441479 29015 net.cpp:148] Setting up label_data_1_split
I0630 01:43:37.441491 29015 net.cpp:155] Top shape: 50 (50)
I0630 01:43:37.441496 29015 net.cpp:155] Top shape: 50 (50)
I0630 01:43:37.441500 29015 net.cpp:155] Top shape: 50 (50)
I0630 01:43:37.441504 29015 net.cpp:163] Memory required for data: 615200
I0630 01:43:37.441506 29015 layer_factory.hpp:77] Creating layer data/bias
I0630 01:43:37.441514 29015 net.cpp:98] Creating Layer data/bias
I0630 01:43:37.441517 29015 net.cpp:439] data/bias <- data
I0630 01:43:37.441522 29015 net.cpp:413] data/bias -> data/bias
I0630 01:43:37.441653 29015 net.cpp:148] Setting up data/bias
I0630 01:43:37.441659 29015 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0630 01:43:37.441663 29015 net.cpp:163] Memory required for data: 1229600
I0630 01:43:37.441669 29015 layer_factory.hpp:77] Creating layer conv1a
I0630 01:43:37.441675 29015 net.cpp:98] Creating Layer conv1a
I0630 01:43:37.441679 29015 net.cpp:439] conv1a <- data/bias
I0630 01:43:37.441684 29015 net.cpp:413] conv1a -> conv1a
I0630 01:43:37.442312 29015 net.cpp:148] Setting up conv1a
I0630 01:43:37.442320 29015 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:43:37.442322 29015 net.cpp:163] Memory required for data: 7783200
I0630 01:43:37.442327 29015 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 01:43:37.442330 29015 net.cpp:98] Creating Layer conv1a/bn
I0630 01:43:37.442333 29015 net.cpp:439] conv1a/bn <- conv1a
I0630 01:43:37.442337 29015 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 01:43:37.443209 29015 net.cpp:148] Setting up conv1a/bn
I0630 01:43:37.443217 29015 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:43:37.443219 29015 net.cpp:163] Memory required for data: 14336800
I0630 01:43:37.443225 29015 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 01:43:37.443236 29015 net.cpp:98] Creating Layer conv1a/relu
I0630 01:43:37.443238 29015 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 01:43:37.443241 29015 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 01:43:37.443245 29015 net.cpp:148] Setting up conv1a/relu
I0630 01:43:37.443248 29015 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:43:37.443251 29015 net.cpp:163] Memory required for data: 20890400
I0630 01:43:37.443255 29015 layer_factory.hpp:77] Creating layer conv1b
I0630 01:43:37.443262 29015 net.cpp:98] Creating Layer conv1b
I0630 01:43:37.443267 29015 net.cpp:439] conv1b <- conv1a/bn
I0630 01:43:37.443272 29015 net.cpp:413] conv1b -> conv1b
I0630 01:43:37.443768 29015 net.cpp:148] Setting up conv1b
I0630 01:43:37.443775 29015 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:43:37.443778 29015 net.cpp:163] Memory required for data: 27444000
I0630 01:43:37.443781 29015 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 01:43:37.443789 29015 net.cpp:98] Creating Layer conv1b/bn
I0630 01:43:37.443791 29015 net.cpp:439] conv1b/bn <- conv1b
I0630 01:43:37.443794 29015 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 01:43:37.444612 29015 net.cpp:148] Setting up conv1b/bn
I0630 01:43:37.444619 29015 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:43:37.444622 29015 net.cpp:163] Memory required for data: 33997600
I0630 01:43:37.444628 29015 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 01:43:37.444630 29015 net.cpp:98] Creating Layer conv1b/relu
I0630 01:43:37.444633 29015 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 01:43:37.444635 29015 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 01:43:37.444638 29015 net.cpp:148] Setting up conv1b/relu
I0630 01:43:37.444640 29015 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:43:37.444643 29015 net.cpp:163] Memory required for data: 40551200
I0630 01:43:37.444644 29015 layer_factory.hpp:77] Creating layer pool1
I0630 01:43:37.444648 29015 net.cpp:98] Creating Layer pool1
I0630 01:43:37.444649 29015 net.cpp:439] pool1 <- conv1b/bn
I0630 01:43:37.444651 29015 net.cpp:413] pool1 -> pool1
I0630 01:43:37.444701 29015 net.cpp:148] Setting up pool1
I0630 01:43:37.444708 29015 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0630 01:43:37.444711 29015 net.cpp:163] Memory required for data: 47104800
I0630 01:43:37.444715 29015 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 01:43:37.444725 29015 net.cpp:98] Creating Layer res2a_branch2a
I0630 01:43:37.444728 29015 net.cpp:439] res2a_branch2a <- pool1
I0630 01:43:37.444735 29015 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 01:43:37.445516 29015 net.cpp:148] Setting up res2a_branch2a
I0630 01:43:37.445523 29015 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:43:37.445525 29015 net.cpp:163] Memory required for data: 60212000
I0630 01:43:37.445529 29015 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 01:43:37.445533 29015 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 01:43:37.445535 29015 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 01:43:37.445538 29015 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 01:43:37.446315 29015 net.cpp:148] Setting up res2a_branch2a/bn
I0630 01:43:37.446321 29015 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:43:37.446323 29015 net.cpp:163] Memory required for data: 73319200
I0630 01:43:37.446328 29015 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 01:43:37.446331 29015 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 01:43:37.446333 29015 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 01:43:37.446336 29015 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 01:43:37.446338 29015 net.cpp:148] Setting up res2a_branch2a/relu
I0630 01:43:37.446341 29015 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:43:37.446342 29015 net.cpp:163] Memory required for data: 86426400
I0630 01:43:37.446344 29015 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 01:43:37.446348 29015 net.cpp:98] Creating Layer res2a_branch2b
I0630 01:43:37.446355 29015 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 01:43:37.446362 29015 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 01:43:37.446949 29015 net.cpp:148] Setting up res2a_branch2b
I0630 01:43:37.446955 29015 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:43:37.446957 29015 net.cpp:163] Memory required for data: 99533600
I0630 01:43:37.446960 29015 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 01:43:37.446964 29015 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 01:43:37.446966 29015 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 01:43:37.446969 29015 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 01:43:37.447753 29015 net.cpp:148] Setting up res2a_branch2b/bn
I0630 01:43:37.447760 29015 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:43:37.447762 29015 net.cpp:163] Memory required for data: 112640800
I0630 01:43:37.447767 29015 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 01:43:37.447770 29015 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 01:43:37.447772 29015 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 01:43:37.447774 29015 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 01:43:37.447778 29015 net.cpp:148] Setting up res2a_branch2b/relu
I0630 01:43:37.447780 29015 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0630 01:43:37.447782 29015 net.cpp:163] Memory required for data: 125748000
I0630 01:43:37.447784 29015 layer_factory.hpp:77] Creating layer pool2
I0630 01:43:37.447788 29015 net.cpp:98] Creating Layer pool2
I0630 01:43:37.447789 29015 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 01:43:37.447791 29015 net.cpp:413] pool2 -> pool2
I0630 01:43:37.447829 29015 net.cpp:148] Setting up pool2
I0630 01:43:37.447835 29015 net.cpp:155] Top shape: 50 64 16 16 (819200)
I0630 01:43:37.447839 29015 net.cpp:163] Memory required for data: 129024800
I0630 01:43:37.447841 29015 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 01:43:37.447847 29015 net.cpp:98] Creating Layer res3a_branch2a
I0630 01:43:37.447851 29015 net.cpp:439] res3a_branch2a <- pool2
I0630 01:43:37.447855 29015 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 01:43:37.450871 29015 net.cpp:148] Setting up res3a_branch2a
I0630 01:43:37.450891 29015 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:43:37.450894 29015 net.cpp:163] Memory required for data: 135578400
I0630 01:43:37.450899 29015 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 01:43:37.450906 29015 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 01:43:37.450908 29015 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 01:43:37.450912 29015 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 01:43:37.451638 29015 net.cpp:148] Setting up res3a_branch2a/bn
I0630 01:43:37.451645 29015 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:43:37.451647 29015 net.cpp:163] Memory required for data: 142132000
I0630 01:43:37.451655 29015 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 01:43:37.451663 29015 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 01:43:37.451665 29015 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 01:43:37.451668 29015 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 01:43:37.451673 29015 net.cpp:148] Setting up res3a_branch2a/relu
I0630 01:43:37.451674 29015 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:43:37.451676 29015 net.cpp:163] Memory required for data: 148685600
I0630 01:43:37.451678 29015 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 01:43:37.451684 29015 net.cpp:98] Creating Layer res3a_branch2b
I0630 01:43:37.451688 29015 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 01:43:37.451690 29015 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 01:43:37.452738 29015 net.cpp:148] Setting up res3a_branch2b
I0630 01:43:37.452744 29015 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:43:37.452745 29015 net.cpp:163] Memory required for data: 155239200
I0630 01:43:37.452749 29015 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 01:43:37.452764 29015 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 01:43:37.452766 29015 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 01:43:37.452769 29015 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 01:43:37.453426 29015 net.cpp:148] Setting up res3a_branch2b/bn
I0630 01:43:37.453433 29015 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:43:37.453435 29015 net.cpp:163] Memory required for data: 161792800
I0630 01:43:37.453440 29015 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 01:43:37.453444 29015 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 01:43:37.453447 29015 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 01:43:37.453449 29015 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 01:43:37.453454 29015 net.cpp:148] Setting up res3a_branch2b/relu
I0630 01:43:37.453457 29015 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:43:37.453459 29015 net.cpp:163] Memory required for data: 168346400
I0630 01:43:37.453461 29015 layer_factory.hpp:77] Creating layer pool3
I0630 01:43:37.453465 29015 net.cpp:98] Creating Layer pool3
I0630 01:43:37.453469 29015 net.cpp:439] pool3 <- res3a_branch2b/bn
I0630 01:43:37.453470 29015 net.cpp:413] pool3 -> pool3
I0630 01:43:37.453510 29015 net.cpp:148] Setting up pool3
I0630 01:43:37.453514 29015 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0630 01:43:37.453517 29015 net.cpp:163] Memory required for data: 174900000
I0630 01:43:37.453519 29015 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 01:43:37.453524 29015 net.cpp:98] Creating Layer res4a_branch2a
I0630 01:43:37.453526 29015 net.cpp:439] res4a_branch2a <- pool3
I0630 01:43:37.453529 29015 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 01:43:37.459630 29015 net.cpp:148] Setting up res4a_branch2a
I0630 01:43:37.459636 29015 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:43:37.459638 29015 net.cpp:163] Memory required for data: 188007200
I0630 01:43:37.459642 29015 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 01:43:37.459647 29015 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 01:43:37.459650 29015 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 01:43:37.459653 29015 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 01:43:37.460295 29015 net.cpp:148] Setting up res4a_branch2a/bn
I0630 01:43:37.460301 29015 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:43:37.460304 29015 net.cpp:163] Memory required for data: 201114400
I0630 01:43:37.460309 29015 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 01:43:37.460312 29015 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 01:43:37.460315 29015 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 01:43:37.460319 29015 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 01:43:37.460322 29015 net.cpp:148] Setting up res4a_branch2a/relu
I0630 01:43:37.460325 29015 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:43:37.460327 29015 net.cpp:163] Memory required for data: 214221600
I0630 01:43:37.460330 29015 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 01:43:37.460333 29015 net.cpp:98] Creating Layer res4a_branch2b
I0630 01:43:37.460336 29015 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 01:43:37.460338 29015 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 01:43:37.463541 29015 net.cpp:148] Setting up res4a_branch2b
I0630 01:43:37.463547 29015 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:43:37.463549 29015 net.cpp:163] Memory required for data: 227328800
I0630 01:43:37.463552 29015 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 01:43:37.463557 29015 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 01:43:37.463559 29015 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 01:43:37.463565 29015 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 01:43:37.464203 29015 net.cpp:148] Setting up res4a_branch2b/bn
I0630 01:43:37.464208 29015 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:43:37.464216 29015 net.cpp:163] Memory required for data: 240436000
I0630 01:43:37.464222 29015 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 01:43:37.464224 29015 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 01:43:37.464226 29015 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 01:43:37.464229 29015 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 01:43:37.464232 29015 net.cpp:148] Setting up res4a_branch2b/relu
I0630 01:43:37.464234 29015 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0630 01:43:37.464236 29015 net.cpp:163] Memory required for data: 253543200
I0630 01:43:37.464238 29015 layer_factory.hpp:77] Creating layer pool4
I0630 01:43:37.464243 29015 net.cpp:98] Creating Layer pool4
I0630 01:43:37.464246 29015 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 01:43:37.464248 29015 net.cpp:413] pool4 -> pool4
I0630 01:43:37.464288 29015 net.cpp:148] Setting up pool4
I0630 01:43:37.464293 29015 net.cpp:155] Top shape: 50 256 8 8 (819200)
I0630 01:43:37.464293 29015 net.cpp:163] Memory required for data: 256820000
I0630 01:43:37.464295 29015 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 01:43:37.464303 29015 net.cpp:98] Creating Layer res5a_branch2a
I0630 01:43:37.464305 29015 net.cpp:439] res5a_branch2a <- pool4
I0630 01:43:37.464308 29015 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 01:43:37.489150 29015 net.cpp:148] Setting up res5a_branch2a
I0630 01:43:37.489171 29015 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:43:37.489172 29015 net.cpp:163] Memory required for data: 263373600
I0630 01:43:37.489178 29015 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 01:43:37.489187 29015 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 01:43:37.489190 29015 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 01:43:37.489194 29015 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 01:43:37.489903 29015 net.cpp:148] Setting up res5a_branch2a/bn
I0630 01:43:37.489909 29015 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:43:37.489912 29015 net.cpp:163] Memory required for data: 269927200
I0630 01:43:37.489917 29015 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 01:43:37.489920 29015 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 01:43:37.489923 29015 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 01:43:37.489925 29015 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 01:43:37.489930 29015 net.cpp:148] Setting up res5a_branch2a/relu
I0630 01:43:37.489933 29015 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:43:37.489934 29015 net.cpp:163] Memory required for data: 276480800
I0630 01:43:37.489936 29015 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 01:43:37.489945 29015 net.cpp:98] Creating Layer res5a_branch2b
I0630 01:43:37.489948 29015 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 01:43:37.489951 29015 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 01:43:37.502794 29015 net.cpp:148] Setting up res5a_branch2b
I0630 01:43:37.502802 29015 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:43:37.502805 29015 net.cpp:163] Memory required for data: 283034400
I0630 01:43:37.502815 29015 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 01:43:37.502820 29015 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 01:43:37.502821 29015 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 01:43:37.502825 29015 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 01:43:37.503551 29015 net.cpp:148] Setting up res5a_branch2b/bn
I0630 01:43:37.503557 29015 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:43:37.503559 29015 net.cpp:163] Memory required for data: 289588000
I0630 01:43:37.503564 29015 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 01:43:37.503568 29015 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 01:43:37.503571 29015 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 01:43:37.503572 29015 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 01:43:37.503587 29015 net.cpp:148] Setting up res5a_branch2b/relu
I0630 01:43:37.503589 29015 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0630 01:43:37.503592 29015 net.cpp:163] Memory required for data: 296141600
I0630 01:43:37.503593 29015 layer_factory.hpp:77] Creating layer pool5
I0630 01:43:37.503597 29015 net.cpp:98] Creating Layer pool5
I0630 01:43:37.503599 29015 net.cpp:439] pool5 <- res5a_branch2b/bn
I0630 01:43:37.503602 29015 net.cpp:413] pool5 -> pool5
I0630 01:43:37.503626 29015 net.cpp:148] Setting up pool5
I0630 01:43:37.503629 29015 net.cpp:155] Top shape: 50 512 1 1 (25600)
I0630 01:43:37.503631 29015 net.cpp:163] Memory required for data: 296244000
I0630 01:43:37.503633 29015 layer_factory.hpp:77] Creating layer fc10
I0630 01:43:37.503643 29015 net.cpp:98] Creating Layer fc10
I0630 01:43:37.503644 29015 net.cpp:439] fc10 <- pool5
I0630 01:43:37.503648 29015 net.cpp:413] fc10 -> fc10
I0630 01:43:37.503885 29015 net.cpp:148] Setting up fc10
I0630 01:43:37.503890 29015 net.cpp:155] Top shape: 50 10 (500)
I0630 01:43:37.503891 29015 net.cpp:163] Memory required for data: 296246000
I0630 01:43:37.503895 29015 layer_factory.hpp:77] Creating layer fc10_fc10_0_split
I0630 01:43:37.503897 29015 net.cpp:98] Creating Layer fc10_fc10_0_split
I0630 01:43:37.503900 29015 net.cpp:439] fc10_fc10_0_split <- fc10
I0630 01:43:37.503901 29015 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0630 01:43:37.503906 29015 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0630 01:43:37.503908 29015 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0630 01:43:37.503969 29015 net.cpp:148] Setting up fc10_fc10_0_split
I0630 01:43:37.503973 29015 net.cpp:155] Top shape: 50 10 (500)
I0630 01:43:37.503976 29015 net.cpp:155] Top shape: 50 10 (500)
I0630 01:43:37.503978 29015 net.cpp:155] Top shape: 50 10 (500)
I0630 01:43:37.503979 29015 net.cpp:163] Memory required for data: 296252000
I0630 01:43:37.503981 29015 layer_factory.hpp:77] Creating layer loss
I0630 01:43:37.503984 29015 net.cpp:98] Creating Layer loss
I0630 01:43:37.503986 29015 net.cpp:439] loss <- fc10_fc10_0_split_0
I0630 01:43:37.503989 29015 net.cpp:439] loss <- label_data_1_split_0
I0630 01:43:37.503991 29015 net.cpp:413] loss -> loss
I0630 01:43:37.503995 29015 layer_factory.hpp:77] Creating layer loss
I0630 01:43:37.504101 29015 net.cpp:148] Setting up loss
I0630 01:43:37.504104 29015 net.cpp:155] Top shape: (1)
I0630 01:43:37.504106 29015 net.cpp:158]     with loss weight 1
I0630 01:43:37.504113 29015 net.cpp:163] Memory required for data: 296252004
I0630 01:43:37.504115 29015 layer_factory.hpp:77] Creating layer accuracy/top1
I0630 01:43:37.504118 29015 net.cpp:98] Creating Layer accuracy/top1
I0630 01:43:37.504120 29015 net.cpp:439] accuracy/top1 <- fc10_fc10_0_split_1
I0630 01:43:37.504123 29015 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0630 01:43:37.504125 29015 net.cpp:413] accuracy/top1 -> accuracy/top1
I0630 01:43:37.504129 29015 net.cpp:148] Setting up accuracy/top1
I0630 01:43:37.504132 29015 net.cpp:155] Top shape: (1)
I0630 01:43:37.504133 29015 net.cpp:163] Memory required for data: 296252008
I0630 01:43:37.504135 29015 layer_factory.hpp:77] Creating layer accuracy/top5
I0630 01:43:37.504142 29015 net.cpp:98] Creating Layer accuracy/top5
I0630 01:43:37.504143 29015 net.cpp:439] accuracy/top5 <- fc10_fc10_0_split_2
I0630 01:43:37.504146 29015 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0630 01:43:37.504150 29015 net.cpp:413] accuracy/top5 -> accuracy/top5
I0630 01:43:37.504154 29015 net.cpp:148] Setting up accuracy/top5
I0630 01:43:37.504158 29015 net.cpp:155] Top shape: (1)
I0630 01:43:37.504159 29015 net.cpp:163] Memory required for data: 296252012
I0630 01:43:37.504161 29015 net.cpp:226] accuracy/top5 does not need backward computation.
I0630 01:43:37.504164 29015 net.cpp:226] accuracy/top1 does not need backward computation.
I0630 01:43:37.504166 29015 net.cpp:224] loss needs backward computation.
I0630 01:43:37.504168 29015 net.cpp:224] fc10_fc10_0_split needs backward computation.
I0630 01:43:37.504170 29015 net.cpp:224] fc10 needs backward computation.
I0630 01:43:37.504179 29015 net.cpp:224] pool5 needs backward computation.
I0630 01:43:37.504180 29015 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 01:43:37.504182 29015 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 01:43:37.504184 29015 net.cpp:224] res5a_branch2b needs backward computation.
I0630 01:43:37.504186 29015 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 01:43:37.504189 29015 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 01:43:37.504190 29015 net.cpp:224] res5a_branch2a needs backward computation.
I0630 01:43:37.504192 29015 net.cpp:224] pool4 needs backward computation.
I0630 01:43:37.504194 29015 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 01:43:37.504196 29015 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 01:43:37.504199 29015 net.cpp:224] res4a_branch2b needs backward computation.
I0630 01:43:37.504200 29015 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 01:43:37.504201 29015 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 01:43:37.504204 29015 net.cpp:224] res4a_branch2a needs backward computation.
I0630 01:43:37.504206 29015 net.cpp:224] pool3 needs backward computation.
I0630 01:43:37.504209 29015 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 01:43:37.504209 29015 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 01:43:37.504211 29015 net.cpp:224] res3a_branch2b needs backward computation.
I0630 01:43:37.504214 29015 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 01:43:37.504215 29015 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 01:43:37.504217 29015 net.cpp:224] res3a_branch2a needs backward computation.
I0630 01:43:37.504220 29015 net.cpp:224] pool2 needs backward computation.
I0630 01:43:37.504221 29015 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 01:43:37.504223 29015 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 01:43:37.504226 29015 net.cpp:224] res2a_branch2b needs backward computation.
I0630 01:43:37.504228 29015 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 01:43:37.504231 29015 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 01:43:37.504233 29015 net.cpp:224] res2a_branch2a needs backward computation.
I0630 01:43:37.504235 29015 net.cpp:224] pool1 needs backward computation.
I0630 01:43:37.504238 29015 net.cpp:224] conv1b/relu needs backward computation.
I0630 01:43:37.504240 29015 net.cpp:224] conv1b/bn needs backward computation.
I0630 01:43:37.504243 29015 net.cpp:224] conv1b needs backward computation.
I0630 01:43:37.504246 29015 net.cpp:224] conv1a/relu needs backward computation.
I0630 01:43:37.504248 29015 net.cpp:224] conv1a/bn needs backward computation.
I0630 01:43:37.504251 29015 net.cpp:224] conv1a needs backward computation.
I0630 01:43:37.504253 29015 net.cpp:226] data/bias does not need backward computation.
I0630 01:43:37.504257 29015 net.cpp:226] label_data_1_split does not need backward computation.
I0630 01:43:37.504261 29015 net.cpp:226] data does not need backward computation.
I0630 01:43:37.504262 29015 net.cpp:268] This network produces output accuracy/top1
I0630 01:43:37.504264 29015 net.cpp:268] This network produces output accuracy/top5
I0630 01:43:37.504266 29015 net.cpp:268] This network produces output loss
I0630 01:43:37.504289 29015 net.cpp:288] Network initialization done.
I0630 01:43:37.504353 29015 solver.cpp:60] Solver scaffolding done.
I0630 01:43:37.507827 29015 caffe.cpp:145] Finetuning from training/cifar10_jacintonet11v2_2017-06-30_01-13-02/initial/cifar10_jacintonet11v2_iter_64000.caffemodel
I0630 01:43:37.534812 29015 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0630 01:43:37.534884 29015 data_layer.cpp:83] output data size: 21,3,32,32
I0630 01:43:37.993855 29015 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0630 01:43:37.993927 29015 data_layer.cpp:83] output data size: 21,3,32,32
I0630 01:43:38.481310 29015 parallel.cpp:334] Starting Optimization
I0630 01:43:38.481366 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:43:38.490753 29015 solver.cpp:413] Solving jacintonet11v2_train
I0630 01:43:38.490769 29015 solver.cpp:414] Learning Rate Policy: poly
I0630 01:43:38.492677 29015 solver.cpp:471] Iteration 0, Testing net (#0)
I0630 01:43:40.161898 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9171
I0630 01:43:40.161918 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9974
I0630 01:43:40.161926 29015 solver.cpp:544]     Test net output #2: loss = 0.2066 (* 1 = 0.2066 loss)
I0630 01:43:40.264863 29015 solver.cpp:290] Iteration 0 (0 iter/s, 1.77402s/100 iter), loss = 0
I0630 01:43:40.264889 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:40.264897 29015 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0630 01:43:40.275413 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.02
I0630 01:43:40.387543 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:43:42.448305 29015 solver.cpp:290] Iteration 100 (45.8012 iter/s, 2.18335s/100 iter), loss = 0
I0630 01:43:42.448330 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:42.448336 29015 sgd_solver.cpp:106] Iteration 100, lr = 0.00998437
I0630 01:43:44.505753 29015 solver.cpp:290] Iteration 200 (48.606 iter/s, 2.05736s/100 iter), loss = 0
I0630 01:43:44.505775 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:44.505781 29015 sgd_solver.cpp:106] Iteration 200, lr = 0.00996875
I0630 01:43:46.563999 29015 solver.cpp:290] Iteration 300 (48.5871 iter/s, 2.05816s/100 iter), loss = 0
I0630 01:43:46.564020 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:46.564028 29015 sgd_solver.cpp:106] Iteration 300, lr = 0.00995312
I0630 01:43:48.626799 29015 solver.cpp:290] Iteration 400 (48.4798 iter/s, 2.06271s/100 iter), loss = 0
I0630 01:43:48.626822 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:48.626829 29015 sgd_solver.cpp:106] Iteration 400, lr = 0.0099375
I0630 01:43:50.683352 29015 solver.cpp:290] Iteration 500 (48.6272 iter/s, 2.05646s/100 iter), loss = 0
I0630 01:43:50.683375 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:50.683384 29015 sgd_solver.cpp:106] Iteration 500, lr = 0.00992187
I0630 01:43:52.744160 29015 solver.cpp:290] Iteration 600 (48.5268 iter/s, 2.06072s/100 iter), loss = 0
I0630 01:43:52.744182 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:52.744189 29015 sgd_solver.cpp:106] Iteration 600, lr = 0.00990625
I0630 01:43:54.803282 29015 solver.cpp:290] Iteration 700 (48.5665 iter/s, 2.05903s/100 iter), loss = 0
I0630 01:43:54.803304 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:54.803311 29015 sgd_solver.cpp:106] Iteration 700, lr = 0.00989062
I0630 01:43:56.861779 29015 solver.cpp:290] Iteration 800 (48.5812 iter/s, 2.05841s/100 iter), loss = 0
I0630 01:43:56.861801 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:56.861809 29015 sgd_solver.cpp:106] Iteration 800, lr = 0.009875
I0630 01:43:58.926098 29015 solver.cpp:290] Iteration 900 (48.4442 iter/s, 2.06423s/100 iter), loss = 0
I0630 01:43:58.926121 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:43:58.926127 29015 sgd_solver.cpp:106] Iteration 900, lr = 0.00985937
I0630 01:44:00.965198 29015 solver.cpp:354] Sparsity after update:
I0630 01:44:00.966459 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:44:00.966466 29015 net.cpp:1851] conv1a_param_0(0.01) 
I0630 01:44:00.966480 29015 net.cpp:1851] conv1b_param_0(0.02) 
I0630 01:44:00.966485 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:44:00.966490 29015 net.cpp:1851] res2a_branch2a_param_0(0.02) 
I0630 01:44:00.966495 29015 net.cpp:1851] res2a_branch2b_param_0(0.02) 
I0630 01:44:00.966500 29015 net.cpp:1851] res3a_branch2a_param_0(0.02) 
I0630 01:44:00.966513 29015 net.cpp:1851] res3a_branch2b_param_0(0.02) 
I0630 01:44:00.966518 29015 net.cpp:1851] res4a_branch2a_param_0(0.02) 
I0630 01:44:00.966522 29015 net.cpp:1851] res4a_branch2b_param_0(0.02) 
I0630 01:44:00.966527 29015 net.cpp:1851] res5a_branch2a_param_0(0.02) 
I0630 01:44:00.966532 29015 net.cpp:1851] res5a_branch2b_param_0(0.02) 
I0630 01:44:00.966536 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (47057/2.3599e+06) 0.0199
I0630 01:44:00.966629 29015 solver.cpp:471] Iteration 1000, Testing net (#0)
I0630 01:44:02.608512 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.918
I0630 01:44:02.608531 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:44:02.608537 29015 solver.cpp:544]     Test net output #2: loss = 0.2049 (* 1 = 0.2049 loss)
I0630 01:44:02.628374 29015 solver.cpp:290] Iteration 1000 (27.0114 iter/s, 3.70214s/100 iter), loss = 0
I0630 01:44:02.628392 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:02.628404 29015 sgd_solver.cpp:106] Iteration 1000, lr = 0.00984375
I0630 01:44:02.628931 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.04
I0630 01:44:02.761804 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:44:04.821288 29015 solver.cpp:290] Iteration 1100 (45.6033 iter/s, 2.19283s/100 iter), loss = 0
I0630 01:44:04.821312 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:04.821318 29015 sgd_solver.cpp:106] Iteration 1100, lr = 0.00982813
I0630 01:44:06.881510 29015 solver.cpp:290] Iteration 1200 (48.5406 iter/s, 2.06013s/100 iter), loss = 0
I0630 01:44:06.881534 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:06.881543 29015 sgd_solver.cpp:106] Iteration 1200, lr = 0.0098125
I0630 01:44:08.952986 29015 solver.cpp:290] Iteration 1300 (48.2768 iter/s, 2.07139s/100 iter), loss = 0
I0630 01:44:08.953083 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:08.953094 29015 sgd_solver.cpp:106] Iteration 1300, lr = 0.00979687
I0630 01:44:11.011071 29015 solver.cpp:290] Iteration 1400 (48.5926 iter/s, 2.05793s/100 iter), loss = 0
I0630 01:44:11.011093 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:11.011101 29015 sgd_solver.cpp:106] Iteration 1400, lr = 0.00978125
I0630 01:44:13.066612 29015 solver.cpp:290] Iteration 1500 (48.6511 iter/s, 2.05545s/100 iter), loss = 0
I0630 01:44:13.066634 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:13.066642 29015 sgd_solver.cpp:106] Iteration 1500, lr = 0.00976562
I0630 01:44:15.122361 29015 solver.cpp:290] Iteration 1600 (48.6461 iter/s, 2.05566s/100 iter), loss = 0
I0630 01:44:15.122383 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:15.122390 29015 sgd_solver.cpp:106] Iteration 1600, lr = 0.00975
I0630 01:44:17.177491 29015 solver.cpp:290] Iteration 1700 (48.6608 iter/s, 2.05504s/100 iter), loss = 0
I0630 01:44:17.177515 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:17.177523 29015 sgd_solver.cpp:106] Iteration 1700, lr = 0.00973437
I0630 01:44:19.232775 29015 solver.cpp:290] Iteration 1800 (48.6571 iter/s, 2.0552s/100 iter), loss = 0
I0630 01:44:19.232796 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:19.232805 29015 sgd_solver.cpp:106] Iteration 1800, lr = 0.00971875
I0630 01:44:21.287411 29015 solver.cpp:290] Iteration 1900 (48.6725 iter/s, 2.05455s/100 iter), loss = 0
I0630 01:44:21.287436 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:21.287444 29015 sgd_solver.cpp:106] Iteration 1900, lr = 0.00970312
I0630 01:44:23.317715 29015 solver.cpp:354] Sparsity after update:
I0630 01:44:23.318977 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:44:23.318984 29015 net.cpp:1851] conv1a_param_0(0.02) 
I0630 01:44:23.318992 29015 net.cpp:1851] conv1b_param_0(0.0399) 
I0630 01:44:23.318995 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:44:23.318998 29015 net.cpp:1851] res2a_branch2a_param_0(0.04) 
I0630 01:44:23.319000 29015 net.cpp:1851] res2a_branch2b_param_0(0.0399) 
I0630 01:44:23.319002 29015 net.cpp:1851] res3a_branch2a_param_0(0.04) 
I0630 01:44:23.319005 29015 net.cpp:1851] res3a_branch2b_param_0(0.04) 
I0630 01:44:23.319007 29015 net.cpp:1851] res4a_branch2a_param_0(0.04) 
I0630 01:44:23.319010 29015 net.cpp:1851] res4a_branch2b_param_0(0.04) 
I0630 01:44:23.319012 29015 net.cpp:1851] res5a_branch2a_param_0(0.04) 
I0630 01:44:23.319015 29015 net.cpp:1851] res5a_branch2b_param_0(0.04) 
I0630 01:44:23.319016 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (94129/2.3599e+06) 0.0399
I0630 01:44:23.319104 29015 solver.cpp:471] Iteration 2000, Testing net (#0)
I0630 01:44:24.964457 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9175
I0630 01:44:24.964476 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:44:24.964481 29015 solver.cpp:544]     Test net output #2: loss = 0.2027 (* 1 = 0.2027 loss)
I0630 01:44:24.984683 29015 solver.cpp:290] Iteration 2000 (27.0479 iter/s, 3.69714s/100 iter), loss = 0
I0630 01:44:24.984699 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:24.984711 29015 sgd_solver.cpp:106] Iteration 2000, lr = 0.0096875
I0630 01:44:24.985227 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.06
I0630 01:44:25.141501 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:44:27.199268 29015 solver.cpp:290] Iteration 2100 (45.157 iter/s, 2.2145s/100 iter), loss = 0
I0630 01:44:27.199290 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:27.199296 29015 sgd_solver.cpp:106] Iteration 2100, lr = 0.00967188
I0630 01:44:29.249996 29015 solver.cpp:290] Iteration 2200 (48.7652 iter/s, 2.05064s/100 iter), loss = 0
I0630 01:44:29.250018 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:29.250044 29015 sgd_solver.cpp:106] Iteration 2200, lr = 0.00965625
I0630 01:44:31.302518 29015 solver.cpp:290] Iteration 2300 (48.7226 iter/s, 2.05243s/100 iter), loss = 0
I0630 01:44:31.302543 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:31.302552 29015 sgd_solver.cpp:106] Iteration 2300, lr = 0.00964062
I0630 01:44:33.366215 29015 solver.cpp:290] Iteration 2400 (48.4588 iter/s, 2.06361s/100 iter), loss = 0
I0630 01:44:33.366238 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:33.366245 29015 sgd_solver.cpp:106] Iteration 2400, lr = 0.009625
I0630 01:44:35.418342 29015 solver.cpp:290] Iteration 2500 (48.732 iter/s, 2.05204s/100 iter), loss = 0
I0630 01:44:35.418365 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:35.418371 29015 sgd_solver.cpp:106] Iteration 2500, lr = 0.00960938
I0630 01:44:37.475153 29015 solver.cpp:290] Iteration 2600 (48.621 iter/s, 2.05672s/100 iter), loss = 0
I0630 01:44:37.475177 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:37.475183 29015 sgd_solver.cpp:106] Iteration 2600, lr = 0.00959375
I0630 01:44:39.526470 29015 solver.cpp:290] Iteration 2700 (48.7513 iter/s, 2.05123s/100 iter), loss = 0
I0630 01:44:39.526537 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:39.526545 29015 sgd_solver.cpp:106] Iteration 2700, lr = 0.00957812
I0630 01:44:41.579648 29015 solver.cpp:290] Iteration 2800 (48.7081 iter/s, 2.05305s/100 iter), loss = 0
I0630 01:44:41.579670 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:41.579679 29015 sgd_solver.cpp:106] Iteration 2800, lr = 0.0095625
I0630 01:44:43.634135 29015 solver.cpp:290] Iteration 2900 (48.676 iter/s, 2.0544s/100 iter), loss = 0
I0630 01:44:43.634157 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:43.634165 29015 sgd_solver.cpp:106] Iteration 2900, lr = 0.00954687
I0630 01:44:45.672564 29015 solver.cpp:354] Sparsity after update:
I0630 01:44:45.673852 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:44:45.673858 29015 net.cpp:1851] conv1a_param_0(0.03) 
I0630 01:44:45.673867 29015 net.cpp:1851] conv1b_param_0(0.0595) 
I0630 01:44:45.673871 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:44:45.673873 29015 net.cpp:1851] res2a_branch2a_param_0(0.06) 
I0630 01:44:45.673877 29015 net.cpp:1851] res2a_branch2b_param_0(0.0599) 
I0630 01:44:45.673879 29015 net.cpp:1851] res3a_branch2a_param_0(0.06) 
I0630 01:44:45.673882 29015 net.cpp:1851] res3a_branch2b_param_0(0.06) 
I0630 01:44:45.673884 29015 net.cpp:1851] res4a_branch2a_param_0(0.06) 
I0630 01:44:45.673887 29015 net.cpp:1851] res4a_branch2b_param_0(0.06) 
I0630 01:44:45.673889 29015 net.cpp:1851] res5a_branch2a_param_0(0.06) 
I0630 01:44:45.673892 29015 net.cpp:1851] res5a_branch2b_param_0(0.06) 
I0630 01:44:45.673893 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (141205/2.3599e+06) 0.0598
I0630 01:44:45.673981 29015 solver.cpp:471] Iteration 3000, Testing net (#0)
I0630 01:44:47.314412 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9176
I0630 01:44:47.314432 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9971
I0630 01:44:47.314437 29015 solver.cpp:544]     Test net output #2: loss = 0.2044 (* 1 = 0.2044 loss)
I0630 01:44:47.334811 29015 solver.cpp:290] Iteration 3000 (27.0231 iter/s, 3.70054s/100 iter), loss = 0
I0630 01:44:47.334836 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:47.334843 29015 sgd_solver.cpp:106] Iteration 3000, lr = 0.00953125
I0630 01:44:47.335373 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.08
I0630 01:44:47.510507 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:44:49.567606 29015 solver.cpp:290] Iteration 3100 (44.7887 iter/s, 2.2327s/100 iter), loss = 0
I0630 01:44:49.567628 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:49.567634 29015 sgd_solver.cpp:106] Iteration 3100, lr = 0.00951563
I0630 01:44:51.622885 29015 solver.cpp:290] Iteration 3200 (48.6572 iter/s, 2.05519s/100 iter), loss = 0
I0630 01:44:51.622908 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:51.622915 29015 sgd_solver.cpp:106] Iteration 3200, lr = 0.0095
I0630 01:44:53.676576 29015 solver.cpp:290] Iteration 3300 (48.6949 iter/s, 2.0536s/100 iter), loss = 0
I0630 01:44:53.676599 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:53.676605 29015 sgd_solver.cpp:106] Iteration 3300, lr = 0.00948437
I0630 01:44:55.734683 29015 solver.cpp:290] Iteration 3400 (48.5904 iter/s, 2.05802s/100 iter), loss = 0
I0630 01:44:55.734705 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:55.734712 29015 sgd_solver.cpp:106] Iteration 3400, lr = 0.00946875
I0630 01:44:57.792510 29015 solver.cpp:290] Iteration 3500 (48.5971 iter/s, 2.05774s/100 iter), loss = 0
I0630 01:44:57.792531 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:57.792538 29015 sgd_solver.cpp:106] Iteration 3500, lr = 0.00945312
I0630 01:44:59.850078 29015 solver.cpp:290] Iteration 3600 (48.6031 iter/s, 2.05748s/100 iter), loss = 0
I0630 01:44:59.850100 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:44:59.850123 29015 sgd_solver.cpp:106] Iteration 3600, lr = 0.0094375
I0630 01:45:01.905992 29015 solver.cpp:290] Iteration 3700 (48.6423 iter/s, 2.05583s/100 iter), loss = 0
I0630 01:45:01.906013 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:01.906020 29015 sgd_solver.cpp:106] Iteration 3700, lr = 0.00942187
I0630 01:45:03.961921 29015 solver.cpp:290] Iteration 3800 (48.6419 iter/s, 2.05584s/100 iter), loss = 0
I0630 01:45:03.961942 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:03.961951 29015 sgd_solver.cpp:106] Iteration 3800, lr = 0.00940625
I0630 01:45:06.019665 29015 solver.cpp:290] Iteration 3900 (48.599 iter/s, 2.05766s/100 iter), loss = 0
I0630 01:45:06.019688 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:06.019695 29015 sgd_solver.cpp:106] Iteration 3900, lr = 0.00939062
I0630 01:45:08.064386 29015 solver.cpp:354] Sparsity after update:
I0630 01:45:08.065672 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:45:08.065680 29015 net.cpp:1851] conv1a_param_0(0.04) 
I0630 01:45:08.065687 29015 net.cpp:1851] conv1b_param_0(0.0799) 
I0630 01:45:08.065690 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:45:08.065692 29015 net.cpp:1851] res2a_branch2a_param_0(0.08) 
I0630 01:45:08.065696 29015 net.cpp:1851] res2a_branch2b_param_0(0.08) 
I0630 01:45:08.065697 29015 net.cpp:1851] res3a_branch2a_param_0(0.08) 
I0630 01:45:08.065701 29015 net.cpp:1851] res3a_branch2b_param_0(0.08) 
I0630 01:45:08.065702 29015 net.cpp:1851] res4a_branch2a_param_0(0.08) 
I0630 01:45:08.065706 29015 net.cpp:1851] res4a_branch2b_param_0(0.08) 
I0630 01:45:08.065707 29015 net.cpp:1851] res5a_branch2a_param_0(0.08) 
I0630 01:45:08.065709 29015 net.cpp:1851] res5a_branch2b_param_0(0.08) 
I0630 01:45:08.065711 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (188276/2.3599e+06) 0.0798
I0630 01:45:08.065798 29015 solver.cpp:471] Iteration 4000, Testing net (#0)
I0630 01:45:09.704046 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9179
I0630 01:45:09.704115 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:45:09.704123 29015 solver.cpp:544]     Test net output #2: loss = 0.1999 (* 1 = 0.1999 loss)
I0630 01:45:09.723783 29015 solver.cpp:290] Iteration 4000 (26.9979 iter/s, 3.70399s/100 iter), loss = 0
I0630 01:45:09.723799 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:09.723814 29015 sgd_solver.cpp:106] Iteration 4000, lr = 0.009375
I0630 01:45:09.724346 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.1
I0630 01:45:09.914379 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:45:11.974668 29015 solver.cpp:290] Iteration 4100 (44.4287 iter/s, 2.2508s/100 iter), loss = 0
I0630 01:45:11.974690 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:11.974696 29015 sgd_solver.cpp:106] Iteration 4100, lr = 0.00935937
I0630 01:45:14.030148 29015 solver.cpp:290] Iteration 4200 (48.6525 iter/s, 2.05539s/100 iter), loss = 0
I0630 01:45:14.030170 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:14.030177 29015 sgd_solver.cpp:106] Iteration 4200, lr = 0.00934375
I0630 01:45:16.089253 29015 solver.cpp:290] Iteration 4300 (48.5669 iter/s, 2.05902s/100 iter), loss = 0
I0630 01:45:16.089275 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:16.089282 29015 sgd_solver.cpp:106] Iteration 4300, lr = 0.00932813
I0630 01:45:18.145922 29015 solver.cpp:290] Iteration 4400 (48.6244 iter/s, 2.05658s/100 iter), loss = 0
I0630 01:45:18.145943 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:18.145951 29015 sgd_solver.cpp:106] Iteration 4400, lr = 0.0093125
I0630 01:45:20.201972 29015 solver.cpp:290] Iteration 4500 (48.639 iter/s, 2.05596s/100 iter), loss = 0
I0630 01:45:20.202003 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:20.202009 29015 sgd_solver.cpp:106] Iteration 4500, lr = 0.00929687
I0630 01:45:22.256750 29015 solver.cpp:290] Iteration 4600 (48.6693 iter/s, 2.05468s/100 iter), loss = 0
I0630 01:45:22.256772 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:22.256778 29015 sgd_solver.cpp:106] Iteration 4600, lr = 0.00928125
I0630 01:45:24.309942 29015 solver.cpp:290] Iteration 4700 (48.7067 iter/s, 2.05311s/100 iter), loss = 0
I0630 01:45:24.309964 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:24.309972 29015 sgd_solver.cpp:106] Iteration 4700, lr = 0.00926562
I0630 01:45:26.367472 29015 solver.cpp:290] Iteration 4800 (48.604 iter/s, 2.05744s/100 iter), loss = 0
I0630 01:45:26.367494 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:26.367503 29015 sgd_solver.cpp:106] Iteration 4800, lr = 0.00925
I0630 01:45:28.423151 29015 solver.cpp:290] Iteration 4900 (48.6478 iter/s, 2.05559s/100 iter), loss = 0
I0630 01:45:28.423173 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:28.423180 29015 sgd_solver.cpp:106] Iteration 4900, lr = 0.00923437
I0630 01:45:30.457130 29015 solver.cpp:354] Sparsity after update:
I0630 01:45:30.458395 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:45:30.458401 29015 net.cpp:1851] conv1a_param_0(0.0492) 
I0630 01:45:30.458411 29015 net.cpp:1851] conv1b_param_0(0.0998) 
I0630 01:45:30.458415 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:45:30.458420 29015 net.cpp:1851] res2a_branch2a_param_0(0.1) 
I0630 01:45:30.458422 29015 net.cpp:1851] res2a_branch2b_param_0(0.0999) 
I0630 01:45:30.458425 29015 net.cpp:1851] res3a_branch2a_param_0(0.1) 
I0630 01:45:30.458426 29015 net.cpp:1851] res3a_branch2b_param_0(0.1) 
I0630 01:45:30.458428 29015 net.cpp:1851] res4a_branch2a_param_0(0.1) 
I0630 01:45:30.458431 29015 net.cpp:1851] res4a_branch2b_param_0(0.1) 
I0630 01:45:30.458432 29015 net.cpp:1851] res5a_branch2a_param_0(0.1) 
I0630 01:45:30.458434 29015 net.cpp:1851] res5a_branch2b_param_0(0.1) 
I0630 01:45:30.458436 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (235348/2.3599e+06) 0.0997
I0630 01:45:30.458539 29015 solver.cpp:471] Iteration 5000, Testing net (#0)
I0630 01:45:32.095787 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9172
I0630 01:45:32.095804 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9973
I0630 01:45:32.095809 29015 solver.cpp:544]     Test net output #2: loss = 0.2018 (* 1 = 0.2018 loss)
I0630 01:45:32.115591 29015 solver.cpp:290] Iteration 5000 (27.0834 iter/s, 3.6923s/100 iter), loss = 0
I0630 01:45:32.115612 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:32.115618 29015 sgd_solver.cpp:106] Iteration 5000, lr = 0.00921875
I0630 01:45:32.116143 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.12
I0630 01:45:32.328747 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:45:34.396525 29015 solver.cpp:290] Iteration 5100 (43.8435 iter/s, 2.28084s/100 iter), loss = 0
I0630 01:45:34.396548 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:34.396555 29015 sgd_solver.cpp:106] Iteration 5100, lr = 0.00920312
I0630 01:45:36.452087 29015 solver.cpp:290] Iteration 5200 (48.6506 iter/s, 2.05547s/100 iter), loss = 0
I0630 01:45:36.452109 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:36.452116 29015 sgd_solver.cpp:106] Iteration 5200, lr = 0.0091875
I0630 01:45:38.509613 29015 solver.cpp:290] Iteration 5300 (48.6041 iter/s, 2.05744s/100 iter), loss = 0
I0630 01:45:38.509635 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:38.509644 29015 sgd_solver.cpp:106] Iteration 5300, lr = 0.00917188
I0630 01:45:40.563597 29015 solver.cpp:290] Iteration 5400 (48.6879 iter/s, 2.0539s/100 iter), loss = 0
I0630 01:45:40.563657 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:40.563670 29015 sgd_solver.cpp:106] Iteration 5400, lr = 0.00915625
I0630 01:45:42.619472 29015 solver.cpp:290] Iteration 5500 (48.644 iter/s, 2.05575s/100 iter), loss = 0
I0630 01:45:42.619494 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:42.619501 29015 sgd_solver.cpp:106] Iteration 5500, lr = 0.00914062
I0630 01:45:44.672056 29015 solver.cpp:290] Iteration 5600 (48.7211 iter/s, 2.0525s/100 iter), loss = 0
I0630 01:45:44.672080 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:44.672089 29015 sgd_solver.cpp:106] Iteration 5600, lr = 0.009125
I0630 01:45:46.728898 29015 solver.cpp:290] Iteration 5700 (48.6203 iter/s, 2.05675s/100 iter), loss = 0
I0630 01:45:46.728922 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:46.728931 29015 sgd_solver.cpp:106] Iteration 5700, lr = 0.00910938
I0630 01:45:48.786849 29015 solver.cpp:290] Iteration 5800 (48.5941 iter/s, 2.05786s/100 iter), loss = 0
I0630 01:45:48.786870 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:48.786876 29015 sgd_solver.cpp:106] Iteration 5800, lr = 0.00909375
I0630 01:45:50.843174 29015 solver.cpp:290] Iteration 5900 (48.6325 iter/s, 2.05624s/100 iter), loss = 0
I0630 01:45:50.843197 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:50.843204 29015 sgd_solver.cpp:106] Iteration 5900, lr = 0.00907812
I0630 01:45:52.880007 29015 solver.cpp:354] Sparsity after update:
I0630 01:45:52.881274 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:45:52.881281 29015 net.cpp:1851] conv1a_param_0(0.0596) 
I0630 01:45:52.881290 29015 net.cpp:1851] conv1b_param_0(0.12) 
I0630 01:45:52.881294 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:45:52.881295 29015 net.cpp:1851] res2a_branch2a_param_0(0.12) 
I0630 01:45:52.881297 29015 net.cpp:1851] res2a_branch2b_param_0(0.12) 
I0630 01:45:52.881299 29015 net.cpp:1851] res3a_branch2a_param_0(0.12) 
I0630 01:45:52.881302 29015 net.cpp:1851] res3a_branch2b_param_0(0.12) 
I0630 01:45:52.881304 29015 net.cpp:1851] res4a_branch2a_param_0(0.12) 
I0630 01:45:52.881307 29015 net.cpp:1851] res4a_branch2b_param_0(0.12) 
I0630 01:45:52.881309 29015 net.cpp:1851] res5a_branch2a_param_0(0.12) 
I0630 01:45:52.881312 29015 net.cpp:1851] res5a_branch2b_param_0(0.12) 
I0630 01:45:52.881314 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (282414/2.3599e+06) 0.12
I0630 01:45:52.881402 29015 solver.cpp:471] Iteration 6000, Testing net (#0)
I0630 01:45:54.518863 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9187
I0630 01:45:54.518885 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9972
I0630 01:45:54.518892 29015 solver.cpp:544]     Test net output #2: loss = 0.2019 (* 1 = 0.2019 loss)
I0630 01:45:54.538581 29015 solver.cpp:290] Iteration 6000 (27.0616 iter/s, 3.69527s/100 iter), loss = 0
I0630 01:45:54.538600 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:54.538609 29015 sgd_solver.cpp:106] Iteration 6000, lr = 0.0090625
I0630 01:45:54.539093 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.14
I0630 01:45:54.773087 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:45:56.829032 29015 solver.cpp:290] Iteration 6100 (43.6613 iter/s, 2.29036s/100 iter), loss = 0
I0630 01:45:56.829066 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:56.829078 29015 sgd_solver.cpp:106] Iteration 6100, lr = 0.00904687
I0630 01:45:58.881546 29015 solver.cpp:290] Iteration 6200 (48.7231 iter/s, 2.05242s/100 iter), loss = 0
I0630 01:45:58.881567 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:45:58.881575 29015 sgd_solver.cpp:106] Iteration 6200, lr = 0.00903125
I0630 01:46:00.940690 29015 solver.cpp:290] Iteration 6300 (48.5659 iter/s, 2.05906s/100 iter), loss = 0
I0630 01:46:00.940713 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:00.940739 29015 sgd_solver.cpp:106] Iteration 6300, lr = 0.00901563
I0630 01:46:02.999428 29015 solver.cpp:290] Iteration 6400 (48.5756 iter/s, 2.05865s/100 iter), loss = 0
I0630 01:46:02.999450 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:02.999456 29015 sgd_solver.cpp:106] Iteration 6400, lr = 0.009
I0630 01:46:05.057929 29015 solver.cpp:290] Iteration 6500 (48.5811 iter/s, 2.05841s/100 iter), loss = 0
I0630 01:46:05.057950 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:05.057956 29015 sgd_solver.cpp:106] Iteration 6500, lr = 0.00898437
I0630 01:46:07.116565 29015 solver.cpp:290] Iteration 6600 (48.5779 iter/s, 2.05855s/100 iter), loss = 0
I0630 01:46:07.116587 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:07.116593 29015 sgd_solver.cpp:106] Iteration 6600, lr = 0.00896875
I0630 01:46:09.197156 29015 solver.cpp:290] Iteration 6700 (48.0653 iter/s, 2.0805s/100 iter), loss = 0
I0630 01:46:09.197178 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:09.197185 29015 sgd_solver.cpp:106] Iteration 6700, lr = 0.00895312
I0630 01:46:11.250587 29015 solver.cpp:290] Iteration 6800 (48.7011 iter/s, 2.05334s/100 iter), loss = 0
I0630 01:46:11.250668 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:11.250676 29015 sgd_solver.cpp:106] Iteration 6800, lr = 0.0089375
I0630 01:46:13.307732 29015 solver.cpp:290] Iteration 6900 (48.6145 iter/s, 2.057s/100 iter), loss = 0
I0630 01:46:13.307757 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:13.307765 29015 sgd_solver.cpp:106] Iteration 6900, lr = 0.00892187
I0630 01:46:15.345660 29015 solver.cpp:354] Sparsity after update:
I0630 01:46:15.346945 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:46:15.346951 29015 net.cpp:1851] conv1a_param_0(0.0696) 
I0630 01:46:15.346958 29015 net.cpp:1851] conv1b_param_0(0.14) 
I0630 01:46:15.346961 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:46:15.346963 29015 net.cpp:1851] res2a_branch2a_param_0(0.14) 
I0630 01:46:15.346966 29015 net.cpp:1851] res2a_branch2b_param_0(0.14) 
I0630 01:46:15.346967 29015 net.cpp:1851] res3a_branch2a_param_0(0.14) 
I0630 01:46:15.346968 29015 net.cpp:1851] res3a_branch2b_param_0(0.14) 
I0630 01:46:15.346971 29015 net.cpp:1851] res4a_branch2a_param_0(0.14) 
I0630 01:46:15.346972 29015 net.cpp:1851] res4a_branch2b_param_0(0.14) 
I0630 01:46:15.346974 29015 net.cpp:1851] res5a_branch2a_param_0(0.14) 
I0630 01:46:15.346976 29015 net.cpp:1851] res5a_branch2b_param_0(0.14) 
I0630 01:46:15.346978 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (329481/2.3599e+06) 0.14
I0630 01:46:15.347069 29015 solver.cpp:471] Iteration 7000, Testing net (#0)
I0630 01:46:16.986615 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9181
I0630 01:46:16.986636 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9971
I0630 01:46:16.986641 29015 solver.cpp:544]     Test net output #2: loss = 0.2022 (* 1 = 0.2022 loss)
I0630 01:46:17.010870 29015 solver.cpp:290] Iteration 7000 (27.0051 iter/s, 3.703s/100 iter), loss = 0
I0630 01:46:17.010887 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:17.010900 29015 sgd_solver.cpp:106] Iteration 7000, lr = 0.00890625
I0630 01:46:17.011425 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.16
I0630 01:46:17.262126 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:46:19.316997 29015 solver.cpp:290] Iteration 7100 (43.3644 iter/s, 2.30604s/100 iter), loss = 0
I0630 01:46:19.317018 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:19.317025 29015 sgd_solver.cpp:106] Iteration 7100, lr = 0.00889063
I0630 01:46:21.374128 29015 solver.cpp:290] Iteration 7200 (48.6134 iter/s, 2.05704s/100 iter), loss = 0
I0630 01:46:21.374150 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:21.374157 29015 sgd_solver.cpp:106] Iteration 7200, lr = 0.008875
I0630 01:46:23.427958 29015 solver.cpp:290] Iteration 7300 (48.6916 iter/s, 2.05374s/100 iter), loss = 0
I0630 01:46:23.427980 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:23.427986 29015 sgd_solver.cpp:106] Iteration 7300, lr = 0.00885937
I0630 01:46:25.483090 29015 solver.cpp:290] Iteration 7400 (48.6608 iter/s, 2.05504s/100 iter), loss = 0
I0630 01:46:25.483111 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:25.483119 29015 sgd_solver.cpp:106] Iteration 7400, lr = 0.00884375
I0630 01:46:27.538563 29015 solver.cpp:290] Iteration 7500 (48.6527 iter/s, 2.05538s/100 iter), loss = 0
I0630 01:46:27.538583 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:27.538590 29015 sgd_solver.cpp:106] Iteration 7500, lr = 0.00882812
I0630 01:46:29.595995 29015 solver.cpp:290] Iteration 7600 (48.6063 iter/s, 2.05734s/100 iter), loss = 0
I0630 01:46:29.596021 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:29.596030 29015 sgd_solver.cpp:106] Iteration 7600, lr = 0.0088125
I0630 01:46:31.652079 29015 solver.cpp:290] Iteration 7700 (48.6383 iter/s, 2.05599s/100 iter), loss = 0
I0630 01:46:31.652102 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:31.652125 29015 sgd_solver.cpp:106] Iteration 7700, lr = 0.00879687
I0630 01:46:33.708396 29015 solver.cpp:290] Iteration 7800 (48.6327 iter/s, 2.05623s/100 iter), loss = 0
I0630 01:46:33.708418 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:33.708425 29015 sgd_solver.cpp:106] Iteration 7800, lr = 0.00878125
I0630 01:46:35.767139 29015 solver.cpp:290] Iteration 7900 (48.5754 iter/s, 2.05866s/100 iter), loss = 0
I0630 01:46:35.767163 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:35.767169 29015 sgd_solver.cpp:106] Iteration 7900, lr = 0.00876562
I0630 01:46:37.806466 29015 solver.cpp:354] Sparsity after update:
I0630 01:46:37.807727 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:46:37.807734 29015 net.cpp:1851] conv1a_param_0(0.0796) 
I0630 01:46:37.807742 29015 net.cpp:1851] conv1b_param_0(0.16) 
I0630 01:46:37.807745 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:46:37.807749 29015 net.cpp:1851] res2a_branch2a_param_0(0.16) 
I0630 01:46:37.807750 29015 net.cpp:1851] res2a_branch2b_param_0(0.16) 
I0630 01:46:37.807752 29015 net.cpp:1851] res3a_branch2a_param_0(0.16) 
I0630 01:46:37.807755 29015 net.cpp:1851] res3a_branch2b_param_0(0.16) 
I0630 01:46:37.807757 29015 net.cpp:1851] res4a_branch2a_param_0(0.16) 
I0630 01:46:37.807760 29015 net.cpp:1851] res4a_branch2b_param_0(0.16) 
I0630 01:46:37.807762 29015 net.cpp:1851] res5a_branch2a_param_0(0.16) 
I0630 01:46:37.807765 29015 net.cpp:1851] res5a_branch2b_param_0(0.16) 
I0630 01:46:37.807766 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (376559/2.3599e+06) 0.16
I0630 01:46:37.807899 29015 solver.cpp:471] Iteration 8000, Testing net (#0)
I0630 01:46:39.444798 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9183
I0630 01:46:39.444818 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9972
I0630 01:46:39.444823 29015 solver.cpp:544]     Test net output #2: loss = 0.2034 (* 1 = 0.2034 loss)
I0630 01:46:39.464555 29015 solver.cpp:290] Iteration 8000 (27.0469 iter/s, 3.69728s/100 iter), loss = 0
I0630 01:46:39.464572 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:39.464584 29015 sgd_solver.cpp:106] Iteration 8000, lr = 0.00875
I0630 01:46:39.465090 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.18
I0630 01:46:39.735254 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:46:41.795044 29015 solver.cpp:290] Iteration 8100 (42.9111 iter/s, 2.3304s/100 iter), loss = 0
I0630 01:46:41.795126 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:41.795135 29015 sgd_solver.cpp:106] Iteration 8100, lr = 0.00873438
I0630 01:46:43.849437 29015 solver.cpp:290] Iteration 8200 (48.6797 iter/s, 2.05425s/100 iter), loss = 0
I0630 01:46:43.849462 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:43.849468 29015 sgd_solver.cpp:106] Iteration 8200, lr = 0.00871875
I0630 01:46:45.909889 29015 solver.cpp:290] Iteration 8300 (48.5351 iter/s, 2.06036s/100 iter), loss = 0
I0630 01:46:45.909914 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:45.909922 29015 sgd_solver.cpp:106] Iteration 8300, lr = 0.00870312
I0630 01:46:47.967157 29015 solver.cpp:290] Iteration 8400 (48.6102 iter/s, 2.05718s/100 iter), loss = 0
I0630 01:46:47.967180 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:47.967187 29015 sgd_solver.cpp:106] Iteration 8400, lr = 0.0086875
I0630 01:46:50.023327 29015 solver.cpp:290] Iteration 8500 (48.6362 iter/s, 2.05608s/100 iter), loss = 0
I0630 01:46:50.023350 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:50.023355 29015 sgd_solver.cpp:106] Iteration 8500, lr = 0.00867188
I0630 01:46:52.080648 29015 solver.cpp:290] Iteration 8600 (48.609 iter/s, 2.05723s/100 iter), loss = 0
I0630 01:46:52.080672 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:52.080680 29015 sgd_solver.cpp:106] Iteration 8600, lr = 0.00865625
I0630 01:46:54.140341 29015 solver.cpp:290] Iteration 8700 (48.553 iter/s, 2.05961s/100 iter), loss = 0
I0630 01:46:54.140363 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:54.140369 29015 sgd_solver.cpp:106] Iteration 8700, lr = 0.00864062
I0630 01:46:56.192591 29015 solver.cpp:290] Iteration 8800 (48.7291 iter/s, 2.05216s/100 iter), loss = 0
I0630 01:46:56.192613 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:56.192620 29015 sgd_solver.cpp:106] Iteration 8800, lr = 0.008625
I0630 01:46:58.251145 29015 solver.cpp:290] Iteration 8900 (48.5798 iter/s, 2.05847s/100 iter), loss = 0
I0630 01:46:58.251168 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:46:58.251174 29015 sgd_solver.cpp:106] Iteration 8900, lr = 0.00860937
I0630 01:47:00.287670 29015 solver.cpp:354] Sparsity after update:
I0630 01:47:00.288939 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:47:00.288946 29015 net.cpp:1851] conv1a_param_0(0.0896) 
I0630 01:47:00.288956 29015 net.cpp:1851] conv1b_param_0(0.18) 
I0630 01:47:00.288961 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:47:00.288966 29015 net.cpp:1851] res2a_branch2a_param_0(0.18) 
I0630 01:47:00.288970 29015 net.cpp:1851] res2a_branch2b_param_0(0.18) 
I0630 01:47:00.288975 29015 net.cpp:1851] res3a_branch2a_param_0(0.18) 
I0630 01:47:00.288980 29015 net.cpp:1851] res3a_branch2b_param_0(0.18) 
I0630 01:47:00.288985 29015 net.cpp:1851] res4a_branch2a_param_0(0.18) 
I0630 01:47:00.288990 29015 net.cpp:1851] res4a_branch2b_param_0(0.18) 
I0630 01:47:00.288995 29015 net.cpp:1851] res5a_branch2a_param_0(0.18) 
I0630 01:47:00.289000 29015 net.cpp:1851] res5a_branch2b_param_0(0.18) 
I0630 01:47:00.289005 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (423631/2.3599e+06) 0.18
I0630 01:47:00.289098 29015 solver.cpp:471] Iteration 9000, Testing net (#0)
I0630 01:47:01.927242 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9185
I0630 01:47:01.927260 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:47:01.927266 29015 solver.cpp:544]     Test net output #2: loss = 0.1984 (* 1 = 0.1984 loss)
I0630 01:47:01.947773 29015 solver.cpp:290] Iteration 9000 (27.0527 iter/s, 3.69649s/100 iter), loss = 0
I0630 01:47:01.947796 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:01.947805 29015 sgd_solver.cpp:106] Iteration 9000, lr = 0.00859375
I0630 01:47:01.948376 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.2
I0630 01:47:02.234014 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:47:04.292466 29015 solver.cpp:290] Iteration 9100 (42.6512 iter/s, 2.3446s/100 iter), loss = 0
I0630 01:47:04.292490 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:04.292495 29015 sgd_solver.cpp:106] Iteration 9100, lr = 0.00857813
I0630 01:47:06.349726 29015 solver.cpp:290] Iteration 9200 (48.6104 iter/s, 2.05717s/100 iter), loss = 0
I0630 01:47:06.349750 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:06.349759 29015 sgd_solver.cpp:106] Iteration 9200, lr = 0.0085625
I0630 01:47:08.420156 29015 solver.cpp:290] Iteration 9300 (48.3012 iter/s, 2.07034s/100 iter), loss = 0
I0630 01:47:08.420176 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:08.420184 29015 sgd_solver.cpp:106] Iteration 9300, lr = 0.00854687
I0630 01:47:10.476302 29015 solver.cpp:290] Iteration 9400 (48.6367 iter/s, 2.05606s/100 iter), loss = 0
I0630 01:47:10.476325 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:10.476332 29015 sgd_solver.cpp:106] Iteration 9400, lr = 0.00853125
I0630 01:47:12.535446 29015 solver.cpp:290] Iteration 9500 (48.566 iter/s, 2.05906s/100 iter), loss = 0
I0630 01:47:12.535501 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:12.535509 29015 sgd_solver.cpp:106] Iteration 9500, lr = 0.00851563
I0630 01:47:14.594506 29015 solver.cpp:290] Iteration 9600 (48.5686 iter/s, 2.05894s/100 iter), loss = 0
I0630 01:47:14.594527 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:14.594534 29015 sgd_solver.cpp:106] Iteration 9600, lr = 0.0085
I0630 01:47:16.652477 29015 solver.cpp:290] Iteration 9700 (48.5936 iter/s, 2.05788s/100 iter), loss = 0
I0630 01:47:16.652503 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:16.652511 29015 sgd_solver.cpp:106] Iteration 9700, lr = 0.00848437
I0630 01:47:18.706369 29015 solver.cpp:290] Iteration 9800 (48.6902 iter/s, 2.0538s/100 iter), loss = 0
I0630 01:47:18.706393 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:18.706398 29015 sgd_solver.cpp:106] Iteration 9800, lr = 0.00846875
I0630 01:47:20.763650 29015 solver.cpp:290] Iteration 9900 (48.6099 iter/s, 2.05719s/100 iter), loss = 0
I0630 01:47:20.763672 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:20.763679 29015 sgd_solver.cpp:106] Iteration 9900, lr = 0.00845312
I0630 01:47:22.800977 29015 solver.cpp:598] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_10000.caffemodel
I0630 01:47:22.826220 29015 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_10000.solverstate
I0630 01:47:22.833421 29015 solver.cpp:354] Sparsity after update:
I0630 01:47:22.834362 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:47:22.834369 29015 net.cpp:1851] conv1a_param_0(0.0996) 
I0630 01:47:22.834378 29015 net.cpp:1851] conv1b_param_0(0.2) 
I0630 01:47:22.834380 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:47:22.834383 29015 net.cpp:1851] res2a_branch2a_param_0(0.2) 
I0630 01:47:22.834385 29015 net.cpp:1851] res2a_branch2b_param_0(0.2) 
I0630 01:47:22.834388 29015 net.cpp:1851] res3a_branch2a_param_0(0.2) 
I0630 01:47:22.834388 29015 net.cpp:1851] res3a_branch2b_param_0(0.2) 
I0630 01:47:22.834390 29015 net.cpp:1851] res4a_branch2a_param_0(0.2) 
I0630 01:47:22.834393 29015 net.cpp:1851] res4a_branch2b_param_0(0.2) 
I0630 01:47:22.834394 29015 net.cpp:1851] res5a_branch2a_param_0(0.2) 
I0630 01:47:22.834398 29015 net.cpp:1851] res5a_branch2b_param_0(0.2) 
I0630 01:47:22.834399 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (470704/2.3599e+06) 0.199
I0630 01:47:22.834496 29015 solver.cpp:471] Iteration 10000, Testing net (#0)
I0630 01:47:24.471724 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9175
I0630 01:47:24.471743 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:47:24.471750 29015 solver.cpp:544]     Test net output #2: loss = 0.1996 (* 1 = 0.1996 loss)
I0630 01:47:24.491643 29015 solver.cpp:290] Iteration 10000 (26.8251 iter/s, 3.72786s/100 iter), loss = 0
I0630 01:47:24.491667 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:24.491673 29015 sgd_solver.cpp:106] Iteration 10000, lr = 0.0084375
I0630 01:47:24.492228 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.22
I0630 01:47:24.798264 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:47:26.852598 29015 solver.cpp:290] Iteration 10100 (42.3575 iter/s, 2.36086s/100 iter), loss = 0
I0630 01:47:26.852622 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:26.852628 29015 sgd_solver.cpp:106] Iteration 10100, lr = 0.00842187
I0630 01:47:28.906924 29015 solver.cpp:290] Iteration 10200 (48.6799 iter/s, 2.05424s/100 iter), loss = 0
I0630 01:47:28.906946 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:28.906954 29015 sgd_solver.cpp:106] Iteration 10200, lr = 0.00840625
I0630 01:47:30.960723 29015 solver.cpp:290] Iteration 10300 (48.6923 iter/s, 2.05371s/100 iter), loss = 0
I0630 01:47:30.960758 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:30.960765 29015 sgd_solver.cpp:106] Iteration 10300, lr = 0.00839063
I0630 01:47:33.019588 29015 solver.cpp:290] Iteration 10400 (48.5728 iter/s, 2.05876s/100 iter), loss = 0
I0630 01:47:33.019616 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:33.019625 29015 sgd_solver.cpp:106] Iteration 10400, lr = 0.008375
I0630 01:47:35.075844 29015 solver.cpp:290] Iteration 10500 (48.6343 iter/s, 2.05616s/100 iter), loss = 0
I0630 01:47:35.075871 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:35.075881 29015 sgd_solver.cpp:106] Iteration 10500, lr = 0.00835937
I0630 01:47:37.136994 29015 solver.cpp:290] Iteration 10600 (48.5187 iter/s, 2.06106s/100 iter), loss = 0
I0630 01:47:37.137017 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:37.137023 29015 sgd_solver.cpp:106] Iteration 10600, lr = 0.00834375
I0630 01:47:39.193583 29015 solver.cpp:290] Iteration 10700 (48.6263 iter/s, 2.0565s/100 iter), loss = 0
I0630 01:47:39.193603 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:39.193611 29015 sgd_solver.cpp:106] Iteration 10700, lr = 0.00832812
I0630 01:47:41.248898 29015 solver.cpp:290] Iteration 10800 (48.6563 iter/s, 2.05523s/100 iter), loss = 0
I0630 01:47:41.248920 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:41.248929 29015 sgd_solver.cpp:106] Iteration 10800, lr = 0.0083125
I0630 01:47:43.301059 29015 solver.cpp:290] Iteration 10900 (48.7312 iter/s, 2.05207s/100 iter), loss = 0
I0630 01:47:43.301126 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:43.301132 29015 sgd_solver.cpp:106] Iteration 10900, lr = 0.00829687
I0630 01:47:45.343042 29015 solver.cpp:354] Sparsity after update:
I0630 01:47:45.344326 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:47:45.344334 29015 net.cpp:1851] conv1a_param_0(0.11) 
I0630 01:47:45.344341 29015 net.cpp:1851] conv1b_param_0(0.22) 
I0630 01:47:45.344344 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:47:45.344347 29015 net.cpp:1851] res2a_branch2a_param_0(0.22) 
I0630 01:47:45.344348 29015 net.cpp:1851] res2a_branch2b_param_0(0.22) 
I0630 01:47:45.344350 29015 net.cpp:1851] res3a_branch2a_param_0(0.22) 
I0630 01:47:45.344352 29015 net.cpp:1851] res3a_branch2b_param_0(0.22) 
I0630 01:47:45.344354 29015 net.cpp:1851] res4a_branch2a_param_0(0.22) 
I0630 01:47:45.344357 29015 net.cpp:1851] res4a_branch2b_param_0(0.22) 
I0630 01:47:45.344358 29015 net.cpp:1851] res5a_branch2a_param_0(0.22) 
I0630 01:47:45.344360 29015 net.cpp:1851] res5a_branch2b_param_0(0.22) 
I0630 01:47:45.344362 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (517782/2.3599e+06) 0.219
I0630 01:47:45.344491 29015 solver.cpp:471] Iteration 11000, Testing net (#0)
I0630 01:47:46.984827 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.918
I0630 01:47:46.984844 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9969
I0630 01:47:46.984850 29015 solver.cpp:544]     Test net output #2: loss = 0.2025 (* 1 = 0.2025 loss)
I0630 01:47:47.006310 29015 solver.cpp:290] Iteration 11000 (26.99 iter/s, 3.70508s/100 iter), loss = 0
I0630 01:47:47.006328 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:47.006355 29015 sgd_solver.cpp:106] Iteration 11000, lr = 0.00828125
I0630 01:47:47.006877 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.24
I0630 01:47:47.335942 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:47:49.392742 29015 solver.cpp:290] Iteration 11100 (41.9052 iter/s, 2.38634s/100 iter), loss = 0
I0630 01:47:49.392767 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:49.392776 29015 sgd_solver.cpp:106] Iteration 11100, lr = 0.00826562
I0630 01:47:51.449079 29015 solver.cpp:290] Iteration 11200 (48.6323 iter/s, 2.05625s/100 iter), loss = 0
I0630 01:47:51.449102 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:51.449111 29015 sgd_solver.cpp:106] Iteration 11200, lr = 0.00825
I0630 01:47:53.501806 29015 solver.cpp:290] Iteration 11300 (48.7178 iter/s, 2.05264s/100 iter), loss = 0
I0630 01:47:53.501827 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:53.501833 29015 sgd_solver.cpp:106] Iteration 11300, lr = 0.00823438
I0630 01:47:55.556226 29015 solver.cpp:290] Iteration 11400 (48.6776 iter/s, 2.05433s/100 iter), loss = 0
I0630 01:47:55.556248 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:55.556257 29015 sgd_solver.cpp:106] Iteration 11400, lr = 0.00821875
I0630 01:47:57.610509 29015 solver.cpp:290] Iteration 11500 (48.6808 iter/s, 2.0542s/100 iter), loss = 0
I0630 01:47:57.610533 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:57.610543 29015 sgd_solver.cpp:106] Iteration 11500, lr = 0.00820312
I0630 01:47:59.669257 29015 solver.cpp:290] Iteration 11600 (48.5753 iter/s, 2.05866s/100 iter), loss = 0
I0630 01:47:59.669279 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:47:59.669286 29015 sgd_solver.cpp:106] Iteration 11600, lr = 0.0081875
I0630 01:48:01.722590 29015 solver.cpp:290] Iteration 11700 (48.7034 iter/s, 2.05324s/100 iter), loss = 0
I0630 01:48:01.722612 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:01.722620 29015 sgd_solver.cpp:106] Iteration 11700, lr = 0.00817188
I0630 01:48:03.775027 29015 solver.cpp:290] Iteration 11800 (48.7247 iter/s, 2.05235s/100 iter), loss = 0
I0630 01:48:03.775049 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:03.775072 29015 sgd_solver.cpp:106] Iteration 11800, lr = 0.00815625
I0630 01:48:05.831758 29015 solver.cpp:290] Iteration 11900 (48.6229 iter/s, 2.05664s/100 iter), loss = 0
I0630 01:48:05.831781 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:05.831789 29015 sgd_solver.cpp:106] Iteration 11900, lr = 0.00814062
I0630 01:48:07.876662 29015 solver.cpp:354] Sparsity after update:
I0630 01:48:07.877951 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:48:07.877959 29015 net.cpp:1851] conv1a_param_0(0.12) 
I0630 01:48:07.877969 29015 net.cpp:1851] conv1b_param_0(0.24) 
I0630 01:48:07.877974 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:48:07.877979 29015 net.cpp:1851] res2a_branch2a_param_0(0.24) 
I0630 01:48:07.877984 29015 net.cpp:1851] res2a_branch2b_param_0(0.24) 
I0630 01:48:07.877988 29015 net.cpp:1851] res3a_branch2a_param_0(0.24) 
I0630 01:48:07.877991 29015 net.cpp:1851] res3a_branch2b_param_0(0.24) 
I0630 01:48:07.877996 29015 net.cpp:1851] res4a_branch2a_param_0(0.24) 
I0630 01:48:07.878000 29015 net.cpp:1851] res4a_branch2b_param_0(0.24) 
I0630 01:48:07.878005 29015 net.cpp:1851] res5a_branch2a_param_0(0.24) 
I0630 01:48:07.878008 29015 net.cpp:1851] res5a_branch2b_param_0(0.24) 
I0630 01:48:07.878013 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (564848/2.3599e+06) 0.239
I0630 01:48:07.878103 29015 solver.cpp:471] Iteration 12000, Testing net (#0)
I0630 01:48:09.527745 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9181
I0630 01:48:09.527765 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:48:09.527770 29015 solver.cpp:544]     Test net output #2: loss = 0.1985 (* 1 = 0.1985 loss)
I0630 01:48:09.547580 29015 solver.cpp:290] Iteration 12000 (26.9129 iter/s, 3.71569s/100 iter), loss = 0
I0630 01:48:09.547598 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:09.547608 29015 sgd_solver.cpp:106] Iteration 12000, lr = 0.008125
I0630 01:48:09.548771 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.26
I0630 01:48:09.891808 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:48:11.955811 29015 solver.cpp:290] Iteration 12100 (41.5258 iter/s, 2.40814s/100 iter), loss = 0
I0630 01:48:11.955834 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:11.955842 29015 sgd_solver.cpp:106] Iteration 12100, lr = 0.00810937
I0630 01:48:14.009627 29015 solver.cpp:290] Iteration 12200 (48.692 iter/s, 2.05373s/100 iter), loss = 0
I0630 01:48:14.009732 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:14.009740 29015 sgd_solver.cpp:106] Iteration 12200, lr = 0.00809375
I0630 01:48:16.065171 29015 solver.cpp:290] Iteration 12300 (48.6529 iter/s, 2.05537s/100 iter), loss = 0
I0630 01:48:16.065192 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:16.065199 29015 sgd_solver.cpp:106] Iteration 12300, lr = 0.00807813
I0630 01:48:18.122608 29015 solver.cpp:290] Iteration 12400 (48.6062 iter/s, 2.05735s/100 iter), loss = 0
I0630 01:48:18.122630 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:18.122637 29015 sgd_solver.cpp:106] Iteration 12400, lr = 0.0080625
I0630 01:48:20.176175 29015 solver.cpp:290] Iteration 12500 (48.6978 iter/s, 2.05348s/100 iter), loss = 0
I0630 01:48:20.176198 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:20.176204 29015 sgd_solver.cpp:106] Iteration 12500, lr = 0.00804687
I0630 01:48:22.231140 29015 solver.cpp:290] Iteration 12600 (48.6647 iter/s, 2.05488s/100 iter), loss = 0
I0630 01:48:22.231163 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:22.231169 29015 sgd_solver.cpp:106] Iteration 12600, lr = 0.00803125
I0630 01:48:24.288444 29015 solver.cpp:290] Iteration 12700 (48.6094 iter/s, 2.05722s/100 iter), loss = 0
I0630 01:48:24.288466 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:24.288473 29015 sgd_solver.cpp:106] Iteration 12700, lr = 0.00801562
I0630 01:48:26.342401 29015 solver.cpp:290] Iteration 12800 (48.6887 iter/s, 2.05387s/100 iter), loss = 0
I0630 01:48:26.342427 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:26.342437 29015 sgd_solver.cpp:106] Iteration 12800, lr = 0.008
I0630 01:48:28.397506 29015 solver.cpp:290] Iteration 12900 (48.6614 iter/s, 2.05502s/100 iter), loss = 0
I0630 01:48:28.397534 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:28.397543 29015 sgd_solver.cpp:106] Iteration 12900, lr = 0.00798437
I0630 01:48:30.431874 29015 solver.cpp:354] Sparsity after update:
I0630 01:48:30.433161 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:48:30.433169 29015 net.cpp:1851] conv1a_param_0(0.13) 
I0630 01:48:30.433176 29015 net.cpp:1851] conv1b_param_0(0.26) 
I0630 01:48:30.433178 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:48:30.433181 29015 net.cpp:1851] res2a_branch2a_param_0(0.26) 
I0630 01:48:30.433182 29015 net.cpp:1851] res2a_branch2b_param_0(0.26) 
I0630 01:48:30.433184 29015 net.cpp:1851] res3a_branch2a_param_0(0.26) 
I0630 01:48:30.433187 29015 net.cpp:1851] res3a_branch2b_param_0(0.26) 
I0630 01:48:30.433188 29015 net.cpp:1851] res4a_branch2a_param_0(0.26) 
I0630 01:48:30.433190 29015 net.cpp:1851] res4a_branch2b_param_0(0.26) 
I0630 01:48:30.433192 29015 net.cpp:1851] res5a_branch2a_param_0(0.26) 
I0630 01:48:30.433194 29015 net.cpp:1851] res5a_branch2b_param_0(0.26) 
I0630 01:48:30.433197 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (611915/2.3599e+06) 0.259
I0630 01:48:30.433292 29015 solver.cpp:471] Iteration 13000, Testing net (#0)
I0630 01:48:32.072629 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9183
I0630 01:48:32.072649 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:48:32.072654 29015 solver.cpp:544]     Test net output #2: loss = 0.1994 (* 1 = 0.1994 loss)
I0630 01:48:32.093062 29015 solver.cpp:290] Iteration 13000 (27.0605 iter/s, 3.69542s/100 iter), loss = 0
I0630 01:48:32.093081 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:32.093091 29015 sgd_solver.cpp:106] Iteration 13000, lr = 0.00796875
I0630 01:48:32.093626 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.28
I0630 01:48:32.443626 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:48:34.501075 29015 solver.cpp:290] Iteration 13100 (41.5296 iter/s, 2.40792s/100 iter), loss = 0
I0630 01:48:34.501097 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:34.501121 29015 sgd_solver.cpp:106] Iteration 13100, lr = 0.00795313
I0630 01:48:36.560833 29015 solver.cpp:290] Iteration 13200 (48.5515 iter/s, 2.05967s/100 iter), loss = 0
I0630 01:48:36.560858 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:36.560866 29015 sgd_solver.cpp:106] Iteration 13200, lr = 0.0079375
I0630 01:48:38.621027 29015 solver.cpp:290] Iteration 13300 (48.5412 iter/s, 2.0601s/100 iter), loss = 0
I0630 01:48:38.621049 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:38.621057 29015 sgd_solver.cpp:106] Iteration 13300, lr = 0.00792187
I0630 01:48:40.675652 29015 solver.cpp:290] Iteration 13400 (48.6728 iter/s, 2.05454s/100 iter), loss = 0
I0630 01:48:40.675674 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:40.675683 29015 sgd_solver.cpp:106] Iteration 13400, lr = 0.00790625
I0630 01:48:42.729413 29015 solver.cpp:290] Iteration 13500 (48.6932 iter/s, 2.05367s/100 iter), loss = 0
I0630 01:48:42.729434 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:42.729440 29015 sgd_solver.cpp:106] Iteration 13500, lr = 0.00789062
I0630 01:48:44.785673 29015 solver.cpp:290] Iteration 13600 (48.634 iter/s, 2.05617s/100 iter), loss = 0
I0630 01:48:44.785758 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:44.785769 29015 sgd_solver.cpp:106] Iteration 13600, lr = 0.007875
I0630 01:48:46.843475 29015 solver.cpp:290] Iteration 13700 (48.599 iter/s, 2.05766s/100 iter), loss = 0
I0630 01:48:46.843498 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:46.843504 29015 sgd_solver.cpp:106] Iteration 13700, lr = 0.00785937
I0630 01:48:48.902039 29015 solver.cpp:290] Iteration 13800 (48.5797 iter/s, 2.05847s/100 iter), loss = 0
I0630 01:48:48.902060 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:48.902067 29015 sgd_solver.cpp:106] Iteration 13800, lr = 0.00784375
I0630 01:48:50.957772 29015 solver.cpp:290] Iteration 13900 (48.6465 iter/s, 2.05565s/100 iter), loss = 0
I0630 01:48:50.957798 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:50.957806 29015 sgd_solver.cpp:106] Iteration 13900, lr = 0.00782812
I0630 01:48:52.992187 29015 solver.cpp:354] Sparsity after update:
I0630 01:48:52.993450 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:48:52.993458 29015 net.cpp:1851] conv1a_param_0(0.139) 
I0630 01:48:52.993464 29015 net.cpp:1851] conv1b_param_0(0.28) 
I0630 01:48:52.993468 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:48:52.993470 29015 net.cpp:1851] res2a_branch2a_param_0(0.28) 
I0630 01:48:52.993474 29015 net.cpp:1851] res2a_branch2b_param_0(0.28) 
I0630 01:48:52.993475 29015 net.cpp:1851] res3a_branch2a_param_0(0.28) 
I0630 01:48:52.993479 29015 net.cpp:1851] res3a_branch2b_param_0(0.28) 
I0630 01:48:52.993480 29015 net.cpp:1851] res4a_branch2a_param_0(0.28) 
I0630 01:48:52.993482 29015 net.cpp:1851] res4a_branch2b_param_0(0.28) 
I0630 01:48:52.993484 29015 net.cpp:1851] res5a_branch2a_param_0(0.28) 
I0630 01:48:52.993486 29015 net.cpp:1851] res5a_branch2b_param_0(0.28) 
I0630 01:48:52.993489 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (658987/2.3599e+06) 0.279
I0630 01:48:52.993578 29015 solver.cpp:471] Iteration 14000, Testing net (#0)
I0630 01:48:54.631808 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9186
I0630 01:48:54.631826 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:48:54.631831 29015 solver.cpp:544]     Test net output #2: loss = 0.1984 (* 1 = 0.1984 loss)
I0630 01:48:54.652035 29015 solver.cpp:290] Iteration 14000 (27.07 iter/s, 3.69413s/100 iter), loss = 0
I0630 01:48:54.652053 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:54.652063 29015 sgd_solver.cpp:106] Iteration 14000, lr = 0.0078125
I0630 01:48:54.652616 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.3
I0630 01:48:55.030233 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:48:57.088340 29015 solver.cpp:290] Iteration 14100 (41.0474 iter/s, 2.43621s/100 iter), loss = 0
I0630 01:48:57.088362 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:57.088371 29015 sgd_solver.cpp:106] Iteration 14100, lr = 0.00779688
I0630 01:48:59.142227 29015 solver.cpp:290] Iteration 14200 (48.6902 iter/s, 2.0538s/100 iter), loss = 0
I0630 01:48:59.142251 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:48:59.142259 29015 sgd_solver.cpp:106] Iteration 14200, lr = 0.00778125
I0630 01:49:01.196597 29015 solver.cpp:290] Iteration 14300 (48.6788 iter/s, 2.05428s/100 iter), loss = 0
I0630 01:49:01.196620 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:01.196630 29015 sgd_solver.cpp:106] Iteration 14300, lr = 0.00776563
I0630 01:49:03.257863 29015 solver.cpp:290] Iteration 14400 (48.516 iter/s, 2.06118s/100 iter), loss = 0
I0630 01:49:03.257886 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:03.257894 29015 sgd_solver.cpp:106] Iteration 14400, lr = 0.00775
I0630 01:49:05.316495 29015 solver.cpp:290] Iteration 14500 (48.5781 iter/s, 2.05854s/100 iter), loss = 0
I0630 01:49:05.316517 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:05.316545 29015 sgd_solver.cpp:106] Iteration 14500, lr = 0.00773437
I0630 01:49:07.383358 29015 solver.cpp:290] Iteration 14600 (48.3846 iter/s, 2.06678s/100 iter), loss = 0
I0630 01:49:07.383379 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:07.383386 29015 sgd_solver.cpp:106] Iteration 14600, lr = 0.00771875
I0630 01:49:09.439498 29015 solver.cpp:290] Iteration 14700 (48.6369 iter/s, 2.05605s/100 iter), loss = 0
I0630 01:49:09.439520 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:09.439528 29015 sgd_solver.cpp:106] Iteration 14700, lr = 0.00770312
I0630 01:49:11.494148 29015 solver.cpp:290] Iteration 14800 (48.6722 iter/s, 2.05456s/100 iter), loss = 0
I0630 01:49:11.494169 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:11.494176 29015 sgd_solver.cpp:106] Iteration 14800, lr = 0.0076875
I0630 01:49:13.548177 29015 solver.cpp:290] Iteration 14900 (48.6869 iter/s, 2.05394s/100 iter), loss = 0
I0630 01:49:13.548198 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:13.548207 29015 sgd_solver.cpp:106] Iteration 14900, lr = 0.00767187
I0630 01:49:15.584455 29015 solver.cpp:354] Sparsity after update:
I0630 01:49:15.585728 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:49:15.585736 29015 net.cpp:1851] conv1a_param_0(0.15) 
I0630 01:49:15.585746 29015 net.cpp:1851] conv1b_param_0(0.3) 
I0630 01:49:15.585749 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:49:15.585753 29015 net.cpp:1851] res2a_branch2a_param_0(0.3) 
I0630 01:49:15.585757 29015 net.cpp:1851] res2a_branch2b_param_0(0.3) 
I0630 01:49:15.585762 29015 net.cpp:1851] res3a_branch2a_param_0(0.3) 
I0630 01:49:15.585767 29015 net.cpp:1851] res3a_branch2b_param_0(0.3) 
I0630 01:49:15.585772 29015 net.cpp:1851] res4a_branch2a_param_0(0.3) 
I0630 01:49:15.585777 29015 net.cpp:1851] res4a_branch2b_param_0(0.3) 
I0630 01:49:15.585782 29015 net.cpp:1851] res5a_branch2a_param_0(0.3) 
I0630 01:49:15.585786 29015 net.cpp:1851] res5a_branch2b_param_0(0.3) 
I0630 01:49:15.585791 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (706065/2.3599e+06) 0.299
I0630 01:49:15.585885 29015 solver.cpp:471] Iteration 15000, Testing net (#0)
I0630 01:49:17.224720 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9185
I0630 01:49:17.224740 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:49:17.224745 29015 solver.cpp:544]     Test net output #2: loss = 0.2003 (* 1 = 0.2003 loss)
I0630 01:49:17.247357 29015 solver.cpp:290] Iteration 15000 (27.034 iter/s, 3.69905s/100 iter), loss = 0
I0630 01:49:17.247377 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:17.247386 29015 sgd_solver.cpp:106] Iteration 15000, lr = 0.00765625
I0630 01:49:17.247922 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.32
I0630 01:49:17.648810 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:49:19.712558 29015 solver.cpp:290] Iteration 15100 (40.5662 iter/s, 2.4651s/100 iter), loss = 0
I0630 01:49:19.712580 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:19.712589 29015 sgd_solver.cpp:106] Iteration 15100, lr = 0.00764062
I0630 01:49:21.772344 29015 solver.cpp:290] Iteration 15200 (48.5508 iter/s, 2.0597s/100 iter), loss = 0
I0630 01:49:21.772366 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:21.772373 29015 sgd_solver.cpp:106] Iteration 15200, lr = 0.007625
I0630 01:49:23.826511 29015 solver.cpp:290] Iteration 15300 (48.6836 iter/s, 2.05408s/100 iter), loss = 0
I0630 01:49:23.826537 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:23.826546 29015 sgd_solver.cpp:106] Iteration 15300, lr = 0.00760937
I0630 01:49:25.880478 29015 solver.cpp:290] Iteration 15400 (48.6884 iter/s, 2.05388s/100 iter), loss = 0
I0630 01:49:25.880502 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:25.880509 29015 sgd_solver.cpp:106] Iteration 15400, lr = 0.00759375
I0630 01:49:27.935227 29015 solver.cpp:290] Iteration 15500 (48.6699 iter/s, 2.05466s/100 iter), loss = 0
I0630 01:49:27.935250 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:27.935256 29015 sgd_solver.cpp:106] Iteration 15500, lr = 0.00757812
I0630 01:49:29.988183 29015 solver.cpp:290] Iteration 15600 (48.7123 iter/s, 2.05287s/100 iter), loss = 0
I0630 01:49:29.988205 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:29.988212 29015 sgd_solver.cpp:106] Iteration 15600, lr = 0.0075625
I0630 01:49:32.049327 29015 solver.cpp:290] Iteration 15700 (48.5188 iter/s, 2.06106s/100 iter), loss = 0
I0630 01:49:32.049350 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:32.049356 29015 sgd_solver.cpp:106] Iteration 15700, lr = 0.00754687
I0630 01:49:34.102576 29015 solver.cpp:290] Iteration 15800 (48.7053 iter/s, 2.05316s/100 iter), loss = 0
I0630 01:49:34.102598 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:34.102605 29015 sgd_solver.cpp:106] Iteration 15800, lr = 0.00753125
I0630 01:49:36.162233 29015 solver.cpp:290] Iteration 15900 (48.5538 iter/s, 2.05957s/100 iter), loss = 0
I0630 01:49:36.162271 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:36.162278 29015 sgd_solver.cpp:106] Iteration 15900, lr = 0.00751562
I0630 01:49:38.195665 29015 solver.cpp:354] Sparsity after update:
I0630 01:49:38.196930 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:49:38.196938 29015 net.cpp:1851] conv1a_param_0(0.16) 
I0630 01:49:38.196945 29015 net.cpp:1851] conv1b_param_0(0.32) 
I0630 01:49:38.196947 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:49:38.196949 29015 net.cpp:1851] res2a_branch2a_param_0(0.32) 
I0630 01:49:38.196951 29015 net.cpp:1851] res2a_branch2b_param_0(0.32) 
I0630 01:49:38.196954 29015 net.cpp:1851] res3a_branch2a_param_0(0.32) 
I0630 01:49:38.196955 29015 net.cpp:1851] res3a_branch2b_param_0(0.32) 
I0630 01:49:38.196957 29015 net.cpp:1851] res4a_branch2a_param_0(0.32) 
I0630 01:49:38.196959 29015 net.cpp:1851] res4a_branch2b_param_0(0.32) 
I0630 01:49:38.196961 29015 net.cpp:1851] res5a_branch2a_param_0(0.32) 
I0630 01:49:38.196964 29015 net.cpp:1851] res5a_branch2b_param_0(0.32) 
I0630 01:49:38.196965 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (753133/2.3599e+06) 0.319
I0630 01:49:38.197063 29015 solver.cpp:471] Iteration 16000, Testing net (#0)
I0630 01:49:39.837420 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9191
I0630 01:49:39.837438 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:49:39.837443 29015 solver.cpp:544]     Test net output #2: loss = 0.1977 (* 1 = 0.1977 loss)
I0630 01:49:39.858006 29015 solver.cpp:290] Iteration 16000 (27.059 iter/s, 3.69563s/100 iter), loss = 0
I0630 01:49:39.858024 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:39.858037 29015 sgd_solver.cpp:106] Iteration 16000, lr = 0.0075
I0630 01:49:39.858592 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.34
I0630 01:49:40.291604 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:49:42.349653 29015 solver.cpp:290] Iteration 16100 (40.1356 iter/s, 2.49155s/100 iter), loss = 0
I0630 01:49:42.349675 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:42.349681 29015 sgd_solver.cpp:106] Iteration 16100, lr = 0.00748438
I0630 01:49:44.406536 29015 solver.cpp:290] Iteration 16200 (48.6193 iter/s, 2.0568s/100 iter), loss = 0
I0630 01:49:44.406558 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:44.406565 29015 sgd_solver.cpp:106] Iteration 16200, lr = 0.00746875
I0630 01:49:46.467320 29015 solver.cpp:290] Iteration 16300 (48.5273 iter/s, 2.0607s/100 iter), loss = 0
I0630 01:49:46.467401 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:46.467408 29015 sgd_solver.cpp:106] Iteration 16300, lr = 0.00745312
I0630 01:49:48.526259 29015 solver.cpp:290] Iteration 16400 (48.5721 iter/s, 2.05879s/100 iter), loss = 0
I0630 01:49:48.526281 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:48.526289 29015 sgd_solver.cpp:106] Iteration 16400, lr = 0.0074375
I0630 01:49:50.583183 29015 solver.cpp:290] Iteration 16500 (48.6184 iter/s, 2.05683s/100 iter), loss = 0
I0630 01:49:50.583209 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:50.583216 29015 sgd_solver.cpp:106] Iteration 16500, lr = 0.00742187
I0630 01:49:52.644371 29015 solver.cpp:290] Iteration 16600 (48.5178 iter/s, 2.0611s/100 iter), loss = 0
I0630 01:49:52.644393 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:52.644399 29015 sgd_solver.cpp:106] Iteration 16600, lr = 0.00740625
I0630 01:49:54.697944 29015 solver.cpp:290] Iteration 16700 (48.6977 iter/s, 2.05349s/100 iter), loss = 0
I0630 01:49:54.697968 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:54.697973 29015 sgd_solver.cpp:106] Iteration 16700, lr = 0.00739062
I0630 01:49:56.753500 29015 solver.cpp:290] Iteration 16800 (48.6507 iter/s, 2.05547s/100 iter), loss = 0
I0630 01:49:56.753523 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:56.753530 29015 sgd_solver.cpp:106] Iteration 16800, lr = 0.007375
I0630 01:49:58.808023 29015 solver.cpp:290] Iteration 16900 (48.6752 iter/s, 2.05444s/100 iter), loss = 0
I0630 01:49:58.808046 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:49:58.808053 29015 sgd_solver.cpp:106] Iteration 16900, lr = 0.00735937
I0630 01:50:00.843952 29015 solver.cpp:354] Sparsity after update:
I0630 01:50:00.845247 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:50:00.845254 29015 net.cpp:1851] conv1a_param_0(0.17) 
I0630 01:50:00.845263 29015 net.cpp:1851] conv1b_param_0(0.34) 
I0630 01:50:00.845268 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:50:00.845273 29015 net.cpp:1851] res2a_branch2a_param_0(0.34) 
I0630 01:50:00.845278 29015 net.cpp:1851] res2a_branch2b_param_0(0.34) 
I0630 01:50:00.845281 29015 net.cpp:1851] res3a_branch2a_param_0(0.34) 
I0630 01:50:00.845285 29015 net.cpp:1851] res3a_branch2b_param_0(0.34) 
I0630 01:50:00.845289 29015 net.cpp:1851] res4a_branch2a_param_0(0.34) 
I0630 01:50:00.845293 29015 net.cpp:1851] res4a_branch2b_param_0(0.34) 
I0630 01:50:00.845297 29015 net.cpp:1851] res5a_branch2a_param_0(0.34) 
I0630 01:50:00.845301 29015 net.cpp:1851] res5a_branch2b_param_0(0.34) 
I0630 01:50:00.845305 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (800205/2.3599e+06) 0.339
I0630 01:50:00.845443 29015 solver.cpp:471] Iteration 17000, Testing net (#0)
I0630 01:50:02.483361 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9192
I0630 01:50:02.483381 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:50:02.483387 29015 solver.cpp:544]     Test net output #2: loss = 0.1984 (* 1 = 0.1984 loss)
I0630 01:50:02.504524 29015 solver.cpp:290] Iteration 17000 (27.0536 iter/s, 3.69637s/100 iter), loss = 0
I0630 01:50:02.504555 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:02.504565 29015 sgd_solver.cpp:106] Iteration 17000, lr = 0.00734375
I0630 01:50:02.505172 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.36
I0630 01:50:02.945701 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:50:05.007841 29015 solver.cpp:290] Iteration 17100 (39.9487 iter/s, 2.50321s/100 iter), loss = 0
I0630 01:50:05.007863 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:05.007872 29015 sgd_solver.cpp:106] Iteration 17100, lr = 0.00732813
I0630 01:50:07.063791 29015 solver.cpp:290] Iteration 17200 (48.6414 iter/s, 2.05586s/100 iter), loss = 0
I0630 01:50:07.063813 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:07.063838 29015 sgd_solver.cpp:106] Iteration 17200, lr = 0.0073125
I0630 01:50:09.140094 29015 solver.cpp:290] Iteration 17300 (48.1645 iter/s, 2.07622s/100 iter), loss = 0
I0630 01:50:09.140115 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:09.140123 29015 sgd_solver.cpp:106] Iteration 17300, lr = 0.00729688
I0630 01:50:11.197922 29015 solver.cpp:290] Iteration 17400 (48.597 iter/s, 2.05774s/100 iter), loss = 0
I0630 01:50:11.197944 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:11.197952 29015 sgd_solver.cpp:106] Iteration 17400, lr = 0.00728125
I0630 01:50:13.259915 29015 solver.cpp:290] Iteration 17500 (48.4988 iter/s, 2.06191s/100 iter), loss = 0
I0630 01:50:13.259938 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:13.259945 29015 sgd_solver.cpp:106] Iteration 17500, lr = 0.00726563
I0630 01:50:15.315577 29015 solver.cpp:290] Iteration 17600 (48.6482 iter/s, 2.05558s/100 iter), loss = 0
I0630 01:50:15.315599 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:15.315608 29015 sgd_solver.cpp:106] Iteration 17600, lr = 0.00725
I0630 01:50:17.370419 29015 solver.cpp:290] Iteration 17700 (48.6677 iter/s, 2.05475s/100 iter), loss = 0
I0630 01:50:17.370504 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:17.370517 29015 sgd_solver.cpp:106] Iteration 17700, lr = 0.00723437
I0630 01:50:19.430191 29015 solver.cpp:290] Iteration 17800 (48.5526 iter/s, 2.05962s/100 iter), loss = 0
I0630 01:50:19.430212 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:19.430219 29015 sgd_solver.cpp:106] Iteration 17800, lr = 0.00721875
I0630 01:50:21.488754 29015 solver.cpp:290] Iteration 17900 (48.5797 iter/s, 2.05847s/100 iter), loss = 0
I0630 01:50:21.488777 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:21.488785 29015 sgd_solver.cpp:106] Iteration 17900, lr = 0.00720312
I0630 01:50:23.523341 29015 solver.cpp:354] Sparsity after update:
I0630 01:50:23.524507 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:50:23.524520 29015 net.cpp:1851] conv1a_param_0(0.18) 
I0630 01:50:23.524530 29015 net.cpp:1851] conv1b_param_0(0.36) 
I0630 01:50:23.524533 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:50:23.524535 29015 net.cpp:1851] res2a_branch2a_param_0(0.36) 
I0630 01:50:23.524538 29015 net.cpp:1851] res2a_branch2b_param_0(0.36) 
I0630 01:50:23.524541 29015 net.cpp:1851] res3a_branch2a_param_0(0.36) 
I0630 01:50:23.524544 29015 net.cpp:1851] res3a_branch2b_param_0(0.36) 
I0630 01:50:23.524545 29015 net.cpp:1851] res4a_branch2a_param_0(0.36) 
I0630 01:50:23.524549 29015 net.cpp:1851] res4a_branch2b_param_0(0.36) 
I0630 01:50:23.524550 29015 net.cpp:1851] res5a_branch2a_param_0(0.36) 
I0630 01:50:23.524554 29015 net.cpp:1851] res5a_branch2b_param_0(0.36) 
I0630 01:50:23.524555 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (847278/2.3599e+06) 0.359
I0630 01:50:23.524670 29015 solver.cpp:471] Iteration 18000, Testing net (#0)
I0630 01:50:25.164630 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9189
I0630 01:50:25.164649 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9972
I0630 01:50:25.164655 29015 solver.cpp:544]     Test net output #2: loss = 0.1968 (* 1 = 0.1968 loss)
I0630 01:50:25.184950 29015 solver.cpp:290] Iteration 18000 (27.0558 iter/s, 3.69607s/100 iter), loss = 0
I0630 01:50:25.184969 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:25.184978 29015 sgd_solver.cpp:106] Iteration 18000, lr = 0.0071875
I0630 01:50:25.185508 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.38
I0630 01:50:25.647279 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:50:27.706604 29015 solver.cpp:290] Iteration 18100 (39.6581 iter/s, 2.52156s/100 iter), loss = 0
I0630 01:50:27.706626 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:27.706632 29015 sgd_solver.cpp:106] Iteration 18100, lr = 0.00717187
I0630 01:50:29.761991 29015 solver.cpp:290] Iteration 18200 (48.6547 iter/s, 2.0553s/100 iter), loss = 0
I0630 01:50:29.762013 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:29.762020 29015 sgd_solver.cpp:106] Iteration 18200, lr = 0.00715625
I0630 01:50:31.819017 29015 solver.cpp:290] Iteration 18300 (48.6159 iter/s, 2.05694s/100 iter), loss = 0
I0630 01:50:31.819041 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:31.819047 29015 sgd_solver.cpp:106] Iteration 18300, lr = 0.00714062
I0630 01:50:33.872910 29015 solver.cpp:290] Iteration 18400 (48.6901 iter/s, 2.05381s/100 iter), loss = 0
I0630 01:50:33.872931 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:33.872938 29015 sgd_solver.cpp:106] Iteration 18400, lr = 0.007125
I0630 01:50:35.927224 29015 solver.cpp:290] Iteration 18500 (48.6801 iter/s, 2.05423s/100 iter), loss = 0
I0630 01:50:35.927248 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:35.927254 29015 sgd_solver.cpp:106] Iteration 18500, lr = 0.00710937
I0630 01:50:37.989910 29015 solver.cpp:290] Iteration 18600 (48.4825 iter/s, 2.0626s/100 iter), loss = 0
I0630 01:50:37.989933 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:37.989959 29015 sgd_solver.cpp:106] Iteration 18600, lr = 0.00709375
I0630 01:50:40.047602 29015 solver.cpp:290] Iteration 18700 (48.6002 iter/s, 2.05761s/100 iter), loss = 0
I0630 01:50:40.047623 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:40.047631 29015 sgd_solver.cpp:106] Iteration 18700, lr = 0.00707812
I0630 01:50:42.102632 29015 solver.cpp:290] Iteration 18800 (48.6632 iter/s, 2.05494s/100 iter), loss = 0
I0630 01:50:42.102654 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:42.102660 29015 sgd_solver.cpp:106] Iteration 18800, lr = 0.0070625
I0630 01:50:44.156960 29015 solver.cpp:290] Iteration 18900 (48.6798 iter/s, 2.05424s/100 iter), loss = 0
I0630 01:50:44.156983 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:44.156991 29015 sgd_solver.cpp:106] Iteration 18900, lr = 0.00704687
I0630 01:50:46.191015 29015 solver.cpp:354] Sparsity after update:
I0630 01:50:46.192294 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:50:46.192302 29015 net.cpp:1851] conv1a_param_0(0.19) 
I0630 01:50:46.192311 29015 net.cpp:1851] conv1b_param_0(0.38) 
I0630 01:50:46.192312 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:50:46.192315 29015 net.cpp:1851] res2a_branch2a_param_0(0.38) 
I0630 01:50:46.192318 29015 net.cpp:1851] res2a_branch2b_param_0(0.38) 
I0630 01:50:46.192320 29015 net.cpp:1851] res3a_branch2a_param_0(0.38) 
I0630 01:50:46.192323 29015 net.cpp:1851] res3a_branch2b_param_0(0.38) 
I0630 01:50:46.192325 29015 net.cpp:1851] res4a_branch2a_param_0(0.38) 
I0630 01:50:46.192328 29015 net.cpp:1851] res4a_branch2b_param_0(0.38) 
I0630 01:50:46.192330 29015 net.cpp:1851] res5a_branch2a_param_0(0.38) 
I0630 01:50:46.192332 29015 net.cpp:1851] res5a_branch2b_param_0(0.38) 
I0630 01:50:46.192334 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (894356/2.3599e+06) 0.379
I0630 01:50:46.192421 29015 solver.cpp:471] Iteration 19000, Testing net (#0)
I0630 01:50:47.830130 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9174
I0630 01:50:47.830240 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9971
I0630 01:50:47.830248 29015 solver.cpp:544]     Test net output #2: loss = 0.2007 (* 1 = 0.2007 loss)
I0630 01:50:47.850164 29015 solver.cpp:290] Iteration 19000 (27.0777 iter/s, 3.69307s/100 iter), loss = 0
I0630 01:50:47.850183 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:47.850194 29015 sgd_solver.cpp:106] Iteration 19000, lr = 0.00703125
I0630 01:50:47.850739 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.4
I0630 01:50:48.334815 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:50:50.396459 29015 solver.cpp:290] Iteration 19100 (39.2743 iter/s, 2.5462s/100 iter), loss = 0
I0630 01:50:50.396486 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:50.396495 29015 sgd_solver.cpp:106] Iteration 19100, lr = 0.00701563
I0630 01:50:52.450054 29015 solver.cpp:290] Iteration 19200 (48.6972 iter/s, 2.0535s/100 iter), loss = 0
I0630 01:50:52.450076 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:52.450083 29015 sgd_solver.cpp:106] Iteration 19200, lr = 0.007
I0630 01:50:54.504504 29015 solver.cpp:290] Iteration 19300 (48.6769 iter/s, 2.05436s/100 iter), loss = 0
I0630 01:50:54.504528 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:54.504534 29015 sgd_solver.cpp:106] Iteration 19300, lr = 0.00698437
I0630 01:50:56.559964 29015 solver.cpp:290] Iteration 19400 (48.6531 iter/s, 2.05537s/100 iter), loss = 0
I0630 01:50:56.559994 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:56.560004 29015 sgd_solver.cpp:106] Iteration 19400, lr = 0.00696875
I0630 01:50:58.620131 29015 solver.cpp:290] Iteration 19500 (48.5419 iter/s, 2.06007s/100 iter), loss = 0
I0630 01:50:58.620153 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:50:58.620160 29015 sgd_solver.cpp:106] Iteration 19500, lr = 0.00695312
I0630 01:51:00.675395 29015 solver.cpp:290] Iteration 19600 (48.6576 iter/s, 2.05518s/100 iter), loss = 0
I0630 01:51:00.675418 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:00.675424 29015 sgd_solver.cpp:106] Iteration 19600, lr = 0.0069375
I0630 01:51:02.731236 29015 solver.cpp:290] Iteration 19700 (48.644 iter/s, 2.05575s/100 iter), loss = 0
I0630 01:51:02.731258 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:02.731264 29015 sgd_solver.cpp:106] Iteration 19700, lr = 0.00692187
I0630 01:51:04.789537 29015 solver.cpp:290] Iteration 19800 (48.5858 iter/s, 2.05822s/100 iter), loss = 0
I0630 01:51:04.789559 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:04.789566 29015 sgd_solver.cpp:106] Iteration 19800, lr = 0.00690625
I0630 01:51:06.848969 29015 solver.cpp:290] Iteration 19900 (48.5591 iter/s, 2.05935s/100 iter), loss = 0
I0630 01:51:06.848992 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:06.848999 29015 sgd_solver.cpp:106] Iteration 19900, lr = 0.00689062
I0630 01:51:08.904639 29015 solver.cpp:598] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_20000.caffemodel
I0630 01:51:08.921322 29015 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_20000.solverstate
I0630 01:51:08.928608 29015 solver.cpp:354] Sparsity after update:
I0630 01:51:08.929569 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:51:08.929577 29015 net.cpp:1851] conv1a_param_0(0.2) 
I0630 01:51:08.929584 29015 net.cpp:1851] conv1b_param_0(0.4) 
I0630 01:51:08.929587 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:51:08.929589 29015 net.cpp:1851] res2a_branch2a_param_0(0.4) 
I0630 01:51:08.929591 29015 net.cpp:1851] res2a_branch2b_param_0(0.4) 
I0630 01:51:08.929594 29015 net.cpp:1851] res3a_branch2a_param_0(0.4) 
I0630 01:51:08.929605 29015 net.cpp:1851] res3a_branch2b_param_0(0.4) 
I0630 01:51:08.929607 29015 net.cpp:1851] res4a_branch2a_param_0(0.4) 
I0630 01:51:08.929610 29015 net.cpp:1851] res4a_branch2b_param_0(0.4) 
I0630 01:51:08.929612 29015 net.cpp:1851] res5a_branch2a_param_0(0.4) 
I0630 01:51:08.929615 29015 net.cpp:1851] res5a_branch2b_param_0(0.4) 
I0630 01:51:08.929616 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (941419/2.3599e+06) 0.399
I0630 01:51:08.929716 29015 solver.cpp:471] Iteration 20000, Testing net (#0)
I0630 01:51:10.571337 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9177
I0630 01:51:10.571358 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9968
I0630 01:51:10.571364 29015 solver.cpp:544]     Test net output #2: loss = 0.2009 (* 1 = 0.2009 loss)
I0630 01:51:10.591573 29015 solver.cpp:290] Iteration 20000 (26.7204 iter/s, 3.74247s/100 iter), loss = 0
I0630 01:51:10.591601 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:10.591609 29015 sgd_solver.cpp:106] Iteration 20000, lr = 0.006875
I0630 01:51:10.592325 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.42
I0630 01:51:11.095958 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:51:13.152346 29015 solver.cpp:290] Iteration 20100 (39.0523 iter/s, 2.56067s/100 iter), loss = 0
I0630 01:51:13.152370 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:13.152379 29015 sgd_solver.cpp:106] Iteration 20100, lr = 0.00685938
I0630 01:51:15.210052 29015 solver.cpp:290] Iteration 20200 (48.5999 iter/s, 2.05762s/100 iter), loss = 0
I0630 01:51:15.210072 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:15.210081 29015 sgd_solver.cpp:106] Iteration 20200, lr = 0.00684375
I0630 01:51:17.263579 29015 solver.cpp:290] Iteration 20300 (48.6987 iter/s, 2.05344s/100 iter), loss = 0
I0630 01:51:17.263602 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:17.263610 29015 sgd_solver.cpp:106] Iteration 20300, lr = 0.00682813
I0630 01:51:19.322036 29015 solver.cpp:290] Iteration 20400 (48.5822 iter/s, 2.05837s/100 iter), loss = 0
I0630 01:51:19.322123 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:19.322131 29015 sgd_solver.cpp:106] Iteration 20400, lr = 0.0068125
I0630 01:51:21.383972 29015 solver.cpp:290] Iteration 20500 (48.5016 iter/s, 2.06179s/100 iter), loss = 0
I0630 01:51:21.383994 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:21.384001 29015 sgd_solver.cpp:106] Iteration 20500, lr = 0.00679688
I0630 01:51:23.442646 29015 solver.cpp:290] Iteration 20600 (48.5771 iter/s, 2.05859s/100 iter), loss = 0
I0630 01:51:23.442672 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:23.442679 29015 sgd_solver.cpp:106] Iteration 20600, lr = 0.00678125
I0630 01:51:25.504473 29015 solver.cpp:290] Iteration 20700 (48.5029 iter/s, 2.06173s/100 iter), loss = 0
I0630 01:51:25.504508 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:25.504519 29015 sgd_solver.cpp:106] Iteration 20700, lr = 0.00676562
I0630 01:51:27.566524 29015 solver.cpp:290] Iteration 20800 (48.4977 iter/s, 2.06195s/100 iter), loss = 0
I0630 01:51:27.566546 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:27.566553 29015 sgd_solver.cpp:106] Iteration 20800, lr = 0.00675
I0630 01:51:29.633307 29015 solver.cpp:290] Iteration 20900 (48.3865 iter/s, 2.06669s/100 iter), loss = 0
I0630 01:51:29.633339 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:29.633348 29015 sgd_solver.cpp:106] Iteration 20900, lr = 0.00673437
I0630 01:51:31.679366 29015 solver.cpp:354] Sparsity after update:
I0630 01:51:31.680635 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:51:31.680642 29015 net.cpp:1851] conv1a_param_0(0.21) 
I0630 01:51:31.680650 29015 net.cpp:1851] conv1b_param_0(0.42) 
I0630 01:51:31.680652 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:51:31.680655 29015 net.cpp:1851] res2a_branch2a_param_0(0.42) 
I0630 01:51:31.680656 29015 net.cpp:1851] res2a_branch2b_param_0(0.42) 
I0630 01:51:31.680658 29015 net.cpp:1851] res3a_branch2a_param_0(0.42) 
I0630 01:51:31.680660 29015 net.cpp:1851] res3a_branch2b_param_0(0.42) 
I0630 01:51:31.680662 29015 net.cpp:1851] res4a_branch2a_param_0(0.42) 
I0630 01:51:31.680665 29015 net.cpp:1851] res4a_branch2b_param_0(0.42) 
I0630 01:51:31.680666 29015 net.cpp:1851] res5a_branch2a_param_0(0.42) 
I0630 01:51:31.680668 29015 net.cpp:1851] res5a_branch2b_param_0(0.42) 
I0630 01:51:31.680670 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (988499/2.3599e+06) 0.419
I0630 01:51:31.680800 29015 solver.cpp:471] Iteration 21000, Testing net (#0)
I0630 01:51:33.330025 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.917
I0630 01:51:33.330046 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9969
I0630 01:51:33.330052 29015 solver.cpp:544]     Test net output #2: loss = 0.1994 (* 1 = 0.1994 loss)
I0630 01:51:33.349984 29015 solver.cpp:290] Iteration 21000 (26.9068 iter/s, 3.71653s/100 iter), loss = 0
I0630 01:51:33.350010 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:33.350018 29015 sgd_solver.cpp:106] Iteration 21000, lr = 0.00671875
I0630 01:51:33.350580 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.44
I0630 01:51:33.865296 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:51:35.925601 29015 solver.cpp:290] Iteration 21100 (38.8272 iter/s, 2.57551s/100 iter), loss = 0
I0630 01:51:35.925622 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:35.925631 29015 sgd_solver.cpp:106] Iteration 21100, lr = 0.00670313
I0630 01:51:37.986672 29015 solver.cpp:290] Iteration 21200 (48.5205 iter/s, 2.06098s/100 iter), loss = 0
I0630 01:51:37.986696 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:37.986703 29015 sgd_solver.cpp:106] Iteration 21200, lr = 0.0066875
I0630 01:51:40.046135 29015 solver.cpp:290] Iteration 21300 (48.5585 iter/s, 2.05937s/100 iter), loss = 0
I0630 01:51:40.046156 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:40.046177 29015 sgd_solver.cpp:106] Iteration 21300, lr = 0.00667187
I0630 01:51:42.099647 29015 solver.cpp:290] Iteration 21400 (48.6991 iter/s, 2.05342s/100 iter), loss = 0
I0630 01:51:42.099668 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:42.099674 29015 sgd_solver.cpp:106] Iteration 21400, lr = 0.00665625
I0630 01:51:44.156528 29015 solver.cpp:290] Iteration 21500 (48.6193 iter/s, 2.0568s/100 iter), loss = 0
I0630 01:51:44.156550 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:44.156559 29015 sgd_solver.cpp:106] Iteration 21500, lr = 0.00664062
I0630 01:51:46.215790 29015 solver.cpp:290] Iteration 21600 (48.5632 iter/s, 2.05917s/100 iter), loss = 0
I0630 01:51:46.215817 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:46.215836 29015 sgd_solver.cpp:106] Iteration 21600, lr = 0.006625
I0630 01:51:48.274821 29015 solver.cpp:290] Iteration 21700 (48.5687 iter/s, 2.05894s/100 iter), loss = 0
I0630 01:51:48.274847 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:48.274857 29015 sgd_solver.cpp:106] Iteration 21700, lr = 0.00660937
I0630 01:51:50.331146 29015 solver.cpp:290] Iteration 21800 (48.6326 iter/s, 2.05624s/100 iter), loss = 0
I0630 01:51:50.331212 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:50.331219 29015 sgd_solver.cpp:106] Iteration 21800, lr = 0.00659375
I0630 01:51:52.392341 29015 solver.cpp:290] Iteration 21900 (48.5186 iter/s, 2.06107s/100 iter), loss = 0
I0630 01:51:52.392362 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:52.392370 29015 sgd_solver.cpp:106] Iteration 21900, lr = 0.00657812
I0630 01:51:54.425671 29015 solver.cpp:354] Sparsity after update:
I0630 01:51:54.426946 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:51:54.426954 29015 net.cpp:1851] conv1a_param_0(0.22) 
I0630 01:51:54.426962 29015 net.cpp:1851] conv1b_param_0(0.44) 
I0630 01:51:54.426966 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:51:54.426970 29015 net.cpp:1851] res2a_branch2a_param_0(0.44) 
I0630 01:51:54.426975 29015 net.cpp:1851] res2a_branch2b_param_0(0.44) 
I0630 01:51:54.426980 29015 net.cpp:1851] res3a_branch2a_param_0(0.44) 
I0630 01:51:54.426985 29015 net.cpp:1851] res3a_branch2b_param_0(0.44) 
I0630 01:51:54.426988 29015 net.cpp:1851] res4a_branch2a_param_0(0.44) 
I0630 01:51:54.426993 29015 net.cpp:1851] res4a_branch2b_param_0(0.44) 
I0630 01:51:54.426997 29015 net.cpp:1851] res5a_branch2a_param_0(0.44) 
I0630 01:51:54.427001 29015 net.cpp:1851] res5a_branch2b_param_0(0.44) 
I0630 01:51:54.427006 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.03556e+06/2.3599e+06) 0.439
I0630 01:51:54.427144 29015 solver.cpp:471] Iteration 22000, Testing net (#0)
I0630 01:51:56.066071 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9173
I0630 01:51:56.066089 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9971
I0630 01:51:56.066094 29015 solver.cpp:544]     Test net output #2: loss = 0.2 (* 1 = 0.2 loss)
I0630 01:51:56.086165 29015 solver.cpp:290] Iteration 22000 (27.0732 iter/s, 3.69369s/100 iter), loss = 0
I0630 01:51:56.086182 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:56.086194 29015 sgd_solver.cpp:106] Iteration 22000, lr = 0.0065625
I0630 01:51:56.086731 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.46
I0630 01:51:56.633684 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:51:58.690937 29015 solver.cpp:290] Iteration 22100 (38.3925 iter/s, 2.60467s/100 iter), loss = 0
I0630 01:51:58.690958 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:51:58.690964 29015 sgd_solver.cpp:106] Iteration 22100, lr = 0.00654687
I0630 01:52:00.746775 29015 solver.cpp:290] Iteration 22200 (48.644 iter/s, 2.05575s/100 iter), loss = 0
I0630 01:52:00.746798 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:00.746806 29015 sgd_solver.cpp:106] Iteration 22200, lr = 0.00653125
I0630 01:52:02.808218 29015 solver.cpp:290] Iteration 22300 (48.5118 iter/s, 2.06135s/100 iter), loss = 0
I0630 01:52:02.808243 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:02.808250 29015 sgd_solver.cpp:106] Iteration 22300, lr = 0.00651562
I0630 01:52:04.868268 29015 solver.cpp:290] Iteration 22400 (48.5446 iter/s, 2.05996s/100 iter), loss = 0
I0630 01:52:04.868288 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:04.868297 29015 sgd_solver.cpp:106] Iteration 22400, lr = 0.0065
I0630 01:52:06.927482 29015 solver.cpp:290] Iteration 22500 (48.5642 iter/s, 2.05913s/100 iter), loss = 0
I0630 01:52:06.927506 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:06.927513 29015 sgd_solver.cpp:106] Iteration 22500, lr = 0.00648437
I0630 01:52:09.000349 29015 solver.cpp:290] Iteration 22600 (48.2444 iter/s, 2.07278s/100 iter), loss = 0
I0630 01:52:09.000370 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:09.000378 29015 sgd_solver.cpp:106] Iteration 22600, lr = 0.00646875
I0630 01:52:11.064291 29015 solver.cpp:290] Iteration 22700 (48.4531 iter/s, 2.06385s/100 iter), loss = 0
I0630 01:52:11.064321 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:11.064352 29015 sgd_solver.cpp:106] Iteration 22700, lr = 0.00645312
I0630 01:52:13.127725 29015 solver.cpp:290] Iteration 22800 (48.4651 iter/s, 2.06334s/100 iter), loss = 0
I0630 01:52:13.127748 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:13.127754 29015 sgd_solver.cpp:106] Iteration 22800, lr = 0.0064375
I0630 01:52:15.186064 29015 solver.cpp:290] Iteration 22900 (48.585 iter/s, 2.05825s/100 iter), loss = 0
I0630 01:52:15.186086 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:15.186092 29015 sgd_solver.cpp:106] Iteration 22900, lr = 0.00642187
I0630 01:52:17.231580 29015 solver.cpp:354] Sparsity after update:
I0630 01:52:17.232861 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:52:17.232867 29015 net.cpp:1851] conv1a_param_0(0.23) 
I0630 01:52:17.232875 29015 net.cpp:1851] conv1b_param_0(0.46) 
I0630 01:52:17.232877 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:52:17.232880 29015 net.cpp:1851] res2a_branch2a_param_0(0.46) 
I0630 01:52:17.232882 29015 net.cpp:1851] res2a_branch2b_param_0(0.46) 
I0630 01:52:17.232885 29015 net.cpp:1851] res3a_branch2a_param_0(0.46) 
I0630 01:52:17.232887 29015 net.cpp:1851] res3a_branch2b_param_0(0.46) 
I0630 01:52:17.232889 29015 net.cpp:1851] res4a_branch2a_param_0(0.46) 
I0630 01:52:17.232892 29015 net.cpp:1851] res4a_branch2b_param_0(0.46) 
I0630 01:52:17.232894 29015 net.cpp:1851] res5a_branch2a_param_0(0.46) 
I0630 01:52:17.232897 29015 net.cpp:1851] res5a_branch2b_param_0(0.46) 
I0630 01:52:17.232898 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.08264e+06/2.3599e+06) 0.459
I0630 01:52:17.232986 29015 solver.cpp:471] Iteration 23000, Testing net (#0)
I0630 01:52:18.869118 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9168
I0630 01:52:18.869138 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9966
I0630 01:52:18.869143 29015 solver.cpp:544]     Test net output #2: loss = 0.1984 (* 1 = 0.1984 loss)
I0630 01:52:18.890843 29015 solver.cpp:290] Iteration 23000 (26.9932 iter/s, 3.70464s/100 iter), loss = 0
I0630 01:52:18.890869 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:18.890878 29015 sgd_solver.cpp:106] Iteration 23000, lr = 0.00640625
I0630 01:52:18.891463 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.48
I0630 01:52:19.460754 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:52:21.515347 29015 solver.cpp:290] Iteration 23100 (38.104 iter/s, 2.6244s/100 iter), loss = 0
I0630 01:52:21.515417 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:21.515425 29015 sgd_solver.cpp:106] Iteration 23100, lr = 0.00639063
I0630 01:52:23.578290 29015 solver.cpp:290] Iteration 23200 (48.4776 iter/s, 2.06281s/100 iter), loss = 0
I0630 01:52:23.578312 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:23.578318 29015 sgd_solver.cpp:106] Iteration 23200, lr = 0.006375
I0630 01:52:25.633970 29015 solver.cpp:290] Iteration 23300 (48.6478 iter/s, 2.05559s/100 iter), loss = 0
I0630 01:52:25.633992 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:25.633999 29015 sgd_solver.cpp:106] Iteration 23300, lr = 0.00635938
I0630 01:52:27.687383 29015 solver.cpp:290] Iteration 23400 (48.7015 iter/s, 2.05333s/100 iter), loss = 0
I0630 01:52:27.687404 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:27.687412 29015 sgd_solver.cpp:106] Iteration 23400, lr = 0.00634375
I0630 01:52:29.746770 29015 solver.cpp:290] Iteration 23500 (48.5602 iter/s, 2.0593s/100 iter), loss = 0
I0630 01:52:29.746791 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:29.746799 29015 sgd_solver.cpp:106] Iteration 23500, lr = 0.00632813
I0630 01:52:31.802999 29015 solver.cpp:290] Iteration 23600 (48.6348 iter/s, 2.05614s/100 iter), loss = 0
I0630 01:52:31.803025 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:31.803031 29015 sgd_solver.cpp:106] Iteration 23600, lr = 0.0063125
I0630 01:52:33.859071 29015 solver.cpp:290] Iteration 23700 (48.6386 iter/s, 2.05598s/100 iter), loss = 0
I0630 01:52:33.859097 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:33.859107 29015 sgd_solver.cpp:106] Iteration 23700, lr = 0.00629687
I0630 01:52:35.915735 29015 solver.cpp:290] Iteration 23800 (48.6245 iter/s, 2.05658s/100 iter), loss = 0
I0630 01:52:35.915758 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:35.915765 29015 sgd_solver.cpp:106] Iteration 23800, lr = 0.00628125
I0630 01:52:37.976523 29015 solver.cpp:290] Iteration 23900 (48.5272 iter/s, 2.0607s/100 iter), loss = 0
I0630 01:52:37.976544 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:37.976552 29015 sgd_solver.cpp:106] Iteration 23900, lr = 0.00626562
I0630 01:52:40.016388 29015 solver.cpp:354] Sparsity after update:
I0630 01:52:40.017664 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:52:40.017670 29015 net.cpp:1851] conv1a_param_0(0.24) 
I0630 01:52:40.017678 29015 net.cpp:1851] conv1b_param_0(0.48) 
I0630 01:52:40.017679 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:52:40.017683 29015 net.cpp:1851] res2a_branch2a_param_0(0.48) 
I0630 01:52:40.017684 29015 net.cpp:1851] res2a_branch2b_param_0(0.48) 
I0630 01:52:40.017686 29015 net.cpp:1851] res3a_branch2a_param_0(0.48) 
I0630 01:52:40.017688 29015 net.cpp:1851] res3a_branch2b_param_0(0.48) 
I0630 01:52:40.017690 29015 net.cpp:1851] res4a_branch2a_param_0(0.48) 
I0630 01:52:40.017693 29015 net.cpp:1851] res4a_branch2b_param_0(0.48) 
I0630 01:52:40.017694 29015 net.cpp:1851] res5a_branch2a_param_0(0.48) 
I0630 01:52:40.017696 29015 net.cpp:1851] res5a_branch2b_param_0(0.48) 
I0630 01:52:40.017699 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.12971e+06/2.3599e+06) 0.479
I0630 01:52:40.017786 29015 solver.cpp:471] Iteration 24000, Testing net (#0)
I0630 01:52:41.664010 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9165
I0630 01:52:41.664029 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9966
I0630 01:52:41.664034 29015 solver.cpp:544]     Test net output #2: loss = 0.198 (* 1 = 0.198 loss)
I0630 01:52:41.684386 29015 solver.cpp:290] Iteration 24000 (26.9707 iter/s, 3.70773s/100 iter), loss = 0
I0630 01:52:41.684414 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:41.684422 29015 sgd_solver.cpp:106] Iteration 24000, lr = 0.00625
I0630 01:52:41.684973 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.5
I0630 01:52:42.292356 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:52:44.353202 29015 solver.cpp:290] Iteration 24100 (37.4713 iter/s, 2.66871s/100 iter), loss = 0
I0630 01:52:44.353224 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:44.353231 29015 sgd_solver.cpp:106] Iteration 24100, lr = 0.00623438
I0630 01:52:46.413815 29015 solver.cpp:290] Iteration 24200 (48.5313 iter/s, 2.06053s/100 iter), loss = 0
I0630 01:52:46.413838 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:46.413846 29015 sgd_solver.cpp:106] Iteration 24200, lr = 0.00621875
I0630 01:52:48.475193 29015 solver.cpp:290] Iteration 24300 (48.5133 iter/s, 2.06129s/100 iter), loss = 0
I0630 01:52:48.475215 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:48.475222 29015 sgd_solver.cpp:106] Iteration 24300, lr = 0.00620312
I0630 01:52:50.529932 29015 solver.cpp:290] Iteration 24400 (48.67 iter/s, 2.05465s/100 iter), loss = 0
I0630 01:52:50.529953 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:50.529960 29015 sgd_solver.cpp:106] Iteration 24400, lr = 0.0061875
I0630 01:52:52.588497 29015 solver.cpp:290] Iteration 24500 (48.5796 iter/s, 2.05848s/100 iter), loss = 0
I0630 01:52:52.588562 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:52.588577 29015 sgd_solver.cpp:106] Iteration 24500, lr = 0.00617187
I0630 01:52:54.646412 29015 solver.cpp:290] Iteration 24600 (48.5958 iter/s, 2.05779s/100 iter), loss = 0
I0630 01:52:54.646436 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:54.646445 29015 sgd_solver.cpp:106] Iteration 24600, lr = 0.00615625
I0630 01:52:56.700996 29015 solver.cpp:290] Iteration 24700 (48.6738 iter/s, 2.0545s/100 iter), loss = 0
I0630 01:52:56.701025 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:56.701035 29015 sgd_solver.cpp:106] Iteration 24700, lr = 0.00614062
I0630 01:52:58.756503 29015 solver.cpp:290] Iteration 24800 (48.6519 iter/s, 2.05542s/100 iter), loss = 0
I0630 01:52:58.756525 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:52:58.756532 29015 sgd_solver.cpp:106] Iteration 24800, lr = 0.006125
I0630 01:53:00.815582 29015 solver.cpp:290] Iteration 24900 (48.5674 iter/s, 2.05899s/100 iter), loss = 0
I0630 01:53:00.815604 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:00.815611 29015 sgd_solver.cpp:106] Iteration 24900, lr = 0.00610937
I0630 01:53:02.852100 29015 solver.cpp:354] Sparsity after update:
I0630 01:53:02.853375 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:53:02.853382 29015 net.cpp:1851] conv1a_param_0(0.25) 
I0630 01:53:02.853389 29015 net.cpp:1851] conv1b_param_0(0.5) 
I0630 01:53:02.853392 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:53:02.853395 29015 net.cpp:1851] res2a_branch2a_param_0(0.5) 
I0630 01:53:02.853399 29015 net.cpp:1851] res2a_branch2b_param_0(0.5) 
I0630 01:53:02.853400 29015 net.cpp:1851] res3a_branch2a_param_0(0.5) 
I0630 01:53:02.853404 29015 net.cpp:1851] res3a_branch2b_param_0(0.5) 
I0630 01:53:02.853405 29015 net.cpp:1851] res4a_branch2a_param_0(0.5) 
I0630 01:53:02.853408 29015 net.cpp:1851] res4a_branch2b_param_0(0.5) 
I0630 01:53:02.853410 29015 net.cpp:1851] res5a_branch2a_param_0(0.5) 
I0630 01:53:02.853413 29015 net.cpp:1851] res5a_branch2b_param_0(0.5) 
I0630 01:53:02.853415 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.17679e+06/2.3599e+06) 0.499
I0630 01:53:02.853509 29015 solver.cpp:471] Iteration 25000, Testing net (#0)
I0630 01:53:04.491672 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.917
I0630 01:53:04.491690 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9965
I0630 01:53:04.491696 29015 solver.cpp:544]     Test net output #2: loss = 0.2016 (* 1 = 0.2016 loss)
I0630 01:53:04.512564 29015 solver.cpp:290] Iteration 25000 (27.0501 iter/s, 3.69685s/100 iter), loss = 0
I0630 01:53:04.512584 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:04.512593 29015 sgd_solver.cpp:106] Iteration 25000, lr = 0.00609375
I0630 01:53:04.513134 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.52
I0630 01:53:05.133620 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:53:07.189388 29015 solver.cpp:290] Iteration 25100 (37.3591 iter/s, 2.67672s/100 iter), loss = 0
I0630 01:53:07.189411 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:07.189419 29015 sgd_solver.cpp:106] Iteration 25100, lr = 0.00607812
I0630 01:53:09.264545 29015 solver.cpp:290] Iteration 25200 (48.1912 iter/s, 2.07507s/100 iter), loss = 0
I0630 01:53:09.264569 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:09.264575 29015 sgd_solver.cpp:106] Iteration 25200, lr = 0.0060625
I0630 01:53:11.316860 29015 solver.cpp:290] Iteration 25300 (48.7276 iter/s, 2.05223s/100 iter), loss = 0
I0630 01:53:11.316884 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:11.316892 29015 sgd_solver.cpp:106] Iteration 25300, lr = 0.00604687
I0630 01:53:13.372911 29015 solver.cpp:290] Iteration 25400 (48.639 iter/s, 2.05596s/100 iter), loss = 0
I0630 01:53:13.372934 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:13.372959 29015 sgd_solver.cpp:106] Iteration 25400, lr = 0.00603125
I0630 01:53:15.426880 29015 solver.cpp:290] Iteration 25500 (48.6883 iter/s, 2.05388s/100 iter), loss = 0
I0630 01:53:15.426903 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:15.426909 29015 sgd_solver.cpp:106] Iteration 25500, lr = 0.00601562
I0630 01:53:17.486565 29015 solver.cpp:290] Iteration 25600 (48.5532 iter/s, 2.0596s/100 iter), loss = 0
I0630 01:53:17.486588 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:17.486595 29015 sgd_solver.cpp:106] Iteration 25600, lr = 0.006
I0630 01:53:19.542414 29015 solver.cpp:290] Iteration 25700 (48.6438 iter/s, 2.05576s/100 iter), loss = 0
I0630 01:53:19.542435 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:19.542443 29015 sgd_solver.cpp:106] Iteration 25700, lr = 0.00598437
I0630 01:53:21.599553 29015 solver.cpp:290] Iteration 25800 (48.6133 iter/s, 2.05705s/100 iter), loss = 0
I0630 01:53:21.599576 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:21.599586 29015 sgd_solver.cpp:106] Iteration 25800, lr = 0.00596875
I0630 01:53:23.658293 29015 solver.cpp:290] Iteration 25900 (48.5755 iter/s, 2.05865s/100 iter), loss = 0
I0630 01:53:23.658345 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:23.658354 29015 sgd_solver.cpp:106] Iteration 25900, lr = 0.00595312
I0630 01:53:25.692698 29015 solver.cpp:354] Sparsity after update:
I0630 01:53:25.693979 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:53:25.693984 29015 net.cpp:1851] conv1a_param_0(0.26) 
I0630 01:53:25.693992 29015 net.cpp:1851] conv1b_param_0(0.52) 
I0630 01:53:25.693994 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:53:25.693997 29015 net.cpp:1851] res2a_branch2a_param_0(0.52) 
I0630 01:53:25.694000 29015 net.cpp:1851] res2a_branch2b_param_0(0.52) 
I0630 01:53:25.694002 29015 net.cpp:1851] res3a_branch2a_param_0(0.52) 
I0630 01:53:25.694005 29015 net.cpp:1851] res3a_branch2b_param_0(0.52) 
I0630 01:53:25.694007 29015 net.cpp:1851] res4a_branch2a_param_0(0.52) 
I0630 01:53:25.694010 29015 net.cpp:1851] res4a_branch2b_param_0(0.52) 
I0630 01:53:25.694012 29015 net.cpp:1851] res5a_branch2a_param_0(0.52) 
I0630 01:53:25.694015 29015 net.cpp:1851] res5a_branch2b_param_0(0.52) 
I0630 01:53:25.694016 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.22386e+06/2.3599e+06) 0.519
I0630 01:53:25.694104 29015 solver.cpp:471] Iteration 26000, Testing net (#0)
I0630 01:53:27.332832 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9162
I0630 01:53:27.332854 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.997
I0630 01:53:27.332859 29015 solver.cpp:544]     Test net output #2: loss = 0.2028 (* 1 = 0.2028 loss)
I0630 01:53:27.354873 29015 solver.cpp:290] Iteration 26000 (27.0532 iter/s, 3.69642s/100 iter), loss = 0
I0630 01:53:27.354892 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:27.354902 29015 sgd_solver.cpp:106] Iteration 26000, lr = 0.0059375
I0630 01:53:27.355448 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.54
I0630 01:53:27.993993 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:53:30.051065 29015 solver.cpp:290] Iteration 26100 (37.0907 iter/s, 2.69609s/100 iter), loss = 0
I0630 01:53:30.051087 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:30.051093 29015 sgd_solver.cpp:106] Iteration 26100, lr = 0.00592188
I0630 01:53:32.106640 29015 solver.cpp:290] Iteration 26200 (48.6502 iter/s, 2.05549s/100 iter), loss = 0
I0630 01:53:32.106662 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:32.106668 29015 sgd_solver.cpp:106] Iteration 26200, lr = 0.00590625
I0630 01:53:34.159575 29015 solver.cpp:290] Iteration 26300 (48.7128 iter/s, 2.05285s/100 iter), loss = 0
I0630 01:53:34.159596 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:34.159605 29015 sgd_solver.cpp:106] Iteration 26300, lr = 0.00589063
I0630 01:53:36.215476 29015 solver.cpp:290] Iteration 26400 (48.6425 iter/s, 2.05582s/100 iter), loss = 0
I0630 01:53:36.215499 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:36.215508 29015 sgd_solver.cpp:106] Iteration 26400, lr = 0.005875
I0630 01:53:38.268193 29015 solver.cpp:290] Iteration 26500 (48.718 iter/s, 2.05263s/100 iter), loss = 0
I0630 01:53:38.268214 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:38.268221 29015 sgd_solver.cpp:106] Iteration 26500, lr = 0.00585938
I0630 01:53:40.323005 29015 solver.cpp:290] Iteration 26600 (48.6683 iter/s, 2.05473s/100 iter), loss = 0
I0630 01:53:40.323029 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:40.323035 29015 sgd_solver.cpp:106] Iteration 26600, lr = 0.00584375
I0630 01:53:42.377677 29015 solver.cpp:290] Iteration 26700 (48.6716 iter/s, 2.05458s/100 iter), loss = 0
I0630 01:53:42.377699 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:42.377706 29015 sgd_solver.cpp:106] Iteration 26700, lr = 0.00582812
I0630 01:53:44.434330 29015 solver.cpp:290] Iteration 26800 (48.6248 iter/s, 2.05657s/100 iter), loss = 0
I0630 01:53:44.434361 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:44.434368 29015 sgd_solver.cpp:106] Iteration 26800, lr = 0.0058125
I0630 01:53:46.487848 29015 solver.cpp:290] Iteration 26900 (48.6992 iter/s, 2.05342s/100 iter), loss = 0
I0630 01:53:46.487874 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:46.487884 29015 sgd_solver.cpp:106] Iteration 26900, lr = 0.00579687
I0630 01:53:48.520813 29015 solver.cpp:354] Sparsity after update:
I0630 01:53:48.522075 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:53:48.522081 29015 net.cpp:1851] conv1a_param_0(0.27) 
I0630 01:53:48.522089 29015 net.cpp:1851] conv1b_param_0(0.54) 
I0630 01:53:48.522091 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:53:48.522094 29015 net.cpp:1851] res2a_branch2a_param_0(0.54) 
I0630 01:53:48.522095 29015 net.cpp:1851] res2a_branch2b_param_0(0.54) 
I0630 01:53:48.522097 29015 net.cpp:1851] res3a_branch2a_param_0(0.54) 
I0630 01:53:48.522099 29015 net.cpp:1851] res3a_branch2b_param_0(0.54) 
I0630 01:53:48.522101 29015 net.cpp:1851] res4a_branch2a_param_0(0.54) 
I0630 01:53:48.522104 29015 net.cpp:1851] res4a_branch2b_param_0(0.54) 
I0630 01:53:48.522105 29015 net.cpp:1851] res5a_branch2a_param_0(0.54) 
I0630 01:53:48.522107 29015 net.cpp:1851] res5a_branch2b_param_0(0.54) 
I0630 01:53:48.522109 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.27093e+06/2.3599e+06) 0.539
I0630 01:53:48.522197 29015 solver.cpp:471] Iteration 27000, Testing net (#0)
I0630 01:53:50.161535 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.915
I0630 01:53:50.161554 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9963
I0630 01:53:50.161561 29015 solver.cpp:544]     Test net output #2: loss = 0.2041 (* 1 = 0.2041 loss)
I0630 01:53:50.181360 29015 solver.cpp:290] Iteration 27000 (27.0755 iter/s, 3.69338s/100 iter), loss = 0
I0630 01:53:50.181376 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:50.181390 29015 sgd_solver.cpp:106] Iteration 27000, lr = 0.00578125
I0630 01:53:50.181923 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.56
I0630 01:53:50.855242 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:53:52.914207 29015 solver.cpp:290] Iteration 27100 (36.5932 iter/s, 2.73275s/100 iter), loss = 0
I0630 01:53:52.914239 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:52.914252 29015 sgd_solver.cpp:106] Iteration 27100, lr = 0.00576563
I0630 01:53:54.969884 29015 solver.cpp:290] Iteration 27200 (48.648 iter/s, 2.05558s/100 iter), loss = 0
I0630 01:53:54.969964 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:54.969975 29015 sgd_solver.cpp:106] Iteration 27200, lr = 0.00575
I0630 01:53:57.024755 29015 solver.cpp:290] Iteration 27300 (48.6682 iter/s, 2.05473s/100 iter), loss = 0
I0630 01:53:57.024777 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:57.024785 29015 sgd_solver.cpp:106] Iteration 27300, lr = 0.00573438
I0630 01:53:59.080796 29015 solver.cpp:290] Iteration 27400 (48.6393 iter/s, 2.05595s/100 iter), loss = 0
I0630 01:53:59.080816 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:53:59.080823 29015 sgd_solver.cpp:106] Iteration 27400, lr = 0.00571875
I0630 01:54:01.140151 29015 solver.cpp:290] Iteration 27500 (48.5609 iter/s, 2.05927s/100 iter), loss = 0
I0630 01:54:01.140172 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:01.140180 29015 sgd_solver.cpp:106] Iteration 27500, lr = 0.00570312
I0630 01:54:03.194702 29015 solver.cpp:290] Iteration 27600 (48.6745 iter/s, 2.05447s/100 iter), loss = 0
I0630 01:54:03.194726 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:03.194736 29015 sgd_solver.cpp:106] Iteration 27600, lr = 0.0056875
I0630 01:54:05.249008 29015 solver.cpp:290] Iteration 27700 (48.6804 iter/s, 2.05422s/100 iter), loss = 0
I0630 01:54:05.249033 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:05.249042 29015 sgd_solver.cpp:106] Iteration 27700, lr = 0.00567187
I0630 01:54:07.306790 29015 solver.cpp:290] Iteration 27800 (48.5982 iter/s, 2.05769s/100 iter), loss = 0
I0630 01:54:07.306849 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:07.306864 29015 sgd_solver.cpp:106] Iteration 27800, lr = 0.00565625
I0630 01:54:09.360980 29015 solver.cpp:290] Iteration 27900 (48.6839 iter/s, 2.05407s/100 iter), loss = 0
I0630 01:54:09.361001 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:09.361008 29015 sgd_solver.cpp:106] Iteration 27900, lr = 0.00564062
I0630 01:54:11.395774 29015 solver.cpp:354] Sparsity after update:
I0630 01:54:11.397043 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:54:11.397050 29015 net.cpp:1851] conv1a_param_0(0.28) 
I0630 01:54:11.397058 29015 net.cpp:1851] conv1b_param_0(0.56) 
I0630 01:54:11.397060 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:54:11.397063 29015 net.cpp:1851] res2a_branch2a_param_0(0.56) 
I0630 01:54:11.397066 29015 net.cpp:1851] res2a_branch2b_param_0(0.56) 
I0630 01:54:11.397068 29015 net.cpp:1851] res3a_branch2a_param_0(0.56) 
I0630 01:54:11.397070 29015 net.cpp:1851] res3a_branch2b_param_0(0.56) 
I0630 01:54:11.397073 29015 net.cpp:1851] res4a_branch2a_param_0(0.56) 
I0630 01:54:11.397074 29015 net.cpp:1851] res4a_branch2b_param_0(0.56) 
I0630 01:54:11.397076 29015 net.cpp:1851] res5a_branch2a_param_0(0.56) 
I0630 01:54:11.397078 29015 net.cpp:1851] res5a_branch2b_param_0(0.56) 
I0630 01:54:11.397081 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.318e+06/2.3599e+06) 0.558
I0630 01:54:11.397168 29015 solver.cpp:471] Iteration 28000, Testing net (#0)
I0630 01:54:13.040478 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9148
I0630 01:54:13.040498 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9963
I0630 01:54:13.040503 29015 solver.cpp:544]     Test net output #2: loss = 0.2025 (* 1 = 0.2025 loss)
I0630 01:54:13.060571 29015 solver.cpp:290] Iteration 28000 (27.031 iter/s, 3.69946s/100 iter), loss = 0
I0630 01:54:13.060590 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:13.060601 29015 sgd_solver.cpp:106] Iteration 28000, lr = 0.005625
I0630 01:54:13.061127 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.58
I0630 01:54:13.753742 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:54:15.808017 29015 solver.cpp:290] Iteration 28100 (36.3988 iter/s, 2.74734s/100 iter), loss = 0
I0630 01:54:15.808055 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:15.808066 29015 sgd_solver.cpp:106] Iteration 28100, lr = 0.00560937
I0630 01:54:17.872401 29015 solver.cpp:290] Iteration 28200 (48.443 iter/s, 2.06428s/100 iter), loss = 0
I0630 01:54:17.872426 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:17.872434 29015 sgd_solver.cpp:106] Iteration 28200, lr = 0.00559375
I0630 01:54:19.926692 29015 solver.cpp:290] Iteration 28300 (48.6807 iter/s, 2.0542s/100 iter), loss = 0
I0630 01:54:19.926715 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:19.926724 29015 sgd_solver.cpp:106] Iteration 28300, lr = 0.00557812
I0630 01:54:21.980610 29015 solver.cpp:290] Iteration 28400 (48.6895 iter/s, 2.05383s/100 iter), loss = 0
I0630 01:54:21.980635 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:21.980644 29015 sgd_solver.cpp:106] Iteration 28400, lr = 0.0055625
I0630 01:54:24.046368 29015 solver.cpp:290] Iteration 28500 (48.4104 iter/s, 2.06567s/100 iter), loss = 0
I0630 01:54:24.046391 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:24.046398 29015 sgd_solver.cpp:106] Iteration 28500, lr = 0.00554687
I0630 01:54:26.104881 29015 solver.cpp:290] Iteration 28600 (48.5808 iter/s, 2.05843s/100 iter), loss = 0
I0630 01:54:26.104943 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:26.104950 29015 sgd_solver.cpp:106] Iteration 28600, lr = 0.00553125
I0630 01:54:28.167084 29015 solver.cpp:290] Iteration 28700 (48.4947 iter/s, 2.06208s/100 iter), loss = 0
I0630 01:54:28.167106 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:28.167114 29015 sgd_solver.cpp:106] Iteration 28700, lr = 0.00551562
I0630 01:54:30.223207 29015 solver.cpp:290] Iteration 28800 (48.6373 iter/s, 2.05604s/100 iter), loss = 0
I0630 01:54:30.223230 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:30.223237 29015 sgd_solver.cpp:106] Iteration 28800, lr = 0.0055
I0630 01:54:32.281286 29015 solver.cpp:290] Iteration 28900 (48.5911 iter/s, 2.05799s/100 iter), loss = 0
I0630 01:54:32.281309 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:32.281316 29015 sgd_solver.cpp:106] Iteration 28900, lr = 0.00548437
I0630 01:54:34.316206 29015 solver.cpp:354] Sparsity after update:
I0630 01:54:34.317490 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:54:34.317498 29015 net.cpp:1851] conv1a_param_0(0.29) 
I0630 01:54:34.317505 29015 net.cpp:1851] conv1b_param_0(0.579) 
I0630 01:54:34.317508 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:54:34.317510 29015 net.cpp:1851] res2a_branch2a_param_0(0.58) 
I0630 01:54:34.317513 29015 net.cpp:1851] res2a_branch2b_param_0(0.58) 
I0630 01:54:34.317515 29015 net.cpp:1851] res3a_branch2a_param_0(0.58) 
I0630 01:54:34.317517 29015 net.cpp:1851] res3a_branch2b_param_0(0.58) 
I0630 01:54:34.317520 29015 net.cpp:1851] res4a_branch2a_param_0(0.58) 
I0630 01:54:34.317523 29015 net.cpp:1851] res4a_branch2b_param_0(0.58) 
I0630 01:54:34.317524 29015 net.cpp:1851] res5a_branch2a_param_0(0.58) 
I0630 01:54:34.317528 29015 net.cpp:1851] res5a_branch2b_param_0(0.58) 
I0630 01:54:34.317530 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.36506e+06/2.3599e+06) 0.578
I0630 01:54:34.317664 29015 solver.cpp:471] Iteration 29000, Testing net (#0)
I0630 01:54:35.960777 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9128
I0630 01:54:35.960798 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9963
I0630 01:54:35.960803 29015 solver.cpp:544]     Test net output #2: loss = 0.2048 (* 1 = 0.2048 loss)
I0630 01:54:35.980444 29015 solver.cpp:290] Iteration 29000 (27.0342 iter/s, 3.69902s/100 iter), loss = 0
I0630 01:54:35.980463 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:35.980473 29015 sgd_solver.cpp:106] Iteration 29000, lr = 0.00546875
I0630 01:54:35.980954 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.6
I0630 01:54:36.694331 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:54:38.753998 29015 solver.cpp:290] Iteration 29100 (36.0562 iter/s, 2.77345s/100 iter), loss = 0
I0630 01:54:38.754020 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:38.754027 29015 sgd_solver.cpp:106] Iteration 29100, lr = 0.00545313
I0630 01:54:40.813805 29015 solver.cpp:290] Iteration 29200 (48.5504 iter/s, 2.05972s/100 iter), loss = 0
I0630 01:54:40.813836 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:40.813843 29015 sgd_solver.cpp:106] Iteration 29200, lr = 0.0054375
I0630 01:54:42.871289 29015 solver.cpp:290] Iteration 29300 (48.6053 iter/s, 2.05739s/100 iter), loss = 0
I0630 01:54:42.871311 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:42.871317 29015 sgd_solver.cpp:106] Iteration 29300, lr = 0.00542188
I0630 01:54:44.925870 29015 solver.cpp:290] Iteration 29400 (48.6738 iter/s, 2.05449s/100 iter), loss = 0
I0630 01:54:44.925895 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:44.925905 29015 sgd_solver.cpp:106] Iteration 29400, lr = 0.00540625
I0630 01:54:46.981989 29015 solver.cpp:290] Iteration 29500 (48.6375 iter/s, 2.05603s/100 iter), loss = 0
I0630 01:54:46.982033 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:46.982039 29015 sgd_solver.cpp:106] Iteration 29500, lr = 0.00539062
I0630 01:54:49.038652 29015 solver.cpp:290] Iteration 29600 (48.625 iter/s, 2.05656s/100 iter), loss = 0
I0630 01:54:49.038674 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:49.038683 29015 sgd_solver.cpp:106] Iteration 29600, lr = 0.005375
I0630 01:54:51.096977 29015 solver.cpp:290] Iteration 29700 (48.5852 iter/s, 2.05824s/100 iter), loss = 0
I0630 01:54:51.096999 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:51.097007 29015 sgd_solver.cpp:106] Iteration 29700, lr = 0.00535937
I0630 01:54:53.151444 29015 solver.cpp:290] Iteration 29800 (48.6765 iter/s, 2.05438s/100 iter), loss = 0
I0630 01:54:53.151466 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:53.151473 29015 sgd_solver.cpp:106] Iteration 29800, lr = 0.00534375
I0630 01:54:55.207819 29015 solver.cpp:290] Iteration 29900 (48.6313 iter/s, 2.05629s/100 iter), loss = 0
I0630 01:54:55.207842 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:55.207849 29015 sgd_solver.cpp:106] Iteration 29900, lr = 0.00532812
I0630 01:54:57.241358 29015 solver.cpp:598] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_30000.caffemodel
I0630 01:54:57.257815 29015 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_30000.solverstate
I0630 01:54:57.265044 29015 solver.cpp:354] Sparsity after update:
I0630 01:54:57.265991 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:54:57.266000 29015 net.cpp:1851] conv1a_param_0(0.3) 
I0630 01:54:57.266007 29015 net.cpp:1851] conv1b_param_0(0.6) 
I0630 01:54:57.266010 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:54:57.266012 29015 net.cpp:1851] res2a_branch2a_param_0(0.6) 
I0630 01:54:57.266014 29015 net.cpp:1851] res2a_branch2b_param_0(0.6) 
I0630 01:54:57.266016 29015 net.cpp:1851] res3a_branch2a_param_0(0.6) 
I0630 01:54:57.266018 29015 net.cpp:1851] res3a_branch2b_param_0(0.6) 
I0630 01:54:57.266021 29015 net.cpp:1851] res4a_branch2a_param_0(0.6) 
I0630 01:54:57.266022 29015 net.cpp:1851] res4a_branch2b_param_0(0.6) 
I0630 01:54:57.266024 29015 net.cpp:1851] res5a_branch2a_param_0(0.6) 
I0630 01:54:57.266026 29015 net.cpp:1851] res5a_branch2b_param_0(0.6) 
I0630 01:54:57.266027 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.41214e+06/2.3599e+06) 0.598
I0630 01:54:57.266125 29015 solver.cpp:471] Iteration 30000, Testing net (#0)
I0630 01:54:58.902070 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9123
I0630 01:54:58.902088 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9962
I0630 01:54:58.902093 29015 solver.cpp:544]     Test net output #2: loss = 0.2109 (* 1 = 0.2109 loss)
I0630 01:54:58.922245 29015 solver.cpp:290] Iteration 30000 (26.923 iter/s, 3.71429s/100 iter), loss = 0
I0630 01:54:58.922263 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:54:58.922276 29015 sgd_solver.cpp:106] Iteration 30000, lr = 0.0053125
I0630 01:54:58.922852 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.62
I0630 01:54:59.694198 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:55:01.749969 29015 solver.cpp:290] Iteration 30100 (35.3655 iter/s, 2.82762s/100 iter), loss = 0
I0630 01:55:01.749991 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:01.749999 29015 sgd_solver.cpp:106] Iteration 30100, lr = 0.00529688
I0630 01:55:03.809131 29015 solver.cpp:290] Iteration 30200 (48.5655 iter/s, 2.05908s/100 iter), loss = 0
I0630 01:55:03.809154 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:03.809160 29015 sgd_solver.cpp:106] Iteration 30200, lr = 0.00528125
I0630 01:55:05.869904 29015 solver.cpp:290] Iteration 30300 (48.5276 iter/s, 2.06068s/100 iter), loss = 0
I0630 01:55:05.869928 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:05.869937 29015 sgd_solver.cpp:106] Iteration 30300, lr = 0.00526563
I0630 01:55:07.925861 29015 solver.cpp:290] Iteration 30400 (48.6413 iter/s, 2.05587s/100 iter), loss = 0
I0630 01:55:07.925884 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:07.925889 29015 sgd_solver.cpp:106] Iteration 30400, lr = 0.00525
I0630 01:55:09.983191 29015 solver.cpp:290] Iteration 30500 (48.6088 iter/s, 2.05724s/100 iter), loss = 0
I0630 01:55:09.983216 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:09.983223 29015 sgd_solver.cpp:106] Iteration 30500, lr = 0.00523437
I0630 01:55:12.039346 29015 solver.cpp:290] Iteration 30600 (48.6365 iter/s, 2.05607s/100 iter), loss = 0
I0630 01:55:12.039369 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:12.039376 29015 sgd_solver.cpp:106] Iteration 30600, lr = 0.00521875
I0630 01:55:14.093045 29015 solver.cpp:290] Iteration 30700 (48.6947 iter/s, 2.05361s/100 iter), loss = 0
I0630 01:55:14.093070 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:14.093076 29015 sgd_solver.cpp:106] Iteration 30700, lr = 0.00520312
I0630 01:55:16.147573 29015 solver.cpp:290] Iteration 30800 (48.6751 iter/s, 2.05444s/100 iter), loss = 0
I0630 01:55:16.147624 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:16.147635 29015 sgd_solver.cpp:106] Iteration 30800, lr = 0.0051875
I0630 01:55:18.199636 29015 solver.cpp:290] Iteration 30900 (48.7341 iter/s, 2.05195s/100 iter), loss = 0
I0630 01:55:18.199659 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:18.199666 29015 sgd_solver.cpp:106] Iteration 30900, lr = 0.00517187
I0630 01:55:20.233336 29015 solver.cpp:354] Sparsity after update:
I0630 01:55:20.234606 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:55:20.234612 29015 net.cpp:1851] conv1a_param_0(0.31) 
I0630 01:55:20.234621 29015 net.cpp:1851] conv1b_param_0(0.62) 
I0630 01:55:20.234622 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:55:20.234624 29015 net.cpp:1851] res2a_branch2a_param_0(0.62) 
I0630 01:55:20.234627 29015 net.cpp:1851] res2a_branch2b_param_0(0.62) 
I0630 01:55:20.234629 29015 net.cpp:1851] res3a_branch2a_param_0(0.62) 
I0630 01:55:20.234630 29015 net.cpp:1851] res3a_branch2b_param_0(0.62) 
I0630 01:55:20.234632 29015 net.cpp:1851] res4a_branch2a_param_0(0.62) 
I0630 01:55:20.234634 29015 net.cpp:1851] res4a_branch2b_param_0(0.62) 
I0630 01:55:20.234637 29015 net.cpp:1851] res5a_branch2a_param_0(0.62) 
I0630 01:55:20.234638 29015 net.cpp:1851] res5a_branch2b_param_0(0.62) 
I0630 01:55:20.234640 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.45921e+06/2.3599e+06) 0.618
I0630 01:55:20.234727 29015 solver.cpp:471] Iteration 31000, Testing net (#0)
I0630 01:55:21.872704 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.914
I0630 01:55:21.872722 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9963
I0630 01:55:21.872728 29015 solver.cpp:544]     Test net output #2: loss = 0.2083 (* 1 = 0.2083 loss)
I0630 01:55:21.892493 29015 solver.cpp:290] Iteration 31000 (27.0803 iter/s, 3.69272s/100 iter), loss = 0
I0630 01:55:21.892520 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:21.892527 29015 sgd_solver.cpp:106] Iteration 31000, lr = 0.00515625
I0630 01:55:21.893142 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.64
I0630 01:55:22.674687 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:55:24.747509 29015 solver.cpp:290] Iteration 31100 (35.0275 iter/s, 2.8549s/100 iter), loss = 0
I0630 01:55:24.747539 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:24.747548 29015 sgd_solver.cpp:106] Iteration 31100, lr = 0.00514062
I0630 01:55:26.803755 29015 solver.cpp:290] Iteration 31200 (48.6345 iter/s, 2.05615s/100 iter), loss = 0
I0630 01:55:26.803781 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:26.803787 29015 sgd_solver.cpp:106] Iteration 31200, lr = 0.005125
I0630 01:55:28.859252 29015 solver.cpp:290] Iteration 31300 (48.6522 iter/s, 2.0554s/100 iter), loss = 0
I0630 01:55:28.859360 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:28.859380 29015 sgd_solver.cpp:106] Iteration 31300, lr = 0.00510937
I0630 01:55:30.917551 29015 solver.cpp:290] Iteration 31400 (48.5878 iter/s, 2.05813s/100 iter), loss = 0
I0630 01:55:30.917574 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:30.917583 29015 sgd_solver.cpp:106] Iteration 31400, lr = 0.00509375
I0630 01:55:32.972857 29015 solver.cpp:290] Iteration 31500 (48.6566 iter/s, 2.05522s/100 iter), loss = 0
I0630 01:55:32.972879 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:32.972887 29015 sgd_solver.cpp:106] Iteration 31500, lr = 0.00507812
I0630 01:55:35.027335 29015 solver.cpp:290] Iteration 31600 (48.6762 iter/s, 2.05439s/100 iter), loss = 0
I0630 01:55:35.027360 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:35.027369 29015 sgd_solver.cpp:106] Iteration 31600, lr = 0.0050625
I0630 01:55:37.085808 29015 solver.cpp:290] Iteration 31700 (48.5819 iter/s, 2.05838s/100 iter), loss = 0
I0630 01:55:37.085832 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:37.085842 29015 sgd_solver.cpp:106] Iteration 31700, lr = 0.00504687
I0630 01:55:39.147521 29015 solver.cpp:290] Iteration 31800 (48.5054 iter/s, 2.06163s/100 iter), loss = 0
I0630 01:55:39.147543 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:39.147552 29015 sgd_solver.cpp:106] Iteration 31800, lr = 0.00503125
I0630 01:55:41.201611 29015 solver.cpp:290] Iteration 31900 (48.6854 iter/s, 2.054s/100 iter), loss = 0
I0630 01:55:41.201632 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:41.201639 29015 sgd_solver.cpp:106] Iteration 31900, lr = 0.00501562
I0630 01:55:43.238438 29015 solver.cpp:354] Sparsity after update:
I0630 01:55:43.239715 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:55:43.239722 29015 net.cpp:1851] conv1a_param_0(0.32) 
I0630 01:55:43.239733 29015 net.cpp:1851] conv1b_param_0(0.64) 
I0630 01:55:43.239739 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:55:43.239743 29015 net.cpp:1851] res2a_branch2a_param_0(0.64) 
I0630 01:55:43.239748 29015 net.cpp:1851] res2a_branch2b_param_0(0.64) 
I0630 01:55:43.239751 29015 net.cpp:1851] res3a_branch2a_param_0(0.64) 
I0630 01:55:43.239756 29015 net.cpp:1851] res3a_branch2b_param_0(0.64) 
I0630 01:55:43.239759 29015 net.cpp:1851] res4a_branch2a_param_0(0.64) 
I0630 01:55:43.239764 29015 net.cpp:1851] res4a_branch2b_param_0(0.64) 
I0630 01:55:43.239768 29015 net.cpp:1851] res5a_branch2a_param_0(0.64) 
I0630 01:55:43.239773 29015 net.cpp:1851] res5a_branch2b_param_0(0.64) 
I0630 01:55:43.239776 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.50628e+06/2.3599e+06) 0.638
I0630 01:55:43.239867 29015 solver.cpp:471] Iteration 32000, Testing net (#0)
I0630 01:55:44.875756 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9094
I0630 01:55:44.875782 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9963
I0630 01:55:44.875788 29015 solver.cpp:544]     Test net output #2: loss = 0.2107 (* 1 = 0.2107 loss)
I0630 01:55:44.896142 29015 solver.cpp:290] Iteration 32000 (27.068 iter/s, 3.6944s/100 iter), loss = 0
I0630 01:55:44.896167 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:44.896176 29015 sgd_solver.cpp:106] Iteration 32000, lr = 0.005
I0630 01:55:44.896739 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.66
I0630 01:55:45.705920 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:55:47.762543 29015 solver.cpp:290] Iteration 32100 (34.8883 iter/s, 2.86629s/100 iter), loss = 0
I0630 01:55:47.762565 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:47.762573 29015 sgd_solver.cpp:106] Iteration 32100, lr = 0.00498438
I0630 01:55:49.818377 29015 solver.cpp:290] Iteration 32200 (48.6441 iter/s, 2.05575s/100 iter), loss = 0
I0630 01:55:49.818400 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:49.818423 29015 sgd_solver.cpp:106] Iteration 32200, lr = 0.00496875
I0630 01:55:51.876652 29015 solver.cpp:290] Iteration 32300 (48.5865 iter/s, 2.05819s/100 iter), loss = 0
I0630 01:55:51.876673 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:51.876680 29015 sgd_solver.cpp:106] Iteration 32300, lr = 0.00495313
I0630 01:55:53.933140 29015 solver.cpp:290] Iteration 32400 (48.6287 iter/s, 2.0564s/100 iter), loss = 0
I0630 01:55:53.933161 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:53.933167 29015 sgd_solver.cpp:106] Iteration 32400, lr = 0.0049375
I0630 01:55:55.988200 29015 solver.cpp:290] Iteration 32500 (48.6624 iter/s, 2.05498s/100 iter), loss = 0
I0630 01:55:55.988224 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:55.988232 29015 sgd_solver.cpp:106] Iteration 32500, lr = 0.00492187
I0630 01:55:58.042055 29015 solver.cpp:290] Iteration 32600 (48.691 iter/s, 2.05377s/100 iter), loss = 0
I0630 01:55:58.042079 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:55:58.042088 29015 sgd_solver.cpp:106] Iteration 32600, lr = 0.00490625
I0630 01:56:00.095729 29015 solver.cpp:290] Iteration 32700 (48.6954 iter/s, 2.05358s/100 iter), loss = 0
I0630 01:56:00.095805 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:00.095814 29015 sgd_solver.cpp:106] Iteration 32700, lr = 0.00489062
I0630 01:56:02.150095 29015 solver.cpp:290] Iteration 32800 (48.6801 iter/s, 2.05423s/100 iter), loss = 0
I0630 01:56:02.150117 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:02.150125 29015 sgd_solver.cpp:106] Iteration 32800, lr = 0.004875
I0630 01:56:04.205725 29015 solver.cpp:290] Iteration 32900 (48.6489 iter/s, 2.05554s/100 iter), loss = 0
I0630 01:56:04.205747 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:04.205756 29015 sgd_solver.cpp:106] Iteration 32900, lr = 0.00485937
I0630 01:56:06.240263 29015 solver.cpp:354] Sparsity after update:
I0630 01:56:06.241513 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:56:06.241520 29015 net.cpp:1851] conv1a_param_0(0.33) 
I0630 01:56:06.241529 29015 net.cpp:1851] conv1b_param_0(0.66) 
I0630 01:56:06.241534 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:56:06.241539 29015 net.cpp:1851] res2a_branch2a_param_0(0.66) 
I0630 01:56:06.241544 29015 net.cpp:1851] res2a_branch2b_param_0(0.66) 
I0630 01:56:06.241549 29015 net.cpp:1851] res3a_branch2a_param_0(0.66) 
I0630 01:56:06.241552 29015 net.cpp:1851] res3a_branch2b_param_0(0.66) 
I0630 01:56:06.241557 29015 net.cpp:1851] res4a_branch2a_param_0(0.66) 
I0630 01:56:06.241561 29015 net.cpp:1851] res4a_branch2b_param_0(0.66) 
I0630 01:56:06.241566 29015 net.cpp:1851] res5a_branch2a_param_0(0.66) 
I0630 01:56:06.241571 29015 net.cpp:1851] res5a_branch2b_param_0(0.66) 
I0630 01:56:06.241575 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.55336e+06/2.3599e+06) 0.658
I0630 01:56:06.241669 29015 solver.cpp:471] Iteration 33000, Testing net (#0)
I0630 01:56:07.878237 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9117
I0630 01:56:07.878257 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9964
I0630 01:56:07.878262 29015 solver.cpp:544]     Test net output #2: loss = 0.2074 (* 1 = 0.2074 loss)
I0630 01:56:07.899209 29015 solver.cpp:290] Iteration 33000 (27.0757 iter/s, 3.69335s/100 iter), loss = 0
I0630 01:56:07.899240 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:07.899250 29015 sgd_solver.cpp:106] Iteration 33000, lr = 0.00484375
I0630 01:56:07.900014 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.68
I0630 01:56:08.738631 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:56:10.800022 29015 solver.cpp:290] Iteration 33100 (34.4745 iter/s, 2.9007s/100 iter), loss = 0
I0630 01:56:10.800045 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:10.800051 29015 sgd_solver.cpp:106] Iteration 33100, lr = 0.00482813
I0630 01:56:12.853410 29015 solver.cpp:290] Iteration 33200 (48.7021 iter/s, 2.0533s/100 iter), loss = 0
I0630 01:56:12.853432 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:12.853440 29015 sgd_solver.cpp:106] Iteration 33200, lr = 0.0048125
I0630 01:56:14.908212 29015 solver.cpp:290] Iteration 33300 (48.6686 iter/s, 2.05471s/100 iter), loss = 0
I0630 01:56:14.908241 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:14.908251 29015 sgd_solver.cpp:106] Iteration 33300, lr = 0.00479688
I0630 01:56:16.964588 29015 solver.cpp:290] Iteration 33400 (48.6314 iter/s, 2.05629s/100 iter), loss = 0
I0630 01:56:16.964612 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:16.964622 29015 sgd_solver.cpp:106] Iteration 33400, lr = 0.00478125
I0630 01:56:19.024181 29015 solver.cpp:290] Iteration 33500 (48.5553 iter/s, 2.05951s/100 iter), loss = 0
I0630 01:56:19.024204 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:19.024210 29015 sgd_solver.cpp:106] Iteration 33500, lr = 0.00476563
I0630 01:56:21.081120 29015 solver.cpp:290] Iteration 33600 (48.618 iter/s, 2.05685s/100 iter), loss = 0
I0630 01:56:21.081158 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:21.081168 29015 sgd_solver.cpp:106] Iteration 33600, lr = 0.00475
I0630 01:56:23.132580 29015 solver.cpp:290] Iteration 33700 (48.7482 iter/s, 2.05136s/100 iter), loss = 0
I0630 01:56:23.132602 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:23.132611 29015 sgd_solver.cpp:106] Iteration 33700, lr = 0.00473437
I0630 01:56:25.187274 29015 solver.cpp:290] Iteration 33800 (48.6711 iter/s, 2.05461s/100 iter), loss = 0
I0630 01:56:25.187297 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:25.187304 29015 sgd_solver.cpp:106] Iteration 33800, lr = 0.00471875
I0630 01:56:27.241703 29015 solver.cpp:290] Iteration 33900 (48.6774 iter/s, 2.05434s/100 iter), loss = 0
I0630 01:56:27.241725 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:27.241732 29015 sgd_solver.cpp:106] Iteration 33900, lr = 0.00470312
I0630 01:56:29.281149 29015 solver.cpp:354] Sparsity after update:
I0630 01:56:29.282420 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:56:29.282428 29015 net.cpp:1851] conv1a_param_0(0.34) 
I0630 01:56:29.282434 29015 net.cpp:1851] conv1b_param_0(0.68) 
I0630 01:56:29.282436 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:56:29.282438 29015 net.cpp:1851] res2a_branch2a_param_0(0.68) 
I0630 01:56:29.282441 29015 net.cpp:1851] res2a_branch2b_param_0(0.68) 
I0630 01:56:29.282444 29015 net.cpp:1851] res3a_branch2a_param_0(0.68) 
I0630 01:56:29.282445 29015 net.cpp:1851] res3a_branch2b_param_0(0.68) 
I0630 01:56:29.282446 29015 net.cpp:1851] res4a_branch2a_param_0(0.68) 
I0630 01:56:29.282449 29015 net.cpp:1851] res4a_branch2b_param_0(0.68) 
I0630 01:56:29.282450 29015 net.cpp:1851] res5a_branch2a_param_0(0.68) 
I0630 01:56:29.282452 29015 net.cpp:1851] res5a_branch2b_param_0(0.68) 
I0630 01:56:29.282454 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.60043e+06/2.3599e+06) 0.678
I0630 01:56:29.282549 29015 solver.cpp:471] Iteration 34000, Testing net (#0)
I0630 01:56:30.918226 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9101
I0630 01:56:30.918309 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9959
I0630 01:56:30.918318 29015 solver.cpp:544]     Test net output #2: loss = 0.2124 (* 1 = 0.2124 loss)
I0630 01:56:30.938485 29015 solver.cpp:290] Iteration 34000 (27.0515 iter/s, 3.69665s/100 iter), loss = 0
I0630 01:56:30.938540 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:30.938560 29015 sgd_solver.cpp:106] Iteration 34000, lr = 0.0046875
I0630 01:56:30.939313 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.7
I0630 01:56:31.809401 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:56:33.865023 29015 solver.cpp:290] Iteration 34100 (34.1717 iter/s, 2.9264s/100 iter), loss = 0
I0630 01:56:33.865046 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:33.865052 29015 sgd_solver.cpp:106] Iteration 34100, lr = 0.00467187
I0630 01:56:35.924119 29015 solver.cpp:290] Iteration 34200 (48.5671 iter/s, 2.05901s/100 iter), loss = 0
I0630 01:56:35.924141 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:35.924147 29015 sgd_solver.cpp:106] Iteration 34200, lr = 0.00465625
I0630 01:56:37.987541 29015 solver.cpp:290] Iteration 34300 (48.4652 iter/s, 2.06334s/100 iter), loss = 0
I0630 01:56:37.987563 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:37.987571 29015 sgd_solver.cpp:106] Iteration 34300, lr = 0.00464062
I0630 01:56:40.040853 29015 solver.cpp:290] Iteration 34400 (48.7039 iter/s, 2.05322s/100 iter), loss = 0
I0630 01:56:40.040879 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:40.040886 29015 sgd_solver.cpp:106] Iteration 34400, lr = 0.004625
I0630 01:56:42.098229 29015 solver.cpp:290] Iteration 34500 (48.6077 iter/s, 2.05729s/100 iter), loss = 0
I0630 01:56:42.098253 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:42.098263 29015 sgd_solver.cpp:106] Iteration 34500, lr = 0.00460937
I0630 01:56:44.152662 29015 solver.cpp:290] Iteration 34600 (48.6773 iter/s, 2.05434s/100 iter), loss = 0
I0630 01:56:44.152683 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:44.152689 29015 sgd_solver.cpp:106] Iteration 34600, lr = 0.00459375
I0630 01:56:46.207557 29015 solver.cpp:290] Iteration 34700 (48.6663 iter/s, 2.05481s/100 iter), loss = 0
I0630 01:56:46.207579 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:46.207587 29015 sgd_solver.cpp:106] Iteration 34700, lr = 0.00457812
I0630 01:56:48.265427 29015 solver.cpp:290] Iteration 34800 (48.596 iter/s, 2.05778s/100 iter), loss = 0
I0630 01:56:48.265451 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:48.265457 29015 sgd_solver.cpp:106] Iteration 34800, lr = 0.0045625
I0630 01:56:50.326642 29015 solver.cpp:290] Iteration 34900 (48.5172 iter/s, 2.06113s/100 iter), loss = 0
I0630 01:56:50.326664 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:50.326671 29015 sgd_solver.cpp:106] Iteration 34900, lr = 0.00454687
I0630 01:56:52.364058 29015 solver.cpp:354] Sparsity after update:
I0630 01:56:52.365321 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:56:52.365327 29015 net.cpp:1851] conv1a_param_0(0.35) 
I0630 01:56:52.365335 29015 net.cpp:1851] conv1b_param_0(0.7) 
I0630 01:56:52.365339 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:56:52.365341 29015 net.cpp:1851] res2a_branch2a_param_0(0.7) 
I0630 01:56:52.365345 29015 net.cpp:1851] res2a_branch2b_param_0(0.7) 
I0630 01:56:52.365347 29015 net.cpp:1851] res3a_branch2a_param_0(0.7) 
I0630 01:56:52.365350 29015 net.cpp:1851] res3a_branch2b_param_0(0.7) 
I0630 01:56:52.365351 29015 net.cpp:1851] res4a_branch2a_param_0(0.7) 
I0630 01:56:52.365355 29015 net.cpp:1851] res4a_branch2b_param_0(0.7) 
I0630 01:56:52.365356 29015 net.cpp:1851] res5a_branch2a_param_0(0.7) 
I0630 01:56:52.365360 29015 net.cpp:1851] res5a_branch2b_param_0(0.7) 
I0630 01:56:52.365362 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.6475e+06/2.3599e+06) 0.698
I0630 01:56:52.365461 29015 solver.cpp:471] Iteration 35000, Testing net (#0)
I0630 01:56:54.000535 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9089
I0630 01:56:54.000555 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9959
I0630 01:56:54.000560 29015 solver.cpp:544]     Test net output #2: loss = 0.214 (* 1 = 0.214 loss)
I0630 01:56:54.020362 29015 solver.cpp:290] Iteration 35000 (27.074 iter/s, 3.69359s/100 iter), loss = 0
I0630 01:56:54.020390 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:54.020395 29015 sgd_solver.cpp:106] Iteration 35000, lr = 0.00453125
I0630 01:56:54.020926 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.72
I0630 01:56:54.926093 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:56:56.985404 29015 solver.cpp:290] Iteration 35100 (33.7277 iter/s, 2.96493s/100 iter), loss = 0
I0630 01:56:56.985426 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:56.985433 29015 sgd_solver.cpp:106] Iteration 35100, lr = 0.00451563
I0630 01:56:59.039710 29015 solver.cpp:290] Iteration 35200 (48.6803 iter/s, 2.05422s/100 iter), loss = 0
I0630 01:56:59.039732 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:56:59.039739 29015 sgd_solver.cpp:106] Iteration 35200, lr = 0.0045
I0630 01:57:01.100965 29015 solver.cpp:290] Iteration 35300 (48.5162 iter/s, 2.06117s/100 iter), loss = 0
I0630 01:57:01.101035 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:01.101043 29015 sgd_solver.cpp:106] Iteration 35300, lr = 0.00448438
I0630 01:57:03.158298 29015 solver.cpp:290] Iteration 35400 (48.6097 iter/s, 2.0572s/100 iter), loss = 0
I0630 01:57:03.158319 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:03.158327 29015 sgd_solver.cpp:106] Iteration 35400, lr = 0.00446875
I0630 01:57:05.213938 29015 solver.cpp:290] Iteration 35500 (48.6487 iter/s, 2.05555s/100 iter), loss = 0
I0630 01:57:05.213961 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:05.213968 29015 sgd_solver.cpp:106] Iteration 35500, lr = 0.00445312
I0630 01:57:07.267876 29015 solver.cpp:290] Iteration 35600 (48.689 iter/s, 2.05385s/100 iter), loss = 0
I0630 01:57:07.267899 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:07.267905 29015 sgd_solver.cpp:106] Iteration 35600, lr = 0.0044375
I0630 01:57:09.330889 29015 solver.cpp:290] Iteration 35700 (48.4749 iter/s, 2.06292s/100 iter), loss = 0
I0630 01:57:09.330914 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:09.330921 29015 sgd_solver.cpp:106] Iteration 35700, lr = 0.00442187
I0630 01:57:11.385553 29015 solver.cpp:290] Iteration 35800 (48.6719 iter/s, 2.05457s/100 iter), loss = 0
I0630 01:57:11.385579 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:11.385588 29015 sgd_solver.cpp:106] Iteration 35800, lr = 0.00440625
I0630 01:57:13.441962 29015 solver.cpp:290] Iteration 35900 (48.6306 iter/s, 2.05632s/100 iter), loss = 0
I0630 01:57:13.441985 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:13.441992 29015 sgd_solver.cpp:106] Iteration 35900, lr = 0.00439062
I0630 01:57:15.477509 29015 solver.cpp:354] Sparsity after update:
I0630 01:57:15.478770 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:57:15.478775 29015 net.cpp:1851] conv1a_param_0(0.36) 
I0630 01:57:15.478782 29015 net.cpp:1851] conv1b_param_0(0.72) 
I0630 01:57:15.478785 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:57:15.478787 29015 net.cpp:1851] res2a_branch2a_param_0(0.72) 
I0630 01:57:15.478790 29015 net.cpp:1851] res2a_branch2b_param_0(0.72) 
I0630 01:57:15.478791 29015 net.cpp:1851] res3a_branch2a_param_0(0.72) 
I0630 01:57:15.478793 29015 net.cpp:1851] res3a_branch2b_param_0(0.72) 
I0630 01:57:15.478796 29015 net.cpp:1851] res4a_branch2a_param_0(0.72) 
I0630 01:57:15.478797 29015 net.cpp:1851] res4a_branch2b_param_0(0.72) 
I0630 01:57:15.478799 29015 net.cpp:1851] res5a_branch2a_param_0(0.72) 
I0630 01:57:15.478801 29015 net.cpp:1851] res5a_branch2b_param_0(0.72) 
I0630 01:57:15.478803 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.69457e+06/2.3599e+06) 0.718
I0630 01:57:15.478894 29015 solver.cpp:471] Iteration 36000, Testing net (#0)
I0630 01:57:17.115919 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.908
I0630 01:57:17.115938 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9955
I0630 01:57:17.115943 29015 solver.cpp:544]     Test net output #2: loss = 0.2171 (* 1 = 0.2171 loss)
I0630 01:57:17.135876 29015 solver.cpp:290] Iteration 36000 (27.0725 iter/s, 3.69378s/100 iter), loss = 0
I0630 01:57:17.135892 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:17.135905 29015 sgd_solver.cpp:106] Iteration 36000, lr = 0.004375
I0630 01:57:17.136431 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.74
I0630 01:57:18.088382 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:57:20.148715 29015 solver.cpp:290] Iteration 36100 (33.1925 iter/s, 3.01273s/100 iter), loss = 0
I0630 01:57:20.148736 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:20.148743 29015 sgd_solver.cpp:106] Iteration 36100, lr = 0.00435938
I0630 01:57:22.205132 29015 solver.cpp:290] Iteration 36200 (48.6303 iter/s, 2.05633s/100 iter), loss = 0
I0630 01:57:22.205168 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:22.205174 29015 sgd_solver.cpp:106] Iteration 36200, lr = 0.00434375
I0630 01:57:24.261343 29015 solver.cpp:290] Iteration 36300 (48.6355 iter/s, 2.05611s/100 iter), loss = 0
I0630 01:57:24.261375 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:24.261386 29015 sgd_solver.cpp:106] Iteration 36300, lr = 0.00432813
I0630 01:57:26.317399 29015 solver.cpp:290] Iteration 36400 (48.6391 iter/s, 2.05596s/100 iter), loss = 0
I0630 01:57:26.317427 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:26.317436 29015 sgd_solver.cpp:106] Iteration 36400, lr = 0.0043125
I0630 01:57:28.374474 29015 solver.cpp:290] Iteration 36500 (48.6149 iter/s, 2.05698s/100 iter), loss = 0
I0630 01:57:28.374495 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:28.374502 29015 sgd_solver.cpp:106] Iteration 36500, lr = 0.00429688
I0630 01:57:30.427381 29015 solver.cpp:290] Iteration 36600 (48.7135 iter/s, 2.05282s/100 iter), loss = 0
I0630 01:57:30.427402 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:30.427408 29015 sgd_solver.cpp:106] Iteration 36600, lr = 0.00428125
I0630 01:57:32.479621 29015 solver.cpp:290] Iteration 36700 (48.7293 iter/s, 2.05215s/100 iter), loss = 0
I0630 01:57:32.479693 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:32.479701 29015 sgd_solver.cpp:106] Iteration 36700, lr = 0.00426562
I0630 01:57:34.534464 29015 solver.cpp:290] Iteration 36800 (48.6687 iter/s, 2.05471s/100 iter), loss = 0
I0630 01:57:34.534488 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:34.534497 29015 sgd_solver.cpp:106] Iteration 36800, lr = 0.00425
I0630 01:57:36.589046 29015 solver.cpp:290] Iteration 36900 (48.6738 iter/s, 2.0545s/100 iter), loss = 0
I0630 01:57:36.589071 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:36.589079 29015 sgd_solver.cpp:106] Iteration 36900, lr = 0.00423437
I0630 01:57:38.628594 29015 solver.cpp:354] Sparsity after update:
I0630 01:57:38.629873 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:57:38.629880 29015 net.cpp:1851] conv1a_param_0(0.37) 
I0630 01:57:38.629889 29015 net.cpp:1851] conv1b_param_0(0.74) 
I0630 01:57:38.629891 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:57:38.629894 29015 net.cpp:1851] res2a_branch2a_param_0(0.74) 
I0630 01:57:38.629897 29015 net.cpp:1851] res2a_branch2b_param_0(0.74) 
I0630 01:57:38.629899 29015 net.cpp:1851] res3a_branch2a_param_0(0.74) 
I0630 01:57:38.629901 29015 net.cpp:1851] res3a_branch2b_param_0(0.74) 
I0630 01:57:38.629904 29015 net.cpp:1851] res4a_branch2a_param_0(0.74) 
I0630 01:57:38.629906 29015 net.cpp:1851] res4a_branch2b_param_0(0.74) 
I0630 01:57:38.629909 29015 net.cpp:1851] res5a_branch2a_param_0(0.74) 
I0630 01:57:38.629911 29015 net.cpp:1851] res5a_branch2b_param_0(0.74) 
I0630 01:57:38.629914 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.74164e+06/2.3599e+06) 0.738
I0630 01:57:38.630000 29015 solver.cpp:471] Iteration 37000, Testing net (#0)
I0630 01:57:40.264883 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9056
I0630 01:57:40.264902 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9956
I0630 01:57:40.264907 29015 solver.cpp:544]     Test net output #2: loss = 0.2166 (* 1 = 0.2166 loss)
I0630 01:57:40.284719 29015 solver.cpp:290] Iteration 37000 (27.0596 iter/s, 3.69554s/100 iter), loss = 0
I0630 01:57:40.284736 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:40.284749 29015 sgd_solver.cpp:106] Iteration 37000, lr = 0.00421875
I0630 01:57:40.285287 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.76
I0630 01:57:41.251469 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:57:43.309186 29015 solver.cpp:290] Iteration 37100 (33.0649 iter/s, 3.02436s/100 iter), loss = 0
I0630 01:57:43.309211 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:43.309217 29015 sgd_solver.cpp:106] Iteration 37100, lr = 0.00420313
I0630 01:57:45.365622 29015 solver.cpp:290] Iteration 37200 (48.6299 iter/s, 2.05635s/100 iter), loss = 0
I0630 01:57:45.365643 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:45.365649 29015 sgd_solver.cpp:106] Iteration 37200, lr = 0.0041875
I0630 01:57:47.420632 29015 solver.cpp:290] Iteration 37300 (48.6636 iter/s, 2.05492s/100 iter), loss = 0
I0630 01:57:47.420655 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:47.420662 29015 sgd_solver.cpp:106] Iteration 37300, lr = 0.00417187
I0630 01:57:49.475750 29015 solver.cpp:290] Iteration 37400 (48.6611 iter/s, 2.05503s/100 iter), loss = 0
I0630 01:57:49.475772 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:49.475780 29015 sgd_solver.cpp:106] Iteration 37400, lr = 0.00415625
I0630 01:57:51.528851 29015 solver.cpp:290] Iteration 37500 (48.7089 iter/s, 2.05301s/100 iter), loss = 0
I0630 01:57:51.528872 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:51.528879 29015 sgd_solver.cpp:106] Iteration 37500, lr = 0.00414062
I0630 01:57:53.587291 29015 solver.cpp:290] Iteration 37600 (48.5825 iter/s, 2.05835s/100 iter), loss = 0
I0630 01:57:53.587327 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:53.587334 29015 sgd_solver.cpp:106] Iteration 37600, lr = 0.004125
I0630 01:57:55.643421 29015 solver.cpp:290] Iteration 37700 (48.6374 iter/s, 2.05603s/100 iter), loss = 0
I0630 01:57:55.643445 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:55.643453 29015 sgd_solver.cpp:106] Iteration 37700, lr = 0.00410937
I0630 01:57:57.698813 29015 solver.cpp:290] Iteration 37800 (48.6546 iter/s, 2.0553s/100 iter), loss = 0
I0630 01:57:57.698843 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:57.698853 29015 sgd_solver.cpp:106] Iteration 37800, lr = 0.00409375
I0630 01:57:59.757151 29015 solver.cpp:290] Iteration 37900 (48.5851 iter/s, 2.05825s/100 iter), loss = 0
I0630 01:57:59.757174 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:57:59.757182 29015 sgd_solver.cpp:106] Iteration 37900, lr = 0.00407812
I0630 01:58:01.793303 29015 solver.cpp:354] Sparsity after update:
I0630 01:58:01.794575 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:58:01.794582 29015 net.cpp:1851] conv1a_param_0(0.38) 
I0630 01:58:01.794589 29015 net.cpp:1851] conv1b_param_0(0.76) 
I0630 01:58:01.794595 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:58:01.794598 29015 net.cpp:1851] res2a_branch2a_param_0(0.76) 
I0630 01:58:01.794603 29015 net.cpp:1851] res2a_branch2b_param_0(0.76) 
I0630 01:58:01.794607 29015 net.cpp:1851] res3a_branch2a_param_0(0.76) 
I0630 01:58:01.794611 29015 net.cpp:1851] res3a_branch2b_param_0(0.76) 
I0630 01:58:01.794615 29015 net.cpp:1851] res4a_branch2a_param_0(0.749) 
I0630 01:58:01.794620 29015 net.cpp:1851] res4a_branch2b_param_0(0.76) 
I0630 01:58:01.794622 29015 net.cpp:1851] res5a_branch2a_param_0(0.76) 
I0630 01:58:01.794626 29015 net.cpp:1851] res5a_branch2b_param_0(0.76) 
I0630 01:58:01.794631 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.78541e+06/2.3599e+06) 0.757
I0630 01:58:01.794721 29015 solver.cpp:471] Iteration 38000, Testing net (#0)
I0630 01:58:03.430794 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9043
I0630 01:58:03.430845 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9954
I0630 01:58:03.430852 29015 solver.cpp:544]     Test net output #2: loss = 0.2219 (* 1 = 0.2219 loss)
I0630 01:58:03.450937 29015 solver.cpp:290] Iteration 38000 (27.0735 iter/s, 3.69365s/100 iter), loss = 0
I0630 01:58:03.450958 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:03.450965 29015 sgd_solver.cpp:106] Iteration 38000, lr = 0.0040625
I0630 01:58:03.451719 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.78
I0630 01:58:04.457129 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:58:06.513922 29015 solver.cpp:290] Iteration 38100 (32.6491 iter/s, 3.06287s/100 iter), loss = 0
I0630 01:58:06.513943 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:06.513952 29015 sgd_solver.cpp:106] Iteration 38100, lr = 0.00404688
I0630 01:58:08.574615 29015 solver.cpp:290] Iteration 38200 (48.5294 iter/s, 2.06061s/100 iter), loss = 0
I0630 01:58:08.574638 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:08.574645 29015 sgd_solver.cpp:106] Iteration 38200, lr = 0.00403125
I0630 01:58:10.630375 29015 solver.cpp:290] Iteration 38300 (48.6459 iter/s, 2.05567s/100 iter), loss = 0
I0630 01:58:10.630398 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:10.630403 29015 sgd_solver.cpp:106] Iteration 38300, lr = 0.00401562
I0630 01:58:12.688112 29015 solver.cpp:290] Iteration 38400 (48.5991 iter/s, 2.05765s/100 iter), loss = 0
I0630 01:58:12.688135 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:12.688143 29015 sgd_solver.cpp:106] Iteration 38400, lr = 0.004
I0630 01:58:14.747319 29015 solver.cpp:290] Iteration 38500 (48.5645 iter/s, 2.05912s/100 iter), loss = 0
I0630 01:58:14.747341 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:14.747347 29015 sgd_solver.cpp:106] Iteration 38500, lr = 0.00398437
I0630 01:58:16.802923 29015 solver.cpp:290] Iteration 38600 (48.6496 iter/s, 2.05552s/100 iter), loss = 0
I0630 01:58:16.802947 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:16.802953 29015 sgd_solver.cpp:106] Iteration 38600, lr = 0.00396875
I0630 01:58:18.861778 29015 solver.cpp:290] Iteration 38700 (48.5728 iter/s, 2.05876s/100 iter), loss = 0
I0630 01:58:18.861799 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:18.861806 29015 sgd_solver.cpp:106] Iteration 38700, lr = 0.00395312
I0630 01:58:20.916635 29015 solver.cpp:290] Iteration 38800 (48.6672 iter/s, 2.05477s/100 iter), loss = 0
I0630 01:58:20.916657 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:20.916664 29015 sgd_solver.cpp:106] Iteration 38800, lr = 0.0039375
I0630 01:58:22.979776 29015 solver.cpp:290] Iteration 38900 (48.4718 iter/s, 2.06305s/100 iter), loss = 0
I0630 01:58:22.979799 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:22.979805 29015 sgd_solver.cpp:106] Iteration 38900, lr = 0.00392187
I0630 01:58:25.013273 29015 solver.cpp:354] Sparsity after update:
I0630 01:58:25.014554 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:58:25.014559 29015 net.cpp:1851] conv1a_param_0(0.39) 
I0630 01:58:25.014566 29015 net.cpp:1851] conv1b_param_0(0.78) 
I0630 01:58:25.014569 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:58:25.014571 29015 net.cpp:1851] res2a_branch2a_param_0(0.78) 
I0630 01:58:25.014574 29015 net.cpp:1851] res2a_branch2b_param_0(0.773) 
I0630 01:58:25.014575 29015 net.cpp:1851] res3a_branch2a_param_0(0.78) 
I0630 01:58:25.014577 29015 net.cpp:1851] res3a_branch2b_param_0(0.78) 
I0630 01:58:25.014580 29015 net.cpp:1851] res4a_branch2a_param_0(0.758) 
I0630 01:58:25.014581 29015 net.cpp:1851] res4a_branch2b_param_0(0.78) 
I0630 01:58:25.014583 29015 net.cpp:1851] res5a_branch2a_param_0(0.78) 
I0630 01:58:25.014585 29015 net.cpp:1851] res5a_branch2b_param_0(0.78) 
I0630 01:58:25.014605 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.82918e+06/2.3599e+06) 0.775
I0630 01:58:25.014693 29015 solver.cpp:471] Iteration 39000, Testing net (#0)
I0630 01:58:26.651511 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.8996
I0630 01:58:26.651531 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9947
I0630 01:58:26.651536 29015 solver.cpp:544]     Test net output #2: loss = 0.2316 (* 1 = 0.2316 loss)
I0630 01:58:26.671046 29015 solver.cpp:290] Iteration 39000 (27.0919 iter/s, 3.69114s/100 iter), loss = 0
I0630 01:58:26.671061 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:26.671073 29015 sgd_solver.cpp:106] Iteration 39000, lr = 0.00390625
I0630 01:58:26.671614 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.8
I0630 01:58:27.700516 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:58:29.755210 29015 solver.cpp:290] Iteration 39100 (32.4248 iter/s, 3.08406s/100 iter), loss = 0
I0630 01:58:29.755234 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:29.755239 29015 sgd_solver.cpp:106] Iteration 39100, lr = 0.00389063
I0630 01:58:31.810211 29015 solver.cpp:290] Iteration 39200 (48.6638 iter/s, 2.05491s/100 iter), loss = 0
I0630 01:58:31.810232 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:31.810240 29015 sgd_solver.cpp:106] Iteration 39200, lr = 0.003875
I0630 01:58:33.868337 29015 solver.cpp:290] Iteration 39300 (48.5899 iter/s, 2.05804s/100 iter), loss = 0
I0630 01:58:33.868407 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:33.868414 29015 sgd_solver.cpp:106] Iteration 39300, lr = 0.00385938
I0630 01:58:35.921938 29015 solver.cpp:290] Iteration 39400 (48.6981 iter/s, 2.05347s/100 iter), loss = 0
I0630 01:58:35.921962 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:35.921970 29015 sgd_solver.cpp:106] Iteration 39400, lr = 0.00384375
I0630 01:58:37.976224 29015 solver.cpp:290] Iteration 39500 (48.6808 iter/s, 2.0542s/100 iter), loss = 0
I0630 01:58:37.976248 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:37.976253 29015 sgd_solver.cpp:106] Iteration 39500, lr = 0.00382812
I0630 01:58:40.035140 29015 solver.cpp:290] Iteration 39600 (48.5714 iter/s, 2.05883s/100 iter), loss = 0
I0630 01:58:40.035161 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:40.035168 29015 sgd_solver.cpp:106] Iteration 39600, lr = 0.0038125
I0630 01:58:42.091280 29015 solver.cpp:290] Iteration 39700 (48.6369 iter/s, 2.05605s/100 iter), loss = 0
I0630 01:58:42.091305 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:42.091315 29015 sgd_solver.cpp:106] Iteration 39700, lr = 0.00379687
I0630 01:58:44.147649 29015 solver.cpp:290] Iteration 39800 (48.6315 iter/s, 2.05628s/100 iter), loss = 0
I0630 01:58:44.147670 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:44.147676 29015 sgd_solver.cpp:106] Iteration 39800, lr = 0.00378125
I0630 01:58:46.201027 29015 solver.cpp:290] Iteration 39900 (48.7023 iter/s, 2.05329s/100 iter), loss = 0
I0630 01:58:46.201050 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:46.201056 29015 sgd_solver.cpp:106] Iteration 39900, lr = 0.00376562
I0630 01:58:48.235278 29015 solver.cpp:598] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_40000.caffemodel
I0630 01:58:48.252578 29015 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_40000.solverstate
I0630 01:58:48.259923 29015 solver.cpp:354] Sparsity after update:
I0630 01:58:48.260892 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:58:48.260900 29015 net.cpp:1851] conv1a_param_0(0.4) 
I0630 01:58:48.260908 29015 net.cpp:1851] conv1b_param_0(0.8) 
I0630 01:58:48.260910 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:58:48.260912 29015 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 01:58:48.260915 29015 net.cpp:1851] res2a_branch2b_param_0(0.777) 
I0630 01:58:48.260916 29015 net.cpp:1851] res3a_branch2a_param_0(0.795) 
I0630 01:58:48.260918 29015 net.cpp:1851] res3a_branch2b_param_0(0.792) 
I0630 01:58:48.260921 29015 net.cpp:1851] res4a_branch2a_param_0(0.769) 
I0630 01:58:48.260922 29015 net.cpp:1851] res4a_branch2b_param_0(0.797) 
I0630 01:58:48.260924 29015 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 01:58:48.260926 29015 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 01:58:48.260928 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.87241e+06/2.3599e+06) 0.793
I0630 01:58:48.261028 29015 solver.cpp:471] Iteration 40000, Testing net (#0)
I0630 01:58:49.896380 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9006
I0630 01:58:49.896399 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9953
I0630 01:58:49.896404 29015 solver.cpp:544]     Test net output #2: loss = 0.2178 (* 1 = 0.2178 loss)
I0630 01:58:49.916481 29015 solver.cpp:290] Iteration 40000 (26.9156 iter/s, 3.71532s/100 iter), loss = 0
I0630 01:58:49.916499 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:49.916509 29015 sgd_solver.cpp:106] Iteration 40000, lr = 0.00375
I0630 01:58:49.917026 29015 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.82
I0630 01:58:51.013628 29015 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 01:58:53.070605 29015 solver.cpp:290] Iteration 40100 (31.7057 iter/s, 3.15401s/100 iter), loss = 0
I0630 01:58:53.070627 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:53.070633 29015 sgd_solver.cpp:106] Iteration 40100, lr = 0.00373438
I0630 01:58:55.128373 29015 solver.cpp:290] Iteration 40200 (48.5984 iter/s, 2.05768s/100 iter), loss = 0
I0630 01:58:55.128398 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:55.128407 29015 sgd_solver.cpp:106] Iteration 40200, lr = 0.00371875
I0630 01:58:57.183152 29015 solver.cpp:290] Iteration 40300 (48.6691 iter/s, 2.05469s/100 iter), loss = 0
I0630 01:58:57.183174 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:58:57.183181 29015 sgd_solver.cpp:106] Iteration 40300, lr = 0.00370313
I0630 01:58:59.235651 29015 solver.cpp:290] Iteration 40400 (48.7231 iter/s, 2.05241s/100 iter), loss = 0.0952381
I0630 01:58:59.235674 29015 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0630 01:58:59.235680 29015 sgd_solver.cpp:106] Iteration 40400, lr = 0.0036875
I0630 01:59:01.291451 29015 solver.cpp:290] Iteration 40500 (48.6449 iter/s, 2.05571s/100 iter), loss = 0
I0630 01:59:01.291472 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:01.291481 29015 sgd_solver.cpp:106] Iteration 40500, lr = 0.00367187
I0630 01:59:03.346729 29015 solver.cpp:290] Iteration 40600 (48.6572 iter/s, 2.05519s/100 iter), loss = 0.047619
I0630 01:59:03.346752 29015 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 01:59:03.346760 29015 sgd_solver.cpp:106] Iteration 40600, lr = 0.00365625
I0630 01:59:05.399518 29015 solver.cpp:290] Iteration 40700 (48.7163 iter/s, 2.0527s/100 iter), loss = 0
I0630 01:59:05.399590 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:05.399597 29015 sgd_solver.cpp:106] Iteration 40700, lr = 0.00364062
I0630 01:59:07.456954 29015 solver.cpp:290] Iteration 40800 (48.6074 iter/s, 2.0573s/100 iter), loss = 0
I0630 01:59:07.456975 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:07.456982 29015 sgd_solver.cpp:106] Iteration 40800, lr = 0.003625
I0630 01:59:09.531613 29015 solver.cpp:290] Iteration 40900 (48.2027 iter/s, 2.07457s/100 iter), loss = 0
I0630 01:59:09.531635 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:09.531642 29015 sgd_solver.cpp:106] Iteration 40900, lr = 0.00360937
I0630 01:59:11.576593 29015 solver.cpp:354] Sparsity after update:
I0630 01:59:11.577878 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:59:11.577884 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 01:59:11.577891 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 01:59:11.577894 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:59:11.577896 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 01:59:11.577898 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 01:59:11.577900 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 01:59:11.577903 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 01:59:11.577904 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 01:59:11.577906 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 01:59:11.577908 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 01:59:11.577910 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 01:59:11.577913 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 01:59:11.578001 29015 solver.cpp:471] Iteration 41000, Testing net (#0)
I0630 01:59:13.216274 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9001
I0630 01:59:13.216295 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9954
I0630 01:59:13.216300 29015 solver.cpp:544]     Test net output #2: loss = 0.223 (* 1 = 0.223 loss)
I0630 01:59:13.235980 29015 solver.cpp:290] Iteration 41000 (26.9961 iter/s, 3.70424s/100 iter), loss = 0
I0630 01:59:13.235996 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:13.236008 29015 sgd_solver.cpp:106] Iteration 41000, lr = 0.00359375
I0630 01:59:15.293154 29015 solver.cpp:290] Iteration 41100 (48.6123 iter/s, 2.05709s/100 iter), loss = 0
I0630 01:59:15.293175 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:15.293182 29015 sgd_solver.cpp:106] Iteration 41100, lr = 0.00357813
I0630 01:59:17.353003 29015 solver.cpp:290] Iteration 41200 (48.5493 iter/s, 2.05976s/100 iter), loss = 0
I0630 01:59:17.353024 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:17.353031 29015 sgd_solver.cpp:106] Iteration 41200, lr = 0.0035625
I0630 01:59:19.412956 29015 solver.cpp:290] Iteration 41300 (48.5468 iter/s, 2.05987s/100 iter), loss = 0
I0630 01:59:19.412978 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:19.412986 29015 sgd_solver.cpp:106] Iteration 41300, lr = 0.00354687
I0630 01:59:21.467670 29015 solver.cpp:290] Iteration 41400 (48.6707 iter/s, 2.05463s/100 iter), loss = 0
I0630 01:59:21.467697 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:21.467706 29015 sgd_solver.cpp:106] Iteration 41400, lr = 0.00353125
I0630 01:59:23.528538 29015 solver.cpp:290] Iteration 41500 (48.5254 iter/s, 2.06078s/100 iter), loss = 0
I0630 01:59:23.528570 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:23.528581 29015 sgd_solver.cpp:106] Iteration 41500, lr = 0.00351562
I0630 01:59:25.585319 29015 solver.cpp:290] Iteration 41600 (48.6219 iter/s, 2.05669s/100 iter), loss = 0
I0630 01:59:25.585342 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:25.585348 29015 sgd_solver.cpp:106] Iteration 41600, lr = 0.0035
I0630 01:59:27.643427 29015 solver.cpp:290] Iteration 41700 (48.5904 iter/s, 2.05802s/100 iter), loss = 0
I0630 01:59:27.643466 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:27.643473 29015 sgd_solver.cpp:106] Iteration 41700, lr = 0.00348437
I0630 01:59:29.698385 29015 solver.cpp:290] Iteration 41800 (48.6652 iter/s, 2.05486s/100 iter), loss = 0
I0630 01:59:29.698407 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:29.698415 29015 sgd_solver.cpp:106] Iteration 41800, lr = 0.00346875
I0630 01:59:31.751552 29015 solver.cpp:290] Iteration 41900 (48.7073 iter/s, 2.05308s/100 iter), loss = 0
I0630 01:59:31.751575 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:31.751581 29015 sgd_solver.cpp:106] Iteration 41900, lr = 0.00345312
I0630 01:59:33.791646 29015 solver.cpp:354] Sparsity after update:
I0630 01:59:33.792919 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:59:33.792927 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 01:59:33.792934 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 01:59:33.792937 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:59:33.792939 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 01:59:33.792943 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 01:59:33.792944 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 01:59:33.792946 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 01:59:33.792948 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 01:59:33.792951 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 01:59:33.792953 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 01:59:33.792955 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 01:59:33.792958 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 01:59:33.793045 29015 solver.cpp:471] Iteration 42000, Testing net (#0)
I0630 01:59:35.432191 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9018
I0630 01:59:35.433840 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9951
I0630 01:59:35.433848 29015 solver.cpp:544]     Test net output #2: loss = 0.2235 (* 1 = 0.2235 loss)
I0630 01:59:35.453702 29015 solver.cpp:290] Iteration 42000 (27.0123 iter/s, 3.70202s/100 iter), loss = 0
I0630 01:59:35.453722 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:35.453729 29015 sgd_solver.cpp:106] Iteration 42000, lr = 0.0034375
I0630 01:59:37.506883 29015 solver.cpp:290] Iteration 42100 (48.7069 iter/s, 2.0531s/100 iter), loss = 0
I0630 01:59:37.506906 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:37.506911 29015 sgd_solver.cpp:106] Iteration 42100, lr = 0.00342188
I0630 01:59:39.559950 29015 solver.cpp:290] Iteration 42200 (48.7097 iter/s, 2.05298s/100 iter), loss = 0
I0630 01:59:39.559972 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:39.559979 29015 sgd_solver.cpp:106] Iteration 42200, lr = 0.00340625
I0630 01:59:41.618245 29015 solver.cpp:290] Iteration 42300 (48.5859 iter/s, 2.05821s/100 iter), loss = 0
I0630 01:59:41.618268 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:41.618274 29015 sgd_solver.cpp:106] Iteration 42300, lr = 0.00339063
I0630 01:59:43.677448 29015 solver.cpp:290] Iteration 42400 (48.5645 iter/s, 2.05912s/100 iter), loss = 0
I0630 01:59:43.677469 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:43.677476 29015 sgd_solver.cpp:106] Iteration 42400, lr = 0.003375
I0630 01:59:45.734961 29015 solver.cpp:290] Iteration 42500 (48.6044 iter/s, 2.05743s/100 iter), loss = 0
I0630 01:59:45.734983 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:45.734992 29015 sgd_solver.cpp:106] Iteration 42500, lr = 0.00335937
I0630 01:59:47.789335 29015 solver.cpp:290] Iteration 42600 (48.6787 iter/s, 2.05429s/100 iter), loss = 0
I0630 01:59:47.789357 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:47.789363 29015 sgd_solver.cpp:106] Iteration 42600, lr = 0.00334375
I0630 01:59:49.849511 29015 solver.cpp:290] Iteration 42700 (48.5416 iter/s, 2.06009s/100 iter), loss = 0
I0630 01:59:49.849532 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:49.849540 29015 sgd_solver.cpp:106] Iteration 42700, lr = 0.00332812
I0630 01:59:51.907040 29015 solver.cpp:290] Iteration 42800 (48.604 iter/s, 2.05744s/100 iter), loss = 0
I0630 01:59:51.907063 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:51.907069 29015 sgd_solver.cpp:106] Iteration 42800, lr = 0.0033125
I0630 01:59:53.971001 29015 solver.cpp:290] Iteration 42900 (48.4526 iter/s, 2.06387s/100 iter), loss = 0
I0630 01:59:53.971024 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:53.971031 29015 sgd_solver.cpp:106] Iteration 42900, lr = 0.00329687
I0630 01:59:56.008731 29015 solver.cpp:354] Sparsity after update:
I0630 01:59:56.010001 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 01:59:56.010009 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 01:59:56.010018 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 01:59:56.010025 29015 net.cpp:1851] fc10_param_0(0) 
I0630 01:59:56.010028 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 01:59:56.010032 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 01:59:56.010037 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 01:59:56.010040 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 01:59:56.010044 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 01:59:56.010048 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 01:59:56.010052 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 01:59:56.010056 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 01:59:56.010061 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 01:59:56.010150 29015 solver.cpp:471] Iteration 43000, Testing net (#0)
I0630 01:59:57.646554 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9035
I0630 01:59:57.646574 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9954
I0630 01:59:57.646579 29015 solver.cpp:544]     Test net output #2: loss = 0.2197 (* 1 = 0.2197 loss)
I0630 01:59:57.666486 29015 solver.cpp:290] Iteration 43000 (27.061 iter/s, 3.69535s/100 iter), loss = 0
I0630 01:59:57.666509 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:57.666515 29015 sgd_solver.cpp:106] Iteration 43000, lr = 0.00328125
I0630 01:59:59.725824 29015 solver.cpp:290] Iteration 43100 (48.5614 iter/s, 2.05925s/100 iter), loss = 0
I0630 01:59:59.725849 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 01:59:59.725857 29015 sgd_solver.cpp:106] Iteration 43100, lr = 0.00326563
I0630 02:00:01.781293 29015 solver.cpp:290] Iteration 43200 (48.6528 iter/s, 2.05538s/100 iter), loss = 0.047619
I0630 02:00:01.781316 29015 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 02:00:01.781322 29015 sgd_solver.cpp:106] Iteration 43200, lr = 0.00325
I0630 02:00:03.834702 29015 solver.cpp:290] Iteration 43300 (48.7016 iter/s, 2.05332s/100 iter), loss = 0
I0630 02:00:03.834724 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:03.834731 29015 sgd_solver.cpp:106] Iteration 43300, lr = 0.00323438
I0630 02:00:05.894106 29015 solver.cpp:290] Iteration 43400 (48.5598 iter/s, 2.05932s/100 iter), loss = 0
I0630 02:00:05.894156 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:05.894165 29015 sgd_solver.cpp:106] Iteration 43400, lr = 0.00321875
I0630 02:00:07.950390 29015 solver.cpp:290] Iteration 43500 (48.6341 iter/s, 2.05617s/100 iter), loss = 0
I0630 02:00:07.950413 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:07.950420 29015 sgd_solver.cpp:106] Iteration 43500, lr = 0.00320312
I0630 02:00:10.054548 29015 solver.cpp:290] Iteration 43600 (47.527 iter/s, 2.10407s/100 iter), loss = 0
I0630 02:00:10.054571 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:10.054577 29015 sgd_solver.cpp:106] Iteration 43600, lr = 0.0031875
I0630 02:00:12.110186 29015 solver.cpp:290] Iteration 43700 (48.6488 iter/s, 2.05555s/100 iter), loss = 0
I0630 02:00:12.110208 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:12.110218 29015 sgd_solver.cpp:106] Iteration 43700, lr = 0.00317187
I0630 02:00:14.167547 29015 solver.cpp:290] Iteration 43800 (48.608 iter/s, 2.05727s/100 iter), loss = 0
I0630 02:00:14.167570 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:14.167577 29015 sgd_solver.cpp:106] Iteration 43800, lr = 0.00315625
I0630 02:00:16.226222 29015 solver.cpp:290] Iteration 43900 (48.577 iter/s, 2.05859s/100 iter), loss = 0
I0630 02:00:16.226243 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:16.226250 29015 sgd_solver.cpp:106] Iteration 43900, lr = 0.00314062
I0630 02:00:18.263720 29015 solver.cpp:354] Sparsity after update:
I0630 02:00:18.264983 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:00:18.264991 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:00:18.264998 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:00:18.265002 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:00:18.265004 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:00:18.265007 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:00:18.265010 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:00:18.265012 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:00:18.265015 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:00:18.265018 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:00:18.265020 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:00:18.265023 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:00:18.265027 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:00:18.265117 29015 solver.cpp:471] Iteration 44000, Testing net (#0)
I0630 02:00:19.900705 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9064
I0630 02:00:19.900723 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.996
I0630 02:00:19.900729 29015 solver.cpp:544]     Test net output #2: loss = 0.2205 (* 1 = 0.2205 loss)
I0630 02:00:19.921542 29015 solver.cpp:290] Iteration 44000 (27.0622 iter/s, 3.69519s/100 iter), loss = 0
I0630 02:00:19.921560 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:19.921571 29015 sgd_solver.cpp:106] Iteration 44000, lr = 0.003125
I0630 02:00:21.975893 29015 solver.cpp:290] Iteration 44100 (48.6791 iter/s, 2.05427s/100 iter), loss = 0.047619
I0630 02:00:21.975914 29015 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 02:00:21.975922 29015 sgd_solver.cpp:106] Iteration 44100, lr = 0.00310938
I0630 02:00:24.036231 29015 solver.cpp:290] Iteration 44200 (48.5377 iter/s, 2.06025s/100 iter), loss = 0
I0630 02:00:24.036257 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:24.036265 29015 sgd_solver.cpp:106] Iteration 44200, lr = 0.00309375
I0630 02:00:26.095580 29015 solver.cpp:290] Iteration 44300 (48.5612 iter/s, 2.05926s/100 iter), loss = 0
I0630 02:00:26.095603 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:26.095613 29015 sgd_solver.cpp:106] Iteration 44300, lr = 0.00307812
I0630 02:00:28.154667 29015 solver.cpp:290] Iteration 44400 (48.5673 iter/s, 2.059s/100 iter), loss = 0
I0630 02:00:28.154690 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:28.154697 29015 sgd_solver.cpp:106] Iteration 44400, lr = 0.0030625
I0630 02:00:30.211853 29015 solver.cpp:290] Iteration 44500 (48.6122 iter/s, 2.0571s/100 iter), loss = 0
I0630 02:00:30.211876 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:30.211885 29015 sgd_solver.cpp:106] Iteration 44500, lr = 0.00304687
I0630 02:00:32.268239 29015 solver.cpp:290] Iteration 44600 (48.6311 iter/s, 2.0563s/100 iter), loss = 0
I0630 02:00:32.268261 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:32.268267 29015 sgd_solver.cpp:106] Iteration 44600, lr = 0.00303125
I0630 02:00:34.327764 29015 solver.cpp:290] Iteration 44700 (48.557 iter/s, 2.05944s/100 iter), loss = 0
I0630 02:00:34.327785 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:34.327791 29015 sgd_solver.cpp:106] Iteration 44700, lr = 0.00301562
I0630 02:00:36.384623 29015 solver.cpp:290] Iteration 44800 (48.6199 iter/s, 2.05677s/100 iter), loss = 0
I0630 02:00:36.384678 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:36.384688 29015 sgd_solver.cpp:106] Iteration 44800, lr = 0.003
I0630 02:00:38.441824 29015 solver.cpp:290] Iteration 44900 (48.6125 iter/s, 2.05708s/100 iter), loss = 0
I0630 02:00:38.441848 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:38.441854 29015 sgd_solver.cpp:106] Iteration 44900, lr = 0.00298437
I0630 02:00:40.476889 29015 solver.cpp:354] Sparsity after update:
I0630 02:00:40.478153 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:00:40.478160 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:00:40.478170 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:00:40.478174 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:00:40.478179 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:00:40.478184 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:00:40.478189 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:00:40.478194 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:00:40.478199 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:00:40.478204 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:00:40.478209 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:00:40.478214 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:00:40.478219 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:00:40.478312 29015 solver.cpp:471] Iteration 45000, Testing net (#0)
I0630 02:00:42.113857 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9044
I0630 02:00:42.113875 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.996
I0630 02:00:42.113880 29015 solver.cpp:544]     Test net output #2: loss = 0.22 (* 1 = 0.22 loss)
I0630 02:00:42.134026 29015 solver.cpp:290] Iteration 45000 (27.0851 iter/s, 3.69207s/100 iter), loss = 0
I0630 02:00:42.134043 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:42.134054 29015 sgd_solver.cpp:106] Iteration 45000, lr = 0.00296875
I0630 02:00:44.194556 29015 solver.cpp:290] Iteration 45100 (48.5331 iter/s, 2.06045s/100 iter), loss = 0
I0630 02:00:44.194579 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:44.194586 29015 sgd_solver.cpp:106] Iteration 45100, lr = 0.00295313
I0630 02:00:46.250414 29015 solver.cpp:290] Iteration 45200 (48.6435 iter/s, 2.05577s/100 iter), loss = 0
I0630 02:00:46.250437 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:46.250443 29015 sgd_solver.cpp:106] Iteration 45200, lr = 0.0029375
I0630 02:00:48.305409 29015 solver.cpp:290] Iteration 45300 (48.664 iter/s, 2.05491s/100 iter), loss = 0
I0630 02:00:48.305430 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:48.305438 29015 sgd_solver.cpp:106] Iteration 45300, lr = 0.00292188
I0630 02:00:50.361637 29015 solver.cpp:290] Iteration 45400 (48.6348 iter/s, 2.05614s/100 iter), loss = 0
I0630 02:00:50.361660 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:50.361668 29015 sgd_solver.cpp:106] Iteration 45400, lr = 0.00290625
I0630 02:00:52.415776 29015 solver.cpp:290] Iteration 45500 (48.6843 iter/s, 2.05405s/100 iter), loss = 0
I0630 02:00:52.415798 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:52.415805 29015 sgd_solver.cpp:106] Iteration 45500, lr = 0.00289063
I0630 02:00:54.474714 29015 solver.cpp:290] Iteration 45600 (48.5708 iter/s, 2.05885s/100 iter), loss = 0
I0630 02:00:54.474740 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:54.474750 29015 sgd_solver.cpp:106] Iteration 45600, lr = 0.002875
I0630 02:00:56.530061 29015 solver.cpp:290] Iteration 45700 (48.6558 iter/s, 2.05526s/100 iter), loss = 0
I0630 02:00:56.530082 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:56.530088 29015 sgd_solver.cpp:106] Iteration 45700, lr = 0.00285937
I0630 02:00:58.581972 29015 solver.cpp:290] Iteration 45800 (48.7371 iter/s, 2.05182s/100 iter), loss = 0
I0630 02:00:58.582010 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:00:58.582017 29015 sgd_solver.cpp:106] Iteration 45800, lr = 0.00284375
I0630 02:01:00.638586 29015 solver.cpp:290] Iteration 45900 (48.626 iter/s, 2.05651s/100 iter), loss = 0
I0630 02:01:00.638609 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:00.638617 29015 sgd_solver.cpp:106] Iteration 45900, lr = 0.00282812
I0630 02:01:02.679064 29015 solver.cpp:354] Sparsity after update:
I0630 02:01:02.680521 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:01:02.680529 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:01:02.680538 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:01:02.680541 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:01:02.680546 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:01:02.680550 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:01:02.680555 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:01:02.680559 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:01:02.680564 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:01:02.680567 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:01:02.680572 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:01:02.680575 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:01:02.680579 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:01:02.680718 29015 solver.cpp:471] Iteration 46000, Testing net (#0)
I0630 02:01:04.316561 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9042
I0630 02:01:04.316581 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.996
I0630 02:01:04.316586 29015 solver.cpp:544]     Test net output #2: loss = 0.2223 (* 1 = 0.2223 loss)
I0630 02:01:04.336379 29015 solver.cpp:290] Iteration 46000 (27.0441 iter/s, 3.69766s/100 iter), loss = 0
I0630 02:01:04.336396 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:04.336408 29015 sgd_solver.cpp:106] Iteration 46000, lr = 0.0028125
I0630 02:01:06.394995 29015 solver.cpp:290] Iteration 46100 (48.5783 iter/s, 2.05853s/100 iter), loss = 0
I0630 02:01:06.395077 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:06.395089 29015 sgd_solver.cpp:106] Iteration 46100, lr = 0.00279688
I0630 02:01:08.456874 29015 solver.cpp:290] Iteration 46200 (48.5028 iter/s, 2.06174s/100 iter), loss = 0
I0630 02:01:08.456897 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:08.456903 29015 sgd_solver.cpp:106] Iteration 46200, lr = 0.00278125
I0630 02:01:10.510331 29015 solver.cpp:290] Iteration 46300 (48.7005 iter/s, 2.05337s/100 iter), loss = 0
I0630 02:01:10.510354 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:10.510360 29015 sgd_solver.cpp:106] Iteration 46300, lr = 0.00276563
I0630 02:01:12.574115 29015 solver.cpp:290] Iteration 46400 (48.4567 iter/s, 2.0637s/100 iter), loss = 0
I0630 02:01:12.574137 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:12.574143 29015 sgd_solver.cpp:106] Iteration 46400, lr = 0.00275
I0630 02:01:14.628897 29015 solver.cpp:290] Iteration 46500 (48.669 iter/s, 2.05469s/100 iter), loss = 0
I0630 02:01:14.628927 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:14.628937 29015 sgd_solver.cpp:106] Iteration 46500, lr = 0.00273437
I0630 02:01:16.685899 29015 solver.cpp:290] Iteration 46600 (48.6167 iter/s, 2.05691s/100 iter), loss = 0
I0630 02:01:16.685925 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:16.685931 29015 sgd_solver.cpp:106] Iteration 46600, lr = 0.00271875
I0630 02:01:18.746517 29015 solver.cpp:290] Iteration 46700 (48.5312 iter/s, 2.06053s/100 iter), loss = 0
I0630 02:01:18.746539 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:18.746546 29015 sgd_solver.cpp:106] Iteration 46700, lr = 0.00270312
I0630 02:01:20.803338 29015 solver.cpp:290] Iteration 46800 (48.6208 iter/s, 2.05673s/100 iter), loss = 0
I0630 02:01:20.803359 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:20.803365 29015 sgd_solver.cpp:106] Iteration 46800, lr = 0.0026875
I0630 02:01:22.859953 29015 solver.cpp:290] Iteration 46900 (48.6256 iter/s, 2.05653s/100 iter), loss = 0
I0630 02:01:22.859975 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:22.859982 29015 sgd_solver.cpp:106] Iteration 46900, lr = 0.00267187
I0630 02:01:24.895453 29015 solver.cpp:354] Sparsity after update:
I0630 02:01:24.896728 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:01:24.896735 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:01:24.896745 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:01:24.896750 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:01:24.896755 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:01:24.896760 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:01:24.896764 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:01:24.896770 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:01:24.896775 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:01:24.896780 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:01:24.896785 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:01:24.896790 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:01:24.896795 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:01:24.896886 29015 solver.cpp:471] Iteration 47000, Testing net (#0)
I0630 02:01:26.533449 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9044
I0630 02:01:26.533468 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9961
I0630 02:01:26.533473 29015 solver.cpp:544]     Test net output #2: loss = 0.2204 (* 1 = 0.2204 loss)
I0630 02:01:26.554072 29015 solver.cpp:290] Iteration 47000 (27.071 iter/s, 3.69399s/100 iter), loss = 0
I0630 02:01:26.554090 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:26.554101 29015 sgd_solver.cpp:106] Iteration 47000, lr = 0.00265625
I0630 02:01:28.611484 29015 solver.cpp:290] Iteration 47100 (48.6067 iter/s, 2.05733s/100 iter), loss = 0
I0630 02:01:28.611521 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:28.611529 29015 sgd_solver.cpp:106] Iteration 47100, lr = 0.00264063
I0630 02:01:30.668738 29015 solver.cpp:290] Iteration 47200 (48.6109 iter/s, 2.05715s/100 iter), loss = 0
I0630 02:01:30.668761 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:30.668769 29015 sgd_solver.cpp:106] Iteration 47200, lr = 0.002625
I0630 02:01:32.729290 29015 solver.cpp:290] Iteration 47300 (48.5328 iter/s, 2.06046s/100 iter), loss = 0
I0630 02:01:32.729312 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:32.729322 29015 sgd_solver.cpp:106] Iteration 47300, lr = 0.00260938
I0630 02:01:34.786485 29015 solver.cpp:290] Iteration 47400 (48.6119 iter/s, 2.05711s/100 iter), loss = 0
I0630 02:01:34.786507 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:34.786514 29015 sgd_solver.cpp:106] Iteration 47400, lr = 0.00259375
I0630 02:01:36.843921 29015 solver.cpp:290] Iteration 47500 (48.6063 iter/s, 2.05735s/100 iter), loss = 0
I0630 02:01:36.843978 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:36.843986 29015 sgd_solver.cpp:106] Iteration 47500, lr = 0.00257812
I0630 02:01:38.902601 29015 solver.cpp:290] Iteration 47600 (48.5777 iter/s, 2.05856s/100 iter), loss = 0
I0630 02:01:38.902626 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:38.902636 29015 sgd_solver.cpp:106] Iteration 47600, lr = 0.0025625
I0630 02:01:40.955061 29015 solver.cpp:290] Iteration 47700 (48.7241 iter/s, 2.05237s/100 iter), loss = 0
I0630 02:01:40.955083 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:40.955090 29015 sgd_solver.cpp:106] Iteration 47700, lr = 0.00254687
I0630 02:01:43.008847 29015 solver.cpp:290] Iteration 47800 (48.6926 iter/s, 2.0537s/100 iter), loss = 0
I0630 02:01:43.008870 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:43.008877 29015 sgd_solver.cpp:106] Iteration 47800, lr = 0.00253125
I0630 02:01:45.064370 29015 solver.cpp:290] Iteration 47900 (48.6515 iter/s, 2.05543s/100 iter), loss = 0
I0630 02:01:45.064393 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:45.064399 29015 sgd_solver.cpp:106] Iteration 47900, lr = 0.00251562
I0630 02:01:47.103688 29015 solver.cpp:354] Sparsity after update:
I0630 02:01:47.104965 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:01:47.104974 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:01:47.104980 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:01:47.104984 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:01:47.104985 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:01:47.104987 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:01:47.104990 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:01:47.104991 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:01:47.104993 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:01:47.104995 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:01:47.104997 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:01:47.105000 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:01:47.105001 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:01:47.105093 29015 solver.cpp:471] Iteration 48000, Testing net (#0)
I0630 02:01:48.742105 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9056
I0630 02:01:48.742123 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9956
I0630 02:01:48.742130 29015 solver.cpp:544]     Test net output #2: loss = 0.224 (* 1 = 0.224 loss)
I0630 02:01:48.761814 29015 solver.cpp:290] Iteration 48000 (27.0467 iter/s, 3.69731s/100 iter), loss = 0
I0630 02:01:48.761833 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:48.761842 29015 sgd_solver.cpp:106] Iteration 48000, lr = 0.0025
I0630 02:01:50.816542 29015 solver.cpp:290] Iteration 48100 (48.6703 iter/s, 2.05464s/100 iter), loss = 0
I0630 02:01:50.816568 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:50.816577 29015 sgd_solver.cpp:106] Iteration 48100, lr = 0.00248438
I0630 02:01:52.872143 29015 solver.cpp:290] Iteration 48200 (48.6498 iter/s, 2.05551s/100 iter), loss = 0
I0630 02:01:52.872169 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:52.872177 29015 sgd_solver.cpp:106] Iteration 48200, lr = 0.00246875
I0630 02:01:54.928942 29015 solver.cpp:290] Iteration 48300 (48.6213 iter/s, 2.05671s/100 iter), loss = 0
I0630 02:01:54.928964 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:54.928972 29015 sgd_solver.cpp:106] Iteration 48300, lr = 0.00245313
I0630 02:01:56.989125 29015 solver.cpp:290] Iteration 48400 (48.5414 iter/s, 2.0601s/100 iter), loss = 0
I0630 02:01:56.989146 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:56.989153 29015 sgd_solver.cpp:106] Iteration 48400, lr = 0.0024375
I0630 02:01:59.042384 29015 solver.cpp:290] Iteration 48500 (48.7051 iter/s, 2.05317s/100 iter), loss = 0
I0630 02:01:59.042421 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:01:59.042428 29015 sgd_solver.cpp:106] Iteration 48500, lr = 0.00242188
I0630 02:02:01.098153 29015 solver.cpp:290] Iteration 48600 (48.646 iter/s, 2.05567s/100 iter), loss = 0
I0630 02:02:01.098175 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:01.098181 29015 sgd_solver.cpp:106] Iteration 48600, lr = 0.00240625
I0630 02:02:03.152230 29015 solver.cpp:290] Iteration 48700 (48.6857 iter/s, 2.05399s/100 iter), loss = 0
I0630 02:02:03.152251 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:03.152258 29015 sgd_solver.cpp:106] Iteration 48700, lr = 0.00239062
I0630 02:02:05.211640 29015 solver.cpp:290] Iteration 48800 (48.5596 iter/s, 2.05932s/100 iter), loss = 0
I0630 02:02:05.211663 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:05.211669 29015 sgd_solver.cpp:106] Iteration 48800, lr = 0.002375
I0630 02:02:07.268162 29015 solver.cpp:290] Iteration 48900 (48.6279 iter/s, 2.05643s/100 iter), loss = 0
I0630 02:02:07.268230 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:07.268239 29015 sgd_solver.cpp:106] Iteration 48900, lr = 0.00235937
I0630 02:02:09.320024 29015 solver.cpp:354] Sparsity after update:
I0630 02:02:09.321174 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:02:09.321182 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:02:09.321189 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:02:09.321192 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:02:09.321194 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:02:09.321197 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:02:09.321198 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:02:09.321200 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:02:09.321202 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:02:09.321204 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:02:09.321207 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:02:09.321208 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:02:09.321210 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:02:09.321297 29015 solver.cpp:471] Iteration 49000, Testing net (#0)
I0630 02:02:10.956575 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9037
I0630 02:02:10.956594 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9954
I0630 02:02:10.956600 29015 solver.cpp:544]     Test net output #2: loss = 0.2248 (* 1 = 0.2248 loss)
I0630 02:02:10.976692 29015 solver.cpp:290] Iteration 49000 (26.9661 iter/s, 3.70835s/100 iter), loss = 0
I0630 02:02:10.976712 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:10.976722 29015 sgd_solver.cpp:106] Iteration 49000, lr = 0.00234375
I0630 02:02:13.036352 29015 solver.cpp:290] Iteration 49100 (48.5537 iter/s, 2.05958s/100 iter), loss = 0
I0630 02:02:13.036375 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:13.036381 29015 sgd_solver.cpp:106] Iteration 49100, lr = 0.00232813
I0630 02:02:15.090219 29015 solver.cpp:290] Iteration 49200 (48.6907 iter/s, 2.05378s/100 iter), loss = 0
I0630 02:02:15.090240 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:15.090247 29015 sgd_solver.cpp:106] Iteration 49200, lr = 0.0023125
I0630 02:02:17.147071 29015 solver.cpp:290] Iteration 49300 (48.62 iter/s, 2.05677s/100 iter), loss = 0
I0630 02:02:17.147094 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:17.147100 29015 sgd_solver.cpp:106] Iteration 49300, lr = 0.00229687
I0630 02:02:19.206521 29015 solver.cpp:290] Iteration 49400 (48.5587 iter/s, 2.05936s/100 iter), loss = 0
I0630 02:02:19.206542 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:19.206548 29015 sgd_solver.cpp:106] Iteration 49400, lr = 0.00228125
I0630 02:02:21.261109 29015 solver.cpp:290] Iteration 49500 (48.6736 iter/s, 2.0545s/100 iter), loss = 0
I0630 02:02:21.261132 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:21.261138 29015 sgd_solver.cpp:106] Iteration 49500, lr = 0.00226562
I0630 02:02:23.317569 29015 solver.cpp:290] Iteration 49600 (48.6295 iter/s, 2.05636s/100 iter), loss = 0
I0630 02:02:23.317612 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:23.317620 29015 sgd_solver.cpp:106] Iteration 49600, lr = 0.00225
I0630 02:02:25.374791 29015 solver.cpp:290] Iteration 49700 (48.6117 iter/s, 2.05712s/100 iter), loss = 0
I0630 02:02:25.374814 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:25.374820 29015 sgd_solver.cpp:106] Iteration 49700, lr = 0.00223437
I0630 02:02:27.434957 29015 solver.cpp:290] Iteration 49800 (48.5418 iter/s, 2.06008s/100 iter), loss = 0
I0630 02:02:27.434979 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:27.434986 29015 sgd_solver.cpp:106] Iteration 49800, lr = 0.00221875
I0630 02:02:29.487274 29015 solver.cpp:290] Iteration 49900 (48.7275 iter/s, 2.05223s/100 iter), loss = 0
I0630 02:02:29.487313 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:29.487319 29015 sgd_solver.cpp:106] Iteration 49900, lr = 0.00220312
I0630 02:02:31.523243 29015 solver.cpp:598] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_50000.caffemodel
I0630 02:02:31.539969 29015 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_50000.solverstate
I0630 02:02:31.547328 29015 solver.cpp:354] Sparsity after update:
I0630 02:02:31.548307 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:02:31.548316 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:02:31.548324 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:02:31.548327 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:02:31.548331 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:02:31.548333 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:02:31.548336 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:02:31.548338 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:02:31.548341 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:02:31.548343 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:02:31.548346 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:02:31.548348 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:02:31.548351 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:02:31.548449 29015 solver.cpp:471] Iteration 50000, Testing net (#0)
I0630 02:02:33.185673 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9056
I0630 02:02:33.185691 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9955
I0630 02:02:33.185696 29015 solver.cpp:544]     Test net output #2: loss = 0.2243 (* 1 = 0.2243 loss)
I0630 02:02:33.205238 29015 solver.cpp:290] Iteration 50000 (26.8975 iter/s, 3.71782s/100 iter), loss = 0
I0630 02:02:33.205256 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:33.205268 29015 sgd_solver.cpp:106] Iteration 50000, lr = 0.0021875
I0630 02:02:35.260809 29015 solver.cpp:290] Iteration 50100 (48.6502 iter/s, 2.05549s/100 iter), loss = 0
I0630 02:02:35.260833 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:35.260838 29015 sgd_solver.cpp:106] Iteration 50100, lr = 0.00217188
I0630 02:02:37.319917 29015 solver.cpp:290] Iteration 50200 (48.5668 iter/s, 2.05902s/100 iter), loss = 0
I0630 02:02:37.319989 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:37.319998 29015 sgd_solver.cpp:106] Iteration 50200, lr = 0.00215625
I0630 02:02:39.381103 29015 solver.cpp:290] Iteration 50300 (48.5189 iter/s, 2.06105s/100 iter), loss = 0
I0630 02:02:39.381124 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:39.381131 29015 sgd_solver.cpp:106] Iteration 50300, lr = 0.00214063
I0630 02:02:41.438710 29015 solver.cpp:290] Iteration 50400 (48.6021 iter/s, 2.05752s/100 iter), loss = 0
I0630 02:02:41.438732 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:41.438740 29015 sgd_solver.cpp:106] Iteration 50400, lr = 0.002125
I0630 02:02:43.493790 29015 solver.cpp:290] Iteration 50500 (48.6619 iter/s, 2.05499s/100 iter), loss = 0
I0630 02:02:43.493811 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:43.493819 29015 sgd_solver.cpp:106] Iteration 50500, lr = 0.00210937
I0630 02:02:45.547883 29015 solver.cpp:290] Iteration 50600 (48.6854 iter/s, 2.05401s/100 iter), loss = 0
I0630 02:02:45.547904 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:45.547912 29015 sgd_solver.cpp:106] Iteration 50600, lr = 0.00209375
I0630 02:02:47.608523 29015 solver.cpp:290] Iteration 50700 (48.5307 iter/s, 2.06055s/100 iter), loss = 0
I0630 02:02:47.608544 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:47.608552 29015 sgd_solver.cpp:106] Iteration 50700, lr = 0.00207812
I0630 02:02:49.665616 29015 solver.cpp:290] Iteration 50800 (48.6144 iter/s, 2.05701s/100 iter), loss = 0
I0630 02:02:49.665640 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:49.665649 29015 sgd_solver.cpp:106] Iteration 50800, lr = 0.0020625
I0630 02:02:51.719471 29015 solver.cpp:290] Iteration 50900 (48.691 iter/s, 2.05377s/100 iter), loss = 0
I0630 02:02:51.719493 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:51.719501 29015 sgd_solver.cpp:106] Iteration 50900, lr = 0.00204687
I0630 02:02:53.755233 29015 solver.cpp:354] Sparsity after update:
I0630 02:02:53.756497 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:02:53.756505 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:02:53.756513 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:02:53.756516 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:02:53.756518 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:02:53.756521 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:02:53.756525 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:02:53.756526 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:02:53.756530 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:02:53.756531 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:02:53.756533 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:02:53.756536 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:02:53.756538 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:02:53.756628 29015 solver.cpp:471] Iteration 51000, Testing net (#0)
I0630 02:02:55.393712 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9042
I0630 02:02:55.393729 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995
I0630 02:02:55.393734 29015 solver.cpp:544]     Test net output #2: loss = 0.223 (* 1 = 0.223 loss)
I0630 02:02:55.413451 29015 solver.cpp:290] Iteration 51000 (27.0721 iter/s, 3.69385s/100 iter), loss = 0
I0630 02:02:55.413477 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:55.413485 29015 sgd_solver.cpp:106] Iteration 51000, lr = 0.00203125
I0630 02:02:57.469255 29015 solver.cpp:290] Iteration 51100 (48.6449 iter/s, 2.05571s/100 iter), loss = 0
I0630 02:02:57.469277 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:57.469285 29015 sgd_solver.cpp:106] Iteration 51100, lr = 0.00201563
I0630 02:02:59.524360 29015 solver.cpp:290] Iteration 51200 (48.6614 iter/s, 2.05502s/100 iter), loss = 0
I0630 02:02:59.524402 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:02:59.524411 29015 sgd_solver.cpp:106] Iteration 51200, lr = 0.002
I0630 02:03:01.580196 29015 solver.cpp:290] Iteration 51300 (48.6445 iter/s, 2.05573s/100 iter), loss = 0
I0630 02:03:01.580219 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:01.580227 29015 sgd_solver.cpp:106] Iteration 51300, lr = 0.00198438
I0630 02:03:03.643579 29015 solver.cpp:290] Iteration 51400 (48.4662 iter/s, 2.06329s/100 iter), loss = 0
I0630 02:03:03.643600 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:03.643607 29015 sgd_solver.cpp:106] Iteration 51400, lr = 0.00196875
I0630 02:03:05.698901 29015 solver.cpp:290] Iteration 51500 (48.6562 iter/s, 2.05523s/100 iter), loss = 0
I0630 02:03:05.698923 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:05.698931 29015 sgd_solver.cpp:106] Iteration 51500, lr = 0.00195312
I0630 02:03:07.761953 29015 solver.cpp:290] Iteration 51600 (48.4739 iter/s, 2.06296s/100 iter), loss = 0
I0630 02:03:07.762030 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:07.762038 29015 sgd_solver.cpp:106] Iteration 51600, lr = 0.0019375
I0630 02:03:09.831660 29015 solver.cpp:290] Iteration 51700 (48.3194 iter/s, 2.06956s/100 iter), loss = 0
I0630 02:03:09.831688 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:09.831697 29015 sgd_solver.cpp:106] Iteration 51700, lr = 0.00192187
I0630 02:03:11.893529 29015 solver.cpp:290] Iteration 51800 (48.5019 iter/s, 2.06178s/100 iter), loss = 0
I0630 02:03:11.893563 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:11.893573 29015 sgd_solver.cpp:106] Iteration 51800, lr = 0.00190625
I0630 02:03:13.954514 29015 solver.cpp:290] Iteration 51900 (48.5228 iter/s, 2.06089s/100 iter), loss = 0
I0630 02:03:13.954537 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:13.954547 29015 sgd_solver.cpp:106] Iteration 51900, lr = 0.00189062
I0630 02:03:15.989691 29015 solver.cpp:354] Sparsity after update:
I0630 02:03:15.990849 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:03:15.990857 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:03:15.990865 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:03:15.990869 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:03:15.990870 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:03:15.990873 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:03:15.990875 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:03:15.990878 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:03:15.990880 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:03:15.990890 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:03:15.990893 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:03:15.990895 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:03:15.990897 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:03:15.990986 29015 solver.cpp:471] Iteration 52000, Testing net (#0)
I0630 02:03:17.628886 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9043
I0630 02:03:17.628906 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9956
I0630 02:03:17.628911 29015 solver.cpp:544]     Test net output #2: loss = 0.2266 (* 1 = 0.2266 loss)
I0630 02:03:17.648641 29015 solver.cpp:290] Iteration 52000 (27.071 iter/s, 3.69399s/100 iter), loss = 0
I0630 02:03:17.648663 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:17.648671 29015 sgd_solver.cpp:106] Iteration 52000, lr = 0.001875
I0630 02:03:19.705670 29015 solver.cpp:290] Iteration 52100 (48.6159 iter/s, 2.05694s/100 iter), loss = 0
I0630 02:03:19.705696 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:19.705705 29015 sgd_solver.cpp:106] Iteration 52100, lr = 0.00185938
I0630 02:03:21.760448 29015 solver.cpp:290] Iteration 52200 (48.6692 iter/s, 2.05469s/100 iter), loss = 0
I0630 02:03:21.760470 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:21.760476 29015 sgd_solver.cpp:106] Iteration 52200, lr = 0.00184375
I0630 02:03:23.815347 29015 solver.cpp:290] Iteration 52300 (48.6662 iter/s, 2.05481s/100 iter), loss = 0
I0630 02:03:23.815371 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:23.815376 29015 sgd_solver.cpp:106] Iteration 52300, lr = 0.00182813
I0630 02:03:25.872169 29015 solver.cpp:290] Iteration 52400 (48.6208 iter/s, 2.05673s/100 iter), loss = 0
I0630 02:03:25.872193 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:25.872202 29015 sgd_solver.cpp:106] Iteration 52400, lr = 0.0018125
I0630 02:03:27.931958 29015 solver.cpp:290] Iteration 52500 (48.5508 iter/s, 2.0597s/100 iter), loss = 0
I0630 02:03:27.931982 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:27.931991 29015 sgd_solver.cpp:106] Iteration 52500, lr = 0.00179687
I0630 02:03:29.988956 29015 solver.cpp:290] Iteration 52600 (48.6166 iter/s, 2.05691s/100 iter), loss = 0
I0630 02:03:29.988996 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:29.989004 29015 sgd_solver.cpp:106] Iteration 52600, lr = 0.00178125
I0630 02:03:32.043334 29015 solver.cpp:290] Iteration 52700 (48.679 iter/s, 2.05427s/100 iter), loss = 0
I0630 02:03:32.043357 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:32.043366 29015 sgd_solver.cpp:106] Iteration 52700, lr = 0.00176562
I0630 02:03:34.102903 29015 solver.cpp:290] Iteration 52800 (48.5559 iter/s, 2.05948s/100 iter), loss = 0
I0630 02:03:34.102926 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:34.102934 29015 sgd_solver.cpp:106] Iteration 52800, lr = 0.00175
I0630 02:03:36.157917 29015 solver.cpp:290] Iteration 52900 (48.6635 iter/s, 2.05493s/100 iter), loss = 0
I0630 02:03:36.157938 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:36.157945 29015 sgd_solver.cpp:106] Iteration 52900, lr = 0.00173437
I0630 02:03:38.192317 29015 solver.cpp:354] Sparsity after update:
I0630 02:03:38.193588 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:03:38.193595 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:03:38.193603 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:03:38.193606 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:03:38.193608 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:03:38.193611 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:03:38.193614 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:03:38.193616 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:03:38.193619 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:03:38.193621 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:03:38.193624 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:03:38.193626 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:03:38.193629 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:03:38.193716 29015 solver.cpp:471] Iteration 53000, Testing net (#0)
I0630 02:03:39.829602 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9054
I0630 02:03:39.829622 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9958
I0630 02:03:39.829627 29015 solver.cpp:544]     Test net output #2: loss = 0.2281 (* 1 = 0.2281 loss)
I0630 02:03:39.849933 29015 solver.cpp:290] Iteration 53000 (27.0864 iter/s, 3.69188s/100 iter), loss = 0
I0630 02:03:39.849952 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:39.849961 29015 sgd_solver.cpp:106] Iteration 53000, lr = 0.00171875
I0630 02:03:41.907095 29015 solver.cpp:290] Iteration 53100 (48.6126 iter/s, 2.05708s/100 iter), loss = 0
I0630 02:03:41.907116 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:41.907124 29015 sgd_solver.cpp:106] Iteration 53100, lr = 0.00170313
I0630 02:03:43.966562 29015 solver.cpp:290] Iteration 53200 (48.5584 iter/s, 2.05938s/100 iter), loss = 0
I0630 02:03:43.966586 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:43.966593 29015 sgd_solver.cpp:106] Iteration 53200, lr = 0.0016875
I0630 02:03:46.019554 29015 solver.cpp:290] Iteration 53300 (48.7115 iter/s, 2.05291s/100 iter), loss = 0
I0630 02:03:46.019577 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:46.019583 29015 sgd_solver.cpp:106] Iteration 53300, lr = 0.00167188
I0630 02:03:48.074405 29015 solver.cpp:290] Iteration 53400 (48.6674 iter/s, 2.05477s/100 iter), loss = 0
I0630 02:03:48.074429 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:48.074435 29015 sgd_solver.cpp:106] Iteration 53400, lr = 0.00165625
I0630 02:03:50.132513 29015 solver.cpp:290] Iteration 53500 (48.5904 iter/s, 2.05802s/100 iter), loss = 0
I0630 02:03:50.132535 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:50.132542 29015 sgd_solver.cpp:106] Iteration 53500, lr = 0.00164062
I0630 02:03:52.186378 29015 solver.cpp:290] Iteration 53600 (48.6907 iter/s, 2.05378s/100 iter), loss = 0
I0630 02:03:52.186400 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:52.186406 29015 sgd_solver.cpp:106] Iteration 53600, lr = 0.001625
I0630 02:03:54.240020 29015 solver.cpp:290] Iteration 53700 (48.6961 iter/s, 2.05355s/100 iter), loss = 0
I0630 02:03:54.240042 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:54.240049 29015 sgd_solver.cpp:106] Iteration 53700, lr = 0.00160937
I0630 02:03:56.321756 29015 solver.cpp:290] Iteration 53800 (48.0388 iter/s, 2.08165s/100 iter), loss = 0
I0630 02:03:56.321779 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:56.321785 29015 sgd_solver.cpp:106] Iteration 53800, lr = 0.00159375
I0630 02:03:58.384086 29015 solver.cpp:290] Iteration 53900 (48.4909 iter/s, 2.06224s/100 iter), loss = 0
I0630 02:03:58.384112 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:03:58.384121 29015 sgd_solver.cpp:106] Iteration 53900, lr = 0.00157812
I0630 02:04:00.420658 29015 solver.cpp:354] Sparsity after update:
I0630 02:04:00.422067 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:04:00.422080 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:04:00.422088 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:04:00.422091 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:04:00.422096 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:04:00.422098 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:04:00.422101 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:04:00.422102 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:04:00.422107 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:04:00.422109 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:04:00.422111 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:04:00.422113 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:04:00.422116 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:04:00.422212 29015 solver.cpp:471] Iteration 54000, Testing net (#0)
I0630 02:04:02.058346 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9043
I0630 02:04:02.058364 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9955
I0630 02:04:02.058370 29015 solver.cpp:544]     Test net output #2: loss = 0.2268 (* 1 = 0.2268 loss)
I0630 02:04:02.078349 29015 solver.cpp:290] Iteration 54000 (27.07 iter/s, 3.69413s/100 iter), loss = 0
I0630 02:04:02.078366 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:02.078377 29015 sgd_solver.cpp:106] Iteration 54000, lr = 0.0015625
I0630 02:04:04.137825 29015 solver.cpp:290] Iteration 54100 (48.558 iter/s, 2.05939s/100 iter), loss = 0
I0630 02:04:04.137847 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:04.137854 29015 sgd_solver.cpp:106] Iteration 54100, lr = 0.00154688
I0630 02:04:06.200726 29015 solver.cpp:290] Iteration 54200 (48.4775 iter/s, 2.06281s/100 iter), loss = 0
I0630 02:04:06.200747 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:06.200754 29015 sgd_solver.cpp:106] Iteration 54200, lr = 0.00153125
I0630 02:04:08.258731 29015 solver.cpp:290] Iteration 54300 (48.5928 iter/s, 2.05792s/100 iter), loss = 0
I0630 02:04:08.258821 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:08.258831 29015 sgd_solver.cpp:106] Iteration 54300, lr = 0.00151563
I0630 02:04:10.317973 29015 solver.cpp:290] Iteration 54400 (48.5651 iter/s, 2.05909s/100 iter), loss = 0
I0630 02:04:10.317996 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:10.318002 29015 sgd_solver.cpp:106] Iteration 54400, lr = 0.0015
I0630 02:04:12.378685 29015 solver.cpp:290] Iteration 54500 (48.5289 iter/s, 2.06063s/100 iter), loss = 0
I0630 02:04:12.378707 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:12.378715 29015 sgd_solver.cpp:106] Iteration 54500, lr = 0.00148437
I0630 02:04:14.441810 29015 solver.cpp:290] Iteration 54600 (48.4725 iter/s, 2.06302s/100 iter), loss = 0
I0630 02:04:14.441861 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:14.441870 29015 sgd_solver.cpp:106] Iteration 54600, lr = 0.00146875
I0630 02:04:16.497994 29015 solver.cpp:290] Iteration 54700 (48.6365 iter/s, 2.05607s/100 iter), loss = 0
I0630 02:04:16.498020 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:16.498028 29015 sgd_solver.cpp:106] Iteration 54700, lr = 0.00145312
I0630 02:04:18.555886 29015 solver.cpp:290] Iteration 54800 (48.5955 iter/s, 2.0578s/100 iter), loss = 0
I0630 02:04:18.555908 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:18.555915 29015 sgd_solver.cpp:106] Iteration 54800, lr = 0.0014375
I0630 02:04:20.613112 29015 solver.cpp:290] Iteration 54900 (48.6112 iter/s, 2.05714s/100 iter), loss = 0
I0630 02:04:20.613137 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:20.613145 29015 sgd_solver.cpp:106] Iteration 54900, lr = 0.00142187
I0630 02:04:22.648934 29015 solver.cpp:354] Sparsity after update:
I0630 02:04:22.650183 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:04:22.650189 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:04:22.650197 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:04:22.650198 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:04:22.650202 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:04:22.650203 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:04:22.650205 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:04:22.650207 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:04:22.650209 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:04:22.650212 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:04:22.650213 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:04:22.650215 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:04:22.650218 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:04:22.650301 29015 solver.cpp:471] Iteration 55000, Testing net (#0)
I0630 02:04:24.287328 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9058
I0630 02:04:24.287348 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9956
I0630 02:04:24.287353 29015 solver.cpp:544]     Test net output #2: loss = 0.2271 (* 1 = 0.2271 loss)
I0630 02:04:24.311683 29015 solver.cpp:290] Iteration 55000 (27.0384 iter/s, 3.69844s/100 iter), loss = 0
I0630 02:04:24.311700 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:24.311712 29015 sgd_solver.cpp:106] Iteration 55000, lr = 0.00140625
I0630 02:04:26.374333 29015 solver.cpp:290] Iteration 55100 (48.4833 iter/s, 2.06257s/100 iter), loss = 0
I0630 02:04:26.374356 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:26.374363 29015 sgd_solver.cpp:106] Iteration 55100, lr = 0.00139063
I0630 02:04:28.434306 29015 solver.cpp:290] Iteration 55200 (48.5464 iter/s, 2.05989s/100 iter), loss = 0
I0630 02:04:28.434329 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:28.434334 29015 sgd_solver.cpp:106] Iteration 55200, lr = 0.001375
I0630 02:04:30.495827 29015 solver.cpp:290] Iteration 55300 (48.5099 iter/s, 2.06143s/100 iter), loss = 0
I0630 02:04:30.495864 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:30.495872 29015 sgd_solver.cpp:106] Iteration 55300, lr = 0.00135938
I0630 02:04:32.561600 29015 solver.cpp:290] Iteration 55400 (48.4105 iter/s, 2.06567s/100 iter), loss = 0
I0630 02:04:32.561625 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:32.561635 29015 sgd_solver.cpp:106] Iteration 55400, lr = 0.00134375
I0630 02:04:34.618835 29015 solver.cpp:290] Iteration 55500 (48.6111 iter/s, 2.05715s/100 iter), loss = 0
I0630 02:04:34.618858 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:34.618867 29015 sgd_solver.cpp:106] Iteration 55500, lr = 0.00132813
I0630 02:04:36.675850 29015 solver.cpp:290] Iteration 55600 (48.6162 iter/s, 2.05693s/100 iter), loss = 0
I0630 02:04:36.675873 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:36.675879 29015 sgd_solver.cpp:106] Iteration 55600, lr = 0.0013125
I0630 02:04:38.730872 29015 solver.cpp:290] Iteration 55700 (48.6634 iter/s, 2.05493s/100 iter), loss = 0
I0630 02:04:38.730978 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:38.730986 29015 sgd_solver.cpp:106] Iteration 55700, lr = 0.00129687
I0630 02:04:40.788645 29015 solver.cpp:290] Iteration 55800 (48.6002 iter/s, 2.05761s/100 iter), loss = 0
I0630 02:04:40.788667 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:40.788674 29015 sgd_solver.cpp:106] Iteration 55800, lr = 0.00128125
I0630 02:04:42.844759 29015 solver.cpp:290] Iteration 55900 (48.6375 iter/s, 2.05603s/100 iter), loss = 0
I0630 02:04:42.844781 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:42.844789 29015 sgd_solver.cpp:106] Iteration 55900, lr = 0.00126562
I0630 02:04:44.883028 29015 solver.cpp:354] Sparsity after update:
I0630 02:04:44.884281 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:04:44.884289 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:04:44.884295 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:04:44.884297 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:04:44.884299 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:04:44.884301 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:04:44.884304 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:04:44.884305 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:04:44.884307 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:04:44.884310 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:04:44.884311 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:04:44.884313 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:04:44.884315 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:04:44.884405 29015 solver.cpp:471] Iteration 56000, Testing net (#0)
I0630 02:04:46.523270 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9059
I0630 02:04:46.523288 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995
I0630 02:04:46.523294 29015 solver.cpp:544]     Test net output #2: loss = 0.2256 (* 1 = 0.2256 loss)
I0630 02:04:46.543218 29015 solver.cpp:290] Iteration 56000 (27.0393 iter/s, 3.69833s/100 iter), loss = 0
I0630 02:04:46.543243 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:46.543249 29015 sgd_solver.cpp:106] Iteration 56000, lr = 0.00125
I0630 02:04:48.611007 29015 solver.cpp:290] Iteration 56100 (48.3629 iter/s, 2.0677s/100 iter), loss = 0
I0630 02:04:48.611030 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:48.611037 29015 sgd_solver.cpp:106] Iteration 56100, lr = 0.00123438
I0630 02:04:50.672852 29015 solver.cpp:290] Iteration 56200 (48.5023 iter/s, 2.06176s/100 iter), loss = 0
I0630 02:04:50.672876 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:50.672883 29015 sgd_solver.cpp:106] Iteration 56200, lr = 0.00121875
I0630 02:04:52.728236 29015 solver.cpp:290] Iteration 56300 (48.6548 iter/s, 2.0553s/100 iter), loss = 0
I0630 02:04:52.728257 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:52.728266 29015 sgd_solver.cpp:106] Iteration 56300, lr = 0.00120313
I0630 02:04:54.787145 29015 solver.cpp:290] Iteration 56400 (48.5714 iter/s, 2.05882s/100 iter), loss = 0
I0630 02:04:54.787168 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:54.787174 29015 sgd_solver.cpp:106] Iteration 56400, lr = 0.0011875
I0630 02:04:56.846226 29015 solver.cpp:290] Iteration 56500 (48.5674 iter/s, 2.05899s/100 iter), loss = 0
I0630 02:04:56.846248 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:56.846256 29015 sgd_solver.cpp:106] Iteration 56500, lr = 0.00117187
I0630 02:04:58.899850 29015 solver.cpp:290] Iteration 56600 (48.6965 iter/s, 2.05354s/100 iter), loss = 0
I0630 02:04:58.899871 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:04:58.899878 29015 sgd_solver.cpp:106] Iteration 56600, lr = 0.00115625
I0630 02:05:00.953819 29015 solver.cpp:290] Iteration 56700 (48.6883 iter/s, 2.05388s/100 iter), loss = 0
I0630 02:05:00.953860 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:00.953871 29015 sgd_solver.cpp:106] Iteration 56700, lr = 0.00114062
I0630 02:05:03.005146 29015 solver.cpp:290] Iteration 56800 (48.7514 iter/s, 2.05122s/100 iter), loss = 0
I0630 02:05:03.005172 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:03.005178 29015 sgd_solver.cpp:106] Iteration 56800, lr = 0.001125
I0630 02:05:05.059659 29015 solver.cpp:290] Iteration 56900 (48.6755 iter/s, 2.05442s/100 iter), loss = 0
I0630 02:05:05.059679 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:05.059686 29015 sgd_solver.cpp:106] Iteration 56900, lr = 0.00110937
I0630 02:05:07.093394 29015 solver.cpp:354] Sparsity after update:
I0630 02:05:07.094655 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:05:07.094662 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:05:07.094671 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:05:07.094674 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:05:07.094678 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:05:07.094683 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:05:07.094688 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:05:07.094692 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:05:07.094696 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:05:07.094700 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:05:07.094704 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:05:07.094708 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:05:07.094712 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:05:07.094856 29015 solver.cpp:471] Iteration 57000, Testing net (#0)
I0630 02:05:08.733505 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9061
I0630 02:05:08.733551 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9956
I0630 02:05:08.733557 29015 solver.cpp:544]     Test net output #2: loss = 0.2275 (* 1 = 0.2275 loss)
I0630 02:05:08.753720 29015 solver.cpp:290] Iteration 57000 (27.0714 iter/s, 3.69393s/100 iter), loss = 0
I0630 02:05:08.753737 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:08.753749 29015 sgd_solver.cpp:106] Iteration 57000, lr = 0.00109375
I0630 02:05:10.808569 29015 solver.cpp:290] Iteration 57100 (48.6673 iter/s, 2.05477s/100 iter), loss = 0
I0630 02:05:10.808591 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:10.808599 29015 sgd_solver.cpp:106] Iteration 57100, lr = 0.00107813
I0630 02:05:12.863256 29015 solver.cpp:290] Iteration 57200 (48.6713 iter/s, 2.0546s/100 iter), loss = 0
I0630 02:05:12.863278 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:12.863284 29015 sgd_solver.cpp:106] Iteration 57200, lr = 0.0010625
I0630 02:05:14.916241 29015 solver.cpp:290] Iteration 57300 (48.7116 iter/s, 2.0529s/100 iter), loss = 0
I0630 02:05:14.916265 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:14.916270 29015 sgd_solver.cpp:106] Iteration 57300, lr = 0.00104688
I0630 02:05:16.969579 29015 solver.cpp:290] Iteration 57400 (48.7033 iter/s, 2.05325s/100 iter), loss = 0
I0630 02:05:16.969601 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:16.969609 29015 sgd_solver.cpp:106] Iteration 57400, lr = 0.00103125
I0630 02:05:19.024271 29015 solver.cpp:290] Iteration 57500 (48.6712 iter/s, 2.0546s/100 iter), loss = 0
I0630 02:05:19.024294 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:19.024303 29015 sgd_solver.cpp:106] Iteration 57500, lr = 0.00101562
I0630 02:05:21.080379 29015 solver.cpp:290] Iteration 57600 (48.6376 iter/s, 2.05602s/100 iter), loss = 0
I0630 02:05:21.080401 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:21.080409 29015 sgd_solver.cpp:106] Iteration 57600, lr = 0.001
I0630 02:05:23.135583 29015 solver.cpp:290] Iteration 57700 (48.659 iter/s, 2.05512s/100 iter), loss = 0
I0630 02:05:23.135606 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:23.135612 29015 sgd_solver.cpp:106] Iteration 57700, lr = 0.000984375
I0630 02:05:25.192625 29015 solver.cpp:290] Iteration 57800 (48.6155 iter/s, 2.05696s/100 iter), loss = 0
I0630 02:05:25.192647 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:25.192656 29015 sgd_solver.cpp:106] Iteration 57800, lr = 0.00096875
I0630 02:05:27.249508 29015 solver.cpp:290] Iteration 57900 (48.6193 iter/s, 2.0568s/100 iter), loss = 0
I0630 02:05:27.249531 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:27.249537 29015 sgd_solver.cpp:106] Iteration 57900, lr = 0.000953125
I0630 02:05:29.288687 29015 solver.cpp:354] Sparsity after update:
I0630 02:05:29.289952 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:05:29.289959 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:05:29.289968 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:05:29.289969 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:05:29.289971 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:05:29.289973 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:05:29.289975 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:05:29.289978 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:05:29.289979 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:05:29.289981 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:05:29.289983 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:05:29.289985 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:05:29.289988 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:05:29.290073 29015 solver.cpp:471] Iteration 58000, Testing net (#0)
I0630 02:05:30.927268 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9047
I0630 02:05:30.927286 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9957
I0630 02:05:30.927292 29015 solver.cpp:544]     Test net output #2: loss = 0.2289 (* 1 = 0.2289 loss)
I0630 02:05:30.948078 29015 solver.cpp:290] Iteration 58000 (27.0384 iter/s, 3.69844s/100 iter), loss = 0
I0630 02:05:30.948096 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:30.948109 29015 sgd_solver.cpp:106] Iteration 58000, lr = 0.0009375
I0630 02:05:33.005215 29015 solver.cpp:290] Iteration 58100 (48.6133 iter/s, 2.05705s/100 iter), loss = 0
I0630 02:05:33.005236 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:33.005244 29015 sgd_solver.cpp:106] Iteration 58100, lr = 0.000921875
I0630 02:05:35.069025 29015 solver.cpp:290] Iteration 58200 (48.4561 iter/s, 2.06372s/100 iter), loss = 0
I0630 02:05:35.069051 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:35.069056 29015 sgd_solver.cpp:106] Iteration 58200, lr = 0.00090625
I0630 02:05:37.127912 29015 solver.cpp:290] Iteration 58300 (48.5721 iter/s, 2.05879s/100 iter), loss = 0
I0630 02:05:37.127935 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:37.127943 29015 sgd_solver.cpp:106] Iteration 58300, lr = 0.000890625
I0630 02:05:39.181345 29015 solver.cpp:290] Iteration 58400 (48.701 iter/s, 2.05335s/100 iter), loss = 0
I0630 02:05:39.181429 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:39.181437 29015 sgd_solver.cpp:106] Iteration 58400, lr = 0.000875
I0630 02:05:41.239459 29015 solver.cpp:290] Iteration 58500 (48.5917 iter/s, 2.05797s/100 iter), loss = 0
I0630 02:05:41.239480 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:41.239487 29015 sgd_solver.cpp:106] Iteration 58500, lr = 0.000859375
I0630 02:05:43.294471 29015 solver.cpp:290] Iteration 58600 (48.6636 iter/s, 2.05493s/100 iter), loss = 0
I0630 02:05:43.294493 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:43.294500 29015 sgd_solver.cpp:106] Iteration 58600, lr = 0.00084375
I0630 02:05:45.346027 29015 solver.cpp:290] Iteration 58700 (48.7455 iter/s, 2.05147s/100 iter), loss = 0
I0630 02:05:45.346050 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:45.346056 29015 sgd_solver.cpp:106] Iteration 58700, lr = 0.000828125
I0630 02:05:47.399729 29015 solver.cpp:290] Iteration 58800 (48.6947 iter/s, 2.05361s/100 iter), loss = 0
I0630 02:05:47.399750 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:47.399757 29015 sgd_solver.cpp:106] Iteration 58800, lr = 0.0008125
I0630 02:05:49.452879 29015 solver.cpp:290] Iteration 58900 (48.7077 iter/s, 2.05306s/100 iter), loss = 0
I0630 02:05:49.452903 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:49.452909 29015 sgd_solver.cpp:106] Iteration 58900, lr = 0.000796875
I0630 02:05:51.485743 29015 solver.cpp:354] Sparsity after update:
I0630 02:05:51.486997 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:05:51.487004 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:05:51.487011 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:05:51.487013 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:05:51.487016 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:05:51.487018 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:05:51.487020 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:05:51.487022 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:05:51.487025 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:05:51.487026 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:05:51.487028 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:05:51.487030 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:05:51.487032 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:05:51.487116 29015 solver.cpp:471] Iteration 59000, Testing net (#0)
I0630 02:05:53.123535 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9057
I0630 02:05:53.123554 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9956
I0630 02:05:53.123559 29015 solver.cpp:544]     Test net output #2: loss = 0.226 (* 1 = 0.226 loss)
I0630 02:05:53.143133 29015 solver.cpp:290] Iteration 59000 (27.0994 iter/s, 3.69012s/100 iter), loss = 0
I0630 02:05:53.143151 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:53.143162 29015 sgd_solver.cpp:106] Iteration 59000, lr = 0.00078125
I0630 02:05:55.199924 29015 solver.cpp:290] Iteration 59100 (48.6214 iter/s, 2.05671s/100 iter), loss = 0
I0630 02:05:55.199947 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:55.199954 29015 sgd_solver.cpp:106] Iteration 59100, lr = 0.000765625
I0630 02:05:57.253805 29015 solver.cpp:290] Iteration 59200 (48.6904 iter/s, 2.05379s/100 iter), loss = 0
I0630 02:05:57.253829 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:57.253836 29015 sgd_solver.cpp:106] Iteration 59200, lr = 0.00075
I0630 02:05:59.306532 29015 solver.cpp:290] Iteration 59300 (48.7178 iter/s, 2.05264s/100 iter), loss = 0
I0630 02:05:59.306557 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:05:59.306566 29015 sgd_solver.cpp:106] Iteration 59300, lr = 0.000734375
I0630 02:06:01.360487 29015 solver.cpp:290] Iteration 59400 (48.6887 iter/s, 2.05387s/100 iter), loss = 0
I0630 02:06:01.360527 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:01.360536 29015 sgd_solver.cpp:106] Iteration 59400, lr = 0.00071875
I0630 02:06:03.419324 29015 solver.cpp:290] Iteration 59500 (48.5735 iter/s, 2.05873s/100 iter), loss = 0
I0630 02:06:03.419345 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:03.419353 29015 sgd_solver.cpp:106] Iteration 59500, lr = 0.000703125
I0630 02:06:05.476352 29015 solver.cpp:290] Iteration 59600 (48.6158 iter/s, 2.05694s/100 iter), loss = 0
I0630 02:06:05.476373 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:05.476382 29015 sgd_solver.cpp:106] Iteration 59600, lr = 0.0006875
I0630 02:06:07.530237 29015 solver.cpp:290] Iteration 59700 (48.6903 iter/s, 2.0538s/100 iter), loss = 0
I0630 02:06:07.530258 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:07.530266 29015 sgd_solver.cpp:106] Iteration 59700, lr = 0.000671875
I0630 02:06:09.596856 29015 solver.cpp:290] Iteration 59800 (48.3902 iter/s, 2.06653s/100 iter), loss = 0
I0630 02:06:09.596920 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:09.596928 29015 sgd_solver.cpp:106] Iteration 59800, lr = 0.00065625
I0630 02:06:11.651645 29015 solver.cpp:290] Iteration 59900 (48.6698 iter/s, 2.05466s/100 iter), loss = 0
I0630 02:06:11.651667 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:11.651674 29015 sgd_solver.cpp:106] Iteration 59900, lr = 0.000640625
I0630 02:06:13.686931 29015 solver.cpp:598] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_60000.caffemodel
I0630 02:06:13.703379 29015 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_60000.solverstate
I0630 02:06:13.710661 29015 solver.cpp:354] Sparsity after update:
I0630 02:06:13.711603 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:06:13.711611 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:06:13.711619 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:06:13.711621 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:06:13.711624 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:06:13.711627 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:06:13.711628 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:06:13.711630 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:06:13.711632 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:06:13.711634 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:06:13.711637 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:06:13.711639 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:06:13.711642 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:06:13.711737 29015 solver.cpp:471] Iteration 60000, Testing net (#0)
I0630 02:06:15.345600 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.906
I0630 02:06:15.345620 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9957
I0630 02:06:15.345625 29015 solver.cpp:544]     Test net output #2: loss = 0.2259 (* 1 = 0.2259 loss)
I0630 02:06:15.365943 29015 solver.cpp:290] Iteration 60000 (26.9239 iter/s, 3.71417s/100 iter), loss = 0
I0630 02:06:15.365960 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:15.365972 29015 sgd_solver.cpp:106] Iteration 60000, lr = 0.000625
I0630 02:06:17.427134 29015 solver.cpp:290] Iteration 60100 (48.5176 iter/s, 2.06111s/100 iter), loss = 0
I0630 02:06:17.427156 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:17.427163 29015 sgd_solver.cpp:106] Iteration 60100, lr = 0.000609375
I0630 02:06:19.480485 29015 solver.cpp:290] Iteration 60200 (48.7029 iter/s, 2.05326s/100 iter), loss = 0
I0630 02:06:19.480507 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:19.480515 29015 sgd_solver.cpp:106] Iteration 60200, lr = 0.00059375
I0630 02:06:21.536072 29015 solver.cpp:290] Iteration 60300 (48.65 iter/s, 2.0555s/100 iter), loss = 0
I0630 02:06:21.536093 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:21.536100 29015 sgd_solver.cpp:106] Iteration 60300, lr = 0.000578125
I0630 02:06:23.590320 29015 solver.cpp:290] Iteration 60400 (48.6817 iter/s, 2.05416s/100 iter), loss = 0
I0630 02:06:23.590343 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:23.590350 29015 sgd_solver.cpp:106] Iteration 60400, lr = 0.0005625
I0630 02:06:25.645503 29015 solver.cpp:290] Iteration 60500 (48.6595 iter/s, 2.0551s/100 iter), loss = 0
I0630 02:06:25.645524 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:25.645531 29015 sgd_solver.cpp:106] Iteration 60500, lr = 0.000546875
I0630 02:06:27.699985 29015 solver.cpp:290] Iteration 60600 (48.6761 iter/s, 2.0544s/100 iter), loss = 0
I0630 02:06:27.700006 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:27.700034 29015 sgd_solver.cpp:106] Iteration 60600, lr = 0.00053125
I0630 02:06:29.754212 29015 solver.cpp:290] Iteration 60700 (48.6822 iter/s, 2.05414s/100 iter), loss = 0
I0630 02:06:29.754235 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:29.754241 29015 sgd_solver.cpp:106] Iteration 60700, lr = 0.000515625
I0630 02:06:31.809944 29015 solver.cpp:290] Iteration 60800 (48.6465 iter/s, 2.05565s/100 iter), loss = 0
I0630 02:06:31.809967 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:31.809974 29015 sgd_solver.cpp:106] Iteration 60800, lr = 0.0005
I0630 02:06:33.862788 29015 solver.cpp:290] Iteration 60900 (48.715 iter/s, 2.05276s/100 iter), loss = 0
I0630 02:06:33.862810 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:33.862817 29015 sgd_solver.cpp:106] Iteration 60900, lr = 0.000484375
I0630 02:06:35.901886 29015 solver.cpp:354] Sparsity after update:
I0630 02:06:35.903144 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:06:35.903151 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:06:35.903159 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:06:35.903162 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:06:35.903164 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:06:35.903167 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:06:35.903169 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:06:35.903172 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:06:35.903174 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:06:35.903177 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:06:35.903179 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:06:35.903182 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:06:35.903183 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:06:35.903268 29015 solver.cpp:471] Iteration 61000, Testing net (#0)
I0630 02:06:37.538398 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9056
I0630 02:06:37.538417 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9955
I0630 02:06:37.538422 29015 solver.cpp:544]     Test net output #2: loss = 0.2262 (* 1 = 0.2262 loss)
I0630 02:06:37.558166 29015 solver.cpp:290] Iteration 61000 (27.0618 iter/s, 3.69525s/100 iter), loss = 0
I0630 02:06:37.558183 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:37.558195 29015 sgd_solver.cpp:106] Iteration 61000, lr = 0.00046875
I0630 02:06:39.611891 29015 solver.cpp:290] Iteration 61100 (48.694 iter/s, 2.05364s/100 iter), loss = 0
I0630 02:06:39.611956 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:39.611964 29015 sgd_solver.cpp:106] Iteration 61100, lr = 0.000453125
I0630 02:06:41.667369 29015 solver.cpp:290] Iteration 61200 (48.6535 iter/s, 2.05535s/100 iter), loss = 0.047619
I0630 02:06:41.667392 29015 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0630 02:06:41.667398 29015 sgd_solver.cpp:106] Iteration 61200, lr = 0.0004375
I0630 02:06:43.728478 29015 solver.cpp:290] Iteration 61300 (48.5196 iter/s, 2.06102s/100 iter), loss = 0
I0630 02:06:43.728500 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:43.728508 29015 sgd_solver.cpp:106] Iteration 61300, lr = 0.000421875
I0630 02:06:45.795836 29015 solver.cpp:290] Iteration 61400 (48.373 iter/s, 2.06727s/100 iter), loss = 0
I0630 02:06:45.795859 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:45.795866 29015 sgd_solver.cpp:106] Iteration 61400, lr = 0.00040625
I0630 02:06:47.851689 29015 solver.cpp:290] Iteration 61500 (48.6437 iter/s, 2.05577s/100 iter), loss = 0
I0630 02:06:47.851712 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:47.851719 29015 sgd_solver.cpp:106] Iteration 61500, lr = 0.000390625
I0630 02:06:49.905979 29015 solver.cpp:290] Iteration 61600 (48.6807 iter/s, 2.0542s/100 iter), loss = 0
I0630 02:06:49.906000 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:49.906008 29015 sgd_solver.cpp:106] Iteration 61600, lr = 0.000375
I0630 02:06:51.961374 29015 solver.cpp:290] Iteration 61700 (48.6545 iter/s, 2.05531s/100 iter), loss = 0
I0630 02:06:51.961396 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:51.961403 29015 sgd_solver.cpp:106] Iteration 61700, lr = 0.000359375
I0630 02:06:54.022766 29015 solver.cpp:290] Iteration 61800 (48.513 iter/s, 2.0613s/100 iter), loss = 0
I0630 02:06:54.022791 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:54.022799 29015 sgd_solver.cpp:106] Iteration 61800, lr = 0.00034375
I0630 02:06:56.079090 29015 solver.cpp:290] Iteration 61900 (48.6326 iter/s, 2.05623s/100 iter), loss = 0
I0630 02:06:56.079118 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:56.079128 29015 sgd_solver.cpp:106] Iteration 61900, lr = 0.000328125
I0630 02:06:58.112349 29015 solver.cpp:354] Sparsity after update:
I0630 02:06:58.113610 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:06:58.113617 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:06:58.113626 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:06:58.113628 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:06:58.113631 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:06:58.113632 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:06:58.113636 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:06:58.113637 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:06:58.113641 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:06:58.113643 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:06:58.113646 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:06:58.113648 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:06:58.113651 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:06:58.113735 29015 solver.cpp:471] Iteration 62000, Testing net (#0)
I0630 02:06:59.750670 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9058
I0630 02:06:59.750689 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9955
I0630 02:06:59.750694 29015 solver.cpp:544]     Test net output #2: loss = 0.2274 (* 1 = 0.2274 loss)
I0630 02:06:59.770423 29015 solver.cpp:290] Iteration 62000 (27.0915 iter/s, 3.6912s/100 iter), loss = 0
I0630 02:06:59.770442 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:06:59.770454 29015 sgd_solver.cpp:106] Iteration 62000, lr = 0.0003125
I0630 02:07:01.827659 29015 solver.cpp:290] Iteration 62100 (48.6109 iter/s, 2.05715s/100 iter), loss = 0
I0630 02:07:01.827682 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:01.827688 29015 sgd_solver.cpp:106] Iteration 62100, lr = 0.000296875
I0630 02:07:03.887364 29015 solver.cpp:290] Iteration 62200 (48.5527 iter/s, 2.05962s/100 iter), loss = 0
I0630 02:07:03.887385 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:03.887393 29015 sgd_solver.cpp:106] Iteration 62200, lr = 0.00028125
I0630 02:07:05.944180 29015 solver.cpp:290] Iteration 62300 (48.6208 iter/s, 2.05673s/100 iter), loss = 0
I0630 02:07:05.944202 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:05.944209 29015 sgd_solver.cpp:106] Iteration 62300, lr = 0.000265625
I0630 02:07:07.999330 29015 solver.cpp:290] Iteration 62400 (48.6603 iter/s, 2.05506s/100 iter), loss = 0
I0630 02:07:07.999352 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:07.999359 29015 sgd_solver.cpp:106] Iteration 62400, lr = 0.00025
I0630 02:07:10.055614 29015 solver.cpp:290] Iteration 62500 (48.6335 iter/s, 2.0562s/100 iter), loss = 0
I0630 02:07:10.055685 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:10.055692 29015 sgd_solver.cpp:106] Iteration 62500, lr = 0.000234375
I0630 02:07:12.111199 29015 solver.cpp:290] Iteration 62600 (48.6511 iter/s, 2.05545s/100 iter), loss = 0
I0630 02:07:12.111222 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:12.111228 29015 sgd_solver.cpp:106] Iteration 62600, lr = 0.00021875
I0630 02:07:14.165974 29015 solver.cpp:290] Iteration 62700 (48.6692 iter/s, 2.05469s/100 iter), loss = 0
I0630 02:07:14.165997 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:14.166003 29015 sgd_solver.cpp:106] Iteration 62700, lr = 0.000203125
I0630 02:07:16.220568 29015 solver.cpp:290] Iteration 62800 (48.6735 iter/s, 2.05451s/100 iter), loss = 0
I0630 02:07:16.220590 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:16.220597 29015 sgd_solver.cpp:106] Iteration 62800, lr = 0.0001875
I0630 02:07:18.275588 29015 solver.cpp:290] Iteration 62900 (48.6634 iter/s, 2.05493s/100 iter), loss = 0
I0630 02:07:18.275611 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:18.275617 29015 sgd_solver.cpp:106] Iteration 62900, lr = 0.000171875
I0630 02:07:20.309784 29015 solver.cpp:354] Sparsity after update:
I0630 02:07:20.311051 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:07:20.311058 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:07:20.311065 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:07:20.311067 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:07:20.311070 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:07:20.311072 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:07:20.311074 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:07:20.311076 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:07:20.311079 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:07:20.311080 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:07:20.311082 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:07:20.311084 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:07:20.311086 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:07:20.311170 29015 solver.cpp:471] Iteration 63000, Testing net (#0)
I0630 02:07:21.948072 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9056
I0630 02:07:21.948091 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9953
I0630 02:07:21.948097 29015 solver.cpp:544]     Test net output #2: loss = 0.2272 (* 1 = 0.2272 loss)
I0630 02:07:21.967617 29015 solver.cpp:290] Iteration 63000 (27.0863 iter/s, 3.6919s/100 iter), loss = 0
I0630 02:07:21.967636 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:21.967648 29015 sgd_solver.cpp:106] Iteration 63000, lr = 0.00015625
I0630 02:07:24.024210 29015 solver.cpp:290] Iteration 63100 (48.6261 iter/s, 2.05651s/100 iter), loss = 0
I0630 02:07:24.024231 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:24.024238 29015 sgd_solver.cpp:106] Iteration 63100, lr = 0.000140625
I0630 02:07:26.081771 29015 solver.cpp:290] Iteration 63200 (48.6033 iter/s, 2.05747s/100 iter), loss = 0
I0630 02:07:26.081794 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:26.081800 29015 sgd_solver.cpp:106] Iteration 63200, lr = 0.000125
I0630 02:07:28.139916 29015 solver.cpp:290] Iteration 63300 (48.5896 iter/s, 2.05806s/100 iter), loss = 0
I0630 02:07:28.139940 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:28.139947 29015 sgd_solver.cpp:106] Iteration 63300, lr = 0.000109375
I0630 02:07:30.194211 29015 solver.cpp:290] Iteration 63400 (48.6806 iter/s, 2.05421s/100 iter), loss = 0
I0630 02:07:30.194233 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:30.194242 29015 sgd_solver.cpp:106] Iteration 63400, lr = 9.37498e-05
I0630 02:07:32.252765 29015 solver.cpp:290] Iteration 63500 (48.5799 iter/s, 2.05847s/100 iter), loss = 0
I0630 02:07:32.252804 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:32.252812 29015 sgd_solver.cpp:106] Iteration 63500, lr = 7.8125e-05
I0630 02:07:34.308624 29015 solver.cpp:290] Iteration 63600 (48.6439 iter/s, 2.05575s/100 iter), loss = 0
I0630 02:07:34.308646 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:34.308652 29015 sgd_solver.cpp:106] Iteration 63600, lr = 6.25002e-05
I0630 02:07:36.365772 29015 solver.cpp:290] Iteration 63700 (48.6131 iter/s, 2.05706s/100 iter), loss = 0
I0630 02:07:36.365793 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:36.365803 29015 sgd_solver.cpp:106] Iteration 63700, lr = 4.68749e-05
I0630 02:07:38.419731 29015 solver.cpp:290] Iteration 63800 (48.6885 iter/s, 2.05387s/100 iter), loss = 0
I0630 02:07:38.419754 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:38.419760 29015 sgd_solver.cpp:106] Iteration 63800, lr = 3.12501e-05
I0630 02:07:40.475477 29015 solver.cpp:290] Iteration 63900 (48.6462 iter/s, 2.05566s/100 iter), loss = 0
I0630 02:07:40.475548 29015 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0630 02:07:40.475555 29015 sgd_solver.cpp:106] Iteration 63900, lr = 1.56248e-05
I0630 02:07:42.513162 29015 solver.cpp:354] Sparsity after update:
I0630 02:07:42.514575 29015 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0630 02:07:42.514581 29015 net.cpp:1851] conv1a_param_0(0.41) 
I0630 02:07:42.514590 29015 net.cpp:1851] conv1b_param_0(0.82) 
I0630 02:07:42.514595 29015 net.cpp:1851] fc10_param_0(0) 
I0630 02:07:42.514600 29015 net.cpp:1851] res2a_branch2a_param_0(0.82) 
I0630 02:07:42.514605 29015 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0630 02:07:42.514611 29015 net.cpp:1851] res3a_branch2a_param_0(0.801) 
I0630 02:07:42.514616 29015 net.cpp:1851] res3a_branch2b_param_0(0.796) 
I0630 02:07:42.514621 29015 net.cpp:1851] res4a_branch2a_param_0(0.776) 
I0630 02:07:42.514626 29015 net.cpp:1851] res4a_branch2b_param_0(0.808) 
I0630 02:07:42.514629 29015 net.cpp:1851] res5a_branch2a_param_0(0.82) 
I0630 02:07:42.514636 29015 net.cpp:1851] res5a_branch2b_param_0(0.82) 
I0630 02:07:42.514639 29015 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.91267e+06/2.3599e+06) 0.81
I0630 02:07:42.514652 29015 solver.cpp:598] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_64000.caffemodel
I0630 02:07:42.530710 29015 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-06-30_01-13-02/sparse/cifar10_jacintonet11v2_iter_64000.solverstate
I0630 02:07:42.542845 29015 solver.cpp:451] Iteration 64000, loss = 0
I0630 02:07:42.542870 29015 solver.cpp:471] Iteration 64000, Testing net (#0)
I0630 02:07:44.179949 29015 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.9062
I0630 02:07:44.179967 29015 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.9959
I0630 02:07:44.179973 29015 solver.cpp:544]     Test net output #2: loss = 0.2254 (* 1 = 0.2254 loss)
I0630 02:07:44.179976 29015 solver.cpp:456] Optimization Done.
I0630 02:07:44.225229 29015 caffe.cpp:246] Optimization Done.
