I0801 13:47:26.329907 24466 caffe.cpp:608] This is NVCaffe 0.16.3 started at Tue Aug  1 13:47:26 2017
I0801 13:47:26.330034 24466 caffe.cpp:611] CuDNN version: 6021
I0801 13:47:26.330039 24466 caffe.cpp:612] CuBLAS version: 8000
I0801 13:47:26.330040 24466 caffe.cpp:613] CUDA version: 8000
I0801 13:47:26.330041 24466 caffe.cpp:614] CUDA driver version: 8000
I0801 13:47:26.581715 24466 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0801 13:47:26.582283 24466 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0801 13:47:26.582803 24466 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0801 13:47:26.583317 24466 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0801 13:47:26.583325 24466 caffe.cpp:208] Using GPUs 0, 1, 2
I0801 13:47:26.583647 24466 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0801 13:47:26.583971 24466 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0801 13:47:26.584291 24466 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0801 13:47:26.584326 24466 solver.cpp:42] Solver data type: FLOAT
I0801 13:47:26.584360 24466 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: true
iter_size: 1
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.02
sparsity_step_iter: 1000
sparsity_start_iter: 4000
sparsity_start_factor: 0
I0801 13:47:26.591029 24466 solver.cpp:77] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/train.prototxt
I0801 13:47:26.591450 24466 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0801 13:47:26.591457 24466 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0801 13:47:26.591512 24466 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0801 13:47:26.591699 24466 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 22
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0801 13:47:26.591804 24466 net.cpp:104] Using FLOAT as default forward math type
I0801 13:47:26.591809 24466 net.cpp:110] Using FLOAT as default backward math type
I0801 13:47:26.591812 24466 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0801 13:47:26.591815 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.591856 24466 net.cpp:184] Created Layer data (0)
I0801 13:47:26.591861 24466 net.cpp:530] data -> data
I0801 13:47:26.591873 24466 net.cpp:530] data -> label
I0801 13:47:26.591900 24466 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0801 13:47:26.591917 24466 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:47:26.593327 24487 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0801 13:47:26.594347 24466 data_layer.cpp:184] [0] ReshapePrefetch 22, 3, 32, 32
I0801 13:47:26.594415 24466 data_layer.cpp:208] [0] Output data size: 22, 3, 32, 32
I0801 13:47:26.594421 24466 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:47:26.594442 24466 net.cpp:245] Setting up data
I0801 13:47:26.594451 24466 net.cpp:252] TRAIN Top shape for layer 0 'data' 22 3 32 32 (67584)
I0801 13:47:26.594463 24466 net.cpp:252] TRAIN Top shape for layer 0 'data' 22 (22)
I0801 13:47:26.594470 24466 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0801 13:47:26.594475 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.594487 24466 net.cpp:184] Created Layer data/bias (1)
I0801 13:47:26.594492 24466 net.cpp:561] data/bias <- data
I0801 13:47:26.594501 24466 net.cpp:530] data/bias -> data/bias
I0801 13:47:26.596505 24466 net.cpp:245] Setting up data/bias
I0801 13:47:26.596515 24466 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 22 3 32 32 (67584)
I0801 13:47:26.596525 24466 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0801 13:47:26.596529 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.596544 24466 net.cpp:184] Created Layer conv1a (2)
I0801 13:47:26.596549 24466 net.cpp:561] conv1a <- data/bias
I0801 13:47:26.596552 24466 net.cpp:530] conv1a -> conv1a
I0801 13:47:26.892632 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.15G, req 0G)
I0801 13:47:26.892652 24466 net.cpp:245] Setting up conv1a
I0801 13:47:26.892659 24466 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 22 32 32 32 (720896)
I0801 13:47:26.892670 24466 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0801 13:47:26.892675 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.892688 24466 net.cpp:184] Created Layer conv1a/bn (3)
I0801 13:47:26.892693 24466 net.cpp:561] conv1a/bn <- conv1a
I0801 13:47:26.892699 24466 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0801 13:47:26.893388 24466 net.cpp:245] Setting up conv1a/bn
I0801 13:47:26.893398 24466 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 22 32 32 32 (720896)
I0801 13:47:26.893414 24466 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0801 13:47:26.893417 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.893425 24466 net.cpp:184] Created Layer conv1a/relu (4)
I0801 13:47:26.893429 24466 net.cpp:561] conv1a/relu <- conv1a
I0801 13:47:26.893432 24466 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0801 13:47:26.893447 24466 net.cpp:245] Setting up conv1a/relu
I0801 13:47:26.893451 24466 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 22 32 32 32 (720896)
I0801 13:47:26.893456 24466 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0801 13:47:26.893460 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.893471 24466 net.cpp:184] Created Layer conv1b (5)
I0801 13:47:26.893473 24466 net.cpp:561] conv1b <- conv1a
I0801 13:47:26.893478 24466 net.cpp:530] conv1b -> conv1b
I0801 13:47:26.900804 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 1 3  (limit 8.13G, req 0G)
I0801 13:47:26.900822 24466 net.cpp:245] Setting up conv1b
I0801 13:47:26.900830 24466 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 22 32 32 32 (720896)
I0801 13:47:26.900838 24466 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0801 13:47:26.900843 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.900861 24466 net.cpp:184] Created Layer conv1b/bn (6)
I0801 13:47:26.900864 24466 net.cpp:561] conv1b/bn <- conv1b
I0801 13:47:26.900868 24466 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0801 13:47:26.901500 24466 net.cpp:245] Setting up conv1b/bn
I0801 13:47:26.901510 24466 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 22 32 32 32 (720896)
I0801 13:47:26.901518 24466 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0801 13:47:26.901522 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.901528 24466 net.cpp:184] Created Layer conv1b/relu (7)
I0801 13:47:26.901531 24466 net.cpp:561] conv1b/relu <- conv1b
I0801 13:47:26.901535 24466 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0801 13:47:26.901541 24466 net.cpp:245] Setting up conv1b/relu
I0801 13:47:26.901546 24466 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 22 32 32 32 (720896)
I0801 13:47:26.901551 24466 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0801 13:47:26.901554 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.901563 24466 net.cpp:184] Created Layer pool1 (8)
I0801 13:47:26.901566 24466 net.cpp:561] pool1 <- conv1b
I0801 13:47:26.901569 24466 net.cpp:530] pool1 -> pool1
I0801 13:47:26.901635 24466 net.cpp:245] Setting up pool1
I0801 13:47:26.901641 24466 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 22 32 32 32 (720896)
I0801 13:47:26.901645 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0801 13:47:26.901649 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.901664 24466 net.cpp:184] Created Layer res2a_branch2a (9)
I0801 13:47:26.901669 24466 net.cpp:561] res2a_branch2a <- pool1
I0801 13:47:26.901674 24466 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0801 13:47:26.913063 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0G)
I0801 13:47:26.913074 24466 net.cpp:245] Setting up res2a_branch2a
I0801 13:47:26.913079 24466 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 22 64 32 32 (1441792)
I0801 13:47:26.913085 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0801 13:47:26.913089 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.913094 24466 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0801 13:47:26.913095 24466 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0801 13:47:26.913099 24466 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0801 13:47:26.913724 24466 net.cpp:245] Setting up res2a_branch2a/bn
I0801 13:47:26.913733 24466 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 22 64 32 32 (1441792)
I0801 13:47:26.913738 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0801 13:47:26.913740 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.913744 24466 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0801 13:47:26.913746 24466 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0801 13:47:26.913748 24466 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0801 13:47:26.913751 24466 net.cpp:245] Setting up res2a_branch2a/relu
I0801 13:47:26.913754 24466 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 22 64 32 32 (1441792)
I0801 13:47:26.913756 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0801 13:47:26.913758 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.913763 24466 net.cpp:184] Created Layer res2a_branch2b (12)
I0801 13:47:26.913766 24466 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0801 13:47:26.913769 24466 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0801 13:47:26.920666 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.1G, req 0G)
I0801 13:47:26.920680 24466 net.cpp:245] Setting up res2a_branch2b
I0801 13:47:26.920686 24466 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 22 64 32 32 (1441792)
I0801 13:47:26.920691 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0801 13:47:26.920696 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.920703 24466 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0801 13:47:26.920707 24466 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0801 13:47:26.920711 24466 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0801 13:47:26.921383 24466 net.cpp:245] Setting up res2a_branch2b/bn
I0801 13:47:26.921392 24466 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 22 64 32 32 (1441792)
I0801 13:47:26.921398 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0801 13:47:26.921401 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.921406 24466 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0801 13:47:26.921408 24466 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0801 13:47:26.921411 24466 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0801 13:47:26.921414 24466 net.cpp:245] Setting up res2a_branch2b/relu
I0801 13:47:26.921418 24466 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 22 64 32 32 (1441792)
I0801 13:47:26.921421 24466 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0801 13:47:26.921422 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.921428 24466 net.cpp:184] Created Layer pool2 (15)
I0801 13:47:26.921432 24466 net.cpp:561] pool2 <- res2a_branch2b
I0801 13:47:26.921433 24466 net.cpp:530] pool2 -> pool2
I0801 13:47:26.921499 24466 net.cpp:245] Setting up pool2
I0801 13:47:26.921505 24466 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 22 64 16 16 (360448)
I0801 13:47:26.921509 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0801 13:47:26.921511 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.921519 24466 net.cpp:184] Created Layer res3a_branch2a (16)
I0801 13:47:26.921521 24466 net.cpp:561] res3a_branch2a <- pool2
I0801 13:47:26.921525 24466 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0801 13:47:26.932653 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 8.09G, req 0G)
I0801 13:47:26.932669 24466 net.cpp:245] Setting up res3a_branch2a
I0801 13:47:26.932675 24466 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 22 128 16 16 (720896)
I0801 13:47:26.932680 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0801 13:47:26.932684 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.932693 24466 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0801 13:47:26.932696 24466 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0801 13:47:26.932699 24466 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0801 13:47:26.933353 24466 net.cpp:245] Setting up res3a_branch2a/bn
I0801 13:47:26.933362 24466 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 22 128 16 16 (720896)
I0801 13:47:26.933370 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0801 13:47:26.933374 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.933378 24466 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0801 13:47:26.933382 24466 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0801 13:47:26.933384 24466 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0801 13:47:26.933388 24466 net.cpp:245] Setting up res3a_branch2a/relu
I0801 13:47:26.933400 24466 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 22 128 16 16 (720896)
I0801 13:47:26.933403 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0801 13:47:26.933406 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.933414 24466 net.cpp:184] Created Layer res3a_branch2b (19)
I0801 13:47:26.933418 24466 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0801 13:47:26.933419 24466 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0801 13:47:26.938220 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.08G, req 0G)
I0801 13:47:26.938231 24466 net.cpp:245] Setting up res3a_branch2b
I0801 13:47:26.938236 24466 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 22 128 16 16 (720896)
I0801 13:47:26.938241 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0801 13:47:26.938244 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.938249 24466 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0801 13:47:26.938252 24466 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0801 13:47:26.938256 24466 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0801 13:47:26.938843 24466 net.cpp:245] Setting up res3a_branch2b/bn
I0801 13:47:26.938849 24466 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 22 128 16 16 (720896)
I0801 13:47:26.938855 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0801 13:47:26.938858 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.938863 24466 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0801 13:47:26.938865 24466 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0801 13:47:26.938868 24466 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0801 13:47:26.938871 24466 net.cpp:245] Setting up res3a_branch2b/relu
I0801 13:47:26.938874 24466 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 22 128 16 16 (720896)
I0801 13:47:26.938876 24466 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0801 13:47:26.938879 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.938882 24466 net.cpp:184] Created Layer pool3 (22)
I0801 13:47:26.938885 24466 net.cpp:561] pool3 <- res3a_branch2b
I0801 13:47:26.938887 24466 net.cpp:530] pool3 -> pool3
I0801 13:47:26.938946 24466 net.cpp:245] Setting up pool3
I0801 13:47:26.938951 24466 net.cpp:252] TRAIN Top shape for layer 22 'pool3' 22 128 16 16 (720896)
I0801 13:47:26.938953 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0801 13:47:26.938956 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.938961 24466 net.cpp:184] Created Layer res4a_branch2a (23)
I0801 13:47:26.938964 24466 net.cpp:561] res4a_branch2a <- pool3
I0801 13:47:26.938966 24466 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0801 13:47:26.958015 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.05G, req 0G)
I0801 13:47:26.958034 24466 net.cpp:245] Setting up res4a_branch2a
I0801 13:47:26.958039 24466 net.cpp:252] TRAIN Top shape for layer 23 'res4a_branch2a' 22 256 16 16 (1441792)
I0801 13:47:26.958045 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0801 13:47:26.958050 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.958063 24466 net.cpp:184] Created Layer res4a_branch2a/bn (24)
I0801 13:47:26.958066 24466 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0801 13:47:26.958070 24466 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0801 13:47:26.958765 24466 net.cpp:245] Setting up res4a_branch2a/bn
I0801 13:47:26.958773 24466 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a/bn' 22 256 16 16 (1441792)
I0801 13:47:26.958789 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0801 13:47:26.958792 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.958796 24466 net.cpp:184] Created Layer res4a_branch2a/relu (25)
I0801 13:47:26.958798 24466 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0801 13:47:26.958801 24466 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0801 13:47:26.958804 24466 net.cpp:245] Setting up res4a_branch2a/relu
I0801 13:47:26.958808 24466 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/relu' 22 256 16 16 (1441792)
I0801 13:47:26.958811 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0801 13:47:26.958814 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.958822 24466 net.cpp:184] Created Layer res4a_branch2b (26)
I0801 13:47:26.958824 24466 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0801 13:47:26.958827 24466 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0801 13:47:26.967553 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.04G, req 0G)
I0801 13:47:26.967566 24466 net.cpp:245] Setting up res4a_branch2b
I0801 13:47:26.967571 24466 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2b' 22 256 16 16 (1441792)
I0801 13:47:26.967576 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0801 13:47:26.967579 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.967583 24466 net.cpp:184] Created Layer res4a_branch2b/bn (27)
I0801 13:47:26.967586 24466 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0801 13:47:26.967589 24466 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0801 13:47:26.968232 24466 net.cpp:245] Setting up res4a_branch2b/bn
I0801 13:47:26.968240 24466 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b/bn' 22 256 16 16 (1441792)
I0801 13:47:26.968246 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0801 13:47:26.968250 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.968253 24466 net.cpp:184] Created Layer res4a_branch2b/relu (28)
I0801 13:47:26.968256 24466 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0801 13:47:26.968258 24466 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0801 13:47:26.968262 24466 net.cpp:245] Setting up res4a_branch2b/relu
I0801 13:47:26.968266 24466 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/relu' 22 256 16 16 (1441792)
I0801 13:47:26.968267 24466 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0801 13:47:26.968271 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.968274 24466 net.cpp:184] Created Layer pool4 (29)
I0801 13:47:26.968276 24466 net.cpp:561] pool4 <- res4a_branch2b
I0801 13:47:26.968279 24466 net.cpp:530] pool4 -> pool4
I0801 13:47:26.968349 24466 net.cpp:245] Setting up pool4
I0801 13:47:26.968355 24466 net.cpp:252] TRAIN Top shape for layer 29 'pool4' 22 256 8 8 (360448)
I0801 13:47:26.968358 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0801 13:47:26.968361 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:26.968370 24466 net.cpp:184] Created Layer res5a_branch2a (30)
I0801 13:47:26.968374 24466 net.cpp:561] res5a_branch2a <- pool4
I0801 13:47:26.968376 24466 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0801 13:47:27.013043 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.02G, req 0.01G)
I0801 13:47:27.013062 24466 net.cpp:245] Setting up res5a_branch2a
I0801 13:47:27.013067 24466 net.cpp:252] TRAIN Top shape for layer 30 'res5a_branch2a' 22 512 8 8 (720896)
I0801 13:47:27.013087 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0801 13:47:27.013090 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.013098 24466 net.cpp:184] Created Layer res5a_branch2a/bn (31)
I0801 13:47:27.013100 24466 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0801 13:47:27.013104 24466 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0801 13:47:27.013770 24466 net.cpp:245] Setting up res5a_branch2a/bn
I0801 13:47:27.013779 24466 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a/bn' 22 512 8 8 (720896)
I0801 13:47:27.013784 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0801 13:47:27.013787 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.013792 24466 net.cpp:184] Created Layer res5a_branch2a/relu (32)
I0801 13:47:27.013794 24466 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0801 13:47:27.013797 24466 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0801 13:47:27.013800 24466 net.cpp:245] Setting up res5a_branch2a/relu
I0801 13:47:27.013803 24466 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/relu' 22 512 8 8 (720896)
I0801 13:47:27.013804 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0801 13:47:27.013808 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.013819 24466 net.cpp:184] Created Layer res5a_branch2b (33)
I0801 13:47:27.013823 24466 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0801 13:47:27.013824 24466 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0801 13:47:27.033175 24466 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8G, req 0.01G)
I0801 13:47:27.033190 24466 net.cpp:245] Setting up res5a_branch2b
I0801 13:47:27.033195 24466 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2b' 22 512 8 8 (720896)
I0801 13:47:27.033205 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0801 13:47:27.033208 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.033221 24466 net.cpp:184] Created Layer res5a_branch2b/bn (34)
I0801 13:47:27.033226 24466 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0801 13:47:27.033228 24466 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0801 13:47:27.033893 24466 net.cpp:245] Setting up res5a_branch2b/bn
I0801 13:47:27.033901 24466 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b/bn' 22 512 8 8 (720896)
I0801 13:47:27.033908 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0801 13:47:27.033911 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.033915 24466 net.cpp:184] Created Layer res5a_branch2b/relu (35)
I0801 13:47:27.033918 24466 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0801 13:47:27.033921 24466 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0801 13:47:27.033926 24466 net.cpp:245] Setting up res5a_branch2b/relu
I0801 13:47:27.033928 24466 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/relu' 22 512 8 8 (720896)
I0801 13:47:27.033931 24466 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0801 13:47:27.033933 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.033938 24466 net.cpp:184] Created Layer pool5 (36)
I0801 13:47:27.033941 24466 net.cpp:561] pool5 <- res5a_branch2b
I0801 13:47:27.033943 24466 net.cpp:530] pool5 -> pool5
I0801 13:47:27.033968 24466 net.cpp:245] Setting up pool5
I0801 13:47:27.033973 24466 net.cpp:252] TRAIN Top shape for layer 36 'pool5' 22 512 1 1 (11264)
I0801 13:47:27.033977 24466 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0801 13:47:27.033978 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.033993 24466 net.cpp:184] Created Layer fc10 (37)
I0801 13:47:27.033996 24466 net.cpp:561] fc10 <- pool5
I0801 13:47:27.033998 24466 net.cpp:530] fc10 -> fc10
I0801 13:47:27.034286 24466 net.cpp:245] Setting up fc10
I0801 13:47:27.034293 24466 net.cpp:252] TRAIN Top shape for layer 37 'fc10' 22 10 (220)
I0801 13:47:27.034298 24466 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0801 13:47:27.034301 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.034312 24466 net.cpp:184] Created Layer loss (38)
I0801 13:47:27.034314 24466 net.cpp:561] loss <- fc10
I0801 13:47:27.034317 24466 net.cpp:561] loss <- label
I0801 13:47:27.034322 24466 net.cpp:530] loss -> loss
I0801 13:47:27.034478 24466 net.cpp:245] Setting up loss
I0801 13:47:27.034485 24466 net.cpp:252] TRAIN Top shape for layer 38 'loss' (1)
I0801 13:47:27.034487 24466 net.cpp:256]     with loss weight 1
I0801 13:47:27.034492 24466 net.cpp:323] loss needs backward computation.
I0801 13:47:27.034495 24466 net.cpp:323] fc10 needs backward computation.
I0801 13:47:27.034497 24466 net.cpp:323] pool5 needs backward computation.
I0801 13:47:27.034500 24466 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0801 13:47:27.034502 24466 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0801 13:47:27.034504 24466 net.cpp:323] res5a_branch2b needs backward computation.
I0801 13:47:27.034507 24466 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0801 13:47:27.034509 24466 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0801 13:47:27.034512 24466 net.cpp:323] res5a_branch2a needs backward computation.
I0801 13:47:27.034513 24466 net.cpp:323] pool4 needs backward computation.
I0801 13:47:27.034517 24466 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0801 13:47:27.034518 24466 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0801 13:47:27.034520 24466 net.cpp:323] res4a_branch2b needs backward computation.
I0801 13:47:27.034523 24466 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0801 13:47:27.034525 24466 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0801 13:47:27.034528 24466 net.cpp:323] res4a_branch2a needs backward computation.
I0801 13:47:27.034529 24466 net.cpp:323] pool3 needs backward computation.
I0801 13:47:27.034531 24466 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0801 13:47:27.034534 24466 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0801 13:47:27.034535 24466 net.cpp:323] res3a_branch2b needs backward computation.
I0801 13:47:27.034538 24466 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0801 13:47:27.034540 24466 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0801 13:47:27.034543 24466 net.cpp:323] res3a_branch2a needs backward computation.
I0801 13:47:27.034545 24466 net.cpp:323] pool2 needs backward computation.
I0801 13:47:27.034548 24466 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0801 13:47:27.034549 24466 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0801 13:47:27.034551 24466 net.cpp:323] res2a_branch2b needs backward computation.
I0801 13:47:27.034554 24466 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0801 13:47:27.034556 24466 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0801 13:47:27.034559 24466 net.cpp:323] res2a_branch2a needs backward computation.
I0801 13:47:27.034560 24466 net.cpp:323] pool1 needs backward computation.
I0801 13:47:27.034562 24466 net.cpp:323] conv1b/relu needs backward computation.
I0801 13:47:27.034564 24466 net.cpp:323] conv1b/bn needs backward computation.
I0801 13:47:27.034566 24466 net.cpp:323] conv1b needs backward computation.
I0801 13:47:27.034569 24466 net.cpp:323] conv1a/relu needs backward computation.
I0801 13:47:27.034570 24466 net.cpp:323] conv1a/bn needs backward computation.
I0801 13:47:27.034572 24466 net.cpp:323] conv1a needs backward computation.
I0801 13:47:27.034580 24466 net.cpp:325] data/bias does not need backward computation.
I0801 13:47:27.034584 24466 net.cpp:325] data does not need backward computation.
I0801 13:47:27.034586 24466 net.cpp:367] This network produces output loss
I0801 13:47:27.034615 24466 net.cpp:389] Top memory (TRAIN) required for data: 121110528 diff: 121110536
I0801 13:47:27.034617 24466 net.cpp:392] Bottom memory (TRAIN) required for data: 121110528 diff: 121110528
I0801 13:47:27.034620 24466 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 80740352 diff: 80740352
I0801 13:47:27.034621 24466 net.cpp:398] Parameters memory (TRAIN) required for data: 9450960 diff: 9450960
I0801 13:47:27.034623 24466 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0801 13:47:27.034626 24466 net.cpp:407] Network initialization done.
I0801 13:47:27.034976 24466 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/test.prototxt
W0801 13:47:27.035027 24466 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0801 13:47:27.035150 24466 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 17
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0801 13:47:27.035238 24466 net.cpp:104] Using FLOAT as default forward math type
I0801 13:47:27.035243 24466 net.cpp:110] Using FLOAT as default backward math type
I0801 13:47:27.035245 24466 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0801 13:47:27.035248 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.035257 24466 net.cpp:184] Created Layer data (0)
I0801 13:47:27.035260 24466 net.cpp:530] data -> data
I0801 13:47:27.035264 24466 net.cpp:530] data -> label
I0801 13:47:27.035269 24466 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0801 13:47:27.035274 24466 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:47:27.036044 24520 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0801 13:47:27.036108 24466 data_layer.cpp:184] (0) ReshapePrefetch 17, 3, 32, 32
I0801 13:47:27.036170 24466 data_layer.cpp:208] (0) Output data size: 17, 3, 32, 32
I0801 13:47:27.036175 24466 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:47:27.036186 24466 net.cpp:245] Setting up data
I0801 13:47:27.036191 24466 net.cpp:252] TEST Top shape for layer 0 'data' 17 3 32 32 (52224)
I0801 13:47:27.036195 24466 net.cpp:252] TEST Top shape for layer 0 'data' 17 (17)
I0801 13:47:27.036196 24466 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0801 13:47:27.036200 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.036209 24466 net.cpp:184] Created Layer label_data_1_split (1)
I0801 13:47:27.036211 24466 net.cpp:561] label_data_1_split <- label
I0801 13:47:27.036214 24466 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0801 13:47:27.036217 24466 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0801 13:47:27.036221 24466 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0801 13:47:27.036283 24466 net.cpp:245] Setting up label_data_1_split
I0801 13:47:27.036288 24466 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0801 13:47:27.036290 24466 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0801 13:47:27.036293 24466 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 17 (17)
I0801 13:47:27.036295 24466 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0801 13:47:27.036298 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.036303 24466 net.cpp:184] Created Layer data/bias (2)
I0801 13:47:27.036305 24466 net.cpp:561] data/bias <- data
I0801 13:47:27.036308 24466 net.cpp:530] data/bias -> data/bias
I0801 13:47:27.036432 24466 net.cpp:245] Setting up data/bias
I0801 13:47:27.036439 24466 net.cpp:252] TEST Top shape for layer 2 'data/bias' 17 3 32 32 (52224)
I0801 13:47:27.036443 24466 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0801 13:47:27.036447 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.036453 24466 net.cpp:184] Created Layer conv1a (3)
I0801 13:47:27.036456 24466 net.cpp:561] conv1a <- data/bias
I0801 13:47:27.036458 24466 net.cpp:530] conv1a -> conv1a
I0801 13:47:27.036857 24521 data_layer.cpp:97] (0) Parser threads: 1
I0801 13:47:27.036865 24521 data_layer.cpp:99] (0) Transformer threads: 1
I0801 13:47:27.039867 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8G, req 0.01G)
I0801 13:47:27.039881 24466 net.cpp:245] Setting up conv1a
I0801 13:47:27.039888 24466 net.cpp:252] TEST Top shape for layer 3 'conv1a' 17 32 32 32 (557056)
I0801 13:47:27.039896 24466 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0801 13:47:27.039899 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.039906 24466 net.cpp:184] Created Layer conv1a/bn (4)
I0801 13:47:27.039911 24466 net.cpp:561] conv1a/bn <- conv1a
I0801 13:47:27.039914 24466 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0801 13:47:27.040726 24466 net.cpp:245] Setting up conv1a/bn
I0801 13:47:27.040736 24466 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 17 32 32 32 (557056)
I0801 13:47:27.040745 24466 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0801 13:47:27.040748 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.040752 24466 net.cpp:184] Created Layer conv1a/relu (5)
I0801 13:47:27.040755 24466 net.cpp:561] conv1a/relu <- conv1a
I0801 13:47:27.040757 24466 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0801 13:47:27.040762 24466 net.cpp:245] Setting up conv1a/relu
I0801 13:47:27.040766 24466 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 17 32 32 32 (557056)
I0801 13:47:27.040767 24466 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0801 13:47:27.040771 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.040781 24466 net.cpp:184] Created Layer conv1b (6)
I0801 13:47:27.040783 24466 net.cpp:561] conv1b <- conv1a
I0801 13:47:27.040786 24466 net.cpp:530] conv1b -> conv1b
I0801 13:47:27.043931 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8G, req 0.01G)
I0801 13:47:27.043941 24466 net.cpp:245] Setting up conv1b
I0801 13:47:27.043946 24466 net.cpp:252] TEST Top shape for layer 6 'conv1b' 17 32 32 32 (557056)
I0801 13:47:27.043952 24466 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0801 13:47:27.043964 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.043972 24466 net.cpp:184] Created Layer conv1b/bn (7)
I0801 13:47:27.043974 24466 net.cpp:561] conv1b/bn <- conv1b
I0801 13:47:27.043978 24466 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0801 13:47:27.044643 24466 net.cpp:245] Setting up conv1b/bn
I0801 13:47:27.044651 24466 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 17 32 32 32 (557056)
I0801 13:47:27.044656 24466 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0801 13:47:27.044659 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.044663 24466 net.cpp:184] Created Layer conv1b/relu (8)
I0801 13:47:27.044667 24466 net.cpp:561] conv1b/relu <- conv1b
I0801 13:47:27.044669 24466 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0801 13:47:27.044673 24466 net.cpp:245] Setting up conv1b/relu
I0801 13:47:27.044677 24466 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 17 32 32 32 (557056)
I0801 13:47:27.044679 24466 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0801 13:47:27.044682 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.044687 24466 net.cpp:184] Created Layer pool1 (9)
I0801 13:47:27.044689 24466 net.cpp:561] pool1 <- conv1b
I0801 13:47:27.044692 24466 net.cpp:530] pool1 -> pool1
I0801 13:47:27.044757 24466 net.cpp:245] Setting up pool1
I0801 13:47:27.044762 24466 net.cpp:252] TEST Top shape for layer 9 'pool1' 17 32 32 32 (557056)
I0801 13:47:27.044765 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0801 13:47:27.044767 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.044778 24466 net.cpp:184] Created Layer res2a_branch2a (10)
I0801 13:47:27.044781 24466 net.cpp:561] res2a_branch2a <- pool1
I0801 13:47:27.044785 24466 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0801 13:47:27.048420 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.99G, req 0.01G)
I0801 13:47:27.048430 24466 net.cpp:245] Setting up res2a_branch2a
I0801 13:47:27.048435 24466 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 17 64 32 32 (1114112)
I0801 13:47:27.048441 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0801 13:47:27.048444 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.048450 24466 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0801 13:47:27.048454 24466 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0801 13:47:27.048456 24466 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0801 13:47:27.049150 24466 net.cpp:245] Setting up res2a_branch2a/bn
I0801 13:47:27.049159 24466 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 17 64 32 32 (1114112)
I0801 13:47:27.049165 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0801 13:47:27.049167 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.049171 24466 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0801 13:47:27.049175 24466 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0801 13:47:27.049176 24466 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0801 13:47:27.049182 24466 net.cpp:245] Setting up res2a_branch2a/relu
I0801 13:47:27.049185 24466 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 17 64 32 32 (1114112)
I0801 13:47:27.049188 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0801 13:47:27.049191 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.049197 24466 net.cpp:184] Created Layer res2a_branch2b (13)
I0801 13:47:27.049201 24466 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0801 13:47:27.049203 24466 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0801 13:47:27.052304 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.98G, req 0.01G)
I0801 13:47:27.052312 24466 net.cpp:245] Setting up res2a_branch2b
I0801 13:47:27.052316 24466 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 17 64 32 32 (1114112)
I0801 13:47:27.052321 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0801 13:47:27.052325 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.052330 24466 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0801 13:47:27.052333 24466 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0801 13:47:27.052336 24466 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0801 13:47:27.053000 24466 net.cpp:245] Setting up res2a_branch2b/bn
I0801 13:47:27.053009 24466 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 17 64 32 32 (1114112)
I0801 13:47:27.053014 24466 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0801 13:47:27.053017 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.053021 24466 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0801 13:47:27.053025 24466 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0801 13:47:27.053027 24466 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0801 13:47:27.053031 24466 net.cpp:245] Setting up res2a_branch2b/relu
I0801 13:47:27.053035 24466 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 17 64 32 32 (1114112)
I0801 13:47:27.053036 24466 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0801 13:47:27.053040 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.053043 24466 net.cpp:184] Created Layer pool2 (16)
I0801 13:47:27.053046 24466 net.cpp:561] pool2 <- res2a_branch2b
I0801 13:47:27.053048 24466 net.cpp:530] pool2 -> pool2
I0801 13:47:27.053108 24466 net.cpp:245] Setting up pool2
I0801 13:47:27.053112 24466 net.cpp:252] TEST Top shape for layer 16 'pool2' 17 64 16 16 (278528)
I0801 13:47:27.053115 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0801 13:47:27.053118 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.053124 24466 net.cpp:184] Created Layer res3a_branch2a (17)
I0801 13:47:27.053128 24466 net.cpp:561] res3a_branch2a <- pool2
I0801 13:47:27.053129 24466 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0801 13:47:27.059074 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.97G, req 0.01G)
I0801 13:47:27.059085 24466 net.cpp:245] Setting up res3a_branch2a
I0801 13:47:27.059090 24466 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 17 128 16 16 (557056)
I0801 13:47:27.059095 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0801 13:47:27.059098 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.059103 24466 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0801 13:47:27.059106 24466 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0801 13:47:27.059110 24466 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0801 13:47:27.059762 24466 net.cpp:245] Setting up res3a_branch2a/bn
I0801 13:47:27.059769 24466 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 17 128 16 16 (557056)
I0801 13:47:27.059777 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0801 13:47:27.059780 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.059784 24466 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0801 13:47:27.059787 24466 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0801 13:47:27.059792 24466 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0801 13:47:27.059795 24466 net.cpp:245] Setting up res3a_branch2a/relu
I0801 13:47:27.059806 24466 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 17 128 16 16 (557056)
I0801 13:47:27.059809 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0801 13:47:27.059811 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.059818 24466 net.cpp:184] Created Layer res3a_branch2b (20)
I0801 13:47:27.059821 24466 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0801 13:47:27.059823 24466 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0801 13:47:27.063128 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.97G, req 0.01G)
I0801 13:47:27.063138 24466 net.cpp:245] Setting up res3a_branch2b
I0801 13:47:27.063143 24466 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 17 128 16 16 (557056)
I0801 13:47:27.063146 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0801 13:47:27.063150 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.063159 24466 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0801 13:47:27.063163 24466 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0801 13:47:27.063165 24466 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0801 13:47:27.063822 24466 net.cpp:245] Setting up res3a_branch2b/bn
I0801 13:47:27.063829 24466 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 17 128 16 16 (557056)
I0801 13:47:27.063835 24466 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0801 13:47:27.063838 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.063841 24466 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0801 13:47:27.063844 24466 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0801 13:47:27.063848 24466 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0801 13:47:27.063851 24466 net.cpp:245] Setting up res3a_branch2b/relu
I0801 13:47:27.063854 24466 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 17 128 16 16 (557056)
I0801 13:47:27.063858 24466 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0801 13:47:27.063859 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.063863 24466 net.cpp:184] Created Layer pool3 (23)
I0801 13:47:27.063866 24466 net.cpp:561] pool3 <- res3a_branch2b
I0801 13:47:27.063868 24466 net.cpp:530] pool3 -> pool3
I0801 13:47:27.063928 24466 net.cpp:245] Setting up pool3
I0801 13:47:27.063932 24466 net.cpp:252] TEST Top shape for layer 23 'pool3' 17 128 16 16 (557056)
I0801 13:47:27.063935 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0801 13:47:27.063938 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.063953 24466 net.cpp:184] Created Layer res4a_branch2a (24)
I0801 13:47:27.063956 24466 net.cpp:561] res4a_branch2a <- pool3
I0801 13:47:27.063959 24466 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0801 13:47:27.074946 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0.01G)
I0801 13:47:27.074961 24466 net.cpp:245] Setting up res4a_branch2a
I0801 13:47:27.074966 24466 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 17 256 16 16 (1114112)
I0801 13:47:27.074973 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0801 13:47:27.074976 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.074985 24466 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0801 13:47:27.074987 24466 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0801 13:47:27.074990 24466 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0801 13:47:27.075716 24466 net.cpp:245] Setting up res4a_branch2a/bn
I0801 13:47:27.075723 24466 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 17 256 16 16 (1114112)
I0801 13:47:27.075739 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0801 13:47:27.075743 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.075747 24466 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0801 13:47:27.075748 24466 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0801 13:47:27.075752 24466 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0801 13:47:27.075754 24466 net.cpp:245] Setting up res4a_branch2a/relu
I0801 13:47:27.075757 24466 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 17 256 16 16 (1114112)
I0801 13:47:27.075759 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0801 13:47:27.075762 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.075773 24466 net.cpp:184] Created Layer res4a_branch2b (27)
I0801 13:47:27.075775 24466 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0801 13:47:27.075778 24466 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0801 13:47:27.081995 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.95G, req 0.01G)
I0801 13:47:27.082006 24466 net.cpp:245] Setting up res4a_branch2b
I0801 13:47:27.082011 24466 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 17 256 16 16 (1114112)
I0801 13:47:27.082015 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0801 13:47:27.082018 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.082025 24466 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0801 13:47:27.082026 24466 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0801 13:47:27.082029 24466 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0801 13:47:27.082783 24466 net.cpp:245] Setting up res4a_branch2b/bn
I0801 13:47:27.082792 24466 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 17 256 16 16 (1114112)
I0801 13:47:27.082798 24466 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0801 13:47:27.082800 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.082803 24466 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0801 13:47:27.082806 24466 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0801 13:47:27.082808 24466 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0801 13:47:27.082813 24466 net.cpp:245] Setting up res4a_branch2b/relu
I0801 13:47:27.082814 24466 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 17 256 16 16 (1114112)
I0801 13:47:27.082816 24466 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0801 13:47:27.082818 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.082823 24466 net.cpp:184] Created Layer pool4 (30)
I0801 13:47:27.082824 24466 net.cpp:561] pool4 <- res4a_branch2b
I0801 13:47:27.082828 24466 net.cpp:530] pool4 -> pool4
I0801 13:47:27.082895 24466 net.cpp:245] Setting up pool4
I0801 13:47:27.082900 24466 net.cpp:252] TEST Top shape for layer 30 'pool4' 17 256 8 8 (278528)
I0801 13:47:27.082901 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0801 13:47:27.082904 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.082911 24466 net.cpp:184] Created Layer res5a_branch2a (31)
I0801 13:47:27.082912 24466 net.cpp:561] res5a_branch2a <- pool4
I0801 13:47:27.082914 24466 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0801 13:47:27.115054 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.94G, req 0.01G)
I0801 13:47:27.115072 24466 net.cpp:245] Setting up res5a_branch2a
I0801 13:47:27.115077 24466 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 17 512 8 8 (557056)
I0801 13:47:27.115083 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0801 13:47:27.115100 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.115109 24466 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0801 13:47:27.115113 24466 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0801 13:47:27.115118 24466 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0801 13:47:27.115838 24466 net.cpp:245] Setting up res5a_branch2a/bn
I0801 13:47:27.115845 24466 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 17 512 8 8 (557056)
I0801 13:47:27.115851 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0801 13:47:27.115854 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.115859 24466 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0801 13:47:27.115861 24466 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0801 13:47:27.115864 24466 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0801 13:47:27.115867 24466 net.cpp:245] Setting up res5a_branch2a/relu
I0801 13:47:27.115869 24466 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 17 512 8 8 (557056)
I0801 13:47:27.115871 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0801 13:47:27.115875 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.115881 24466 net.cpp:184] Created Layer res5a_branch2b (34)
I0801 13:47:27.115883 24466 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0801 13:47:27.115886 24466 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0801 13:47:27.133141 24466 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.93G, req 0.01G)
I0801 13:47:27.133159 24466 net.cpp:245] Setting up res5a_branch2b
I0801 13:47:27.133165 24466 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 17 512 8 8 (557056)
I0801 13:47:27.133178 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0801 13:47:27.133183 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.133194 24466 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0801 13:47:27.133198 24466 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0801 13:47:27.133203 24466 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0801 13:47:27.134007 24466 net.cpp:245] Setting up res5a_branch2b/bn
I0801 13:47:27.134017 24466 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 17 512 8 8 (557056)
I0801 13:47:27.134026 24466 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0801 13:47:27.134029 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.134038 24466 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0801 13:47:27.134042 24466 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0801 13:47:27.134045 24466 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0801 13:47:27.134050 24466 net.cpp:245] Setting up res5a_branch2b/relu
I0801 13:47:27.134053 24466 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 17 512 8 8 (557056)
I0801 13:47:27.134057 24466 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0801 13:47:27.134059 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.134065 24466 net.cpp:184] Created Layer pool5 (37)
I0801 13:47:27.134068 24466 net.cpp:561] pool5 <- res5a_branch2b
I0801 13:47:27.134070 24466 net.cpp:530] pool5 -> pool5
I0801 13:47:27.134099 24466 net.cpp:245] Setting up pool5
I0801 13:47:27.134104 24466 net.cpp:252] TEST Top shape for layer 37 'pool5' 17 512 1 1 (8704)
I0801 13:47:27.134106 24466 layer_factory.hpp:136] Creating layer 'fc10' of type 'InnerProduct'
I0801 13:47:27.134110 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.134116 24466 net.cpp:184] Created Layer fc10 (38)
I0801 13:47:27.134130 24466 net.cpp:561] fc10 <- pool5
I0801 13:47:27.134132 24466 net.cpp:530] fc10 -> fc10
I0801 13:47:27.134435 24466 net.cpp:245] Setting up fc10
I0801 13:47:27.134443 24466 net.cpp:252] TEST Top shape for layer 38 'fc10' 17 10 (170)
I0801 13:47:27.134447 24466 layer_factory.hpp:136] Creating layer 'fc10_fc10_0_split' of type 'Split'
I0801 13:47:27.134450 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.134454 24466 net.cpp:184] Created Layer fc10_fc10_0_split (39)
I0801 13:47:27.134457 24466 net.cpp:561] fc10_fc10_0_split <- fc10
I0801 13:47:27.134460 24466 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0801 13:47:27.134464 24466 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0801 13:47:27.134467 24466 net.cpp:530] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0801 13:47:27.134541 24466 net.cpp:245] Setting up fc10_fc10_0_split
I0801 13:47:27.134546 24466 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0801 13:47:27.134548 24466 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0801 13:47:27.134552 24466 net.cpp:252] TEST Top shape for layer 39 'fc10_fc10_0_split' 17 10 (170)
I0801 13:47:27.134555 24466 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0801 13:47:27.134557 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.134567 24466 net.cpp:184] Created Layer loss (40)
I0801 13:47:27.134570 24466 net.cpp:561] loss <- fc10_fc10_0_split_0
I0801 13:47:27.134572 24466 net.cpp:561] loss <- label_data_1_split_0
I0801 13:47:27.134577 24466 net.cpp:530] loss -> loss
I0801 13:47:27.134747 24466 net.cpp:245] Setting up loss
I0801 13:47:27.134754 24466 net.cpp:252] TEST Top shape for layer 40 'loss' (1)
I0801 13:47:27.134758 24466 net.cpp:256]     with loss weight 1
I0801 13:47:27.134764 24466 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0801 13:47:27.134769 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.134783 24466 net.cpp:184] Created Layer accuracy/top1 (41)
I0801 13:47:27.134788 24466 net.cpp:561] accuracy/top1 <- fc10_fc10_0_split_1
I0801 13:47:27.134791 24466 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0801 13:47:27.134795 24466 net.cpp:530] accuracy/top1 -> accuracy/top1
I0801 13:47:27.134802 24466 net.cpp:245] Setting up accuracy/top1
I0801 13:47:27.134806 24466 net.cpp:252] TEST Top shape for layer 41 'accuracy/top1' (1)
I0801 13:47:27.134810 24466 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0801 13:47:27.134814 24466 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0801 13:47:27.134819 24466 net.cpp:184] Created Layer accuracy/top5 (42)
I0801 13:47:27.134824 24466 net.cpp:561] accuracy/top5 <- fc10_fc10_0_split_2
I0801 13:47:27.134827 24466 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0801 13:47:27.134832 24466 net.cpp:530] accuracy/top5 -> accuracy/top5
I0801 13:47:27.134838 24466 net.cpp:245] Setting up accuracy/top5
I0801 13:47:27.134842 24466 net.cpp:252] TEST Top shape for layer 42 'accuracy/top5' (1)
I0801 13:47:27.134846 24466 net.cpp:325] accuracy/top5 does not need backward computation.
I0801 13:47:27.134851 24466 net.cpp:325] accuracy/top1 does not need backward computation.
I0801 13:47:27.134855 24466 net.cpp:323] loss needs backward computation.
I0801 13:47:27.134860 24466 net.cpp:323] fc10_fc10_0_split needs backward computation.
I0801 13:47:27.134863 24466 net.cpp:323] fc10 needs backward computation.
I0801 13:47:27.134867 24466 net.cpp:323] pool5 needs backward computation.
I0801 13:47:27.134871 24466 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0801 13:47:27.134874 24466 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0801 13:47:27.134878 24466 net.cpp:323] res5a_branch2b needs backward computation.
I0801 13:47:27.134882 24466 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0801 13:47:27.134891 24466 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0801 13:47:27.134896 24466 net.cpp:323] res5a_branch2a needs backward computation.
I0801 13:47:27.134901 24466 net.cpp:323] pool4 needs backward computation.
I0801 13:47:27.134904 24466 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0801 13:47:27.134907 24466 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0801 13:47:27.134912 24466 net.cpp:323] res4a_branch2b needs backward computation.
I0801 13:47:27.134915 24466 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0801 13:47:27.134919 24466 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0801 13:47:27.134923 24466 net.cpp:323] res4a_branch2a needs backward computation.
I0801 13:47:27.134927 24466 net.cpp:323] pool3 needs backward computation.
I0801 13:47:27.134932 24466 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0801 13:47:27.134935 24466 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0801 13:47:27.134939 24466 net.cpp:323] res3a_branch2b needs backward computation.
I0801 13:47:27.134943 24466 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0801 13:47:27.134946 24466 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0801 13:47:27.134950 24466 net.cpp:323] res3a_branch2a needs backward computation.
I0801 13:47:27.134954 24466 net.cpp:323] pool2 needs backward computation.
I0801 13:47:27.134958 24466 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0801 13:47:27.134961 24466 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0801 13:47:27.134965 24466 net.cpp:323] res2a_branch2b needs backward computation.
I0801 13:47:27.134969 24466 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0801 13:47:27.134973 24466 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0801 13:47:27.134976 24466 net.cpp:323] res2a_branch2a needs backward computation.
I0801 13:47:27.134980 24466 net.cpp:323] pool1 needs backward computation.
I0801 13:47:27.134984 24466 net.cpp:323] conv1b/relu needs backward computation.
I0801 13:47:27.134989 24466 net.cpp:323] conv1b/bn needs backward computation.
I0801 13:47:27.134992 24466 net.cpp:323] conv1b needs backward computation.
I0801 13:47:27.134995 24466 net.cpp:323] conv1a/relu needs backward computation.
I0801 13:47:27.134999 24466 net.cpp:323] conv1a/bn needs backward computation.
I0801 13:47:27.135004 24466 net.cpp:323] conv1a needs backward computation.
I0801 13:47:27.135007 24466 net.cpp:325] data/bias does not need backward computation.
I0801 13:47:27.135012 24466 net.cpp:325] label_data_1_split does not need backward computation.
I0801 13:47:27.135016 24466 net.cpp:325] data does not need backward computation.
I0801 13:47:27.135020 24466 net.cpp:367] This network produces output accuracy/top1
I0801 13:47:27.135025 24466 net.cpp:367] This network produces output accuracy/top5
I0801 13:47:27.135027 24466 net.cpp:367] This network produces output loss
I0801 13:47:27.135057 24466 net.cpp:389] Top memory (TEST) required for data: 93585408 diff: 8
I0801 13:47:27.135061 24466 net.cpp:392] Bottom memory (TEST) required for data: 93585408 diff: 93585408
I0801 13:47:27.135064 24466 net.cpp:395] Shared (in-place) memory (TEST) by data: 62390272 diff: 62390272
I0801 13:47:27.135067 24466 net.cpp:398] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0801 13:47:27.135071 24466 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0801 13:47:27.135076 24466 net.cpp:407] Network initialization done.
I0801 13:47:27.135128 24466 solver.cpp:56] Solver scaffolding done.
I0801 13:47:27.139250 24466 caffe.cpp:137] Finetuning from training/cifar10_jacintonet11v2_2017-08-01_13-11-28/l1reg/cifar10_jacintonet11v2_iter_64000.caffemodel
I0801 13:47:27.143579 24466 net.cpp:1089] Copying source layer data Type:Data #blobs=0
I0801 13:47:27.143599 24466 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0801 13:47:27.143635 24466 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0801 13:47:27.143658 24466 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.143918 24466 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0801 13:47:27.143923 24466 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0801 13:47:27.143934 24466 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.144088 24466 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0801 13:47:27.144093 24466 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0801 13:47:27.144096 24466 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0801 13:47:27.144114 24466 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.144268 24466 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0801 13:47:27.144273 24466 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0801 13:47:27.144286 24466 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.144428 24466 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0801 13:47:27.144433 24466 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0801 13:47:27.144438 24466 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0801 13:47:27.144476 24466 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.144608 24466 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0801 13:47:27.144613 24466 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0801 13:47:27.144635 24466 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.144755 24466 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0801 13:47:27.144759 24466 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0801 13:47:27.144763 24466 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0801 13:47:27.144888 24466 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.145015 24466 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0801 13:47:27.145020 24466 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0801 13:47:27.145081 24466 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.145202 24466 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0801 13:47:27.145207 24466 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0801 13:47:27.145210 24466 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0801 13:47:27.145592 24466 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.145725 24466 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0801 13:47:27.145730 24466 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0801 13:47:27.145889 24466 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.146013 24466 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0801 13:47:27.146019 24466 net.cpp:1089] Copying source layer pool5 Type:Pooling #blobs=0
I0801 13:47:27.146023 24466 net.cpp:1089] Copying source layer fc10 Type:InnerProduct #blobs=2
I0801 13:47:27.146035 24466 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0801 13:47:27.148739 24466 net.cpp:1089] Copying source layer data Type:Data #blobs=0
I0801 13:47:27.148758 24466 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0801 13:47:27.148789 24466 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0801 13:47:27.148803 24466 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.149056 24466 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0801 13:47:27.149071 24466 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0801 13:47:27.149085 24466 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.149235 24466 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0801 13:47:27.149241 24466 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0801 13:47:27.149245 24466 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0801 13:47:27.149261 24466 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.149413 24466 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0801 13:47:27.149420 24466 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0801 13:47:27.149435 24466 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.149581 24466 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0801 13:47:27.149586 24466 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0801 13:47:27.149590 24466 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0801 13:47:27.149629 24466 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.149767 24466 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0801 13:47:27.149772 24466 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0801 13:47:27.149796 24466 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.149914 24466 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0801 13:47:27.149919 24466 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0801 13:47:27.149922 24466 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0801 13:47:27.150037 24466 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.150164 24466 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0801 13:47:27.150169 24466 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0801 13:47:27.150228 24466 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.150358 24466 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0801 13:47:27.150363 24466 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0801 13:47:27.150367 24466 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0801 13:47:27.150707 24466 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0801 13:47:27.150835 24466 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0801 13:47:27.150840 24466 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0801 13:47:27.150990 24466 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0801 13:47:27.151120 24466 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0801 13:47:27.151125 24466 net.cpp:1089] Copying source layer pool5 Type:Pooling #blobs=0
I0801 13:47:27.151129 24466 net.cpp:1089] Copying source layer fc10 Type:InnerProduct #blobs=2
I0801 13:47:27.151140 24466 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0801 13:47:27.151204 24466 parallel.cpp:108] [0 - 0] P2pSync adding callback
I0801 13:47:27.151214 24466 parallel.cpp:108] [1 - 1] P2pSync adding callback
I0801 13:47:27.151218 24466 parallel.cpp:108] [2 - 2] P2pSync adding callback
I0801 13:47:27.151221 24466 parallel.cpp:61] Starting Optimization
I0801 13:47:27.151226 24466 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0801 13:47:27.151252 24466 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:47:27.151278 24466 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:47:27.151942 24522 device_alternate.hpp:116] NVML initialized on thread 140400203826944
I0801 13:47:27.164360 24522 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0801 13:47:27.164420 24523 device_alternate.hpp:116] NVML initialized on thread 140400195434240
I0801 13:47:27.165562 24523 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0801 13:47:27.165578 24524 device_alternate.hpp:116] NVML initialized on thread 140400187041536
I0801 13:47:27.166246 24524 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0801 13:47:27.169977 24523 solver.cpp:42] Solver data type: FLOAT
W0801 13:47:27.170514 24523 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0801 13:47:27.170626 24523 net.cpp:104] Using FLOAT as default forward math type
I0801 13:47:27.170634 24523 net.cpp:110] Using FLOAT as default backward math type
I0801 13:47:27.170677 24523 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0801 13:47:27.170694 24523 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:47:27.174593 24524 solver.cpp:42] Solver data type: FLOAT
W0801 13:47:27.174952 24524 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 64 to 66
I0801 13:47:27.175016 24524 net.cpp:104] Using FLOAT as default forward math type
I0801 13:47:27.175020 24524 net.cpp:110] Using FLOAT as default backward math type
I0801 13:47:27.175042 24524 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 22
I0801 13:47:27.175050 24524 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:47:27.175292 24525 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0801 13:47:27.176281 24527 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0801 13:47:27.176419 24523 data_layer.cpp:184] [1] ReshapePrefetch 22, 3, 32, 32
I0801 13:47:27.177389 24524 data_layer.cpp:184] [2] ReshapePrefetch 22, 3, 32, 32
I0801 13:47:27.177464 24524 data_layer.cpp:208] [2] Output data size: 22, 3, 32, 32
I0801 13:47:27.177469 24524 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:47:27.177531 24523 data_layer.cpp:208] [1] Output data size: 22, 3, 32, 32
I0801 13:47:27.177541 24523 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:47:27.605844 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.25G, req 0G)
I0801 13:47:27.630036 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 8.25G, req 0G)
I0801 13:47:27.632150 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.23G, req 0G)
I0801 13:47:27.639448 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 8.23G, req 0G)
I0801 13:47:27.646047 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.21G, req 0G)
I0801 13:47:27.651338 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 8.21G, req 0G)
I0801 13:47:27.658550 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.19G, req 0G)
I0801 13:47:27.680593 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 8.19G, req 0G)
I0801 13:47:27.698572 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.18G, req 0.01G)
I0801 13:47:27.703321 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 8.18G, req 0.01G)
I0801 13:47:27.705633 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0.01G)
I0801 13:47:27.709846 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 8.17G, req 0.01G)
I0801 13:47:27.728874 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.15G, req 0.01G)
I0801 13:47:27.730916 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 8.15G, req 0.01G)
I0801 13:47:27.739718 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.14G, req 0.01G)
I0801 13:47:27.743654 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 8.14G, req 0.01G)
I0801 13:47:27.787050 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0.01G)
I0801 13:47:27.790648 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 8.11G, req 0.01G)
I0801 13:47:27.808516 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8.1G, req 0.01G)
I0801 13:47:27.811020 24524 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/test.prototxt
W0801 13:47:27.811067 24524 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0801 13:47:27.811173 24524 net.cpp:104] Using FLOAT as default forward math type
I0801 13:47:27.811178 24524 net.cpp:110] Using FLOAT as default backward math type
I0801 13:47:27.811194 24524 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0801 13:47:27.811204 24524 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:47:27.812021 24543 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0801 13:47:27.812157 24524 data_layer.cpp:184] (2) ReshapePrefetch 17, 3, 32, 32
I0801 13:47:27.812347 24524 data_layer.cpp:208] (2) Output data size: 17, 3, 32, 32
I0801 13:47:27.812355 24524 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0801 13:47:27.812383 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 8.1G, req 0.01G)
I0801 13:47:27.813047 24544 data_layer.cpp:97] (2) Parser threads: 1
I0801 13:47:27.813055 24544 data_layer.cpp:99] (2) Transformer threads: 1
I0801 13:47:27.815155 24523 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/test.prototxt
W0801 13:47:27.815207 24523 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 50 to 51
I0801 13:47:27.815305 24523 net.cpp:104] Using FLOAT as default forward math type
I0801 13:47:27.815310 24523 net.cpp:110] Using FLOAT as default backward math type
I0801 13:47:27.815326 24523 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 17
I0801 13:47:27.815335 24523 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:47:27.816347 24545 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0801 13:47:27.816426 24523 data_layer.cpp:184] (1) ReshapePrefetch 17, 3, 32, 32
I0801 13:47:27.816517 24523 data_layer.cpp:208] (1) Output data size: 17, 3, 32, 32
I0801 13:47:27.816522 24523 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0801 13:47:27.817234 24546 data_layer.cpp:97] (1) Parser threads: 1
I0801 13:47:27.817240 24546 data_layer.cpp:99] (1) Transformer threads: 1
I0801 13:47:27.819483 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8.1G, req 0.01G)
I0801 13:47:27.820569 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 8.1G, req 0.01G)
I0801 13:47:27.825067 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.09G, req 0.01G)
I0801 13:47:27.825990 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 8.09G, req 0.01G)
I0801 13:47:27.830546 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0.01G)
I0801 13:47:27.832154 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 8.08G, req 0.01G)
I0801 13:47:27.835326 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.08G, req 0.01G)
I0801 13:47:27.837288 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 8.08G, req 0.01G)
I0801 13:47:27.844128 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.07G, req 0.01G)
I0801 13:47:27.845638 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 8.07G, req 0.01G)
I0801 13:47:27.850589 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.06G, req 0.01G)
I0801 13:47:27.850803 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 8.06G, req 0.01G)
I0801 13:47:27.863771 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0.01G)
I0801 13:47:27.865610 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 8.05G, req 0.01G)
I0801 13:47:27.871058 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.05G, req 0.01G)
I0801 13:47:27.872862 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 8.05G, req 0.01G)
I0801 13:47:27.905182 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.03G, req 0.01G)
I0801 13:47:27.911846 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 8.03G, req 0.01G)
I0801 13:47:27.924582 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8.02G, req 0.01G)
I0801 13:47:27.926373 24523 solver.cpp:56] Solver scaffolding done.
I0801 13:47:27.929606 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 8.02G, req 0.01G)
I0801 13:47:27.932032 24524 solver.cpp:56] Solver scaffolding done.
I0801 13:47:27.978685 24523 parallel.cpp:164] [1 - 1] P2pSync adding callback
I0801 13:47:27.978711 24524 parallel.cpp:164] [2 - 2] P2pSync adding callback
I0801 13:47:27.978745 24522 parallel.cpp:164] [0 - 0] P2pSync adding callback
I0801 13:47:28.184315 24522 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:47:28.188086 24524 solver.cpp:479] Solving jacintonet11v2_train
I0801 13:47:28.188097 24524 solver.cpp:480] Learning Rate Policy: poly
I0801 13:47:28.191659 24523 solver.cpp:479] Solving jacintonet11v2_train
I0801 13:47:28.191671 24523 solver.cpp:480] Learning Rate Policy: poly
I0801 13:47:28.198138 24522 solver.cpp:479] Solving jacintonet11v2_train
I0801 13:47:28.198151 24522 solver.cpp:480] Learning Rate Policy: poly
I0801 13:47:28.205061 24523 solver.cpp:268] Starting Optimization on GPU 1
I0801 13:47:28.205066 24524 solver.cpp:268] Starting Optimization on GPU 2
I0801 13:47:28.205093 24522 solver.cpp:268] Starting Optimization on GPU 0
I0801 13:47:28.205219 24547 device_alternate.hpp:116] NVML initialized on thread 140399423227648
I0801 13:47:28.205240 24547 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0801 13:47:28.205278 24522 solver.cpp:550] Iteration 0, Testing net (#0)
I0801 13:47:28.205919 24548 device_alternate.hpp:116] NVML initialized on thread 140399431620352
I0801 13:47:28.205931 24548 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0801 13:47:28.205951 24549 device_alternate.hpp:116] NVML initialized on thread 140399414834944
I0801 13:47:28.205965 24549 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0801 13:47:28.215507 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.98G, req 0.01G)
I0801 13:47:28.215780 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.98G, req 0.01G)
I0801 13:47:28.221525 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.97G, req 0.01G)
I0801 13:47:28.221905 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.97G, req 0.01G)
I0801 13:47:28.222198 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 7.92G, req 0G)
I0801 13:47:28.229967 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0.01G)
I0801 13:47:28.230974 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.9G, req 0G)
I0801 13:47:28.231447 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.96G, req 0.01G)
I0801 13:47:28.237018 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.95G, req 0.01G)
I0801 13:47:28.239029 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.95G, req 0.01G)
I0801 13:47:28.241389 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.89G, req 0G)
I0801 13:47:28.243556 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.93G, req 0.01G)
I0801 13:47:28.244863 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.93G, req 0.01G)
I0801 13:47:28.247980 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0G)
I0801 13:47:28.249577 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.92G, req 0.01G)
I0801 13:47:28.251190 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.92G, req 0.01G)
I0801 13:47:28.256014 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.86G, req 0G)
I0801 13:47:28.259230 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.91G, req 0.01G)
I0801 13:47:28.260452 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.91G, req 0.01G)
I0801 13:47:28.260726 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.85G, req 0G)
I0801 13:47:28.265595 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.9G, req 0.01G)
I0801 13:47:28.267150 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.9G, req 0.01G)
I0801 13:47:28.269918 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.84G, req 0G)
I0801 13:47:28.274474 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.83G, req 0G)
I0801 13:47:28.275120 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.88G, req 0.01G)
I0801 13:47:28.276909 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.88G, req 0.01G)
I0801 13:47:28.282027 24524 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0.01G)
I0801 13:47:28.283643 24523 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0.01G)
I0801 13:47:28.285234 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.81G, req 0G)
I0801 13:47:28.289938 24522 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7.8G, req 0G)
I0801 13:47:28.292285 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 1
I0801 13:47:28.292294 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0801 13:47:28.292299 24522 solver.cpp:635]     Test net output #2: loss = 0.0169944 (* 1 = 0.0169944 loss)
I0801 13:47:28.292301 24522 solver.cpp:295] [MultiGPU] Initial Test completed
I0801 13:47:28.292318 24524 blocking_queue.cpp:40] Data layer prefetch queue empty
I0801 13:47:28.301594 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.87G, req 0.01G)
I0801 13:47:28.302810 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.87G, req 0.01G)
I0801 13:47:28.303113 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 7.8G, req 0G)
I0801 13:47:28.310590 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.86G, req 0.01G)
I0801 13:47:28.312160 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.86G, req 0.01G)
I0801 13:47:28.312436 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 1 1 3  (limit 7.79G, req 0G)
I0801 13:47:28.322090 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.85G, req 0.01G)
I0801 13:47:28.324101 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.85G, req 0.01G)
I0801 13:47:28.325076 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.77G, req 0G)
I0801 13:47:28.329833 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0.01G)
I0801 13:47:28.332134 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0.01G)
I0801 13:47:28.333263 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.76G, req 0G)
I0801 13:47:28.340365 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.82G, req 0.01G)
I0801 13:47:28.343231 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.82G, req 0.01G)
I0801 13:47:28.344734 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 5  (limit 7.75G, req 0.01G)
I0801 13:47:28.347297 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.81G, req 0.01G)
I0801 13:47:28.350663 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.81G, req 0.01G)
I0801 13:47:28.351954 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.74G, req 0.01G)
I0801 13:47:28.362238 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.8G, req 0.01G)
I0801 13:47:28.365849 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.8G, req 0.01G)
I0801 13:47:28.367575 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.72G, req 0.01G)
I0801 13:47:28.369717 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.78G, req 0.01G)
I0801 13:47:28.374197 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.78G, req 0.01G)
I0801 13:47:28.375723 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.71G, req 0.01G)
I0801 13:47:28.389847 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.77G, req 0.01G)
I0801 13:47:28.394136 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.77G, req 0.01G)
I0801 13:47:28.396167 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.02G/1 6 4 1  (limit 7.69G, req 0.01G)
I0801 13:47:28.397151 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.76G, req 0.01G)
I0801 13:47:28.402323 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.76G, req 0.01G)
I0801 13:47:28.404592 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.02G/2 6 4 5  (limit 7.68G, req 0.01G)
I0801 13:47:28.423187 24529 data_layer.cpp:97] [1] Parser threads: 1
I0801 13:47:28.423202 24529 data_layer.cpp:99] [1] Transformer threads: 1
I0801 13:47:28.434536 24488 data_layer.cpp:97] [0] Parser threads: 1
I0801 13:47:28.434551 24488 data_layer.cpp:99] [0] Transformer threads: 1
I0801 13:47:28.435797 24528 data_layer.cpp:97] [2] Parser threads: 1
I0801 13:47:28.435811 24528 data_layer.cpp:99] [2] Transformer threads: 1
I0801 13:47:28.441738 24522 solver.cpp:358] Iteration 0 (0.149399 s), loss = 0.000534833
I0801 13:47:28.441758 24522 solver.cpp:375]     Train net output #0: loss = 0.000534833 (* 1 = 0.000534833 loss)
I0801 13:47:28.441764 24522 sgd_solver.cpp:136] Iteration 0, lr = 0.01, m = 0.9
I0801 13:47:28.469102 24522 solver.cpp:358] Iteration 1 (0.0273572 s), loss = 0.00206478
I0801 13:47:28.469130 24522 solver.cpp:375]     Train net output #0: loss = 0.00206478 (* 1 = 0.00206478 loss)
I0801 13:47:28.479344 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 7.06G, req 0.01G)
I0801 13:47:28.480532 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 6.97G, req 0.01G)
I0801 13:47:28.481356 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.64G/1 1 0 3  (limit 7.06G, req 0.01G)
I0801 13:47:28.488248 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.42G, req 0.01G)
I0801 13:47:28.490032 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.33G, req 0.01G)
I0801 13:47:28.490975 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 1.29G/2 1 1 3  (limit 6.42G, req 0.01G)
I0801 13:47:28.502267 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.42G, req 0.01G)
I0801 13:47:28.504338 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.33G, req 0.01G)
I0801 13:47:28.504865 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.29G/1 6 4 3  (limit 6.42G, req 0.01G)
I0801 13:47:28.509312 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.01G)
I0801 13:47:28.511430 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.33G, req 0.01G)
I0801 13:47:28.515529 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.01G)
I0801 13:47:28.519434 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.42G, req 0.01G)
I0801 13:47:28.524116 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.33G, req 0.01G)
I0801 13:47:28.524274 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.01G)
I0801 13:47:28.526057 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.29G/1 6 4 5  (limit 6.42G, req 0.01G)
I0801 13:47:28.528772 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 0  (limit 6.33G, req 0.01G)
I0801 13:47:28.530575 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.29G/2 6 4 0  (limit 6.42G, req 0.01G)
I0801 13:47:28.550166 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.42G, req 0.02G)
I0801 13:47:28.552214 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.33G, req 0.02G)
I0801 13:47:28.554939 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.29G/1 6 4 5  (limit 6.42G, req 0.02G)
I0801 13:47:28.558421 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.02G)
I0801 13:47:28.559480 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.33G, req 0.02G)
I0801 13:47:28.563061 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.29G/2 6 4 3  (limit 6.42G, req 0.02G)
I0801 13:47:28.594821 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.33G, req 0.03G)
I0801 13:47:28.595392 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.42G, req 0.03G)
I0801 13:47:28.599827 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 1.29G/1 7 5 5  (limit 6.42G, req 0.03G)
I0801 13:47:28.603843 24522 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 5 5  (limit 6.33G, req 0.03G)
I0801 13:47:28.604480 24523 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.42G, req 0.03G)
I0801 13:47:28.608052 24524 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 1.29G/2 6 4 5  (limit 6.42G, req 0.03G)
I0801 13:47:28.619050 24522 solver.cpp:358] Iteration 2 (0.149937 s), loss = 0.000599201
I0801 13:47:28.619074 24524 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0801 13:47:28.619077 24522 solver.cpp:375]     Train net output #0: loss = 0.000599201 (* 1 = 0.000599201 loss)
I0801 13:47:28.619084 24523 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0801 13:47:28.619364 24522 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 1.29G -> 0.07G
I0801 13:47:30.179291 24522 solver.cpp:353] Iteration 100 (62.8129 iter/s, 1.56019s/98 iter), loss = 0.000651761
I0801 13:47:30.179316 24522 solver.cpp:375]     Train net output #0: loss = 0.000651762 (* 1 = 0.000651762 loss)
I0801 13:47:30.179322 24522 sgd_solver.cpp:136] Iteration 100, lr = 0.00998437, m = 0.9
I0801 13:47:31.753546 24522 solver.cpp:353] Iteration 200 (63.524 iter/s, 1.57421s/100 iter), loss = 0.00582214
I0801 13:47:31.753602 24522 solver.cpp:375]     Train net output #0: loss = 0.00582214 (* 1 = 0.00582214 loss)
I0801 13:47:31.753609 24522 sgd_solver.cpp:136] Iteration 200, lr = 0.00996875, m = 0.9
I0801 13:47:33.341953 24522 solver.cpp:353] Iteration 300 (62.9581 iter/s, 1.58836s/100 iter), loss = 0.000438397
I0801 13:47:33.341980 24522 solver.cpp:375]     Train net output #0: loss = 0.000438398 (* 1 = 0.000438398 loss)
I0801 13:47:33.341985 24522 sgd_solver.cpp:136] Iteration 300, lr = 0.00995312, m = 0.9
I0801 13:47:34.923933 24522 solver.cpp:353] Iteration 400 (63.2139 iter/s, 1.58193s/100 iter), loss = 0.00109726
I0801 13:47:34.923959 24522 solver.cpp:375]     Train net output #0: loss = 0.00109726 (* 1 = 0.00109726 loss)
I0801 13:47:34.923964 24522 sgd_solver.cpp:136] Iteration 400, lr = 0.0099375, m = 0.9
I0801 13:47:36.513214 24522 solver.cpp:353] Iteration 500 (62.9235 iter/s, 1.58923s/100 iter), loss = 0.00251341
I0801 13:47:36.513263 24522 solver.cpp:375]     Train net output #0: loss = 0.00251341 (* 1 = 0.00251341 loss)
I0801 13:47:36.513278 24522 sgd_solver.cpp:136] Iteration 500, lr = 0.00992187, m = 0.9
I0801 13:47:38.115597 24522 solver.cpp:353] Iteration 600 (62.4091 iter/s, 1.60233s/100 iter), loss = 0.02622
I0801 13:47:38.115619 24522 solver.cpp:375]     Train net output #0: loss = 0.02622 (* 1 = 0.02622 loss)
I0801 13:47:38.115623 24522 sgd_solver.cpp:136] Iteration 600, lr = 0.00990625, m = 0.9
I0801 13:47:39.699136 24522 solver.cpp:353] Iteration 700 (63.1516 iter/s, 1.58349s/100 iter), loss = 0.329899
I0801 13:47:39.699173 24522 solver.cpp:375]     Train net output #0: loss = 0.329899 (* 1 = 0.329899 loss)
I0801 13:47:39.699179 24522 sgd_solver.cpp:136] Iteration 700, lr = 0.00989062, m = 0.9
I0801 13:47:40.545948 24487 data_reader.cpp:264] Starting prefetch of epoch 1
I0801 13:47:41.275439 24522 solver.cpp:353] Iteration 800 (63.4415 iter/s, 1.57626s/100 iter), loss = 0.357397
I0801 13:47:41.275485 24522 solver.cpp:375]     Train net output #0: loss = 0.357397 (* 1 = 0.357397 loss)
I0801 13:47:41.275496 24522 sgd_solver.cpp:136] Iteration 800, lr = 0.009875, m = 0.9
I0801 13:47:42.872835 24522 solver.cpp:353] Iteration 900 (62.6039 iter/s, 1.59734s/100 iter), loss = 0.0303912
I0801 13:47:42.872861 24522 solver.cpp:375]     Train net output #0: loss = 0.030391 (* 1 = 0.030391 loss)
I0801 13:47:42.872867 24522 sgd_solver.cpp:136] Iteration 900, lr = 0.00985937, m = 0.9
I0801 13:47:44.455584 24522 solver.cpp:404] Sparsity after update:
I0801 13:47:44.460052 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:47:44.460064 24522 net.cpp:2270] conv1a_param_0(0) 
I0801 13:47:44.460078 24522 net.cpp:2270] conv1b_param_0(0) 
I0801 13:47:44.460083 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:47:44.460085 24522 net.cpp:2270] res2a_branch2a_param_0(0) 
I0801 13:47:44.460089 24522 net.cpp:2270] res2a_branch2b_param_0(0) 
I0801 13:47:44.460093 24522 net.cpp:2270] res3a_branch2a_param_0(0) 
I0801 13:47:44.460095 24522 net.cpp:2270] res3a_branch2b_param_0(0) 
I0801 13:47:44.460099 24522 net.cpp:2270] res4a_branch2a_param_0(0) 
I0801 13:47:44.460103 24522 net.cpp:2270] res4a_branch2b_param_0(0) 
I0801 13:47:44.460105 24522 net.cpp:2270] res5a_branch2a_param_0(0) 
I0801 13:47:44.460108 24522 net.cpp:2270] res5a_branch2b_param_0(0) 
I0801 13:47:44.460111 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0801 13:47:44.460121 24522 solver.cpp:550] Iteration 1000, Testing net (#0)
I0801 13:47:45.284344 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.655588
I0801 13:47:45.284363 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.946765
I0801 13:47:45.284368 24522 solver.cpp:635]     Test net output #2: loss = 1.32485 (* 1 = 1.32485 loss)
I0801 13:47:45.284381 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.824232s
I0801 13:47:45.305763 24522 solver.cpp:353] Iteration 1000 (41.104 iter/s, 2.43286s/100 iter), loss = 0.868658
I0801 13:47:45.305786 24522 solver.cpp:375]     Train net output #0: loss = 0.868658 (* 1 = 0.868658 loss)
I0801 13:47:45.305794 24522 sgd_solver.cpp:136] Iteration 1000, lr = 0.00984375, m = 0.9
I0801 13:47:46.903251 24522 solver.cpp:353] Iteration 1100 (62.6002 iter/s, 1.59744s/100 iter), loss = 0.60125
I0801 13:47:46.903301 24522 solver.cpp:375]     Train net output #0: loss = 0.60125 (* 1 = 0.60125 loss)
I0801 13:47:46.903312 24522 sgd_solver.cpp:136] Iteration 1100, lr = 0.00982813, m = 0.9
I0801 13:47:48.494046 24522 solver.cpp:353] Iteration 1200 (62.8636 iter/s, 1.59074s/100 iter), loss = 0.443024
I0801 13:47:48.494072 24522 solver.cpp:375]     Train net output #0: loss = 0.443024 (* 1 = 0.443024 loss)
I0801 13:47:48.494078 24522 sgd_solver.cpp:136] Iteration 1200, lr = 0.0098125, m = 0.9
I0801 13:47:50.071902 24522 solver.cpp:353] Iteration 1300 (63.3792 iter/s, 1.5778s/100 iter), loss = 0.133561
I0801 13:47:50.071924 24522 solver.cpp:375]     Train net output #0: loss = 0.133561 (* 1 = 0.133561 loss)
I0801 13:47:50.071929 24522 sgd_solver.cpp:136] Iteration 1300, lr = 0.00979687, m = 0.9
I0801 13:47:51.656603 24522 solver.cpp:353] Iteration 1400 (63.1053 iter/s, 1.58465s/100 iter), loss = 0.216859
I0801 13:47:51.656627 24522 solver.cpp:375]     Train net output #0: loss = 0.216859 (* 1 = 0.216859 loss)
I0801 13:47:51.656630 24522 sgd_solver.cpp:136] Iteration 1400, lr = 0.00978125, m = 0.9
I0801 13:47:53.241462 24522 solver.cpp:353] Iteration 1500 (63.099 iter/s, 1.58481s/100 iter), loss = 0.0974944
I0801 13:47:53.241487 24522 solver.cpp:375]     Train net output #0: loss = 0.0974946 (* 1 = 0.0974946 loss)
I0801 13:47:53.243049 24522 sgd_solver.cpp:136] Iteration 1500, lr = 0.00976562, m = 0.9
I0801 13:47:54.831667 24522 solver.cpp:353] Iteration 1600 (62.8871 iter/s, 1.59015s/100 iter), loss = 0.198752
I0801 13:47:54.831707 24522 solver.cpp:375]     Train net output #0: loss = 0.198752 (* 1 = 0.198752 loss)
I0801 13:47:54.831712 24522 sgd_solver.cpp:136] Iteration 1600, lr = 0.00975, m = 0.9
I0801 13:47:56.419111 24522 solver.cpp:353] Iteration 1700 (62.9961 iter/s, 1.5874s/100 iter), loss = 0.322344
I0801 13:47:56.419229 24522 solver.cpp:375]     Train net output #0: loss = 0.322344 (* 1 = 0.322344 loss)
I0801 13:47:56.419237 24522 sgd_solver.cpp:136] Iteration 1700, lr = 0.00973437, m = 0.9
I0801 13:47:58.010743 24522 solver.cpp:353] Iteration 1800 (62.8306 iter/s, 1.59158s/100 iter), loss = 0.176526
I0801 13:47:58.010768 24522 solver.cpp:375]     Train net output #0: loss = 0.176526 (* 1 = 0.176526 loss)
I0801 13:47:58.010774 24522 sgd_solver.cpp:136] Iteration 1800, lr = 0.00971875, m = 0.9
I0801 13:47:59.592622 24522 solver.cpp:353] Iteration 1900 (63.218 iter/s, 1.58183s/100 iter), loss = 0.0715274
I0801 13:47:59.592648 24522 solver.cpp:375]     Train net output #0: loss = 0.0715274 (* 1 = 0.0715274 loss)
I0801 13:47:59.592654 24522 sgd_solver.cpp:136] Iteration 1900, lr = 0.00970312, m = 0.9
I0801 13:48:01.176017 24522 solver.cpp:404] Sparsity after update:
I0801 13:48:01.177908 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:48:01.177919 24522 net.cpp:2270] conv1a_param_0(0) 
I0801 13:48:01.177929 24522 net.cpp:2270] conv1b_param_0(0) 
I0801 13:48:01.177934 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:48:01.177937 24522 net.cpp:2270] res2a_branch2a_param_0(0) 
I0801 13:48:01.177942 24522 net.cpp:2270] res2a_branch2b_param_0(0) 
I0801 13:48:01.177945 24522 net.cpp:2270] res3a_branch2a_param_0(0) 
I0801 13:48:01.177948 24522 net.cpp:2270] res3a_branch2b_param_0(0) 
I0801 13:48:01.177953 24522 net.cpp:2270] res4a_branch2a_param_0(0) 
I0801 13:48:01.177956 24522 net.cpp:2270] res4a_branch2b_param_0(0) 
I0801 13:48:01.177959 24522 net.cpp:2270] res5a_branch2a_param_0(0) 
I0801 13:48:01.177963 24522 net.cpp:2270] res5a_branch2b_param_0(0) 
I0801 13:48:01.177966 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0801 13:48:01.177978 24522 solver.cpp:550] Iteration 2000, Testing net (#0)
I0801 13:48:01.995277 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.786472
I0801 13:48:01.995297 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.983235
I0801 13:48:01.995302 24522 solver.cpp:635]     Test net output #2: loss = 0.820027 (* 1 = 0.820027 loss)
I0801 13:48:01.995318 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.817312s
I0801 13:48:02.010977 24522 solver.cpp:353] Iteration 2000 (41.3516 iter/s, 2.41829s/100 iter), loss = 0.286471
I0801 13:48:02.010996 24522 solver.cpp:375]     Train net output #0: loss = 0.286471 (* 1 = 0.286471 loss)
I0801 13:48:02.011001 24522 sgd_solver.cpp:136] Iteration 2000, lr = 0.0096875, m = 0.9
I0801 13:48:03.592183 24522 solver.cpp:353] Iteration 2100 (63.245 iter/s, 1.58115s/100 iter), loss = 0.206765
I0801 13:48:03.592208 24522 solver.cpp:375]     Train net output #0: loss = 0.206765 (* 1 = 0.206765 loss)
I0801 13:48:03.592216 24522 sgd_solver.cpp:136] Iteration 2100, lr = 0.00967188, m = 0.9
I0801 13:48:05.187500 24522 solver.cpp:353] Iteration 2200 (62.6855 iter/s, 1.59526s/100 iter), loss = 0.0686356
I0801 13:48:05.187553 24522 solver.cpp:375]     Train net output #0: loss = 0.0686357 (* 1 = 0.0686357 loss)
I0801 13:48:05.187568 24522 sgd_solver.cpp:136] Iteration 2200, lr = 0.00965625, m = 0.9
I0801 13:48:06.773969 24522 solver.cpp:353] Iteration 2300 (63.035 iter/s, 1.58642s/100 iter), loss = 0.107024
I0801 13:48:06.773994 24522 solver.cpp:375]     Train net output #0: loss = 0.107024 (* 1 = 0.107024 loss)
I0801 13:48:06.773998 24522 sgd_solver.cpp:136] Iteration 2300, lr = 0.00964062, m = 0.9
I0801 13:48:08.365300 24522 solver.cpp:353] Iteration 2400 (62.8424 iter/s, 1.59128s/100 iter), loss = 0.206561
I0801 13:48:08.365324 24522 solver.cpp:375]     Train net output #0: loss = 0.206561 (* 1 = 0.206561 loss)
I0801 13:48:08.365329 24522 sgd_solver.cpp:136] Iteration 2400, lr = 0.009625, m = 0.9
I0801 13:48:09.951424 24522 solver.cpp:353] Iteration 2500 (63.0488 iter/s, 1.58607s/100 iter), loss = 0.10762
I0801 13:48:09.951448 24522 solver.cpp:375]     Train net output #0: loss = 0.10762 (* 1 = 0.10762 loss)
I0801 13:48:09.951452 24522 sgd_solver.cpp:136] Iteration 2500, lr = 0.00960938, m = 0.9
I0801 13:48:11.526700 24522 solver.cpp:353] Iteration 2600 (63.4829 iter/s, 1.57523s/100 iter), loss = 0.142416
I0801 13:48:11.526724 24522 solver.cpp:375]     Train net output #0: loss = 0.142416 (* 1 = 0.142416 loss)
I0801 13:48:11.526729 24522 sgd_solver.cpp:136] Iteration 2600, lr = 0.00959375, m = 0.9
I0801 13:48:13.108774 24522 solver.cpp:353] Iteration 2700 (63.2102 iter/s, 1.58202s/100 iter), loss = 0.0513683
I0801 13:48:13.108822 24522 solver.cpp:375]     Train net output #0: loss = 0.0513684 (* 1 = 0.0513684 loss)
I0801 13:48:13.108835 24522 sgd_solver.cpp:136] Iteration 2700, lr = 0.00957812, m = 0.9
I0801 13:48:14.693248 24522 solver.cpp:353] Iteration 2800 (63.1144 iter/s, 1.58442s/100 iter), loss = 0.130509
I0801 13:48:14.693272 24522 solver.cpp:375]     Train net output #0: loss = 0.130509 (* 1 = 0.130509 loss)
I0801 13:48:14.693277 24522 sgd_solver.cpp:136] Iteration 2800, lr = 0.0095625, m = 0.9
I0801 13:48:16.292270 24522 solver.cpp:353] Iteration 2900 (62.54 iter/s, 1.59898s/100 iter), loss = 0.148904
I0801 13:48:16.292296 24522 solver.cpp:375]     Train net output #0: loss = 0.148904 (* 1 = 0.148904 loss)
I0801 13:48:16.292301 24522 sgd_solver.cpp:136] Iteration 2900, lr = 0.00954687, m = 0.9
I0801 13:48:17.854998 24522 solver.cpp:404] Sparsity after update:
I0801 13:48:17.857084 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:48:17.857101 24522 net.cpp:2270] conv1a_param_0(0) 
I0801 13:48:17.857107 24522 net.cpp:2270] conv1b_param_0(0) 
I0801 13:48:17.857110 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:48:17.857111 24522 net.cpp:2270] res2a_branch2a_param_0(0) 
I0801 13:48:17.857113 24522 net.cpp:2270] res2a_branch2b_param_0(0) 
I0801 13:48:17.857115 24522 net.cpp:2270] res3a_branch2a_param_0(0) 
I0801 13:48:17.857117 24522 net.cpp:2270] res3a_branch2b_param_0(0) 
I0801 13:48:17.857120 24522 net.cpp:2270] res4a_branch2a_param_0(0) 
I0801 13:48:17.857123 24522 net.cpp:2270] res4a_branch2b_param_0(0) 
I0801 13:48:17.857125 24522 net.cpp:2270] res5a_branch2a_param_0(0) 
I0801 13:48:17.857127 24522 net.cpp:2270] res5a_branch2b_param_0(0) 
I0801 13:48:17.857128 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0801 13:48:17.857137 24522 solver.cpp:550] Iteration 3000, Testing net (#0)
I0801 13:48:18.672202 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.843237
I0801 13:48:18.672220 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.991765
I0801 13:48:18.672225 24522 solver.cpp:635]     Test net output #2: loss = 0.548438 (* 1 = 0.548438 loss)
I0801 13:48:18.672240 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.815076s
I0801 13:48:18.688145 24522 solver.cpp:353] Iteration 3000 (41.7396 iter/s, 2.3958s/100 iter), loss = 0.068511
I0801 13:48:18.688163 24522 solver.cpp:375]     Train net output #0: loss = 0.068511 (* 1 = 0.068511 loss)
I0801 13:48:18.688169 24522 sgd_solver.cpp:136] Iteration 3000, lr = 0.00953125, m = 0.9
I0801 13:48:20.272599 24522 solver.cpp:353] Iteration 3100 (63.1152 iter/s, 1.5844s/100 iter), loss = 0.0336655
I0801 13:48:20.272649 24522 solver.cpp:375]     Train net output #0: loss = 0.0336654 (* 1 = 0.0336654 loss)
I0801 13:48:20.272661 24522 sgd_solver.cpp:136] Iteration 3100, lr = 0.00951563, m = 0.9
I0801 13:48:21.866626 24522 solver.cpp:353] Iteration 3200 (62.7362 iter/s, 1.59398s/100 iter), loss = 0.0705109
I0801 13:48:21.866652 24522 solver.cpp:375]     Train net output #0: loss = 0.0705108 (* 1 = 0.0705108 loss)
I0801 13:48:21.866657 24522 sgd_solver.cpp:136] Iteration 3200, lr = 0.0095, m = 0.9
I0801 13:48:23.465806 24522 solver.cpp:353] Iteration 3300 (62.534 iter/s, 1.59913s/100 iter), loss = 0.105086
I0801 13:48:23.465831 24522 solver.cpp:375]     Train net output #0: loss = 0.105086 (* 1 = 0.105086 loss)
I0801 13:48:23.465837 24522 sgd_solver.cpp:136] Iteration 3300, lr = 0.00948437, m = 0.9
I0801 13:48:25.047185 24522 solver.cpp:353] Iteration 3400 (63.238 iter/s, 1.58133s/100 iter), loss = 0.0295291
I0801 13:48:25.047211 24522 solver.cpp:375]     Train net output #0: loss = 0.029529 (* 1 = 0.029529 loss)
I0801 13:48:25.047235 24522 sgd_solver.cpp:136] Iteration 3400, lr = 0.00946875, m = 0.9
I0801 13:48:26.619125 24522 solver.cpp:353] Iteration 3500 (63.6175 iter/s, 1.57189s/100 iter), loss = 0.0694349
I0801 13:48:26.619246 24522 solver.cpp:375]     Train net output #0: loss = 0.0694347 (* 1 = 0.0694347 loss)
I0801 13:48:26.619261 24522 sgd_solver.cpp:136] Iteration 3500, lr = 0.00945312, m = 0.9
I0801 13:48:28.199262 24522 solver.cpp:353] Iteration 3600 (63.2876 iter/s, 1.58009s/100 iter), loss = 0.0974736
I0801 13:48:28.199292 24522 solver.cpp:375]     Train net output #0: loss = 0.0974734 (* 1 = 0.0974734 loss)
I0801 13:48:28.199300 24522 sgd_solver.cpp:136] Iteration 3600, lr = 0.0094375, m = 0.9
I0801 13:48:29.776561 24522 solver.cpp:353] Iteration 3700 (63.4016 iter/s, 1.57725s/100 iter), loss = 0.102818
I0801 13:48:29.776589 24522 solver.cpp:375]     Train net output #0: loss = 0.102818 (* 1 = 0.102818 loss)
I0801 13:48:29.776595 24522 sgd_solver.cpp:136] Iteration 3700, lr = 0.00942187, m = 0.9
I0801 13:48:31.362355 24522 solver.cpp:353] Iteration 3800 (63.0618 iter/s, 1.58575s/100 iter), loss = 0.171872
I0801 13:48:31.362380 24522 solver.cpp:375]     Train net output #0: loss = 0.171872 (* 1 = 0.171872 loss)
I0801 13:48:31.362386 24522 sgd_solver.cpp:136] Iteration 3800, lr = 0.00940625, m = 0.9
I0801 13:48:32.938639 24522 solver.cpp:353] Iteration 3900 (63.4424 iter/s, 1.57623s/100 iter), loss = 0.141611
I0801 13:48:32.938663 24522 solver.cpp:375]     Train net output #0: loss = 0.14161 (* 1 = 0.14161 loss)
I0801 13:48:32.938668 24522 sgd_solver.cpp:136] Iteration 3900, lr = 0.00939062, m = 0.9
I0801 13:48:34.518767 24522 solver.cpp:404] Sparsity after update:
I0801 13:48:34.520565 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:48:34.520575 24522 net.cpp:2270] conv1a_param_0(0) 
I0801 13:48:34.520581 24522 net.cpp:2270] conv1b_param_0(0) 
I0801 13:48:34.520584 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:48:34.520586 24522 net.cpp:2270] res2a_branch2a_param_0(0) 
I0801 13:48:34.520588 24522 net.cpp:2270] res2a_branch2b_param_0(0) 
I0801 13:48:34.520591 24522 net.cpp:2270] res3a_branch2a_param_0(0) 
I0801 13:48:34.520593 24522 net.cpp:2270] res3a_branch2b_param_0(0) 
I0801 13:48:34.520596 24522 net.cpp:2270] res4a_branch2a_param_0(0) 
I0801 13:48:34.520597 24522 net.cpp:2270] res4a_branch2b_param_0(0) 
I0801 13:48:34.520601 24522 net.cpp:2270] res5a_branch2a_param_0(0) 
I0801 13:48:34.520602 24522 net.cpp:2270] res5a_branch2b_param_0(0) 
I0801 13:48:34.520604 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0801 13:48:34.520612 24522 solver.cpp:550] Iteration 4000, Testing net (#0)
I0801 13:48:35.347121 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.829119
I0801 13:48:35.347141 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.99
I0801 13:48:35.347146 24522 solver.cpp:635]     Test net output #2: loss = 0.645051 (* 1 = 0.645051 loss)
I0801 13:48:35.347160 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.82652s
I0801 13:48:35.363096 24549 solver.cpp:450] Finding and applying sparsity: 0.02
I0801 13:48:55.879442 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:48:55.881356 24522 solver.cpp:353] Iteration 4000 (4.3588 iter/s, 22.9421s/100 iter), loss = 0.101566
I0801 13:48:55.881374 24522 solver.cpp:375]     Train net output #0: loss = 0.101566 (* 1 = 0.101566 loss)
I0801 13:48:55.881382 24522 sgd_solver.cpp:136] Iteration 4000, lr = 0.009375, m = 0.9
I0801 13:48:57.702824 24522 solver.cpp:353] Iteration 4100 (54.9024 iter/s, 1.82141s/100 iter), loss = 0.0278589
I0801 13:48:57.702904 24522 solver.cpp:375]     Train net output #0: loss = 0.0278587 (* 1 = 0.0278587 loss)
I0801 13:48:57.702911 24522 sgd_solver.cpp:136] Iteration 4100, lr = 0.00935937, m = 0.9
I0801 13:48:59.289101 24522 solver.cpp:353] Iteration 4200 (63.0427 iter/s, 1.58623s/100 iter), loss = 0.267889
I0801 13:48:59.289125 24522 solver.cpp:375]     Train net output #0: loss = 0.267889 (* 1 = 0.267889 loss)
I0801 13:48:59.289130 24522 sgd_solver.cpp:136] Iteration 4200, lr = 0.00934375, m = 0.9
I0801 13:49:00.852496 24522 solver.cpp:353] Iteration 4300 (63.9655 iter/s, 1.56334s/100 iter), loss = 0.0601831
I0801 13:49:00.852524 24522 solver.cpp:375]     Train net output #0: loss = 0.060183 (* 1 = 0.060183 loss)
I0801 13:49:00.852530 24522 sgd_solver.cpp:136] Iteration 4300, lr = 0.00932813, m = 0.9
I0801 13:49:02.431870 24522 solver.cpp:353] Iteration 4400 (63.3181 iter/s, 1.57933s/100 iter), loss = 0.267935
I0801 13:49:02.431896 24522 solver.cpp:375]     Train net output #0: loss = 0.267935 (* 1 = 0.267935 loss)
I0801 13:49:02.431902 24522 sgd_solver.cpp:136] Iteration 4400, lr = 0.0093125, m = 0.9
I0801 13:49:04.000684 24522 solver.cpp:353] Iteration 4500 (63.7446 iter/s, 1.56876s/100 iter), loss = 0.0253828
I0801 13:49:04.000748 24522 solver.cpp:375]     Train net output #0: loss = 0.0253826 (* 1 = 0.0253826 loss)
I0801 13:49:04.000766 24522 sgd_solver.cpp:136] Iteration 4500, lr = 0.00929687, m = 0.9
I0801 13:49:05.579517 24522 solver.cpp:353] Iteration 4600 (63.3399 iter/s, 1.57878s/100 iter), loss = 0.0383846
I0801 13:49:05.579543 24522 solver.cpp:375]     Train net output #0: loss = 0.0383843 (* 1 = 0.0383843 loss)
I0801 13:49:05.579550 24522 sgd_solver.cpp:136] Iteration 4600, lr = 0.00928125, m = 0.9
I0801 13:49:07.160811 24522 solver.cpp:353] Iteration 4700 (63.2412 iter/s, 1.58125s/100 iter), loss = 0.039688
I0801 13:49:07.160841 24522 solver.cpp:375]     Train net output #0: loss = 0.0396878 (* 1 = 0.0396878 loss)
I0801 13:49:07.160847 24522 sgd_solver.cpp:136] Iteration 4700, lr = 0.00926562, m = 0.9
I0801 13:49:08.753296 24522 solver.cpp:353] Iteration 4800 (62.797 iter/s, 1.59243s/100 iter), loss = 0.164733
I0801 13:49:08.753320 24522 solver.cpp:375]     Train net output #0: loss = 0.164733 (* 1 = 0.164733 loss)
I0801 13:49:08.753325 24522 sgd_solver.cpp:136] Iteration 4800, lr = 0.00925, m = 0.9
I0801 13:49:10.342250 24522 solver.cpp:353] Iteration 4900 (62.9365 iter/s, 1.5889s/100 iter), loss = 0.136636
I0801 13:49:10.342277 24522 solver.cpp:375]     Train net output #0: loss = 0.136636 (* 1 = 0.136636 loss)
I0801 13:49:10.342280 24522 sgd_solver.cpp:136] Iteration 4900, lr = 0.00923437, m = 0.9
I0801 13:49:11.914005 24522 solver.cpp:404] Sparsity after update:
I0801 13:49:11.915362 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:49:11.915382 24522 net.cpp:2270] conv1a_param_0(0) 
I0801 13:49:11.915390 24522 net.cpp:2270] conv1b_param_0(0.00694) 
I0801 13:49:11.915393 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:49:11.915396 24522 net.cpp:2270] res2a_branch2a_param_0(0.0174) 
I0801 13:49:11.915400 24522 net.cpp:2270] res2a_branch2b_param_0(0.0139) 
I0801 13:49:11.915405 24522 net.cpp:2270] res3a_branch2a_param_0(0.0191) 
I0801 13:49:11.915407 24522 net.cpp:2270] res3a_branch2b_param_0(0.0174) 
I0801 13:49:11.915410 24522 net.cpp:2270] res4a_branch2a_param_0(0.02) 
I0801 13:49:11.915413 24522 net.cpp:2270] res4a_branch2b_param_0(0.0191) 
I0801 13:49:11.915416 24522 net.cpp:2270] res5a_branch2a_param_0(0.0198) 
I0801 13:49:11.915419 24522 net.cpp:2270] res5a_branch2b_param_0(0.0198) 
I0801 13:49:11.915423 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (46254/2.3599e+06) 0.0196
I0801 13:49:11.915437 24522 solver.cpp:550] Iteration 5000, Testing net (#0)
I0801 13:49:12.586000 24520 data_reader.cpp:264] Starting prefetch of epoch 1
I0801 13:49:12.728308 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.86706
I0801 13:49:12.728328 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995
I0801 13:49:12.728333 24522 solver.cpp:635]     Test net output #2: loss = 0.465939 (* 1 = 0.465939 loss)
I0801 13:49:12.728361 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.812898s
I0801 13:49:12.743962 24549 solver.cpp:450] Finding and applying sparsity: 0.04
I0801 13:49:33.584513 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:49:33.586470 24522 solver.cpp:353] Iteration 5000 (4.30226 iter/s, 23.2436s/100 iter), loss = 0.0490053
I0801 13:49:33.586493 24522 solver.cpp:375]     Train net output #0: loss = 0.0490049 (* 1 = 0.0490049 loss)
I0801 13:49:33.586501 24522 sgd_solver.cpp:136] Iteration 5000, lr = 0.00921875, m = 0.9
I0801 13:49:35.416996 24522 solver.cpp:353] Iteration 5100 (54.6309 iter/s, 1.83047s/100 iter), loss = 0.0253344
I0801 13:49:35.417047 24522 solver.cpp:375]     Train net output #0: loss = 0.025334 (* 1 = 0.025334 loss)
I0801 13:49:35.417059 24522 sgd_solver.cpp:136] Iteration 5100, lr = 0.00920312, m = 0.9
I0801 13:49:37.016727 24522 solver.cpp:353] Iteration 5200 (62.5125 iter/s, 1.59968s/100 iter), loss = 0.024886
I0801 13:49:37.016753 24522 solver.cpp:375]     Train net output #0: loss = 0.0248856 (* 1 = 0.0248856 loss)
I0801 13:49:37.016759 24522 sgd_solver.cpp:136] Iteration 5200, lr = 0.0091875, m = 0.9
I0801 13:49:38.616523 24522 solver.cpp:353] Iteration 5300 (62.51 iter/s, 1.59974s/100 iter), loss = 0.0160695
I0801 13:49:38.616547 24522 solver.cpp:375]     Train net output #0: loss = 0.0160691 (* 1 = 0.0160691 loss)
I0801 13:49:38.616552 24522 sgd_solver.cpp:136] Iteration 5300, lr = 0.00917188, m = 0.9
I0801 13:49:40.205629 24522 solver.cpp:353] Iteration 5400 (62.9304 iter/s, 1.58906s/100 iter), loss = 0.0603123
I0801 13:49:40.205654 24522 solver.cpp:375]     Train net output #0: loss = 0.0603119 (* 1 = 0.0603119 loss)
I0801 13:49:40.205660 24522 sgd_solver.cpp:136] Iteration 5400, lr = 0.00915625, m = 0.9
I0801 13:49:41.791944 24522 solver.cpp:353] Iteration 5500 (63.0413 iter/s, 1.58626s/100 iter), loss = 0.0314787
I0801 13:49:41.791972 24522 solver.cpp:375]     Train net output #0: loss = 0.0314783 (* 1 = 0.0314783 loss)
I0801 13:49:41.791980 24522 sgd_solver.cpp:136] Iteration 5500, lr = 0.00914062, m = 0.9
I0801 13:49:43.375090 24522 solver.cpp:353] Iteration 5600 (63.1673 iter/s, 1.5831s/100 iter), loss = 0.0160304
I0801 13:49:43.375116 24522 solver.cpp:375]     Train net output #0: loss = 0.01603 (* 1 = 0.01603 loss)
I0801 13:49:43.375123 24522 sgd_solver.cpp:136] Iteration 5600, lr = 0.009125, m = 0.9
I0801 13:49:44.982098 24522 solver.cpp:353] Iteration 5700 (62.2293 iter/s, 1.60696s/100 iter), loss = 0.0360691
I0801 13:49:44.982125 24522 solver.cpp:375]     Train net output #0: loss = 0.0360687 (* 1 = 0.0360687 loss)
I0801 13:49:44.982132 24522 sgd_solver.cpp:136] Iteration 5700, lr = 0.00910938, m = 0.9
I0801 13:49:46.580613 24522 solver.cpp:353] Iteration 5800 (62.5601 iter/s, 1.59846s/100 iter), loss = 0.25557
I0801 13:49:46.580638 24522 solver.cpp:375]     Train net output #0: loss = 0.25557 (* 1 = 0.25557 loss)
I0801 13:49:46.580643 24522 sgd_solver.cpp:136] Iteration 5800, lr = 0.00909375, m = 0.9
I0801 13:49:48.161201 24522 solver.cpp:353] Iteration 5900 (63.2695 iter/s, 1.58054s/100 iter), loss = 0.0340505
I0801 13:49:48.161223 24522 solver.cpp:375]     Train net output #0: loss = 0.0340501 (* 1 = 0.0340501 loss)
I0801 13:49:48.161227 24522 sgd_solver.cpp:136] Iteration 5900, lr = 0.00907812, m = 0.9
I0801 13:49:49.733209 24522 solver.cpp:404] Sparsity after update:
I0801 13:49:49.735102 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:49:49.735113 24522 net.cpp:2270] conv1a_param_0(0.0125) 
I0801 13:49:49.735126 24522 net.cpp:2270] conv1b_param_0(0.0139) 
I0801 13:49:49.735131 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:49:49.735134 24522 net.cpp:2270] res2a_branch2a_param_0(0.0382) 
I0801 13:49:49.735138 24522 net.cpp:2270] res2a_branch2b_param_0(0.0347) 
I0801 13:49:49.735141 24522 net.cpp:2270] res3a_branch2a_param_0(0.0399) 
I0801 13:49:49.735144 24522 net.cpp:2270] res3a_branch2b_param_0(0.0382) 
I0801 13:49:49.735149 24522 net.cpp:2270] res4a_branch2a_param_0(0.0399) 
I0801 13:49:49.735153 24522 net.cpp:2270] res4a_branch2b_param_0(0.0399) 
I0801 13:49:49.735157 24522 net.cpp:2270] res5a_branch2a_param_0(0.0397) 
I0801 13:49:49.735162 24522 net.cpp:2270] res5a_branch2b_param_0(0.0399) 
I0801 13:49:49.735165 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (93479/2.3599e+06) 0.0396
I0801 13:49:49.735188 24522 solver.cpp:550] Iteration 6000, Testing net (#0)
I0801 13:49:50.553411 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.865295
I0801 13:49:50.553431 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.994412
I0801 13:49:50.553436 24522 solver.cpp:635]     Test net output #2: loss = 0.493651 (* 1 = 0.493651 loss)
I0801 13:49:50.553457 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.81824s
I0801 13:49:50.572526 24549 solver.cpp:450] Finding and applying sparsity: 0.06
I0801 13:50:11.473465 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:50:11.475404 24522 solver.cpp:353] Iteration 6000 (4.28935 iter/s, 23.3136s/100 iter), loss = 0.00997581
I0801 13:50:11.475422 24522 solver.cpp:375]     Train net output #0: loss = 0.00997545 (* 1 = 0.00997545 loss)
I0801 13:50:11.475427 24522 sgd_solver.cpp:136] Iteration 6000, lr = 0.0090625, m = 0.9
I0801 13:50:13.393908 24522 solver.cpp:353] Iteration 6100 (52.1255 iter/s, 1.91845s/100 iter), loss = 0.0885491
I0801 13:50:13.393934 24522 solver.cpp:375]     Train net output #0: loss = 0.0885488 (* 1 = 0.0885488 loss)
I0801 13:50:13.393939 24522 sgd_solver.cpp:136] Iteration 6100, lr = 0.00904687, m = 0.9
I0801 13:50:14.989401 24522 solver.cpp:353] Iteration 6200 (62.6787 iter/s, 1.59544s/100 iter), loss = 0.134932
I0801 13:50:14.989433 24522 solver.cpp:375]     Train net output #0: loss = 0.134931 (* 1 = 0.134931 loss)
I0801 13:50:14.989439 24522 sgd_solver.cpp:136] Iteration 6200, lr = 0.00903125, m = 0.9
I0801 13:50:16.575060 24522 solver.cpp:353] Iteration 6300 (63.0673 iter/s, 1.58561s/100 iter), loss = 0.00373273
I0801 13:50:16.575089 24522 solver.cpp:375]     Train net output #0: loss = 0.00373237 (* 1 = 0.00373237 loss)
I0801 13:50:16.575096 24522 sgd_solver.cpp:136] Iteration 6300, lr = 0.00901563, m = 0.9
I0801 13:50:18.147588 24522 solver.cpp:353] Iteration 6400 (63.5938 iter/s, 1.57248s/100 iter), loss = 0.0401329
I0801 13:50:18.147613 24522 solver.cpp:375]     Train net output #0: loss = 0.0401325 (* 1 = 0.0401325 loss)
I0801 13:50:18.147619 24522 sgd_solver.cpp:136] Iteration 6400, lr = 0.009, m = 0.9
I0801 13:50:19.720974 24522 solver.cpp:353] Iteration 6500 (63.5591 iter/s, 1.57334s/100 iter), loss = 0.0665383
I0801 13:50:19.720999 24522 solver.cpp:375]     Train net output #0: loss = 0.0665379 (* 1 = 0.0665379 loss)
I0801 13:50:19.721006 24522 sgd_solver.cpp:136] Iteration 6500, lr = 0.00898437, m = 0.9
I0801 13:50:21.303812 24522 solver.cpp:353] Iteration 6600 (63.1798 iter/s, 1.58278s/100 iter), loss = 0.175659
I0801 13:50:21.303843 24522 solver.cpp:375]     Train net output #0: loss = 0.175659 (* 1 = 0.175659 loss)
I0801 13:50:21.303848 24522 sgd_solver.cpp:136] Iteration 6600, lr = 0.00896875, m = 0.9
I0801 13:50:22.888906 24522 solver.cpp:353] Iteration 6700 (63.0897 iter/s, 1.58505s/100 iter), loss = 0.00646419
I0801 13:50:22.888933 24522 solver.cpp:375]     Train net output #0: loss = 0.00646384 (* 1 = 0.00646384 loss)
I0801 13:50:22.888938 24522 sgd_solver.cpp:136] Iteration 6700, lr = 0.00895312, m = 0.9
I0801 13:50:24.462033 24522 solver.cpp:353] Iteration 6800 (63.5696 iter/s, 1.57308s/100 iter), loss = 0.0760818
I0801 13:50:24.462082 24522 solver.cpp:375]     Train net output #0: loss = 0.0760815 (* 1 = 0.0760815 loss)
I0801 13:50:24.462095 24522 sgd_solver.cpp:136] Iteration 6800, lr = 0.0089375, m = 0.9
I0801 13:50:26.041519 24522 solver.cpp:353] Iteration 6900 (63.3137 iter/s, 1.57944s/100 iter), loss = 0.197167
I0801 13:50:26.041548 24522 solver.cpp:375]     Train net output #0: loss = 0.197167 (* 1 = 0.197167 loss)
I0801 13:50:26.041554 24522 sgd_solver.cpp:136] Iteration 6900, lr = 0.00892187, m = 0.9
I0801 13:50:27.608525 24522 solver.cpp:404] Sparsity after update:
I0801 13:50:27.610123 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:50:27.610133 24522 net.cpp:2270] conv1a_param_0(0.025) 
I0801 13:50:27.610143 24522 net.cpp:2270] conv1b_param_0(0.0273) 
I0801 13:50:27.610148 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:50:27.610152 24522 net.cpp:2270] res2a_branch2a_param_0(0.059) 
I0801 13:50:27.610157 24522 net.cpp:2270] res2a_branch2b_param_0(0.0556) 
I0801 13:50:27.610162 24522 net.cpp:2270] res3a_branch2a_param_0(0.059) 
I0801 13:50:27.610165 24522 net.cpp:2270] res3a_branch2b_param_0(0.059) 
I0801 13:50:27.610169 24522 net.cpp:2270] res4a_branch2a_param_0(0.0599) 
I0801 13:50:27.610173 24522 net.cpp:2270] res4a_branch2b_param_0(0.059) 
I0801 13:50:27.610177 24522 net.cpp:2270] res5a_branch2a_param_0(0.0596) 
I0801 13:50:27.610182 24522 net.cpp:2270] res5a_branch2b_param_0(0.0598) 
I0801 13:50:27.610186 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (140212/2.3599e+06) 0.0594
I0801 13:50:27.610208 24522 solver.cpp:550] Iteration 7000, Testing net (#0)
I0801 13:50:28.417639 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.874413
I0801 13:50:28.417659 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.994118
I0801 13:50:28.417665 24522 solver.cpp:635]     Test net output #2: loss = 0.469216 (* 1 = 0.469216 loss)
I0801 13:50:28.417683 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.807447s
I0801 13:50:28.433418 24549 solver.cpp:450] Finding and applying sparsity: 0.08
I0801 13:50:48.890005 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:50:48.891901 24522 solver.cpp:353] Iteration 7000 (4.37642 iter/s, 22.8497s/100 iter), loss = 0.00813367
I0801 13:50:48.891926 24522 solver.cpp:375]     Train net output #0: loss = 0.00813329 (* 1 = 0.00813329 loss)
I0801 13:50:48.891934 24522 sgd_solver.cpp:136] Iteration 7000, lr = 0.00890625, m = 0.9
I0801 13:50:50.777755 24522 solver.cpp:353] Iteration 7100 (53.0281 iter/s, 1.88579s/100 iter), loss = 0.0493996
I0801 13:50:50.777778 24522 solver.cpp:375]     Train net output #0: loss = 0.0493993 (* 1 = 0.0493993 loss)
I0801 13:50:50.777782 24522 sgd_solver.cpp:136] Iteration 7100, lr = 0.00889063, m = 0.9
I0801 13:50:52.368500 24522 solver.cpp:353] Iteration 7200 (62.8655 iter/s, 1.5907s/100 iter), loss = 0.0734331
I0801 13:50:52.368527 24522 solver.cpp:375]     Train net output #0: loss = 0.0734326 (* 1 = 0.0734326 loss)
I0801 13:50:52.368531 24522 sgd_solver.cpp:136] Iteration 7200, lr = 0.008875, m = 0.9
I0801 13:50:53.949064 24522 solver.cpp:353] Iteration 7300 (63.2705 iter/s, 1.58051s/100 iter), loss = 0.152591
I0801 13:50:53.949113 24522 solver.cpp:375]     Train net output #0: loss = 0.152591 (* 1 = 0.152591 loss)
I0801 13:50:53.949127 24522 sgd_solver.cpp:136] Iteration 7300, lr = 0.00885937, m = 0.9
I0801 13:50:55.538905 24522 solver.cpp:353] Iteration 7400 (62.9015 iter/s, 1.58979s/100 iter), loss = 0.00469898
I0801 13:50:55.538957 24522 solver.cpp:375]     Train net output #0: loss = 0.0046985 (* 1 = 0.0046985 loss)
I0801 13:50:55.538970 24522 sgd_solver.cpp:136] Iteration 7400, lr = 0.00884375, m = 0.9
I0801 13:50:57.135196 24522 solver.cpp:353] Iteration 7500 (62.6472 iter/s, 1.59624s/100 iter), loss = 0.0361564
I0801 13:50:57.135222 24522 solver.cpp:375]     Train net output #0: loss = 0.036156 (* 1 = 0.036156 loss)
I0801 13:50:57.135228 24522 sgd_solver.cpp:136] Iteration 7500, lr = 0.00882812, m = 0.9
I0801 13:50:58.727721 24522 solver.cpp:353] Iteration 7600 (62.7952 iter/s, 1.59248s/100 iter), loss = 0.0873567
I0801 13:50:58.727748 24522 solver.cpp:375]     Train net output #0: loss = 0.0873561 (* 1 = 0.0873561 loss)
I0801 13:50:58.727756 24522 sgd_solver.cpp:136] Iteration 7600, lr = 0.0088125, m = 0.9
I0801 13:51:00.358357 24522 solver.cpp:353] Iteration 7700 (61.3278 iter/s, 1.63058s/100 iter), loss = 0.00261982
I0801 13:51:00.358383 24522 solver.cpp:375]     Train net output #0: loss = 0.00261929 (* 1 = 0.00261929 loss)
I0801 13:51:00.358388 24522 sgd_solver.cpp:136] Iteration 7700, lr = 0.00879687, m = 0.9
I0801 13:51:01.987216 24522 solver.cpp:353] Iteration 7800 (61.3945 iter/s, 1.62881s/100 iter), loss = 0.0142309
I0801 13:51:01.987267 24522 solver.cpp:375]     Train net output #0: loss = 0.0142303 (* 1 = 0.0142303 loss)
I0801 13:51:01.987279 24522 sgd_solver.cpp:136] Iteration 7800, lr = 0.00878125, m = 0.9
I0801 13:51:03.597430 24522 solver.cpp:353] Iteration 7900 (62.1056 iter/s, 1.61016s/100 iter), loss = 0.113554
I0801 13:51:03.597456 24522 solver.cpp:375]     Train net output #0: loss = 0.113553 (* 1 = 0.113553 loss)
I0801 13:51:03.597462 24522 sgd_solver.cpp:136] Iteration 7900, lr = 0.00876562, m = 0.9
I0801 13:51:05.217314 24522 solver.cpp:404] Sparsity after update:
I0801 13:51:05.218951 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:51:05.218961 24522 net.cpp:2270] conv1a_param_0(0.0367) 
I0801 13:51:05.218967 24522 net.cpp:2270] conv1b_param_0(0.0339) 
I0801 13:51:05.218969 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:51:05.218971 24522 net.cpp:2270] res2a_branch2a_param_0(0.0799) 
I0801 13:51:05.218974 24522 net.cpp:2270] res2a_branch2b_param_0(0.0764) 
I0801 13:51:05.218976 24522 net.cpp:2270] res3a_branch2a_param_0(0.0799) 
I0801 13:51:05.218978 24522 net.cpp:2270] res3a_branch2b_param_0(0.0799) 
I0801 13:51:05.218981 24522 net.cpp:2270] res4a_branch2a_param_0(0.0799) 
I0801 13:51:05.218983 24522 net.cpp:2270] res4a_branch2b_param_0(0.0798) 
I0801 13:51:05.218986 24522 net.cpp:2270] res5a_branch2a_param_0(0.0795) 
I0801 13:51:05.218988 24522 net.cpp:2270] res5a_branch2b_param_0(0.0798) 
I0801 13:51:05.218991 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (187363/2.3599e+06) 0.0794
I0801 13:51:05.219012 24522 solver.cpp:550] Iteration 8000, Testing net (#0)
I0801 13:51:06.027817 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.856178
I0801 13:51:06.027838 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995588
I0801 13:51:06.027843 24522 solver.cpp:635]     Test net output #2: loss = 0.555665 (* 1 = 0.555665 loss)
I0801 13:51:06.027866 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.808827s
I0801 13:51:06.045866 24549 solver.cpp:450] Finding and applying sparsity: 0.1
I0801 13:51:27.131675 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:51:27.133592 24522 solver.cpp:353] Iteration 8000 (4.2489 iter/s, 23.5355s/100 iter), loss = 0.0636911
I0801 13:51:27.133615 24522 solver.cpp:375]     Train net output #0: loss = 0.0636906 (* 1 = 0.0636906 loss)
I0801 13:51:27.133623 24522 sgd_solver.cpp:136] Iteration 8000, lr = 0.00875, m = 0.9
I0801 13:51:28.965428 24522 solver.cpp:353] Iteration 8100 (54.5918 iter/s, 1.83178s/100 iter), loss = 0.0425892
I0801 13:51:28.965453 24522 solver.cpp:375]     Train net output #0: loss = 0.0425887 (* 1 = 0.0425887 loss)
I0801 13:51:28.965458 24522 sgd_solver.cpp:136] Iteration 8100, lr = 0.00873438, m = 0.9
I0801 13:51:30.567754 24522 solver.cpp:353] Iteration 8200 (62.4112 iter/s, 1.60228s/100 iter), loss = 0.00256245
I0801 13:51:30.567777 24522 solver.cpp:375]     Train net output #0: loss = 0.00256192 (* 1 = 0.00256192 loss)
I0801 13:51:30.567781 24522 sgd_solver.cpp:136] Iteration 8200, lr = 0.00871875, m = 0.9
I0801 13:51:32.162586 24522 solver.cpp:353] Iteration 8300 (62.7045 iter/s, 1.59478s/100 iter), loss = 0.0987593
I0801 13:51:32.162633 24522 solver.cpp:375]     Train net output #0: loss = 0.0987587 (* 1 = 0.0987587 loss)
I0801 13:51:32.162644 24522 sgd_solver.cpp:136] Iteration 8300, lr = 0.00870312, m = 0.9
I0801 13:51:33.751250 24522 solver.cpp:353] Iteration 8400 (62.948 iter/s, 1.58861s/100 iter), loss = 0.0758556
I0801 13:51:33.751304 24522 solver.cpp:375]     Train net output #0: loss = 0.0758551 (* 1 = 0.0758551 loss)
I0801 13:51:33.751319 24522 sgd_solver.cpp:136] Iteration 8400, lr = 0.0086875, m = 0.9
I0801 13:51:35.330265 24522 solver.cpp:353] Iteration 8500 (63.3327 iter/s, 1.57896s/100 iter), loss = 0.10946
I0801 13:51:35.330289 24522 solver.cpp:375]     Train net output #0: loss = 0.10946 (* 1 = 0.10946 loss)
I0801 13:51:35.330294 24522 sgd_solver.cpp:136] Iteration 8500, lr = 0.00867188, m = 0.9
I0801 13:51:36.910332 24522 solver.cpp:353] Iteration 8600 (63.2903 iter/s, 1.58002s/100 iter), loss = 0.00420942
I0801 13:51:36.910382 24522 solver.cpp:375]     Train net output #0: loss = 0.00420892 (* 1 = 0.00420892 loss)
I0801 13:51:36.910392 24522 sgd_solver.cpp:136] Iteration 8600, lr = 0.00865625, m = 0.9
I0801 13:51:38.507546 24522 solver.cpp:353] Iteration 8700 (62.611 iter/s, 1.59716s/100 iter), loss = 0.0117062
I0801 13:51:38.507572 24522 solver.cpp:375]     Train net output #0: loss = 0.0117057 (* 1 = 0.0117057 loss)
I0801 13:51:38.507577 24522 sgd_solver.cpp:136] Iteration 8700, lr = 0.00864062, m = 0.9
I0801 13:51:40.087502 24522 solver.cpp:353] Iteration 8800 (63.295 iter/s, 1.5799s/100 iter), loss = 0.061608
I0801 13:51:40.087529 24522 solver.cpp:375]     Train net output #0: loss = 0.0616075 (* 1 = 0.0616075 loss)
I0801 13:51:40.087537 24522 sgd_solver.cpp:136] Iteration 8800, lr = 0.008625, m = 0.9
I0801 13:51:41.661341 24522 solver.cpp:353] Iteration 8900 (63.5409 iter/s, 1.57379s/100 iter), loss = 0.177315
I0801 13:51:41.661366 24522 solver.cpp:375]     Train net output #0: loss = 0.177315 (* 1 = 0.177315 loss)
I0801 13:51:41.661372 24522 sgd_solver.cpp:136] Iteration 8900, lr = 0.00860937, m = 0.9
I0801 13:51:43.235909 24522 solver.cpp:404] Sparsity after update:
I0801 13:51:43.237548 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:51:43.237557 24522 net.cpp:2270] conv1a_param_0(0.0371) 
I0801 13:51:43.237566 24522 net.cpp:2270] conv1b_param_0(0.0273) 
I0801 13:51:43.237571 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:51:43.237576 24522 net.cpp:2270] res2a_branch2a_param_0(0.0972) 
I0801 13:51:43.237581 24522 net.cpp:2270] res2a_branch2b_param_0(0.0972) 
I0801 13:51:43.237584 24522 net.cpp:2270] res3a_branch2a_param_0(0.099) 
I0801 13:51:43.237589 24522 net.cpp:2270] res3a_branch2b_param_0(0.0972) 
I0801 13:51:43.237593 24522 net.cpp:2270] res4a_branch2a_param_0(0.0998) 
I0801 13:51:43.237597 24522 net.cpp:2270] res4a_branch2b_param_0(0.099) 
I0801 13:51:43.237601 24522 net.cpp:2270] res5a_branch2a_param_0(0.0991) 
I0801 13:51:43.237606 24522 net.cpp:2270] res5a_branch2b_param_0(0.0998) 
I0801 13:51:43.237610 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (233478/2.3599e+06) 0.0989
I0801 13:51:43.237634 24522 solver.cpp:550] Iteration 9000, Testing net (#0)
I0801 13:51:44.083238 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.890001
I0801 13:51:44.083261 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.994118
I0801 13:51:44.083271 24522 solver.cpp:635]     Test net output #2: loss = 0.452671 (* 1 = 0.452671 loss)
I0801 13:51:44.083297 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.845632s
I0801 13:51:44.101447 24549 solver.cpp:450] Finding and applying sparsity: 0.12
I0801 13:52:04.706492 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:52:04.708408 24522 solver.cpp:353] Iteration 9000 (4.33907 iter/s, 23.0464s/100 iter), loss = 0.00604583
I0801 13:52:04.708426 24522 solver.cpp:375]     Train net output #0: loss = 0.00604533 (* 1 = 0.00604533 loss)
I0801 13:52:04.708432 24522 sgd_solver.cpp:136] Iteration 9000, lr = 0.00859375, m = 0.9
I0801 13:52:06.327913 24487 data_reader.cpp:264] Starting prefetch of epoch 2
I0801 13:52:06.535255 24522 solver.cpp:353] Iteration 9100 (54.7409 iter/s, 1.82679s/100 iter), loss = 0.044862
I0801 13:52:06.535284 24522 solver.cpp:375]     Train net output #0: loss = 0.0448615 (* 1 = 0.0448615 loss)
I0801 13:52:06.535289 24522 sgd_solver.cpp:136] Iteration 9100, lr = 0.00857813, m = 0.9
I0801 13:52:08.141448 24522 solver.cpp:353] Iteration 9200 (62.261 iter/s, 1.60614s/100 iter), loss = 0.0270382
I0801 13:52:08.141470 24522 solver.cpp:375]     Train net output #0: loss = 0.0270377 (* 1 = 0.0270377 loss)
I0801 13:52:08.141474 24522 sgd_solver.cpp:136] Iteration 9200, lr = 0.0085625, m = 0.9
I0801 13:52:09.712714 24522 solver.cpp:353] Iteration 9300 (63.6449 iter/s, 1.57122s/100 iter), loss = 0.018198
I0801 13:52:09.712766 24522 solver.cpp:375]     Train net output #0: loss = 0.0181974 (* 1 = 0.0181974 loss)
I0801 13:52:09.712783 24522 sgd_solver.cpp:136] Iteration 9300, lr = 0.00854687, m = 0.9
I0801 13:52:11.308285 24522 solver.cpp:353] Iteration 9400 (62.6754 iter/s, 1.59552s/100 iter), loss = 0.194368
I0801 13:52:11.308332 24522 solver.cpp:375]     Train net output #0: loss = 0.194368 (* 1 = 0.194368 loss)
I0801 13:52:11.308346 24522 sgd_solver.cpp:136] Iteration 9400, lr = 0.00853125, m = 0.9
I0801 13:52:12.912261 24522 solver.cpp:353] Iteration 9500 (62.3471 iter/s, 1.60392s/100 iter), loss = 0.00284404
I0801 13:52:12.912288 24522 solver.cpp:375]     Train net output #0: loss = 0.00284352 (* 1 = 0.00284352 loss)
I0801 13:52:12.912294 24522 sgd_solver.cpp:136] Iteration 9500, lr = 0.00851563, m = 0.9
I0801 13:52:14.489341 24522 solver.cpp:353] Iteration 9600 (63.4104 iter/s, 1.57703s/100 iter), loss = 0.00472684
I0801 13:52:14.489364 24522 solver.cpp:375]     Train net output #0: loss = 0.00472632 (* 1 = 0.00472632 loss)
I0801 13:52:14.489369 24522 sgd_solver.cpp:136] Iteration 9600, lr = 0.0085, m = 0.9
I0801 13:52:16.074249 24522 solver.cpp:353] Iteration 9700 (63.0969 iter/s, 1.58486s/100 iter), loss = 0.0140821
I0801 13:52:16.074275 24522 solver.cpp:375]     Train net output #0: loss = 0.0140815 (* 1 = 0.0140815 loss)
I0801 13:52:16.074282 24522 sgd_solver.cpp:136] Iteration 9700, lr = 0.00848437, m = 0.9
I0801 13:52:17.661612 24522 solver.cpp:353] Iteration 9800 (62.9996 iter/s, 1.58731s/100 iter), loss = 0.0271521
I0801 13:52:17.661672 24522 solver.cpp:375]     Train net output #0: loss = 0.0271516 (* 1 = 0.0271516 loss)
I0801 13:52:17.661689 24522 sgd_solver.cpp:136] Iteration 9800, lr = 0.00846875, m = 0.9
I0801 13:52:19.241834 24522 solver.cpp:353] Iteration 9900 (63.2842 iter/s, 1.58017s/100 iter), loss = 0.0127208
I0801 13:52:19.241858 24522 solver.cpp:375]     Train net output #0: loss = 0.0127203 (* 1 = 0.0127203 loss)
I0801 13:52:19.241861 24522 sgd_solver.cpp:136] Iteration 9900, lr = 0.00845312, m = 0.9
I0801 13:52:20.808187 24522 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_10000.caffemodel
I0801 13:52:20.826392 24522 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_10000.solverstate
I0801 13:52:20.830085 24522 solver.cpp:404] Sparsity after update:
I0801 13:52:20.831630 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:52:20.831638 24522 net.cpp:2270] conv1a_param_0(0.0496) 
I0801 13:52:20.831647 24522 net.cpp:2270] conv1b_param_0(0.0382) 
I0801 13:52:20.831652 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:52:20.831655 24522 net.cpp:2270] res2a_branch2a_param_0(0.118) 
I0801 13:52:20.831660 24522 net.cpp:2270] res2a_branch2b_param_0(0.118) 
I0801 13:52:20.831665 24522 net.cpp:2270] res3a_branch2a_param_0(0.12) 
I0801 13:52:20.831676 24522 net.cpp:2270] res3a_branch2b_param_0(0.118) 
I0801 13:52:20.831681 24522 net.cpp:2270] res4a_branch2a_param_0(0.12) 
I0801 13:52:20.831686 24522 net.cpp:2270] res4a_branch2b_param_0(0.12) 
I0801 13:52:20.831689 24522 net.cpp:2270] res5a_branch2a_param_0(0.119) 
I0801 13:52:20.831693 24522 net.cpp:2270] res5a_branch2b_param_0(0.12) 
I0801 13:52:20.831698 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (280589/2.3599e+06) 0.119
I0801 13:52:20.831709 24522 solver.cpp:550] Iteration 10000, Testing net (#0)
I0801 13:52:21.623633 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.879119
I0801 13:52:21.623653 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 13:52:21.623659 24522 solver.cpp:635]     Test net output #2: loss = 0.462688 (* 1 = 0.462688 loss)
I0801 13:52:21.623677 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.791939s
I0801 13:52:21.639384 24549 solver.cpp:450] Finding and applying sparsity: 0.14
I0801 13:52:41.768389 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:52:41.770337 24522 solver.cpp:353] Iteration 10000 (4.43894 iter/s, 22.5279s/100 iter), loss = 0.019464
I0801 13:52:41.770355 24522 solver.cpp:375]     Train net output #0: loss = 0.0194634 (* 1 = 0.0194634 loss)
I0801 13:52:41.770364 24522 sgd_solver.cpp:136] Iteration 10000, lr = 0.0084375, m = 0.9
I0801 13:52:43.621383 24522 solver.cpp:353] Iteration 10100 (54.0252 iter/s, 1.85099s/100 iter), loss = 0.00561036
I0801 13:52:43.621430 24522 solver.cpp:375]     Train net output #0: loss = 0.00560979 (* 1 = 0.00560979 loss)
I0801 13:52:43.621443 24522 sgd_solver.cpp:136] Iteration 10100, lr = 0.00842187, m = 0.9
I0801 13:52:45.231158 24522 solver.cpp:353] Iteration 10200 (62.1225 iter/s, 1.60972s/100 iter), loss = 0.0049929
I0801 13:52:45.231184 24522 solver.cpp:375]     Train net output #0: loss = 0.00499233 (* 1 = 0.00499233 loss)
I0801 13:52:45.231189 24522 sgd_solver.cpp:136] Iteration 10200, lr = 0.00840625, m = 0.9
I0801 13:52:46.800251 24522 solver.cpp:353] Iteration 10300 (63.733 iter/s, 1.56905s/100 iter), loss = 0.00173509
I0801 13:52:46.800277 24522 solver.cpp:375]     Train net output #0: loss = 0.00173452 (* 1 = 0.00173452 loss)
I0801 13:52:46.800282 24522 sgd_solver.cpp:136] Iteration 10300, lr = 0.00839063, m = 0.9
I0801 13:52:48.375249 24522 solver.cpp:353] Iteration 10400 (63.4941 iter/s, 1.57495s/100 iter), loss = 0.00155696
I0801 13:52:48.375277 24522 solver.cpp:375]     Train net output #0: loss = 0.00155639 (* 1 = 0.00155639 loss)
I0801 13:52:48.375283 24522 sgd_solver.cpp:136] Iteration 10400, lr = 0.008375, m = 0.9
I0801 13:52:49.958428 24522 solver.cpp:353] Iteration 10500 (63.1662 iter/s, 1.58313s/100 iter), loss = 0.0358158
I0801 13:52:49.958457 24522 solver.cpp:375]     Train net output #0: loss = 0.0358152 (* 1 = 0.0358152 loss)
I0801 13:52:49.958463 24522 sgd_solver.cpp:136] Iteration 10500, lr = 0.00835937, m = 0.9
I0801 13:52:51.542914 24522 solver.cpp:353] Iteration 10600 (63.1139 iter/s, 1.58444s/100 iter), loss = 0.0181446
I0801 13:52:51.542943 24522 solver.cpp:375]     Train net output #0: loss = 0.018144 (* 1 = 0.018144 loss)
I0801 13:52:51.542950 24522 sgd_solver.cpp:136] Iteration 10600, lr = 0.00834375, m = 0.9
I0801 13:52:53.116212 24522 solver.cpp:353] Iteration 10700 (63.5627 iter/s, 1.57325s/100 iter), loss = 0.0869192
I0801 13:52:53.116236 24522 solver.cpp:375]     Train net output #0: loss = 0.0869186 (* 1 = 0.0869186 loss)
I0801 13:52:53.116241 24522 sgd_solver.cpp:136] Iteration 10700, lr = 0.00832812, m = 0.9
I0801 13:52:54.704728 24522 solver.cpp:353] Iteration 10800 (62.9539 iter/s, 1.58846s/100 iter), loss = 0.02794
I0801 13:52:54.704754 24522 solver.cpp:375]     Train net output #0: loss = 0.0279395 (* 1 = 0.0279395 loss)
I0801 13:52:54.704759 24522 sgd_solver.cpp:136] Iteration 10800, lr = 0.0083125, m = 0.9
I0801 13:52:56.299711 24522 solver.cpp:353] Iteration 10900 (62.6985 iter/s, 1.59493s/100 iter), loss = 0.0697004
I0801 13:52:56.299764 24522 solver.cpp:375]     Train net output #0: loss = 0.0696998 (* 1 = 0.0696998 loss)
I0801 13:52:56.299778 24522 sgd_solver.cpp:136] Iteration 10900, lr = 0.00829687, m = 0.9
I0801 13:52:57.875438 24522 solver.cpp:404] Sparsity after update:
I0801 13:52:57.877035 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:52:57.877044 24522 net.cpp:2270] conv1a_param_0(0.06) 
I0801 13:52:57.877051 24522 net.cpp:2270] conv1b_param_0(0.104) 
I0801 13:52:57.877053 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:52:57.877055 24522 net.cpp:2270] res2a_branch2a_param_0(0.139) 
I0801 13:52:57.877059 24522 net.cpp:2270] res2a_branch2b_param_0(0.139) 
I0801 13:52:57.877061 24522 net.cpp:2270] res3a_branch2a_param_0(0.139) 
I0801 13:52:57.877063 24522 net.cpp:2270] res3a_branch2b_param_0(0.139) 
I0801 13:52:57.877064 24522 net.cpp:2270] res4a_branch2a_param_0(0.14) 
I0801 13:52:57.877066 24522 net.cpp:2270] res4a_branch2b_param_0(0.139) 
I0801 13:52:57.877068 24522 net.cpp:2270] res5a_branch2a_param_0(0.139) 
I0801 13:52:57.877070 24522 net.cpp:2270] res5a_branch2b_param_0(0.14) 
I0801 13:52:57.877073 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (327396/2.3599e+06) 0.139
I0801 13:52:57.877090 24522 solver.cpp:550] Iteration 11000, Testing net (#0)
I0801 13:52:58.686298 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.879413
I0801 13:52:58.686317 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.993529
I0801 13:52:58.686322 24522 solver.cpp:635]     Test net output #2: loss = 0.489197 (* 1 = 0.489197 loss)
I0801 13:52:58.686336 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.809219s
I0801 13:52:58.701864 24549 solver.cpp:450] Finding and applying sparsity: 0.16
I0801 13:53:18.956146 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:53:18.962685 24522 solver.cpp:353] Iteration 11000 (4.41261 iter/s, 22.6623s/100 iter), loss = 0.00418054
I0801 13:53:18.962710 24522 solver.cpp:375]     Train net output #0: loss = 0.0041799 (* 1 = 0.0041799 loss)
I0801 13:53:18.962718 24522 sgd_solver.cpp:136] Iteration 11000, lr = 0.00828125, m = 0.9
I0801 13:53:20.818753 24522 solver.cpp:353] Iteration 11100 (53.879 iter/s, 1.85601s/100 iter), loss = 0.00273855
I0801 13:53:20.818776 24522 solver.cpp:375]     Train net output #0: loss = 0.00273792 (* 1 = 0.00273792 loss)
I0801 13:53:20.818783 24522 sgd_solver.cpp:136] Iteration 11100, lr = 0.00826562, m = 0.9
I0801 13:53:22.397426 24522 solver.cpp:353] Iteration 11200 (63.3464 iter/s, 1.57862s/100 iter), loss = 0.00614682
I0801 13:53:22.397454 24522 solver.cpp:375]     Train net output #0: loss = 0.00614619 (* 1 = 0.00614619 loss)
I0801 13:53:22.397460 24522 sgd_solver.cpp:136] Iteration 11200, lr = 0.00825, m = 0.9
I0801 13:53:23.977322 24522 solver.cpp:353] Iteration 11300 (63.2972 iter/s, 1.57985s/100 iter), loss = 0.0379475
I0801 13:53:23.977350 24522 solver.cpp:375]     Train net output #0: loss = 0.0379469 (* 1 = 0.0379469 loss)
I0801 13:53:23.977356 24522 sgd_solver.cpp:136] Iteration 11300, lr = 0.00823438, m = 0.9
I0801 13:53:25.558254 24522 solver.cpp:353] Iteration 11400 (63.2559 iter/s, 1.58088s/100 iter), loss = 0.00605096
I0801 13:53:25.558303 24522 solver.cpp:375]     Train net output #0: loss = 0.00605031 (* 1 = 0.00605031 loss)
I0801 13:53:25.558315 24522 sgd_solver.cpp:136] Iteration 11400, lr = 0.00821875, m = 0.9
I0801 13:53:27.145300 24522 solver.cpp:353] Iteration 11500 (63.0121 iter/s, 1.587s/100 iter), loss = 0.0296246
I0801 13:53:27.145329 24522 solver.cpp:375]     Train net output #0: loss = 0.029624 (* 1 = 0.029624 loss)
I0801 13:53:27.145336 24522 sgd_solver.cpp:136] Iteration 11500, lr = 0.00820312, m = 0.9
I0801 13:53:28.762676 24522 solver.cpp:353] Iteration 11600 (61.8305 iter/s, 1.61733s/100 iter), loss = 0.00596845
I0801 13:53:28.762701 24522 solver.cpp:375]     Train net output #0: loss = 0.0059678 (* 1 = 0.0059678 loss)
I0801 13:53:28.762706 24522 sgd_solver.cpp:136] Iteration 11600, lr = 0.0081875, m = 0.9
I0801 13:53:30.372643 24522 solver.cpp:353] Iteration 11700 (62.115 iter/s, 1.60992s/100 iter), loss = 0.00206239
I0801 13:53:30.372668 24522 solver.cpp:375]     Train net output #0: loss = 0.00206173 (* 1 = 0.00206173 loss)
I0801 13:53:30.372673 24522 sgd_solver.cpp:136] Iteration 11700, lr = 0.00817188, m = 0.9
I0801 13:53:31.955338 24522 solver.cpp:353] Iteration 11800 (63.1854 iter/s, 1.58264s/100 iter), loss = 0.00612272
I0801 13:53:31.955363 24522 solver.cpp:375]     Train net output #0: loss = 0.00612208 (* 1 = 0.00612208 loss)
I0801 13:53:31.955369 24522 sgd_solver.cpp:136] Iteration 11800, lr = 0.00815625, m = 0.9
I0801 13:53:33.543051 24522 solver.cpp:353] Iteration 11900 (62.9857 iter/s, 1.58766s/100 iter), loss = 0.00345716
I0801 13:53:33.543076 24522 solver.cpp:375]     Train net output #0: loss = 0.00345652 (* 1 = 0.00345652 loss)
I0801 13:53:33.543082 24522 sgd_solver.cpp:136] Iteration 11900, lr = 0.00814062, m = 0.9
I0801 13:53:35.118019 24522 solver.cpp:404] Sparsity after update:
I0801 13:53:35.119595 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:53:35.119603 24522 net.cpp:2270] conv1a_param_0(0.0613) 
I0801 13:53:35.119611 24522 net.cpp:2270] conv1b_param_0(0.115) 
I0801 13:53:35.119612 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:53:35.119616 24522 net.cpp:2270] res2a_branch2a_param_0(0.16) 
I0801 13:53:35.119618 24522 net.cpp:2270] res2a_branch2b_param_0(0.16) 
I0801 13:53:35.119622 24522 net.cpp:2270] res3a_branch2a_param_0(0.16) 
I0801 13:53:35.119626 24522 net.cpp:2270] res3a_branch2b_param_0(0.16) 
I0801 13:53:35.119629 24522 net.cpp:2270] res4a_branch2a_param_0(0.16) 
I0801 13:53:35.119632 24522 net.cpp:2270] res4a_branch2b_param_0(0.16) 
I0801 13:53:35.119637 24522 net.cpp:2270] res5a_branch2a_param_0(0.158) 
I0801 13:53:35.119640 24522 net.cpp:2270] res5a_branch2b_param_0(0.16) 
I0801 13:53:35.119644 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (374110/2.3599e+06) 0.159
I0801 13:53:35.119673 24522 solver.cpp:550] Iteration 12000, Testing net (#0)
I0801 13:53:35.927697 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.886178
I0801 13:53:35.927717 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995294
I0801 13:53:35.927722 24522 solver.cpp:635]     Test net output #2: loss = 0.475343 (* 1 = 0.475343 loss)
I0801 13:53:35.927736 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.808037s
I0801 13:53:35.943308 24549 solver.cpp:450] Finding and applying sparsity: 0.18
I0801 13:53:55.797487 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:53:55.799330 24522 solver.cpp:353] Iteration 12000 (4.49324 iter/s, 22.2557s/100 iter), loss = 0.0141433
I0801 13:53:55.799355 24522 solver.cpp:375]     Train net output #0: loss = 0.0141427 (* 1 = 0.0141427 loss)
I0801 13:53:55.799363 24522 sgd_solver.cpp:136] Iteration 12000, lr = 0.008125, m = 0.9
I0801 13:53:57.640625 24522 solver.cpp:353] Iteration 12100 (54.3113 iter/s, 1.84124s/100 iter), loss = 0.00646575
I0801 13:53:57.640650 24522 solver.cpp:375]     Train net output #0: loss = 0.00646508 (* 1 = 0.00646508 loss)
I0801 13:53:57.640656 24522 sgd_solver.cpp:136] Iteration 12100, lr = 0.00810937, m = 0.9
I0801 13:53:59.226744 24522 solver.cpp:353] Iteration 12200 (63.049 iter/s, 1.58607s/100 iter), loss = 0.00407551
I0801 13:53:59.226771 24522 solver.cpp:375]     Train net output #0: loss = 0.00407485 (* 1 = 0.00407485 loss)
I0801 13:53:59.226778 24522 sgd_solver.cpp:136] Iteration 12200, lr = 0.00809375, m = 0.9
I0801 13:54:00.822548 24522 solver.cpp:353] Iteration 12300 (62.6663 iter/s, 1.59575s/100 iter), loss = 0.0106629
I0801 13:54:00.822573 24522 solver.cpp:375]     Train net output #0: loss = 0.0106622 (* 1 = 0.0106622 loss)
I0801 13:54:00.822579 24522 sgd_solver.cpp:136] Iteration 12300, lr = 0.00807813, m = 0.9
I0801 13:54:02.400616 24522 solver.cpp:353] Iteration 12400 (63.3706 iter/s, 1.57802s/100 iter), loss = 0.00690153
I0801 13:54:02.400676 24522 solver.cpp:375]     Train net output #0: loss = 0.00690087 (* 1 = 0.00690087 loss)
I0801 13:54:02.400694 24522 sgd_solver.cpp:136] Iteration 12400, lr = 0.0080625, m = 0.9
I0801 13:54:03.985097 24522 solver.cpp:353] Iteration 12500 (63.1141 iter/s, 1.58443s/100 iter), loss = 0.000736088
I0801 13:54:03.985123 24522 solver.cpp:375]     Train net output #0: loss = 0.000735427 (* 1 = 0.000735427 loss)
I0801 13:54:03.985128 24522 sgd_solver.cpp:136] Iteration 12500, lr = 0.00804687, m = 0.9
I0801 13:54:05.571285 24522 solver.cpp:353] Iteration 12600 (63.0462 iter/s, 1.58614s/100 iter), loss = 0.129925
I0801 13:54:05.571310 24522 solver.cpp:375]     Train net output #0: loss = 0.129924 (* 1 = 0.129924 loss)
I0801 13:54:05.571315 24522 sgd_solver.cpp:136] Iteration 12600, lr = 0.00803125, m = 0.9
I0801 13:54:07.160998 24522 solver.cpp:353] Iteration 12700 (62.9063 iter/s, 1.58967s/100 iter), loss = 0.00184096
I0801 13:54:07.161023 24522 solver.cpp:375]     Train net output #0: loss = 0.00184029 (* 1 = 0.00184029 loss)
I0801 13:54:07.161028 24522 sgd_solver.cpp:136] Iteration 12700, lr = 0.00801562, m = 0.9
I0801 13:54:08.755039 24522 solver.cpp:353] Iteration 12800 (62.7356 iter/s, 1.59399s/100 iter), loss = 0.00403571
I0801 13:54:08.755067 24522 solver.cpp:375]     Train net output #0: loss = 0.00403503 (* 1 = 0.00403503 loss)
I0801 13:54:08.755074 24522 sgd_solver.cpp:136] Iteration 12800, lr = 0.008, m = 0.9
I0801 13:54:10.333870 24522 solver.cpp:353] Iteration 12900 (63.3401 iter/s, 1.57878s/100 iter), loss = 0.0089635
I0801 13:54:10.333902 24522 solver.cpp:375]     Train net output #0: loss = 0.00896281 (* 1 = 0.00896281 loss)
I0801 13:54:10.333909 24522 sgd_solver.cpp:136] Iteration 12900, lr = 0.00798437, m = 0.9
I0801 13:54:11.898337 24522 solver.cpp:404] Sparsity after update:
I0801 13:54:11.900096 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:54:11.900105 24522 net.cpp:2270] conv1a_param_0(0.0729) 
I0801 13:54:11.900115 24522 net.cpp:2270] conv1b_param_0(0.125) 
I0801 13:54:11.900120 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:54:11.900123 24522 net.cpp:2270] res2a_branch2a_param_0(0.177) 
I0801 13:54:11.900127 24522 net.cpp:2270] res2a_branch2b_param_0(0.174) 
I0801 13:54:11.900131 24522 net.cpp:2270] res3a_branch2a_param_0(0.179) 
I0801 13:54:11.900135 24522 net.cpp:2270] res3a_branch2b_param_0(0.177) 
I0801 13:54:11.900140 24522 net.cpp:2270] res4a_branch2a_param_0(0.18) 
I0801 13:54:11.900144 24522 net.cpp:2270] res4a_branch2b_param_0(0.179) 
I0801 13:54:11.900148 24522 net.cpp:2270] res5a_branch2a_param_0(0.177) 
I0801 13:54:11.900151 24522 net.cpp:2270] res5a_branch2b_param_0(0.179) 
I0801 13:54:11.900156 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (419373/2.3599e+06) 0.178
I0801 13:54:11.900182 24522 solver.cpp:550] Iteration 13000, Testing net (#0)
I0801 13:54:12.707868 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.896472
I0801 13:54:12.707886 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995
I0801 13:54:12.707893 24522 solver.cpp:635]     Test net output #2: loss = 0.416913 (* 1 = 0.416913 loss)
I0801 13:54:12.707914 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.807706s
I0801 13:54:12.725986 24549 solver.cpp:450] Finding and applying sparsity: 0.2
I0801 13:54:32.868176 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:54:32.870106 24522 solver.cpp:353] Iteration 13000 (4.43742 iter/s, 22.5356s/100 iter), loss = 0.0015026
I0801 13:54:32.870126 24522 solver.cpp:375]     Train net output #0: loss = 0.00150192 (* 1 = 0.00150192 loss)
I0801 13:54:32.870133 24522 sgd_solver.cpp:136] Iteration 13000, lr = 0.00796875, m = 0.9
I0801 13:54:34.666558 24522 solver.cpp:353] Iteration 13100 (55.6671 iter/s, 1.79639s/100 iter), loss = 0.0182494
I0801 13:54:34.666582 24522 solver.cpp:375]     Train net output #0: loss = 0.0182487 (* 1 = 0.0182487 loss)
I0801 13:54:34.666589 24522 sgd_solver.cpp:136] Iteration 13100, lr = 0.00795313, m = 0.9
I0801 13:54:36.255295 24522 solver.cpp:353] Iteration 13200 (62.9452 iter/s, 1.58868s/100 iter), loss = 0.00488502
I0801 13:54:36.255364 24522 solver.cpp:375]     Train net output #0: loss = 0.00488433 (* 1 = 0.00488433 loss)
I0801 13:54:36.255383 24522 sgd_solver.cpp:136] Iteration 13200, lr = 0.0079375, m = 0.9
I0801 13:54:37.842905 24522 solver.cpp:353] Iteration 13300 (62.9897 iter/s, 1.58756s/100 iter), loss = 0.00324234
I0801 13:54:37.842954 24522 solver.cpp:375]     Train net output #0: loss = 0.00324164 (* 1 = 0.00324164 loss)
I0801 13:54:37.842967 24522 sgd_solver.cpp:136] Iteration 13300, lr = 0.00792187, m = 0.9
I0801 13:54:39.423236 24522 solver.cpp:353] Iteration 13400 (63.2799 iter/s, 1.58028s/100 iter), loss = 0.000507683
I0801 13:54:39.423265 24522 solver.cpp:375]     Train net output #0: loss = 0.000506988 (* 1 = 0.000506988 loss)
I0801 13:54:39.423272 24522 sgd_solver.cpp:136] Iteration 13400, lr = 0.00790625, m = 0.9
I0801 13:54:40.998116 24522 solver.cpp:353] Iteration 13500 (63.499 iter/s, 1.57483s/100 iter), loss = 0.00423786
I0801 13:54:40.998138 24522 solver.cpp:375]     Train net output #0: loss = 0.00423716 (* 1 = 0.00423716 loss)
I0801 13:54:40.998142 24522 sgd_solver.cpp:136] Iteration 13500, lr = 0.00789062, m = 0.9
I0801 13:54:42.579310 24522 solver.cpp:353] Iteration 13600 (63.2452 iter/s, 1.58115s/100 iter), loss = 0.000787131
I0801 13:54:42.579335 24522 solver.cpp:375]     Train net output #0: loss = 0.000786428 (* 1 = 0.000786428 loss)
I0801 13:54:42.579341 24522 sgd_solver.cpp:136] Iteration 13600, lr = 0.007875, m = 0.9
I0801 13:54:43.100240 24487 data_reader.cpp:264] Starting prefetch of epoch 3
I0801 13:54:44.164624 24522 solver.cpp:353] Iteration 13700 (63.0809 iter/s, 1.58527s/100 iter), loss = 0.00111901
I0801 13:54:44.164652 24522 solver.cpp:375]     Train net output #0: loss = 0.00111831 (* 1 = 0.00111831 loss)
I0801 13:54:44.164660 24522 sgd_solver.cpp:136] Iteration 13700, lr = 0.00785937, m = 0.9
I0801 13:54:45.757823 24522 solver.cpp:353] Iteration 13800 (62.7688 iter/s, 1.59315s/100 iter), loss = 0.00202114
I0801 13:54:45.757853 24522 solver.cpp:375]     Train net output #0: loss = 0.00202045 (* 1 = 0.00202045 loss)
I0801 13:54:45.757858 24522 sgd_solver.cpp:136] Iteration 13800, lr = 0.00784375, m = 0.9
I0801 13:54:47.347936 24522 solver.cpp:353] Iteration 13900 (62.8906 iter/s, 1.59006s/100 iter), loss = 0.00124864
I0801 13:54:47.347959 24522 solver.cpp:375]     Train net output #0: loss = 0.00124795 (* 1 = 0.00124795 loss)
I0801 13:54:47.347964 24522 sgd_solver.cpp:136] Iteration 13900, lr = 0.00782812, m = 0.9
I0801 13:54:48.899562 24522 solver.cpp:404] Sparsity after update:
I0801 13:54:48.901207 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:54:48.901216 24522 net.cpp:2270] conv1a_param_0(0.0838) 
I0801 13:54:48.901222 24522 net.cpp:2270] conv1b_param_0(0.146) 
I0801 13:54:48.901224 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:54:48.901228 24522 net.cpp:2270] res2a_branch2a_param_0(0.198) 
I0801 13:54:48.901232 24522 net.cpp:2270] res2a_branch2b_param_0(0.194) 
I0801 13:54:48.901237 24522 net.cpp:2270] res3a_branch2a_param_0(0.2) 
I0801 13:54:48.901240 24522 net.cpp:2270] res3a_branch2b_param_0(0.198) 
I0801 13:54:48.901242 24522 net.cpp:2270] res4a_branch2a_param_0(0.2) 
I0801 13:54:48.901245 24522 net.cpp:2270] res4a_branch2b_param_0(0.2) 
I0801 13:54:48.901249 24522 net.cpp:2270] res5a_branch2a_param_0(0.198) 
I0801 13:54:48.901253 24522 net.cpp:2270] res5a_branch2b_param_0(0.2) 
I0801 13:54:48.901268 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (468129/2.3599e+06) 0.198
I0801 13:54:48.901278 24522 solver.cpp:550] Iteration 14000, Testing net (#0)
I0801 13:54:49.716732 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.897942
I0801 13:54:49.716753 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.993824
I0801 13:54:49.716758 24522 solver.cpp:635]     Test net output #2: loss = 0.434368 (* 1 = 0.434368 loss)
I0801 13:54:49.716771 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.815466s
I0801 13:54:49.732343 24549 solver.cpp:450] Finding and applying sparsity: 0.22
I0801 13:55:10.056449 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:55:10.058365 24522 solver.cpp:353] Iteration 14000 (4.40339 iter/s, 22.7098s/100 iter), loss = 0.0746899
I0801 13:55:10.058384 24522 solver.cpp:375]     Train net output #0: loss = 0.0746892 (* 1 = 0.0746892 loss)
I0801 13:55:10.058390 24522 sgd_solver.cpp:136] Iteration 14000, lr = 0.0078125, m = 0.9
I0801 13:55:11.886555 24522 solver.cpp:353] Iteration 14100 (54.7006 iter/s, 1.82813s/100 iter), loss = 0.00371545
I0801 13:55:11.886582 24522 solver.cpp:375]     Train net output #0: loss = 0.00371474 (* 1 = 0.00371474 loss)
I0801 13:55:11.886589 24522 sgd_solver.cpp:136] Iteration 14100, lr = 0.00779688, m = 0.9
I0801 13:55:13.489128 24522 solver.cpp:353] Iteration 14200 (62.4017 iter/s, 1.60252s/100 iter), loss = 0.00108607
I0801 13:55:13.489173 24522 solver.cpp:375]     Train net output #0: loss = 0.00108537 (* 1 = 0.00108537 loss)
I0801 13:55:13.489186 24522 sgd_solver.cpp:136] Iteration 14200, lr = 0.00778125, m = 0.9
I0801 13:55:15.073106 24522 solver.cpp:353] Iteration 14300 (63.1343 iter/s, 1.58392s/100 iter), loss = 0.00541784
I0801 13:55:15.073134 24522 solver.cpp:375]     Train net output #0: loss = 0.00541714 (* 1 = 0.00541714 loss)
I0801 13:55:15.073141 24522 sgd_solver.cpp:136] Iteration 14300, lr = 0.00776563, m = 0.9
I0801 13:55:16.650166 24522 solver.cpp:353] Iteration 14400 (63.411 iter/s, 1.57701s/100 iter), loss = 0.00494886
I0801 13:55:16.650195 24522 solver.cpp:375]     Train net output #0: loss = 0.00494817 (* 1 = 0.00494817 loss)
I0801 13:55:16.650200 24522 sgd_solver.cpp:136] Iteration 14400, lr = 0.00775, m = 0.9
I0801 13:55:18.222170 24522 solver.cpp:353] Iteration 14500 (63.615 iter/s, 1.57196s/100 iter), loss = 0.000253498
I0801 13:55:18.222218 24522 solver.cpp:375]     Train net output #0: loss = 0.000252815 (* 1 = 0.000252815 loss)
I0801 13:55:18.222230 24522 sgd_solver.cpp:136] Iteration 14500, lr = 0.00773437, m = 0.9
I0801 13:55:19.824872 24522 solver.cpp:353] Iteration 14600 (62.3967 iter/s, 1.60265s/100 iter), loss = 0.00158708
I0801 13:55:19.824921 24522 solver.cpp:375]     Train net output #0: loss = 0.0015864 (* 1 = 0.0015864 loss)
I0801 13:55:19.824934 24522 sgd_solver.cpp:136] Iteration 14600, lr = 0.00771875, m = 0.9
I0801 13:55:21.410377 24522 solver.cpp:353] Iteration 14700 (63.0734 iter/s, 1.58545s/100 iter), loss = 0.00110149
I0801 13:55:21.410410 24522 solver.cpp:375]     Train net output #0: loss = 0.0011008 (* 1 = 0.0011008 loss)
I0801 13:55:21.410418 24522 sgd_solver.cpp:136] Iteration 14700, lr = 0.00770312, m = 0.9
I0801 13:55:22.980284 24522 solver.cpp:353] Iteration 14800 (63.7 iter/s, 1.56986s/100 iter), loss = 0.00253098
I0801 13:55:22.980309 24522 solver.cpp:375]     Train net output #0: loss = 0.00253029 (* 1 = 0.00253029 loss)
I0801 13:55:22.980316 24522 sgd_solver.cpp:136] Iteration 14800, lr = 0.0076875, m = 0.9
I0801 13:55:24.583889 24522 solver.cpp:353] Iteration 14900 (62.3615 iter/s, 1.60355s/100 iter), loss = 0.00933057
I0801 13:55:24.583915 24522 solver.cpp:375]     Train net output #0: loss = 0.00932988 (* 1 = 0.00932988 loss)
I0801 13:55:24.583920 24522 sgd_solver.cpp:136] Iteration 14900, lr = 0.00767187, m = 0.9
I0801 13:55:26.157485 24522 solver.cpp:404] Sparsity after update:
I0801 13:55:26.159116 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:55:26.159123 24522 net.cpp:2270] conv1a_param_0(0.0954) 
I0801 13:55:26.159129 24522 net.cpp:2270] conv1b_param_0(0.156) 
I0801 13:55:26.159133 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:55:26.159137 24522 net.cpp:2270] res2a_branch2a_param_0(0.219) 
I0801 13:55:26.159139 24522 net.cpp:2270] res2a_branch2b_param_0(0.215) 
I0801 13:55:26.159143 24522 net.cpp:2270] res3a_branch2a_param_0(0.219) 
I0801 13:55:26.159152 24522 net.cpp:2270] res3a_branch2b_param_0(0.219) 
I0801 13:55:26.159157 24522 net.cpp:2270] res4a_branch2a_param_0(0.22) 
I0801 13:55:26.159162 24522 net.cpp:2270] res4a_branch2b_param_0(0.219) 
I0801 13:55:26.159164 24522 net.cpp:2270] res5a_branch2a_param_0(0.215) 
I0801 13:55:26.159170 24522 net.cpp:2270] res5a_branch2b_param_0(0.219) 
I0801 13:55:26.159175 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (510611/2.3599e+06) 0.216
I0801 13:55:26.159198 24522 solver.cpp:550] Iteration 15000, Testing net (#0)
I0801 13:55:26.971050 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.892942
I0801 13:55:26.971070 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995294
I0801 13:55:26.971076 24522 solver.cpp:635]     Test net output #2: loss = 0.444005 (* 1 = 0.444005 loss)
I0801 13:55:26.971092 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.811868s
I0801 13:55:26.986596 24549 solver.cpp:450] Finding and applying sparsity: 0.24
I0801 13:55:46.458355 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:55:46.460271 24522 solver.cpp:353] Iteration 15000 (4.57127 iter/s, 21.8758s/100 iter), loss = 0.00190435
I0801 13:55:46.460289 24522 solver.cpp:375]     Train net output #0: loss = 0.00190366 (* 1 = 0.00190366 loss)
I0801 13:55:46.460295 24522 sgd_solver.cpp:136] Iteration 15000, lr = 0.00765625, m = 0.9
I0801 13:55:48.317044 24522 solver.cpp:353] Iteration 15100 (53.8586 iter/s, 1.85672s/100 iter), loss = 0.000148196
I0801 13:55:48.317068 24522 solver.cpp:375]     Train net output #0: loss = 0.000147504 (* 1 = 0.000147504 loss)
I0801 13:55:48.317073 24522 sgd_solver.cpp:136] Iteration 15100, lr = 0.00764062, m = 0.9
I0801 13:55:49.914598 24522 solver.cpp:353] Iteration 15200 (62.5977 iter/s, 1.5975s/100 iter), loss = 0.0015119
I0801 13:55:49.914645 24522 solver.cpp:375]     Train net output #0: loss = 0.0015112 (* 1 = 0.0015112 loss)
I0801 13:55:49.914657 24522 sgd_solver.cpp:136] Iteration 15200, lr = 0.007625, m = 0.9
I0801 13:55:51.504467 24522 solver.cpp:353] Iteration 15300 (62.9003 iter/s, 1.58982s/100 iter), loss = 0.00547053
I0801 13:55:51.504523 24522 solver.cpp:375]     Train net output #0: loss = 0.00546984 (* 1 = 0.00546984 loss)
I0801 13:55:51.504539 24522 sgd_solver.cpp:136] Iteration 15300, lr = 0.00760937, m = 0.9
I0801 13:55:53.081877 24522 solver.cpp:353] Iteration 15400 (63.3971 iter/s, 1.57736s/100 iter), loss = 0.0100126
I0801 13:55:53.081904 24522 solver.cpp:375]     Train net output #0: loss = 0.0100119 (* 1 = 0.0100119 loss)
I0801 13:55:53.081908 24522 sgd_solver.cpp:136] Iteration 15400, lr = 0.00759375, m = 0.9
I0801 13:55:54.679930 24522 solver.cpp:353] Iteration 15500 (62.5782 iter/s, 1.598s/100 iter), loss = 0.00443748
I0801 13:55:54.679985 24522 solver.cpp:375]     Train net output #0: loss = 0.00443678 (* 1 = 0.00443678 loss)
I0801 13:55:54.679999 24522 sgd_solver.cpp:136] Iteration 15500, lr = 0.00757812, m = 0.9
I0801 13:55:56.289757 24522 solver.cpp:353] Iteration 15600 (62.1204 iter/s, 1.60978s/100 iter), loss = 0.00679582
I0801 13:55:56.289780 24522 solver.cpp:375]     Train net output #0: loss = 0.00679514 (* 1 = 0.00679514 loss)
I0801 13:55:56.289785 24522 sgd_solver.cpp:136] Iteration 15600, lr = 0.0075625, m = 0.9
I0801 13:55:57.877805 24522 solver.cpp:353] Iteration 15700 (62.9724 iter/s, 1.588s/100 iter), loss = 0.000648394
I0801 13:55:57.877830 24522 solver.cpp:375]     Train net output #0: loss = 0.000647708 (* 1 = 0.000647708 loss)
I0801 13:55:57.877833 24522 sgd_solver.cpp:136] Iteration 15700, lr = 0.00754687, m = 0.9
I0801 13:55:59.468271 24522 solver.cpp:353] Iteration 15800 (62.8765 iter/s, 1.59042s/100 iter), loss = 0.000742716
I0801 13:55:59.468298 24522 solver.cpp:375]     Train net output #0: loss = 0.000742026 (* 1 = 0.000742026 loss)
I0801 13:55:59.468304 24522 sgd_solver.cpp:136] Iteration 15800, lr = 0.00753125, m = 0.9
I0801 13:56:01.057001 24522 solver.cpp:353] Iteration 15900 (62.9453 iter/s, 1.58868s/100 iter), loss = 0.00565604
I0801 13:56:01.057025 24522 solver.cpp:375]     Train net output #0: loss = 0.00565536 (* 1 = 0.00565536 loss)
I0801 13:56:01.057029 24522 sgd_solver.cpp:136] Iteration 15900, lr = 0.00751562, m = 0.9
I0801 13:56:02.623759 24522 solver.cpp:404] Sparsity after update:
I0801 13:56:02.625677 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:56:02.625689 24522 net.cpp:2270] conv1a_param_0(0.0962) 
I0801 13:56:02.625697 24522 net.cpp:2270] conv1b_param_0(0.177) 
I0801 13:56:02.625702 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:56:02.625707 24522 net.cpp:2270] res2a_branch2a_param_0(0.24) 
I0801 13:56:02.625712 24522 net.cpp:2270] res2a_branch2b_param_0(0.236) 
I0801 13:56:02.625717 24522 net.cpp:2270] res3a_branch2a_param_0(0.24) 
I0801 13:56:02.625721 24522 net.cpp:2270] res3a_branch2b_param_0(0.24) 
I0801 13:56:02.625726 24522 net.cpp:2270] res4a_branch2a_param_0(0.24) 
I0801 13:56:02.625730 24522 net.cpp:2270] res4a_branch2b_param_0(0.24) 
I0801 13:56:02.625735 24522 net.cpp:2270] res5a_branch2a_param_0(0.236) 
I0801 13:56:02.625738 24522 net.cpp:2270] res5a_branch2b_param_0(0.239) 
I0801 13:56:02.625743 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (559723/2.3599e+06) 0.237
I0801 13:56:02.625771 24522 solver.cpp:550] Iteration 16000, Testing net (#0)
I0801 13:56:03.476935 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.905295
I0801 13:56:03.476953 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 13:56:03.476958 24522 solver.cpp:635]     Test net output #2: loss = 0.382189 (* 1 = 0.382189 loss)
I0801 13:56:03.476974 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.851174s
I0801 13:56:03.497334 24549 solver.cpp:450] Finding and applying sparsity: 0.26
I0801 13:56:21.726263 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:56:21.728179 24522 solver.cpp:353] Iteration 16000 (4.83779 iter/s, 20.6706s/100 iter), loss = 0.00498618
I0801 13:56:21.728199 24522 solver.cpp:375]     Train net output #0: loss = 0.0049855 (* 1 = 0.0049855 loss)
I0801 13:56:21.728204 24522 sgd_solver.cpp:136] Iteration 16000, lr = 0.0075, m = 0.9
I0801 13:56:23.539106 24522 solver.cpp:353] Iteration 16100 (55.2221 iter/s, 1.81087s/100 iter), loss = 0.000448718
I0801 13:56:23.539129 24522 solver.cpp:375]     Train net output #0: loss = 0.000448034 (* 1 = 0.000448034 loss)
I0801 13:56:23.539136 24522 sgd_solver.cpp:136] Iteration 16100, lr = 0.00748438, m = 0.9
I0801 13:56:25.124323 24522 solver.cpp:353] Iteration 16200 (63.0848 iter/s, 1.58517s/100 iter), loss = 0.00204645
I0801 13:56:25.124349 24522 solver.cpp:375]     Train net output #0: loss = 0.00204577 (* 1 = 0.00204577 loss)
I0801 13:56:25.124356 24522 sgd_solver.cpp:136] Iteration 16200, lr = 0.00746875, m = 0.9
I0801 13:56:26.715216 24522 solver.cpp:353] Iteration 16300 (62.8597 iter/s, 1.59084s/100 iter), loss = 0.00216135
I0801 13:56:26.715245 24522 solver.cpp:375]     Train net output #0: loss = 0.00216066 (* 1 = 0.00216066 loss)
I0801 13:56:26.715250 24522 sgd_solver.cpp:136] Iteration 16300, lr = 0.00745312, m = 0.9
I0801 13:56:28.322585 24522 solver.cpp:353] Iteration 16400 (62.2154 iter/s, 1.60732s/100 iter), loss = 0.000297942
I0801 13:56:28.322613 24522 solver.cpp:375]     Train net output #0: loss = 0.000297254 (* 1 = 0.000297254 loss)
I0801 13:56:28.322618 24522 sgd_solver.cpp:136] Iteration 16400, lr = 0.0074375, m = 0.9
I0801 13:56:29.910639 24522 solver.cpp:353] Iteration 16500 (62.9722 iter/s, 1.588s/100 iter), loss = 0.00249876
I0801 13:56:29.910666 24522 solver.cpp:375]     Train net output #0: loss = 0.00249807 (* 1 = 0.00249807 loss)
I0801 13:56:29.910675 24522 sgd_solver.cpp:136] Iteration 16500, lr = 0.00742187, m = 0.9
I0801 13:56:31.492115 24522 solver.cpp:353] Iteration 16600 (63.2341 iter/s, 1.58143s/100 iter), loss = 0.0026068
I0801 13:56:31.492141 24522 solver.cpp:375]     Train net output #0: loss = 0.00260611 (* 1 = 0.00260611 loss)
I0801 13:56:31.492147 24522 sgd_solver.cpp:136] Iteration 16600, lr = 0.00740625, m = 0.9
I0801 13:56:33.075289 24522 solver.cpp:353] Iteration 16700 (63.1661 iter/s, 1.58313s/100 iter), loss = 0.0143397
I0801 13:56:33.075337 24522 solver.cpp:375]     Train net output #0: loss = 0.0143391 (* 1 = 0.0143391 loss)
I0801 13:56:33.075350 24522 sgd_solver.cpp:136] Iteration 16700, lr = 0.00739062, m = 0.9
I0801 13:56:34.683002 24522 solver.cpp:353] Iteration 16800 (62.2021 iter/s, 1.60766s/100 iter), loss = 0.00202817
I0801 13:56:34.683027 24522 solver.cpp:375]     Train net output #0: loss = 0.00202749 (* 1 = 0.00202749 loss)
I0801 13:56:34.683032 24522 sgd_solver.cpp:136] Iteration 16800, lr = 0.007375, m = 0.9
I0801 13:56:36.256855 24522 solver.cpp:353] Iteration 16900 (63.5404 iter/s, 1.5738s/100 iter), loss = 0.00129148
I0801 13:56:36.256882 24522 solver.cpp:375]     Train net output #0: loss = 0.0012908 (* 1 = 0.0012908 loss)
I0801 13:56:36.256888 24522 sgd_solver.cpp:136] Iteration 16900, lr = 0.00735937, m = 0.9
I0801 13:56:37.830687 24522 solver.cpp:404] Sparsity after update:
I0801 13:56:37.832265 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:56:37.832273 24522 net.cpp:2270] conv1a_param_0(0.104) 
I0801 13:56:37.832283 24522 net.cpp:2270] conv1b_param_0(0.187) 
I0801 13:56:37.832288 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:56:37.832293 24522 net.cpp:2270] res2a_branch2a_param_0(0.257) 
I0801 13:56:37.832300 24522 net.cpp:2270] res2a_branch2b_param_0(0.257) 
I0801 13:56:37.832304 24522 net.cpp:2270] res3a_branch2a_param_0(0.259) 
I0801 13:56:37.832307 24522 net.cpp:2270] res3a_branch2b_param_0(0.257) 
I0801 13:56:37.832309 24522 net.cpp:2270] res4a_branch2a_param_0(0.26) 
I0801 13:56:37.832314 24522 net.cpp:2270] res4a_branch2b_param_0(0.259) 
I0801 13:56:37.832316 24522 net.cpp:2270] res5a_branch2a_param_0(0.253) 
I0801 13:56:37.832320 24522 net.cpp:2270] res5a_branch2b_param_0(0.258) 
I0801 13:56:37.832325 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (602331/2.3599e+06) 0.255
I0801 13:56:37.832350 24522 solver.cpp:550] Iteration 17000, Testing net (#0)
I0801 13:56:38.640887 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.916766
I0801 13:56:38.640907 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 13:56:38.640913 24522 solver.cpp:635]     Test net output #2: loss = 0.331567 (* 1 = 0.331567 loss)
I0801 13:56:38.640929 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.808553s
I0801 13:56:38.656332 24549 solver.cpp:450] Finding and applying sparsity: 0.28
I0801 13:56:55.725636 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:56:55.727560 24522 solver.cpp:353] Iteration 17000 (5.13606 iter/s, 19.4702s/100 iter), loss = 0.000756713
I0801 13:56:55.727576 24522 solver.cpp:375]     Train net output #0: loss = 0.000756031 (* 1 = 0.000756031 loss)
I0801 13:56:55.727581 24522 sgd_solver.cpp:136] Iteration 17000, lr = 0.00734375, m = 0.9
I0801 13:56:57.569291 24522 solver.cpp:353] Iteration 17100 (54.2984 iter/s, 1.84167s/100 iter), loss = 0.00433522
I0801 13:56:57.569315 24522 solver.cpp:375]     Train net output #0: loss = 0.00433454 (* 1 = 0.00433454 loss)
I0801 13:56:57.569321 24522 sgd_solver.cpp:136] Iteration 17100, lr = 0.00732813, m = 0.9
I0801 13:56:59.154312 24522 solver.cpp:353] Iteration 17200 (63.0927 iter/s, 1.58497s/100 iter), loss = 0.000114296
I0801 13:56:59.154338 24522 solver.cpp:375]     Train net output #0: loss = 0.000113617 (* 1 = 0.000113617 loss)
I0801 13:56:59.154345 24522 sgd_solver.cpp:136] Iteration 17200, lr = 0.0073125, m = 0.9
I0801 13:57:00.737691 24522 solver.cpp:353] Iteration 17300 (63.158 iter/s, 1.58333s/100 iter), loss = 0.000381985
I0801 13:57:00.737717 24522 solver.cpp:375]     Train net output #0: loss = 0.000381311 (* 1 = 0.000381311 loss)
I0801 13:57:00.737722 24522 sgd_solver.cpp:136] Iteration 17300, lr = 0.00729688, m = 0.9
I0801 13:57:02.313124 24522 solver.cpp:353] Iteration 17400 (63.4766 iter/s, 1.57538s/100 iter), loss = 0.00426756
I0801 13:57:02.313148 24522 solver.cpp:375]     Train net output #0: loss = 0.00426689 (* 1 = 0.00426689 loss)
I0801 13:57:02.313151 24522 sgd_solver.cpp:136] Iteration 17400, lr = 0.00728125, m = 0.9
I0801 13:57:03.922804 24522 solver.cpp:353] Iteration 17500 (62.126 iter/s, 1.60963s/100 iter), loss = 0.00130369
I0801 13:57:03.922832 24522 solver.cpp:375]     Train net output #0: loss = 0.00130301 (* 1 = 0.00130301 loss)
I0801 13:57:03.922837 24522 sgd_solver.cpp:136] Iteration 17500, lr = 0.00726563, m = 0.9
I0801 13:57:05.513883 24522 solver.cpp:353] Iteration 17600 (62.8525 iter/s, 1.59103s/100 iter), loss = 0.00159859
I0801 13:57:05.513907 24522 solver.cpp:375]     Train net output #0: loss = 0.00159792 (* 1 = 0.00159792 loss)
I0801 13:57:05.513911 24522 sgd_solver.cpp:136] Iteration 17600, lr = 0.00725, m = 0.9
I0801 13:57:07.091053 24522 solver.cpp:353] Iteration 17700 (63.4067 iter/s, 1.57712s/100 iter), loss = 0.000897399
I0801 13:57:07.091079 24522 solver.cpp:375]     Train net output #0: loss = 0.000896723 (* 1 = 0.000896723 loss)
I0801 13:57:07.091085 24522 sgd_solver.cpp:136] Iteration 17700, lr = 0.00723437, m = 0.9
I0801 13:57:08.676369 24522 solver.cpp:353] Iteration 17800 (63.0809 iter/s, 1.58527s/100 iter), loss = 0.000582915
I0801 13:57:08.676391 24522 solver.cpp:375]     Train net output #0: loss = 0.000582237 (* 1 = 0.000582237 loss)
I0801 13:57:08.676398 24522 sgd_solver.cpp:136] Iteration 17800, lr = 0.00721875, m = 0.9
I0801 13:57:10.255614 24522 solver.cpp:353] Iteration 17900 (63.3234 iter/s, 1.5792s/100 iter), loss = 0.00998031
I0801 13:57:10.255638 24522 solver.cpp:375]     Train net output #0: loss = 0.00997963 (* 1 = 0.00997963 loss)
I0801 13:57:10.255645 24522 sgd_solver.cpp:136] Iteration 17900, lr = 0.00720312, m = 0.9
I0801 13:57:11.841758 24522 solver.cpp:404] Sparsity after update:
I0801 13:57:11.843515 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:57:11.843523 24522 net.cpp:2270] conv1a_param_0(0.115) 
I0801 13:57:11.843528 24522 net.cpp:2270] conv1b_param_0(0.208) 
I0801 13:57:11.843531 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:57:11.843533 24522 net.cpp:2270] res2a_branch2a_param_0(0.278) 
I0801 13:57:11.843536 24522 net.cpp:2270] res2a_branch2b_param_0(0.278) 
I0801 13:57:11.843538 24522 net.cpp:2270] res3a_branch2a_param_0(0.28) 
I0801 13:57:11.843540 24522 net.cpp:2270] res3a_branch2b_param_0(0.278) 
I0801 13:57:11.843542 24522 net.cpp:2270] res4a_branch2a_param_0(0.28) 
I0801 13:57:11.843544 24522 net.cpp:2270] res4a_branch2b_param_0(0.28) 
I0801 13:57:11.843546 24522 net.cpp:2270] res5a_branch2a_param_0(0.273) 
I0801 13:57:11.843549 24522 net.cpp:2270] res5a_branch2b_param_0(0.277) 
I0801 13:57:11.843551 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (648018/2.3599e+06) 0.275
I0801 13:57:11.843580 24522 solver.cpp:550] Iteration 18000, Testing net (#0)
I0801 13:57:12.316167 24520 data_reader.cpp:264] Starting prefetch of epoch 2
I0801 13:57:12.651342 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.918824
I0801 13:57:12.651361 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 13:57:12.651367 24522 solver.cpp:635]     Test net output #2: loss = 0.344375 (* 1 = 0.344375 loss)
I0801 13:57:12.651386 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.807777s
I0801 13:57:12.666775 24549 solver.cpp:450] Finding and applying sparsity: 0.3
I0801 13:57:30.058078 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:57:30.059989 24522 solver.cpp:353] Iteration 18000 (5.04953 iter/s, 19.8038s/100 iter), loss = 0.00144948
I0801 13:57:30.060006 24522 solver.cpp:375]     Train net output #0: loss = 0.0014488 (* 1 = 0.0014488 loss)
I0801 13:57:30.060011 24522 sgd_solver.cpp:136] Iteration 18000, lr = 0.0071875, m = 0.9
I0801 13:57:31.906461 24522 solver.cpp:353] Iteration 18100 (54.1591 iter/s, 1.84641s/100 iter), loss = 0.00037554
I0801 13:57:31.906487 24522 solver.cpp:375]     Train net output #0: loss = 0.000374862 (* 1 = 0.000374862 loss)
I0801 13:57:31.906493 24522 sgd_solver.cpp:136] Iteration 18100, lr = 0.00717187, m = 0.9
I0801 13:57:33.497629 24522 solver.cpp:353] Iteration 18200 (62.849 iter/s, 1.59111s/100 iter), loss = 0.000225688
I0801 13:57:33.497658 24522 solver.cpp:375]     Train net output #0: loss = 0.000225008 (* 1 = 0.000225008 loss)
I0801 13:57:33.497664 24522 sgd_solver.cpp:136] Iteration 18200, lr = 0.00715625, m = 0.9
I0801 13:57:35.080292 24522 solver.cpp:353] Iteration 18300 (63.1866 iter/s, 1.58262s/100 iter), loss = 0.00360334
I0801 13:57:35.080320 24522 solver.cpp:375]     Train net output #0: loss = 0.00360265 (* 1 = 0.00360265 loss)
I0801 13:57:35.080327 24522 sgd_solver.cpp:136] Iteration 18300, lr = 0.00714062, m = 0.9
I0801 13:57:36.662710 24522 solver.cpp:353] Iteration 18400 (63.1964 iter/s, 1.58237s/100 iter), loss = 0.00109441
I0801 13:57:36.662734 24522 solver.cpp:375]     Train net output #0: loss = 0.00109373 (* 1 = 0.00109373 loss)
I0801 13:57:36.662739 24522 sgd_solver.cpp:136] Iteration 18400, lr = 0.007125, m = 0.9
I0801 13:57:38.255584 24522 solver.cpp:353] Iteration 18500 (62.7816 iter/s, 1.59282s/100 iter), loss = 0.000474275
I0801 13:57:38.255633 24522 solver.cpp:375]     Train net output #0: loss = 0.000473593 (* 1 = 0.000473593 loss)
I0801 13:57:38.255645 24522 sgd_solver.cpp:136] Iteration 18500, lr = 0.00710937, m = 0.9
I0801 13:57:39.841053 24522 solver.cpp:353] Iteration 18600 (63.0748 iter/s, 1.58542s/100 iter), loss = 0.0027402
I0801 13:57:39.841078 24522 solver.cpp:375]     Train net output #0: loss = 0.00273952 (* 1 = 0.00273952 loss)
I0801 13:57:39.841084 24522 sgd_solver.cpp:136] Iteration 18600, lr = 0.00709375, m = 0.9
I0801 13:57:41.435984 24522 solver.cpp:353] Iteration 18700 (62.7006 iter/s, 1.59488s/100 iter), loss = 0.000393333
I0801 13:57:41.436009 24522 solver.cpp:375]     Train net output #0: loss = 0.000392651 (* 1 = 0.000392651 loss)
I0801 13:57:41.436015 24522 sgd_solver.cpp:136] Iteration 18700, lr = 0.00707812, m = 0.9
I0801 13:57:43.007207 24522 solver.cpp:353] Iteration 18800 (63.6467 iter/s, 1.57117s/100 iter), loss = 0.000203356
I0801 13:57:43.007233 24522 solver.cpp:375]     Train net output #0: loss = 0.000202672 (* 1 = 0.000202672 loss)
I0801 13:57:43.007238 24522 sgd_solver.cpp:136] Iteration 18800, lr = 0.0070625, m = 0.9
I0801 13:57:44.594230 24522 solver.cpp:353] Iteration 18900 (63.0131 iter/s, 1.58697s/100 iter), loss = 0.00201048
I0801 13:57:44.594255 24522 solver.cpp:375]     Train net output #0: loss = 0.0020098 (* 1 = 0.0020098 loss)
I0801 13:57:44.594261 24522 sgd_solver.cpp:136] Iteration 18900, lr = 0.00704687, m = 0.9
I0801 13:57:46.166934 24522 solver.cpp:404] Sparsity after update:
I0801 13:57:46.168529 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:57:46.168537 24522 net.cpp:2270] conv1a_param_0(0.129) 
I0801 13:57:46.168543 24522 net.cpp:2270] conv1b_param_0(0.219) 
I0801 13:57:46.168546 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:57:46.168548 24522 net.cpp:2270] res2a_branch2a_param_0(0.299) 
I0801 13:57:46.168550 24522 net.cpp:2270] res2a_branch2b_param_0(0.299) 
I0801 13:57:46.168552 24522 net.cpp:2270] res3a_branch2a_param_0(0.299) 
I0801 13:57:46.168555 24522 net.cpp:2270] res3a_branch2b_param_0(0.299) 
I0801 13:57:46.168556 24522 net.cpp:2270] res4a_branch2a_param_0(0.299) 
I0801 13:57:46.168558 24522 net.cpp:2270] res4a_branch2b_param_0(0.299) 
I0801 13:57:46.168560 24522 net.cpp:2270] res5a_branch2a_param_0(0.293) 
I0801 13:57:46.168562 24522 net.cpp:2270] res5a_branch2b_param_0(0.298) 
I0801 13:57:46.168565 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (695432/2.3599e+06) 0.295
I0801 13:57:46.168584 24522 solver.cpp:550] Iteration 19000, Testing net (#0)
I0801 13:57:46.978953 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.915001
I0801 13:57:46.978971 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 13:57:46.978976 24522 solver.cpp:635]     Test net output #2: loss = 0.352645 (* 1 = 0.352645 loss)
I0801 13:57:46.978991 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.81038s
I0801 13:57:46.994683 24549 solver.cpp:450] Finding and applying sparsity: 0.32
I0801 13:58:02.749421 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:58:02.751353 24522 solver.cpp:353] Iteration 19000 (5.50763 iter/s, 18.1566s/100 iter), loss = 0.000496591
I0801 13:58:02.751372 24522 solver.cpp:375]     Train net output #0: loss = 0.000495906 (* 1 = 0.000495906 loss)
I0801 13:58:02.751379 24522 sgd_solver.cpp:136] Iteration 19000, lr = 0.00703125, m = 0.9
I0801 13:58:04.537607 24522 solver.cpp:353] Iteration 19100 (55.9848 iter/s, 1.7862s/100 iter), loss = 0.000308639
I0801 13:58:04.537859 24522 solver.cpp:375]     Train net output #0: loss = 0.000307952 (* 1 = 0.000307952 loss)
I0801 13:58:04.537866 24522 sgd_solver.cpp:136] Iteration 19100, lr = 0.00701563, m = 0.9
I0801 13:58:06.115314 24522 solver.cpp:353] Iteration 19200 (63.3851 iter/s, 1.57766s/100 iter), loss = 0.00019857
I0801 13:58:06.115365 24522 solver.cpp:375]     Train net output #0: loss = 0.000197882 (* 1 = 0.000197882 loss)
I0801 13:58:06.115378 24522 sgd_solver.cpp:136] Iteration 19200, lr = 0.007, m = 0.9
I0801 13:58:07.705724 24522 solver.cpp:353] Iteration 19300 (62.8789 iter/s, 1.59036s/100 iter), loss = 0.0019933
I0801 13:58:07.705770 24522 solver.cpp:375]     Train net output #0: loss = 0.00199261 (* 1 = 0.00199261 loss)
I0801 13:58:07.705785 24522 sgd_solver.cpp:136] Iteration 19300, lr = 0.00698437, m = 0.9
I0801 13:58:09.286679 24522 solver.cpp:353] Iteration 19400 (63.2549 iter/s, 1.58091s/100 iter), loss = 0.00596585
I0801 13:58:09.286701 24522 solver.cpp:375]     Train net output #0: loss = 0.00596516 (* 1 = 0.00596516 loss)
I0801 13:58:09.286707 24522 sgd_solver.cpp:136] Iteration 19400, lr = 0.00696875, m = 0.9
I0801 13:58:10.888733 24522 solver.cpp:353] Iteration 19500 (62.4218 iter/s, 1.60201s/100 iter), loss = 0.000232034
I0801 13:58:10.888758 24522 solver.cpp:375]     Train net output #0: loss = 0.000231345 (* 1 = 0.000231345 loss)
I0801 13:58:10.888764 24522 sgd_solver.cpp:136] Iteration 19500, lr = 0.00695312, m = 0.9
I0801 13:58:12.475275 24522 solver.cpp:353] Iteration 19600 (63.0322 iter/s, 1.58649s/100 iter), loss = 0.000536199
I0801 13:58:12.475298 24522 solver.cpp:375]     Train net output #0: loss = 0.000535511 (* 1 = 0.000535511 loss)
I0801 13:58:12.475304 24522 sgd_solver.cpp:136] Iteration 19600, lr = 0.0069375, m = 0.9
I0801 13:58:14.051605 24522 solver.cpp:353] Iteration 19700 (63.4404 iter/s, 1.57628s/100 iter), loss = 0.000480874
I0801 13:58:14.051630 24522 solver.cpp:375]     Train net output #0: loss = 0.000480187 (* 1 = 0.000480187 loss)
I0801 13:58:14.051635 24522 sgd_solver.cpp:136] Iteration 19700, lr = 0.00692187, m = 0.9
I0801 13:58:15.632913 24522 solver.cpp:353] Iteration 19800 (63.2408 iter/s, 1.58126s/100 iter), loss = 0.000496421
I0801 13:58:15.632977 24522 solver.cpp:375]     Train net output #0: loss = 0.000495733 (* 1 = 0.000495733 loss)
I0801 13:58:15.632994 24522 sgd_solver.cpp:136] Iteration 19800, lr = 0.00690625, m = 0.9
I0801 13:58:17.224186 24522 solver.cpp:353] Iteration 19900 (62.8447 iter/s, 1.59122s/100 iter), loss = 0.00133293
I0801 13:58:17.224210 24522 solver.cpp:375]     Train net output #0: loss = 0.00133224 (* 1 = 0.00133224 loss)
I0801 13:58:17.224216 24522 sgd_solver.cpp:136] Iteration 19900, lr = 0.00689062, m = 0.9
I0801 13:58:18.787400 24522 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_20000.caffemodel
I0801 13:58:18.797099 24522 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_20000.solverstate
I0801 13:58:18.801990 24522 solver.cpp:404] Sparsity after update:
I0801 13:58:18.804904 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:58:18.804916 24522 net.cpp:2270] conv1a_param_0(0.143) 
I0801 13:58:18.804927 24522 net.cpp:2270] conv1b_param_0(0.239) 
I0801 13:58:18.804941 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:58:18.804955 24522 net.cpp:2270] res2a_branch2a_param_0(0.319) 
I0801 13:58:18.804965 24522 net.cpp:2270] res2a_branch2b_param_0(0.319) 
I0801 13:58:18.804973 24522 net.cpp:2270] res3a_branch2a_param_0(0.319) 
I0801 13:58:18.804993 24522 net.cpp:2270] res3a_branch2b_param_0(0.319) 
I0801 13:58:18.805003 24522 net.cpp:2270] res4a_branch2a_param_0(0.319) 
I0801 13:58:18.805013 24522 net.cpp:2270] res4a_branch2b_param_0(0.319) 
I0801 13:58:18.805022 24522 net.cpp:2270] res5a_branch2a_param_0(0.307) 
I0801 13:58:18.805030 24522 net.cpp:2270] res5a_branch2b_param_0(0.313) 
I0801 13:58:18.805042 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (732930/2.3599e+06) 0.311
I0801 13:58:18.805058 24522 solver.cpp:550] Iteration 20000, Testing net (#0)
I0801 13:58:19.597762 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.914707
I0801 13:58:19.597780 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 13:58:19.597785 24522 solver.cpp:635]     Test net output #2: loss = 0.34708 (* 1 = 0.34708 loss)
I0801 13:58:19.597803 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.792718s
I0801 13:58:19.613271 24549 solver.cpp:450] Finding and applying sparsity: 0.34
I0801 13:58:35.654861 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:58:35.660473 24522 solver.cpp:353] Iteration 20000 (5.42424 iter/s, 18.4358s/100 iter), loss = 0.000365448
I0801 13:58:35.660490 24522 solver.cpp:375]     Train net output #0: loss = 0.000364761 (* 1 = 0.000364761 loss)
I0801 13:58:35.660496 24522 sgd_solver.cpp:136] Iteration 20000, lr = 0.006875, m = 0.9
I0801 13:58:37.502418 24522 solver.cpp:353] Iteration 20100 (54.2922 iter/s, 1.84189s/100 iter), loss = 0.000430974
I0801 13:58:37.502468 24522 solver.cpp:375]     Train net output #0: loss = 0.000430287 (* 1 = 0.000430287 loss)
I0801 13:58:37.502482 24522 sgd_solver.cpp:136] Iteration 20100, lr = 0.00685938, m = 0.9
I0801 13:58:39.111367 24522 solver.cpp:353] Iteration 20200 (62.1544 iter/s, 1.6089s/100 iter), loss = 0.00107454
I0801 13:58:39.111393 24522 solver.cpp:375]     Train net output #0: loss = 0.00107385 (* 1 = 0.00107385 loss)
I0801 13:58:39.111399 24522 sgd_solver.cpp:136] Iteration 20200, lr = 0.00684375, m = 0.9
I0801 13:58:40.749510 24522 solver.cpp:353] Iteration 20300 (61.0466 iter/s, 1.63809s/100 iter), loss = 0.000396971
I0801 13:58:40.749557 24522 solver.cpp:375]     Train net output #0: loss = 0.000396286 (* 1 = 0.000396286 loss)
I0801 13:58:40.749567 24522 sgd_solver.cpp:136] Iteration 20300, lr = 0.00682813, m = 0.9
I0801 13:58:42.376346 24522 solver.cpp:353] Iteration 20400 (61.4709 iter/s, 1.62679s/100 iter), loss = 0.0013729
I0801 13:58:42.376371 24522 solver.cpp:375]     Train net output #0: loss = 0.00137222 (* 1 = 0.00137222 loss)
I0801 13:58:42.376377 24522 sgd_solver.cpp:136] Iteration 20400, lr = 0.0068125, m = 0.9
I0801 13:58:43.978286 24522 solver.cpp:353] Iteration 20500 (62.4263 iter/s, 1.60189s/100 iter), loss = 0.00158632
I0801 13:58:43.978312 24522 solver.cpp:375]     Train net output #0: loss = 0.00158563 (* 1 = 0.00158563 loss)
I0801 13:58:43.978317 24522 sgd_solver.cpp:136] Iteration 20500, lr = 0.00679688, m = 0.9
I0801 13:58:45.573654 24522 solver.cpp:353] Iteration 20600 (62.6835 iter/s, 1.59531s/100 iter), loss = 0.000759837
I0801 13:58:45.573683 24522 solver.cpp:375]     Train net output #0: loss = 0.000759153 (* 1 = 0.000759153 loss)
I0801 13:58:45.573689 24522 sgd_solver.cpp:136] Iteration 20600, lr = 0.00678125, m = 0.9
I0801 13:58:47.181362 24522 solver.cpp:353] Iteration 20700 (62.2024 iter/s, 1.60766s/100 iter), loss = 0.00299108
I0801 13:58:47.181430 24522 solver.cpp:375]     Train net output #0: loss = 0.0029904 (* 1 = 0.0029904 loss)
I0801 13:58:47.181449 24522 sgd_solver.cpp:136] Iteration 20700, lr = 0.00676562, m = 0.9
I0801 13:58:48.755483 24522 solver.cpp:353] Iteration 20800 (63.5294 iter/s, 1.57407s/100 iter), loss = 0.0014712
I0801 13:58:48.755508 24522 solver.cpp:375]     Train net output #0: loss = 0.00147051 (* 1 = 0.00147051 loss)
I0801 13:58:48.755514 24522 sgd_solver.cpp:136] Iteration 20800, lr = 0.00675, m = 0.9
I0801 13:58:50.329507 24522 solver.cpp:353] Iteration 20900 (63.5334 iter/s, 1.57397s/100 iter), loss = 0.00284135
I0801 13:58:50.329533 24522 solver.cpp:375]     Train net output #0: loss = 0.00284066 (* 1 = 0.00284066 loss)
I0801 13:58:50.329540 24522 sgd_solver.cpp:136] Iteration 20900, lr = 0.00673437, m = 0.9
I0801 13:58:51.899386 24522 solver.cpp:404] Sparsity after update:
I0801 13:58:51.901020 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:58:51.901029 24522 net.cpp:2270] conv1a_param_0(0.138) 
I0801 13:58:51.901036 24522 net.cpp:2270] conv1b_param_0(0.25) 
I0801 13:58:51.901039 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:58:51.901043 24522 net.cpp:2270] res2a_branch2a_param_0(0.337) 
I0801 13:58:51.901051 24522 net.cpp:2270] res2a_branch2b_param_0(0.333) 
I0801 13:58:51.901057 24522 net.cpp:2270] res3a_branch2a_param_0(0.339) 
I0801 13:58:51.901062 24522 net.cpp:2270] res3a_branch2b_param_0(0.337) 
I0801 13:58:51.901065 24522 net.cpp:2270] res4a_branch2a_param_0(0.339) 
I0801 13:58:51.901069 24522 net.cpp:2270] res4a_branch2b_param_0(0.339) 
I0801 13:58:51.901073 24522 net.cpp:2270] res5a_branch2a_param_0(0.324) 
I0801 13:58:51.901077 24522 net.cpp:2270] res5a_branch2b_param_0(0.333) 
I0801 13:58:51.901082 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (776960/2.3599e+06) 0.329
I0801 13:58:51.901104 24522 solver.cpp:550] Iteration 21000, Testing net (#0)
I0801 13:58:52.711611 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.908825
I0801 13:58:52.711630 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997353
I0801 13:58:52.711637 24522 solver.cpp:635]     Test net output #2: loss = 0.367676 (* 1 = 0.367676 loss)
I0801 13:58:52.711655 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.810524s
I0801 13:58:52.727190 24549 solver.cpp:450] Finding and applying sparsity: 0.36
I0801 13:59:09.764240 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:59:09.766211 24522 solver.cpp:353] Iteration 21000 (5.14505 iter/s, 19.4362s/100 iter), loss = 0.000756806
I0801 13:59:09.766228 24522 solver.cpp:375]     Train net output #0: loss = 0.000756126 (* 1 = 0.000756126 loss)
I0801 13:59:09.766234 24522 sgd_solver.cpp:136] Iteration 21000, lr = 0.00671875, m = 0.9
I0801 13:59:11.567276 24522 solver.cpp:353] Iteration 21100 (55.5245 iter/s, 1.80101s/100 iter), loss = 0.000562546
I0801 13:59:11.567301 24522 solver.cpp:375]     Train net output #0: loss = 0.000561866 (* 1 = 0.000561866 loss)
I0801 13:59:11.567306 24522 sgd_solver.cpp:136] Iteration 21100, lr = 0.00670313, m = 0.9
I0801 13:59:13.159042 24522 solver.cpp:353] Iteration 21200 (62.8252 iter/s, 1.59172s/100 iter), loss = 0.000511892
I0801 13:59:13.159068 24522 solver.cpp:375]     Train net output #0: loss = 0.000511212 (* 1 = 0.000511212 loss)
I0801 13:59:13.159072 24522 sgd_solver.cpp:136] Iteration 21200, lr = 0.0066875, m = 0.9
I0801 13:59:14.751349 24522 solver.cpp:353] Iteration 21300 (62.8039 iter/s, 1.59226s/100 iter), loss = 0.000537717
I0801 13:59:14.751374 24522 solver.cpp:375]     Train net output #0: loss = 0.000537038 (* 1 = 0.000537038 loss)
I0801 13:59:14.751379 24522 sgd_solver.cpp:136] Iteration 21300, lr = 0.00667187, m = 0.9
I0801 13:59:16.345367 24522 solver.cpp:353] Iteration 21400 (62.7365 iter/s, 1.59397s/100 iter), loss = 0.000267403
I0801 13:59:16.345418 24522 solver.cpp:375]     Train net output #0: loss = 0.000266724 (* 1 = 0.000266724 loss)
I0801 13:59:16.345427 24522 sgd_solver.cpp:136] Iteration 21400, lr = 0.00665625, m = 0.9
I0801 13:59:17.931365 24522 solver.cpp:353] Iteration 21500 (63.0539 iter/s, 1.58595s/100 iter), loss = 0.00149765
I0801 13:59:17.931411 24522 solver.cpp:375]     Train net output #0: loss = 0.00149697 (* 1 = 0.00149697 loss)
I0801 13:59:17.931422 24522 sgd_solver.cpp:136] Iteration 21500, lr = 0.00664062, m = 0.9
I0801 13:59:19.516399 24522 solver.cpp:353] Iteration 21600 (63.092 iter/s, 1.58499s/100 iter), loss = 0.000861838
I0801 13:59:19.516427 24522 solver.cpp:375]     Train net output #0: loss = 0.000861159 (* 1 = 0.000861159 loss)
I0801 13:59:19.516434 24522 sgd_solver.cpp:136] Iteration 21600, lr = 0.006625, m = 0.9
I0801 13:59:21.099822 24522 solver.cpp:353] Iteration 21700 (63.1564 iter/s, 1.58337s/100 iter), loss = 0.00154741
I0801 13:59:21.099846 24522 solver.cpp:375]     Train net output #0: loss = 0.00154673 (* 1 = 0.00154673 loss)
I0801 13:59:21.099853 24522 sgd_solver.cpp:136] Iteration 21700, lr = 0.00660937, m = 0.9
I0801 13:59:22.695293 24522 solver.cpp:353] Iteration 21800 (62.6793 iter/s, 1.59542s/100 iter), loss = 0.000359854
I0801 13:59:22.695343 24522 solver.cpp:375]     Train net output #0: loss = 0.000359174 (* 1 = 0.000359174 loss)
I0801 13:59:22.695361 24522 sgd_solver.cpp:136] Iteration 21800, lr = 0.00659375, m = 0.9
I0801 13:59:24.301708 24522 solver.cpp:353] Iteration 21900 (62.2524 iter/s, 1.60636s/100 iter), loss = 0.00283178
I0801 13:59:24.301736 24522 solver.cpp:375]     Train net output #0: loss = 0.00283109 (* 1 = 0.00283109 loss)
I0801 13:59:24.301743 24522 sgd_solver.cpp:136] Iteration 21900, lr = 0.00657812, m = 0.9
I0801 13:59:25.868069 24522 solver.cpp:404] Sparsity after update:
I0801 13:59:25.869652 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:59:25.869662 24522 net.cpp:2270] conv1a_param_0(0.149) 
I0801 13:59:25.869669 24522 net.cpp:2270] conv1b_param_0(0.26) 
I0801 13:59:25.869673 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:59:25.869678 24522 net.cpp:2270] res2a_branch2a_param_0(0.358) 
I0801 13:59:25.869681 24522 net.cpp:2270] res2a_branch2b_param_0(0.354) 
I0801 13:59:25.869684 24522 net.cpp:2270] res3a_branch2a_param_0(0.359) 
I0801 13:59:25.869688 24522 net.cpp:2270] res3a_branch2b_param_0(0.358) 
I0801 13:59:25.869690 24522 net.cpp:2270] res4a_branch2a_param_0(0.359) 
I0801 13:59:25.869693 24522 net.cpp:2270] res4a_branch2b_param_0(0.359) 
I0801 13:59:25.869696 24522 net.cpp:2270] res5a_branch2a_param_0(0.347) 
I0801 13:59:25.869699 24522 net.cpp:2270] res5a_branch2b_param_0(0.357) 
I0801 13:59:25.869716 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (828998/2.3599e+06) 0.351
I0801 13:59:25.869729 24522 solver.cpp:550] Iteration 22000, Testing net (#0)
I0801 13:59:26.283421 24520 data_reader.cpp:264] Starting prefetch of epoch 3
I0801 13:59:26.689656 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.907942
I0801 13:59:26.689674 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997941
I0801 13:59:26.689682 24522 solver.cpp:635]     Test net output #2: loss = 0.36923 (* 1 = 0.36923 loss)
I0801 13:59:26.689700 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.819944s
I0801 13:59:26.705417 24549 solver.cpp:450] Finding and applying sparsity: 0.38
I0801 13:59:42.596545 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 13:59:42.598449 24522 solver.cpp:353] Iteration 22000 (5.46561 iter/s, 18.2962s/100 iter), loss = 0.00113687
I0801 13:59:42.598469 24522 solver.cpp:375]     Train net output #0: loss = 0.00113619 (* 1 = 0.00113619 loss)
I0801 13:59:42.598477 24522 sgd_solver.cpp:136] Iteration 22000, lr = 0.0065625, m = 0.9
I0801 13:59:44.440263 24522 solver.cpp:353] Iteration 22100 (54.296 iter/s, 1.84176s/100 iter), loss = 0.0013199
I0801 13:59:44.440286 24522 solver.cpp:375]     Train net output #0: loss = 0.00131922 (* 1 = 0.00131922 loss)
I0801 13:59:44.440290 24522 sgd_solver.cpp:136] Iteration 22100, lr = 0.00654687, m = 0.9
I0801 13:59:46.028795 24522 solver.cpp:353] Iteration 22200 (62.9533 iter/s, 1.58848s/100 iter), loss = 0.000506676
I0801 13:59:46.028825 24522 solver.cpp:375]     Train net output #0: loss = 0.000505997 (* 1 = 0.000505997 loss)
I0801 13:59:46.028828 24522 sgd_solver.cpp:136] Iteration 22200, lr = 0.00653125, m = 0.9
I0801 13:59:47.622074 24522 solver.cpp:353] Iteration 22300 (62.7655 iter/s, 1.59323s/100 iter), loss = 0.00015816
I0801 13:59:47.622103 24522 solver.cpp:375]     Train net output #0: loss = 0.00015748 (* 1 = 0.00015748 loss)
I0801 13:59:47.622110 24522 sgd_solver.cpp:136] Iteration 22300, lr = 0.00651562, m = 0.9
I0801 13:59:49.206606 24522 solver.cpp:353] Iteration 22400 (63.1121 iter/s, 1.58448s/100 iter), loss = 0.000416087
I0801 13:59:49.206653 24522 solver.cpp:375]     Train net output #0: loss = 0.000415403 (* 1 = 0.000415403 loss)
I0801 13:59:49.206662 24522 sgd_solver.cpp:136] Iteration 22400, lr = 0.0065, m = 0.9
I0801 13:59:50.813978 24522 solver.cpp:353] Iteration 22500 (62.2153 iter/s, 1.60732s/100 iter), loss = 0.000283355
I0801 13:59:50.814025 24522 solver.cpp:375]     Train net output #0: loss = 0.00028267 (* 1 = 0.00028267 loss)
I0801 13:59:50.814040 24522 sgd_solver.cpp:136] Iteration 22500, lr = 0.00648437, m = 0.9
I0801 13:59:52.408310 24522 solver.cpp:353] Iteration 22600 (62.7241 iter/s, 1.59428s/100 iter), loss = 0.00102661
I0801 13:59:52.408360 24522 solver.cpp:375]     Train net output #0: loss = 0.00102593 (* 1 = 0.00102593 loss)
I0801 13:59:52.408371 24522 sgd_solver.cpp:136] Iteration 22600, lr = 0.00646875, m = 0.9
I0801 13:59:53.990494 24522 solver.cpp:353] Iteration 22700 (63.2058 iter/s, 1.58213s/100 iter), loss = 0.0051047
I0801 13:59:53.990530 24522 solver.cpp:375]     Train net output #0: loss = 0.00510402 (* 1 = 0.00510402 loss)
I0801 13:59:53.990538 24522 sgd_solver.cpp:136] Iteration 22700, lr = 0.00645312, m = 0.9
I0801 13:59:55.593287 24522 solver.cpp:353] Iteration 22800 (62.3931 iter/s, 1.60274s/100 iter), loss = 0.00132588
I0801 13:59:55.593338 24522 solver.cpp:375]     Train net output #0: loss = 0.0013252 (* 1 = 0.0013252 loss)
I0801 13:59:55.593363 24522 sgd_solver.cpp:136] Iteration 22800, lr = 0.0064375, m = 0.9
I0801 13:59:57.189414 24522 solver.cpp:353] Iteration 22900 (62.6538 iter/s, 1.59607s/100 iter), loss = 7.60802e-05
I0801 13:59:57.189604 24522 solver.cpp:375]     Train net output #0: loss = 7.53939e-05 (* 1 = 7.53939e-05 loss)
I0801 13:59:57.189682 24522 sgd_solver.cpp:136] Iteration 22900, lr = 0.00642187, m = 0.9
I0801 13:59:58.751377 24522 solver.cpp:404] Sparsity after update:
I0801 13:59:58.753204 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 13:59:58.753214 24522 net.cpp:2270] conv1a_param_0(0.16) 
I0801 13:59:58.753223 24522 net.cpp:2270] conv1b_param_0(0.281) 
I0801 13:59:58.753237 24522 net.cpp:2270] fc10_param_0(0) 
I0801 13:59:58.753242 24522 net.cpp:2270] res2a_branch2a_param_0(0.378) 
I0801 13:59:58.753245 24522 net.cpp:2270] res2a_branch2b_param_0(0.375) 
I0801 13:59:58.753249 24522 net.cpp:2270] res3a_branch2a_param_0(0.378) 
I0801 13:59:58.753257 24522 net.cpp:2270] res3a_branch2b_param_0(0.378) 
I0801 13:59:58.753260 24522 net.cpp:2270] res4a_branch2a_param_0(0.379) 
I0801 13:59:58.753264 24522 net.cpp:2270] res4a_branch2b_param_0(0.378) 
I0801 13:59:58.753268 24522 net.cpp:2270] res5a_branch2a_param_0(0.361) 
I0801 13:59:58.753271 24522 net.cpp:2270] res5a_branch2b_param_0(0.373) 
I0801 13:59:58.753275 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (866878/2.3599e+06) 0.367
I0801 13:59:58.753301 24522 solver.cpp:550] Iteration 23000, Testing net (#0)
I0801 13:59:59.569330 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.910295
I0801 13:59:59.569350 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 13:59:59.569355 24522 solver.cpp:635]     Test net output #2: loss = 0.346102 (* 1 = 0.346102 loss)
I0801 13:59:59.569370 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.81604s
I0801 13:59:59.584956 24549 solver.cpp:450] Finding and applying sparsity: 0.4
I0801 14:00:16.261052 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:00:16.262981 24522 solver.cpp:353] Iteration 23000 (5.243 iter/s, 19.073s/100 iter), loss = 0.00203507
I0801 14:00:16.263005 24522 solver.cpp:375]     Train net output #0: loss = 0.00203438 (* 1 = 0.00203438 loss)
I0801 14:00:16.263011 24522 sgd_solver.cpp:136] Iteration 23000, lr = 0.00640625, m = 0.9
I0801 14:00:18.072160 24522 solver.cpp:353] Iteration 23100 (55.2754 iter/s, 1.80912s/100 iter), loss = 0.00179673
I0801 14:00:18.072185 24522 solver.cpp:375]     Train net output #0: loss = 0.00179605 (* 1 = 0.00179605 loss)
I0801 14:00:18.072190 24522 sgd_solver.cpp:136] Iteration 23100, lr = 0.00639063, m = 0.9
I0801 14:00:19.657762 24522 solver.cpp:353] Iteration 23200 (63.0696 iter/s, 1.58555s/100 iter), loss = 0.0020372
I0801 14:00:19.657788 24522 solver.cpp:375]     Train net output #0: loss = 0.00203651 (* 1 = 0.00203651 loss)
I0801 14:00:19.657794 24522 sgd_solver.cpp:136] Iteration 23200, lr = 0.006375, m = 0.9
I0801 14:00:21.257598 24522 solver.cpp:353] Iteration 23300 (62.5083 iter/s, 1.59979s/100 iter), loss = 0.000830606
I0801 14:00:21.257627 24522 solver.cpp:375]     Train net output #0: loss = 0.000829923 (* 1 = 0.000829923 loss)
I0801 14:00:21.257632 24522 sgd_solver.cpp:136] Iteration 23300, lr = 0.00635938, m = 0.9
I0801 14:00:22.842555 24522 solver.cpp:353] Iteration 23400 (63.0952 iter/s, 1.58491s/100 iter), loss = 0.000698262
I0801 14:00:22.842578 24522 solver.cpp:375]     Train net output #0: loss = 0.000697579 (* 1 = 0.000697579 loss)
I0801 14:00:22.842583 24522 sgd_solver.cpp:136] Iteration 23400, lr = 0.00634375, m = 0.9
I0801 14:00:24.424775 24522 solver.cpp:353] Iteration 23500 (63.2043 iter/s, 1.58217s/100 iter), loss = 0.000264578
I0801 14:00:24.424829 24522 solver.cpp:375]     Train net output #0: loss = 0.000263896 (* 1 = 0.000263896 loss)
I0801 14:00:24.424844 24522 sgd_solver.cpp:136] Iteration 23500, lr = 0.00632813, m = 0.9
I0801 14:00:25.999186 24522 solver.cpp:353] Iteration 23600 (63.5177 iter/s, 1.57436s/100 iter), loss = 0.00102565
I0801 14:00:25.999212 24522 solver.cpp:375]     Train net output #0: loss = 0.00102496 (* 1 = 0.00102496 loss)
I0801 14:00:25.999218 24522 sgd_solver.cpp:136] Iteration 23600, lr = 0.0063125, m = 0.9
I0801 14:00:27.586519 24522 solver.cpp:353] Iteration 23700 (63.0007 iter/s, 1.58728s/100 iter), loss = 0.00135949
I0801 14:00:27.586567 24522 solver.cpp:375]     Train net output #0: loss = 0.00135881 (* 1 = 0.00135881 loss)
I0801 14:00:27.586580 24522 sgd_solver.cpp:136] Iteration 23700, lr = 0.00629687, m = 0.9
I0801 14:00:29.196213 24522 solver.cpp:353] Iteration 23800 (62.1255 iter/s, 1.60964s/100 iter), loss = 0.00185312
I0801 14:00:29.196240 24522 solver.cpp:375]     Train net output #0: loss = 0.00185244 (* 1 = 0.00185244 loss)
I0801 14:00:29.196247 24522 sgd_solver.cpp:136] Iteration 23800, lr = 0.00628125, m = 0.9
I0801 14:00:30.781970 24522 solver.cpp:353] Iteration 23900 (63.0634 iter/s, 1.58571s/100 iter), loss = 0.00177764
I0801 14:00:30.782044 24522 solver.cpp:375]     Train net output #0: loss = 0.00177696 (* 1 = 0.00177696 loss)
I0801 14:00:30.782064 24522 sgd_solver.cpp:136] Iteration 23900, lr = 0.00626562, m = 0.9
I0801 14:00:32.335335 24522 solver.cpp:404] Sparsity after update:
I0801 14:00:32.336989 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:00:32.336998 24522 net.cpp:2270] conv1a_param_0(0.171) 
I0801 14:00:32.337005 24522 net.cpp:2270] conv1b_param_0(0.316) 
I0801 14:00:32.337007 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:00:32.337009 24522 net.cpp:2270] res2a_branch2a_param_0(0.399) 
I0801 14:00:32.337011 24522 net.cpp:2270] res2a_branch2b_param_0(0.396) 
I0801 14:00:32.337013 24522 net.cpp:2270] res3a_branch2a_param_0(0.399) 
I0801 14:00:32.337015 24522 net.cpp:2270] res3a_branch2b_param_0(0.399) 
I0801 14:00:32.337023 24522 net.cpp:2270] res4a_branch2a_param_0(0.399) 
I0801 14:00:32.337024 24522 net.cpp:2270] res4a_branch2b_param_0(0.399) 
I0801 14:00:32.337026 24522 net.cpp:2270] res5a_branch2a_param_0(0.386) 
I0801 14:00:32.337031 24522 net.cpp:2270] res5a_branch2b_param_0(0.395) 
I0801 14:00:32.337033 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (920650/2.3599e+06) 0.39
I0801 14:00:32.337056 24522 solver.cpp:550] Iteration 24000, Testing net (#0)
I0801 14:00:33.145632 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.905883
I0801 14:00:33.145651 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 14:00:33.145656 24522 solver.cpp:635]     Test net output #2: loss = 0.355252 (* 1 = 0.355252 loss)
I0801 14:00:33.145670 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.808586s
I0801 14:00:33.161176 24549 solver.cpp:450] Finding and applying sparsity: 0.42
I0801 14:00:50.090773 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:00:50.092679 24522 solver.cpp:353] Iteration 24000 (5.17862 iter/s, 19.3102s/100 iter), loss = 0.00269516
I0801 14:00:50.092696 24522 solver.cpp:375]     Train net output #0: loss = 0.00269448 (* 1 = 0.00269448 loss)
I0801 14:00:50.092703 24522 sgd_solver.cpp:136] Iteration 24000, lr = 0.00625, m = 0.9
I0801 14:00:51.916821 24522 solver.cpp:353] Iteration 24100 (54.8221 iter/s, 1.82408s/100 iter), loss = 0.00157487
I0801 14:00:51.916846 24522 solver.cpp:375]     Train net output #0: loss = 0.00157419 (* 1 = 0.00157419 loss)
I0801 14:00:51.916852 24522 sgd_solver.cpp:136] Iteration 24100, lr = 0.00623438, m = 0.9
I0801 14:00:53.497592 24522 solver.cpp:353] Iteration 24200 (63.2623 iter/s, 1.58072s/100 iter), loss = 0.00141924
I0801 14:00:53.497618 24522 solver.cpp:375]     Train net output #0: loss = 0.00141856 (* 1 = 0.00141856 loss)
I0801 14:00:53.497624 24522 sgd_solver.cpp:136] Iteration 24200, lr = 0.00621875, m = 0.9
I0801 14:00:55.088201 24522 solver.cpp:353] Iteration 24300 (62.871 iter/s, 1.59056s/100 iter), loss = 0.000578608
I0801 14:00:55.088224 24522 solver.cpp:375]     Train net output #0: loss = 0.000577928 (* 1 = 0.000577928 loss)
I0801 14:00:55.088230 24522 sgd_solver.cpp:136] Iteration 24300, lr = 0.00620312, m = 0.9
I0801 14:00:56.686486 24522 solver.cpp:353] Iteration 24400 (62.5689 iter/s, 1.59824s/100 iter), loss = 0.00076415
I0801 14:00:56.686511 24522 solver.cpp:375]     Train net output #0: loss = 0.00076347 (* 1 = 0.00076347 loss)
I0801 14:00:56.686517 24522 sgd_solver.cpp:136] Iteration 24400, lr = 0.0061875, m = 0.9
I0801 14:00:58.261090 24522 solver.cpp:353] Iteration 24500 (63.51 iter/s, 1.57455s/100 iter), loss = 0.00047016
I0801 14:00:58.261140 24522 solver.cpp:375]     Train net output #0: loss = 0.000469478 (* 1 = 0.000469478 loss)
I0801 14:00:58.261153 24522 sgd_solver.cpp:136] Iteration 24500, lr = 0.00617187, m = 0.9
I0801 14:00:59.853148 24522 solver.cpp:353] Iteration 24600 (62.8137 iter/s, 1.59201s/100 iter), loss = 0.00197787
I0801 14:00:59.853170 24522 solver.cpp:375]     Train net output #0: loss = 0.00197719 (* 1 = 0.00197719 loss)
I0801 14:00:59.853176 24522 sgd_solver.cpp:136] Iteration 24600, lr = 0.00615625, m = 0.9
I0801 14:01:01.441236 24522 solver.cpp:353] Iteration 24700 (62.9708 iter/s, 1.58804s/100 iter), loss = 0.00088127
I0801 14:01:01.441259 24522 solver.cpp:375]     Train net output #0: loss = 0.000880588 (* 1 = 0.000880588 loss)
I0801 14:01:01.441263 24522 sgd_solver.cpp:136] Iteration 24700, lr = 0.00614062, m = 0.9
I0801 14:01:03.023525 24522 solver.cpp:353] Iteration 24800 (63.2016 iter/s, 1.58224s/100 iter), loss = 0.000719587
I0801 14:01:03.023552 24522 solver.cpp:375]     Train net output #0: loss = 0.000718907 (* 1 = 0.000718907 loss)
I0801 14:01:03.023558 24522 sgd_solver.cpp:136] Iteration 24800, lr = 0.006125, m = 0.9
I0801 14:01:04.597826 24522 solver.cpp:353] Iteration 24900 (63.5222 iter/s, 1.57425s/100 iter), loss = 5.16188e-05
I0801 14:01:04.597852 24522 solver.cpp:375]     Train net output #0: loss = 5.09386e-05 (* 1 = 5.09386e-05 loss)
I0801 14:01:04.597858 24522 sgd_solver.cpp:136] Iteration 24900, lr = 0.00610937, m = 0.9
I0801 14:01:06.180382 24522 solver.cpp:404] Sparsity after update:
I0801 14:01:06.182008 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:01:06.182016 24522 net.cpp:2270] conv1a_param_0(0.176) 
I0801 14:01:06.182024 24522 net.cpp:2270] conv1b_param_0(0.352) 
I0801 14:01:06.182029 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:01:06.182034 24522 net.cpp:2270] res2a_branch2a_param_0(0.417) 
I0801 14:01:06.182039 24522 net.cpp:2270] res2a_branch2b_param_0(0.417) 
I0801 14:01:06.182042 24522 net.cpp:2270] res3a_branch2a_param_0(0.418) 
I0801 14:01:06.182046 24522 net.cpp:2270] res3a_branch2b_param_0(0.417) 
I0801 14:01:06.182051 24522 net.cpp:2270] res4a_branch2a_param_0(0.419) 
I0801 14:01:06.182056 24522 net.cpp:2270] res4a_branch2b_param_0(0.418) 
I0801 14:01:06.182061 24522 net.cpp:2270] res5a_branch2a_param_0(0.396) 
I0801 14:01:06.182065 24522 net.cpp:2270] res5a_branch2b_param_0(0.414) 
I0801 14:01:06.182070 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (956290/2.3599e+06) 0.405
I0801 14:01:06.183681 24522 solver.cpp:550] Iteration 25000, Testing net (#0)
I0801 14:01:06.989956 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.90853
I0801 14:01:06.989975 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.994706
I0801 14:01:06.989981 24522 solver.cpp:635]     Test net output #2: loss = 0.373472 (* 1 = 0.373472 loss)
I0801 14:01:06.989997 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.806288s
I0801 14:01:07.005564 24549 solver.cpp:450] Finding and applying sparsity: 0.44
I0801 14:01:23.681139 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:01:23.683244 24522 solver.cpp:353] Iteration 25000 (5.23975 iter/s, 19.0849s/100 iter), loss = 0.000589835
I0801 14:01:23.683264 24522 solver.cpp:375]     Train net output #0: loss = 0.000589153 (* 1 = 0.000589153 loss)
I0801 14:01:23.683274 24522 sgd_solver.cpp:136] Iteration 25000, lr = 0.00609375, m = 0.9
I0801 14:01:25.503509 24522 solver.cpp:353] Iteration 25100 (54.9388 iter/s, 1.82021s/100 iter), loss = 0.000248309
I0801 14:01:25.503535 24522 solver.cpp:375]     Train net output #0: loss = 0.000247629 (* 1 = 0.000247629 loss)
I0801 14:01:25.503540 24522 sgd_solver.cpp:136] Iteration 25100, lr = 0.00607812, m = 0.9
I0801 14:01:27.095652 24522 solver.cpp:353] Iteration 25200 (62.8105 iter/s, 1.59209s/100 iter), loss = 0.00324748
I0801 14:01:27.095677 24522 solver.cpp:375]     Train net output #0: loss = 0.0032468 (* 1 = 0.0032468 loss)
I0801 14:01:27.095685 24522 sgd_solver.cpp:136] Iteration 25200, lr = 0.0060625, m = 0.9
I0801 14:01:28.663059 24522 solver.cpp:353] Iteration 25300 (63.8015 iter/s, 1.56736s/100 iter), loss = 0.00015576
I0801 14:01:28.663084 24522 solver.cpp:375]     Train net output #0: loss = 0.000155079 (* 1 = 0.000155079 loss)
I0801 14:01:28.663090 24522 sgd_solver.cpp:136] Iteration 25300, lr = 0.00604687, m = 0.9
I0801 14:01:30.247774 24522 solver.cpp:353] Iteration 25400 (63.1048 iter/s, 1.58466s/100 iter), loss = 0.00015903
I0801 14:01:30.247802 24522 solver.cpp:375]     Train net output #0: loss = 0.000158349 (* 1 = 0.000158349 loss)
I0801 14:01:30.247805 24522 sgd_solver.cpp:136] Iteration 25400, lr = 0.00603125, m = 0.9
I0801 14:01:31.844226 24522 solver.cpp:353] Iteration 25500 (62.6408 iter/s, 1.5964s/100 iter), loss = 0.00114769
I0801 14:01:31.844251 24522 solver.cpp:375]     Train net output #0: loss = 0.00114701 (* 1 = 0.00114701 loss)
I0801 14:01:31.844256 24522 sgd_solver.cpp:136] Iteration 25500, lr = 0.00601562, m = 0.9
I0801 14:01:33.431915 24522 solver.cpp:353] Iteration 25600 (62.9866 iter/s, 1.58764s/100 iter), loss = 0.00179576
I0801 14:01:33.431941 24522 solver.cpp:375]     Train net output #0: loss = 0.00179508 (* 1 = 0.00179508 loss)
I0801 14:01:33.431946 24522 sgd_solver.cpp:136] Iteration 25600, lr = 0.006, m = 0.9
I0801 14:01:35.015244 24522 solver.cpp:353] Iteration 25700 (63.16 iter/s, 1.58328s/100 iter), loss = 0.000338449
I0801 14:01:35.015291 24522 solver.cpp:375]     Train net output #0: loss = 0.000337767 (* 1 = 0.000337767 loss)
I0801 14:01:35.015300 24522 sgd_solver.cpp:136] Iteration 25700, lr = 0.00598437, m = 0.9
I0801 14:01:36.604722 24522 solver.cpp:353] Iteration 25800 (62.9158 iter/s, 1.58943s/100 iter), loss = 0.00127182
I0801 14:01:36.604748 24522 solver.cpp:375]     Train net output #0: loss = 0.00127114 (* 1 = 0.00127114 loss)
I0801 14:01:36.604753 24522 sgd_solver.cpp:136] Iteration 25800, lr = 0.00596875, m = 0.9
I0801 14:01:38.195598 24522 solver.cpp:353] Iteration 25900 (62.8604 iter/s, 1.59083s/100 iter), loss = 6.05088e-05
I0801 14:01:38.195623 24522 solver.cpp:375]     Train net output #0: loss = 5.98277e-05 (* 1 = 5.98277e-05 loss)
I0801 14:01:38.195628 24522 sgd_solver.cpp:136] Iteration 25900, lr = 0.00595312, m = 0.9
I0801 14:01:39.767071 24522 solver.cpp:404] Sparsity after update:
I0801 14:01:39.769075 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:01:39.769088 24522 net.cpp:2270] conv1a_param_0(0.183) 
I0801 14:01:39.769096 24522 net.cpp:2270] conv1b_param_0(0.363) 
I0801 14:01:39.769100 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:01:39.769104 24522 net.cpp:2270] res2a_branch2a_param_0(0.437) 
I0801 14:01:39.769109 24522 net.cpp:2270] res2a_branch2b_param_0(0.438) 
I0801 14:01:39.769112 24522 net.cpp:2270] res3a_branch2a_param_0(0.439) 
I0801 14:01:39.769115 24522 net.cpp:2270] res3a_branch2b_param_0(0.438) 
I0801 14:01:39.769119 24522 net.cpp:2270] res4a_branch2a_param_0(0.439) 
I0801 14:01:39.769124 24522 net.cpp:2270] res4a_branch2b_param_0(0.439) 
I0801 14:01:39.769126 24522 net.cpp:2270] res5a_branch2a_param_0(0.416) 
I0801 14:01:39.769130 24522 net.cpp:2270] res5a_branch2b_param_0(0.433) 
I0801 14:01:39.769134 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.00221e+06/2.3599e+06) 0.425
I0801 14:01:39.769160 24522 solver.cpp:550] Iteration 26000, Testing net (#0)
I0801 14:01:40.577688 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.910001
I0801 14:01:40.577708 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995588
I0801 14:01:40.577714 24522 solver.cpp:635]     Test net output #2: loss = 0.372696 (* 1 = 0.372696 loss)
I0801 14:01:40.577730 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.808544s
I0801 14:01:40.594132 24549 solver.cpp:450] Finding and applying sparsity: 0.46
I0801 14:01:58.013207 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:01:58.015166 24522 solver.cpp:353] Iteration 26000 (5.04566 iter/s, 19.819s/100 iter), loss = 0.00121238
I0801 14:01:58.015185 24522 solver.cpp:375]     Train net output #0: loss = 0.0012117 (* 1 = 0.0012117 loss)
I0801 14:01:58.015193 24522 sgd_solver.cpp:136] Iteration 26000, lr = 0.0059375, m = 0.9
I0801 14:01:59.840090 24522 solver.cpp:353] Iteration 26100 (54.7986 iter/s, 1.82487s/100 iter), loss = 0.00205299
I0801 14:01:59.840113 24522 solver.cpp:375]     Train net output #0: loss = 0.00205231 (* 1 = 0.00205231 loss)
I0801 14:01:59.840118 24522 sgd_solver.cpp:136] Iteration 26100, lr = 0.00592188, m = 0.9
I0801 14:02:01.428261 24522 solver.cpp:353] Iteration 26200 (62.9675 iter/s, 1.58812s/100 iter), loss = 0.00045883
I0801 14:02:01.428288 24522 solver.cpp:375]     Train net output #0: loss = 0.000458149 (* 1 = 0.000458149 loss)
I0801 14:02:01.428297 24522 sgd_solver.cpp:136] Iteration 26200, lr = 0.00590625, m = 0.9
I0801 14:02:03.018095 24522 solver.cpp:353] Iteration 26300 (62.9016 iter/s, 1.58978s/100 iter), loss = 0.000833806
I0801 14:02:03.018115 24522 solver.cpp:375]     Train net output #0: loss = 0.000833125 (* 1 = 0.000833125 loss)
I0801 14:02:03.018120 24522 sgd_solver.cpp:136] Iteration 26300, lr = 0.00589063, m = 0.9
I0801 14:02:04.686153 24522 solver.cpp:353] Iteration 26400 (59.9519 iter/s, 1.668s/100 iter), loss = 0.000658299
I0801 14:02:04.686199 24522 solver.cpp:375]     Train net output #0: loss = 0.000657618 (* 1 = 0.000657618 loss)
I0801 14:02:04.686223 24522 sgd_solver.cpp:136] Iteration 26400, lr = 0.005875, m = 0.9
I0801 14:02:06.366571 24522 solver.cpp:353] Iteration 26500 (59.5108 iter/s, 1.68037s/100 iter), loss = 0.00105419
I0801 14:02:06.366600 24522 solver.cpp:375]     Train net output #0: loss = 0.0010535 (* 1 = 0.0010535 loss)
I0801 14:02:06.366605 24522 sgd_solver.cpp:136] Iteration 26500, lr = 0.00585938, m = 0.9
I0801 14:02:06.557858 24487 data_reader.cpp:264] Starting prefetch of epoch 4
I0801 14:02:07.963193 24522 solver.cpp:353] Iteration 26600 (62.6341 iter/s, 1.59657s/100 iter), loss = 0.000371216
I0801 14:02:07.963238 24522 solver.cpp:375]     Train net output #0: loss = 0.000370534 (* 1 = 0.000370534 loss)
I0801 14:02:07.963250 24522 sgd_solver.cpp:136] Iteration 26600, lr = 0.00584375, m = 0.9
I0801 14:02:09.620812 24522 solver.cpp:353] Iteration 26700 (60.3294 iter/s, 1.65757s/100 iter), loss = 0.000280486
I0801 14:02:09.620843 24522 solver.cpp:375]     Train net output #0: loss = 0.000279804 (* 1 = 0.000279804 loss)
I0801 14:02:09.620848 24522 sgd_solver.cpp:136] Iteration 26700, lr = 0.00582812, m = 0.9
I0801 14:02:11.279279 24522 solver.cpp:353] Iteration 26800 (60.2985 iter/s, 1.65841s/100 iter), loss = 0.00187292
I0801 14:02:11.279302 24522 solver.cpp:375]     Train net output #0: loss = 0.00187223 (* 1 = 0.00187223 loss)
I0801 14:02:11.279309 24522 sgd_solver.cpp:136] Iteration 26800, lr = 0.0058125, m = 0.9
I0801 14:02:12.873612 24522 solver.cpp:353] Iteration 26900 (62.7246 iter/s, 1.59427s/100 iter), loss = 0.000148487
I0801 14:02:12.873666 24522 solver.cpp:375]     Train net output #0: loss = 0.000147803 (* 1 = 0.000147803 loss)
I0801 14:02:12.873675 24522 sgd_solver.cpp:136] Iteration 26900, lr = 0.00579687, m = 0.9
I0801 14:02:14.457173 24522 solver.cpp:404] Sparsity after update:
I0801 14:02:14.458777 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:02:14.458786 24522 net.cpp:2270] conv1a_param_0(0.201) 
I0801 14:02:14.458796 24522 net.cpp:2270] conv1b_param_0(0.401) 
I0801 14:02:14.458801 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:02:14.458806 24522 net.cpp:2270] res2a_branch2a_param_0(0.458) 
I0801 14:02:14.458809 24522 net.cpp:2270] res2a_branch2b_param_0(0.458) 
I0801 14:02:14.458813 24522 net.cpp:2270] res3a_branch2a_param_0(0.458) 
I0801 14:02:14.458818 24522 net.cpp:2270] res3a_branch2b_param_0(0.458) 
I0801 14:02:14.458822 24522 net.cpp:2270] res4a_branch2a_param_0(0.459) 
I0801 14:02:14.458827 24522 net.cpp:2270] res4a_branch2b_param_0(0.458) 
I0801 14:02:14.458829 24522 net.cpp:2270] res5a_branch2a_param_0(0.438) 
I0801 14:02:14.458842 24522 net.cpp:2270] res5a_branch2b_param_0(0.454) 
I0801 14:02:14.458847 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.05178e+06/2.3599e+06) 0.446
I0801 14:02:14.458858 24522 solver.cpp:550] Iteration 27000, Testing net (#0)
I0801 14:02:15.353816 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.909119
I0801 14:02:15.353857 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995588
I0801 14:02:15.353874 24522 solver.cpp:635]     Test net output #2: loss = 0.371314 (* 1 = 0.371314 loss)
I0801 14:02:15.353905 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.895014s
I0801 14:02:15.374022 24549 solver.cpp:450] Finding and applying sparsity: 0.48
I0801 14:02:35.550765 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:02:35.552743 24522 solver.cpp:353] Iteration 27000 (4.40946 iter/s, 22.6785s/100 iter), loss = 0.00244515
I0801 14:02:35.552764 24522 solver.cpp:375]     Train net output #0: loss = 0.00244447 (* 1 = 0.00244447 loss)
I0801 14:02:35.552773 24522 sgd_solver.cpp:136] Iteration 27000, lr = 0.00578125, m = 0.9
I0801 14:02:37.381306 24522 solver.cpp:353] Iteration 27100 (54.6896 iter/s, 1.8285s/100 iter), loss = 0.00411485
I0801 14:02:37.381335 24522 solver.cpp:375]     Train net output #0: loss = 0.00411417 (* 1 = 0.00411417 loss)
I0801 14:02:37.381342 24522 sgd_solver.cpp:136] Iteration 27100, lr = 0.00576563, m = 0.9
I0801 14:02:38.981580 24522 solver.cpp:353] Iteration 27200 (62.4912 iter/s, 1.60022s/100 iter), loss = 0.000874988
I0801 14:02:38.981601 24522 solver.cpp:375]     Train net output #0: loss = 0.000874304 (* 1 = 0.000874304 loss)
I0801 14:02:38.981606 24522 sgd_solver.cpp:136] Iteration 27200, lr = 0.00575, m = 0.9
I0801 14:02:40.569185 24522 solver.cpp:353] Iteration 27300 (62.9899 iter/s, 1.58756s/100 iter), loss = 0.00108895
I0801 14:02:40.569208 24522 solver.cpp:375]     Train net output #0: loss = 0.00108827 (* 1 = 0.00108827 loss)
I0801 14:02:40.569216 24522 sgd_solver.cpp:136] Iteration 27300, lr = 0.00573438, m = 0.9
I0801 14:02:42.152681 24522 solver.cpp:353] Iteration 27400 (63.1533 iter/s, 1.58345s/100 iter), loss = 0.000591896
I0801 14:02:42.152709 24522 solver.cpp:375]     Train net output #0: loss = 0.00059121 (* 1 = 0.00059121 loss)
I0801 14:02:42.152714 24522 sgd_solver.cpp:136] Iteration 27400, lr = 0.00571875, m = 0.9
I0801 14:02:43.725005 24522 solver.cpp:353] Iteration 27500 (63.6023 iter/s, 1.57227s/100 iter), loss = 0.000968419
I0801 14:02:43.725036 24522 solver.cpp:375]     Train net output #0: loss = 0.000967733 (* 1 = 0.000967733 loss)
I0801 14:02:43.725044 24522 sgd_solver.cpp:136] Iteration 27500, lr = 0.00570312, m = 0.9
I0801 14:02:45.313719 24522 solver.cpp:353] Iteration 27600 (62.9461 iter/s, 1.58866s/100 iter), loss = 0.00258634
I0801 14:02:45.313771 24522 solver.cpp:375]     Train net output #0: loss = 0.00258566 (* 1 = 0.00258566 loss)
I0801 14:02:45.313784 24522 sgd_solver.cpp:136] Iteration 27600, lr = 0.0056875, m = 0.9
I0801 14:02:46.907027 24522 solver.cpp:353] Iteration 27700 (62.7644 iter/s, 1.59326s/100 iter), loss = 0.00035698
I0801 14:02:46.907074 24522 solver.cpp:375]     Train net output #0: loss = 0.000356294 (* 1 = 0.000356294 loss)
I0801 14:02:46.907230 24522 sgd_solver.cpp:136] Iteration 27700, lr = 0.00567187, m = 0.9
I0801 14:02:48.494879 24522 solver.cpp:353] Iteration 27800 (62.9801 iter/s, 1.5878s/100 iter), loss = 0.00169794
I0801 14:02:48.494928 24522 solver.cpp:375]     Train net output #0: loss = 0.00169725 (* 1 = 0.00169725 loss)
I0801 14:02:48.494940 24522 sgd_solver.cpp:136] Iteration 27800, lr = 0.00565625, m = 0.9
I0801 14:02:50.100929 24522 solver.cpp:353] Iteration 27900 (62.2665 iter/s, 1.606s/100 iter), loss = 0.000554908
I0801 14:02:50.100952 24522 solver.cpp:375]     Train net output #0: loss = 0.000554223 (* 1 = 0.000554223 loss)
I0801 14:02:50.100956 24522 sgd_solver.cpp:136] Iteration 27900, lr = 0.00564062, m = 0.9
I0801 14:02:51.673272 24522 solver.cpp:404] Sparsity after update:
I0801 14:02:51.674899 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:02:51.674907 24522 net.cpp:2270] conv1a_param_0(0.205) 
I0801 14:02:51.674914 24522 net.cpp:2270] conv1b_param_0(0.457) 
I0801 14:02:51.674917 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:02:51.674921 24522 net.cpp:2270] res2a_branch2a_param_0(0.479) 
I0801 14:02:51.674922 24522 net.cpp:2270] res2a_branch2b_param_0(0.479) 
I0801 14:02:51.674926 24522 net.cpp:2270] res3a_branch2a_param_0(0.479) 
I0801 14:02:51.674927 24522 net.cpp:2270] res3a_branch2b_param_0(0.479) 
I0801 14:02:51.674929 24522 net.cpp:2270] res4a_branch2a_param_0(0.479) 
I0801 14:02:51.674932 24522 net.cpp:2270] res4a_branch2b_param_0(0.479) 
I0801 14:02:51.674935 24522 net.cpp:2270] res5a_branch2a_param_0(0.455) 
I0801 14:02:51.674937 24522 net.cpp:2270] res5a_branch2b_param_0(0.476) 
I0801 14:02:51.674939 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.09693e+06/2.3599e+06) 0.465
I0801 14:02:51.674959 24522 solver.cpp:550] Iteration 28000, Testing net (#0)
I0801 14:02:52.486750 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.911472
I0801 14:02:52.486773 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 14:02:52.486781 24522 solver.cpp:635]     Test net output #2: loss = 0.369701 (* 1 = 0.369701 loss)
I0801 14:02:52.486804 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.811816s
I0801 14:02:52.508786 24549 solver.cpp:450] Finding and applying sparsity: 0.5
I0801 14:03:11.925927 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:03:11.927820 24522 solver.cpp:353] Iteration 28000 (4.58163 iter/s, 21.8263s/100 iter), loss = 0.0033412
I0801 14:03:11.927845 24522 solver.cpp:375]     Train net output #0: loss = 0.00334052 (* 1 = 0.00334052 loss)
I0801 14:03:11.927855 24522 sgd_solver.cpp:136] Iteration 28000, lr = 0.005625, m = 0.9
I0801 14:03:13.726022 24522 solver.cpp:353] Iteration 28100 (55.613 iter/s, 1.79814s/100 iter), loss = 0.00172988
I0801 14:03:13.726073 24522 solver.cpp:375]     Train net output #0: loss = 0.00172919 (* 1 = 0.00172919 loss)
I0801 14:03:13.726089 24522 sgd_solver.cpp:136] Iteration 28100, lr = 0.00560937, m = 0.9
I0801 14:03:15.316104 24522 solver.cpp:353] Iteration 28200 (62.8917 iter/s, 1.59003s/100 iter), loss = 0.000275048
I0801 14:03:15.316130 24522 solver.cpp:375]     Train net output #0: loss = 0.000274363 (* 1 = 0.000274363 loss)
I0801 14:03:15.316136 24522 sgd_solver.cpp:136] Iteration 28200, lr = 0.00559375, m = 0.9
I0801 14:03:16.930045 24522 solver.cpp:353] Iteration 28300 (61.9622 iter/s, 1.61389s/100 iter), loss = 0.000165993
I0801 14:03:16.930073 24522 solver.cpp:375]     Train net output #0: loss = 0.000165308 (* 1 = 0.000165308 loss)
I0801 14:03:16.930078 24522 sgd_solver.cpp:136] Iteration 28300, lr = 0.00557812, m = 0.9
I0801 14:03:18.510673 24522 solver.cpp:353] Iteration 28400 (63.268 iter/s, 1.58058s/100 iter), loss = 0.00266383
I0801 14:03:18.510720 24522 solver.cpp:375]     Train net output #0: loss = 0.00266314 (* 1 = 0.00266314 loss)
I0801 14:03:18.510732 24522 sgd_solver.cpp:136] Iteration 28400, lr = 0.0055625, m = 0.9
I0801 14:03:20.090108 24522 solver.cpp:353] Iteration 28500 (63.3158 iter/s, 1.57939s/100 iter), loss = 0.00164701
I0801 14:03:20.090133 24522 solver.cpp:375]     Train net output #0: loss = 0.00164632 (* 1 = 0.00164632 loss)
I0801 14:03:20.090139 24522 sgd_solver.cpp:136] Iteration 28500, lr = 0.00554687, m = 0.9
I0801 14:03:21.669046 24522 solver.cpp:353] Iteration 28600 (63.3358 iter/s, 1.57889s/100 iter), loss = 0.000725548
I0801 14:03:21.669070 24522 solver.cpp:375]     Train net output #0: loss = 0.000724866 (* 1 = 0.000724866 loss)
I0801 14:03:21.669075 24522 sgd_solver.cpp:136] Iteration 28600, lr = 0.00553125, m = 0.9
I0801 14:03:23.253962 24522 solver.cpp:353] Iteration 28700 (63.0968 iter/s, 1.58487s/100 iter), loss = 0.00091369
I0801 14:03:23.254016 24522 solver.cpp:375]     Train net output #0: loss = 0.000913008 (* 1 = 0.000913008 loss)
I0801 14:03:23.254024 24522 sgd_solver.cpp:136] Iteration 28700, lr = 0.00551562, m = 0.9
I0801 14:03:24.857395 24522 solver.cpp:353] Iteration 28800 (62.3683 iter/s, 1.60338s/100 iter), loss = 0.000768929
I0801 14:03:24.857422 24522 solver.cpp:375]     Train net output #0: loss = 0.000768246 (* 1 = 0.000768246 loss)
I0801 14:03:24.857430 24522 sgd_solver.cpp:136] Iteration 28800, lr = 0.0055, m = 0.9
I0801 14:03:26.449237 24522 solver.cpp:353] Iteration 28900 (62.8223 iter/s, 1.59179s/100 iter), loss = 0.00406896
I0801 14:03:26.449288 24522 solver.cpp:375]     Train net output #0: loss = 0.00406828 (* 1 = 0.00406828 loss)
I0801 14:03:26.449301 24522 sgd_solver.cpp:136] Iteration 28900, lr = 0.00548437, m = 0.9
I0801 14:03:28.035491 24522 solver.cpp:404] Sparsity after update:
I0801 14:03:28.037149 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:03:28.037158 24522 net.cpp:2270] conv1a_param_0(0.213) 
I0801 14:03:28.037165 24522 net.cpp:2270] conv1b_param_0(0.5) 
I0801 14:03:28.037168 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:03:28.037171 24522 net.cpp:2270] res2a_branch2a_param_0(0.5) 
I0801 14:03:28.037173 24522 net.cpp:2270] res2a_branch2b_param_0(0.5) 
I0801 14:03:28.037175 24522 net.cpp:2270] res3a_branch2a_param_0(0.5) 
I0801 14:03:28.037178 24522 net.cpp:2270] res3a_branch2b_param_0(0.5) 
I0801 14:03:28.037180 24522 net.cpp:2270] res4a_branch2a_param_0(0.5) 
I0801 14:03:28.037183 24522 net.cpp:2270] res4a_branch2b_param_0(0.5) 
I0801 14:03:28.037184 24522 net.cpp:2270] res5a_branch2a_param_0(0.477) 
I0801 14:03:28.037187 24522 net.cpp:2270] res5a_branch2b_param_0(0.497) 
I0801 14:03:28.037189 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.14827e+06/2.3599e+06) 0.487
I0801 14:03:28.037212 24522 solver.cpp:550] Iteration 29000, Testing net (#0)
I0801 14:03:28.843344 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.915589
I0801 14:03:28.843364 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 14:03:28.843369 24522 solver.cpp:635]     Test net output #2: loss = 0.33576 (* 1 = 0.33576 loss)
I0801 14:03:28.843382 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.806142s
I0801 14:03:28.859042 24549 solver.cpp:450] Finding and applying sparsity: 0.52
I0801 14:03:48.609366 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:03:48.611320 24522 solver.cpp:353] Iteration 29000 (4.51234 iter/s, 22.1615s/100 iter), loss = 0.000157744
I0801 14:03:48.611340 24522 solver.cpp:375]     Train net output #0: loss = 0.000157061 (* 1 = 0.000157061 loss)
I0801 14:03:48.611344 24522 sgd_solver.cpp:136] Iteration 29000, lr = 0.00546875, m = 0.9
I0801 14:03:50.405907 24522 solver.cpp:353] Iteration 29100 (55.725 iter/s, 1.79453s/100 iter), loss = 0.0015412
I0801 14:03:50.405987 24522 solver.cpp:375]     Train net output #0: loss = 0.00154051 (* 1 = 0.00154051 loss)
I0801 14:03:50.406009 24522 sgd_solver.cpp:136] Iteration 29100, lr = 0.00545313, m = 0.9
I0801 14:03:52.009536 24522 solver.cpp:353] Iteration 29200 (62.3605 iter/s, 1.60358s/100 iter), loss = 0.000392898
I0801 14:03:52.009583 24522 solver.cpp:375]     Train net output #0: loss = 0.000392217 (* 1 = 0.000392217 loss)
I0801 14:03:52.009591 24522 sgd_solver.cpp:136] Iteration 29200, lr = 0.0054375, m = 0.9
I0801 14:03:53.586814 24522 solver.cpp:353] Iteration 29300 (63.4024 iter/s, 1.57723s/100 iter), loss = 0.000653554
I0801 14:03:53.586935 24522 solver.cpp:375]     Train net output #0: loss = 0.000652872 (* 1 = 0.000652872 loss)
I0801 14:03:53.586942 24522 sgd_solver.cpp:136] Iteration 29300, lr = 0.00542188, m = 0.9
I0801 14:03:55.176892 24522 solver.cpp:353] Iteration 29400 (62.892 iter/s, 1.59003s/100 iter), loss = 0.00232988
I0801 14:03:55.176957 24522 solver.cpp:375]     Train net output #0: loss = 0.0023292 (* 1 = 0.0023292 loss)
I0801 14:03:55.176975 24522 sgd_solver.cpp:136] Iteration 29400, lr = 0.00540625, m = 0.9
I0801 14:03:56.744509 24522 solver.cpp:353] Iteration 29500 (63.7932 iter/s, 1.56757s/100 iter), loss = 0.00128368
I0801 14:03:56.744532 24522 solver.cpp:375]     Train net output #0: loss = 0.001283 (* 1 = 0.001283 loss)
I0801 14:03:56.744537 24522 sgd_solver.cpp:136] Iteration 29500, lr = 0.00539062, m = 0.9
I0801 14:03:58.336319 24522 solver.cpp:353] Iteration 29600 (62.8236 iter/s, 1.59176s/100 iter), loss = 0.000563355
I0801 14:03:58.336349 24522 solver.cpp:375]     Train net output #0: loss = 0.000562674 (* 1 = 0.000562674 loss)
I0801 14:03:58.336355 24522 sgd_solver.cpp:136] Iteration 29600, lr = 0.005375, m = 0.9
I0801 14:03:59.924002 24522 solver.cpp:353] Iteration 29700 (62.987 iter/s, 1.58763s/100 iter), loss = 0.00328812
I0801 14:03:59.924062 24522 solver.cpp:375]     Train net output #0: loss = 0.00328744 (* 1 = 0.00328744 loss)
I0801 14:03:59.924082 24522 sgd_solver.cpp:136] Iteration 29700, lr = 0.00535937, m = 0.9
I0801 14:04:01.507500 24522 solver.cpp:353] Iteration 29800 (63.1533 iter/s, 1.58345s/100 iter), loss = 0.00116331
I0801 14:04:01.507529 24522 solver.cpp:375]     Train net output #0: loss = 0.00116262 (* 1 = 0.00116262 loss)
I0801 14:04:01.507534 24522 sgd_solver.cpp:136] Iteration 29800, lr = 0.00534375, m = 0.9
I0801 14:04:03.082367 24522 solver.cpp:353] Iteration 29900 (63.4994 iter/s, 1.57482s/100 iter), loss = 0.00119165
I0801 14:04:03.082415 24522 solver.cpp:375]     Train net output #0: loss = 0.00119096 (* 1 = 0.00119096 loss)
I0801 14:04:03.082427 24522 sgd_solver.cpp:136] Iteration 29900, lr = 0.00532812, m = 0.9
I0801 14:04:04.637586 24522 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_30000.caffemodel
I0801 14:04:04.645647 24522 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_30000.solverstate
I0801 14:04:04.649188 24522 solver.cpp:404] Sparsity after update:
I0801 14:04:04.651032 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:04:04.651041 24522 net.cpp:2270] conv1a_param_0(0.217) 
I0801 14:04:04.651051 24522 net.cpp:2270] conv1b_param_0(0.498) 
I0801 14:04:04.651056 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:04:04.651059 24522 net.cpp:2270] res2a_branch2a_param_0(0.517) 
I0801 14:04:04.651063 24522 net.cpp:2270] res2a_branch2b_param_0(0.514) 
I0801 14:04:04.651067 24522 net.cpp:2270] res3a_branch2a_param_0(0.519) 
I0801 14:04:04.651080 24522 net.cpp:2270] res3a_branch2b_param_0(0.517) 
I0801 14:04:04.651085 24522 net.cpp:2270] res4a_branch2a_param_0(0.52) 
I0801 14:04:04.651089 24522 net.cpp:2270] res4a_branch2b_param_0(0.519) 
I0801 14:04:04.651093 24522 net.cpp:2270] res5a_branch2a_param_0(0.494) 
I0801 14:04:04.651098 24522 net.cpp:2270] res5a_branch2b_param_0(0.517) 
I0801 14:04:04.651103 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.1906e+06/2.3599e+06) 0.505
I0801 14:04:04.651113 24522 solver.cpp:550] Iteration 30000, Testing net (#0)
I0801 14:04:05.451701 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.91853
I0801 14:04:05.451720 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 14:04:05.451727 24522 solver.cpp:635]     Test net output #2: loss = 0.321198 (* 1 = 0.321198 loss)
I0801 14:04:05.451743 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.800602s
I0801 14:04:05.467329 24549 solver.cpp:450] Finding and applying sparsity: 0.54
I0801 14:04:25.876719 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:04:25.878650 24522 solver.cpp:353] Iteration 30000 (4.38681 iter/s, 22.7956s/100 iter), loss = 0.00103505
I0801 14:04:25.878669 24522 solver.cpp:375]     Train net output #0: loss = 0.00103437 (* 1 = 0.00103437 loss)
I0801 14:04:25.878674 24522 sgd_solver.cpp:136] Iteration 30000, lr = 0.0053125, m = 0.9
I0801 14:04:27.723160 24522 solver.cpp:353] Iteration 30100 (54.2168 iter/s, 1.84445s/100 iter), loss = 0.00212161
I0801 14:04:27.723182 24522 solver.cpp:375]     Train net output #0: loss = 0.00212093 (* 1 = 0.00212093 loss)
I0801 14:04:27.723186 24522 sgd_solver.cpp:136] Iteration 30100, lr = 0.00529688, m = 0.9
I0801 14:04:29.311976 24522 solver.cpp:353] Iteration 30200 (62.9419 iter/s, 1.58877s/100 iter), loss = 0.00112022
I0801 14:04:29.312002 24522 solver.cpp:375]     Train net output #0: loss = 0.00111954 (* 1 = 0.00111954 loss)
I0801 14:04:29.312008 24522 sgd_solver.cpp:136] Iteration 30200, lr = 0.00528125, m = 0.9
I0801 14:04:30.884639 24522 solver.cpp:353] Iteration 30300 (63.5884 iter/s, 1.57261s/100 iter), loss = 0.000508138
I0801 14:04:30.884663 24522 solver.cpp:375]     Train net output #0: loss = 0.000507456 (* 1 = 0.000507456 loss)
I0801 14:04:30.884670 24522 sgd_solver.cpp:136] Iteration 30300, lr = 0.00526563, m = 0.9
I0801 14:04:32.478104 24522 solver.cpp:353] Iteration 30400 (62.7584 iter/s, 1.59341s/100 iter), loss = 0.00455598
I0801 14:04:32.478133 24522 solver.cpp:375]     Train net output #0: loss = 0.0045553 (* 1 = 0.0045553 loss)
I0801 14:04:32.478137 24522 sgd_solver.cpp:136] Iteration 30400, lr = 0.00525, m = 0.9
I0801 14:04:34.062613 24522 solver.cpp:353] Iteration 30500 (63.1131 iter/s, 1.58446s/100 iter), loss = 0.00179638
I0801 14:04:34.062640 24522 solver.cpp:375]     Train net output #0: loss = 0.00179569 (* 1 = 0.00179569 loss)
I0801 14:04:34.062644 24522 sgd_solver.cpp:136] Iteration 30500, lr = 0.00523437, m = 0.9
I0801 14:04:35.645773 24522 solver.cpp:353] Iteration 30600 (63.1667 iter/s, 1.58311s/100 iter), loss = 0.00195572
I0801 14:04:35.645823 24522 solver.cpp:375]     Train net output #0: loss = 0.00195503 (* 1 = 0.00195503 loss)
I0801 14:04:35.645835 24522 sgd_solver.cpp:136] Iteration 30600, lr = 0.00521875, m = 0.9
I0801 14:04:37.243264 24522 solver.cpp:353] Iteration 30700 (62.6001 iter/s, 1.59744s/100 iter), loss = 0.000156336
I0801 14:04:37.243293 24522 solver.cpp:375]     Train net output #0: loss = 0.000155652 (* 1 = 0.000155652 loss)
I0801 14:04:37.243297 24522 sgd_solver.cpp:136] Iteration 30700, lr = 0.00520312, m = 0.9
I0801 14:04:38.834988 24522 solver.cpp:353] Iteration 30800 (62.827 iter/s, 1.59167s/100 iter), loss = 0.000737585
I0801 14:04:38.835013 24522 solver.cpp:375]     Train net output #0: loss = 0.000736901 (* 1 = 0.000736901 loss)
I0801 14:04:38.835019 24522 sgd_solver.cpp:136] Iteration 30800, lr = 0.0051875, m = 0.9
I0801 14:04:40.426770 24522 solver.cpp:353] Iteration 30900 (62.8246 iter/s, 1.59173s/100 iter), loss = 0.00145899
I0801 14:04:40.426795 24522 solver.cpp:375]     Train net output #0: loss = 0.0014583 (* 1 = 0.0014583 loss)
I0801 14:04:40.426800 24522 sgd_solver.cpp:136] Iteration 30900, lr = 0.00517187, m = 0.9
I0801 14:04:41.985601 24522 solver.cpp:404] Sparsity after update:
I0801 14:04:41.987268 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:04:41.987277 24522 net.cpp:2270] conv1a_param_0(0.234) 
I0801 14:04:41.987287 24522 net.cpp:2270] conv1b_param_0(0.528) 
I0801 14:04:41.987296 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:04:41.987301 24522 net.cpp:2270] res2a_branch2a_param_0(0.538) 
I0801 14:04:41.987309 24522 net.cpp:2270] res2a_branch2b_param_0(0.534) 
I0801 14:04:41.987314 24522 net.cpp:2270] res3a_branch2a_param_0(0.54) 
I0801 14:04:41.987321 24522 net.cpp:2270] res3a_branch2b_param_0(0.538) 
I0801 14:04:41.987326 24522 net.cpp:2270] res4a_branch2a_param_0(0.54) 
I0801 14:04:41.987334 24522 net.cpp:2270] res4a_branch2b_param_0(0.54) 
I0801 14:04:41.987337 24522 net.cpp:2270] res5a_branch2a_param_0(0.514) 
I0801 14:04:41.987345 24522 net.cpp:2270] res5a_branch2b_param_0(0.537) 
I0801 14:04:41.987349 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.23835e+06/2.3599e+06) 0.525
I0801 14:04:41.987375 24522 solver.cpp:550] Iteration 31000, Testing net (#0)
I0801 14:04:42.271692 24520 data_reader.cpp:264] Starting prefetch of epoch 4
I0801 14:04:42.803303 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.920883
I0801 14:04:42.803321 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.998529
I0801 14:04:42.803328 24522 solver.cpp:635]     Test net output #2: loss = 0.308905 (* 1 = 0.308905 loss)
I0801 14:04:42.803345 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.815942s
I0801 14:04:42.818851 24549 solver.cpp:450] Finding and applying sparsity: 0.56
I0801 14:05:04.068400 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:05:04.070423 24522 solver.cpp:353] Iteration 31000 (4.22959 iter/s, 23.643s/100 iter), loss = 0.000266724
I0801 14:05:04.070443 24522 solver.cpp:375]     Train net output #0: loss = 0.00026604 (* 1 = 0.00026604 loss)
I0801 14:05:04.070448 24522 sgd_solver.cpp:136] Iteration 31000, lr = 0.00515625, m = 0.9
I0801 14:05:05.888232 24522 solver.cpp:353] Iteration 31100 (55.013 iter/s, 1.81775s/100 iter), loss = 0.00394839
I0801 14:05:05.888258 24522 solver.cpp:375]     Train net output #0: loss = 0.0039477 (* 1 = 0.0039477 loss)
I0801 14:05:05.888264 24522 sgd_solver.cpp:136] Iteration 31100, lr = 0.00514062, m = 0.9
I0801 14:05:07.480041 24522 solver.cpp:353] Iteration 31200 (62.8237 iter/s, 1.59175s/100 iter), loss = 0.000950691
I0801 14:05:07.480064 24522 solver.cpp:375]     Train net output #0: loss = 0.000950009 (* 1 = 0.000950009 loss)
I0801 14:05:07.480069 24522 sgd_solver.cpp:136] Iteration 31200, lr = 0.005125, m = 0.9
I0801 14:05:09.084508 24522 solver.cpp:353] Iteration 31300 (62.3279 iter/s, 1.60442s/100 iter), loss = 0.00140777
I0801 14:05:09.084561 24522 solver.cpp:375]     Train net output #0: loss = 0.00140709 (* 1 = 0.00140709 loss)
I0801 14:05:09.084574 24522 sgd_solver.cpp:136] Iteration 31300, lr = 0.00510937, m = 0.9
I0801 14:05:10.679778 24522 solver.cpp:353] Iteration 31400 (62.6874 iter/s, 1.59522s/100 iter), loss = 0.00301608
I0801 14:05:10.679848 24522 solver.cpp:375]     Train net output #0: loss = 0.00301539 (* 1 = 0.00301539 loss)
I0801 14:05:10.679869 24522 sgd_solver.cpp:136] Iteration 31400, lr = 0.00509375, m = 0.9
I0801 14:05:12.254474 24522 solver.cpp:353] Iteration 31500 (63.5064 iter/s, 1.57464s/100 iter), loss = 0.00108426
I0801 14:05:12.254498 24522 solver.cpp:375]     Train net output #0: loss = 0.00108357 (* 1 = 0.00108357 loss)
I0801 14:05:12.254503 24522 sgd_solver.cpp:136] Iteration 31500, lr = 0.00507812, m = 0.9
I0801 14:05:13.825881 24522 solver.cpp:353] Iteration 31600 (63.6393 iter/s, 1.57136s/100 iter), loss = 0.00465516
I0801 14:05:13.825908 24522 solver.cpp:375]     Train net output #0: loss = 0.00465447 (* 1 = 0.00465447 loss)
I0801 14:05:13.825913 24522 sgd_solver.cpp:136] Iteration 31600, lr = 0.0050625, m = 0.9
I0801 14:05:15.397774 24522 solver.cpp:353] Iteration 31700 (63.6195 iter/s, 1.57185s/100 iter), loss = 0.000435742
I0801 14:05:15.397825 24522 solver.cpp:375]     Train net output #0: loss = 0.000435058 (* 1 = 0.000435058 loss)
I0801 14:05:15.397836 24522 sgd_solver.cpp:136] Iteration 31700, lr = 0.00504687, m = 0.9
I0801 14:05:16.979081 24522 solver.cpp:353] Iteration 31800 (63.2408 iter/s, 1.58126s/100 iter), loss = 0.000659436
I0801 14:05:16.979130 24522 solver.cpp:375]     Train net output #0: loss = 0.000658751 (* 1 = 0.000658751 loss)
I0801 14:05:16.979142 24522 sgd_solver.cpp:136] Iteration 31800, lr = 0.00503125, m = 0.9
I0801 14:05:18.561480 24522 solver.cpp:353] Iteration 31900 (63.1972 iter/s, 1.58235s/100 iter), loss = 0.00112373
I0801 14:05:18.561525 24522 solver.cpp:375]     Train net output #0: loss = 0.00112305 (* 1 = 0.00112305 loss)
I0801 14:05:18.561533 24522 sgd_solver.cpp:136] Iteration 31900, lr = 0.00501562, m = 0.9
I0801 14:05:20.123788 24522 solver.cpp:404] Sparsity after update:
I0801 14:05:20.125413 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:05:20.125422 24522 net.cpp:2270] conv1a_param_0(0.239) 
I0801 14:05:20.125429 24522 net.cpp:2270] conv1b_param_0(0.556) 
I0801 14:05:20.125432 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:05:20.125434 24522 net.cpp:2270] res2a_branch2a_param_0(0.559) 
I0801 14:05:20.125438 24522 net.cpp:2270] res2a_branch2b_param_0(0.554) 
I0801 14:05:20.125442 24522 net.cpp:2270] res3a_branch2a_param_0(0.559) 
I0801 14:05:20.125447 24522 net.cpp:2270] res3a_branch2b_param_0(0.559) 
I0801 14:05:20.125452 24522 net.cpp:2270] res4a_branch2a_param_0(0.56) 
I0801 14:05:20.125454 24522 net.cpp:2270] res4a_branch2b_param_0(0.559) 
I0801 14:05:20.125458 24522 net.cpp:2270] res5a_branch2a_param_0(0.53) 
I0801 14:05:20.125461 24522 net.cpp:2270] res5a_branch2b_param_0(0.557) 
I0801 14:05:20.125465 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.28034e+06/2.3599e+06) 0.543
I0801 14:05:20.125494 24522 solver.cpp:550] Iteration 32000, Testing net (#0)
I0801 14:05:20.933603 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.909413
I0801 14:05:20.933620 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 14:05:20.933625 24522 solver.cpp:635]     Test net output #2: loss = 0.363852 (* 1 = 0.363852 loss)
I0801 14:05:20.933640 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.808118s
I0801 14:05:20.949180 24549 solver.cpp:450] Finding and applying sparsity: 0.58
I0801 14:05:43.964661 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:05:43.967488 24522 solver.cpp:353] Iteration 32000 (3.93619 iter/s, 25.4053s/100 iter), loss = 0.00194346
I0801 14:05:43.967510 24522 solver.cpp:375]     Train net output #0: loss = 0.00194278 (* 1 = 0.00194278 loss)
I0801 14:05:43.967519 24522 sgd_solver.cpp:136] Iteration 32000, lr = 0.005, m = 0.9
I0801 14:05:46.122074 24522 solver.cpp:353] Iteration 32100 (46.4144 iter/s, 2.15451s/100 iter), loss = 0.000252229
I0801 14:05:46.122162 24522 solver.cpp:375]     Train net output #0: loss = 0.000251544 (* 1 = 0.000251544 loss)
I0801 14:05:46.122192 24522 sgd_solver.cpp:136] Iteration 32100, lr = 0.00498438, m = 0.9
I0801 14:05:47.732707 24522 solver.cpp:353] Iteration 32200 (62.0891 iter/s, 1.61059s/100 iter), loss = 0.00838257
I0801 14:05:47.732755 24522 solver.cpp:375]     Train net output #0: loss = 0.00838189 (* 1 = 0.00838189 loss)
I0801 14:05:47.732764 24522 sgd_solver.cpp:136] Iteration 32200, lr = 0.00496875, m = 0.9
I0801 14:05:49.363518 24522 solver.cpp:353] Iteration 32300 (61.3212 iter/s, 1.63076s/100 iter), loss = 0.00626172
I0801 14:05:49.363543 24522 solver.cpp:375]     Train net output #0: loss = 0.00626103 (* 1 = 0.00626103 loss)
I0801 14:05:49.363549 24522 sgd_solver.cpp:136] Iteration 32300, lr = 0.00495313, m = 0.9
I0801 14:05:51.162928 24522 solver.cpp:353] Iteration 32400 (55.5756 iter/s, 1.79935s/100 iter), loss = 0.000468706
I0801 14:05:51.162966 24522 solver.cpp:375]     Train net output #0: loss = 0.000468021 (* 1 = 0.000468021 loss)
I0801 14:05:51.162976 24522 sgd_solver.cpp:136] Iteration 32400, lr = 0.0049375, m = 0.9
I0801 14:05:52.836819 24522 solver.cpp:353] Iteration 32500 (59.7431 iter/s, 1.67383s/100 iter), loss = 0.000786399
I0801 14:05:52.836846 24522 solver.cpp:375]     Train net output #0: loss = 0.000785715 (* 1 = 0.000785715 loss)
I0801 14:05:52.836851 24522 sgd_solver.cpp:136] Iteration 32500, lr = 0.00492187, m = 0.9
I0801 14:05:54.526526 24522 solver.cpp:353] Iteration 32600 (59.1836 iter/s, 1.68966s/100 iter), loss = 0.00193591
I0801 14:05:54.526551 24522 solver.cpp:375]     Train net output #0: loss = 0.00193522 (* 1 = 0.00193522 loss)
I0801 14:05:54.526556 24522 sgd_solver.cpp:136] Iteration 32600, lr = 0.00490625, m = 0.9
I0801 14:05:56.232028 24522 solver.cpp:353] Iteration 32700 (58.6356 iter/s, 1.70545s/100 iter), loss = 0.000320954
I0801 14:05:56.232077 24522 solver.cpp:375]     Train net output #0: loss = 0.00032027 (* 1 = 0.00032027 loss)
I0801 14:05:56.232089 24522 sgd_solver.cpp:136] Iteration 32700, lr = 0.00489062, m = 0.9
I0801 14:05:57.937086 24522 solver.cpp:353] Iteration 32800 (58.6508 iter/s, 1.70501s/100 iter), loss = 0.000869558
I0801 14:05:57.937135 24522 solver.cpp:375]     Train net output #0: loss = 0.000868874 (* 1 = 0.000868874 loss)
I0801 14:05:57.937147 24522 sgd_solver.cpp:136] Iteration 32800, lr = 0.004875, m = 0.9
I0801 14:05:59.645192 24522 solver.cpp:353] Iteration 32900 (58.5462 iter/s, 1.70805s/100 iter), loss = 0.00133214
I0801 14:05:59.645215 24522 solver.cpp:375]     Train net output #0: loss = 0.00133146 (* 1 = 0.00133146 loss)
I0801 14:05:59.645220 24522 sgd_solver.cpp:136] Iteration 32900, lr = 0.00485937, m = 0.9
I0801 14:06:01.405740 24522 solver.cpp:404] Sparsity after update:
I0801 14:06:01.408229 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:06:01.408248 24522 net.cpp:2270] conv1a_param_0(0.248) 
I0801 14:06:01.408265 24522 net.cpp:2270] conv1b_param_0(0.569) 
I0801 14:06:01.408272 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:06:01.408280 24522 net.cpp:2270] res2a_branch2a_param_0(0.58) 
I0801 14:06:01.408287 24522 net.cpp:2270] res2a_branch2b_param_0(0.573) 
I0801 14:06:01.408293 24522 net.cpp:2270] res3a_branch2a_param_0(0.58) 
I0801 14:06:01.408300 24522 net.cpp:2270] res3a_branch2b_param_0(0.58) 
I0801 14:06:01.408308 24522 net.cpp:2270] res4a_branch2a_param_0(0.58) 
I0801 14:06:01.408314 24522 net.cpp:2270] res4a_branch2b_param_0(0.58) 
I0801 14:06:01.408323 24522 net.cpp:2270] res5a_branch2a_param_0(0.549) 
I0801 14:06:01.408329 24522 net.cpp:2270] res5a_branch2b_param_0(0.577) 
I0801 14:06:01.408336 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.32676e+06/2.3599e+06) 0.562
I0801 14:06:01.408372 24522 solver.cpp:550] Iteration 33000, Testing net (#0)
I0801 14:06:02.259765 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.907648
I0801 14:06:02.259806 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997353
I0801 14:06:02.259824 24522 solver.cpp:635]     Test net output #2: loss = 0.377064 (* 1 = 0.377064 loss)
I0801 14:06:02.259860 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.851454s
I0801 14:06:02.276757 24549 solver.cpp:450] Finding and applying sparsity: 0.6
I0801 14:06:32.441519 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:06:32.443683 24522 solver.cpp:353] Iteration 33000 (3.04901 iter/s, 32.7976s/100 iter), loss = 0.00166459
I0801 14:06:32.443716 24522 solver.cpp:375]     Train net output #0: loss = 0.00166391 (* 1 = 0.00166391 loss)
I0801 14:06:32.443725 24522 sgd_solver.cpp:136] Iteration 33000, lr = 0.00484375, m = 0.9
I0801 14:06:34.456316 24522 solver.cpp:353] Iteration 33100 (49.6879 iter/s, 2.01256s/100 iter), loss = 0.010466
I0801 14:06:34.456360 24522 solver.cpp:375]     Train net output #0: loss = 0.0104653 (* 1 = 0.0104653 loss)
I0801 14:06:34.456504 24522 sgd_solver.cpp:136] Iteration 33100, lr = 0.00482813, m = 0.9
I0801 14:06:36.163357 24522 solver.cpp:353] Iteration 33200 (58.5834 iter/s, 1.70697s/100 iter), loss = 0.000222067
I0801 14:06:36.163646 24522 solver.cpp:375]     Train net output #0: loss = 0.000221384 (* 1 = 0.000221384 loss)
I0801 14:06:36.163805 24522 sgd_solver.cpp:136] Iteration 33200, lr = 0.0048125, m = 0.9
I0801 14:06:37.785192 24522 solver.cpp:353] Iteration 33300 (61.6602 iter/s, 1.62179s/100 iter), loss = 0.00110086
I0801 14:06:37.785220 24522 solver.cpp:375]     Train net output #0: loss = 0.00110017 (* 1 = 0.00110017 loss)
I0801 14:06:37.785228 24522 sgd_solver.cpp:136] Iteration 33300, lr = 0.00479688, m = 0.9
I0801 14:06:39.391163 24522 solver.cpp:353] Iteration 33400 (62.2697 iter/s, 1.60592s/100 iter), loss = 0.00038006
I0801 14:06:39.391186 24522 solver.cpp:375]     Train net output #0: loss = 0.000379376 (* 1 = 0.000379376 loss)
I0801 14:06:39.391191 24522 sgd_solver.cpp:136] Iteration 33400, lr = 0.00478125, m = 0.9
I0801 14:06:40.991514 24522 solver.cpp:353] Iteration 33500 (62.4884 iter/s, 1.6003s/100 iter), loss = 0.00146767
I0801 14:06:40.991544 24522 solver.cpp:375]     Train net output #0: loss = 0.00146699 (* 1 = 0.00146699 loss)
I0801 14:06:40.991551 24522 sgd_solver.cpp:136] Iteration 33500, lr = 0.00476563, m = 0.9
I0801 14:06:42.586542 24522 solver.cpp:353] Iteration 33600 (62.6967 iter/s, 1.59498s/100 iter), loss = 0.000466732
I0801 14:06:42.586565 24522 solver.cpp:375]     Train net output #0: loss = 0.00046605 (* 1 = 0.00046605 loss)
I0801 14:06:42.586570 24522 sgd_solver.cpp:136] Iteration 33600, lr = 0.00475, m = 0.9
I0801 14:06:44.175961 24522 solver.cpp:353] Iteration 33700 (62.9182 iter/s, 1.58937s/100 iter), loss = 0.00111176
I0801 14:06:44.175992 24522 solver.cpp:375]     Train net output #0: loss = 0.00111108 (* 1 = 0.00111108 loss)
I0801 14:06:44.175998 24522 sgd_solver.cpp:136] Iteration 33700, lr = 0.00473437, m = 0.9
I0801 14:06:45.890115 24522 solver.cpp:353] Iteration 33800 (58.3397 iter/s, 1.7141s/100 iter), loss = 0.00480793
I0801 14:06:45.890161 24522 solver.cpp:375]     Train net output #0: loss = 0.00480724 (* 1 = 0.00480724 loss)
I0801 14:06:45.890172 24522 sgd_solver.cpp:136] Iteration 33800, lr = 0.00471875, m = 0.9
I0801 14:06:47.577992 24522 solver.cpp:353] Iteration 33900 (59.2478 iter/s, 1.68783s/100 iter), loss = 0.00157025
I0801 14:06:47.578019 24522 solver.cpp:375]     Train net output #0: loss = 0.00156957 (* 1 = 0.00156957 loss)
I0801 14:06:47.578025 24522 sgd_solver.cpp:136] Iteration 33900, lr = 0.00470312, m = 0.9
I0801 14:06:49.134011 24522 solver.cpp:404] Sparsity after update:
I0801 14:06:49.135589 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:06:49.135597 24522 net.cpp:2270] conv1a_param_0(0.25) 
I0801 14:06:49.135604 24522 net.cpp:2270] conv1b_param_0(0.596) 
I0801 14:06:49.135607 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:06:49.135609 24522 net.cpp:2270] res2a_branch2a_param_0(0.597) 
I0801 14:06:49.135612 24522 net.cpp:2270] res2a_branch2b_param_0(0.591) 
I0801 14:06:49.135614 24522 net.cpp:2270] res3a_branch2a_param_0(0.599) 
I0801 14:06:49.135617 24522 net.cpp:2270] res3a_branch2b_param_0(0.597) 
I0801 14:06:49.135618 24522 net.cpp:2270] res4a_branch2a_param_0(0.6) 
I0801 14:06:49.135622 24522 net.cpp:2270] res4a_branch2b_param_0(0.599) 
I0801 14:06:49.135623 24522 net.cpp:2270] res5a_branch2a_param_0(0.565) 
I0801 14:06:49.135627 24522 net.cpp:2270] res5a_branch2b_param_0(0.599) 
I0801 14:06:49.135628 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.36923e+06/2.3599e+06) 0.58
I0801 14:06:49.135646 24522 solver.cpp:550] Iteration 34000, Testing net (#0)
I0801 14:06:49.942524 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.899413
I0801 14:06:49.942543 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 14:06:49.942548 24522 solver.cpp:635]     Test net output #2: loss = 0.395503 (* 1 = 0.395503 loss)
I0801 14:06:49.942561 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.806887s
I0801 14:06:49.958849 24549 solver.cpp:450] Finding and applying sparsity: 0.62
I0801 14:07:20.742476 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:07:20.744415 24522 solver.cpp:353] Iteration 34000 (3.01518 iter/s, 33.1655s/100 iter), loss = 0.00122027
I0801 14:07:20.744432 24522 solver.cpp:375]     Train net output #0: loss = 0.00121958 (* 1 = 0.00121958 loss)
I0801 14:07:20.744437 24522 sgd_solver.cpp:136] Iteration 34000, lr = 0.0046875, m = 0.9
I0801 14:07:22.673514 24522 solver.cpp:353] Iteration 34100 (51.8393 iter/s, 1.92904s/100 iter), loss = 0.00212065
I0801 14:07:22.673562 24522 solver.cpp:375]     Train net output #0: loss = 0.00211996 (* 1 = 0.00211996 loss)
I0801 14:07:22.673573 24522 sgd_solver.cpp:136] Iteration 34100, lr = 0.00467187, m = 0.9
I0801 14:07:24.374264 24522 solver.cpp:353] Iteration 34200 (58.8001 iter/s, 1.70068s/100 iter), loss = 0.00101198
I0801 14:07:24.374346 24522 solver.cpp:375]     Train net output #0: loss = 0.00101129 (* 1 = 0.00101129 loss)
I0801 14:07:24.374363 24522 sgd_solver.cpp:136] Iteration 34200, lr = 0.00465625, m = 0.9
I0801 14:07:26.109272 24522 solver.cpp:353] Iteration 34300 (57.6381 iter/s, 1.73496s/100 iter), loss = 0.000643793
I0801 14:07:26.109298 24522 solver.cpp:375]     Train net output #0: loss = 0.000643105 (* 1 = 0.000643105 loss)
I0801 14:07:26.109304 24522 sgd_solver.cpp:136] Iteration 34300, lr = 0.00464062, m = 0.9
I0801 14:07:27.798760 24522 solver.cpp:353] Iteration 34400 (59.1915 iter/s, 1.68943s/100 iter), loss = 0.00125563
I0801 14:07:27.798785 24522 solver.cpp:375]     Train net output #0: loss = 0.00125494 (* 1 = 0.00125494 loss)
I0801 14:07:27.798791 24522 sgd_solver.cpp:136] Iteration 34400, lr = 0.004625, m = 0.9
I0801 14:07:29.510670 24522 solver.cpp:353] Iteration 34500 (58.4163 iter/s, 1.71185s/100 iter), loss = 0.000458118
I0801 14:07:29.510715 24522 solver.cpp:375]     Train net output #0: loss = 0.000457431 (* 1 = 0.000457431 loss)
I0801 14:07:29.510725 24522 sgd_solver.cpp:136] Iteration 34500, lr = 0.00460937, m = 0.9
I0801 14:07:31.218269 24522 solver.cpp:353] Iteration 34600 (58.5646 iter/s, 1.70752s/100 iter), loss = 0.00183656
I0801 14:07:31.219427 24522 solver.cpp:375]     Train net output #0: loss = 0.00183587 (* 1 = 0.00183587 loss)
I0801 14:07:31.219542 24522 sgd_solver.cpp:136] Iteration 34600, lr = 0.00459375, m = 0.9
I0801 14:07:32.923406 24522 solver.cpp:353] Iteration 34700 (58.6477 iter/s, 1.7051s/100 iter), loss = 0.00396041
I0801 14:07:32.923429 24522 solver.cpp:375]     Train net output #0: loss = 0.00395972 (* 1 = 0.00395972 loss)
I0801 14:07:32.923435 24522 sgd_solver.cpp:136] Iteration 34700, lr = 0.00457812, m = 0.9
I0801 14:07:34.652912 24522 solver.cpp:353] Iteration 34800 (57.8218 iter/s, 1.72945s/100 iter), loss = 0.00166477
I0801 14:07:34.652935 24522 solver.cpp:375]     Train net output #0: loss = 0.00166408 (* 1 = 0.00166408 loss)
I0801 14:07:34.652942 24522 sgd_solver.cpp:136] Iteration 34800, lr = 0.0045625, m = 0.9
I0801 14:07:36.352766 24522 solver.cpp:353] Iteration 34900 (58.8307 iter/s, 1.69979s/100 iter), loss = 0.000569476
I0801 14:07:36.352807 24522 solver.cpp:375]     Train net output #0: loss = 0.00056879 (* 1 = 0.00056879 loss)
I0801 14:07:36.352823 24522 sgd_solver.cpp:136] Iteration 34900, lr = 0.00454687, m = 0.9
I0801 14:07:38.052497 24522 solver.cpp:404] Sparsity after update:
I0801 14:07:38.054111 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:07:38.054119 24522 net.cpp:2270] conv1a_param_0(0.26) 
I0801 14:07:38.054126 24522 net.cpp:2270] conv1b_param_0(0.609) 
I0801 14:07:38.054128 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:07:38.054131 24522 net.cpp:2270] res2a_branch2a_param_0(0.618) 
I0801 14:07:38.054133 24522 net.cpp:2270] res2a_branch2b_param_0(0.608) 
I0801 14:07:38.054136 24522 net.cpp:2270] res3a_branch2a_param_0(0.62) 
I0801 14:07:38.054139 24522 net.cpp:2270] res3a_branch2b_param_0(0.618) 
I0801 14:07:38.054142 24522 net.cpp:2270] res4a_branch2a_param_0(0.62) 
I0801 14:07:38.054145 24522 net.cpp:2270] res4a_branch2b_param_0(0.62) 
I0801 14:07:38.054148 24522 net.cpp:2270] res5a_branch2a_param_0(0.588) 
I0801 14:07:38.054152 24522 net.cpp:2270] res5a_branch2b_param_0(0.618) 
I0801 14:07:38.054155 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.42039e+06/2.3599e+06) 0.602
I0801 14:07:38.054184 24522 solver.cpp:550] Iteration 35000, Testing net (#0)
I0801 14:07:38.284687 24520 data_reader.cpp:264] Starting prefetch of epoch 5
I0801 14:07:38.917353 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.89706
I0801 14:07:38.917371 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 14:07:38.917376 24522 solver.cpp:635]     Test net output #2: loss = 0.4105 (* 1 = 0.4105 loss)
I0801 14:07:38.917392 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.863178s
I0801 14:07:38.933218 24549 solver.cpp:450] Finding and applying sparsity: 0.64
I0801 14:08:10.141633 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:08:10.143838 24522 solver.cpp:353] Iteration 35000 (2.95945 iter/s, 33.7901s/100 iter), loss = 0.0178318
I0801 14:08:10.143877 24522 solver.cpp:375]     Train net output #0: loss = 0.0178311 (* 1 = 0.0178311 loss)
I0801 14:08:10.143892 24522 sgd_solver.cpp:136] Iteration 35000, lr = 0.00453125, m = 0.9
I0801 14:08:12.214037 24522 solver.cpp:353] Iteration 35100 (48.3061 iter/s, 2.07013s/100 iter), loss = 0.00150618
I0801 14:08:12.214076 24522 solver.cpp:375]     Train net output #0: loss = 0.0015055 (* 1 = 0.0015055 loss)
I0801 14:08:12.214088 24522 sgd_solver.cpp:136] Iteration 35100, lr = 0.00451563, m = 0.9
I0801 14:08:13.854209 24522 solver.cpp:353] Iteration 35200 (60.971 iter/s, 1.64012s/100 iter), loss = 0.00050316
I0801 14:08:13.854261 24522 solver.cpp:375]     Train net output #0: loss = 0.000502479 (* 1 = 0.000502479 loss)
I0801 14:08:13.854285 24522 sgd_solver.cpp:136] Iteration 35200, lr = 0.0045, m = 0.9
I0801 14:08:15.659462 24522 solver.cpp:353] Iteration 35300 (55.3957 iter/s, 1.8052s/100 iter), loss = 0.00171614
I0801 14:08:15.659487 24522 solver.cpp:375]     Train net output #0: loss = 0.00171545 (* 1 = 0.00171545 loss)
I0801 14:08:15.659492 24522 sgd_solver.cpp:136] Iteration 35300, lr = 0.00448438, m = 0.9
I0801 14:08:17.321251 24522 solver.cpp:353] Iteration 35400 (60.1783 iter/s, 1.66173s/100 iter), loss = 0.000770561
I0801 14:08:17.321300 24522 solver.cpp:375]     Train net output #0: loss = 0.000769875 (* 1 = 0.000769875 loss)
I0801 14:08:17.321315 24522 sgd_solver.cpp:136] Iteration 35400, lr = 0.00446875, m = 0.9
I0801 14:08:18.946945 24522 solver.cpp:353] Iteration 35500 (61.5141 iter/s, 1.62564s/100 iter), loss = 0.00129598
I0801 14:08:18.946972 24522 solver.cpp:375]     Train net output #0: loss = 0.00129529 (* 1 = 0.00129529 loss)
I0801 14:08:18.946979 24522 sgd_solver.cpp:136] Iteration 35500, lr = 0.00445312, m = 0.9
I0801 14:08:20.677410 24522 solver.cpp:353] Iteration 35600 (57.79 iter/s, 1.7304s/100 iter), loss = 0.0021757
I0801 14:08:20.677480 24522 solver.cpp:375]     Train net output #0: loss = 0.00217501 (* 1 = 0.00217501 loss)
I0801 14:08:20.677500 24522 sgd_solver.cpp:136] Iteration 35600, lr = 0.0044375, m = 0.9
I0801 14:08:22.414221 24522 solver.cpp:353] Iteration 35700 (57.5788 iter/s, 1.73675s/100 iter), loss = 0.0013021
I0801 14:08:22.414263 24522 solver.cpp:375]     Train net output #0: loss = 0.00130141 (* 1 = 0.00130141 loss)
I0801 14:08:22.414273 24522 sgd_solver.cpp:136] Iteration 35700, lr = 0.00442187, m = 0.9
I0801 14:08:24.130125 24522 solver.cpp:353] Iteration 35800 (58.28 iter/s, 1.71586s/100 iter), loss = 0.00560029
I0801 14:08:24.130147 24522 solver.cpp:375]     Train net output #0: loss = 0.00559961 (* 1 = 0.00559961 loss)
I0801 14:08:24.130153 24522 sgd_solver.cpp:136] Iteration 35800, lr = 0.00440625, m = 0.9
I0801 14:08:25.793414 24522 solver.cpp:353] Iteration 35900 (60.1238 iter/s, 1.66324s/100 iter), loss = 0.000155516
I0801 14:08:25.793833 24522 solver.cpp:375]     Train net output #0: loss = 0.000154831 (* 1 = 0.000154831 loss)
I0801 14:08:25.793853 24522 sgd_solver.cpp:136] Iteration 35900, lr = 0.00439062, m = 0.9
I0801 14:08:27.436554 24522 solver.cpp:404] Sparsity after update:
I0801 14:08:27.438174 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:08:27.438185 24522 net.cpp:2270] conv1a_param_0(0.255) 
I0801 14:08:27.438194 24522 net.cpp:2270] conv1b_param_0(0.635) 
I0801 14:08:27.438199 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:08:27.438205 24522 net.cpp:2270] res2a_branch2a_param_0(0.639) 
I0801 14:08:27.438208 24522 net.cpp:2270] res2a_branch2b_param_0(0.623) 
I0801 14:08:27.438212 24522 net.cpp:2270] res3a_branch2a_param_0(0.639) 
I0801 14:08:27.438216 24522 net.cpp:2270] res3a_branch2b_param_0(0.639) 
I0801 14:08:27.438220 24522 net.cpp:2270] res4a_branch2a_param_0(0.64) 
I0801 14:08:27.438225 24522 net.cpp:2270] res4a_branch2b_param_0(0.639) 
I0801 14:08:27.438228 24522 net.cpp:2270] res5a_branch2a_param_0(0.604) 
I0801 14:08:27.438233 24522 net.cpp:2270] res5a_branch2b_param_0(0.639) 
I0801 14:08:27.438236 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.46319e+06/2.3599e+06) 0.62
I0801 14:08:27.438261 24522 solver.cpp:550] Iteration 36000, Testing net (#0)
I0801 14:08:27.461385 24521 blocking_queue.cpp:40] Waiting for datum
I0801 14:08:28.329282 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.902354
I0801 14:08:28.329311 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997353
I0801 14:08:28.329319 24522 solver.cpp:635]     Test net output #2: loss = 0.382221 (* 1 = 0.382221 loss)
I0801 14:08:28.329342 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.891048s
I0801 14:08:28.355633 24549 solver.cpp:450] Finding and applying sparsity: 0.66
I0801 14:09:01.426973 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:09:01.428946 24522 solver.cpp:353] Iteration 36000 (2.80627 iter/s, 35.6345s/100 iter), loss = 0.000940949
I0801 14:09:01.428966 24522 solver.cpp:375]     Train net output #0: loss = 0.000940263 (* 1 = 0.000940263 loss)
I0801 14:09:01.428973 24522 sgd_solver.cpp:136] Iteration 36000, lr = 0.004375, m = 0.9
I0801 14:09:03.330200 24522 solver.cpp:353] Iteration 36100 (52.5987 iter/s, 1.90119s/100 iter), loss = 0.000881535
I0801 14:09:03.330343 24522 solver.cpp:375]     Train net output #0: loss = 0.000880851 (* 1 = 0.000880851 loss)
I0801 14:09:03.330394 24522 sgd_solver.cpp:136] Iteration 36100, lr = 0.00435938, m = 0.9
I0801 14:09:05.014261 24522 solver.cpp:353] Iteration 36200 (59.3822 iter/s, 1.68401s/100 iter), loss = 0.000535197
I0801 14:09:05.014310 24522 solver.cpp:375]     Train net output #0: loss = 0.000534513 (* 1 = 0.000534513 loss)
I0801 14:09:05.014323 24522 sgd_solver.cpp:136] Iteration 36200, lr = 0.00434375, m = 0.9
I0801 14:09:06.740676 24522 solver.cpp:353] Iteration 36300 (57.9254 iter/s, 1.72636s/100 iter), loss = 0.000626915
I0801 14:09:06.740702 24522 solver.cpp:375]     Train net output #0: loss = 0.000626229 (* 1 = 0.000626229 loss)
I0801 14:09:06.740708 24522 sgd_solver.cpp:136] Iteration 36300, lr = 0.00432813, m = 0.9
I0801 14:09:08.377117 24522 solver.cpp:353] Iteration 36400 (61.1101 iter/s, 1.63639s/100 iter), loss = 0.00291853
I0801 14:09:08.377143 24522 solver.cpp:375]     Train net output #0: loss = 0.00291784 (* 1 = 0.00291784 loss)
I0801 14:09:08.377148 24522 sgd_solver.cpp:136] Iteration 36400, lr = 0.0043125, m = 0.9
I0801 14:09:09.952302 24522 solver.cpp:353] Iteration 36500 (63.4867 iter/s, 1.57513s/100 iter), loss = 0.00191148
I0801 14:09:09.952354 24522 solver.cpp:375]     Train net output #0: loss = 0.0019108 (* 1 = 0.0019108 loss)
I0801 14:09:09.952366 24522 sgd_solver.cpp:136] Iteration 36500, lr = 0.00429688, m = 0.9
I0801 14:09:11.556238 24522 solver.cpp:353] Iteration 36600 (62.3486 iter/s, 1.60389s/100 iter), loss = 0.00161355
I0801 14:09:11.556300 24522 solver.cpp:375]     Train net output #0: loss = 0.00161286 (* 1 = 0.00161286 loss)
I0801 14:09:11.556318 24522 sgd_solver.cpp:136] Iteration 36600, lr = 0.00428125, m = 0.9
I0801 14:09:13.129235 24522 solver.cpp:353] Iteration 36700 (63.575 iter/s, 1.57294s/100 iter), loss = 0.00100522
I0801 14:09:13.129261 24522 solver.cpp:375]     Train net output #0: loss = 0.00100454 (* 1 = 0.00100454 loss)
I0801 14:09:13.129266 24522 sgd_solver.cpp:136] Iteration 36700, lr = 0.00426562, m = 0.9
I0801 14:09:14.788202 24522 solver.cpp:353] Iteration 36800 (60.2803 iter/s, 1.65892s/100 iter), loss = 0.000662595
I0801 14:09:14.788226 24522 solver.cpp:375]     Train net output #0: loss = 0.000661914 (* 1 = 0.000661914 loss)
I0801 14:09:14.788231 24522 sgd_solver.cpp:136] Iteration 36800, lr = 0.00425, m = 0.9
I0801 14:09:16.426545 24522 solver.cpp:353] Iteration 36900 (61.0392 iter/s, 1.63829s/100 iter), loss = 0.00266728
I0801 14:09:16.426579 24522 solver.cpp:375]     Train net output #0: loss = 0.00266659 (* 1 = 0.00266659 loss)
I0801 14:09:16.426585 24522 sgd_solver.cpp:136] Iteration 36900, lr = 0.00423437, m = 0.9
I0801 14:09:18.112174 24522 solver.cpp:404] Sparsity after update:
I0801 14:09:18.113868 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:09:18.113876 24522 net.cpp:2270] conv1a_param_0(0.278) 
I0801 14:09:18.113883 24522 net.cpp:2270] conv1b_param_0(0.648) 
I0801 14:09:18.113886 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:09:18.113891 24522 net.cpp:2270] res2a_branch2a_param_0(0.66) 
I0801 14:09:18.113894 24522 net.cpp:2270] res2a_branch2b_param_0(0.636) 
I0801 14:09:18.113898 24522 net.cpp:2270] res3a_branch2a_param_0(0.66) 
I0801 14:09:18.113901 24522 net.cpp:2270] res3a_branch2b_param_0(0.659) 
I0801 14:09:18.113905 24522 net.cpp:2270] res4a_branch2a_param_0(0.66) 
I0801 14:09:18.113909 24522 net.cpp:2270] res4a_branch2b_param_0(0.66) 
I0801 14:09:18.113912 24522 net.cpp:2270] res5a_branch2a_param_0(0.623) 
I0801 14:09:18.113916 24522 net.cpp:2270] res5a_branch2b_param_0(0.659) 
I0801 14:09:18.113921 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.50816e+06/2.3599e+06) 0.639
I0801 14:09:18.113946 24522 solver.cpp:550] Iteration 37000, Testing net (#0)
I0801 14:09:18.988343 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.906472
I0801 14:09:18.988360 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 14:09:18.988365 24522 solver.cpp:635]     Test net output #2: loss = 0.37651 (* 1 = 0.37651 loss)
I0801 14:09:18.988382 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.874406s
I0801 14:09:19.004165 24549 solver.cpp:450] Finding and applying sparsity: 0.68
I0801 14:09:55.190399 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:09:55.192335 24522 solver.cpp:353] Iteration 37000 (2.57967 iter/s, 38.7647s/100 iter), loss = 0.0028661
I0801 14:09:55.192355 24522 solver.cpp:375]     Train net output #0: loss = 0.00286542 (* 1 = 0.00286542 loss)
I0801 14:09:55.192363 24522 sgd_solver.cpp:136] Iteration 37000, lr = 0.00421875, m = 0.9
I0801 14:09:57.214676 24522 solver.cpp:353] Iteration 37100 (49.4493 iter/s, 2.02227s/100 iter), loss = 0.00875254
I0801 14:09:57.214705 24522 solver.cpp:375]     Train net output #0: loss = 0.00875185 (* 1 = 0.00875185 loss)
I0801 14:09:57.214711 24522 sgd_solver.cpp:136] Iteration 37100, lr = 0.00420313, m = 0.9
I0801 14:09:58.991840 24522 solver.cpp:353] Iteration 37200 (56.2711 iter/s, 1.77711s/100 iter), loss = 0.00189968
I0801 14:09:58.991866 24522 solver.cpp:375]     Train net output #0: loss = 0.001899 (* 1 = 0.001899 loss)
I0801 14:09:58.991873 24522 sgd_solver.cpp:136] Iteration 37200, lr = 0.0041875, m = 0.9
I0801 14:10:00.771112 24522 solver.cpp:353] Iteration 37300 (56.2046 iter/s, 1.77921s/100 iter), loss = 0.000891273
I0801 14:10:00.771138 24522 solver.cpp:375]     Train net output #0: loss = 0.000890591 (* 1 = 0.000890591 loss)
I0801 14:10:00.771142 24522 sgd_solver.cpp:136] Iteration 37300, lr = 0.00417187, m = 0.9
I0801 14:10:02.515305 24522 solver.cpp:353] Iteration 37400 (57.3353 iter/s, 1.74413s/100 iter), loss = 0.00224734
I0801 14:10:02.515349 24522 solver.cpp:375]     Train net output #0: loss = 0.00224666 (* 1 = 0.00224666 loss)
I0801 14:10:02.515360 24522 sgd_solver.cpp:136] Iteration 37400, lr = 0.00415625, m = 0.9
I0801 14:10:04.140736 24522 solver.cpp:353] Iteration 37500 (61.5238 iter/s, 1.62539s/100 iter), loss = 0.000962321
I0801 14:10:04.140764 24522 solver.cpp:375]     Train net output #0: loss = 0.000961639 (* 1 = 0.000961639 loss)
I0801 14:10:04.140771 24522 sgd_solver.cpp:136] Iteration 37500, lr = 0.00414062, m = 0.9
I0801 14:10:05.977324 24522 solver.cpp:353] Iteration 37600 (54.4506 iter/s, 1.83653s/100 iter), loss = 0.00236573
I0801 14:10:05.977378 24522 solver.cpp:375]     Train net output #0: loss = 0.00236505 (* 1 = 0.00236505 loss)
I0801 14:10:05.977391 24522 sgd_solver.cpp:136] Iteration 37600, lr = 0.004125, m = 0.9
I0801 14:10:07.672257 24522 solver.cpp:353] Iteration 37700 (59.0013 iter/s, 1.69488s/100 iter), loss = 0.000909939
I0801 14:10:07.672286 24522 solver.cpp:375]     Train net output #0: loss = 0.00090926 (* 1 = 0.00090926 loss)
I0801 14:10:07.672291 24522 sgd_solver.cpp:136] Iteration 37700, lr = 0.00410937, m = 0.9
I0801 14:10:09.256923 24522 solver.cpp:353] Iteration 37800 (63.1068 iter/s, 1.58462s/100 iter), loss = 0.00248125
I0801 14:10:09.256947 24522 solver.cpp:375]     Train net output #0: loss = 0.00248057 (* 1 = 0.00248057 loss)
I0801 14:10:09.256953 24522 sgd_solver.cpp:136] Iteration 37800, lr = 0.00409375, m = 0.9
I0801 14:10:10.830955 24522 solver.cpp:353] Iteration 37900 (63.5332 iter/s, 1.57398s/100 iter), loss = 0.000365001
I0801 14:10:10.830981 24522 solver.cpp:375]     Train net output #0: loss = 0.000364321 (* 1 = 0.000364321 loss)
I0801 14:10:10.830987 24522 sgd_solver.cpp:136] Iteration 37900, lr = 0.00407812, m = 0.9
I0801 14:10:12.401451 24522 solver.cpp:404] Sparsity after update:
I0801 14:10:12.403046 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:10:12.403055 24522 net.cpp:2270] conv1a_param_0(0.283) 
I0801 14:10:12.403064 24522 net.cpp:2270] conv1b_param_0(0.661) 
I0801 14:10:12.403069 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:10:12.403072 24522 net.cpp:2270] res2a_branch2a_param_0(0.677) 
I0801 14:10:12.403075 24522 net.cpp:2270] res2a_branch2b_param_0(0.644) 
I0801 14:10:12.403079 24522 net.cpp:2270] res3a_branch2a_param_0(0.679) 
I0801 14:10:12.403084 24522 net.cpp:2270] res3a_branch2b_param_0(0.675) 
I0801 14:10:12.403087 24522 net.cpp:2270] res4a_branch2a_param_0(0.68) 
I0801 14:10:12.403091 24522 net.cpp:2270] res4a_branch2b_param_0(0.679) 
I0801 14:10:12.403095 24522 net.cpp:2270] res5a_branch2a_param_0(0.642) 
I0801 14:10:12.403100 24522 net.cpp:2270] res5a_branch2b_param_0(0.678) 
I0801 14:10:12.403105 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.55303e+06/2.3599e+06) 0.658
I0801 14:10:12.403123 24522 solver.cpp:550] Iteration 38000, Testing net (#0)
I0801 14:10:13.209719 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.903236
I0801 14:10:13.209738 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 14:10:13.209744 24522 solver.cpp:635]     Test net output #2: loss = 0.408611 (* 1 = 0.408611 loss)
I0801 14:10:13.209764 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.806612s
I0801 14:10:13.225162 24549 solver.cpp:450] Finding and applying sparsity: 0.7
I0801 14:10:49.397742 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:10:49.400092 24522 solver.cpp:353] Iteration 38000 (2.59282 iter/s, 38.568s/100 iter), loss = 0.00074572
I0801 14:10:49.400135 24522 solver.cpp:375]     Train net output #0: loss = 0.000745041 (* 1 = 0.000745041 loss)
I0801 14:10:49.400151 24522 sgd_solver.cpp:136] Iteration 38000, lr = 0.0040625, m = 0.9
I0801 14:10:51.456254 24522 solver.cpp:353] Iteration 38100 (48.6359 iter/s, 2.05609s/100 iter), loss = 0.00136866
I0801 14:10:51.456324 24522 solver.cpp:375]     Train net output #0: loss = 0.00136798 (* 1 = 0.00136798 loss)
I0801 14:10:51.456343 24522 sgd_solver.cpp:136] Iteration 38100, lr = 0.00404688, m = 0.9
I0801 14:10:53.212067 24522 solver.cpp:353] Iteration 38200 (56.9553 iter/s, 1.75576s/100 iter), loss = 0.00255316
I0801 14:10:53.212118 24522 solver.cpp:375]     Train net output #0: loss = 0.00255249 (* 1 = 0.00255249 loss)
I0801 14:10:53.212131 24522 sgd_solver.cpp:136] Iteration 38200, lr = 0.00403125, m = 0.9
I0801 14:10:54.967011 24522 solver.cpp:353] Iteration 38300 (56.9837 iter/s, 1.75489s/100 iter), loss = 0.00273153
I0801 14:10:54.967062 24522 solver.cpp:375]     Train net output #0: loss = 0.00273085 (* 1 = 0.00273085 loss)
I0801 14:10:54.967075 24522 sgd_solver.cpp:136] Iteration 38300, lr = 0.00401562, m = 0.9
I0801 14:10:56.653669 24522 solver.cpp:353] Iteration 38400 (59.2907 iter/s, 1.6866s/100 iter), loss = 0.000153298
I0801 14:10:56.653694 24522 solver.cpp:375]     Train net output #0: loss = 0.000152624 (* 1 = 0.000152624 loss)
I0801 14:10:56.653699 24522 sgd_solver.cpp:136] Iteration 38400, lr = 0.004, m = 0.9
I0801 14:10:58.332831 24522 solver.cpp:353] Iteration 38500 (59.5553 iter/s, 1.67911s/100 iter), loss = 0.0178774
I0801 14:10:58.332892 24522 solver.cpp:375]     Train net output #0: loss = 0.0178768 (* 1 = 0.0178768 loss)
I0801 14:10:58.332909 24522 sgd_solver.cpp:136] Iteration 38500, lr = 0.00398437, m = 0.9
I0801 14:10:59.938215 24522 solver.cpp:353] Iteration 38600 (62.2924 iter/s, 1.60533s/100 iter), loss = 0.003889
I0801 14:10:59.938268 24522 solver.cpp:375]     Train net output #0: loss = 0.00388832 (* 1 = 0.00388832 loss)
I0801 14:10:59.938282 24522 sgd_solver.cpp:136] Iteration 38600, lr = 0.00396875, m = 0.9
I0801 14:11:01.585222 24522 solver.cpp:353] Iteration 38700 (60.7182 iter/s, 1.64695s/100 iter), loss = 0.00149916
I0801 14:11:01.585250 24522 solver.cpp:375]     Train net output #0: loss = 0.00149849 (* 1 = 0.00149849 loss)
I0801 14:11:01.585256 24522 sgd_solver.cpp:136] Iteration 38700, lr = 0.00395312, m = 0.9
I0801 14:11:03.204519 24522 solver.cpp:353] Iteration 38800 (61.7575 iter/s, 1.61924s/100 iter), loss = 0.000409661
I0801 14:11:03.204568 24522 solver.cpp:375]     Train net output #0: loss = 0.000408985 (* 1 = 0.000408985 loss)
I0801 14:11:03.204579 24522 sgd_solver.cpp:136] Iteration 38800, lr = 0.0039375, m = 0.9
I0801 14:11:04.907641 24522 solver.cpp:353] Iteration 38900 (58.7174 iter/s, 1.70307s/100 iter), loss = 0.00306468
I0801 14:11:04.907666 24522 solver.cpp:375]     Train net output #0: loss = 0.00306401 (* 1 = 0.00306401 loss)
I0801 14:11:04.907671 24522 sgd_solver.cpp:136] Iteration 38900, lr = 0.00392187, m = 0.9
I0801 14:11:06.602929 24522 solver.cpp:404] Sparsity after update:
I0801 14:11:06.605270 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:11:06.605286 24522 net.cpp:2270] conv1a_param_0(0.306) 
I0801 14:11:06.605301 24522 net.cpp:2270] conv1b_param_0(0.684) 
I0801 14:11:06.605312 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:11:06.605322 24522 net.cpp:2270] res2a_branch2a_param_0(0.698) 
I0801 14:11:06.605329 24522 net.cpp:2270] res2a_branch2b_param_0(0.654) 
I0801 14:11:06.605339 24522 net.cpp:2270] res3a_branch2a_param_0(0.7) 
I0801 14:11:06.605348 24522 net.cpp:2270] res3a_branch2b_param_0(0.692) 
I0801 14:11:06.605358 24522 net.cpp:2270] res4a_branch2a_param_0(0.7) 
I0801 14:11:06.605367 24522 net.cpp:2270] res4a_branch2b_param_0(0.7) 
I0801 14:11:06.605376 24522 net.cpp:2270] res5a_branch2a_param_0(0.658) 
I0801 14:11:06.605384 24522 net.cpp:2270] res5a_branch2b_param_0(0.699) 
I0801 14:11:06.605393 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.59683e+06/2.3599e+06) 0.677
I0801 14:11:06.605448 24522 solver.cpp:550] Iteration 39000, Testing net (#0)
I0801 14:11:07.426787 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.902648
I0801 14:11:07.426822 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995588
I0801 14:11:07.426836 24522 solver.cpp:635]     Test net output #2: loss = 0.411482 (* 1 = 0.411482 loss)
I0801 14:11:07.426862 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.821387s
I0801 14:11:07.443611 24549 solver.cpp:450] Finding and applying sparsity: 0.72
I0801 14:11:45.012931 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:11:45.014853 24522 solver.cpp:353] Iteration 39000 (2.49339 iter/s, 40.1061s/100 iter), loss = 0.00184714
I0801 14:11:45.014870 24522 solver.cpp:375]     Train net output #0: loss = 0.00184647 (* 1 = 0.00184647 loss)
I0801 14:11:45.014876 24522 sgd_solver.cpp:136] Iteration 39000, lr = 0.00390625, m = 0.9
I0801 14:11:46.896061 24522 solver.cpp:353] Iteration 39100 (53.159 iter/s, 1.88115s/100 iter), loss = 0.000735196
I0801 14:11:46.896086 24522 solver.cpp:375]     Train net output #0: loss = 0.00073452 (* 1 = 0.00073452 loss)
I0801 14:11:46.896091 24522 sgd_solver.cpp:136] Iteration 39100, lr = 0.00389063, m = 0.9
I0801 14:11:48.571569 24522 solver.cpp:353] Iteration 39200 (59.6855 iter/s, 1.67545s/100 iter), loss = 0.00034909
I0801 14:11:48.571642 24522 solver.cpp:375]     Train net output #0: loss = 0.000348415 (* 1 = 0.000348415 loss)
I0801 14:11:48.571661 24522 sgd_solver.cpp:136] Iteration 39200, lr = 0.003875, m = 0.9
I0801 14:11:50.240010 24522 solver.cpp:353] Iteration 39300 (59.9381 iter/s, 1.66839s/100 iter), loss = 0.0030348
I0801 14:11:50.240032 24522 solver.cpp:375]     Train net output #0: loss = 0.00303413 (* 1 = 0.00303413 loss)
I0801 14:11:50.240037 24522 sgd_solver.cpp:136] Iteration 39300, lr = 0.00385938, m = 0.9
I0801 14:11:51.764344 24487 data_reader.cpp:264] Starting prefetch of epoch 5
I0801 14:11:51.928975 24522 solver.cpp:353] Iteration 39400 (59.2099 iter/s, 1.68891s/100 iter), loss = 0.0163882
I0801 14:11:51.929021 24522 solver.cpp:375]     Train net output #0: loss = 0.0163875 (* 1 = 0.0163875 loss)
I0801 14:11:51.929031 24522 sgd_solver.cpp:136] Iteration 39400, lr = 0.00384375, m = 0.9
I0801 14:11:53.525588 24522 solver.cpp:353] Iteration 39500 (62.6347 iter/s, 1.59656s/100 iter), loss = 0.00590614
I0801 14:11:53.525627 24522 solver.cpp:375]     Train net output #0: loss = 0.00590547 (* 1 = 0.00590547 loss)
I0801 14:11:53.525638 24522 sgd_solver.cpp:136] Iteration 39500, lr = 0.00382812, m = 0.9
I0801 14:11:55.149586 24522 solver.cpp:353] Iteration 39600 (61.5784 iter/s, 1.62395s/100 iter), loss = 0.00156271
I0801 14:11:55.149611 24522 solver.cpp:375]     Train net output #0: loss = 0.00156204 (* 1 = 0.00156204 loss)
I0801 14:11:55.149617 24522 sgd_solver.cpp:136] Iteration 39600, lr = 0.0038125, m = 0.9
I0801 14:11:56.832955 24522 solver.cpp:353] Iteration 39700 (59.4065 iter/s, 1.68332s/100 iter), loss = 0.00151379
I0801 14:11:56.832981 24522 solver.cpp:375]     Train net output #0: loss = 0.00151312 (* 1 = 0.00151312 loss)
I0801 14:11:56.832988 24522 sgd_solver.cpp:136] Iteration 39700, lr = 0.00379687, m = 0.9
I0801 14:11:58.533768 24522 solver.cpp:353] Iteration 39800 (58.7984 iter/s, 1.70073s/100 iter), loss = 0.00136631
I0801 14:11:58.533939 24522 solver.cpp:375]     Train net output #0: loss = 0.00136564 (* 1 = 0.00136564 loss)
I0801 14:11:58.533975 24522 sgd_solver.cpp:136] Iteration 39800, lr = 0.00378125, m = 0.9
I0801 14:12:00.318666 24522 solver.cpp:353] Iteration 39900 (56.0268 iter/s, 1.78486s/100 iter), loss = 0.00534749
I0801 14:12:00.318691 24522 solver.cpp:375]     Train net output #0: loss = 0.00534681 (* 1 = 0.00534681 loss)
I0801 14:12:00.318696 24522 sgd_solver.cpp:136] Iteration 39900, lr = 0.00376562, m = 0.9
I0801 14:12:02.071998 24522 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_40000.caffemodel
I0801 14:12:02.080021 24522 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_40000.solverstate
I0801 14:12:02.083555 24522 solver.cpp:404] Sparsity after update:
I0801 14:12:02.085233 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:12:02.085245 24522 net.cpp:2270] conv1a_param_0(0.294) 
I0801 14:12:02.085259 24522 net.cpp:2270] conv1b_param_0(0.696) 
I0801 14:12:02.085263 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:12:02.085268 24522 net.cpp:2270] res2a_branch2a_param_0(0.719) 
I0801 14:12:02.085273 24522 net.cpp:2270] res2a_branch2b_param_0(0.663) 
I0801 14:12:02.085295 24522 net.cpp:2270] res3a_branch2a_param_0(0.719) 
I0801 14:12:02.085301 24522 net.cpp:2270] res3a_branch2b_param_0(0.708) 
I0801 14:12:02.085305 24522 net.cpp:2270] res4a_branch2a_param_0(0.72) 
I0801 14:12:02.085310 24522 net.cpp:2270] res4a_branch2b_param_0(0.719) 
I0801 14:12:02.085314 24522 net.cpp:2270] res5a_branch2a_param_0(0.681) 
I0801 14:12:02.085319 24522 net.cpp:2270] res5a_branch2b_param_0(0.719) 
I0801 14:12:02.085324 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.64632e+06/2.3599e+06) 0.698
I0801 14:12:02.085336 24522 solver.cpp:550] Iteration 40000, Testing net (#0)
I0801 14:12:02.883498 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.906766
I0801 14:12:02.883517 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 14:12:02.883523 24522 solver.cpp:635]     Test net output #2: loss = 0.386997 (* 1 = 0.386997 loss)
I0801 14:12:02.883541 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.798176s
I0801 14:12:02.899124 24549 solver.cpp:450] Finding and applying sparsity: 0.74
I0801 14:12:37.964088 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:12:37.966007 24522 solver.cpp:353] Iteration 40000 (2.65631 iter/s, 37.6463s/100 iter), loss = 0.000424887
I0801 14:12:37.966027 24522 solver.cpp:375]     Train net output #0: loss = 0.000424212 (* 1 = 0.000424212 loss)
I0801 14:12:37.966033 24522 sgd_solver.cpp:136] Iteration 40000, lr = 0.00375, m = 0.9
I0801 14:12:39.869068 24522 solver.cpp:353] Iteration 40100 (52.5487 iter/s, 1.903s/100 iter), loss = 0.00453139
I0801 14:12:39.869097 24522 solver.cpp:375]     Train net output #0: loss = 0.00453071 (* 1 = 0.00453071 loss)
I0801 14:12:39.869103 24522 sgd_solver.cpp:136] Iteration 40100, lr = 0.00373438, m = 0.9
I0801 14:12:41.444445 24522 solver.cpp:353] Iteration 40200 (63.4789 iter/s, 1.57533s/100 iter), loss = 0.000411164
I0801 14:12:41.444500 24522 solver.cpp:375]     Train net output #0: loss = 0.000410478 (* 1 = 0.000410478 loss)
I0801 14:12:41.444524 24522 sgd_solver.cpp:136] Iteration 40200, lr = 0.00371875, m = 0.9
I0801 14:12:43.041960 24522 solver.cpp:353] Iteration 40300 (62.5992 iter/s, 1.59746s/100 iter), loss = 0.00524889
I0801 14:12:43.041985 24522 solver.cpp:375]     Train net output #0: loss = 0.00524821 (* 1 = 0.00524821 loss)
I0801 14:12:43.041991 24522 sgd_solver.cpp:136] Iteration 40300, lr = 0.00370313, m = 0.9
I0801 14:12:44.661520 24522 solver.cpp:353] Iteration 40400 (61.747 iter/s, 1.61951s/100 iter), loss = 0.00711263
I0801 14:12:44.661543 24522 solver.cpp:375]     Train net output #0: loss = 0.00711195 (* 1 = 0.00711195 loss)
I0801 14:12:44.661546 24522 sgd_solver.cpp:136] Iteration 40400, lr = 0.0036875, m = 0.9
I0801 14:12:46.243746 24522 solver.cpp:353] Iteration 40500 (63.2043 iter/s, 1.58217s/100 iter), loss = 0.00528817
I0801 14:12:46.243769 24522 solver.cpp:375]     Train net output #0: loss = 0.00528749 (* 1 = 0.00528749 loss)
I0801 14:12:46.243773 24522 sgd_solver.cpp:136] Iteration 40500, lr = 0.00367187, m = 0.9
I0801 14:12:47.813280 24522 solver.cpp:353] Iteration 40600 (63.7152 iter/s, 1.56948s/100 iter), loss = 0.00175298
I0801 14:12:47.813305 24522 solver.cpp:375]     Train net output #0: loss = 0.00175231 (* 1 = 0.00175231 loss)
I0801 14:12:47.813311 24522 sgd_solver.cpp:136] Iteration 40600, lr = 0.00365625, m = 0.9
I0801 14:12:49.391880 24522 solver.cpp:353] Iteration 40700 (63.3493 iter/s, 1.57855s/100 iter), loss = 0.00247242
I0801 14:12:49.391908 24522 solver.cpp:375]     Train net output #0: loss = 0.00247174 (* 1 = 0.00247174 loss)
I0801 14:12:49.391914 24522 sgd_solver.cpp:136] Iteration 40700, lr = 0.00364062, m = 0.9
I0801 14:12:50.970894 24522 solver.cpp:353] Iteration 40800 (63.3327 iter/s, 1.57896s/100 iter), loss = 0.00171148
I0801 14:12:50.970943 24522 solver.cpp:375]     Train net output #0: loss = 0.00171081 (* 1 = 0.00171081 loss)
I0801 14:12:50.970957 24522 sgd_solver.cpp:136] Iteration 40800, lr = 0.003625, m = 0.9
I0801 14:12:52.558768 24522 solver.cpp:353] Iteration 40900 (62.9795 iter/s, 1.58782s/100 iter), loss = 0.00655072
I0801 14:12:52.558790 24522 solver.cpp:375]     Train net output #0: loss = 0.00655005 (* 1 = 0.00655005 loss)
I0801 14:12:52.558794 24522 sgd_solver.cpp:136] Iteration 40900, lr = 0.00360937, m = 0.9
I0801 14:12:54.118893 24522 solver.cpp:404] Sparsity after update:
I0801 14:12:54.120523 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:12:54.120532 24522 net.cpp:2270] conv1a_param_0(0.317) 
I0801 14:12:54.120537 24522 net.cpp:2270] conv1b_param_0(0.716) 
I0801 14:12:54.120539 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:12:54.120542 24522 net.cpp:2270] res2a_branch2a_param_0(0.739) 
I0801 14:12:54.120543 24522 net.cpp:2270] res2a_branch2b_param_0(0.671) 
I0801 14:12:54.120545 24522 net.cpp:2270] res3a_branch2a_param_0(0.739) 
I0801 14:12:54.120548 24522 net.cpp:2270] res3a_branch2b_param_0(0.722) 
I0801 14:12:54.120549 24522 net.cpp:2270] res4a_branch2a_param_0(0.74) 
I0801 14:12:54.120551 24522 net.cpp:2270] res4a_branch2b_param_0(0.739) 
I0801 14:12:54.120553 24522 net.cpp:2270] res5a_branch2a_param_0(0.696) 
I0801 14:12:54.120555 24522 net.cpp:2270] res5a_branch2b_param_0(0.739) 
I0801 14:12:54.120558 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.68711e+06/2.3599e+06) 0.715
I0801 14:12:54.120579 24522 solver.cpp:550] Iteration 41000, Testing net (#0)
I0801 14:12:54.935829 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.907648
I0801 14:12:54.935849 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 14:12:54.935854 24522 solver.cpp:635]     Test net output #2: loss = 0.398976 (* 1 = 0.398976 loss)
I0801 14:12:54.935868 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.815261s
I0801 14:12:54.951495 24549 solver.cpp:450] Finding and applying sparsity: 0.76
I0801 14:13:31.081903 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:13:31.083894 24522 solver.cpp:353] Iteration 41000 (2.59578 iter/s, 38.524s/100 iter), loss = 0.00940124
I0801 14:13:31.083912 24522 solver.cpp:375]     Train net output #0: loss = 0.00940057 (* 1 = 0.00940057 loss)
I0801 14:13:31.083920 24522 sgd_solver.cpp:136] Iteration 41000, lr = 0.00359375, m = 0.9
I0801 14:13:32.926358 24522 solver.cpp:353] Iteration 41100 (54.2769 iter/s, 1.8424s/100 iter), loss = 0.00087284
I0801 14:13:32.926384 24522 solver.cpp:375]     Train net output #0: loss = 0.000872173 (* 1 = 0.000872173 loss)
I0801 14:13:32.926388 24522 sgd_solver.cpp:136] Iteration 41100, lr = 0.00357813, m = 0.9
I0801 14:13:34.503262 24522 solver.cpp:353] Iteration 41200 (63.4174 iter/s, 1.57685s/100 iter), loss = 0.00157554
I0801 14:13:34.503314 24522 solver.cpp:375]     Train net output #0: loss = 0.00157488 (* 1 = 0.00157488 loss)
I0801 14:13:34.503326 24522 sgd_solver.cpp:136] Iteration 41200, lr = 0.0035625, m = 0.9
I0801 14:13:36.103199 24522 solver.cpp:353] Iteration 41300 (62.5045 iter/s, 1.59988s/100 iter), loss = 0.00709238
I0801 14:13:36.103288 24522 solver.cpp:375]     Train net output #0: loss = 0.00709171 (* 1 = 0.00709171 loss)
I0801 14:13:36.103296 24522 sgd_solver.cpp:136] Iteration 41300, lr = 0.00354687, m = 0.9
I0801 14:13:37.713596 24522 solver.cpp:353] Iteration 41400 (62.0985 iter/s, 1.61035s/100 iter), loss = 0.00969981
I0801 14:13:37.713623 24522 solver.cpp:375]     Train net output #0: loss = 0.00969914 (* 1 = 0.00969914 loss)
I0801 14:13:37.713629 24522 sgd_solver.cpp:136] Iteration 41400, lr = 0.00353125, m = 0.9
I0801 14:13:39.285046 24522 solver.cpp:353] Iteration 41500 (63.6376 iter/s, 1.5714s/100 iter), loss = 0.00412774
I0801 14:13:39.285071 24522 solver.cpp:375]     Train net output #0: loss = 0.00412706 (* 1 = 0.00412706 loss)
I0801 14:13:39.285075 24522 sgd_solver.cpp:136] Iteration 41500, lr = 0.00351562, m = 0.9
I0801 14:13:40.877753 24522 solver.cpp:353] Iteration 41600 (62.7883 iter/s, 1.59265s/100 iter), loss = 0.0164018
I0801 14:13:40.877776 24522 solver.cpp:375]     Train net output #0: loss = 0.0164011 (* 1 = 0.0164011 loss)
I0801 14:13:40.877782 24522 sgd_solver.cpp:136] Iteration 41600, lr = 0.0035, m = 0.9
I0801 14:13:42.461941 24522 solver.cpp:353] Iteration 41700 (63.1259 iter/s, 1.58414s/100 iter), loss = 0.0111854
I0801 14:13:42.461992 24522 solver.cpp:375]     Train net output #0: loss = 0.0111847 (* 1 = 0.0111847 loss)
I0801 14:13:42.462005 24522 sgd_solver.cpp:136] Iteration 41700, lr = 0.00348437, m = 0.9
I0801 14:13:44.061113 24522 solver.cpp:353] Iteration 41800 (62.5342 iter/s, 1.59912s/100 iter), loss = 0.00165816
I0801 14:13:44.061187 24522 solver.cpp:375]     Train net output #0: loss = 0.00165749 (* 1 = 0.00165749 loss)
I0801 14:13:44.061214 24522 sgd_solver.cpp:136] Iteration 41800, lr = 0.00346875, m = 0.9
I0801 14:13:45.639443 24522 solver.cpp:353] Iteration 41900 (63.3602 iter/s, 1.57828s/100 iter), loss = 0.0020735
I0801 14:13:45.639469 24522 solver.cpp:375]     Train net output #0: loss = 0.00207283 (* 1 = 0.00207283 loss)
I0801 14:13:45.639474 24522 sgd_solver.cpp:136] Iteration 41900, lr = 0.00345312, m = 0.9
I0801 14:13:47.200484 24522 solver.cpp:404] Sparsity after update:
I0801 14:13:47.202095 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:13:47.202105 24522 net.cpp:2270] conv1a_param_0(0.317) 
I0801 14:13:47.202111 24522 net.cpp:2270] conv1b_param_0(0.727) 
I0801 14:13:47.202112 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:13:47.202116 24522 net.cpp:2270] res2a_branch2a_param_0(0.756) 
I0801 14:13:47.202119 24522 net.cpp:2270] res2a_branch2b_param_0(0.678) 
I0801 14:13:47.202124 24522 net.cpp:2270] res3a_branch2a_param_0(0.758) 
I0801 14:13:47.202128 24522 net.cpp:2270] res3a_branch2b_param_0(0.732) 
I0801 14:13:47.202131 24522 net.cpp:2270] res4a_branch2a_param_0(0.76) 
I0801 14:13:47.202133 24522 net.cpp:2270] res4a_branch2b_param_0(0.758) 
I0801 14:13:47.202137 24522 net.cpp:2270] res5a_branch2a_param_0(0.721) 
I0801 14:13:47.202141 24522 net.cpp:2270] res5a_branch2b_param_0(0.759) 
I0801 14:13:47.202142 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.73967e+06/2.3599e+06) 0.737
I0801 14:13:47.202172 24522 solver.cpp:550] Iteration 42000, Testing net (#0)
I0801 14:13:48.009711 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.911177
I0801 14:13:48.009728 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 14:13:48.009733 24522 solver.cpp:635]     Test net output #2: loss = 0.363961 (* 1 = 0.363961 loss)
I0801 14:13:48.009747 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.807547s
I0801 14:13:48.025243 24549 solver.cpp:450] Finding and applying sparsity: 0.78
I0801 14:14:26.526052 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:14:26.527971 24522 solver.cpp:353] Iteration 42000 (2.44574 iter/s, 40.8874s/100 iter), loss = 0.010395
I0801 14:14:26.527994 24522 solver.cpp:375]     Train net output #0: loss = 0.0103944 (* 1 = 0.0103944 loss)
I0801 14:14:26.528004 24522 sgd_solver.cpp:136] Iteration 42000, lr = 0.0034375, m = 0.9
I0801 14:14:28.376772 24522 solver.cpp:353] Iteration 42100 (54.0909 iter/s, 1.84874s/100 iter), loss = 0.00208231
I0801 14:14:28.376798 24522 solver.cpp:375]     Train net output #0: loss = 0.00208164 (* 1 = 0.00208164 loss)
I0801 14:14:28.376802 24522 sgd_solver.cpp:136] Iteration 42100, lr = 0.00342188, m = 0.9
I0801 14:14:29.963157 24522 solver.cpp:353] Iteration 42200 (63.0384 iter/s, 1.58633s/100 iter), loss = 0.00147983
I0801 14:14:29.963182 24522 solver.cpp:375]     Train net output #0: loss = 0.00147916 (* 1 = 0.00147916 loss)
I0801 14:14:29.963188 24522 sgd_solver.cpp:136] Iteration 42200, lr = 0.00340625, m = 0.9
I0801 14:14:31.533463 24522 solver.cpp:353] Iteration 42300 (63.6839 iter/s, 1.57026s/100 iter), loss = 0.191607
I0801 14:14:31.533488 24522 solver.cpp:375]     Train net output #0: loss = 0.191607 (* 1 = 0.191607 loss)
I0801 14:14:31.533493 24522 sgd_solver.cpp:136] Iteration 42300, lr = 0.00339063, m = 0.9
I0801 14:14:33.124956 24522 solver.cpp:353] Iteration 42400 (62.836 iter/s, 1.59145s/100 iter), loss = 0.00274655
I0801 14:14:33.124984 24522 solver.cpp:375]     Train net output #0: loss = 0.00274586 (* 1 = 0.00274586 loss)
I0801 14:14:33.124989 24522 sgd_solver.cpp:136] Iteration 42400, lr = 0.003375, m = 0.9
I0801 14:14:34.715101 24522 solver.cpp:353] Iteration 42500 (62.8895 iter/s, 1.59009s/100 iter), loss = 0.00266444
I0801 14:14:34.715124 24522 solver.cpp:375]     Train net output #0: loss = 0.00266375 (* 1 = 0.00266375 loss)
I0801 14:14:34.715131 24522 sgd_solver.cpp:136] Iteration 42500, lr = 0.00335937, m = 0.9
I0801 14:14:36.298630 24522 solver.cpp:353] Iteration 42600 (63.152 iter/s, 1.58348s/100 iter), loss = 0.00141241
I0801 14:14:36.298655 24522 solver.cpp:375]     Train net output #0: loss = 0.00141173 (* 1 = 0.00141173 loss)
I0801 14:14:36.298658 24522 sgd_solver.cpp:136] Iteration 42600, lr = 0.00334375, m = 0.9
I0801 14:14:37.886467 24522 solver.cpp:353] Iteration 42700 (62.9808 iter/s, 1.58778s/100 iter), loss = 0.00380942
I0801 14:14:37.886492 24522 solver.cpp:375]     Train net output #0: loss = 0.00380874 (* 1 = 0.00380874 loss)
I0801 14:14:37.886497 24522 sgd_solver.cpp:136] Iteration 42700, lr = 0.00332812, m = 0.9
I0801 14:14:39.458547 24522 solver.cpp:353] Iteration 42800 (63.612 iter/s, 1.57203s/100 iter), loss = 0.0081753
I0801 14:14:39.458572 24522 solver.cpp:375]     Train net output #0: loss = 0.00817463 (* 1 = 0.00817463 loss)
I0801 14:14:39.458578 24522 sgd_solver.cpp:136] Iteration 42800, lr = 0.0033125, m = 0.9
I0801 14:14:41.035668 24522 solver.cpp:353] Iteration 42900 (63.4088 iter/s, 1.57707s/100 iter), loss = 0.0278557
I0801 14:14:41.035693 24522 solver.cpp:375]     Train net output #0: loss = 0.027855 (* 1 = 0.027855 loss)
I0801 14:14:41.035699 24522 sgd_solver.cpp:136] Iteration 42900, lr = 0.00329687, m = 0.9
I0801 14:14:42.603211 24522 solver.cpp:404] Sparsity after update:
I0801 14:14:42.604795 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:14:42.604804 24522 net.cpp:2270] conv1a_param_0(0.328) 
I0801 14:14:42.604830 24522 net.cpp:2270] conv1b_param_0(0.742) 
I0801 14:14:42.604835 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:14:42.604838 24522 net.cpp:2270] res2a_branch2a_param_0(0.776) 
I0801 14:14:42.604842 24522 net.cpp:2270] res2a_branch2b_param_0(0.683) 
I0801 14:14:42.604846 24522 net.cpp:2270] res3a_branch2a_param_0(0.778) 
I0801 14:14:42.604849 24522 net.cpp:2270] res3a_branch2b_param_0(0.741) 
I0801 14:14:42.604852 24522 net.cpp:2270] res4a_branch2a_param_0(0.779) 
I0801 14:14:42.604856 24522 net.cpp:2270] res4a_branch2b_param_0(0.778) 
I0801 14:14:42.604859 24522 net.cpp:2270] res5a_branch2a_param_0(0.742) 
I0801 14:14:42.604863 24522 net.cpp:2270] res5a_branch2b_param_0(0.779) 
I0801 14:14:42.604866 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.788e+06/2.3599e+06) 0.758
I0801 14:14:42.604887 24522 solver.cpp:550] Iteration 43000, Testing net (#0)
I0801 14:14:43.409391 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.907648
I0801 14:14:43.409409 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 14:14:43.409415 24522 solver.cpp:635]     Test net output #2: loss = 0.400647 (* 1 = 0.400647 loss)
I0801 14:14:43.409430 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.804516s
I0801 14:14:43.424891 24549 solver.cpp:450] Finding and applying sparsity: 0.8
I0801 14:15:28.291119 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:15:28.293056 24522 solver.cpp:353] Iteration 43000 (2.11613 iter/s, 47.2561s/100 iter), loss = 0.00154071
I0801 14:15:28.293076 24522 solver.cpp:375]     Train net output #0: loss = 0.00154003 (* 1 = 0.00154003 loss)
I0801 14:15:28.293082 24522 sgd_solver.cpp:136] Iteration 43000, lr = 0.00328125, m = 0.9
I0801 14:15:30.262547 24522 solver.cpp:353] Iteration 43100 (50.7762 iter/s, 1.96943s/100 iter), loss = 0.000349341
I0801 14:15:30.262573 24522 solver.cpp:375]     Train net output #0: loss = 0.000348662 (* 1 = 0.000348662 loss)
I0801 14:15:30.262579 24522 sgd_solver.cpp:136] Iteration 43100, lr = 0.00326563, m = 0.9
I0801 14:15:31.959344 24522 solver.cpp:353] Iteration 43200 (58.9365 iter/s, 1.69674s/100 iter), loss = 0.00637403
I0801 14:15:31.959378 24522 solver.cpp:375]     Train net output #0: loss = 0.00637335 (* 1 = 0.00637335 loss)
I0801 14:15:31.959385 24522 sgd_solver.cpp:136] Iteration 43200, lr = 0.00325, m = 0.9
I0801 14:15:33.587316 24522 solver.cpp:353] Iteration 43300 (61.4285 iter/s, 1.62791s/100 iter), loss = 0.00871517
I0801 14:15:33.587362 24522 solver.cpp:375]     Train net output #0: loss = 0.00871448 (* 1 = 0.00871448 loss)
I0801 14:15:33.587373 24522 sgd_solver.cpp:136] Iteration 43300, lr = 0.00323438, m = 0.9
I0801 14:15:35.245312 24522 solver.cpp:353] Iteration 43400 (60.3155 iter/s, 1.65795s/100 iter), loss = 0.00761329
I0801 14:15:35.245337 24522 solver.cpp:375]     Train net output #0: loss = 0.00761259 (* 1 = 0.00761259 loss)
I0801 14:15:35.245343 24522 sgd_solver.cpp:136] Iteration 43400, lr = 0.00321875, m = 0.9
I0801 14:15:36.926390 24522 solver.cpp:353] Iteration 43500 (59.4876 iter/s, 1.68102s/100 iter), loss = 0.00296994
I0801 14:15:36.926417 24522 solver.cpp:375]     Train net output #0: loss = 0.00296925 (* 1 = 0.00296925 loss)
I0801 14:15:36.926422 24522 sgd_solver.cpp:136] Iteration 43500, lr = 0.00320312, m = 0.9
I0801 14:15:38.513463 24522 solver.cpp:353] Iteration 43600 (63.0111 iter/s, 1.58702s/100 iter), loss = 0.0096679
I0801 14:15:38.513494 24522 solver.cpp:375]     Train net output #0: loss = 0.00966721 (* 1 = 0.00966721 loss)
I0801 14:15:38.513500 24522 sgd_solver.cpp:136] Iteration 43600, lr = 0.0031875, m = 0.9
I0801 14:15:40.127918 24522 solver.cpp:353] Iteration 43700 (61.9423 iter/s, 1.6144s/100 iter), loss = 0.0034968
I0801 14:15:40.127944 24522 solver.cpp:375]     Train net output #0: loss = 0.00349611 (* 1 = 0.00349611 loss)
I0801 14:15:40.127950 24522 sgd_solver.cpp:136] Iteration 43700, lr = 0.00317187, m = 0.9
I0801 14:15:41.826576 24522 solver.cpp:353] Iteration 43800 (58.8734 iter/s, 1.69856s/100 iter), loss = 0.000262671
I0801 14:15:41.827525 24522 solver.cpp:375]     Train net output #0: loss = 0.000261987 (* 1 = 0.000261987 loss)
I0801 14:15:41.827685 24522 sgd_solver.cpp:136] Iteration 43800, lr = 0.00315625, m = 0.9
I0801 14:15:43.438757 24522 solver.cpp:353] Iteration 43900 (62.0291 iter/s, 1.61215s/100 iter), loss = 0.00158668
I0801 14:15:43.438782 24522 solver.cpp:375]     Train net output #0: loss = 0.00158598 (* 1 = 0.00158598 loss)
I0801 14:15:43.438788 24522 sgd_solver.cpp:136] Iteration 43900, lr = 0.00314062, m = 0.9
I0801 14:15:44.056126 24487 data_reader.cpp:264] Starting prefetch of epoch 6
I0801 14:15:45.089807 24522 solver.cpp:404] Sparsity after update:
I0801 14:15:45.091423 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:15:45.091431 24522 net.cpp:2270] conv1a_param_0(0.329) 
I0801 14:15:45.091440 24522 net.cpp:2270] conv1b_param_0(0.748) 
I0801 14:15:45.091445 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:15:45.091449 24522 net.cpp:2270] res2a_branch2a_param_0(0.794) 
I0801 14:15:45.091455 24522 net.cpp:2270] res2a_branch2b_param_0(0.689) 
I0801 14:15:45.091461 24522 net.cpp:2270] res3a_branch2a_param_0(0.795) 
I0801 14:15:45.091467 24522 net.cpp:2270] res3a_branch2b_param_0(0.749) 
I0801 14:15:45.091472 24522 net.cpp:2270] res4a_branch2a_param_0(0.799) 
I0801 14:15:45.091477 24522 net.cpp:2270] res4a_branch2b_param_0(0.796) 
I0801 14:15:45.091482 24522 net.cpp:2270] res5a_branch2a_param_0(0.759) 
I0801 14:15:45.091485 24522 net.cpp:2270] res5a_branch2b_param_0(0.799) 
I0801 14:15:45.091503 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.83015e+06/2.3599e+06) 0.776
I0801 14:15:45.091516 24522 solver.cpp:550] Iteration 44000, Testing net (#0)
I0801 14:15:45.957854 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.906766
I0801 14:15:45.957890 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 14:15:45.957909 24522 solver.cpp:635]     Test net output #2: loss = 0.379241 (* 1 = 0.379241 loss)
I0801 14:15:45.957939 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.86639s
I0801 14:15:45.981411 24549 solver.cpp:450] Finding and applying sparsity: 0.82
I0801 14:16:35.432785 24549 net.cpp:2244] All zero weights of convolution layers are frozen
I0801 14:16:35.434698 24522 solver.cpp:353] Iteration 44000 (1.92328 iter/s, 51.9945s/100 iter), loss = 0.00441611
I0801 14:16:35.434716 24522 solver.cpp:375]     Train net output #0: loss = 0.00441542 (* 1 = 0.00441542 loss)
I0801 14:16:35.434722 24522 sgd_solver.cpp:136] Iteration 44000, lr = 0.003125, m = 0.9
I0801 14:16:37.317042 24522 solver.cpp:353] Iteration 44100 (53.127 iter/s, 1.88228s/100 iter), loss = 0.00303087
I0801 14:16:37.317068 24522 solver.cpp:375]     Train net output #0: loss = 0.00303017 (* 1 = 0.00303017 loss)
I0801 14:16:37.317075 24522 sgd_solver.cpp:136] Iteration 44100, lr = 0.00310938, m = 0.9
I0801 14:16:39.009191 24522 solver.cpp:353] Iteration 44200 (59.0986 iter/s, 1.69209s/100 iter), loss = 0.013145
I0801 14:16:39.009269 24522 solver.cpp:375]     Train net output #0: loss = 0.0131443 (* 1 = 0.0131443 loss)
I0801 14:16:39.009296 24522 sgd_solver.cpp:136] Iteration 44200, lr = 0.00309375, m = 0.9
I0801 14:16:40.624222 24522 solver.cpp:353] Iteration 44300 (61.9202 iter/s, 1.61498s/100 iter), loss = 0.0693668
I0801 14:16:40.624244 24522 solver.cpp:375]     Train net output #0: loss = 0.0693661 (* 1 = 0.0693661 loss)
I0801 14:16:40.624249 24522 sgd_solver.cpp:136] Iteration 44300, lr = 0.00307812, m = 0.9
I0801 14:16:42.339866 24522 solver.cpp:353] Iteration 44400 (58.2891 iter/s, 1.71559s/100 iter), loss = 0.00209093
I0801 14:16:42.339895 24522 solver.cpp:375]     Train net output #0: loss = 0.00209024 (* 1 = 0.00209024 loss)
I0801 14:16:42.339900 24522 sgd_solver.cpp:136] Iteration 44400, lr = 0.0030625, m = 0.9
I0801 14:16:43.984433 24522 solver.cpp:353] Iteration 44500 (60.8084 iter/s, 1.64451s/100 iter), loss = 0.0071657
I0801 14:16:43.984508 24522 solver.cpp:375]     Train net output #0: loss = 0.007165 (* 1 = 0.007165 loss)
I0801 14:16:43.984530 24522 sgd_solver.cpp:136] Iteration 44500, lr = 0.00304687, m = 0.9
I0801 14:16:45.578915 24522 solver.cpp:353] Iteration 44600 (62.7183 iter/s, 1.59443s/100 iter), loss = 0.0170949
I0801 14:16:45.578969 24522 solver.cpp:375]     Train net output #0: loss = 0.0170942 (* 1 = 0.0170942 loss)
I0801 14:16:45.578979 24522 sgd_solver.cpp:136] Iteration 44600, lr = 0.00303125, m = 0.9
I0801 14:16:47.231216 24522 solver.cpp:353] Iteration 44700 (60.5235 iter/s, 1.65225s/100 iter), loss = 0.00102719
I0801 14:16:47.231242 24522 solver.cpp:375]     Train net output #0: loss = 0.0010265 (* 1 = 0.0010265 loss)
I0801 14:16:47.231248 24522 sgd_solver.cpp:136] Iteration 44700, lr = 0.00301562, m = 0.9
I0801 14:16:48.884407 24522 solver.cpp:353] Iteration 44800 (60.4911 iter/s, 1.65313s/100 iter), loss = 0.00328837
I0801 14:16:48.884459 24522 solver.cpp:375]     Train net output #0: loss = 0.00328768 (* 1 = 0.00328768 loss)
I0801 14:16:48.884475 24522 sgd_solver.cpp:136] Iteration 44800, lr = 0.003, m = 0.9
I0801 14:16:50.607966 24522 solver.cpp:353] Iteration 44900 (58.0213 iter/s, 1.72351s/100 iter), loss = 0.0102283
I0801 14:16:50.607990 24522 solver.cpp:375]     Train net output #0: loss = 0.0102276 (* 1 = 0.0102276 loss)
I0801 14:16:50.607996 24522 sgd_solver.cpp:136] Iteration 44900, lr = 0.00298437, m = 0.9
I0801 14:16:52.250116 24522 solver.cpp:404] Sparsity after update:
I0801 14:16:52.252400 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:16:52.252418 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:16:52.252450 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:16:52.252470 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:16:52.252487 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:16:52.252503 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:16:52.252518 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:16:52.252535 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:16:52.252550 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:16:52.252565 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:16:52.252581 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:16:52.252599 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:16:52.252615 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:16:52.252671 24522 solver.cpp:550] Iteration 45000, Testing net (#0)
I0801 14:16:53.111562 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.890295
I0801 14:16:53.111582 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 14:16:53.111588 24522 solver.cpp:635]     Test net output #2: loss = 0.460744 (* 1 = 0.460744 loss)
I0801 14:16:53.111604 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.858904s
I0801 14:16:53.127462 24522 solver.cpp:353] Iteration 45000 (39.6916 iter/s, 2.51942s/100 iter), loss = 0.00216663
I0801 14:16:53.127480 24522 solver.cpp:375]     Train net output #0: loss = 0.00216594 (* 1 = 0.00216594 loss)
I0801 14:16:53.127483 24522 sgd_solver.cpp:136] Iteration 45000, lr = 0.00296875, m = 0.9
I0801 14:16:54.834941 24522 solver.cpp:353] Iteration 45100 (58.5678 iter/s, 1.70742s/100 iter), loss = 0.00130178
I0801 14:16:54.834969 24522 solver.cpp:375]     Train net output #0: loss = 0.00130109 (* 1 = 0.00130109 loss)
I0801 14:16:54.834975 24522 sgd_solver.cpp:136] Iteration 45100, lr = 0.00295313, m = 0.9
I0801 14:16:56.452070 24522 solver.cpp:353] Iteration 45200 (61.8399 iter/s, 1.61708s/100 iter), loss = 0.0192443
I0801 14:16:56.452096 24522 solver.cpp:375]     Train net output #0: loss = 0.0192436 (* 1 = 0.0192436 loss)
I0801 14:16:56.452102 24522 sgd_solver.cpp:136] Iteration 45200, lr = 0.0029375, m = 0.9
I0801 14:16:58.082116 24522 solver.cpp:353] Iteration 45300 (61.3499 iter/s, 1.62999s/100 iter), loss = 0.000816064
I0801 14:16:58.082147 24522 solver.cpp:375]     Train net output #0: loss = 0.000815364 (* 1 = 0.000815364 loss)
I0801 14:16:58.082154 24522 sgd_solver.cpp:136] Iteration 45300, lr = 0.00292188, m = 0.9
I0801 14:16:59.719563 24522 solver.cpp:353] Iteration 45400 (61.0727 iter/s, 1.63739s/100 iter), loss = 0.0135891
I0801 14:16:59.719594 24522 solver.cpp:375]     Train net output #0: loss = 0.0135884 (* 1 = 0.0135884 loss)
I0801 14:16:59.719600 24522 sgd_solver.cpp:136] Iteration 45400, lr = 0.00290625, m = 0.9
I0801 14:17:01.361320 24522 solver.cpp:353] Iteration 45500 (60.9124 iter/s, 1.6417s/100 iter), loss = 0.00669323
I0801 14:17:01.361346 24522 solver.cpp:375]     Train net output #0: loss = 0.00669253 (* 1 = 0.00669253 loss)
I0801 14:17:01.361353 24522 sgd_solver.cpp:136] Iteration 45500, lr = 0.00289063, m = 0.9
I0801 14:17:03.045578 24522 solver.cpp:353] Iteration 45600 (59.3752 iter/s, 1.6842s/100 iter), loss = 0.00199184
I0801 14:17:03.045604 24522 solver.cpp:375]     Train net output #0: loss = 0.00199114 (* 1 = 0.00199114 loss)
I0801 14:17:03.045608 24522 sgd_solver.cpp:136] Iteration 45600, lr = 0.002875, m = 0.9
I0801 14:17:04.613617 24522 solver.cpp:353] Iteration 45700 (63.7758 iter/s, 1.56799s/100 iter), loss = 0.00368762
I0801 14:17:04.613683 24522 solver.cpp:375]     Train net output #0: loss = 0.00368692 (* 1 = 0.00368692 loss)
I0801 14:17:04.613700 24522 sgd_solver.cpp:136] Iteration 45700, lr = 0.00285937, m = 0.9
I0801 14:17:06.342447 24522 solver.cpp:353] Iteration 45800 (57.8445 iter/s, 1.72877s/100 iter), loss = 0.0120308
I0801 14:17:06.342514 24522 solver.cpp:375]     Train net output #0: loss = 0.0120301 (* 1 = 0.0120301 loss)
I0801 14:17:06.342521 24522 sgd_solver.cpp:136] Iteration 45800, lr = 0.00284375, m = 0.9
I0801 14:17:07.999166 24522 solver.cpp:353] Iteration 45900 (60.3622 iter/s, 1.65667s/100 iter), loss = 0.00030727
I0801 14:17:07.999215 24522 solver.cpp:375]     Train net output #0: loss = 0.000306588 (* 1 = 0.000306588 loss)
I0801 14:17:07.999228 24522 sgd_solver.cpp:136] Iteration 45900, lr = 0.00282812, m = 0.9
I0801 14:17:09.594343 24522 solver.cpp:404] Sparsity after update:
I0801 14:17:09.595916 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:17:09.595924 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:17:09.595932 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:17:09.595937 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:17:09.595942 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:17:09.595945 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:17:09.595949 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:17:09.595953 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:17:09.595957 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:17:09.595962 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:17:09.595965 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:17:09.595970 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:17:09.595974 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:17:09.595985 24522 solver.cpp:550] Iteration 46000, Testing net (#0)
I0801 14:17:10.407780 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.894413
I0801 14:17:10.407799 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 14:17:10.407804 24522 solver.cpp:635]     Test net output #2: loss = 0.423443 (* 1 = 0.423443 loss)
I0801 14:17:10.407820 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.811807s
I0801 14:17:10.427357 24522 solver.cpp:353] Iteration 46000 (41.1842 iter/s, 2.42812s/100 iter), loss = 0.00280235
I0801 14:17:10.427379 24522 solver.cpp:375]     Train net output #0: loss = 0.00280165 (* 1 = 0.00280165 loss)
I0801 14:17:10.427383 24522 sgd_solver.cpp:136] Iteration 46000, lr = 0.0028125, m = 0.9
I0801 14:17:12.006662 24522 solver.cpp:353] Iteration 46100 (63.321 iter/s, 1.57926s/100 iter), loss = 0.00362323
I0801 14:17:12.006690 24522 solver.cpp:375]     Train net output #0: loss = 0.00362253 (* 1 = 0.00362253 loss)
I0801 14:17:12.006695 24522 sgd_solver.cpp:136] Iteration 46100, lr = 0.00279688, m = 0.9
I0801 14:17:13.587709 24522 solver.cpp:353] Iteration 46200 (63.2513 iter/s, 1.581s/100 iter), loss = 0.000994526
I0801 14:17:13.587756 24522 solver.cpp:375]     Train net output #0: loss = 0.000993819 (* 1 = 0.000993819 loss)
I0801 14:17:13.587775 24522 sgd_solver.cpp:136] Iteration 46200, lr = 0.00278125, m = 0.9
I0801 14:17:15.301759 24522 solver.cpp:353] Iteration 46300 (58.3434 iter/s, 1.71399s/100 iter), loss = 0.00218274
I0801 14:17:15.301831 24522 solver.cpp:375]     Train net output #0: loss = 0.00218203 (* 1 = 0.00218203 loss)
I0801 14:17:15.301852 24522 sgd_solver.cpp:136] Iteration 46300, lr = 0.00276563, m = 0.9
I0801 14:17:16.966101 24522 solver.cpp:353] Iteration 46400 (60.0858 iter/s, 1.66429s/100 iter), loss = 0.0371972
I0801 14:17:16.966130 24522 solver.cpp:375]     Train net output #0: loss = 0.0371965 (* 1 = 0.0371965 loss)
I0801 14:17:16.966138 24522 sgd_solver.cpp:136] Iteration 46400, lr = 0.00275, m = 0.9
I0801 14:17:18.594499 24522 solver.cpp:353] Iteration 46500 (61.412 iter/s, 1.62835s/100 iter), loss = 0.00422712
I0801 14:17:18.594527 24522 solver.cpp:375]     Train net output #0: loss = 0.00422642 (* 1 = 0.00422642 loss)
I0801 14:17:18.594532 24522 sgd_solver.cpp:136] Iteration 46500, lr = 0.00273437, m = 0.9
I0801 14:17:20.174248 24522 solver.cpp:353] Iteration 46600 (63.3032 iter/s, 1.5797s/100 iter), loss = 0.000499518
I0801 14:17:20.174274 24522 solver.cpp:375]     Train net output #0: loss = 0.000498813 (* 1 = 0.000498813 loss)
I0801 14:17:20.174304 24522 sgd_solver.cpp:136] Iteration 46600, lr = 0.00271875, m = 0.9
I0801 14:17:21.761479 24522 solver.cpp:353] Iteration 46700 (63.0048 iter/s, 1.58718s/100 iter), loss = 0.00333456
I0801 14:17:21.761505 24522 solver.cpp:375]     Train net output #0: loss = 0.00333386 (* 1 = 0.00333386 loss)
I0801 14:17:21.761512 24522 sgd_solver.cpp:136] Iteration 46700, lr = 0.00270312, m = 0.9
I0801 14:17:23.334183 24522 solver.cpp:353] Iteration 46800 (63.5868 iter/s, 1.57265s/100 iter), loss = 0.00218479
I0801 14:17:23.334208 24522 solver.cpp:375]     Train net output #0: loss = 0.00218408 (* 1 = 0.00218408 loss)
I0801 14:17:23.334213 24522 sgd_solver.cpp:136] Iteration 46800, lr = 0.0026875, m = 0.9
I0801 14:17:24.945040 24522 solver.cpp:353] Iteration 46900 (62.0807 iter/s, 1.61081s/100 iter), loss = 0.000859444
I0801 14:17:24.945070 24522 solver.cpp:375]     Train net output #0: loss = 0.00085874 (* 1 = 0.00085874 loss)
I0801 14:17:24.945075 24522 sgd_solver.cpp:136] Iteration 46900, lr = 0.00267187, m = 0.9
I0801 14:17:26.546816 24522 solver.cpp:404] Sparsity after update:
I0801 14:17:26.548458 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:17:26.548467 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:17:26.548473 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:17:26.548476 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:17:26.548480 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:17:26.548481 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:17:26.548485 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:17:26.548486 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:17:26.548488 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:17:26.548491 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:17:26.548492 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:17:26.548494 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:17:26.548496 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:17:26.548504 24522 solver.cpp:550] Iteration 47000, Testing net (#0)
I0801 14:17:27.433969 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.891178
I0801 14:17:27.433987 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 14:17:27.433992 24522 solver.cpp:635]     Test net output #2: loss = 0.439066 (* 1 = 0.439066 loss)
I0801 14:17:27.434006 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.885472s
I0801 14:17:27.449875 24522 solver.cpp:353] Iteration 47000 (39.924 iter/s, 2.50476s/100 iter), loss = 0.00263097
I0801 14:17:27.449893 24522 solver.cpp:375]     Train net output #0: loss = 0.00263027 (* 1 = 0.00263027 loss)
I0801 14:17:27.449899 24522 sgd_solver.cpp:136] Iteration 47000, lr = 0.00265625, m = 0.9
I0801 14:17:29.122589 24522 solver.cpp:353] Iteration 47100 (59.7851 iter/s, 1.67266s/100 iter), loss = 0.0120141
I0801 14:17:29.122613 24522 solver.cpp:375]     Train net output #0: loss = 0.0120134 (* 1 = 0.0120134 loss)
I0801 14:17:29.122618 24522 sgd_solver.cpp:136] Iteration 47100, lr = 0.00264063, m = 0.9
I0801 14:17:30.738884 24522 solver.cpp:353] Iteration 47200 (61.872 iter/s, 1.61624s/100 iter), loss = 0.004296
I0801 14:17:30.738914 24522 solver.cpp:375]     Train net output #0: loss = 0.0042953 (* 1 = 0.0042953 loss)
I0801 14:17:30.738919 24522 sgd_solver.cpp:136] Iteration 47200, lr = 0.002625, m = 0.9
I0801 14:17:32.417963 24522 solver.cpp:353] Iteration 47300 (59.5582 iter/s, 1.67903s/100 iter), loss = 0.00577798
I0801 14:17:32.417991 24522 solver.cpp:375]     Train net output #0: loss = 0.00577728 (* 1 = 0.00577728 loss)
I0801 14:17:32.417997 24522 sgd_solver.cpp:136] Iteration 47300, lr = 0.00260938, m = 0.9
I0801 14:17:34.117321 24522 solver.cpp:353] Iteration 47400 (58.8478 iter/s, 1.6993s/100 iter), loss = 0.00228793
I0801 14:17:34.117354 24522 solver.cpp:375]     Train net output #0: loss = 0.00228723 (* 1 = 0.00228723 loss)
I0801 14:17:34.117360 24522 sgd_solver.cpp:136] Iteration 47400, lr = 0.00259375, m = 0.9
I0801 14:17:35.716992 24522 solver.cpp:353] Iteration 47500 (62.5148 iter/s, 1.59962s/100 iter), loss = 0.00365444
I0801 14:17:35.717020 24522 solver.cpp:375]     Train net output #0: loss = 0.00365374 (* 1 = 0.00365374 loss)
I0801 14:17:35.717025 24522 sgd_solver.cpp:136] Iteration 47500, lr = 0.00257812, m = 0.9
I0801 14:17:37.393082 24522 solver.cpp:353] Iteration 47600 (59.6645 iter/s, 1.67604s/100 iter), loss = 0.000827092
I0801 14:17:37.393167 24522 solver.cpp:375]     Train net output #0: loss = 0.000826394 (* 1 = 0.000826394 loss)
I0801 14:17:37.393173 24522 sgd_solver.cpp:136] Iteration 47600, lr = 0.0025625, m = 0.9
I0801 14:17:39.101155 24522 solver.cpp:353] Iteration 47700 (58.5475 iter/s, 1.70801s/100 iter), loss = 0.0213427
I0801 14:17:39.101183 24522 solver.cpp:375]     Train net output #0: loss = 0.021342 (* 1 = 0.021342 loss)
I0801 14:17:39.101191 24522 sgd_solver.cpp:136] Iteration 47700, lr = 0.00254687, m = 0.9
I0801 14:17:40.770917 24522 solver.cpp:353] Iteration 47800 (59.8905 iter/s, 1.66971s/100 iter), loss = 0.000689803
I0801 14:17:40.770944 24522 solver.cpp:375]     Train net output #0: loss = 0.000689094 (* 1 = 0.000689094 loss)
I0801 14:17:40.770951 24522 sgd_solver.cpp:136] Iteration 47800, lr = 0.00253125, m = 0.9
I0801 14:17:42.497822 24522 solver.cpp:353] Iteration 47900 (57.9091 iter/s, 1.72684s/100 iter), loss = 0.00046019
I0801 14:17:42.497867 24522 solver.cpp:375]     Train net output #0: loss = 0.000459478 (* 1 = 0.000459478 loss)
I0801 14:17:42.497889 24522 sgd_solver.cpp:136] Iteration 47900, lr = 0.00251562, m = 0.9
I0801 14:17:44.221591 24522 solver.cpp:404] Sparsity after update:
I0801 14:17:44.223492 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:17:44.223505 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:17:44.223513 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:17:44.223518 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:17:44.223522 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:17:44.223526 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:17:44.223531 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:17:44.223533 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:17:44.223538 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:17:44.223541 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:17:44.223546 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:17:44.223549 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:17:44.223554 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:17:44.223565 24522 solver.cpp:550] Iteration 48000, Testing net (#0)
I0801 14:17:44.258483 24520 data_reader.cpp:264] Starting prefetch of epoch 6
I0801 14:17:45.053171 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.89206
I0801 14:17:45.053189 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 14:17:45.053195 24522 solver.cpp:635]     Test net output #2: loss = 0.430369 (* 1 = 0.430369 loss)
I0801 14:17:45.053211 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.829616s
I0801 14:17:45.068876 24522 solver.cpp:353] Iteration 48000 (38.8956 iter/s, 2.57098s/100 iter), loss = 0.00114082
I0801 14:17:45.068910 24522 solver.cpp:375]     Train net output #0: loss = 0.00114011 (* 1 = 0.00114011 loss)
I0801 14:17:45.068925 24522 sgd_solver.cpp:136] Iteration 48000, lr = 0.0025, m = 0.9
I0801 14:17:46.778707 24522 solver.cpp:353] Iteration 48100 (58.4873 iter/s, 1.70977s/100 iter), loss = 0.00418524
I0801 14:17:46.778729 24522 solver.cpp:375]     Train net output #0: loss = 0.00418453 (* 1 = 0.00418453 loss)
I0801 14:17:46.778733 24522 sgd_solver.cpp:136] Iteration 48100, lr = 0.00248438, m = 0.9
I0801 14:17:48.405146 24522 solver.cpp:353] Iteration 48200 (61.4861 iter/s, 1.62638s/100 iter), loss = 0.00690877
I0801 14:17:48.405192 24522 solver.cpp:375]     Train net output #0: loss = 0.00690806 (* 1 = 0.00690806 loss)
I0801 14:17:48.405205 24522 sgd_solver.cpp:136] Iteration 48200, lr = 0.00246875, m = 0.9
I0801 14:17:50.030041 24522 solver.cpp:353] Iteration 48300 (61.5446 iter/s, 1.62484s/100 iter), loss = 0.000810311
I0801 14:17:50.030088 24522 solver.cpp:375]     Train net output #0: loss = 0.000809602 (* 1 = 0.000809602 loss)
I0801 14:17:50.030108 24522 sgd_solver.cpp:136] Iteration 48300, lr = 0.00245313, m = 0.9
I0801 14:17:51.647233 24522 solver.cpp:353] Iteration 48400 (61.8373 iter/s, 1.61715s/100 iter), loss = 0.00223008
I0801 14:17:51.647306 24522 solver.cpp:375]     Train net output #0: loss = 0.00222937 (* 1 = 0.00222937 loss)
I0801 14:17:51.647322 24522 sgd_solver.cpp:136] Iteration 48400, lr = 0.0024375, m = 0.9
I0801 14:17:53.312355 24522 solver.cpp:353] Iteration 48500 (60.0575 iter/s, 1.66507s/100 iter), loss = 0.000370159
I0801 14:17:53.312381 24522 solver.cpp:375]     Train net output #0: loss = 0.000369446 (* 1 = 0.000369446 loss)
I0801 14:17:53.312388 24522 sgd_solver.cpp:136] Iteration 48500, lr = 0.00242188, m = 0.9
I0801 14:17:54.992120 24522 solver.cpp:353] Iteration 48600 (59.5342 iter/s, 1.67971s/100 iter), loss = 0.0039023
I0801 14:17:54.992187 24522 solver.cpp:375]     Train net output #0: loss = 0.00390159 (* 1 = 0.00390159 loss)
I0801 14:17:54.992202 24522 sgd_solver.cpp:136] Iteration 48600, lr = 0.00240625, m = 0.9
I0801 14:17:56.657294 24522 solver.cpp:353] Iteration 48700 (60.0558 iter/s, 1.66512s/100 iter), loss = 0.001293
I0801 14:17:56.657325 24522 solver.cpp:375]     Train net output #0: loss = 0.00129229 (* 1 = 0.00129229 loss)
I0801 14:17:56.657331 24522 sgd_solver.cpp:136] Iteration 48700, lr = 0.00239062, m = 0.9
I0801 14:17:58.392566 24522 solver.cpp:353] Iteration 48800 (57.6297 iter/s, 1.73522s/100 iter), loss = 0.000860412
I0801 14:17:58.392593 24522 solver.cpp:375]     Train net output #0: loss = 0.000859705 (* 1 = 0.000859705 loss)
I0801 14:17:58.392599 24522 sgd_solver.cpp:136] Iteration 48800, lr = 0.002375, m = 0.9
I0801 14:18:00.055704 24522 solver.cpp:353] Iteration 48900 (60.1296 iter/s, 1.66308s/100 iter), loss = 0.00330686
I0801 14:18:00.055769 24522 solver.cpp:375]     Train net output #0: loss = 0.00330615 (* 1 = 0.00330615 loss)
I0801 14:18:00.055790 24522 sgd_solver.cpp:136] Iteration 48900, lr = 0.00235937, m = 0.9
I0801 14:18:01.747787 24522 solver.cpp:404] Sparsity after update:
I0801 14:18:01.749672 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:18:01.749682 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:18:01.749691 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:18:01.749696 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:18:01.749701 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:18:01.749706 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:18:01.749711 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:18:01.749714 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:18:01.749718 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:18:01.749722 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:18:01.749725 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:18:01.749729 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:18:01.749733 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:18:01.749744 24522 solver.cpp:550] Iteration 49000, Testing net (#0)
I0801 14:18:02.578824 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.902942
I0801 14:18:02.578848 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995588
I0801 14:18:02.578853 24522 solver.cpp:635]     Test net output #2: loss = 0.412203 (* 1 = 0.412203 loss)
I0801 14:18:02.578872 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.829099s
I0801 14:18:02.595520 24522 solver.cpp:353] Iteration 49000 (39.3741 iter/s, 2.53974s/100 iter), loss = 0.00834225
I0801 14:18:02.595556 24522 solver.cpp:375]     Train net output #0: loss = 0.00834155 (* 1 = 0.00834155 loss)
I0801 14:18:02.595566 24522 sgd_solver.cpp:136] Iteration 49000, lr = 0.00234375, m = 0.9
I0801 14:18:04.229593 24522 solver.cpp:353] Iteration 49100 (61.1987 iter/s, 1.63402s/100 iter), loss = 0.00212511
I0801 14:18:04.229619 24522 solver.cpp:375]     Train net output #0: loss = 0.00212441 (* 1 = 0.00212441 loss)
I0801 14:18:04.229625 24522 sgd_solver.cpp:136] Iteration 49100, lr = 0.00232813, m = 0.9
I0801 14:18:05.870824 24522 solver.cpp:353] Iteration 49200 (60.9318 iter/s, 1.64118s/100 iter), loss = 0.00224913
I0801 14:18:05.870875 24522 solver.cpp:375]     Train net output #0: loss = 0.00224843 (* 1 = 0.00224843 loss)
I0801 14:18:05.870903 24522 sgd_solver.cpp:136] Iteration 49200, lr = 0.0023125, m = 0.9
I0801 14:18:07.518133 24522 solver.cpp:353] Iteration 49300 (60.707 iter/s, 1.64726s/100 iter), loss = 0.00177838
I0801 14:18:07.518210 24522 solver.cpp:375]     Train net output #0: loss = 0.00177768 (* 1 = 0.00177768 loss)
I0801 14:18:07.518218 24522 sgd_solver.cpp:136] Iteration 49300, lr = 0.00229687, m = 0.9
I0801 14:18:09.174814 24522 solver.cpp:353] Iteration 49400 (60.3636 iter/s, 1.65663s/100 iter), loss = 0.000718735
I0801 14:18:09.174836 24522 solver.cpp:375]     Train net output #0: loss = 0.000718035 (* 1 = 0.000718035 loss)
I0801 14:18:09.174840 24522 sgd_solver.cpp:136] Iteration 49400, lr = 0.00228125, m = 0.9
I0801 14:18:10.771766 24522 solver.cpp:353] Iteration 49500 (62.6214 iter/s, 1.5969s/100 iter), loss = 0.000590206
I0801 14:18:10.771816 24522 solver.cpp:375]     Train net output #0: loss = 0.000589506 (* 1 = 0.000589506 loss)
I0801 14:18:10.771829 24522 sgd_solver.cpp:136] Iteration 49500, lr = 0.00226562, m = 0.9
I0801 14:18:12.404362 24522 solver.cpp:353] Iteration 49600 (61.2541 iter/s, 1.63254s/100 iter), loss = 0.00594818
I0801 14:18:12.404412 24522 solver.cpp:375]     Train net output #0: loss = 0.00594748 (* 1 = 0.00594748 loss)
I0801 14:18:12.404424 24522 sgd_solver.cpp:136] Iteration 49600, lr = 0.00225, m = 0.9
I0801 14:18:14.096853 24522 solver.cpp:353] Iteration 49700 (59.0863 iter/s, 1.69244s/100 iter), loss = 0.000240295
I0801 14:18:14.096902 24522 solver.cpp:375]     Train net output #0: loss = 0.000239594 (* 1 = 0.000239594 loss)
I0801 14:18:14.096913 24522 sgd_solver.cpp:136] Iteration 49700, lr = 0.00223437, m = 0.9
I0801 14:18:15.750527 24522 solver.cpp:353] Iteration 49800 (60.4733 iter/s, 1.65362s/100 iter), loss = 0.000420505
I0801 14:18:15.750556 24522 solver.cpp:375]     Train net output #0: loss = 0.000419805 (* 1 = 0.000419805 loss)
I0801 14:18:15.750562 24522 sgd_solver.cpp:136] Iteration 49800, lr = 0.00221875, m = 0.9
I0801 14:18:17.460803 24522 solver.cpp:353] Iteration 49900 (58.472 iter/s, 1.71022s/100 iter), loss = 7.42731e-05
I0801 14:18:17.460831 24522 solver.cpp:375]     Train net output #0: loss = 7.35756e-05 (* 1 = 7.35756e-05 loss)
I0801 14:18:17.460835 24522 sgd_solver.cpp:136] Iteration 49900, lr = 0.00220312, m = 0.9
I0801 14:18:19.045475 24522 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_50000.caffemodel
I0801 14:18:19.053601 24522 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_50000.solverstate
I0801 14:18:19.057268 24522 solver.cpp:404] Sparsity after update:
I0801 14:18:19.059078 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:18:19.059087 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:18:19.059092 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:18:19.059094 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:18:19.059096 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:18:19.059098 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:18:19.059100 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:18:19.059103 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:18:19.059104 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:18:19.059106 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:18:19.059108 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:18:19.059111 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:18:19.059113 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:18:19.059120 24522 solver.cpp:550] Iteration 50000, Testing net (#0)
I0801 14:18:19.863417 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.895295
I0801 14:18:19.863436 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995294
I0801 14:18:19.863441 24522 solver.cpp:635]     Test net output #2: loss = 0.442876 (* 1 = 0.442876 loss)
I0801 14:18:19.863456 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.804308s
I0801 14:18:19.879592 24522 solver.cpp:353] Iteration 50000 (41.3442 iter/s, 2.41872s/100 iter), loss = 0.000229349
I0801 14:18:19.879623 24522 solver.cpp:375]     Train net output #0: loss = 0.000228651 (* 1 = 0.000228651 loss)
I0801 14:18:19.879628 24522 sgd_solver.cpp:136] Iteration 50000, lr = 0.0021875, m = 0.9
I0801 14:18:21.449702 24522 solver.cpp:353] Iteration 50100 (63.6919 iter/s, 1.57006s/100 iter), loss = 0.000642494
I0801 14:18:21.449753 24522 solver.cpp:375]     Train net output #0: loss = 0.000641796 (* 1 = 0.000641796 loss)
I0801 14:18:21.449767 24522 sgd_solver.cpp:136] Iteration 50100, lr = 0.00217188, m = 0.9
I0801 14:18:23.041909 24522 solver.cpp:353] Iteration 50200 (62.8079 iter/s, 1.59216s/100 iter), loss = 0.0054778
I0801 14:18:23.041934 24522 solver.cpp:375]     Train net output #0: loss = 0.0054771 (* 1 = 0.0054771 loss)
I0801 14:18:23.041940 24522 sgd_solver.cpp:136] Iteration 50200, lr = 0.00215625, m = 0.9
I0801 14:18:24.626925 24522 solver.cpp:353] Iteration 50300 (63.0929 iter/s, 1.58496s/100 iter), loss = 5.7159e-05
I0801 14:18:24.626950 24522 solver.cpp:375]     Train net output #0: loss = 5.64621e-05 (* 1 = 5.64621e-05 loss)
I0801 14:18:24.626957 24522 sgd_solver.cpp:136] Iteration 50300, lr = 0.00214063, m = 0.9
I0801 14:18:26.235194 24522 solver.cpp:353] Iteration 50400 (62.1807 iter/s, 1.60822s/100 iter), loss = 0.000540474
I0801 14:18:26.235225 24522 solver.cpp:375]     Train net output #0: loss = 0.000539779 (* 1 = 0.000539779 loss)
I0801 14:18:26.235232 24522 sgd_solver.cpp:136] Iteration 50400, lr = 0.002125, m = 0.9
I0801 14:18:27.841547 24522 solver.cpp:353] Iteration 50500 (62.2548 iter/s, 1.6063s/100 iter), loss = 0.00529038
I0801 14:18:27.841754 24522 solver.cpp:375]     Train net output #0: loss = 0.00528969 (* 1 = 0.00528969 loss)
I0801 14:18:27.841814 24522 sgd_solver.cpp:136] Iteration 50500, lr = 0.00210937, m = 0.9
I0801 14:18:29.430892 24522 solver.cpp:353] Iteration 50600 (62.921 iter/s, 1.5893s/100 iter), loss = 0.00183118
I0801 14:18:29.430920 24522 solver.cpp:375]     Train net output #0: loss = 0.00183048 (* 1 = 0.00183048 loss)
I0801 14:18:29.430927 24522 sgd_solver.cpp:136] Iteration 50600, lr = 0.00209375, m = 0.9
I0801 14:18:31.008646 24522 solver.cpp:353] Iteration 50700 (63.3833 iter/s, 1.5777s/100 iter), loss = 0.000184943
I0801 14:18:31.008673 24522 solver.cpp:375]     Train net output #0: loss = 0.00018425 (* 1 = 0.00018425 loss)
I0801 14:18:31.008679 24522 sgd_solver.cpp:136] Iteration 50700, lr = 0.00207812, m = 0.9
I0801 14:18:32.612749 24522 solver.cpp:353] Iteration 50800 (62.3421 iter/s, 1.60405s/100 iter), loss = 0.00224082
I0801 14:18:32.612777 24522 solver.cpp:375]     Train net output #0: loss = 0.00224013 (* 1 = 0.00224013 loss)
I0801 14:18:32.612782 24522 sgd_solver.cpp:136] Iteration 50800, lr = 0.0020625, m = 0.9
I0801 14:18:34.199223 24522 solver.cpp:353] Iteration 50900 (63.035 iter/s, 1.58642s/100 iter), loss = 0.00072819
I0801 14:18:34.199249 24522 solver.cpp:375]     Train net output #0: loss = 0.0007275 (* 1 = 0.0007275 loss)
I0801 14:18:34.199255 24522 sgd_solver.cpp:136] Iteration 50900, lr = 0.00204687, m = 0.9
I0801 14:18:35.757325 24522 solver.cpp:404] Sparsity after update:
I0801 14:18:35.758927 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:18:35.758936 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:18:35.758944 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:18:35.758949 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:18:35.758954 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:18:35.758958 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:18:35.758962 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:18:35.758966 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:18:35.758970 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:18:35.758975 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:18:35.758978 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:18:35.758983 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:18:35.758987 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:18:35.758998 24522 solver.cpp:550] Iteration 51000, Testing net (#0)
I0801 14:18:36.610667 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.901178
I0801 14:18:36.610687 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.994706
I0801 14:18:36.610693 24522 solver.cpp:635]     Test net output #2: loss = 0.433684 (* 1 = 0.433684 loss)
I0801 14:18:36.610713 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.851686s
I0801 14:18:36.626394 24522 solver.cpp:353] Iteration 51000 (41.2014 iter/s, 2.4271s/100 iter), loss = 0.00252364
I0801 14:18:36.626411 24522 solver.cpp:375]     Train net output #0: loss = 0.00252295 (* 1 = 0.00252295 loss)
I0801 14:18:36.626417 24522 sgd_solver.cpp:136] Iteration 51000, lr = 0.00203125, m = 0.9
I0801 14:18:38.355356 24522 solver.cpp:353] Iteration 51100 (57.8402 iter/s, 1.7289s/100 iter), loss = 0.000679729
I0801 14:18:38.355465 24522 solver.cpp:375]     Train net output #0: loss = 0.00067904 (* 1 = 0.00067904 loss)
I0801 14:18:38.355478 24522 sgd_solver.cpp:136] Iteration 51100, lr = 0.00201563, m = 0.9
I0801 14:18:39.969552 24522 solver.cpp:353] Iteration 51200 (61.9523 iter/s, 1.61415s/100 iter), loss = 0.000356135
I0801 14:18:39.969624 24522 solver.cpp:375]     Train net output #0: loss = 0.000355447 (* 1 = 0.000355447 loss)
I0801 14:18:39.969640 24522 sgd_solver.cpp:136] Iteration 51200, lr = 0.002, m = 0.9
I0801 14:18:41.599146 24522 solver.cpp:353] Iteration 51300 (61.3674 iter/s, 1.62953s/100 iter), loss = 0.000994919
I0801 14:18:41.599345 24522 solver.cpp:375]     Train net output #0: loss = 0.000994231 (* 1 = 0.000994231 loss)
I0801 14:18:41.599449 24522 sgd_solver.cpp:136] Iteration 51300, lr = 0.00198438, m = 0.9
I0801 14:18:43.245909 24522 solver.cpp:353] Iteration 51400 (60.7269 iter/s, 1.64672s/100 iter), loss = 0.00365058
I0801 14:18:43.245936 24522 solver.cpp:375]     Train net output #0: loss = 0.00364989 (* 1 = 0.00364989 loss)
I0801 14:18:43.245941 24522 sgd_solver.cpp:136] Iteration 51400, lr = 0.00196875, m = 0.9
I0801 14:18:44.926818 24522 solver.cpp:353] Iteration 51500 (59.4935 iter/s, 1.68086s/100 iter), loss = 0.000449854
I0801 14:18:44.926843 24522 solver.cpp:375]     Train net output #0: loss = 0.000449164 (* 1 = 0.000449164 loss)
I0801 14:18:44.926849 24522 sgd_solver.cpp:136] Iteration 51500, lr = 0.00195312, m = 0.9
I0801 14:18:46.739648 24522 solver.cpp:353] Iteration 51600 (55.1641 iter/s, 1.81277s/100 iter), loss = 0.00108968
I0801 14:18:46.739672 24522 solver.cpp:375]     Train net output #0: loss = 0.00108899 (* 1 = 0.00108899 loss)
I0801 14:18:46.739677 24522 sgd_solver.cpp:136] Iteration 51600, lr = 0.0019375, m = 0.9
I0801 14:18:48.373059 24522 solver.cpp:353] Iteration 51700 (61.2235 iter/s, 1.63336s/100 iter), loss = 0.00243366
I0801 14:18:48.373090 24522 solver.cpp:375]     Train net output #0: loss = 0.00243297 (* 1 = 0.00243297 loss)
I0801 14:18:48.373095 24522 sgd_solver.cpp:136] Iteration 51700, lr = 0.00192187, m = 0.9
I0801 14:18:50.083575 24522 solver.cpp:353] Iteration 51800 (58.4637 iter/s, 1.71046s/100 iter), loss = 0.000328123
I0801 14:18:50.083600 24522 solver.cpp:375]     Train net output #0: loss = 0.000327435 (* 1 = 0.000327435 loss)
I0801 14:18:50.083605 24522 sgd_solver.cpp:136] Iteration 51800, lr = 0.00190625, m = 0.9
I0801 14:18:51.761446 24522 solver.cpp:353] Iteration 51900 (59.6013 iter/s, 1.67782s/100 iter), loss = 0.000230751
I0801 14:18:51.761471 24522 solver.cpp:375]     Train net output #0: loss = 0.000230063 (* 1 = 0.000230063 loss)
I0801 14:18:51.761476 24522 sgd_solver.cpp:136] Iteration 51900, lr = 0.00189062, m = 0.9
I0801 14:18:53.420941 24522 solver.cpp:404] Sparsity after update:
I0801 14:18:53.422505 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:18:53.422513 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:18:53.422519 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:18:53.422521 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:18:53.422523 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:18:53.422525 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:18:53.422533 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:18:53.422534 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:18:53.422536 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:18:53.422538 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:18:53.422539 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:18:53.422541 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:18:53.422544 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:18:53.422551 24522 solver.cpp:550] Iteration 52000, Testing net (#0)
I0801 14:18:54.143776 24520 data_reader.cpp:264] Starting prefetch of epoch 7
I0801 14:18:54.233335 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.90353
I0801 14:18:54.233351 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 14:18:54.233373 24522 solver.cpp:635]     Test net output #2: loss = 0.419384 (* 1 = 0.419384 loss)
I0801 14:18:54.233393 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.810812s
I0801 14:18:54.249472 24522 solver.cpp:353] Iteration 52000 (40.1937 iter/s, 2.48795s/100 iter), loss = 0.00122811
I0801 14:18:54.249491 24522 solver.cpp:375]     Train net output #0: loss = 0.00122742 (* 1 = 0.00122742 loss)
I0801 14:18:54.249497 24522 sgd_solver.cpp:136] Iteration 52000, lr = 0.001875, m = 0.9
I0801 14:18:55.996017 24522 solver.cpp:353] Iteration 52100 (57.2578 iter/s, 1.74649s/100 iter), loss = 0.00152307
I0801 14:18:55.996047 24522 solver.cpp:375]     Train net output #0: loss = 0.00152238 (* 1 = 0.00152238 loss)
I0801 14:18:55.996073 24522 sgd_solver.cpp:136] Iteration 52100, lr = 0.00185938, m = 0.9
I0801 14:18:57.664580 24522 solver.cpp:353] Iteration 52200 (59.9338 iter/s, 1.66851s/100 iter), loss = 0.0010322
I0801 14:18:57.664611 24522 solver.cpp:375]     Train net output #0: loss = 0.00103151 (* 1 = 0.00103151 loss)
I0801 14:18:57.664618 24522 sgd_solver.cpp:136] Iteration 52200, lr = 0.00184375, m = 0.9
I0801 14:18:59.292511 24522 solver.cpp:353] Iteration 52300 (61.4295 iter/s, 1.62788s/100 iter), loss = 0.000537884
I0801 14:18:59.292537 24522 solver.cpp:375]     Train net output #0: loss = 0.000537197 (* 1 = 0.000537197 loss)
I0801 14:18:59.292543 24522 sgd_solver.cpp:136] Iteration 52300, lr = 0.00182813, m = 0.9
I0801 14:19:00.917201 24522 solver.cpp:353] Iteration 52400 (61.5524 iter/s, 1.62463s/100 iter), loss = 0.000921667
I0801 14:19:00.917312 24522 solver.cpp:375]     Train net output #0: loss = 0.00092098 (* 1 = 0.00092098 loss)
I0801 14:19:00.917335 24522 sgd_solver.cpp:136] Iteration 52400, lr = 0.0018125, m = 0.9
I0801 14:19:02.633437 24522 solver.cpp:353] Iteration 52500 (58.2688 iter/s, 1.71618s/100 iter), loss = 0.000841595
I0801 14:19:02.633509 24522 solver.cpp:375]     Train net output #0: loss = 0.000840908 (* 1 = 0.000840908 loss)
I0801 14:19:02.633524 24522 sgd_solver.cpp:136] Iteration 52500, lr = 0.00179687, m = 0.9
I0801 14:19:04.297587 24522 solver.cpp:353] Iteration 52600 (60.0926 iter/s, 1.6641s/100 iter), loss = 0.00260883
I0801 14:19:04.297641 24522 solver.cpp:375]     Train net output #0: loss = 0.00260814 (* 1 = 0.00260814 loss)
I0801 14:19:04.297657 24522 sgd_solver.cpp:136] Iteration 52600, lr = 0.00178125, m = 0.9
I0801 14:19:06.012064 24522 solver.cpp:353] Iteration 52700 (58.3287 iter/s, 1.71442s/100 iter), loss = 0.000245146
I0801 14:19:06.012133 24522 solver.cpp:375]     Train net output #0: loss = 0.000244458 (* 1 = 0.000244458 loss)
I0801 14:19:06.012152 24522 sgd_solver.cpp:136] Iteration 52700, lr = 0.00176562, m = 0.9
I0801 14:19:07.720930 24522 solver.cpp:353] Iteration 52800 (58.5201 iter/s, 1.70881s/100 iter), loss = 0.00142103
I0801 14:19:07.720955 24522 solver.cpp:375]     Train net output #0: loss = 0.00142034 (* 1 = 0.00142034 loss)
I0801 14:19:07.720962 24522 sgd_solver.cpp:136] Iteration 52800, lr = 0.00175, m = 0.9
I0801 14:19:09.428107 24522 solver.cpp:353] Iteration 52900 (58.5781 iter/s, 1.70712s/100 iter), loss = 0.000235937
I0801 14:19:09.428182 24522 solver.cpp:375]     Train net output #0: loss = 0.000235251 (* 1 = 0.000235251 loss)
I0801 14:19:09.428189 24522 sgd_solver.cpp:136] Iteration 52900, lr = 0.00173437, m = 0.9
I0801 14:19:11.044386 24522 solver.cpp:404] Sparsity after update:
I0801 14:19:11.046187 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:19:11.046197 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:19:11.046206 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:19:11.046211 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:19:11.046216 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:19:11.046218 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:19:11.046221 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:19:11.046224 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:19:11.046228 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:19:11.046231 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:19:11.046234 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:19:11.046237 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:19:11.046241 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:19:11.046250 24522 solver.cpp:550] Iteration 53000, Testing net (#0)
I0801 14:19:11.928683 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.910295
I0801 14:19:11.928711 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 14:19:11.928719 24522 solver.cpp:635]     Test net output #2: loss = 0.394632 (* 1 = 0.394632 loss)
I0801 14:19:11.928743 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.882461s
I0801 14:19:11.945868 24522 solver.cpp:353] Iteration 53000 (39.7192 iter/s, 2.51768s/100 iter), loss = 0.00126861
I0801 14:19:11.945909 24522 solver.cpp:375]     Train net output #0: loss = 0.00126792 (* 1 = 0.00126792 loss)
I0801 14:19:11.945921 24522 sgd_solver.cpp:136] Iteration 53000, lr = 0.00171875, m = 0.9
I0801 14:19:13.665908 24522 solver.cpp:353] Iteration 53100 (58.1398 iter/s, 1.71999s/100 iter), loss = 0.000434517
I0801 14:19:13.665961 24522 solver.cpp:375]     Train net output #0: loss = 0.00043383 (* 1 = 0.00043383 loss)
I0801 14:19:13.665978 24522 sgd_solver.cpp:136] Iteration 53100, lr = 0.00170313, m = 0.9
I0801 14:19:15.393741 24522 solver.cpp:353] Iteration 53200 (57.8781 iter/s, 1.72777s/100 iter), loss = 0.000196463
I0801 14:19:15.393793 24522 solver.cpp:375]     Train net output #0: loss = 0.000195778 (* 1 = 0.000195778 loss)
I0801 14:19:15.393806 24522 sgd_solver.cpp:136] Iteration 53200, lr = 0.0016875, m = 0.9
I0801 14:19:17.140846 24522 solver.cpp:353] Iteration 53300 (57.2393 iter/s, 1.74705s/100 iter), loss = 7.75023e-05
I0801 14:19:17.140892 24522 solver.cpp:375]     Train net output #0: loss = 7.68179e-05 (* 1 = 7.68179e-05 loss)
I0801 14:19:17.140903 24522 sgd_solver.cpp:136] Iteration 53300, lr = 0.00167188, m = 0.9
I0801 14:19:18.849714 24522 solver.cpp:353] Iteration 53400 (58.5201 iter/s, 1.70882s/100 iter), loss = 0.000684103
I0801 14:19:18.849740 24522 solver.cpp:375]     Train net output #0: loss = 0.000683419 (* 1 = 0.000683419 loss)
I0801 14:19:18.849745 24522 sgd_solver.cpp:136] Iteration 53400, lr = 0.00165625, m = 0.9
I0801 14:19:20.545780 24522 solver.cpp:353] Iteration 53500 (58.9618 iter/s, 1.69601s/100 iter), loss = 0.000975593
I0801 14:19:20.545807 24522 solver.cpp:375]     Train net output #0: loss = 0.000974909 (* 1 = 0.000974909 loss)
I0801 14:19:20.545812 24522 sgd_solver.cpp:136] Iteration 53500, lr = 0.00164062, m = 0.9
I0801 14:19:22.259513 24522 solver.cpp:353] Iteration 53600 (58.3543 iter/s, 1.71367s/100 iter), loss = 0.000919639
I0801 14:19:22.259618 24522 solver.cpp:375]     Train net output #0: loss = 0.000918956 (* 1 = 0.000918956 loss)
I0801 14:19:22.259642 24522 sgd_solver.cpp:136] Iteration 53600, lr = 0.001625, m = 0.9
I0801 14:19:23.870846 24522 solver.cpp:353] Iteration 53700 (62.0622 iter/s, 1.61129s/100 iter), loss = 0.0016663
I0801 14:19:23.870873 24522 solver.cpp:375]     Train net output #0: loss = 0.00166562 (* 1 = 0.00166562 loss)
I0801 14:19:23.870900 24522 sgd_solver.cpp:136] Iteration 53700, lr = 0.00160937, m = 0.9
I0801 14:19:25.553444 24522 solver.cpp:353] Iteration 53800 (59.4338 iter/s, 1.68254s/100 iter), loss = 0.00363187
I0801 14:19:25.553557 24522 solver.cpp:375]     Train net output #0: loss = 0.00363118 (* 1 = 0.00363118 loss)
I0801 14:19:25.553576 24522 sgd_solver.cpp:136] Iteration 53800, lr = 0.00159375, m = 0.9
I0801 14:19:27.320119 24522 solver.cpp:353] Iteration 53900 (56.6052 iter/s, 1.76662s/100 iter), loss = 0.0119178
I0801 14:19:27.320142 24522 solver.cpp:375]     Train net output #0: loss = 0.0119172 (* 1 = 0.0119172 loss)
I0801 14:19:27.320145 24522 sgd_solver.cpp:136] Iteration 53900, lr = 0.00157812, m = 0.9
I0801 14:19:28.903018 24522 solver.cpp:404] Sparsity after update:
I0801 14:19:28.904639 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:19:28.904647 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:19:28.904655 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:19:28.904660 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:19:28.904664 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:19:28.904670 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:19:28.904673 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:19:28.904677 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:19:28.904681 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:19:28.904685 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:19:28.904688 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:19:28.904692 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:19:28.904695 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:19:28.904706 24522 solver.cpp:550] Iteration 54000, Testing net (#0)
I0801 14:19:29.711675 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.915295
I0801 14:19:29.711699 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996177
I0801 14:19:29.711705 24522 solver.cpp:635]     Test net output #2: loss = 0.373765 (* 1 = 0.373765 loss)
I0801 14:19:29.711724 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.80699s
I0801 14:19:29.727924 24522 solver.cpp:353] Iteration 54000 (41.5329 iter/s, 2.40773s/100 iter), loss = 0.000687217
I0801 14:19:29.728178 24522 solver.cpp:375]     Train net output #0: loss = 0.000686533 (* 1 = 0.000686533 loss)
I0801 14:19:29.728185 24522 sgd_solver.cpp:136] Iteration 54000, lr = 0.0015625, m = 0.9
I0801 14:19:31.315908 24522 solver.cpp:353] Iteration 54100 (62.975 iter/s, 1.58793s/100 iter), loss = 0.00132573
I0801 14:19:31.315937 24522 solver.cpp:375]     Train net output #0: loss = 0.00132505 (* 1 = 0.00132505 loss)
I0801 14:19:31.315943 24522 sgd_solver.cpp:136] Iteration 54100, lr = 0.00154688, m = 0.9
I0801 14:19:32.904285 24522 solver.cpp:353] Iteration 54200 (62.9594 iter/s, 1.58833s/100 iter), loss = 0.000376965
I0801 14:19:32.904312 24522 solver.cpp:375]     Train net output #0: loss = 0.000376281 (* 1 = 0.000376281 loss)
I0801 14:19:32.904319 24522 sgd_solver.cpp:136] Iteration 54200, lr = 0.00153125, m = 0.9
I0801 14:19:34.495860 24522 solver.cpp:353] Iteration 54300 (62.8328 iter/s, 1.59153s/100 iter), loss = 0.0021527
I0801 14:19:34.495949 24522 solver.cpp:375]     Train net output #0: loss = 0.00215202 (* 1 = 0.00215202 loss)
I0801 14:19:34.495955 24522 sgd_solver.cpp:136] Iteration 54300, lr = 0.00151563, m = 0.9
I0801 14:19:36.139536 24522 solver.cpp:353] Iteration 54400 (60.8415 iter/s, 1.64362s/100 iter), loss = 0.000795495
I0801 14:19:36.139576 24522 solver.cpp:375]     Train net output #0: loss = 0.00079481 (* 1 = 0.00079481 loss)
I0801 14:19:36.139586 24522 sgd_solver.cpp:136] Iteration 54400, lr = 0.0015, m = 0.9
I0801 14:19:37.780740 24522 solver.cpp:353] Iteration 54500 (60.9326 iter/s, 1.64116s/100 iter), loss = 0.00175581
I0801 14:19:37.780764 24522 solver.cpp:375]     Train net output #0: loss = 0.00175512 (* 1 = 0.00175512 loss)
I0801 14:19:37.780768 24522 sgd_solver.cpp:136] Iteration 54500, lr = 0.00148437, m = 0.9
I0801 14:19:39.435153 24522 solver.cpp:353] Iteration 54600 (60.4464 iter/s, 1.65436s/100 iter), loss = 0.000470823
I0801 14:19:39.435240 24522 solver.cpp:375]     Train net output #0: loss = 0.000470138 (* 1 = 0.000470138 loss)
I0801 14:19:39.435246 24522 sgd_solver.cpp:136] Iteration 54600, lr = 0.00146875, m = 0.9
I0801 14:19:41.012408 24522 solver.cpp:353] Iteration 54700 (63.4033 iter/s, 1.57721s/100 iter), loss = 0.00411865
I0801 14:19:41.012434 24522 solver.cpp:375]     Train net output #0: loss = 0.00411797 (* 1 = 0.00411797 loss)
I0801 14:19:41.012440 24522 sgd_solver.cpp:136] Iteration 54700, lr = 0.00145312, m = 0.9
I0801 14:19:42.579473 24522 solver.cpp:353] Iteration 54800 (63.8156 iter/s, 1.56702s/100 iter), loss = 0.00133246
I0801 14:19:42.579526 24522 solver.cpp:375]     Train net output #0: loss = 0.00133178 (* 1 = 0.00133178 loss)
I0801 14:19:42.579542 24522 sgd_solver.cpp:136] Iteration 54800, lr = 0.0014375, m = 0.9
I0801 14:19:44.172754 24522 solver.cpp:353] Iteration 54900 (62.7657 iter/s, 1.59323s/100 iter), loss = 0.00110558
I0801 14:19:44.172785 24522 solver.cpp:375]     Train net output #0: loss = 0.00110489 (* 1 = 0.00110489 loss)
I0801 14:19:44.172791 24522 sgd_solver.cpp:136] Iteration 54900, lr = 0.00142187, m = 0.9
I0801 14:19:45.748021 24522 solver.cpp:404] Sparsity after update:
I0801 14:19:45.749553 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:19:45.749563 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:19:45.749572 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:19:45.749577 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:19:45.749580 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:19:45.749584 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:19:45.749588 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:19:45.749593 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:19:45.749598 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:19:45.749601 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:19:45.749605 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:19:45.749609 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:19:45.749613 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:19:45.749624 24522 solver.cpp:550] Iteration 55000, Testing net (#0)
I0801 14:19:46.586872 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.917942
I0801 14:19:46.586891 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 14:19:46.586899 24522 solver.cpp:635]     Test net output #2: loss = 0.347735 (* 1 = 0.347735 loss)
I0801 14:19:46.586915 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.837263s
I0801 14:19:46.602589 24522 solver.cpp:353] Iteration 55000 (41.1562 iter/s, 2.42977s/100 iter), loss = 0.00118939
I0801 14:19:46.602607 24522 solver.cpp:375]     Train net output #0: loss = 0.0011887 (* 1 = 0.0011887 loss)
I0801 14:19:46.602612 24522 sgd_solver.cpp:136] Iteration 55000, lr = 0.00140625, m = 0.9
I0801 14:19:48.258368 24522 solver.cpp:353] Iteration 55100 (60.3966 iter/s, 1.65572s/100 iter), loss = 0.00404965
I0801 14:19:48.258394 24522 solver.cpp:375]     Train net output #0: loss = 0.00404897 (* 1 = 0.00404897 loss)
I0801 14:19:48.258400 24522 sgd_solver.cpp:136] Iteration 55100, lr = 0.00139063, m = 0.9
I0801 14:19:49.955258 24522 solver.cpp:353] Iteration 55200 (58.9332 iter/s, 1.69684s/100 iter), loss = 0.00161667
I0801 14:19:49.955284 24522 solver.cpp:375]     Train net output #0: loss = 0.00161598 (* 1 = 0.00161598 loss)
I0801 14:19:49.955289 24522 sgd_solver.cpp:136] Iteration 55200, lr = 0.001375, m = 0.9
I0801 14:19:51.655750 24522 solver.cpp:353] Iteration 55300 (58.8083 iter/s, 1.70044s/100 iter), loss = 0.000898675
I0801 14:19:51.655773 24522 solver.cpp:375]     Train net output #0: loss = 0.000897992 (* 1 = 0.000897992 loss)
I0801 14:19:51.655778 24522 sgd_solver.cpp:136] Iteration 55300, lr = 0.00135938, m = 0.9
I0801 14:19:53.337860 24522 solver.cpp:353] Iteration 55400 (59.4512 iter/s, 1.68205s/100 iter), loss = 0.000742849
I0801 14:19:53.337906 24522 solver.cpp:375]     Train net output #0: loss = 0.000742167 (* 1 = 0.000742167 loss)
I0801 14:19:53.337945 24522 sgd_solver.cpp:136] Iteration 55400, lr = 0.00134375, m = 0.9
I0801 14:19:55.000679 24522 solver.cpp:353] Iteration 55500 (60.1408 iter/s, 1.66276s/100 iter), loss = 0.000307954
I0801 14:19:55.000792 24522 solver.cpp:375]     Train net output #0: loss = 0.000307272 (* 1 = 0.000307272 loss)
I0801 14:19:55.000830 24522 sgd_solver.cpp:136] Iteration 55500, lr = 0.00132813, m = 0.9
I0801 14:19:56.675070 24522 solver.cpp:353] Iteration 55600 (59.725 iter/s, 1.67434s/100 iter), loss = 0.000522966
I0801 14:19:56.675096 24522 solver.cpp:375]     Train net output #0: loss = 0.000522284 (* 1 = 0.000522284 loss)
I0801 14:19:56.675104 24522 sgd_solver.cpp:136] Iteration 55600, lr = 0.0013125, m = 0.9
I0801 14:19:58.372941 24522 solver.cpp:353] Iteration 55700 (58.8992 iter/s, 1.69782s/100 iter), loss = 0.00158203
I0801 14:19:58.372966 24522 solver.cpp:375]     Train net output #0: loss = 0.00158135 (* 1 = 0.00158135 loss)
I0801 14:19:58.372969 24522 sgd_solver.cpp:136] Iteration 55700, lr = 0.00129687, m = 0.9
I0801 14:20:00.094506 24522 solver.cpp:353] Iteration 55800 (58.0885 iter/s, 1.72151s/100 iter), loss = 0.000438428
I0801 14:20:00.094533 24522 solver.cpp:375]     Train net output #0: loss = 0.000437746 (* 1 = 0.000437746 loss)
I0801 14:20:00.094538 24522 sgd_solver.cpp:136] Iteration 55800, lr = 0.00128125, m = 0.9
I0801 14:20:01.788017 24522 solver.cpp:353] Iteration 55900 (59.0508 iter/s, 1.69346s/100 iter), loss = 0.00079541
I0801 14:20:01.788043 24522 solver.cpp:375]     Train net output #0: loss = 0.000794729 (* 1 = 0.000794729 loss)
I0801 14:20:01.788049 24522 sgd_solver.cpp:136] Iteration 55900, lr = 0.00126562, m = 0.9
I0801 14:20:03.419282 24522 solver.cpp:404] Sparsity after update:
I0801 14:20:03.420864 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:20:03.420873 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:20:03.420881 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:20:03.420882 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:20:03.420884 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:20:03.420886 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:20:03.420888 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:20:03.420891 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:20:03.420892 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:20:03.420895 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:20:03.420897 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:20:03.420900 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:20:03.420902 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:20:03.420909 24522 solver.cpp:550] Iteration 56000, Testing net (#0)
I0801 14:20:04.322536 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.917354
I0801 14:20:04.322571 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 14:20:04.322584 24522 solver.cpp:635]     Test net output #2: loss = 0.359344 (* 1 = 0.359344 loss)
I0801 14:20:04.322623 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.901678s
I0801 14:20:04.341557 24522 solver.cpp:353] Iteration 56000 (39.1625 iter/s, 2.55346s/100 iter), loss = 0.000330347
I0801 14:20:04.341601 24522 solver.cpp:375]     Train net output #0: loss = 0.000329666 (* 1 = 0.000329666 loss)
I0801 14:20:04.341615 24522 sgd_solver.cpp:136] Iteration 56000, lr = 0.00125, m = 0.9
I0801 14:20:05.322149 24487 data_reader.cpp:264] Starting prefetch of epoch 7
I0801 14:20:06.064107 24522 solver.cpp:353] Iteration 56100 (58.0553 iter/s, 1.72249s/100 iter), loss = 0.00223643
I0801 14:20:06.064133 24522 solver.cpp:375]     Train net output #0: loss = 0.00223575 (* 1 = 0.00223575 loss)
I0801 14:20:06.064137 24522 sgd_solver.cpp:136] Iteration 56100, lr = 0.00123438, m = 0.9
I0801 14:20:07.719738 24522 solver.cpp:353] Iteration 56200 (60.4018 iter/s, 1.65558s/100 iter), loss = 0.000612436
I0801 14:20:07.719804 24522 solver.cpp:375]     Train net output #0: loss = 0.000611753 (* 1 = 0.000611753 loss)
I0801 14:20:07.719833 24522 sgd_solver.cpp:136] Iteration 56200, lr = 0.00121875, m = 0.9
I0801 14:20:09.345963 24522 solver.cpp:353] Iteration 56300 (61.494 iter/s, 1.62617s/100 iter), loss = 0.000548362
I0801 14:20:09.346012 24522 solver.cpp:375]     Train net output #0: loss = 0.000547679 (* 1 = 0.000547679 loss)
I0801 14:20:09.346025 24522 sgd_solver.cpp:136] Iteration 56300, lr = 0.00120313, m = 0.9
I0801 14:20:10.947382 24522 solver.cpp:353] Iteration 56400 (62.4468 iter/s, 1.60136s/100 iter), loss = 0.00118859
I0801 14:20:10.947531 24522 solver.cpp:375]     Train net output #0: loss = 0.00118791 (* 1 = 0.00118791 loss)
I0801 14:20:10.947549 24522 sgd_solver.cpp:136] Iteration 56400, lr = 0.0011875, m = 0.9
I0801 14:20:12.621181 24522 solver.cpp:353] Iteration 56500 (59.7462 iter/s, 1.67375s/100 iter), loss = 0.00249184
I0801 14:20:12.621206 24522 solver.cpp:375]     Train net output #0: loss = 0.00249115 (* 1 = 0.00249115 loss)
I0801 14:20:12.621210 24522 sgd_solver.cpp:136] Iteration 56500, lr = 0.00117187, m = 0.9
I0801 14:20:14.317194 24522 solver.cpp:353] Iteration 56600 (58.9636 iter/s, 1.69596s/100 iter), loss = 0.00155856
I0801 14:20:14.317246 24522 solver.cpp:375]     Train net output #0: loss = 0.00155788 (* 1 = 0.00155788 loss)
I0801 14:20:14.317260 24522 sgd_solver.cpp:136] Iteration 56600, lr = 0.00115625, m = 0.9
I0801 14:20:15.986099 24522 solver.cpp:353] Iteration 56700 (59.9215 iter/s, 1.66885s/100 iter), loss = 0.000490599
I0801 14:20:15.986122 24522 solver.cpp:375]     Train net output #0: loss = 0.000489917 (* 1 = 0.000489917 loss)
I0801 14:20:15.986129 24522 sgd_solver.cpp:136] Iteration 56700, lr = 0.00114062, m = 0.9
I0801 14:20:17.740176 24522 solver.cpp:353] Iteration 56800 (57.012 iter/s, 1.75402s/100 iter), loss = 0.000975586
I0801 14:20:17.740217 24522 solver.cpp:375]     Train net output #0: loss = 0.000974904 (* 1 = 0.000974904 loss)
I0801 14:20:17.740228 24522 sgd_solver.cpp:136] Iteration 56800, lr = 0.001125, m = 0.9
I0801 14:20:19.365447 24522 solver.cpp:353] Iteration 56900 (61.53 iter/s, 1.62522s/100 iter), loss = 0.000540173
I0801 14:20:19.365469 24522 solver.cpp:375]     Train net output #0: loss = 0.000539492 (* 1 = 0.000539492 loss)
I0801 14:20:19.365473 24522 sgd_solver.cpp:136] Iteration 56900, lr = 0.00110937, m = 0.9
I0801 14:20:20.964462 24522 solver.cpp:404] Sparsity after update:
I0801 14:20:20.966892 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:20:20.966908 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:20:20.966928 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:20:20.966936 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:20:20.966945 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:20:20.966953 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:20:20.966962 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:20:20.966969 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:20:20.966977 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:20:20.966985 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:20:20.966995 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:20:20.967005 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:20:20.967015 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:20:20.967042 24522 solver.cpp:550] Iteration 57000, Testing net (#0)
I0801 14:20:21.858446 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.909707
I0801 14:20:21.858465 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996471
I0801 14:20:21.858472 24522 solver.cpp:635]     Test net output #2: loss = 0.382904 (* 1 = 0.382904 loss)
I0801 14:20:21.858494 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.891422s
I0801 14:20:21.877668 24522 solver.cpp:353] Iteration 57000 (39.8066 iter/s, 2.51215s/100 iter), loss = 0.00109887
I0801 14:20:21.877694 24522 solver.cpp:375]     Train net output #0: loss = 0.00109819 (* 1 = 0.00109819 loss)
I0801 14:20:21.877699 24522 sgd_solver.cpp:136] Iteration 57000, lr = 0.00109375, m = 0.9
I0801 14:20:23.558136 24522 solver.cpp:353] Iteration 57100 (59.5093 iter/s, 1.68041s/100 iter), loss = 0.000436448
I0801 14:20:23.558209 24522 solver.cpp:375]     Train net output #0: loss = 0.000435767 (* 1 = 0.000435767 loss)
I0801 14:20:23.558226 24522 sgd_solver.cpp:136] Iteration 57100, lr = 0.00107813, m = 0.9
I0801 14:20:25.293696 24522 solver.cpp:353] Iteration 57200 (57.6202 iter/s, 1.7355s/100 iter), loss = 0.00138693
I0801 14:20:25.293834 24522 solver.cpp:375]     Train net output #0: loss = 0.00138625 (* 1 = 0.00138625 loss)
I0801 14:20:25.295370 24522 sgd_solver.cpp:136] Iteration 57200, lr = 0.0010625, m = 0.9
I0801 14:20:26.901485 24522 solver.cpp:353] Iteration 57300 (62.1994 iter/s, 1.60773s/100 iter), loss = 0.000443452
I0801 14:20:26.901685 24522 solver.cpp:375]     Train net output #0: loss = 0.00044277 (* 1 = 0.00044277 loss)
I0801 14:20:26.901692 24522 sgd_solver.cpp:136] Iteration 57300, lr = 0.00104688, m = 0.9
I0801 14:20:28.579989 24522 solver.cpp:353] Iteration 57400 (59.5785 iter/s, 1.67846s/100 iter), loss = 0.00109717
I0801 14:20:28.580036 24522 solver.cpp:375]     Train net output #0: loss = 0.00109649 (* 1 = 0.00109649 loss)
I0801 14:20:28.580049 24522 sgd_solver.cpp:136] Iteration 57400, lr = 0.00103125, m = 0.9
I0801 14:20:30.251385 24522 solver.cpp:353] Iteration 57500 (59.8322 iter/s, 1.67134s/100 iter), loss = 0.000879905
I0801 14:20:30.251412 24522 solver.cpp:375]     Train net output #0: loss = 0.000879221 (* 1 = 0.000879221 loss)
I0801 14:20:30.251416 24522 sgd_solver.cpp:136] Iteration 57500, lr = 0.00101562, m = 0.9
I0801 14:20:32.012910 24522 solver.cpp:353] Iteration 57600 (56.771 iter/s, 1.76146s/100 iter), loss = 0.000840449
I0801 14:20:32.013165 24522 solver.cpp:375]     Train net output #0: loss = 0.000839764 (* 1 = 0.000839764 loss)
I0801 14:20:32.013281 24522 sgd_solver.cpp:136] Iteration 57600, lr = 0.001, m = 0.9
I0801 14:20:33.610136 24522 solver.cpp:353] Iteration 57700 (62.6106 iter/s, 1.59717s/100 iter), loss = 0.000397246
I0801 14:20:33.610167 24522 solver.cpp:375]     Train net output #0: loss = 0.000396562 (* 1 = 0.000396562 loss)
I0801 14:20:33.610174 24522 sgd_solver.cpp:136] Iteration 57700, lr = 0.000984375, m = 0.9
I0801 14:20:35.386374 24522 solver.cpp:353] Iteration 57800 (56.3005 iter/s, 1.77618s/100 iter), loss = 0.00046873
I0801 14:20:35.386401 24522 solver.cpp:375]     Train net output #0: loss = 0.000468048 (* 1 = 0.000468048 loss)
I0801 14:20:35.386406 24522 sgd_solver.cpp:136] Iteration 57800, lr = 0.00096875, m = 0.9
I0801 14:20:37.005722 24522 solver.cpp:353] Iteration 57900 (61.7552 iter/s, 1.6193s/100 iter), loss = 0.00146076
I0801 14:20:37.005745 24522 solver.cpp:375]     Train net output #0: loss = 0.00146008 (* 1 = 0.00146008 loss)
I0801 14:20:37.005751 24522 sgd_solver.cpp:136] Iteration 57900, lr = 0.000953125, m = 0.9
I0801 14:20:38.671591 24522 solver.cpp:404] Sparsity after update:
I0801 14:20:38.673188 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:20:38.673195 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:20:38.673203 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:20:38.673207 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:20:38.673209 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:20:38.673211 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:20:38.673214 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:20:38.673216 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:20:38.673218 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:20:38.673220 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:20:38.673223 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:20:38.673225 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:20:38.673228 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:20:38.673235 24522 solver.cpp:550] Iteration 58000, Testing net (#0)
I0801 14:20:39.501029 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.897648
I0801 14:20:39.501047 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.997059
I0801 14:20:39.501051 24522 solver.cpp:635]     Test net output #2: loss = 0.41192 (* 1 = 0.41192 loss)
I0801 14:20:39.501067 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.827804s
I0801 14:20:39.516733 24522 solver.cpp:353] Iteration 58000 (39.8258 iter/s, 2.51094s/100 iter), loss = 0.00284762
I0801 14:20:39.516751 24522 solver.cpp:375]     Train net output #0: loss = 0.00284694 (* 1 = 0.00284694 loss)
I0801 14:20:39.516754 24522 sgd_solver.cpp:136] Iteration 58000, lr = 0.0009375, m = 0.9
I0801 14:20:41.090122 24522 solver.cpp:353] Iteration 58100 (63.5591 iter/s, 1.57334s/100 iter), loss = 0.00377831
I0801 14:20:41.090188 24522 solver.cpp:375]     Train net output #0: loss = 0.00377763 (* 1 = 0.00377763 loss)
I0801 14:20:41.090195 24522 sgd_solver.cpp:136] Iteration 58100, lr = 0.000921875, m = 0.9
I0801 14:20:42.680183 24522 solver.cpp:353] Iteration 58200 (62.8927 iter/s, 1.59001s/100 iter), loss = 0.000217579
I0801 14:20:42.680208 24522 solver.cpp:375]     Train net output #0: loss = 0.000216897 (* 1 = 0.000216897 loss)
I0801 14:20:42.680213 24522 sgd_solver.cpp:136] Iteration 58200, lr = 0.00090625, m = 0.9
I0801 14:20:44.252410 24522 solver.cpp:353] Iteration 58300 (63.6062 iter/s, 1.57217s/100 iter), loss = 0.000764377
I0801 14:20:44.252463 24522 solver.cpp:375]     Train net output #0: loss = 0.000763695 (* 1 = 0.000763695 loss)
I0801 14:20:44.252477 24522 sgd_solver.cpp:136] Iteration 58300, lr = 0.000890625, m = 0.9
I0801 14:20:45.945662 24522 solver.cpp:353] Iteration 58400 (59.0597 iter/s, 1.6932s/100 iter), loss = 0.000379177
I0801 14:20:45.945690 24522 solver.cpp:375]     Train net output #0: loss = 0.000378496 (* 1 = 0.000378496 loss)
I0801 14:20:45.945695 24522 sgd_solver.cpp:136] Iteration 58400, lr = 0.000875, m = 0.9
I0801 14:20:47.577828 24522 solver.cpp:353] Iteration 58500 (61.2703 iter/s, 1.63211s/100 iter), loss = 0.00354146
I0801 14:20:47.577859 24522 solver.cpp:375]     Train net output #0: loss = 0.00354077 (* 1 = 0.00354077 loss)
I0801 14:20:47.577865 24522 sgd_solver.cpp:136] Iteration 58500, lr = 0.000859375, m = 0.9
I0801 14:20:49.236249 24522 solver.cpp:353] Iteration 58600 (60.3003 iter/s, 1.65837s/100 iter), loss = 0.000801043
I0801 14:20:49.236280 24522 solver.cpp:375]     Train net output #0: loss = 0.000800363 (* 1 = 0.000800363 loss)
I0801 14:20:49.236284 24522 sgd_solver.cpp:136] Iteration 58600, lr = 0.00084375, m = 0.9
I0801 14:20:50.819182 24522 solver.cpp:353] Iteration 58700 (63.1758 iter/s, 1.58288s/100 iter), loss = 0.000514017
I0801 14:20:50.819232 24522 solver.cpp:375]     Train net output #0: loss = 0.000513336 (* 1 = 0.000513336 loss)
I0801 14:20:50.819244 24522 sgd_solver.cpp:136] Iteration 58700, lr = 0.000828125, m = 0.9
I0801 14:20:52.404875 24522 solver.cpp:353] Iteration 58800 (63.0659 iter/s, 1.58564s/100 iter), loss = 0.00201116
I0801 14:20:52.404913 24522 solver.cpp:375]     Train net output #0: loss = 0.00201048 (* 1 = 0.00201048 loss)
I0801 14:20:52.404924 24522 sgd_solver.cpp:136] Iteration 58800, lr = 0.0008125, m = 0.9
I0801 14:20:53.983258 24522 solver.cpp:353] Iteration 58900 (63.3581 iter/s, 1.57833s/100 iter), loss = 0.000969029
I0801 14:20:53.983283 24522 solver.cpp:375]     Train net output #0: loss = 0.000968348 (* 1 = 0.000968348 loss)
I0801 14:20:53.983286 24522 sgd_solver.cpp:136] Iteration 58900, lr = 0.000796875, m = 0.9
I0801 14:20:55.605139 24522 solver.cpp:404] Sparsity after update:
I0801 14:20:55.606736 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:20:55.606745 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:20:55.606751 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:20:55.606755 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:20:55.606756 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:20:55.606760 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:20:55.606761 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:20:55.606763 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:20:55.606765 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:20:55.606767 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:20:55.606770 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:20:55.606772 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:20:55.606775 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:20:55.606782 24522 solver.cpp:550] Iteration 59000, Testing net (#0)
I0801 14:20:56.419036 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.895883
I0801 14:20:56.419055 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.996765
I0801 14:20:56.419060 24522 solver.cpp:635]     Test net output #2: loss = 0.410575 (* 1 = 0.410575 loss)
I0801 14:20:56.419090 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.812279s
I0801 14:20:56.434784 24522 solver.cpp:353] Iteration 59000 (40.7921 iter/s, 2.45146s/100 iter), loss = 0.000709541
I0801 14:20:56.434801 24522 solver.cpp:375]     Train net output #0: loss = 0.000708861 (* 1 = 0.000708861 loss)
I0801 14:20:56.434808 24522 sgd_solver.cpp:136] Iteration 59000, lr = 0.00078125, m = 0.9
I0801 14:20:58.121834 24522 solver.cpp:353] Iteration 59100 (59.2771 iter/s, 1.68699s/100 iter), loss = 0.000790055
I0801 14:20:58.121858 24522 solver.cpp:375]     Train net output #0: loss = 0.000789374 (* 1 = 0.000789374 loss)
I0801 14:20:58.121863 24522 sgd_solver.cpp:136] Iteration 59100, lr = 0.000765625, m = 0.9
I0801 14:20:59.785359 24522 solver.cpp:353] Iteration 59200 (60.1152 iter/s, 1.66347s/100 iter), loss = 0.00106546
I0801 14:20:59.785383 24522 solver.cpp:375]     Train net output #0: loss = 0.00106478 (* 1 = 0.00106478 loss)
I0801 14:20:59.785389 24522 sgd_solver.cpp:136] Iteration 59200, lr = 0.00075, m = 0.9
I0801 14:21:01.467123 24522 solver.cpp:353] Iteration 59300 (59.4633 iter/s, 1.68171s/100 iter), loss = 0.00101248
I0801 14:21:01.467147 24522 solver.cpp:375]     Train net output #0: loss = 0.0010118 (* 1 = 0.0010118 loss)
I0801 14:21:01.467152 24522 sgd_solver.cpp:136] Iteration 59300, lr = 0.000734375, m = 0.9
I0801 14:21:03.068954 24522 solver.cpp:353] Iteration 59400 (62.4304 iter/s, 1.60178s/100 iter), loss = 0.00226807
I0801 14:21:03.068979 24522 solver.cpp:375]     Train net output #0: loss = 0.00226739 (* 1 = 0.00226739 loss)
I0801 14:21:03.068984 24522 sgd_solver.cpp:136] Iteration 59400, lr = 0.00071875, m = 0.9
I0801 14:21:04.765573 24522 solver.cpp:353] Iteration 59500 (58.9427 iter/s, 1.69656s/100 iter), loss = 0.000377451
I0801 14:21:04.765630 24522 solver.cpp:375]     Train net output #0: loss = 0.000376768 (* 1 = 0.000376768 loss)
I0801 14:21:04.765645 24522 sgd_solver.cpp:136] Iteration 59500, lr = 0.000703125, m = 0.9
I0801 14:21:06.430711 24522 solver.cpp:353] Iteration 59600 (60.057 iter/s, 1.66508s/100 iter), loss = 0.00220853
I0801 14:21:06.430733 24522 solver.cpp:375]     Train net output #0: loss = 0.00220785 (* 1 = 0.00220785 loss)
I0801 14:21:06.430737 24522 sgd_solver.cpp:136] Iteration 59600, lr = 0.0006875, m = 0.9
I0801 14:21:08.018141 24522 solver.cpp:353] Iteration 59700 (62.997 iter/s, 1.58738s/100 iter), loss = 0.00176195
I0801 14:21:08.018220 24522 solver.cpp:375]     Train net output #0: loss = 0.00176127 (* 1 = 0.00176127 loss)
I0801 14:21:08.018241 24522 sgd_solver.cpp:136] Iteration 59700, lr = 0.000671875, m = 0.9
I0801 14:21:09.731151 24522 solver.cpp:353] Iteration 59800 (58.3785 iter/s, 1.71296s/100 iter), loss = 0.000802004
I0801 14:21:09.731176 24522 solver.cpp:375]     Train net output #0: loss = 0.000801322 (* 1 = 0.000801322 loss)
I0801 14:21:09.731182 24522 sgd_solver.cpp:136] Iteration 59800, lr = 0.00065625, m = 0.9
I0801 14:21:11.339314 24522 solver.cpp:353] Iteration 59900 (62.1849 iter/s, 1.60811s/100 iter), loss = 0.000409011
I0801 14:21:11.339417 24522 solver.cpp:375]     Train net output #0: loss = 0.000408328 (* 1 = 0.000408328 loss)
I0801 14:21:11.339424 24522 sgd_solver.cpp:136] Iteration 59900, lr = 0.000640625, m = 0.9
I0801 14:21:12.972980 24522 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_60000.caffemodel
I0801 14:21:12.981503 24522 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_60000.solverstate
I0801 14:21:12.985080 24522 solver.cpp:404] Sparsity after update:
I0801 14:21:12.986915 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:21:12.986924 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:21:12.986932 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:21:12.986933 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:21:12.986937 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:21:12.986940 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:21:12.986943 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:21:12.986944 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:21:12.986946 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:21:12.986948 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:21:12.986949 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:21:12.986951 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:21:12.986953 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:21:12.986963 24522 solver.cpp:550] Iteration 60000, Testing net (#0)
I0801 14:21:13.790906 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.892942
I0801 14:21:13.790925 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995882
I0801 14:21:13.790930 24522 solver.cpp:635]     Test net output #2: loss = 0.442116 (* 1 = 0.442116 loss)
I0801 14:21:13.790946 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.803956s
I0801 14:21:13.806895 24522 solver.cpp:353] Iteration 60000 (40.5267 iter/s, 2.46751s/100 iter), loss = 0.000898302
I0801 14:21:13.806912 24522 solver.cpp:375]     Train net output #0: loss = 0.00089762 (* 1 = 0.00089762 loss)
I0801 14:21:13.806916 24522 sgd_solver.cpp:136] Iteration 60000, lr = 0.000625, m = 0.9
I0801 14:21:15.464609 24522 solver.cpp:353] Iteration 60100 (60.3261 iter/s, 1.65766s/100 iter), loss = 0.000589767
I0801 14:21:15.464669 24522 solver.cpp:375]     Train net output #0: loss = 0.000589085 (* 1 = 0.000589085 loss)
I0801 14:21:15.464682 24522 sgd_solver.cpp:136] Iteration 60100, lr = 0.000609375, m = 0.9
I0801 14:21:17.127920 24522 solver.cpp:353] Iteration 60200 (60.1229 iter/s, 1.66326s/100 iter), loss = 0.0017615
I0801 14:21:17.127944 24522 solver.cpp:375]     Train net output #0: loss = 0.00176082 (* 1 = 0.00176082 loss)
I0801 14:21:17.127948 24522 sgd_solver.cpp:136] Iteration 60200, lr = 0.00059375, m = 0.9
I0801 14:21:18.725980 24522 solver.cpp:353] Iteration 60300 (62.5781 iter/s, 1.598s/100 iter), loss = 0.000648605
I0801 14:21:18.726161 24522 solver.cpp:375]     Train net output #0: loss = 0.00064792 (* 1 = 0.00064792 loss)
I0801 14:21:18.726184 24522 sgd_solver.cpp:136] Iteration 60300, lr = 0.000578125, m = 0.9
I0801 14:21:20.466401 24522 solver.cpp:353] Iteration 60400 (57.4593 iter/s, 1.74036s/100 iter), loss = 0.000894886
I0801 14:21:20.466457 24522 solver.cpp:375]     Train net output #0: loss = 0.000894202 (* 1 = 0.000894202 loss)
I0801 14:21:20.466472 24522 sgd_solver.cpp:136] Iteration 60400, lr = 0.0005625, m = 0.9
I0801 14:21:22.125368 24522 solver.cpp:353] Iteration 60500 (60.2806 iter/s, 1.65891s/100 iter), loss = 0.00153954
I0801 14:21:22.125485 24522 solver.cpp:375]     Train net output #0: loss = 0.00153885 (* 1 = 0.00153885 loss)
I0801 14:21:22.125517 24522 sgd_solver.cpp:136] Iteration 60500, lr = 0.000546875, m = 0.9
I0801 14:21:23.722739 24522 solver.cpp:353] Iteration 60600 (62.6047 iter/s, 1.59733s/100 iter), loss = 0.00140742
I0801 14:21:23.722801 24522 solver.cpp:375]     Train net output #0: loss = 0.00140673 (* 1 = 0.00140673 loss)
I0801 14:21:23.722816 24522 sgd_solver.cpp:136] Iteration 60600, lr = 0.00053125, m = 0.9
I0801 14:21:23.769779 24487 data_reader.cpp:264] Starting prefetch of epoch 8
I0801 14:21:25.420588 24522 solver.cpp:353] Iteration 60700 (58.9001 iter/s, 1.69779s/100 iter), loss = 0.00115334
I0801 14:21:25.420677 24522 solver.cpp:375]     Train net output #0: loss = 0.00115265 (* 1 = 0.00115265 loss)
I0801 14:21:25.420704 24522 sgd_solver.cpp:136] Iteration 60700, lr = 0.000515625, m = 0.9
I0801 14:21:27.021435 24522 solver.cpp:353] Iteration 60800 (62.4689 iter/s, 1.6008s/100 iter), loss = 0.00300468
I0801 14:21:27.021526 24522 solver.cpp:375]     Train net output #0: loss = 0.003004 (* 1 = 0.003004 loss)
I0801 14:21:27.021546 24522 sgd_solver.cpp:136] Iteration 60800, lr = 0.0005, m = 0.9
I0801 14:21:28.692592 24522 solver.cpp:353] Iteration 60900 (59.8406 iter/s, 1.67111s/100 iter), loss = 0.000379021
I0801 14:21:28.692615 24522 solver.cpp:375]     Train net output #0: loss = 0.000378337 (* 1 = 0.000378337 loss)
I0801 14:21:28.692621 24522 sgd_solver.cpp:136] Iteration 60900, lr = 0.000484375, m = 0.9
I0801 14:21:30.390024 24522 solver.cpp:404] Sparsity after update:
I0801 14:21:30.391685 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:21:30.391695 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:21:30.391703 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:21:30.391717 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:21:30.391722 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:21:30.391732 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:21:30.391737 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:21:30.391741 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:21:30.391748 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:21:30.391753 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:21:30.391757 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:21:30.391764 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:21:30.391769 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:21:30.391793 24522 solver.cpp:550] Iteration 61000, Testing net (#0)
I0801 14:21:31.211928 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.894119
I0801 14:21:31.211947 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995294
I0801 14:21:31.211953 24522 solver.cpp:635]     Test net output #2: loss = 0.431654 (* 1 = 0.431654 loss)
I0801 14:21:31.211971 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.820149s
I0801 14:21:31.227753 24522 solver.cpp:353] Iteration 61000 (39.4464 iter/s, 2.53509s/100 iter), loss = 0.00122136
I0801 14:21:31.227771 24522 solver.cpp:375]     Train net output #0: loss = 0.00122068 (* 1 = 0.00122068 loss)
I0801 14:21:31.227776 24522 sgd_solver.cpp:136] Iteration 61000, lr = 0.00046875, m = 0.9
I0801 14:21:32.907415 24522 solver.cpp:353] Iteration 61100 (59.5378 iter/s, 1.67961s/100 iter), loss = 0.000702031
I0801 14:21:32.907441 24522 solver.cpp:375]     Train net output #0: loss = 0.000701346 (* 1 = 0.000701346 loss)
I0801 14:21:32.907446 24522 sgd_solver.cpp:136] Iteration 61100, lr = 0.000453125, m = 0.9
I0801 14:21:34.541358 24522 solver.cpp:353] Iteration 61200 (61.2037 iter/s, 1.63389s/100 iter), loss = 0.000449848
I0801 14:21:34.541404 24522 solver.cpp:375]     Train net output #0: loss = 0.000449165 (* 1 = 0.000449165 loss)
I0801 14:21:34.541424 24522 sgd_solver.cpp:136] Iteration 61200, lr = 0.0004375, m = 0.9
I0801 14:21:36.195171 24522 solver.cpp:353] Iteration 61300 (60.4683 iter/s, 1.65376s/100 iter), loss = 0.00162333
I0801 14:21:36.195209 24522 solver.cpp:375]     Train net output #0: loss = 0.00162264 (* 1 = 0.00162264 loss)
I0801 14:21:36.195231 24522 sgd_solver.cpp:136] Iteration 61300, lr = 0.000421875, m = 0.9
I0801 14:21:37.822137 24522 solver.cpp:353] Iteration 61400 (61.4661 iter/s, 1.62691s/100 iter), loss = 0.00137754
I0801 14:21:37.822182 24522 solver.cpp:375]     Train net output #0: loss = 0.00137685 (* 1 = 0.00137685 loss)
I0801 14:21:37.822188 24522 sgd_solver.cpp:136] Iteration 61400, lr = 0.00040625, m = 0.9
I0801 14:21:39.509935 24522 solver.cpp:353] Iteration 61500 (59.2506 iter/s, 1.68775s/100 iter), loss = 0.00248597
I0801 14:21:39.509984 24522 solver.cpp:375]     Train net output #0: loss = 0.00248529 (* 1 = 0.00248529 loss)
I0801 14:21:39.509996 24522 sgd_solver.cpp:136] Iteration 61500, lr = 0.000390625, m = 0.9
I0801 14:21:41.103821 24522 solver.cpp:353] Iteration 61600 (62.7421 iter/s, 1.59383s/100 iter), loss = 0.000967365
I0801 14:21:41.103886 24522 solver.cpp:375]     Train net output #0: loss = 0.00096668 (* 1 = 0.00096668 loss)
I0801 14:21:41.103905 24522 sgd_solver.cpp:136] Iteration 61600, lr = 0.000375, m = 0.9
I0801 14:21:42.763725 24522 solver.cpp:353] Iteration 61700 (60.2462 iter/s, 1.65985s/100 iter), loss = 0.000702511
I0801 14:21:42.763800 24522 solver.cpp:375]     Train net output #0: loss = 0.000701827 (* 1 = 0.000701827 loss)
I0801 14:21:42.763808 24522 sgd_solver.cpp:136] Iteration 61700, lr = 0.000359375, m = 0.9
I0801 14:21:44.493425 24522 solver.cpp:353] Iteration 61800 (57.8153 iter/s, 1.72965s/100 iter), loss = 0.000503246
I0801 14:21:44.493474 24522 solver.cpp:375]     Train net output #0: loss = 0.000502562 (* 1 = 0.000502562 loss)
I0801 14:21:44.493485 24522 sgd_solver.cpp:136] Iteration 61800, lr = 0.00034375, m = 0.9
I0801 14:21:46.206616 24522 solver.cpp:353] Iteration 61900 (58.3726 iter/s, 1.71313s/100 iter), loss = 0.001723
I0801 14:21:46.206647 24522 solver.cpp:375]     Train net output #0: loss = 0.00172231 (* 1 = 0.00172231 loss)
I0801 14:21:46.206655 24522 sgd_solver.cpp:136] Iteration 61900, lr = 0.000328125, m = 0.9
I0801 14:21:47.880311 24522 solver.cpp:404] Sparsity after update:
I0801 14:21:47.882267 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:21:47.882282 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:21:47.882304 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:21:47.882311 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:21:47.882316 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:21:47.882321 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:21:47.882325 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:21:47.882329 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:21:47.882333 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:21:47.882338 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:21:47.882342 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:21:47.882354 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:21:47.882360 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:21:47.882375 24522 solver.cpp:550] Iteration 62000, Testing net (#0)
I0801 14:21:48.691216 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.895295
I0801 14:21:48.691234 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995
I0801 14:21:48.691239 24522 solver.cpp:635]     Test net output #2: loss = 0.44996 (* 1 = 0.44996 loss)
I0801 14:21:48.691254 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.808852s
I0801 14:21:48.710692 24522 solver.cpp:353] Iteration 62000 (39.9361 iter/s, 2.504s/100 iter), loss = 0.000290017
I0801 14:21:48.710713 24522 solver.cpp:375]     Train net output #0: loss = 0.000289332 (* 1 = 0.000289332 loss)
I0801 14:21:48.710719 24522 sgd_solver.cpp:136] Iteration 62000, lr = 0.0003125, m = 0.9
I0801 14:21:50.392333 24522 solver.cpp:353] Iteration 62100 (59.4677 iter/s, 1.68159s/100 iter), loss = 0.00163699
I0801 14:21:50.392361 24522 solver.cpp:375]     Train net output #0: loss = 0.00163631 (* 1 = 0.00163631 loss)
I0801 14:21:50.392366 24522 sgd_solver.cpp:136] Iteration 62100, lr = 0.000296875, m = 0.9
I0801 14:21:51.976951 24522 solver.cpp:353] Iteration 62200 (63.1087 iter/s, 1.58457s/100 iter), loss = 0.000902461
I0801 14:21:51.976979 24522 solver.cpp:375]     Train net output #0: loss = 0.000901777 (* 1 = 0.000901777 loss)
I0801 14:21:51.976985 24522 sgd_solver.cpp:136] Iteration 62200, lr = 0.00028125, m = 0.9
I0801 14:21:53.557356 24522 solver.cpp:353] Iteration 62300 (63.2769 iter/s, 1.58036s/100 iter), loss = 0.00104048
I0801 14:21:53.557382 24522 solver.cpp:375]     Train net output #0: loss = 0.0010398 (* 1 = 0.0010398 loss)
I0801 14:21:53.557387 24522 sgd_solver.cpp:136] Iteration 62300, lr = 0.000265625, m = 0.9
I0801 14:21:55.168427 24522 solver.cpp:353] Iteration 62400 (62.0725 iter/s, 1.61102s/100 iter), loss = 0.000764285
I0801 14:21:55.168454 24522 solver.cpp:375]     Train net output #0: loss = 0.000763599 (* 1 = 0.000763599 loss)
I0801 14:21:55.168462 24522 sgd_solver.cpp:136] Iteration 62400, lr = 0.00025, m = 0.9
I0801 14:21:56.787053 24522 solver.cpp:353] Iteration 62500 (61.7828 iter/s, 1.61857s/100 iter), loss = 0.000269326
I0801 14:21:56.787077 24522 solver.cpp:375]     Train net output #0: loss = 0.000268639 (* 1 = 0.000268639 loss)
I0801 14:21:56.787099 24522 sgd_solver.cpp:136] Iteration 62500, lr = 0.000234375, m = 0.9
I0801 14:21:58.486905 24522 solver.cpp:353] Iteration 62600 (58.8304 iter/s, 1.6998s/100 iter), loss = 0.00164891
I0801 14:21:58.486932 24522 solver.cpp:375]     Train net output #0: loss = 0.00164823 (* 1 = 0.00164823 loss)
I0801 14:21:58.486937 24522 sgd_solver.cpp:136] Iteration 62600, lr = 0.00021875, m = 0.9
I0801 14:22:00.092016 24522 solver.cpp:353] Iteration 62700 (62.303 iter/s, 1.60506s/100 iter), loss = 0.000908071
I0801 14:22:00.092042 24522 solver.cpp:375]     Train net output #0: loss = 0.000907384 (* 1 = 0.000907384 loss)
I0801 14:22:00.092047 24522 sgd_solver.cpp:136] Iteration 62700, lr = 0.000203125, m = 0.9
I0801 14:22:01.663326 24522 solver.cpp:353] Iteration 62800 (63.6431 iter/s, 1.57126s/100 iter), loss = 0.00151747
I0801 14:22:01.663390 24522 solver.cpp:375]     Train net output #0: loss = 0.00151678 (* 1 = 0.00151678 loss)
I0801 14:22:01.663409 24522 sgd_solver.cpp:136] Iteration 62800, lr = 0.0001875, m = 0.9
I0801 14:22:03.249698 24522 solver.cpp:353] Iteration 62900 (63.0389 iter/s, 1.58632s/100 iter), loss = 0.00072543
I0801 14:22:03.249725 24522 solver.cpp:375]     Train net output #0: loss = 0.000724745 (* 1 = 0.000724745 loss)
I0801 14:22:03.249732 24522 sgd_solver.cpp:136] Iteration 62900, lr = 0.000171875, m = 0.9
I0801 14:22:04.825531 24522 solver.cpp:404] Sparsity after update:
I0801 14:22:04.827145 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:22:04.827154 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:22:04.827162 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:22:04.827167 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:22:04.827174 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:22:04.827179 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:22:04.827185 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:22:04.827190 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:22:04.827195 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:22:04.827199 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:22:04.827203 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:22:04.827208 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:22:04.827211 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:22:04.827222 24522 solver.cpp:550] Iteration 63000, Testing net (#0)
I0801 14:22:05.643880 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.900883
I0801 14:22:05.643899 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.995
I0801 14:22:05.643906 24522 solver.cpp:635]     Test net output #2: loss = 0.442553 (* 1 = 0.442553 loss)
I0801 14:22:05.643924 24522 solver.cpp:305] [MultiGPU] Tests completed in 0.816674s
I0801 14:22:05.660951 24522 solver.cpp:353] Iteration 63000 (41.4735 iter/s, 2.41118s/100 iter), loss = 0.00178825
I0801 14:22:05.660981 24522 solver.cpp:375]     Train net output #0: loss = 0.00178757 (* 1 = 0.00178757 loss)
I0801 14:22:05.660992 24522 sgd_solver.cpp:136] Iteration 63000, lr = 0.00015625, m = 0.9
I0801 14:22:07.280298 24522 solver.cpp:353] Iteration 63100 (61.7554 iter/s, 1.61929s/100 iter), loss = 0.00159019
I0801 14:22:07.280323 24522 solver.cpp:375]     Train net output #0: loss = 0.00158951 (* 1 = 0.00158951 loss)
I0801 14:22:07.280328 24522 sgd_solver.cpp:136] Iteration 63100, lr = 0.000140625, m = 0.9
I0801 14:22:09.003165 24522 solver.cpp:353] Iteration 63200 (58.0446 iter/s, 1.72281s/100 iter), loss = 0.000952651
I0801 14:22:09.003186 24522 solver.cpp:375]     Train net output #0: loss = 0.000951967 (* 1 = 0.000951967 loss)
I0801 14:22:09.003190 24522 sgd_solver.cpp:136] Iteration 63200, lr = 0.000125, m = 0.9
I0801 14:22:10.647856 24522 solver.cpp:353] Iteration 63300 (60.8036 iter/s, 1.64464s/100 iter), loss = 0.00199844
I0801 14:22:10.647881 24522 solver.cpp:375]     Train net output #0: loss = 0.00199775 (* 1 = 0.00199775 loss)
I0801 14:22:10.647886 24522 sgd_solver.cpp:136] Iteration 63300, lr = 0.000109375, m = 0.9
I0801 14:22:12.386957 24522 solver.cpp:353] Iteration 63400 (57.503 iter/s, 1.73904s/100 iter), loss = 0.000258135
I0801 14:22:12.387025 24522 solver.cpp:375]     Train net output #0: loss = 0.000257451 (* 1 = 0.000257451 loss)
I0801 14:22:12.387042 24522 sgd_solver.cpp:136] Iteration 63400, lr = 9.37498e-05, m = 0.9
I0801 14:22:14.095674 24522 solver.cpp:353] Iteration 63500 (58.5252 iter/s, 1.70866s/100 iter), loss = 0.000511712
I0801 14:22:14.095752 24522 solver.cpp:375]     Train net output #0: loss = 0.000511028 (* 1 = 0.000511028 loss)
I0801 14:22:14.095757 24522 sgd_solver.cpp:136] Iteration 63500, lr = 7.8125e-05, m = 0.9
I0801 14:22:15.673202 24522 solver.cpp:353] Iteration 63600 (63.3924 iter/s, 1.57748s/100 iter), loss = 0.000551771
I0801 14:22:15.673252 24522 solver.cpp:375]     Train net output #0: loss = 0.000551087 (* 1 = 0.000551087 loss)
I0801 14:22:15.673266 24522 sgd_solver.cpp:136] Iteration 63600, lr = 6.25002e-05, m = 0.9
I0801 14:22:17.371356 24522 solver.cpp:353] Iteration 63700 (58.8893 iter/s, 1.6981s/100 iter), loss = 0.000419886
I0801 14:22:17.371381 24522 solver.cpp:375]     Train net output #0: loss = 0.000419201 (* 1 = 0.000419201 loss)
I0801 14:22:17.371386 24522 sgd_solver.cpp:136] Iteration 63700, lr = 4.68749e-05, m = 0.9
I0801 14:22:19.045405 24522 solver.cpp:353] Iteration 63800 (59.7373 iter/s, 1.674s/100 iter), loss = 0.000453459
I0801 14:22:19.045430 24522 solver.cpp:375]     Train net output #0: loss = 0.000452775 (* 1 = 0.000452775 loss)
I0801 14:22:19.045435 24522 sgd_solver.cpp:136] Iteration 63800, lr = 3.12501e-05, m = 0.9
I0801 14:22:20.723067 24522 solver.cpp:353] Iteration 63900 (59.609 iter/s, 1.6776s/100 iter), loss = 0.00280178
I0801 14:22:20.723110 24522 solver.cpp:375]     Train net output #0: loss = 0.00280109 (* 1 = 0.00280109 loss)
I0801 14:22:20.723122 24522 sgd_solver.cpp:136] Iteration 63900, lr = 1.56248e-05, m = 0.9
I0801 14:22:22.356325 24522 solver.cpp:353] Iteration 63999 (60.6175 iter/s, 1.63319s/99 iter), loss = 0.000880025
I0801 14:22:22.356400 24522 solver.cpp:375]     Train net output #0: loss = 0.000879341 (* 1 = 0.000879341 loss)
I0801 14:22:22.356412 24522 solver.cpp:404] Sparsity after update:
I0801 14:22:22.358965 24522 net.cpp:2261] Num Params(11), Sparsity (zero_weights/count): 
I0801 14:22:22.358976 24522 net.cpp:2270] conv1a_param_0(0.342) 
I0801 14:22:22.358989 24522 net.cpp:2270] conv1b_param_0(0.759) 
I0801 14:22:22.358996 24522 net.cpp:2270] fc10_param_0(0) 
I0801 14:22:22.359004 24522 net.cpp:2270] res2a_branch2a_param_0(0.811) 
I0801 14:22:22.359011 24522 net.cpp:2270] res2a_branch2b_param_0(0.693) 
I0801 14:22:22.359019 24522 net.cpp:2270] res3a_branch2a_param_0(0.809) 
I0801 14:22:22.359026 24522 net.cpp:2270] res3a_branch2b_param_0(0.755) 
I0801 14:22:22.359032 24522 net.cpp:2270] res4a_branch2a_param_0(0.819) 
I0801 14:22:22.359040 24522 net.cpp:2270] res4a_branch2b_param_0(0.812) 
I0801 14:22:22.359045 24522 net.cpp:2270] res5a_branch2a_param_0(0.786) 
I0801 14:22:22.359052 24522 net.cpp:2270] res5a_branch2b_param_0(0.819) 
I0801 14:22:22.359058 24522 net.cpp:2272] Total Sparsity (zero_weights/count) =  (1.88323e+06/2.3599e+06) 0.798
I0801 14:22:22.359262 24522 solver.cpp:680] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_64000.caffemodel
I0801 14:22:22.369220 24522 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-08-01_13-11-28/sparse/cifar10_jacintonet11v2_iter_64000.solverstate
I0801 14:22:22.378031 24522 solver.cpp:527] Iteration 64000, loss = 0.000240207
I0801 14:22:22.378051 24522 solver.cpp:550] Iteration 64000, Testing net (#0)
I0801 14:22:23.196447 24522 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.902648
I0801 14:22:23.196468 24522 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.994706
I0801 14:22:23.196475 24522 solver.cpp:635]     Test net output #2: loss = 0.42992 (* 1 = 0.42992 loss)
I0801 14:22:23.199661 24466 parallel.cpp:73] Root Solver performance on device 0: 30.57 * 22 = 672.5 img/sec (64000 itr in 2094 sec)
I0801 14:22:23.199681 24466 parallel.cpp:78]      Solver performance on device 1: 30.57 * 22 = 672.5 img/sec (64000 itr in 2094 sec)
I0801 14:22:23.199687 24466 parallel.cpp:78]      Solver performance on device 2: 30.57 * 22 = 672.5 img/sec (64000 itr in 2094 sec)
I0801 14:22:23.199688 24466 parallel.cpp:81] Overall multi-GPU performance: 2017.42 img/sec
I0801 14:22:23.302027 24466 caffe.cpp:247] Optimization Done in 34m 57s
