Logging output to training/cifar10_jacintonet11v2_2017-07-04_07-19-29/train-log_2017-07-04_07-19-29.txt
I0704 07:19:30.486915 22258 caffe.cpp:209] Using GPUs 0, 1, 2
I0704 07:19:30.488133 22258 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0704 07:19:30.488473 22258 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0704 07:19:30.488826 22258 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0704 07:19:30.888567 22258 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.1
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: true
iter_size: 1
type: "SGD"
I0704 07:19:30.888665 22258 solver.cpp:82] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/train.prototxt
I0704 07:19:30.889276 22258 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0704 07:19:30.889282 22258 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0704 07:19:30.889478 22258 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 21
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0704 07:19:30.889600 22258 layer_factory.hpp:77] Creating layer data
I0704 07:19:30.889694 22258 net.cpp:98] Creating Layer data
I0704 07:19:30.889703 22258 net.cpp:413] data -> data
I0704 07:19:30.889722 22258 net.cpp:413] data -> label
I0704 07:19:30.951930 22288 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0704 07:19:30.985878 22258 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0704 07:19:30.986153 22258 data_layer.cpp:83] output data size: 21,3,32,32
I0704 07:19:30.988332 22258 net.cpp:148] Setting up data
I0704 07:19:30.988348 22258 net.cpp:155] Top shape: 21 3 32 32 (64512)
I0704 07:19:30.988350 22258 net.cpp:155] Top shape: 21 (21)
I0704 07:19:30.988353 22258 net.cpp:163] Memory required for data: 258132
I0704 07:19:30.988359 22258 layer_factory.hpp:77] Creating layer data/bias
I0704 07:19:30.988374 22258 net.cpp:98] Creating Layer data/bias
I0704 07:19:30.988381 22258 net.cpp:439] data/bias <- data
I0704 07:19:30.988392 22258 net.cpp:413] data/bias -> data/bias
I0704 07:19:30.990209 22258 net.cpp:148] Setting up data/bias
I0704 07:19:30.990221 22258 net.cpp:155] Top shape: 21 3 32 32 (64512)
I0704 07:19:30.990223 22258 net.cpp:163] Memory required for data: 516180
I0704 07:19:30.990236 22258 layer_factory.hpp:77] Creating layer conv1a
I0704 07:19:30.990247 22258 net.cpp:98] Creating Layer conv1a
I0704 07:19:30.990252 22258 net.cpp:439] conv1a <- data/bias
I0704 07:19:30.990257 22258 net.cpp:413] conv1a -> conv1a
I0704 07:19:30.990470 22293 blocking_queue.cpp:50] Waiting for data
I0704 07:19:30.995522 22258 net.cpp:148] Setting up conv1a
I0704 07:19:30.995532 22258 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:19:30.995534 22258 net.cpp:163] Memory required for data: 3268692
I0704 07:19:30.995539 22258 layer_factory.hpp:77] Creating layer conv1a/bn
I0704 07:19:30.995548 22258 net.cpp:98] Creating Layer conv1a/bn
I0704 07:19:30.995551 22258 net.cpp:439] conv1a/bn <- conv1a
I0704 07:19:30.995554 22258 net.cpp:413] conv1a/bn -> conv1a/bn
I0704 07:19:30.996318 22258 net.cpp:148] Setting up conv1a/bn
I0704 07:19:30.996326 22258 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:19:30.996328 22258 net.cpp:163] Memory required for data: 6021204
I0704 07:19:30.996335 22258 layer_factory.hpp:77] Creating layer conv1a/relu
I0704 07:19:30.996340 22258 net.cpp:98] Creating Layer conv1a/relu
I0704 07:19:30.996341 22258 net.cpp:439] conv1a/relu <- conv1a/bn
I0704 07:19:30.996345 22258 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0704 07:19:30.996356 22258 net.cpp:148] Setting up conv1a/relu
I0704 07:19:30.996358 22258 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:19:30.996361 22258 net.cpp:163] Memory required for data: 8773716
I0704 07:19:30.996364 22258 layer_factory.hpp:77] Creating layer conv1b
I0704 07:19:30.996379 22258 net.cpp:98] Creating Layer conv1b
I0704 07:19:30.996382 22258 net.cpp:439] conv1b <- conv1a/bn
I0704 07:19:30.996384 22258 net.cpp:413] conv1b -> conv1b
I0704 07:19:30.996711 22258 net.cpp:148] Setting up conv1b
I0704 07:19:30.996716 22258 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:19:30.996719 22258 net.cpp:163] Memory required for data: 11526228
I0704 07:19:30.996723 22258 layer_factory.hpp:77] Creating layer conv1b/bn
I0704 07:19:30.996727 22258 net.cpp:98] Creating Layer conv1b/bn
I0704 07:19:30.996729 22258 net.cpp:439] conv1b/bn <- conv1b
I0704 07:19:30.996732 22258 net.cpp:413] conv1b/bn -> conv1b/bn
I0704 07:19:30.997391 22258 net.cpp:148] Setting up conv1b/bn
I0704 07:19:30.997397 22258 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:19:30.997400 22258 net.cpp:163] Memory required for data: 14278740
I0704 07:19:30.997404 22258 layer_factory.hpp:77] Creating layer conv1b/relu
I0704 07:19:30.997407 22258 net.cpp:98] Creating Layer conv1b/relu
I0704 07:19:30.997409 22258 net.cpp:439] conv1b/relu <- conv1b/bn
I0704 07:19:30.997411 22258 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0704 07:19:30.997416 22258 net.cpp:148] Setting up conv1b/relu
I0704 07:19:30.997418 22258 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:19:30.997421 22258 net.cpp:163] Memory required for data: 17031252
I0704 07:19:30.997421 22258 layer_factory.hpp:77] Creating layer pool1
I0704 07:19:30.997427 22258 net.cpp:98] Creating Layer pool1
I0704 07:19:30.997429 22258 net.cpp:439] pool1 <- conv1b/bn
I0704 07:19:30.997436 22258 net.cpp:413] pool1 -> pool1
I0704 07:19:30.997516 22258 net.cpp:148] Setting up pool1
I0704 07:19:30.997536 22258 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:19:30.997544 22258 net.cpp:163] Memory required for data: 19783764
I0704 07:19:30.997548 22258 layer_factory.hpp:77] Creating layer res2a_branch2a
I0704 07:19:30.997560 22258 net.cpp:98] Creating Layer res2a_branch2a
I0704 07:19:30.997565 22258 net.cpp:439] res2a_branch2a <- pool1
I0704 07:19:30.997573 22258 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0704 07:19:30.998594 22258 net.cpp:148] Setting up res2a_branch2a
I0704 07:19:30.998605 22258 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:19:30.998610 22258 net.cpp:163] Memory required for data: 25288788
I0704 07:19:30.998616 22258 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0704 07:19:30.998625 22258 net.cpp:98] Creating Layer res2a_branch2a/bn
I0704 07:19:30.998628 22258 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0704 07:19:30.998632 22258 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0704 07:19:30.999430 22258 net.cpp:148] Setting up res2a_branch2a/bn
I0704 07:19:30.999439 22258 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:19:30.999441 22258 net.cpp:163] Memory required for data: 30793812
I0704 07:19:30.999446 22258 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0704 07:19:30.999451 22258 net.cpp:98] Creating Layer res2a_branch2a/relu
I0704 07:19:30.999454 22258 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0704 07:19:30.999457 22258 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0704 07:19:30.999462 22258 net.cpp:148] Setting up res2a_branch2a/relu
I0704 07:19:30.999467 22258 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:19:30.999469 22258 net.cpp:163] Memory required for data: 36298836
I0704 07:19:30.999470 22258 layer_factory.hpp:77] Creating layer res2a_branch2b
I0704 07:19:30.999475 22258 net.cpp:98] Creating Layer res2a_branch2b
I0704 07:19:30.999477 22258 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0704 07:19:30.999480 22258 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0704 07:19:31.003311 22258 net.cpp:148] Setting up res2a_branch2b
I0704 07:19:31.003324 22258 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:19:31.003327 22258 net.cpp:163] Memory required for data: 41803860
I0704 07:19:31.003332 22258 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0704 07:19:31.003338 22258 net.cpp:98] Creating Layer res2a_branch2b/bn
I0704 07:19:31.003341 22258 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0704 07:19:31.003356 22258 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0704 07:19:31.004042 22258 net.cpp:148] Setting up res2a_branch2b/bn
I0704 07:19:31.004050 22258 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:19:31.004055 22258 net.cpp:163] Memory required for data: 47308884
I0704 07:19:31.004063 22258 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0704 07:19:31.004068 22258 net.cpp:98] Creating Layer res2a_branch2b/relu
I0704 07:19:31.004073 22258 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0704 07:19:31.004078 22258 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0704 07:19:31.004088 22258 net.cpp:148] Setting up res2a_branch2b/relu
I0704 07:19:31.004092 22258 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:19:31.004096 22258 net.cpp:163] Memory required for data: 52813908
I0704 07:19:31.004099 22258 layer_factory.hpp:77] Creating layer pool2
I0704 07:19:31.004106 22258 net.cpp:98] Creating Layer pool2
I0704 07:19:31.004108 22258 net.cpp:439] pool2 <- res2a_branch2b/bn
I0704 07:19:31.004113 22258 net.cpp:413] pool2 -> pool2
I0704 07:19:31.004154 22258 net.cpp:148] Setting up pool2
I0704 07:19:31.004160 22258 net.cpp:155] Top shape: 21 64 16 16 (344064)
I0704 07:19:31.004164 22258 net.cpp:163] Memory required for data: 54190164
I0704 07:19:31.004168 22258 layer_factory.hpp:77] Creating layer res3a_branch2a
I0704 07:19:31.004180 22258 net.cpp:98] Creating Layer res3a_branch2a
I0704 07:19:31.004184 22258 net.cpp:439] res3a_branch2a <- pool2
I0704 07:19:31.004189 22258 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0704 07:19:31.012516 22258 net.cpp:148] Setting up res3a_branch2a
I0704 07:19:31.012529 22258 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:19:31.012532 22258 net.cpp:163] Memory required for data: 56942676
I0704 07:19:31.012539 22258 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0704 07:19:31.012547 22258 net.cpp:98] Creating Layer res3a_branch2a/bn
I0704 07:19:31.012552 22258 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0704 07:19:31.012558 22258 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0704 07:19:31.013175 22258 net.cpp:148] Setting up res3a_branch2a/bn
I0704 07:19:31.013182 22258 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:19:31.013187 22258 net.cpp:163] Memory required for data: 59695188
I0704 07:19:31.013203 22258 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0704 07:19:31.013209 22258 net.cpp:98] Creating Layer res3a_branch2a/relu
I0704 07:19:31.013213 22258 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0704 07:19:31.013218 22258 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0704 07:19:31.013224 22258 net.cpp:148] Setting up res3a_branch2a/relu
I0704 07:19:31.013228 22258 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:19:31.013232 22258 net.cpp:163] Memory required for data: 62447700
I0704 07:19:31.013236 22258 layer_factory.hpp:77] Creating layer res3a_branch2b
I0704 07:19:31.013244 22258 net.cpp:98] Creating Layer res3a_branch2b
I0704 07:19:31.013247 22258 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0704 07:19:31.013253 22258 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0704 07:19:31.014269 22258 net.cpp:148] Setting up res3a_branch2b
I0704 07:19:31.014276 22258 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:19:31.014279 22258 net.cpp:163] Memory required for data: 65200212
I0704 07:19:31.014286 22258 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0704 07:19:31.014292 22258 net.cpp:98] Creating Layer res3a_branch2b/bn
I0704 07:19:31.014297 22258 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0704 07:19:31.014302 22258 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0704 07:19:31.014928 22258 net.cpp:148] Setting up res3a_branch2b/bn
I0704 07:19:31.014935 22258 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:19:31.014940 22258 net.cpp:163] Memory required for data: 67952724
I0704 07:19:31.014948 22258 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0704 07:19:31.014961 22258 net.cpp:98] Creating Layer res3a_branch2b/relu
I0704 07:19:31.014966 22258 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0704 07:19:31.014969 22258 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0704 07:19:31.014976 22258 net.cpp:148] Setting up res3a_branch2b/relu
I0704 07:19:31.014981 22258 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:19:31.014984 22258 net.cpp:163] Memory required for data: 70705236
I0704 07:19:31.014988 22258 layer_factory.hpp:77] Creating layer pool3
I0704 07:19:31.014994 22258 net.cpp:98] Creating Layer pool3
I0704 07:19:31.014997 22258 net.cpp:439] pool3 <- res3a_branch2b/bn
I0704 07:19:31.015002 22258 net.cpp:413] pool3 -> pool3
I0704 07:19:31.015039 22258 net.cpp:148] Setting up pool3
I0704 07:19:31.015044 22258 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:19:31.015048 22258 net.cpp:163] Memory required for data: 73457748
I0704 07:19:31.015053 22258 layer_factory.hpp:77] Creating layer res4a_branch2a
I0704 07:19:31.015058 22258 net.cpp:98] Creating Layer res4a_branch2a
I0704 07:19:31.015063 22258 net.cpp:439] res4a_branch2a <- pool3
I0704 07:19:31.015067 22258 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0704 07:19:31.021193 22258 net.cpp:148] Setting up res4a_branch2a
I0704 07:19:31.021201 22258 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:19:31.021205 22258 net.cpp:163] Memory required for data: 78962772
I0704 07:19:31.021212 22258 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0704 07:19:31.021219 22258 net.cpp:98] Creating Layer res4a_branch2a/bn
I0704 07:19:31.021222 22258 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0704 07:19:31.021227 22258 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0704 07:19:31.021842 22258 net.cpp:148] Setting up res4a_branch2a/bn
I0704 07:19:31.021849 22258 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:19:31.021852 22258 net.cpp:163] Memory required for data: 84467796
I0704 07:19:31.021860 22258 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0704 07:19:31.021864 22258 net.cpp:98] Creating Layer res4a_branch2a/relu
I0704 07:19:31.021869 22258 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0704 07:19:31.021874 22258 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0704 07:19:31.021879 22258 net.cpp:148] Setting up res4a_branch2a/relu
I0704 07:19:31.021884 22258 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:19:31.021888 22258 net.cpp:163] Memory required for data: 89972820
I0704 07:19:31.021891 22258 layer_factory.hpp:77] Creating layer res4a_branch2b
I0704 07:19:31.021898 22258 net.cpp:98] Creating Layer res4a_branch2b
I0704 07:19:31.021901 22258 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0704 07:19:31.021908 22258 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0704 07:19:31.025095 22258 net.cpp:148] Setting up res4a_branch2b
I0704 07:19:31.025102 22258 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:19:31.025106 22258 net.cpp:163] Memory required for data: 95477844
I0704 07:19:31.025112 22258 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0704 07:19:31.025118 22258 net.cpp:98] Creating Layer res4a_branch2b/bn
I0704 07:19:31.025122 22258 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0704 07:19:31.025127 22258 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0704 07:19:31.025734 22258 net.cpp:148] Setting up res4a_branch2b/bn
I0704 07:19:31.025740 22258 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:19:31.025745 22258 net.cpp:163] Memory required for data: 100982868
I0704 07:19:31.025753 22258 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0704 07:19:31.025758 22258 net.cpp:98] Creating Layer res4a_branch2b/relu
I0704 07:19:31.025763 22258 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0704 07:19:31.025766 22258 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0704 07:19:31.025773 22258 net.cpp:148] Setting up res4a_branch2b/relu
I0704 07:19:31.025777 22258 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:19:31.025787 22258 net.cpp:163] Memory required for data: 106487892
I0704 07:19:31.025790 22258 layer_factory.hpp:77] Creating layer pool4
I0704 07:19:31.025795 22258 net.cpp:98] Creating Layer pool4
I0704 07:19:31.025799 22258 net.cpp:439] pool4 <- res4a_branch2b/bn
I0704 07:19:31.025804 22258 net.cpp:413] pool4 -> pool4
I0704 07:19:31.025841 22258 net.cpp:148] Setting up pool4
I0704 07:19:31.025846 22258 net.cpp:155] Top shape: 21 256 8 8 (344064)
I0704 07:19:31.025851 22258 net.cpp:163] Memory required for data: 107864148
I0704 07:19:31.025854 22258 layer_factory.hpp:77] Creating layer res5a_branch2a
I0704 07:19:31.025861 22258 net.cpp:98] Creating Layer res5a_branch2a
I0704 07:19:31.025864 22258 net.cpp:439] res5a_branch2a <- pool4
I0704 07:19:31.025869 22258 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0704 07:19:31.050856 22258 net.cpp:148] Setting up res5a_branch2a
I0704 07:19:31.050873 22258 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:19:31.050876 22258 net.cpp:163] Memory required for data: 110616660
I0704 07:19:31.050884 22258 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0704 07:19:31.050892 22258 net.cpp:98] Creating Layer res5a_branch2a/bn
I0704 07:19:31.050896 22258 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0704 07:19:31.050902 22258 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0704 07:19:31.051568 22258 net.cpp:148] Setting up res5a_branch2a/bn
I0704 07:19:31.051575 22258 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:19:31.051578 22258 net.cpp:163] Memory required for data: 113369172
I0704 07:19:31.051587 22258 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0704 07:19:31.051594 22258 net.cpp:98] Creating Layer res5a_branch2a/relu
I0704 07:19:31.051597 22258 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0704 07:19:31.051601 22258 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0704 07:19:31.051607 22258 net.cpp:148] Setting up res5a_branch2a/relu
I0704 07:19:31.051612 22258 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:19:31.051616 22258 net.cpp:163] Memory required for data: 116121684
I0704 07:19:31.051620 22258 layer_factory.hpp:77] Creating layer res5a_branch2b
I0704 07:19:31.051635 22258 net.cpp:98] Creating Layer res5a_branch2b
I0704 07:19:31.051638 22258 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0704 07:19:31.051645 22258 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0704 07:19:31.064442 22258 net.cpp:148] Setting up res5a_branch2b
I0704 07:19:31.064452 22258 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:19:31.064456 22258 net.cpp:163] Memory required for data: 118874196
I0704 07:19:31.064469 22258 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0704 07:19:31.064476 22258 net.cpp:98] Creating Layer res5a_branch2b/bn
I0704 07:19:31.064481 22258 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0704 07:19:31.064486 22258 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0704 07:19:31.065147 22258 net.cpp:148] Setting up res5a_branch2b/bn
I0704 07:19:31.065155 22258 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:19:31.065158 22258 net.cpp:163] Memory required for data: 121626708
I0704 07:19:31.065167 22258 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0704 07:19:31.065172 22258 net.cpp:98] Creating Layer res5a_branch2b/relu
I0704 07:19:31.065176 22258 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0704 07:19:31.065181 22258 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0704 07:19:31.065186 22258 net.cpp:148] Setting up res5a_branch2b/relu
I0704 07:19:31.065191 22258 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:19:31.065194 22258 net.cpp:163] Memory required for data: 124379220
I0704 07:19:31.065198 22258 layer_factory.hpp:77] Creating layer pool5
I0704 07:19:31.065206 22258 net.cpp:98] Creating Layer pool5
I0704 07:19:31.065209 22258 net.cpp:439] pool5 <- res5a_branch2b/bn
I0704 07:19:31.065213 22258 net.cpp:413] pool5 -> pool5
I0704 07:19:31.065244 22258 net.cpp:148] Setting up pool5
I0704 07:19:31.065249 22258 net.cpp:155] Top shape: 21 512 1 1 (10752)
I0704 07:19:31.065261 22258 net.cpp:163] Memory required for data: 124422228
I0704 07:19:31.065265 22258 layer_factory.hpp:77] Creating layer fc10
I0704 07:19:31.065273 22258 net.cpp:98] Creating Layer fc10
I0704 07:19:31.065276 22258 net.cpp:439] fc10 <- pool5
I0704 07:19:31.065281 22258 net.cpp:413] fc10 -> fc10
I0704 07:19:31.065511 22258 net.cpp:148] Setting up fc10
I0704 07:19:31.065517 22258 net.cpp:155] Top shape: 21 10 (210)
I0704 07:19:31.065521 22258 net.cpp:163] Memory required for data: 124423068
I0704 07:19:31.065526 22258 layer_factory.hpp:77] Creating layer loss
I0704 07:19:31.065536 22258 net.cpp:98] Creating Layer loss
I0704 07:19:31.065541 22258 net.cpp:439] loss <- fc10
I0704 07:19:31.065544 22258 net.cpp:439] loss <- label
I0704 07:19:31.065551 22258 net.cpp:413] loss -> loss
I0704 07:19:31.065559 22258 layer_factory.hpp:77] Creating layer loss
I0704 07:19:31.065676 22258 net.cpp:148] Setting up loss
I0704 07:19:31.065682 22258 net.cpp:155] Top shape: (1)
I0704 07:19:31.065685 22258 net.cpp:158]     with loss weight 1
I0704 07:19:31.065699 22258 net.cpp:163] Memory required for data: 124423072
I0704 07:19:31.065702 22258 net.cpp:224] loss needs backward computation.
I0704 07:19:31.065707 22258 net.cpp:224] fc10 needs backward computation.
I0704 07:19:31.065711 22258 net.cpp:224] pool5 needs backward computation.
I0704 07:19:31.065716 22258 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0704 07:19:31.065719 22258 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0704 07:19:31.065723 22258 net.cpp:224] res5a_branch2b needs backward computation.
I0704 07:19:31.065727 22258 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0704 07:19:31.065732 22258 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0704 07:19:31.065735 22258 net.cpp:224] res5a_branch2a needs backward computation.
I0704 07:19:31.065739 22258 net.cpp:224] pool4 needs backward computation.
I0704 07:19:31.065743 22258 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0704 07:19:31.065747 22258 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0704 07:19:31.065752 22258 net.cpp:224] res4a_branch2b needs backward computation.
I0704 07:19:31.065755 22258 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0704 07:19:31.065759 22258 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0704 07:19:31.065763 22258 net.cpp:224] res4a_branch2a needs backward computation.
I0704 07:19:31.065768 22258 net.cpp:224] pool3 needs backward computation.
I0704 07:19:31.065771 22258 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0704 07:19:31.065775 22258 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0704 07:19:31.065779 22258 net.cpp:224] res3a_branch2b needs backward computation.
I0704 07:19:31.065784 22258 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0704 07:19:31.065788 22258 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0704 07:19:31.065791 22258 net.cpp:224] res3a_branch2a needs backward computation.
I0704 07:19:31.065795 22258 net.cpp:224] pool2 needs backward computation.
I0704 07:19:31.065800 22258 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0704 07:19:31.065804 22258 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0704 07:19:31.065807 22258 net.cpp:224] res2a_branch2b needs backward computation.
I0704 07:19:31.065812 22258 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0704 07:19:31.065815 22258 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0704 07:19:31.065820 22258 net.cpp:224] res2a_branch2a needs backward computation.
I0704 07:19:31.065824 22258 net.cpp:224] pool1 needs backward computation.
I0704 07:19:31.065829 22258 net.cpp:224] conv1b/relu needs backward computation.
I0704 07:19:31.065832 22258 net.cpp:224] conv1b/bn needs backward computation.
I0704 07:19:31.065836 22258 net.cpp:224] conv1b needs backward computation.
I0704 07:19:31.065840 22258 net.cpp:224] conv1a/relu needs backward computation.
I0704 07:19:31.065845 22258 net.cpp:224] conv1a/bn needs backward computation.
I0704 07:19:31.065853 22258 net.cpp:224] conv1a needs backward computation.
I0704 07:19:31.065858 22258 net.cpp:226] data/bias does not need backward computation.
I0704 07:19:31.065863 22258 net.cpp:226] data does not need backward computation.
I0704 07:19:31.065867 22258 net.cpp:268] This network produces output loss
I0704 07:19:31.065886 22258 net.cpp:288] Network initialization done.
I0704 07:19:31.066323 22258 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/test.prototxt
I0704 07:19:31.066514 22258 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0704 07:19:31.066610 22258 layer_factory.hpp:77] Creating layer data
I0704 07:19:31.066674 22258 net.cpp:98] Creating Layer data
I0704 07:19:31.066680 22258 net.cpp:413] data -> data
I0704 07:19:31.066687 22258 net.cpp:413] data -> label
I0704 07:19:31.079757 22294 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0704 07:19:31.080672 22258 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0704 07:19:31.080978 22258 data_layer.cpp:83] output data size: 50,3,32,32
I0704 07:19:31.089512 22258 net.cpp:148] Setting up data
I0704 07:19:31.089548 22258 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0704 07:19:31.089565 22258 net.cpp:155] Top shape: 50 (50)
I0704 07:19:31.089581 22258 net.cpp:163] Memory required for data: 614600
I0704 07:19:31.089598 22258 layer_factory.hpp:77] Creating layer label_data_1_split
I0704 07:19:31.089623 22258 net.cpp:98] Creating Layer label_data_1_split
I0704 07:19:31.089638 22258 net.cpp:439] label_data_1_split <- label
I0704 07:19:31.089655 22258 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0704 07:19:31.089679 22258 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0704 07:19:31.089699 22258 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0704 07:19:31.089928 22258 net.cpp:148] Setting up label_data_1_split
I0704 07:19:31.089948 22258 net.cpp:155] Top shape: 50 (50)
I0704 07:19:31.089965 22258 net.cpp:155] Top shape: 50 (50)
I0704 07:19:31.089982 22258 net.cpp:155] Top shape: 50 (50)
I0704 07:19:31.089994 22258 net.cpp:163] Memory required for data: 615200
I0704 07:19:31.090010 22258 layer_factory.hpp:77] Creating layer data/bias
I0704 07:19:31.090034 22258 net.cpp:98] Creating Layer data/bias
I0704 07:19:31.090046 22258 net.cpp:439] data/bias <- data
I0704 07:19:31.090065 22258 net.cpp:413] data/bias -> data/bias
I0704 07:19:31.090380 22258 net.cpp:148] Setting up data/bias
I0704 07:19:31.090423 22258 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0704 07:19:31.090438 22258 net.cpp:163] Memory required for data: 1229600
I0704 07:19:31.090461 22258 layer_factory.hpp:77] Creating layer conv1a
I0704 07:19:31.090487 22258 net.cpp:98] Creating Layer conv1a
I0704 07:19:31.090499 22258 net.cpp:439] conv1a <- data/bias
I0704 07:19:31.090518 22258 net.cpp:413] conv1a -> conv1a
I0704 07:19:31.092212 22258 net.cpp:148] Setting up conv1a
I0704 07:19:31.092239 22258 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:19:31.092254 22258 net.cpp:163] Memory required for data: 7783200
I0704 07:19:31.092283 22258 layer_factory.hpp:77] Creating layer conv1a/bn
I0704 07:19:31.092304 22258 net.cpp:98] Creating Layer conv1a/bn
I0704 07:19:31.092319 22258 net.cpp:439] conv1a/bn <- conv1a
I0704 07:19:31.092339 22258 net.cpp:413] conv1a/bn -> conv1a/bn
I0704 07:19:31.094861 22258 net.cpp:148] Setting up conv1a/bn
I0704 07:19:31.094882 22258 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:19:31.094892 22258 net.cpp:163] Memory required for data: 14336800
I0704 07:19:31.094916 22258 layer_factory.hpp:77] Creating layer conv1a/relu
I0704 07:19:31.094929 22258 net.cpp:98] Creating Layer conv1a/relu
I0704 07:19:31.094940 22258 net.cpp:439] conv1a/relu <- conv1a/bn
I0704 07:19:31.094969 22258 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0704 07:19:31.094986 22258 net.cpp:148] Setting up conv1a/relu
I0704 07:19:31.095000 22258 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:19:31.095010 22258 net.cpp:163] Memory required for data: 20890400
I0704 07:19:31.095021 22258 layer_factory.hpp:77] Creating layer conv1b
I0704 07:19:31.095037 22258 net.cpp:98] Creating Layer conv1b
I0704 07:19:31.095046 22258 net.cpp:439] conv1b <- conv1a/bn
I0704 07:19:31.095060 22258 net.cpp:413] conv1b -> conv1b
I0704 07:19:31.095975 22258 net.cpp:148] Setting up conv1b
I0704 07:19:31.095993 22258 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:19:31.096002 22258 net.cpp:163] Memory required for data: 27444000
I0704 07:19:31.096021 22258 layer_factory.hpp:77] Creating layer conv1b/bn
I0704 07:19:31.096048 22258 net.cpp:98] Creating Layer conv1b/bn
I0704 07:19:31.096058 22258 net.cpp:439] conv1b/bn <- conv1b
I0704 07:19:31.096071 22258 net.cpp:413] conv1b/bn -> conv1b/bn
I0704 07:19:31.098311 22258 net.cpp:148] Setting up conv1b/bn
I0704 07:19:31.098341 22258 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:19:31.098356 22258 net.cpp:163] Memory required for data: 33997600
I0704 07:19:31.098400 22258 layer_factory.hpp:77] Creating layer conv1b/relu
I0704 07:19:31.098422 22258 net.cpp:98] Creating Layer conv1b/relu
I0704 07:19:31.098436 22258 net.cpp:439] conv1b/relu <- conv1b/bn
I0704 07:19:31.098453 22258 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0704 07:19:31.098474 22258 net.cpp:148] Setting up conv1b/relu
I0704 07:19:31.098491 22258 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:19:31.098505 22258 net.cpp:163] Memory required for data: 40551200
I0704 07:19:31.098517 22258 layer_factory.hpp:77] Creating layer pool1
I0704 07:19:31.098536 22258 net.cpp:98] Creating Layer pool1
I0704 07:19:31.098549 22258 net.cpp:439] pool1 <- conv1b/bn
I0704 07:19:31.098567 22258 net.cpp:413] pool1 -> pool1
I0704 07:19:31.098729 22258 net.cpp:148] Setting up pool1
I0704 07:19:31.098747 22258 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:19:31.098760 22258 net.cpp:163] Memory required for data: 47104800
I0704 07:19:31.098773 22258 layer_factory.hpp:77] Creating layer res2a_branch2a
I0704 07:19:31.098805 22258 net.cpp:98] Creating Layer res2a_branch2a
I0704 07:19:31.098819 22258 net.cpp:439] res2a_branch2a <- pool1
I0704 07:19:31.098834 22258 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0704 07:19:31.100829 22258 net.cpp:148] Setting up res2a_branch2a
I0704 07:19:31.100847 22258 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:19:31.100857 22258 net.cpp:163] Memory required for data: 60212000
I0704 07:19:31.100873 22258 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0704 07:19:31.100886 22258 net.cpp:98] Creating Layer res2a_branch2a/bn
I0704 07:19:31.100895 22258 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0704 07:19:31.100906 22258 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0704 07:19:31.102996 22258 net.cpp:148] Setting up res2a_branch2a/bn
I0704 07:19:31.103014 22258 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:19:31.103024 22258 net.cpp:163] Memory required for data: 73319200
I0704 07:19:31.103041 22258 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0704 07:19:31.103052 22258 net.cpp:98] Creating Layer res2a_branch2a/relu
I0704 07:19:31.103061 22258 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0704 07:19:31.103070 22258 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0704 07:19:31.103083 22258 net.cpp:148] Setting up res2a_branch2a/relu
I0704 07:19:31.103092 22258 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:19:31.103101 22258 net.cpp:163] Memory required for data: 86426400
I0704 07:19:31.103109 22258 layer_factory.hpp:77] Creating layer res2a_branch2b
I0704 07:19:31.103123 22258 net.cpp:98] Creating Layer res2a_branch2b
I0704 07:19:31.103132 22258 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0704 07:19:31.103143 22258 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0704 07:19:31.104482 22258 net.cpp:148] Setting up res2a_branch2b
I0704 07:19:31.104496 22258 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:19:31.104504 22258 net.cpp:163] Memory required for data: 99533600
I0704 07:19:31.104516 22258 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0704 07:19:31.104528 22258 net.cpp:98] Creating Layer res2a_branch2b/bn
I0704 07:19:31.104537 22258 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0704 07:19:31.104545 22258 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0704 07:19:31.106292 22258 net.cpp:148] Setting up res2a_branch2b/bn
I0704 07:19:31.106305 22258 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:19:31.106313 22258 net.cpp:163] Memory required for data: 112640800
I0704 07:19:31.106328 22258 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0704 07:19:31.106336 22258 net.cpp:98] Creating Layer res2a_branch2b/relu
I0704 07:19:31.106343 22258 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0704 07:19:31.106350 22258 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0704 07:19:31.106360 22258 net.cpp:148] Setting up res2a_branch2b/relu
I0704 07:19:31.106369 22258 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:19:31.106375 22258 net.cpp:163] Memory required for data: 125748000
I0704 07:19:31.106382 22258 layer_factory.hpp:77] Creating layer pool2
I0704 07:19:31.106396 22258 net.cpp:98] Creating Layer pool2
I0704 07:19:31.106403 22258 net.cpp:439] pool2 <- res2a_branch2b/bn
I0704 07:19:31.106412 22258 net.cpp:413] pool2 -> pool2
I0704 07:19:31.106498 22258 net.cpp:148] Setting up pool2
I0704 07:19:31.106508 22258 net.cpp:155] Top shape: 50 64 16 16 (819200)
I0704 07:19:31.106515 22258 net.cpp:163] Memory required for data: 129024800
I0704 07:19:31.106521 22258 layer_factory.hpp:77] Creating layer res3a_branch2a
I0704 07:19:31.106531 22258 net.cpp:98] Creating Layer res3a_branch2a
I0704 07:19:31.106539 22258 net.cpp:439] res3a_branch2a <- pool2
I0704 07:19:31.106547 22258 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0704 07:19:31.111567 22258 net.cpp:148] Setting up res3a_branch2a
I0704 07:19:31.111582 22258 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:19:31.111587 22258 net.cpp:163] Memory required for data: 135578400
I0704 07:19:31.111593 22258 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0704 07:19:31.111600 22258 net.cpp:98] Creating Layer res3a_branch2a/bn
I0704 07:19:31.111605 22258 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0704 07:19:31.111611 22258 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0704 07:19:31.112572 22258 net.cpp:148] Setting up res3a_branch2a/bn
I0704 07:19:31.112581 22258 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:19:31.112586 22258 net.cpp:163] Memory required for data: 142132000
I0704 07:19:31.112594 22258 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0704 07:19:31.112607 22258 net.cpp:98] Creating Layer res3a_branch2a/relu
I0704 07:19:31.112612 22258 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0704 07:19:31.112617 22258 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0704 07:19:31.112623 22258 net.cpp:148] Setting up res3a_branch2a/relu
I0704 07:19:31.112628 22258 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:19:31.112632 22258 net.cpp:163] Memory required for data: 148685600
I0704 07:19:31.112635 22258 layer_factory.hpp:77] Creating layer res3a_branch2b
I0704 07:19:31.112642 22258 net.cpp:98] Creating Layer res3a_branch2b
I0704 07:19:31.112645 22258 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0704 07:19:31.112649 22258 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0704 07:19:31.114183 22258 net.cpp:148] Setting up res3a_branch2b
I0704 07:19:31.114192 22258 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:19:31.114195 22258 net.cpp:163] Memory required for data: 155239200
I0704 07:19:31.114200 22258 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0704 07:19:31.114205 22258 net.cpp:98] Creating Layer res3a_branch2b/bn
I0704 07:19:31.114209 22258 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0704 07:19:31.114223 22258 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0704 07:19:31.115072 22258 net.cpp:148] Setting up res3a_branch2b/bn
I0704 07:19:31.115079 22258 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:19:31.115083 22258 net.cpp:163] Memory required for data: 161792800
I0704 07:19:31.115090 22258 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0704 07:19:31.115094 22258 net.cpp:98] Creating Layer res3a_branch2b/relu
I0704 07:19:31.115098 22258 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0704 07:19:31.115103 22258 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0704 07:19:31.115108 22258 net.cpp:148] Setting up res3a_branch2b/relu
I0704 07:19:31.115111 22258 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:19:31.115113 22258 net.cpp:163] Memory required for data: 168346400
I0704 07:19:31.115118 22258 layer_factory.hpp:77] Creating layer pool3
I0704 07:19:31.115121 22258 net.cpp:98] Creating Layer pool3
I0704 07:19:31.115124 22258 net.cpp:439] pool3 <- res3a_branch2b/bn
I0704 07:19:31.115128 22258 net.cpp:413] pool3 -> pool3
I0704 07:19:31.115181 22258 net.cpp:148] Setting up pool3
I0704 07:19:31.115186 22258 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:19:31.115190 22258 net.cpp:163] Memory required for data: 174900000
I0704 07:19:31.115192 22258 layer_factory.hpp:77] Creating layer res4a_branch2a
I0704 07:19:31.115198 22258 net.cpp:98] Creating Layer res4a_branch2a
I0704 07:19:31.115202 22258 net.cpp:439] res4a_branch2a <- pool3
I0704 07:19:31.115206 22258 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0704 07:19:31.124558 22258 net.cpp:148] Setting up res4a_branch2a
I0704 07:19:31.124575 22258 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:19:31.124578 22258 net.cpp:163] Memory required for data: 188007200
I0704 07:19:31.124586 22258 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0704 07:19:31.124594 22258 net.cpp:98] Creating Layer res4a_branch2a/bn
I0704 07:19:31.124600 22258 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0704 07:19:31.124608 22258 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0704 07:19:31.125502 22258 net.cpp:148] Setting up res4a_branch2a/bn
I0704 07:19:31.125511 22258 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:19:31.125515 22258 net.cpp:163] Memory required for data: 201114400
I0704 07:19:31.125524 22258 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0704 07:19:31.125530 22258 net.cpp:98] Creating Layer res4a_branch2a/relu
I0704 07:19:31.125533 22258 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0704 07:19:31.125538 22258 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0704 07:19:31.125545 22258 net.cpp:148] Setting up res4a_branch2a/relu
I0704 07:19:31.125550 22258 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:19:31.125552 22258 net.cpp:163] Memory required for data: 214221600
I0704 07:19:31.125557 22258 layer_factory.hpp:77] Creating layer res4a_branch2b
I0704 07:19:31.125566 22258 net.cpp:98] Creating Layer res4a_branch2b
I0704 07:19:31.125569 22258 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0704 07:19:31.125574 22258 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0704 07:19:31.128950 22258 net.cpp:148] Setting up res4a_branch2b
I0704 07:19:31.128958 22258 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:19:31.128962 22258 net.cpp:163] Memory required for data: 227328800
I0704 07:19:31.128968 22258 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0704 07:19:31.128974 22258 net.cpp:98] Creating Layer res4a_branch2b/bn
I0704 07:19:31.128978 22258 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0704 07:19:31.128989 22258 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0704 07:19:31.129662 22258 net.cpp:148] Setting up res4a_branch2b/bn
I0704 07:19:31.129668 22258 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:19:31.129673 22258 net.cpp:163] Memory required for data: 240436000
I0704 07:19:31.129680 22258 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0704 07:19:31.129693 22258 net.cpp:98] Creating Layer res4a_branch2b/relu
I0704 07:19:31.129698 22258 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0704 07:19:31.129703 22258 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0704 07:19:31.129709 22258 net.cpp:148] Setting up res4a_branch2b/relu
I0704 07:19:31.129714 22258 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:19:31.129717 22258 net.cpp:163] Memory required for data: 253543200
I0704 07:19:31.129721 22258 layer_factory.hpp:77] Creating layer pool4
I0704 07:19:31.129727 22258 net.cpp:98] Creating Layer pool4
I0704 07:19:31.129730 22258 net.cpp:439] pool4 <- res4a_branch2b/bn
I0704 07:19:31.129735 22258 net.cpp:413] pool4 -> pool4
I0704 07:19:31.129779 22258 net.cpp:148] Setting up pool4
I0704 07:19:31.129786 22258 net.cpp:155] Top shape: 50 256 8 8 (819200)
I0704 07:19:31.129789 22258 net.cpp:163] Memory required for data: 256820000
I0704 07:19:31.129793 22258 layer_factory.hpp:77] Creating layer res5a_branch2a
I0704 07:19:31.129801 22258 net.cpp:98] Creating Layer res5a_branch2a
I0704 07:19:31.129804 22258 net.cpp:439] res5a_branch2a <- pool4
I0704 07:19:31.129809 22258 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0704 07:19:31.155391 22258 net.cpp:148] Setting up res5a_branch2a
I0704 07:19:31.155414 22258 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:19:31.155417 22258 net.cpp:163] Memory required for data: 263373600
I0704 07:19:31.155426 22258 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0704 07:19:31.155436 22258 net.cpp:98] Creating Layer res5a_branch2a/bn
I0704 07:19:31.155441 22258 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0704 07:19:31.155448 22258 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0704 07:19:31.156158 22258 net.cpp:148] Setting up res5a_branch2a/bn
I0704 07:19:31.156172 22258 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:19:31.156175 22258 net.cpp:163] Memory required for data: 269927200
I0704 07:19:31.156183 22258 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0704 07:19:31.156188 22258 net.cpp:98] Creating Layer res5a_branch2a/relu
I0704 07:19:31.156193 22258 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0704 07:19:31.156198 22258 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0704 07:19:31.156204 22258 net.cpp:148] Setting up res5a_branch2a/relu
I0704 07:19:31.156209 22258 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:19:31.156213 22258 net.cpp:163] Memory required for data: 276480800
I0704 07:19:31.156217 22258 layer_factory.hpp:77] Creating layer res5a_branch2b
I0704 07:19:31.156225 22258 net.cpp:98] Creating Layer res5a_branch2b
I0704 07:19:31.156229 22258 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0704 07:19:31.156234 22258 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0704 07:19:31.169116 22258 net.cpp:148] Setting up res5a_branch2b
I0704 07:19:31.169127 22258 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:19:31.169131 22258 net.cpp:163] Memory required for data: 283034400
I0704 07:19:31.169143 22258 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0704 07:19:31.169152 22258 net.cpp:98] Creating Layer res5a_branch2b/bn
I0704 07:19:31.169155 22258 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0704 07:19:31.169162 22258 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0704 07:19:31.169863 22258 net.cpp:148] Setting up res5a_branch2b/bn
I0704 07:19:31.169870 22258 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:19:31.169874 22258 net.cpp:163] Memory required for data: 289588000
I0704 07:19:31.169883 22258 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0704 07:19:31.169888 22258 net.cpp:98] Creating Layer res5a_branch2b/relu
I0704 07:19:31.169893 22258 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0704 07:19:31.169898 22258 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0704 07:19:31.169903 22258 net.cpp:148] Setting up res5a_branch2b/relu
I0704 07:19:31.169909 22258 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:19:31.169911 22258 net.cpp:163] Memory required for data: 296141600
I0704 07:19:31.169924 22258 layer_factory.hpp:77] Creating layer pool5
I0704 07:19:31.169930 22258 net.cpp:98] Creating Layer pool5
I0704 07:19:31.169934 22258 net.cpp:439] pool5 <- res5a_branch2b/bn
I0704 07:19:31.169939 22258 net.cpp:413] pool5 -> pool5
I0704 07:19:31.169966 22258 net.cpp:148] Setting up pool5
I0704 07:19:31.169971 22258 net.cpp:155] Top shape: 50 512 1 1 (25600)
I0704 07:19:31.169975 22258 net.cpp:163] Memory required for data: 296244000
I0704 07:19:31.169980 22258 layer_factory.hpp:77] Creating layer fc10
I0704 07:19:31.169991 22258 net.cpp:98] Creating Layer fc10
I0704 07:19:31.169996 22258 net.cpp:439] fc10 <- pool5
I0704 07:19:31.169999 22258 net.cpp:413] fc10 -> fc10
I0704 07:19:31.170241 22258 net.cpp:148] Setting up fc10
I0704 07:19:31.170248 22258 net.cpp:155] Top shape: 50 10 (500)
I0704 07:19:31.170251 22258 net.cpp:163] Memory required for data: 296246000
I0704 07:19:31.170258 22258 layer_factory.hpp:77] Creating layer fc10_fc10_0_split
I0704 07:19:31.170262 22258 net.cpp:98] Creating Layer fc10_fc10_0_split
I0704 07:19:31.170266 22258 net.cpp:439] fc10_fc10_0_split <- fc10
I0704 07:19:31.170271 22258 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0704 07:19:31.170279 22258 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0704 07:19:31.170284 22258 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0704 07:19:31.170347 22258 net.cpp:148] Setting up fc10_fc10_0_split
I0704 07:19:31.170353 22258 net.cpp:155] Top shape: 50 10 (500)
I0704 07:19:31.170357 22258 net.cpp:155] Top shape: 50 10 (500)
I0704 07:19:31.170362 22258 net.cpp:155] Top shape: 50 10 (500)
I0704 07:19:31.170367 22258 net.cpp:163] Memory required for data: 296252000
I0704 07:19:31.170370 22258 layer_factory.hpp:77] Creating layer loss
I0704 07:19:31.170377 22258 net.cpp:98] Creating Layer loss
I0704 07:19:31.170380 22258 net.cpp:439] loss <- fc10_fc10_0_split_0
I0704 07:19:31.170388 22258 net.cpp:439] loss <- label_data_1_split_0
I0704 07:19:31.170393 22258 net.cpp:413] loss -> loss
I0704 07:19:31.170400 22258 layer_factory.hpp:77] Creating layer loss
I0704 07:19:31.170509 22258 net.cpp:148] Setting up loss
I0704 07:19:31.170514 22258 net.cpp:155] Top shape: (1)
I0704 07:19:31.170518 22258 net.cpp:158]     with loss weight 1
I0704 07:19:31.170527 22258 net.cpp:163] Memory required for data: 296252004
I0704 07:19:31.170532 22258 layer_factory.hpp:77] Creating layer accuracy/top1
I0704 07:19:31.170542 22258 net.cpp:98] Creating Layer accuracy/top1
I0704 07:19:31.170547 22258 net.cpp:439] accuracy/top1 <- fc10_fc10_0_split_1
I0704 07:19:31.170550 22258 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0704 07:19:31.170557 22258 net.cpp:413] accuracy/top1 -> accuracy/top1
I0704 07:19:31.170563 22258 net.cpp:148] Setting up accuracy/top1
I0704 07:19:31.170567 22258 net.cpp:155] Top shape: (1)
I0704 07:19:31.170572 22258 net.cpp:163] Memory required for data: 296252008
I0704 07:19:31.170575 22258 layer_factory.hpp:77] Creating layer accuracy/top5
I0704 07:19:31.170586 22258 net.cpp:98] Creating Layer accuracy/top5
I0704 07:19:31.170589 22258 net.cpp:439] accuracy/top5 <- fc10_fc10_0_split_2
I0704 07:19:31.170593 22258 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0704 07:19:31.170598 22258 net.cpp:413] accuracy/top5 -> accuracy/top5
I0704 07:19:31.170604 22258 net.cpp:148] Setting up accuracy/top5
I0704 07:19:31.170609 22258 net.cpp:155] Top shape: (1)
I0704 07:19:31.170613 22258 net.cpp:163] Memory required for data: 296252012
I0704 07:19:31.170616 22258 net.cpp:226] accuracy/top5 does not need backward computation.
I0704 07:19:31.170621 22258 net.cpp:226] accuracy/top1 does not need backward computation.
I0704 07:19:31.170625 22258 net.cpp:224] loss needs backward computation.
I0704 07:19:31.170629 22258 net.cpp:224] fc10_fc10_0_split needs backward computation.
I0704 07:19:31.170634 22258 net.cpp:224] fc10 needs backward computation.
I0704 07:19:31.170637 22258 net.cpp:224] pool5 needs backward computation.
I0704 07:19:31.170641 22258 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0704 07:19:31.170650 22258 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0704 07:19:31.170655 22258 net.cpp:224] res5a_branch2b needs backward computation.
I0704 07:19:31.170660 22258 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0704 07:19:31.170663 22258 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0704 07:19:31.170667 22258 net.cpp:224] res5a_branch2a needs backward computation.
I0704 07:19:31.170672 22258 net.cpp:224] pool4 needs backward computation.
I0704 07:19:31.170676 22258 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0704 07:19:31.170680 22258 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0704 07:19:31.170684 22258 net.cpp:224] res4a_branch2b needs backward computation.
I0704 07:19:31.170688 22258 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0704 07:19:31.170692 22258 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0704 07:19:31.170696 22258 net.cpp:224] res4a_branch2a needs backward computation.
I0704 07:19:31.170701 22258 net.cpp:224] pool3 needs backward computation.
I0704 07:19:31.170706 22258 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0704 07:19:31.170708 22258 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0704 07:19:31.170713 22258 net.cpp:224] res3a_branch2b needs backward computation.
I0704 07:19:31.170717 22258 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0704 07:19:31.170722 22258 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0704 07:19:31.170725 22258 net.cpp:224] res3a_branch2a needs backward computation.
I0704 07:19:31.170729 22258 net.cpp:224] pool2 needs backward computation.
I0704 07:19:31.170733 22258 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0704 07:19:31.170738 22258 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0704 07:19:31.170742 22258 net.cpp:224] res2a_branch2b needs backward computation.
I0704 07:19:31.170747 22258 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0704 07:19:31.170750 22258 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0704 07:19:31.170754 22258 net.cpp:224] res2a_branch2a needs backward computation.
I0704 07:19:31.170758 22258 net.cpp:224] pool1 needs backward computation.
I0704 07:19:31.170763 22258 net.cpp:224] conv1b/relu needs backward computation.
I0704 07:19:31.170766 22258 net.cpp:224] conv1b/bn needs backward computation.
I0704 07:19:31.170770 22258 net.cpp:224] conv1b needs backward computation.
I0704 07:19:31.170775 22258 net.cpp:224] conv1a/relu needs backward computation.
I0704 07:19:31.170779 22258 net.cpp:224] conv1a/bn needs backward computation.
I0704 07:19:31.170783 22258 net.cpp:224] conv1a needs backward computation.
I0704 07:19:31.170788 22258 net.cpp:226] data/bias does not need backward computation.
I0704 07:19:31.170792 22258 net.cpp:226] label_data_1_split does not need backward computation.
I0704 07:19:31.170797 22258 net.cpp:226] data does not need backward computation.
I0704 07:19:31.170801 22258 net.cpp:268] This network produces output accuracy/top1
I0704 07:19:31.170805 22258 net.cpp:268] This network produces output accuracy/top5
I0704 07:19:31.170809 22258 net.cpp:268] This network produces output loss
I0704 07:19:31.170835 22258 net.cpp:288] Network initialization done.
I0704 07:19:31.170899 22258 solver.cpp:60] Solver scaffolding done.
I0704 07:19:31.183224 22258 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0704 07:19:31.183305 22258 data_layer.cpp:83] output data size: 21,3,32,32
I0704 07:19:31.641721 22258 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0704 07:19:31.641786 22258 data_layer.cpp:83] output data size: 21,3,32,32
I0704 07:19:32.127071 22258 parallel.cpp:334] Starting Optimization
I0704 07:19:32.127126 22258 solver.cpp:408] Solving jacintonet11v2_train
I0704 07:19:32.127131 22258 solver.cpp:409] Learning Rate Policy: poly
I0704 07:19:32.132556 22258 solver.cpp:466] Iteration 0, Testing net (#0)
I0704 07:19:33.810977 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.1
I0704 07:19:33.811008 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.504
I0704 07:19:33.811014 22258 solver.cpp:539]     Test net output #2: loss = 78.2999 (* 1 = 78.2999 loss)
I0704 07:19:33.926604 22258 solver.cpp:290] Iteration 0 (0 iter/s, 1.7994s/100 iter), loss = 1.57143
I0704 07:19:33.926630 22258 solver.cpp:309]     Train net output #0: loss = 1.57143 (* 1 = 1.57143 loss)
I0704 07:19:33.926642 22258 sgd_solver.cpp:106] Iteration 0, lr = 0.1
I0704 07:19:35.901722 22258 solver.cpp:290] Iteration 100 (50.6321 iter/s, 1.97503s/100 iter), loss = 1.19048
I0704 07:19:35.901743 22258 solver.cpp:309]     Train net output #0: loss = 1.19048 (* 1 = 1.19048 loss)
I0704 07:19:35.901751 22258 sgd_solver.cpp:106] Iteration 100, lr = 0.0998438
I0704 07:19:37.855943 22258 solver.cpp:290] Iteration 200 (51.1734 iter/s, 1.95414s/100 iter), loss = 1.04762
I0704 07:19:37.855967 22258 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0704 07:19:37.855973 22258 sgd_solver.cpp:106] Iteration 200, lr = 0.0996875
I0704 07:19:39.809893 22258 solver.cpp:290] Iteration 300 (51.1806 iter/s, 1.95386s/100 iter), loss = 0.761905
I0704 07:19:39.809918 22258 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0704 07:19:39.809926 22258 sgd_solver.cpp:106] Iteration 300, lr = 0.0995313
I0704 07:19:41.765260 22258 solver.cpp:290] Iteration 400 (51.1435 iter/s, 1.95528s/100 iter), loss = 0.809524
I0704 07:19:41.765283 22258 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0704 07:19:41.765290 22258 sgd_solver.cpp:106] Iteration 400, lr = 0.099375
I0704 07:19:43.718288 22258 solver.cpp:290] Iteration 500 (51.2047 iter/s, 1.95294s/100 iter), loss = 1.04762
I0704 07:19:43.718310 22258 solver.cpp:309]     Train net output #0: loss = 1.04762 (* 1 = 1.04762 loss)
I0704 07:19:43.718317 22258 sgd_solver.cpp:106] Iteration 500, lr = 0.0992187
I0704 07:19:45.703239 22258 solver.cpp:290] Iteration 600 (50.3812 iter/s, 1.98487s/100 iter), loss = 0.666667
I0704 07:19:45.703263 22258 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0704 07:19:45.703271 22258 sgd_solver.cpp:106] Iteration 600, lr = 0.0990625
I0704 07:19:47.709741 22258 solver.cpp:290] Iteration 700 (49.8401 iter/s, 2.00642s/100 iter), loss = 0.809524
I0704 07:19:47.709765 22258 solver.cpp:309]     Train net output #0: loss = 0.809524 (* 1 = 0.809524 loss)
I0704 07:19:47.709771 22258 sgd_solver.cpp:106] Iteration 700, lr = 0.0989062
I0704 07:19:49.733322 22258 solver.cpp:290] Iteration 800 (49.4195 iter/s, 2.02349s/100 iter), loss = 0.238095
I0704 07:19:49.733345 22258 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0704 07:19:49.733351 22258 sgd_solver.cpp:106] Iteration 800, lr = 0.09875
I0704 07:19:51.756181 22258 solver.cpp:290] Iteration 900 (49.4371 iter/s, 2.02277s/100 iter), loss = 0.571428
I0704 07:19:51.756204 22258 solver.cpp:309]     Train net output #0: loss = 0.571429 (* 1 = 0.571429 loss)
I0704 07:19:51.756211 22258 sgd_solver.cpp:106] Iteration 900, lr = 0.0985937
I0704 07:19:53.777221 22258 solver.cpp:466] Iteration 1000, Testing net (#0)
I0704 07:19:55.417130 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.4962
I0704 07:19:55.417148 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.947001
I0704 07:19:55.417155 22258 solver.cpp:539]     Test net output #2: loss = 1.2498 (* 1 = 1.2498 loss)
I0704 07:19:55.437294 22258 solver.cpp:290] Iteration 1000 (27.1667 iter/s, 3.68098s/100 iter), loss = 0.238095
I0704 07:19:55.437324 22258 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0704 07:19:55.437335 22258 sgd_solver.cpp:106] Iteration 1000, lr = 0.0984375
I0704 07:19:57.488809 22258 solver.cpp:290] Iteration 1100 (48.7466 iter/s, 2.05143s/100 iter), loss = 0.0952379
I0704 07:19:57.488831 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:19:57.488838 22258 sgd_solver.cpp:106] Iteration 1100, lr = 0.0982813
I0704 07:19:59.545660 22258 solver.cpp:290] Iteration 1200 (48.62 iter/s, 2.05677s/100 iter), loss = 0.428571
I0704 07:19:59.545681 22258 solver.cpp:309]     Train net output #0: loss = 0.428571 (* 1 = 0.428571 loss)
I0704 07:19:59.545689 22258 sgd_solver.cpp:106] Iteration 1200, lr = 0.098125
I0704 07:20:01.602787 22258 solver.cpp:290] Iteration 1300 (48.6135 iter/s, 2.05704s/100 iter), loss = 0.666666
I0704 07:20:01.602885 22258 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0704 07:20:01.602895 22258 sgd_solver.cpp:106] Iteration 1300, lr = 0.0979687
I0704 07:20:03.662585 22258 solver.cpp:290] Iteration 1400 (48.5522 iter/s, 2.05964s/100 iter), loss = 0.619047
I0704 07:20:03.662608 22258 solver.cpp:309]     Train net output #0: loss = 0.619048 (* 1 = 0.619048 loss)
I0704 07:20:03.662617 22258 sgd_solver.cpp:106] Iteration 1400, lr = 0.0978125
I0704 07:20:05.717823 22258 solver.cpp:290] Iteration 1500 (48.6582 iter/s, 2.05515s/100 iter), loss = 0.428571
I0704 07:20:05.717846 22258 solver.cpp:309]     Train net output #0: loss = 0.428571 (* 1 = 0.428571 loss)
I0704 07:20:05.717852 22258 sgd_solver.cpp:106] Iteration 1500, lr = 0.0976562
I0704 07:20:07.775698 22258 solver.cpp:290] Iteration 1600 (48.5959 iter/s, 2.05779s/100 iter), loss = 0.190475
I0704 07:20:07.775720 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:20:07.775727 22258 sgd_solver.cpp:106] Iteration 1600, lr = 0.0975
I0704 07:20:09.834818 22258 solver.cpp:290] Iteration 1700 (48.5665 iter/s, 2.05903s/100 iter), loss = 0.523809
I0704 07:20:09.834841 22258 solver.cpp:309]     Train net output #0: loss = 0.52381 (* 1 = 0.52381 loss)
I0704 07:20:09.834847 22258 sgd_solver.cpp:106] Iteration 1700, lr = 0.0973438
I0704 07:20:11.894435 22258 solver.cpp:290] Iteration 1800 (48.5548 iter/s, 2.05953s/100 iter), loss = 0.666666
I0704 07:20:11.894457 22258 solver.cpp:309]     Train net output #0: loss = 0.666667 (* 1 = 0.666667 loss)
I0704 07:20:11.894465 22258 sgd_solver.cpp:106] Iteration 1800, lr = 0.0971875
I0704 07:20:13.950047 22258 solver.cpp:290] Iteration 1900 (48.6494 iter/s, 2.05552s/100 iter), loss = 0.380952
I0704 07:20:13.950073 22258 solver.cpp:309]     Train net output #0: loss = 0.380952 (* 1 = 0.380952 loss)
I0704 07:20:13.950081 22258 sgd_solver.cpp:106] Iteration 1900, lr = 0.0970313
I0704 07:20:15.987185 22258 solver.cpp:466] Iteration 2000, Testing net (#0)
I0704 07:20:17.645512 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.6411
I0704 07:20:17.645531 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.965501
I0704 07:20:17.645536 22258 solver.cpp:539]     Test net output #2: loss = 0.6985 (* 1 = 0.6985 loss)
I0704 07:20:17.665336 22258 solver.cpp:290] Iteration 2000 (26.9167 iter/s, 3.71516s/100 iter), loss = 0.190476
I0704 07:20:17.665354 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:20:17.665365 22258 sgd_solver.cpp:106] Iteration 2000, lr = 0.096875
I0704 07:20:19.721021 22258 solver.cpp:290] Iteration 2100 (48.6475 iter/s, 2.0556s/100 iter), loss = 0.47619
I0704 07:20:19.721047 22258 solver.cpp:309]     Train net output #0: loss = 0.47619 (* 1 = 0.47619 loss)
I0704 07:20:19.721056 22258 sgd_solver.cpp:106] Iteration 2100, lr = 0.0967188
I0704 07:20:21.781574 22258 solver.cpp:290] Iteration 2200 (48.5328 iter/s, 2.06046s/100 iter), loss = 0.380951
I0704 07:20:21.781601 22258 solver.cpp:309]     Train net output #0: loss = 0.380952 (* 1 = 0.380952 loss)
I0704 07:20:21.781610 22258 sgd_solver.cpp:106] Iteration 2200, lr = 0.0965625
I0704 07:20:23.839416 22258 solver.cpp:290] Iteration 2300 (48.5967 iter/s, 2.05775s/100 iter), loss = 0.190475
I0704 07:20:23.839438 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:20:23.839447 22258 sgd_solver.cpp:106] Iteration 2300, lr = 0.0964063
I0704 07:20:25.894953 22258 solver.cpp:290] Iteration 2400 (48.6511 iter/s, 2.05545s/100 iter), loss = 0.380951
I0704 07:20:25.894975 22258 solver.cpp:309]     Train net output #0: loss = 0.380952 (* 1 = 0.380952 loss)
I0704 07:20:25.894981 22258 sgd_solver.cpp:106] Iteration 2400, lr = 0.09625
I0704 07:20:27.948998 22258 solver.cpp:290] Iteration 2500 (48.6865 iter/s, 2.05396s/100 iter), loss = 0.714285
I0704 07:20:27.949019 22258 solver.cpp:309]     Train net output #0: loss = 0.714286 (* 1 = 0.714286 loss)
I0704 07:20:27.949026 22258 sgd_solver.cpp:106] Iteration 2500, lr = 0.0960938
I0704 07:20:30.003603 22258 solver.cpp:290] Iteration 2600 (48.6731 iter/s, 2.05452s/100 iter), loss = 0.190475
I0704 07:20:30.003625 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:20:30.003633 22258 sgd_solver.cpp:106] Iteration 2600, lr = 0.0959375
I0704 07:20:32.061794 22258 solver.cpp:290] Iteration 2700 (48.5884 iter/s, 2.0581s/100 iter), loss = 0.761904
I0704 07:20:32.061873 22258 solver.cpp:309]     Train net output #0: loss = 0.761905 (* 1 = 0.761905 loss)
I0704 07:20:32.061882 22258 sgd_solver.cpp:106] Iteration 2700, lr = 0.0957813
I0704 07:20:34.148313 22258 solver.cpp:290] Iteration 2800 (47.9299 iter/s, 2.08638s/100 iter), loss = 0.333332
I0704 07:20:34.148335 22258 solver.cpp:309]     Train net output #0: loss = 0.333333 (* 1 = 0.333333 loss)
I0704 07:20:34.148344 22258 sgd_solver.cpp:106] Iteration 2800, lr = 0.095625
I0704 07:20:36.210000 22258 solver.cpp:290] Iteration 2900 (48.506 iter/s, 2.0616s/100 iter), loss = 0.333332
I0704 07:20:36.210023 22258 solver.cpp:309]     Train net output #0: loss = 0.333333 (* 1 = 0.333333 loss)
I0704 07:20:36.210031 22258 sgd_solver.cpp:106] Iteration 2900, lr = 0.0954688
I0704 07:20:38.247095 22258 solver.cpp:466] Iteration 3000, Testing net (#0)
I0704 07:20:39.902560 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.7264
I0704 07:20:39.902580 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9854
I0704 07:20:39.902585 22258 solver.cpp:539]     Test net output #2: loss = 0.5192 (* 1 = 0.5192 loss)
I0704 07:20:39.922394 22258 solver.cpp:290] Iteration 3000 (26.9378 iter/s, 3.71226s/100 iter), loss = 0.285713
I0704 07:20:39.922412 22258 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0704 07:20:39.922420 22258 sgd_solver.cpp:106] Iteration 3000, lr = 0.0953125
I0704 07:20:41.982990 22258 solver.cpp:290] Iteration 3100 (48.5316 iter/s, 2.06051s/100 iter), loss = 0.42857
I0704 07:20:41.983014 22258 solver.cpp:309]     Train net output #0: loss = 0.428571 (* 1 = 0.428571 loss)
I0704 07:20:41.983023 22258 sgd_solver.cpp:106] Iteration 3100, lr = 0.0951563
I0704 07:20:44.041141 22258 solver.cpp:290] Iteration 3200 (48.5893 iter/s, 2.05806s/100 iter), loss = 0.571428
I0704 07:20:44.041163 22258 solver.cpp:309]     Train net output #0: loss = 0.571429 (* 1 = 0.571429 loss)
I0704 07:20:44.041170 22258 sgd_solver.cpp:106] Iteration 3200, lr = 0.095
I0704 07:20:46.101352 22258 solver.cpp:290] Iteration 3300 (48.5407 iter/s, 2.06013s/100 iter), loss = 0.095237
I0704 07:20:46.101377 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:20:46.101387 22258 sgd_solver.cpp:106] Iteration 3300, lr = 0.0948438
I0704 07:20:48.164460 22258 solver.cpp:290] Iteration 3400 (48.4727 iter/s, 2.06302s/100 iter), loss = 0.238094
I0704 07:20:48.164486 22258 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0704 07:20:48.164495 22258 sgd_solver.cpp:106] Iteration 3400, lr = 0.0946875
I0704 07:20:50.221149 22258 solver.cpp:290] Iteration 3500 (48.624 iter/s, 2.0566s/100 iter), loss = 0.0476179
I0704 07:20:50.221175 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:20:50.221182 22258 sgd_solver.cpp:106] Iteration 3500, lr = 0.0945313
I0704 07:20:52.279947 22258 solver.cpp:290] Iteration 3600 (48.5742 iter/s, 2.05871s/100 iter), loss = 0.285713
I0704 07:20:52.279968 22258 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0704 07:20:52.279975 22258 sgd_solver.cpp:106] Iteration 3600, lr = 0.094375
I0704 07:20:54.348860 22258 solver.cpp:290] Iteration 3700 (48.3365 iter/s, 2.06883s/100 iter), loss = -1.19209e-06
I0704 07:20:54.348881 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:20:54.348888 22258 sgd_solver.cpp:106] Iteration 3700, lr = 0.0942188
I0704 07:20:56.410411 22258 solver.cpp:290] Iteration 3800 (48.5092 iter/s, 2.06147s/100 iter), loss = 0.380951
I0704 07:20:56.410432 22258 solver.cpp:309]     Train net output #0: loss = 0.380952 (* 1 = 0.380952 loss)
I0704 07:20:56.410439 22258 sgd_solver.cpp:106] Iteration 3800, lr = 0.0940625
I0704 07:20:58.468077 22258 solver.cpp:290] Iteration 3900 (48.6007 iter/s, 2.05758s/100 iter), loss = 0.285713
I0704 07:20:58.468099 22258 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0704 07:20:58.468107 22258 sgd_solver.cpp:106] Iteration 3900, lr = 0.0939062
I0704 07:21:00.508558 22258 solver.cpp:466] Iteration 4000, Testing net (#0)
I0704 07:21:02.147413 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.7707
I0704 07:21:02.147483 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9865
I0704 07:21:02.147490 22258 solver.cpp:539]     Test net output #2: loss = 0.4097 (* 1 = 0.4097 loss)
I0704 07:21:02.167141 22258 solver.cpp:290] Iteration 4000 (27.0348 iter/s, 3.69894s/100 iter), loss = 0.190475
I0704 07:21:02.167160 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:21:02.167171 22258 sgd_solver.cpp:106] Iteration 4000, lr = 0.09375
I0704 07:21:04.227687 22258 solver.cpp:290] Iteration 4100 (48.5328 iter/s, 2.06046s/100 iter), loss = 0.0476176
I0704 07:21:04.227710 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:21:04.227716 22258 sgd_solver.cpp:106] Iteration 4100, lr = 0.0935938
I0704 07:21:06.285501 22258 solver.cpp:290] Iteration 4200 (48.5973 iter/s, 2.05773s/100 iter), loss = 0.190475
I0704 07:21:06.285523 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:21:06.285531 22258 sgd_solver.cpp:106] Iteration 4200, lr = 0.0934375
I0704 07:21:08.347141 22258 solver.cpp:290] Iteration 4300 (48.5071 iter/s, 2.06155s/100 iter), loss = -1.40071e-06
I0704 07:21:08.347164 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:21:08.347172 22258 sgd_solver.cpp:106] Iteration 4300, lr = 0.0932813
I0704 07:21:10.412467 22258 solver.cpp:290] Iteration 4400 (48.4206 iter/s, 2.06524s/100 iter), loss = 0.142856
I0704 07:21:10.412489 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:21:10.412495 22258 sgd_solver.cpp:106] Iteration 4400, lr = 0.093125
I0704 07:21:12.479701 22258 solver.cpp:290] Iteration 4500 (48.3758 iter/s, 2.06715s/100 iter), loss = 0.238094
I0704 07:21:12.479727 22258 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0704 07:21:12.479734 22258 sgd_solver.cpp:106] Iteration 4500, lr = 0.0929688
I0704 07:21:14.536234 22258 solver.cpp:290] Iteration 4600 (48.6277 iter/s, 2.05644s/100 iter), loss = 0.42857
I0704 07:21:14.536264 22258 solver.cpp:309]     Train net output #0: loss = 0.428571 (* 1 = 0.428571 loss)
I0704 07:21:14.536275 22258 sgd_solver.cpp:106] Iteration 4600, lr = 0.0928125
I0704 07:21:16.596940 22258 solver.cpp:290] Iteration 4700 (48.5293 iter/s, 2.06061s/100 iter), loss = 0.285713
I0704 07:21:16.596961 22258 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0704 07:21:16.596967 22258 sgd_solver.cpp:106] Iteration 4700, lr = 0.0926562
I0704 07:21:18.655751 22258 solver.cpp:290] Iteration 4800 (48.5737 iter/s, 2.05873s/100 iter), loss = 0.0952365
I0704 07:21:18.655776 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:21:18.655784 22258 sgd_solver.cpp:106] Iteration 4800, lr = 0.0925
I0704 07:21:20.712956 22258 solver.cpp:290] Iteration 4900 (48.6117 iter/s, 2.05712s/100 iter), loss = 0.333332
I0704 07:21:20.712980 22258 solver.cpp:309]     Train net output #0: loss = 0.333333 (* 1 = 0.333333 loss)
I0704 07:21:20.712988 22258 sgd_solver.cpp:106] Iteration 4900, lr = 0.0923437
I0704 07:21:22.753774 22258 solver.cpp:466] Iteration 5000, Testing net (#0)
I0704 07:21:24.391381 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.7852
I0704 07:21:24.391399 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9853
I0704 07:21:24.391404 22258 solver.cpp:539]     Test net output #2: loss = 0.4558 (* 1 = 0.4558 loss)
I0704 07:21:24.412003 22258 solver.cpp:290] Iteration 5000 (27.035 iter/s, 3.69891s/100 iter), loss = 0.238094
I0704 07:21:24.412029 22258 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0704 07:21:24.412035 22258 sgd_solver.cpp:106] Iteration 5000, lr = 0.0921875
I0704 07:21:26.471104 22258 solver.cpp:290] Iteration 5100 (48.567 iter/s, 2.05901s/100 iter), loss = 0.0952365
I0704 07:21:26.471130 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:21:26.471140 22258 sgd_solver.cpp:106] Iteration 5100, lr = 0.0920313
I0704 07:21:28.527952 22258 solver.cpp:290] Iteration 5200 (48.6202 iter/s, 2.05676s/100 iter), loss = 0.285713
I0704 07:21:28.527976 22258 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0704 07:21:28.527981 22258 sgd_solver.cpp:106] Iteration 5200, lr = 0.091875
I0704 07:21:30.591810 22258 solver.cpp:290] Iteration 5300 (48.455 iter/s, 2.06377s/100 iter), loss = 0.0476174
I0704 07:21:30.591835 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:21:30.591845 22258 sgd_solver.cpp:106] Iteration 5300, lr = 0.0917188
I0704 07:21:32.653333 22258 solver.cpp:290] Iteration 5400 (48.5099 iter/s, 2.06144s/100 iter), loss = 0.285713
I0704 07:21:32.653398 22258 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0704 07:21:32.653406 22258 sgd_solver.cpp:106] Iteration 5400, lr = 0.0915625
I0704 07:21:34.744319 22258 solver.cpp:290] Iteration 5500 (47.8272 iter/s, 2.09086s/100 iter), loss = 0.238093
I0704 07:21:34.744343 22258 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0704 07:21:34.744349 22258 sgd_solver.cpp:106] Iteration 5500, lr = 0.0914062
I0704 07:21:36.806982 22258 solver.cpp:290] Iteration 5600 (48.4831 iter/s, 2.06257s/100 iter), loss = 0.190474
I0704 07:21:36.807004 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:21:36.807011 22258 sgd_solver.cpp:106] Iteration 5600, lr = 0.09125
I0704 07:21:38.865597 22258 solver.cpp:290] Iteration 5700 (48.5784 iter/s, 2.05853s/100 iter), loss = 0.285712
I0704 07:21:38.865618 22258 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0704 07:21:38.865625 22258 sgd_solver.cpp:106] Iteration 5700, lr = 0.0910937
I0704 07:21:40.926132 22258 solver.cpp:290] Iteration 5800 (48.5331 iter/s, 2.06045s/100 iter), loss = 0.285712
I0704 07:21:40.926154 22258 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0704 07:21:40.926162 22258 sgd_solver.cpp:106] Iteration 5800, lr = 0.0909375
I0704 07:21:42.985342 22258 solver.cpp:290] Iteration 5900 (48.5643 iter/s, 2.05912s/100 iter), loss = 0.523808
I0704 07:21:42.985365 22258 solver.cpp:309]     Train net output #0: loss = 0.52381 (* 1 = 0.52381 loss)
I0704 07:21:42.985373 22258 sgd_solver.cpp:106] Iteration 5900, lr = 0.0907812
I0704 07:21:45.024083 22258 solver.cpp:466] Iteration 6000, Testing net (#0)
I0704 07:21:46.665801 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.7851
I0704 07:21:46.665819 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9876
I0704 07:21:46.665824 22258 solver.cpp:539]     Test net output #2: loss = 0.457 (* 1 = 0.457 loss)
I0704 07:21:46.685428 22258 solver.cpp:290] Iteration 6000 (27.0273 iter/s, 3.69996s/100 iter), loss = 0.0952361
I0704 07:21:46.685447 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:21:46.685459 22258 sgd_solver.cpp:106] Iteration 6000, lr = 0.090625
I0704 07:21:48.743474 22258 solver.cpp:290] Iteration 6100 (48.5918 iter/s, 2.05796s/100 iter), loss = 0.095236
I0704 07:21:48.743496 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:21:48.743504 22258 sgd_solver.cpp:106] Iteration 6100, lr = 0.0904688
I0704 07:21:50.807554 22258 solver.cpp:290] Iteration 6200 (48.4498 iter/s, 2.06399s/100 iter), loss = 0.142855
I0704 07:21:50.807579 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:21:50.807588 22258 sgd_solver.cpp:106] Iteration 6200, lr = 0.0903125
I0704 07:21:52.867614 22258 solver.cpp:290] Iteration 6300 (48.5444 iter/s, 2.05997s/100 iter), loss = 0.428569
I0704 07:21:52.867635 22258 solver.cpp:309]     Train net output #0: loss = 0.428571 (* 1 = 0.428571 loss)
I0704 07:21:52.867642 22258 sgd_solver.cpp:106] Iteration 6300, lr = 0.0901562
I0704 07:21:54.932174 22258 solver.cpp:290] Iteration 6400 (48.4385 iter/s, 2.06448s/100 iter), loss = 0.0952359
I0704 07:21:54.932199 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:21:54.932206 22258 sgd_solver.cpp:106] Iteration 6400, lr = 0.09
I0704 07:21:56.990273 22258 solver.cpp:290] Iteration 6500 (48.5906 iter/s, 2.05801s/100 iter), loss = 0.0476168
I0704 07:21:56.990298 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:21:56.990306 22258 sgd_solver.cpp:106] Iteration 6500, lr = 0.0898438
I0704 07:21:59.056643 22258 solver.cpp:290] Iteration 6600 (48.396 iter/s, 2.06628s/100 iter), loss = 0.0952359
I0704 07:21:59.056666 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:21:59.056675 22258 sgd_solver.cpp:106] Iteration 6600, lr = 0.0896875
I0704 07:22:01.114162 22258 solver.cpp:290] Iteration 6700 (48.6043 iter/s, 2.05743s/100 iter), loss = 0.428569
I0704 07:22:01.114186 22258 solver.cpp:309]     Train net output #0: loss = 0.428571 (* 1 = 0.428571 loss)
I0704 07:22:01.114192 22258 sgd_solver.cpp:106] Iteration 6700, lr = 0.0895313
I0704 07:22:03.174007 22258 solver.cpp:290] Iteration 6800 (48.5494 iter/s, 2.05976s/100 iter), loss = 0.285712
I0704 07:22:03.174074 22258 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0704 07:22:03.174082 22258 sgd_solver.cpp:106] Iteration 6800, lr = 0.089375
I0704 07:22:05.230687 22258 solver.cpp:290] Iteration 6900 (48.6251 iter/s, 2.05655s/100 iter), loss = -2.26498e-06
I0704 07:22:05.230710 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:22:05.230716 22258 sgd_solver.cpp:106] Iteration 6900, lr = 0.0892188
I0704 07:22:07.268862 22258 solver.cpp:466] Iteration 7000, Testing net (#0)
I0704 07:22:08.912811 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.798
I0704 07:22:08.912830 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9846
I0704 07:22:08.912837 22258 solver.cpp:539]     Test net output #2: loss = 0.449 (* 1 = 0.449 loss)
I0704 07:22:08.932890 22258 solver.cpp:290] Iteration 7000 (27.0119 iter/s, 3.70208s/100 iter), loss = 0.0952358
I0704 07:22:08.932909 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:22:08.932920 22258 sgd_solver.cpp:106] Iteration 7000, lr = 0.0890625
I0704 07:22:10.993188 22258 solver.cpp:290] Iteration 7100 (48.5386 iter/s, 2.06022s/100 iter), loss = 0.0476168
I0704 07:22:10.993211 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:22:10.993221 22258 sgd_solver.cpp:106] Iteration 7100, lr = 0.0889063
I0704 07:22:13.058210 22258 solver.cpp:290] Iteration 7200 (48.4277 iter/s, 2.06494s/100 iter), loss = 0.0952358
I0704 07:22:13.058233 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:22:13.058239 22258 sgd_solver.cpp:106] Iteration 7200, lr = 0.08875
I0704 07:22:15.125329 22258 solver.cpp:290] Iteration 7300 (48.3785 iter/s, 2.06703s/100 iter), loss = 0.142855
I0704 07:22:15.125350 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:22:15.125357 22258 sgd_solver.cpp:106] Iteration 7300, lr = 0.0885938
I0704 07:22:17.182032 22258 solver.cpp:290] Iteration 7400 (48.6235 iter/s, 2.05662s/100 iter), loss = 0.142855
I0704 07:22:17.182054 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:22:17.182061 22258 sgd_solver.cpp:106] Iteration 7400, lr = 0.0884375
I0704 07:22:19.239243 22258 solver.cpp:290] Iteration 7500 (48.6115 iter/s, 2.05712s/100 iter), loss = -2.41399e-06
I0704 07:22:19.239265 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:22:19.239272 22258 sgd_solver.cpp:106] Iteration 7500, lr = 0.0882813
I0704 07:22:21.300228 22258 solver.cpp:290] Iteration 7600 (48.5225 iter/s, 2.0609s/100 iter), loss = 0.190474
I0704 07:22:21.300253 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:22:21.300263 22258 sgd_solver.cpp:106] Iteration 7600, lr = 0.088125
I0704 07:22:23.358940 22258 solver.cpp:290] Iteration 7700 (48.5761 iter/s, 2.05862s/100 iter), loss = 0.142855
I0704 07:22:23.358963 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:22:23.358970 22258 sgd_solver.cpp:106] Iteration 7700, lr = 0.0879688
I0704 07:22:25.421277 22258 solver.cpp:290] Iteration 7800 (48.4907 iter/s, 2.06225s/100 iter), loss = 0.0952357
I0704 07:22:25.421304 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:22:25.421314 22258 sgd_solver.cpp:106] Iteration 7800, lr = 0.0878125
I0704 07:22:27.481004 22258 solver.cpp:290] Iteration 7900 (48.5522 iter/s, 2.05964s/100 iter), loss = 0.0476167
I0704 07:22:27.481027 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:22:27.481034 22258 sgd_solver.cpp:106] Iteration 7900, lr = 0.0876563
I0704 07:22:29.517899 22258 solver.cpp:466] Iteration 8000, Testing net (#0)
I0704 07:22:31.164504 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.7681
I0704 07:22:31.164525 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9778
I0704 07:22:31.164530 22258 solver.cpp:539]     Test net output #2: loss = 0.5832 (* 1 = 0.5832 loss)
I0704 07:22:31.184648 22258 solver.cpp:290] Iteration 8000 (27.0014 iter/s, 3.70352s/100 iter), loss = 0.0476167
I0704 07:22:31.184669 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:22:31.184679 22258 sgd_solver.cpp:106] Iteration 8000, lr = 0.0875
I0704 07:22:33.287837 22258 solver.cpp:290] Iteration 8100 (47.5488 iter/s, 2.1031s/100 iter), loss = 0.238093
I0704 07:22:33.287891 22258 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0704 07:22:33.287899 22258 sgd_solver.cpp:106] Iteration 8100, lr = 0.0873438
I0704 07:22:35.348384 22258 solver.cpp:290] Iteration 8200 (48.5336 iter/s, 2.06043s/100 iter), loss = -2.38419e-06
I0704 07:22:35.348410 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:22:35.348419 22258 sgd_solver.cpp:106] Iteration 8200, lr = 0.0871875
I0704 07:22:37.409181 22258 solver.cpp:290] Iteration 8300 (48.527 iter/s, 2.06071s/100 iter), loss = 0.142855
I0704 07:22:37.409204 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:22:37.409212 22258 sgd_solver.cpp:106] Iteration 8300, lr = 0.0870313
I0704 07:22:39.466778 22258 solver.cpp:290] Iteration 8400 (48.6024 iter/s, 2.05751s/100 iter), loss = 0.0952357
I0704 07:22:39.466799 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:22:39.466809 22258 sgd_solver.cpp:106] Iteration 8400, lr = 0.086875
I0704 07:22:41.524201 22258 solver.cpp:290] Iteration 8500 (48.6065 iter/s, 2.05734s/100 iter), loss = 0.142855
I0704 07:22:41.524224 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:22:41.524230 22258 sgd_solver.cpp:106] Iteration 8500, lr = 0.0867188
I0704 07:22:43.585454 22258 solver.cpp:290] Iteration 8600 (48.5162 iter/s, 2.06117s/100 iter), loss = 0.285712
I0704 07:22:43.585477 22258 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0704 07:22:43.585484 22258 sgd_solver.cpp:106] Iteration 8600, lr = 0.0865625
I0704 07:22:45.646983 22258 solver.cpp:290] Iteration 8700 (48.5098 iter/s, 2.06144s/100 iter), loss = 0.142855
I0704 07:22:45.647014 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:22:45.647024 22258 sgd_solver.cpp:106] Iteration 8700, lr = 0.0864063
I0704 07:22:47.706723 22258 solver.cpp:290] Iteration 8800 (48.552 iter/s, 2.05965s/100 iter), loss = 0.0952355
I0704 07:22:47.706748 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:22:47.706754 22258 sgd_solver.cpp:106] Iteration 8800, lr = 0.08625
I0704 07:22:49.768494 22258 solver.cpp:290] Iteration 8900 (48.5041 iter/s, 2.06168s/100 iter), loss = 0.190474
I0704 07:22:49.768518 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:22:49.768525 22258 sgd_solver.cpp:106] Iteration 8900, lr = 0.0860937
I0704 07:22:51.805833 22258 solver.cpp:466] Iteration 9000, Testing net (#0)
I0704 07:22:53.451304 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.5532
I0704 07:22:53.451325 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.952601
I0704 07:22:53.451330 22258 solver.cpp:539]     Test net output #2: loss = 1.7537 (* 1 = 1.7537 loss)
I0704 07:22:53.471254 22258 solver.cpp:290] Iteration 9000 (27.0078 iter/s, 3.70263s/100 iter), loss = 0.142855
I0704 07:22:53.471272 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:22:53.471287 22258 sgd_solver.cpp:106] Iteration 9000, lr = 0.0859375
I0704 07:22:55.532963 22258 solver.cpp:290] Iteration 9100 (48.5054 iter/s, 2.06163s/100 iter), loss = 0.0952355
I0704 07:22:55.532986 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:22:55.532996 22258 sgd_solver.cpp:106] Iteration 9100, lr = 0.0857813
I0704 07:22:57.599922 22258 solver.cpp:290] Iteration 9200 (48.3823 iter/s, 2.06687s/100 iter), loss = 0.0476165
I0704 07:22:57.599947 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:22:57.599957 22258 sgd_solver.cpp:106] Iteration 9200, lr = 0.085625
I0704 07:22:59.661468 22258 solver.cpp:290] Iteration 9300 (48.5095 iter/s, 2.06145s/100 iter), loss = -2.65986e-06
I0704 07:22:59.661501 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:22:59.661510 22258 sgd_solver.cpp:106] Iteration 9300, lr = 0.0854688
I0704 07:23:01.718189 22258 solver.cpp:290] Iteration 9400 (48.6233 iter/s, 2.05663s/100 iter), loss = -2.71201e-06
I0704 07:23:01.718209 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:23:01.718216 22258 sgd_solver.cpp:106] Iteration 9400, lr = 0.0853125
I0704 07:23:03.776391 22258 solver.cpp:290] Iteration 9500 (48.5881 iter/s, 2.05812s/100 iter), loss = 0.0952353
I0704 07:23:03.776464 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:23:03.776471 22258 sgd_solver.cpp:106] Iteration 9500, lr = 0.0851563
I0704 07:23:05.838305 22258 solver.cpp:290] Iteration 9600 (48.5018 iter/s, 2.06178s/100 iter), loss = 0.0476163
I0704 07:23:05.838328 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:23:05.838336 22258 sgd_solver.cpp:106] Iteration 9600, lr = 0.085
I0704 07:23:07.898310 22258 solver.cpp:290] Iteration 9700 (48.5456 iter/s, 2.05992s/100 iter), loss = 0.0476163
I0704 07:23:07.898334 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:23:07.898341 22258 sgd_solver.cpp:106] Iteration 9700, lr = 0.0848437
I0704 07:23:09.959601 22258 solver.cpp:290] Iteration 9800 (48.5153 iter/s, 2.0612s/100 iter), loss = -2.74181e-06
I0704 07:23:09.959625 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:23:09.959632 22258 sgd_solver.cpp:106] Iteration 9800, lr = 0.0846875
I0704 07:23:12.015470 22258 solver.cpp:290] Iteration 9900 (48.6433 iter/s, 2.05578s/100 iter), loss = 0.0476163
I0704 07:23:12.015492 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:23:12.015498 22258 sgd_solver.cpp:106] Iteration 9900, lr = 0.0845312
I0704 07:23:14.055552 22258 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/cifar10_jacintonet11v2_iter_10000.caffemodel
I0704 07:23:14.080358 22258 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/cifar10_jacintonet11v2_iter_10000.solverstate
I0704 07:23:14.087997 22258 solver.cpp:466] Iteration 10000, Testing net (#0)
I0704 07:23:15.735076 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8068
I0704 07:23:15.735095 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9923
I0704 07:23:15.735100 22258 solver.cpp:539]     Test net output #2: loss = 0.4304 (* 1 = 0.4304 loss)
I0704 07:23:15.754745 22258 solver.cpp:290] Iteration 10000 (26.7441 iter/s, 3.73915s/100 iter), loss = -2.77162e-06
I0704 07:23:15.754762 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:23:15.754773 22258 sgd_solver.cpp:106] Iteration 10000, lr = 0.084375
I0704 07:23:17.812039 22258 solver.cpp:290] Iteration 10100 (48.6094 iter/s, 2.05721s/100 iter), loss = 0.142854
I0704 07:23:17.812062 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:23:17.812069 22258 sgd_solver.cpp:106] Iteration 10100, lr = 0.0842188
I0704 07:23:19.868369 22258 solver.cpp:290] Iteration 10200 (48.6324 iter/s, 2.05624s/100 iter), loss = 0.0476162
I0704 07:23:19.868393 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:23:19.868399 22258 sgd_solver.cpp:106] Iteration 10200, lr = 0.0840625
I0704 07:23:21.925618 22258 solver.cpp:290] Iteration 10300 (48.6106 iter/s, 2.05716s/100 iter), loss = 0.0476162
I0704 07:23:21.925640 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:23:21.925647 22258 sgd_solver.cpp:106] Iteration 10300, lr = 0.0839063
I0704 07:23:23.985685 22258 solver.cpp:290] Iteration 10400 (48.5441 iter/s, 2.05998s/100 iter), loss = 0.428569
I0704 07:23:23.985719 22258 solver.cpp:309]     Train net output #0: loss = 0.428571 (* 1 = 0.428571 loss)
I0704 07:23:23.985730 22258 sgd_solver.cpp:106] Iteration 10400, lr = 0.08375
I0704 07:23:26.046288 22258 solver.cpp:290] Iteration 10500 (48.5317 iter/s, 2.06051s/100 iter), loss = 0.142854
I0704 07:23:26.046314 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:23:26.046320 22258 sgd_solver.cpp:106] Iteration 10500, lr = 0.0835937
I0704 07:23:28.104130 22258 solver.cpp:290] Iteration 10600 (48.5966 iter/s, 2.05776s/100 iter), loss = 0.0476161
I0704 07:23:28.104153 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:23:28.104177 22258 sgd_solver.cpp:106] Iteration 10600, lr = 0.0834375
I0704 07:23:30.163024 22258 solver.cpp:290] Iteration 10700 (48.5718 iter/s, 2.05881s/100 iter), loss = 0.142854
I0704 07:23:30.163048 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:23:30.163054 22258 sgd_solver.cpp:106] Iteration 10700, lr = 0.0832812
I0704 07:23:32.221820 22258 solver.cpp:290] Iteration 10800 (48.5741 iter/s, 2.05871s/100 iter), loss = -3.01749e-06
I0704 07:23:32.221843 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:23:32.221851 22258 sgd_solver.cpp:106] Iteration 10800, lr = 0.083125
I0704 07:23:34.288861 22258 solver.cpp:290] Iteration 10900 (48.3804 iter/s, 2.06695s/100 iter), loss = -3.02494e-06
I0704 07:23:34.288936 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:23:34.288944 22258 sgd_solver.cpp:106] Iteration 10900, lr = 0.0829687
I0704 07:23:36.326272 22258 solver.cpp:466] Iteration 11000, Testing net (#0)
I0704 07:23:37.972976 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.7851
I0704 07:23:37.972997 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9859
I0704 07:23:37.973002 22258 solver.cpp:539]     Test net output #2: loss = 0.5706 (* 1 = 0.5706 loss)
I0704 07:23:37.992532 22258 solver.cpp:290] Iteration 11000 (27.0015 iter/s, 3.70349s/100 iter), loss = 0.047616
I0704 07:23:37.992547 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:23:37.992561 22258 sgd_solver.cpp:106] Iteration 11000, lr = 0.0828125
I0704 07:23:40.049545 22258 solver.cpp:290] Iteration 11100 (48.6161 iter/s, 2.05693s/100 iter), loss = 0.238092
I0704 07:23:40.049566 22258 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0704 07:23:40.049573 22258 sgd_solver.cpp:106] Iteration 11100, lr = 0.0826563
I0704 07:23:42.109294 22258 solver.cpp:290] Iteration 11200 (48.5516 iter/s, 2.05966s/100 iter), loss = 0.33333
I0704 07:23:42.109316 22258 solver.cpp:309]     Train net output #0: loss = 0.333333 (* 1 = 0.333333 loss)
I0704 07:23:42.109325 22258 sgd_solver.cpp:106] Iteration 11200, lr = 0.0825
I0704 07:23:44.168130 22258 solver.cpp:290] Iteration 11300 (48.5731 iter/s, 2.05875s/100 iter), loss = 0.047616
I0704 07:23:44.168153 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:23:44.168159 22258 sgd_solver.cpp:106] Iteration 11300, lr = 0.0823437
I0704 07:23:46.228310 22258 solver.cpp:290] Iteration 11400 (48.5414 iter/s, 2.0601s/100 iter), loss = 0.095235
I0704 07:23:46.228333 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:23:46.228340 22258 sgd_solver.cpp:106] Iteration 11400, lr = 0.0821875
I0704 07:23:48.283812 22258 solver.cpp:290] Iteration 11500 (48.652 iter/s, 2.05541s/100 iter), loss = 0.095235
I0704 07:23:48.283836 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:23:48.283845 22258 sgd_solver.cpp:106] Iteration 11500, lr = 0.0820312
I0704 07:23:50.342164 22258 solver.cpp:290] Iteration 11600 (48.5845 iter/s, 2.05827s/100 iter), loss = 0.0476159
I0704 07:23:50.342186 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:23:50.342192 22258 sgd_solver.cpp:106] Iteration 11600, lr = 0.081875
I0704 07:23:52.403970 22258 solver.cpp:290] Iteration 11700 (48.5032 iter/s, 2.06172s/100 iter), loss = 0.0476159
I0704 07:23:52.403992 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:23:52.404000 22258 sgd_solver.cpp:106] Iteration 11700, lr = 0.0817188
I0704 07:23:54.459450 22258 solver.cpp:290] Iteration 11800 (48.6525 iter/s, 2.05539s/100 iter), loss = 0.0476159
I0704 07:23:54.459472 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:23:54.459480 22258 sgd_solver.cpp:106] Iteration 11800, lr = 0.0815625
I0704 07:23:56.517051 22258 solver.cpp:290] Iteration 11900 (48.6023 iter/s, 2.05752s/100 iter), loss = 0.142854
I0704 07:23:56.517076 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:23:56.517081 22258 sgd_solver.cpp:106] Iteration 11900, lr = 0.0814063
I0704 07:23:58.553408 22258 solver.cpp:466] Iteration 12000, Testing net (#0)
I0704 07:24:00.204812 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.7398
I0704 07:24:00.204833 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9619
I0704 07:24:00.204838 22258 solver.cpp:539]     Test net output #2: loss = 0.7685 (* 1 = 0.7685 loss)
I0704 07:24:00.225667 22258 solver.cpp:290] Iteration 12000 (26.9652 iter/s, 3.70848s/100 iter), loss = 0.142854
I0704 07:24:00.225692 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:24:00.225711 22258 sgd_solver.cpp:106] Iteration 12000, lr = 0.08125
I0704 07:24:02.284162 22258 solver.cpp:290] Iteration 12100 (48.5812 iter/s, 2.05841s/100 iter), loss = 0.238092
I0704 07:24:02.284188 22258 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0704 07:24:02.284194 22258 sgd_solver.cpp:106] Iteration 12100, lr = 0.0810938
I0704 07:24:04.339680 22258 solver.cpp:290] Iteration 12200 (48.6516 iter/s, 2.05543s/100 iter), loss = -3.18885e-06
I0704 07:24:04.339752 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:24:04.339762 22258 sgd_solver.cpp:106] Iteration 12200, lr = 0.0809375
I0704 07:24:06.395663 22258 solver.cpp:290] Iteration 12300 (48.6417 iter/s, 2.05585s/100 iter), loss = -3.15905e-06
I0704 07:24:06.395685 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:24:06.395692 22258 sgd_solver.cpp:106] Iteration 12300, lr = 0.0807813
I0704 07:24:08.457631 22258 solver.cpp:290] Iteration 12400 (48.4994 iter/s, 2.06188s/100 iter), loss = 0.476187
I0704 07:24:08.457654 22258 solver.cpp:309]     Train net output #0: loss = 0.47619 (* 1 = 0.47619 loss)
I0704 07:24:08.457661 22258 sgd_solver.cpp:106] Iteration 12400, lr = 0.080625
I0704 07:24:10.520961 22258 solver.cpp:290] Iteration 12500 (48.4674 iter/s, 2.06324s/100 iter), loss = 0.0476159
I0704 07:24:10.520985 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:24:10.520992 22258 sgd_solver.cpp:106] Iteration 12500, lr = 0.0804688
I0704 07:24:12.581516 22258 solver.cpp:290] Iteration 12600 (48.5327 iter/s, 2.06047s/100 iter), loss = 0.190473
I0704 07:24:12.581539 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:24:12.581545 22258 sgd_solver.cpp:106] Iteration 12600, lr = 0.0803125
I0704 07:24:14.638375 22258 solver.cpp:290] Iteration 12700 (48.6199 iter/s, 2.05677s/100 iter), loss = 0.142854
I0704 07:24:14.638401 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:24:14.638408 22258 sgd_solver.cpp:106] Iteration 12700, lr = 0.0801563
I0704 07:24:16.696307 22258 solver.cpp:290] Iteration 12800 (48.5946 iter/s, 2.05784s/100 iter), loss = -3.18885e-06
I0704 07:24:16.696329 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:24:16.696336 22258 sgd_solver.cpp:106] Iteration 12800, lr = 0.08
I0704 07:24:18.754549 22258 solver.cpp:290] Iteration 12900 (48.5872 iter/s, 2.05816s/100 iter), loss = 0.285711
I0704 07:24:18.754572 22258 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0704 07:24:18.754580 22258 sgd_solver.cpp:106] Iteration 12900, lr = 0.0798438
I0704 07:24:20.795524 22258 solver.cpp:466] Iteration 13000, Testing net (#0)
I0704 07:24:22.445063 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.7142
I0704 07:24:22.445085 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9782
I0704 07:24:22.445089 22258 solver.cpp:539]     Test net output #2: loss = 0.9118 (* 1 = 0.9118 loss)
I0704 07:24:22.464874 22258 solver.cpp:290] Iteration 13000 (26.9527 iter/s, 3.7102s/100 iter), loss = 0.0952349
I0704 07:24:22.464895 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:24:22.464903 22258 sgd_solver.cpp:106] Iteration 13000, lr = 0.0796875
I0704 07:24:24.532933 22258 solver.cpp:290] Iteration 13100 (48.3565 iter/s, 2.06798s/100 iter), loss = 0.0952349
I0704 07:24:24.532958 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:24:24.532964 22258 sgd_solver.cpp:106] Iteration 13100, lr = 0.0795313
I0704 07:24:26.593739 22258 solver.cpp:290] Iteration 13200 (48.5268 iter/s, 2.06072s/100 iter), loss = 0.0476158
I0704 07:24:26.593762 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:24:26.593770 22258 sgd_solver.cpp:106] Iteration 13200, lr = 0.079375
I0704 07:24:28.656080 22258 solver.cpp:290] Iteration 13300 (48.4907 iter/s, 2.06225s/100 iter), loss = 0.0952349
I0704 07:24:28.656105 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:24:28.656113 22258 sgd_solver.cpp:106] Iteration 13300, lr = 0.0792188
I0704 07:24:30.715008 22258 solver.cpp:290] Iteration 13400 (48.5711 iter/s, 2.05884s/100 iter), loss = 0.190473
I0704 07:24:30.715029 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:24:30.715036 22258 sgd_solver.cpp:106] Iteration 13400, lr = 0.0790625
I0704 07:24:32.775218 22258 solver.cpp:290] Iteration 13500 (48.5408 iter/s, 2.06012s/100 iter), loss = -3.21865e-06
I0704 07:24:32.775245 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:24:32.775252 22258 sgd_solver.cpp:106] Iteration 13500, lr = 0.0789063
I0704 07:24:34.876209 22258 solver.cpp:290] Iteration 13600 (47.5986 iter/s, 2.1009s/100 iter), loss = -3.23355e-06
I0704 07:24:34.876286 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:24:34.876293 22258 sgd_solver.cpp:106] Iteration 13600, lr = 0.07875
I0704 07:24:36.932730 22258 solver.cpp:290] Iteration 13700 (48.6291 iter/s, 2.05638s/100 iter), loss = -3.22238e-06
I0704 07:24:36.932752 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:24:36.932760 22258 sgd_solver.cpp:106] Iteration 13700, lr = 0.0785938
I0704 07:24:38.989332 22258 solver.cpp:290] Iteration 13800 (48.6259 iter/s, 2.05652s/100 iter), loss = 0.0952349
I0704 07:24:38.989356 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:24:38.989362 22258 sgd_solver.cpp:106] Iteration 13800, lr = 0.0784375
I0704 07:24:41.045222 22258 solver.cpp:290] Iteration 13900 (48.6428 iter/s, 2.0558s/100 iter), loss = -3.17395e-06
I0704 07:24:41.045244 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:24:41.045253 22258 sgd_solver.cpp:106] Iteration 13900, lr = 0.0782812
I0704 07:24:43.083863 22258 solver.cpp:466] Iteration 14000, Testing net (#0)
I0704 07:24:44.727706 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.7197
I0704 07:24:44.727725 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9706
I0704 07:24:44.727731 22258 solver.cpp:539]     Test net output #2: loss = 0.996 (* 1 = 0.996 loss)
I0704 07:24:44.747865 22258 solver.cpp:290] Iteration 14000 (27.0087 iter/s, 3.70251s/100 iter), loss = -3.18885e-06
I0704 07:24:44.747889 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:24:44.747895 22258 sgd_solver.cpp:106] Iteration 14000, lr = 0.078125
I0704 07:24:46.807466 22258 solver.cpp:290] Iteration 14100 (48.5552 iter/s, 2.05951s/100 iter), loss = 0.476187
I0704 07:24:46.807487 22258 solver.cpp:309]     Train net output #0: loss = 0.47619 (* 1 = 0.47619 loss)
I0704 07:24:46.807493 22258 sgd_solver.cpp:106] Iteration 14100, lr = 0.0779688
I0704 07:24:48.866183 22258 solver.cpp:290] Iteration 14200 (48.5759 iter/s, 2.05863s/100 iter), loss = 0.0476159
I0704 07:24:48.866209 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:24:48.866217 22258 sgd_solver.cpp:106] Iteration 14200, lr = 0.0778125
I0704 07:24:50.927176 22258 solver.cpp:290] Iteration 14300 (48.5224 iter/s, 2.06091s/100 iter), loss = 0.0952349
I0704 07:24:50.927199 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:24:50.927206 22258 sgd_solver.cpp:106] Iteration 14300, lr = 0.0776563
I0704 07:24:52.988940 22258 solver.cpp:290] Iteration 14400 (48.5042 iter/s, 2.06168s/100 iter), loss = -3.26335e-06
I0704 07:24:52.988963 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:24:52.988970 22258 sgd_solver.cpp:106] Iteration 14400, lr = 0.0775
I0704 07:24:55.049326 22258 solver.cpp:290] Iteration 14500 (48.5366 iter/s, 2.0603s/100 iter), loss = 0.380949
I0704 07:24:55.049350 22258 solver.cpp:309]     Train net output #0: loss = 0.380952 (* 1 = 0.380952 loss)
I0704 07:24:55.049356 22258 sgd_solver.cpp:106] Iteration 14500, lr = 0.0773438
I0704 07:24:57.106719 22258 solver.cpp:290] Iteration 14600 (48.6073 iter/s, 2.05731s/100 iter), loss = -3.24845e-06
I0704 07:24:57.106741 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:24:57.106747 22258 sgd_solver.cpp:106] Iteration 14600, lr = 0.0771875
I0704 07:24:59.164484 22258 solver.cpp:290] Iteration 14700 (48.5984 iter/s, 2.05768s/100 iter), loss = 0.142854
I0704 07:24:59.164507 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:24:59.164515 22258 sgd_solver.cpp:106] Iteration 14700, lr = 0.0770312
I0704 07:25:01.222239 22258 solver.cpp:290] Iteration 14800 (48.5987 iter/s, 2.05767s/100 iter), loss = -3.24845e-06
I0704 07:25:01.222262 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:25:01.222271 22258 sgd_solver.cpp:106] Iteration 14800, lr = 0.076875
I0704 07:25:03.280920 22258 solver.cpp:290] Iteration 14900 (48.5768 iter/s, 2.0586s/100 iter), loss = -3.24845e-06
I0704 07:25:03.280961 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:25:03.280968 22258 sgd_solver.cpp:106] Iteration 14900, lr = 0.0767187
I0704 07:25:05.317160 22258 solver.cpp:466] Iteration 15000, Testing net (#0)
I0704 07:25:06.973078 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.6644
I0704 07:25:06.973098 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.967
I0704 07:25:06.973103 22258 solver.cpp:539]     Test net output #2: loss = 1.2482 (* 1 = 1.2482 loss)
I0704 07:25:06.992837 22258 solver.cpp:290] Iteration 15000 (26.9413 iter/s, 3.71177s/100 iter), loss = -3.23355e-06
I0704 07:25:06.992856 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:25:06.992867 22258 sgd_solver.cpp:106] Iteration 15000, lr = 0.0765625
I0704 07:25:09.053123 22258 solver.cpp:290] Iteration 15100 (48.5389 iter/s, 2.0602s/100 iter), loss = 0.142854
I0704 07:25:09.053145 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:25:09.053151 22258 sgd_solver.cpp:106] Iteration 15100, lr = 0.0764063
I0704 07:25:11.110478 22258 solver.cpp:290] Iteration 15200 (48.6081 iter/s, 2.05727s/100 iter), loss = 0.142854
I0704 07:25:11.110502 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:25:11.110507 22258 sgd_solver.cpp:106] Iteration 15200, lr = 0.07625
I0704 07:25:13.173645 22258 solver.cpp:290] Iteration 15300 (48.4712 iter/s, 2.06308s/100 iter), loss = 0.142854
I0704 07:25:13.173668 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:25:13.173674 22258 sgd_solver.cpp:106] Iteration 15300, lr = 0.0760938
I0704 07:25:15.232308 22258 solver.cpp:290] Iteration 15400 (48.5773 iter/s, 2.05857s/100 iter), loss = -3.30806e-06
I0704 07:25:15.232333 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:25:15.232342 22258 sgd_solver.cpp:106] Iteration 15400, lr = 0.0759375
I0704 07:25:17.294422 22258 solver.cpp:290] Iteration 15500 (48.496 iter/s, 2.06203s/100 iter), loss = 0.0476157
I0704 07:25:17.294446 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:25:17.294456 22258 sgd_solver.cpp:106] Iteration 15500, lr = 0.0757812
I0704 07:25:19.353046 22258 solver.cpp:290] Iteration 15600 (48.5782 iter/s, 2.05854s/100 iter), loss = 0.0476158
I0704 07:25:19.353068 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:25:19.353076 22258 sgd_solver.cpp:106] Iteration 15600, lr = 0.075625
I0704 07:25:21.409766 22258 solver.cpp:290] Iteration 15700 (48.6232 iter/s, 2.05663s/100 iter), loss = -3.31551e-06
I0704 07:25:21.409793 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:25:21.409801 22258 sgd_solver.cpp:106] Iteration 15700, lr = 0.0754687
I0704 07:25:23.472437 22258 solver.cpp:290] Iteration 15800 (48.483 iter/s, 2.06258s/100 iter), loss = 0.142854
I0704 07:25:23.472462 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:25:23.472471 22258 sgd_solver.cpp:106] Iteration 15800, lr = 0.0753125
I0704 07:25:25.532737 22258 solver.cpp:290] Iteration 15900 (48.5387 iter/s, 2.06021s/100 iter), loss = 0.238092
I0704 07:25:25.532760 22258 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0704 07:25:25.532766 22258 sgd_solver.cpp:106] Iteration 15900, lr = 0.0751562
I0704 07:25:27.567837 22258 solver.cpp:466] Iteration 16000, Testing net (#0)
I0704 07:25:29.214742 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8027
I0704 07:25:29.214762 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9901
I0704 07:25:29.214767 22258 solver.cpp:539]     Test net output #2: loss = 0.5056 (* 1 = 0.5056 loss)
I0704 07:25:29.234799 22258 solver.cpp:290] Iteration 16000 (27.0129 iter/s, 3.70193s/100 iter), loss = -3.36766e-06
I0704 07:25:29.234818 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:25:29.234832 22258 sgd_solver.cpp:106] Iteration 16000, lr = 0.075
I0704 07:25:31.291779 22258 solver.cpp:290] Iteration 16100 (48.6169 iter/s, 2.0569s/100 iter), loss = -3.38256e-06
I0704 07:25:31.291802 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:25:31.291824 22258 sgd_solver.cpp:106] Iteration 16100, lr = 0.0748438
I0704 07:25:33.377936 22258 solver.cpp:290] Iteration 16200 (47.9371 iter/s, 2.08607s/100 iter), loss = 0.33333
I0704 07:25:33.377959 22258 solver.cpp:309]     Train net output #0: loss = 0.333333 (* 1 = 0.333333 loss)
I0704 07:25:33.377966 22258 sgd_solver.cpp:106] Iteration 16200, lr = 0.0746875
I0704 07:25:35.443126 22258 solver.cpp:290] Iteration 16300 (48.4237 iter/s, 2.0651s/100 iter), loss = 0.190473
I0704 07:25:35.443178 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:25:35.443186 22258 sgd_solver.cpp:106] Iteration 16300, lr = 0.0745312
I0704 07:25:37.502892 22258 solver.cpp:290] Iteration 16400 (48.5519 iter/s, 2.05965s/100 iter), loss = 0.0476157
I0704 07:25:37.502918 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:25:37.502925 22258 sgd_solver.cpp:106] Iteration 16400, lr = 0.074375
I0704 07:25:39.564786 22258 solver.cpp:290] Iteration 16500 (48.5011 iter/s, 2.06181s/100 iter), loss = 0.0476157
I0704 07:25:39.564808 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:25:39.564817 22258 sgd_solver.cpp:106] Iteration 16500, lr = 0.0742188
I0704 07:25:41.625705 22258 solver.cpp:290] Iteration 16600 (48.5241 iter/s, 2.06083s/100 iter), loss = 0.0952347
I0704 07:25:41.625728 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:25:41.625735 22258 sgd_solver.cpp:106] Iteration 16600, lr = 0.0740625
I0704 07:25:43.681887 22258 solver.cpp:290] Iteration 16700 (48.6359 iter/s, 2.0561s/100 iter), loss = 0.142854
I0704 07:25:43.681910 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:25:43.681918 22258 sgd_solver.cpp:106] Iteration 16700, lr = 0.0739063
I0704 07:25:45.740257 22258 solver.cpp:290] Iteration 16800 (48.5842 iter/s, 2.05828s/100 iter), loss = -3.44589e-06
I0704 07:25:45.740279 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:25:45.740288 22258 sgd_solver.cpp:106] Iteration 16800, lr = 0.07375
I0704 07:25:47.797284 22258 solver.cpp:290] Iteration 16900 (48.6159 iter/s, 2.05694s/100 iter), loss = 0.285711
I0704 07:25:47.797307 22258 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0704 07:25:47.797314 22258 sgd_solver.cpp:106] Iteration 16900, lr = 0.0735938
I0704 07:25:49.840057 22258 solver.cpp:466] Iteration 17000, Testing net (#0)
I0704 07:25:51.495672 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.7954
I0704 07:25:51.495692 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9886
I0704 07:25:51.495698 22258 solver.cpp:539]     Test net output #2: loss = 0.5607 (* 1 = 0.5607 loss)
I0704 07:25:51.515286 22258 solver.cpp:290] Iteration 17000 (26.8971 iter/s, 3.71787s/100 iter), loss = -3.50177e-06
I0704 07:25:51.515302 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:25:51.515316 22258 sgd_solver.cpp:106] Iteration 17000, lr = 0.0734375
I0704 07:25:53.572353 22258 solver.cpp:290] Iteration 17100 (48.6148 iter/s, 2.05699s/100 iter), loss = -3.48687e-06
I0704 07:25:53.572376 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:25:53.572382 22258 sgd_solver.cpp:106] Iteration 17100, lr = 0.0732813
I0704 07:25:55.629361 22258 solver.cpp:290] Iteration 17200 (48.6163 iter/s, 2.05692s/100 iter), loss = 0.380949
I0704 07:25:55.629382 22258 solver.cpp:309]     Train net output #0: loss = 0.380952 (* 1 = 0.380952 loss)
I0704 07:25:55.629390 22258 sgd_solver.cpp:106] Iteration 17200, lr = 0.073125
I0704 07:25:57.685542 22258 solver.cpp:290] Iteration 17300 (48.6359 iter/s, 2.05609s/100 iter), loss = -3.51667e-06
I0704 07:25:57.685567 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:25:57.685575 22258 sgd_solver.cpp:106] Iteration 17300, lr = 0.0729688
I0704 07:25:59.742167 22258 solver.cpp:290] Iteration 17400 (48.6254 iter/s, 2.05654s/100 iter), loss = -3.5204e-06
I0704 07:25:59.742192 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:25:59.742198 22258 sgd_solver.cpp:106] Iteration 17400, lr = 0.0728125
I0704 07:26:01.806875 22258 solver.cpp:290] Iteration 17500 (48.4351 iter/s, 2.06462s/100 iter), loss = -3.51667e-06
I0704 07:26:01.806901 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:26:01.806910 22258 sgd_solver.cpp:106] Iteration 17500, lr = 0.0726563
I0704 07:26:03.868075 22258 solver.cpp:290] Iteration 17600 (48.5175 iter/s, 2.06111s/100 iter), loss = -3.56138e-06
I0704 07:26:03.868113 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:26:03.868119 22258 sgd_solver.cpp:106] Iteration 17600, lr = 0.0725
I0704 07:26:05.927270 22258 solver.cpp:290] Iteration 17700 (48.565 iter/s, 2.0591s/100 iter), loss = 0.0476155
I0704 07:26:05.927330 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:26:05.927336 22258 sgd_solver.cpp:106] Iteration 17700, lr = 0.0723438
I0704 07:26:07.987530 22258 solver.cpp:290] Iteration 17800 (48.5404 iter/s, 2.06014s/100 iter), loss = 0.0952346
I0704 07:26:07.987555 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:26:07.987561 22258 sgd_solver.cpp:106] Iteration 17800, lr = 0.0721875
I0704 07:26:10.045223 22258 solver.cpp:290] Iteration 17900 (48.6002 iter/s, 2.05761s/100 iter), loss = -3.56138e-06
I0704 07:26:10.045248 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:26:10.045253 22258 sgd_solver.cpp:106] Iteration 17900, lr = 0.0720313
I0704 07:26:12.085294 22258 solver.cpp:466] Iteration 18000, Testing net (#0)
I0704 07:26:13.739964 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.7827
I0704 07:26:13.739984 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9892
I0704 07:26:13.739989 22258 solver.cpp:539]     Test net output #2: loss = 0.7233 (* 1 = 0.7233 loss)
I0704 07:26:13.759665 22258 solver.cpp:290] Iteration 18000 (26.9229 iter/s, 3.71431s/100 iter), loss = 0.285711
I0704 07:26:13.759682 22258 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0704 07:26:13.759693 22258 sgd_solver.cpp:106] Iteration 18000, lr = 0.071875
I0704 07:26:15.822888 22258 solver.cpp:290] Iteration 18100 (48.4698 iter/s, 2.06314s/100 iter), loss = -3.53158e-06
I0704 07:26:15.822916 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:26:15.822923 22258 sgd_solver.cpp:106] Iteration 18100, lr = 0.0717188
I0704 07:26:17.884469 22258 solver.cpp:290] Iteration 18200 (48.5086 iter/s, 2.06149s/100 iter), loss = 0.0476155
I0704 07:26:17.884492 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:26:17.884498 22258 sgd_solver.cpp:106] Iteration 18200, lr = 0.0715625
I0704 07:26:19.947657 22258 solver.cpp:290] Iteration 18300 (48.4707 iter/s, 2.0631s/100 iter), loss = 0.0952345
I0704 07:26:19.947680 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:26:19.947687 22258 sgd_solver.cpp:106] Iteration 18300, lr = 0.0714063
I0704 07:26:22.012011 22258 solver.cpp:290] Iteration 18400 (48.4433 iter/s, 2.06427s/100 iter), loss = 0.0476155
I0704 07:26:22.012032 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:26:22.012040 22258 sgd_solver.cpp:106] Iteration 18400, lr = 0.07125
I0704 07:26:24.067898 22258 solver.cpp:290] Iteration 18500 (48.6428 iter/s, 2.0558s/100 iter), loss = -3.59863e-06
I0704 07:26:24.067920 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:26:24.067927 22258 sgd_solver.cpp:106] Iteration 18500, lr = 0.0710938
I0704 07:26:26.125273 22258 solver.cpp:290] Iteration 18600 (48.6076 iter/s, 2.05729s/100 iter), loss = 0.0952345
I0704 07:26:26.125298 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:26:26.125306 22258 sgd_solver.cpp:106] Iteration 18600, lr = 0.0709375
I0704 07:26:28.183293 22258 solver.cpp:290] Iteration 18700 (48.5924 iter/s, 2.05793s/100 iter), loss = -3.60608e-06
I0704 07:26:28.183317 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:26:28.183323 22258 sgd_solver.cpp:106] Iteration 18700, lr = 0.0707813
I0704 07:26:30.246688 22258 solver.cpp:290] Iteration 18800 (48.4658 iter/s, 2.06331s/100 iter), loss = 0.0476154
I0704 07:26:30.246711 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:26:30.246718 22258 sgd_solver.cpp:106] Iteration 18800, lr = 0.070625
I0704 07:26:32.306803 22258 solver.cpp:290] Iteration 18900 (48.543 iter/s, 2.06003s/100 iter), loss = -3.60608e-06
I0704 07:26:32.306826 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:26:32.306833 22258 sgd_solver.cpp:106] Iteration 18900, lr = 0.0704687
I0704 07:26:34.368695 22258 solver.cpp:466] Iteration 19000, Testing net (#0)
I0704 07:26:36.012907 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8114
I0704 07:26:36.012991 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9895
I0704 07:26:36.012998 22258 solver.cpp:539]     Test net output #2: loss = 0.5442 (* 1 = 0.5442 loss)
I0704 07:26:36.033138 22258 solver.cpp:290] Iteration 19000 (26.837 iter/s, 3.7262s/100 iter), loss = -3.62098e-06
I0704 07:26:36.033164 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:26:36.033170 22258 sgd_solver.cpp:106] Iteration 19000, lr = 0.0703125
I0704 07:26:38.093029 22258 solver.cpp:290] Iteration 19100 (48.5483 iter/s, 2.0598s/100 iter), loss = 0.0476154
I0704 07:26:38.093051 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:26:38.093058 22258 sgd_solver.cpp:106] Iteration 19100, lr = 0.0701563
I0704 07:26:40.154656 22258 solver.cpp:290] Iteration 19200 (48.5074 iter/s, 2.06154s/100 iter), loss = 0.0476154
I0704 07:26:40.154680 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:26:40.154688 22258 sgd_solver.cpp:106] Iteration 19200, lr = 0.07
I0704 07:26:42.214084 22258 solver.cpp:290] Iteration 19300 (48.5592 iter/s, 2.05934s/100 iter), loss = -3.60608e-06
I0704 07:26:42.214107 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:26:42.214113 22258 sgd_solver.cpp:106] Iteration 19300, lr = 0.0698438
I0704 07:26:44.271369 22258 solver.cpp:290] Iteration 19400 (48.6098 iter/s, 2.0572s/100 iter), loss = 0.0476154
I0704 07:26:44.271392 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:26:44.271400 22258 sgd_solver.cpp:106] Iteration 19400, lr = 0.0696875
I0704 07:26:46.332141 22258 solver.cpp:290] Iteration 19500 (48.5276 iter/s, 2.06068s/100 iter), loss = 0.190473
I0704 07:26:46.332165 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:26:46.332171 22258 sgd_solver.cpp:106] Iteration 19500, lr = 0.0695313
I0704 07:26:48.393635 22258 solver.cpp:290] Iteration 19600 (48.5105 iter/s, 2.06141s/100 iter), loss = -3.68059e-06
I0704 07:26:48.393658 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:26:48.393664 22258 sgd_solver.cpp:106] Iteration 19600, lr = 0.069375
I0704 07:26:50.456995 22258 solver.cpp:290] Iteration 19700 (48.4667 iter/s, 2.06327s/100 iter), loss = 0.0476154
I0704 07:26:50.457016 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:26:50.457023 22258 sgd_solver.cpp:106] Iteration 19700, lr = 0.0692187
I0704 07:26:52.516158 22258 solver.cpp:290] Iteration 19800 (48.5654 iter/s, 2.05908s/100 iter), loss = -3.66569e-06
I0704 07:26:52.516181 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:26:52.516188 22258 sgd_solver.cpp:106] Iteration 19800, lr = 0.0690625
I0704 07:26:54.574957 22258 solver.cpp:290] Iteration 19900 (48.574 iter/s, 2.05871s/100 iter), loss = -3.71039e-06
I0704 07:26:54.574980 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:26:54.574987 22258 sgd_solver.cpp:106] Iteration 19900, lr = 0.0689062
I0704 07:26:56.610410 22258 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/cifar10_jacintonet11v2_iter_20000.caffemodel
I0704 07:26:56.626845 22258 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/cifar10_jacintonet11v2_iter_20000.solverstate
I0704 07:26:56.634588 22258 solver.cpp:466] Iteration 20000, Testing net (#0)
I0704 07:26:58.279952 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8185
I0704 07:26:58.279973 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9914
I0704 07:26:58.279978 22258 solver.cpp:539]     Test net output #2: loss = 0.4478 (* 1 = 0.4478 loss)
I0704 07:26:58.299794 22258 solver.cpp:290] Iteration 20000 (26.8478 iter/s, 3.72471s/100 iter), loss = 0.0952344
I0704 07:26:58.299813 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:26:58.299831 22258 sgd_solver.cpp:106] Iteration 20000, lr = 0.06875
I0704 07:27:00.356817 22258 solver.cpp:290] Iteration 20100 (48.6159 iter/s, 2.05694s/100 iter), loss = 0.0476153
I0704 07:27:00.356838 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:27:00.356845 22258 sgd_solver.cpp:106] Iteration 20100, lr = 0.0685938
I0704 07:27:02.417096 22258 solver.cpp:290] Iteration 20200 (48.5391 iter/s, 2.06019s/100 iter), loss = 0.142853
I0704 07:27:02.417119 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:27:02.417126 22258 sgd_solver.cpp:106] Iteration 20200, lr = 0.0684375
I0704 07:27:04.476371 22258 solver.cpp:290] Iteration 20300 (48.5629 iter/s, 2.05919s/100 iter), loss = 0.190472
I0704 07:27:04.476393 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:27:04.476399 22258 sgd_solver.cpp:106] Iteration 20300, lr = 0.0682813
I0704 07:27:06.537237 22258 solver.cpp:290] Iteration 20400 (48.5253 iter/s, 2.06078s/100 iter), loss = -3.7998e-06
I0704 07:27:06.537291 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:27:06.537298 22258 sgd_solver.cpp:106] Iteration 20400, lr = 0.068125
I0704 07:27:08.605237 22258 solver.cpp:290] Iteration 20500 (48.3586 iter/s, 2.06789s/100 iter), loss = -3.7998e-06
I0704 07:27:08.605260 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:27:08.605268 22258 sgd_solver.cpp:106] Iteration 20500, lr = 0.0679687
I0704 07:27:10.662663 22258 solver.cpp:290] Iteration 20600 (48.6065 iter/s, 2.05734s/100 iter), loss = -3.78489e-06
I0704 07:27:10.662686 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:27:10.662693 22258 sgd_solver.cpp:106] Iteration 20600, lr = 0.0678125
I0704 07:27:12.718039 22258 solver.cpp:290] Iteration 20700 (48.655 iter/s, 2.05529s/100 iter), loss = -3.78489e-06
I0704 07:27:12.718061 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:27:12.718067 22258 sgd_solver.cpp:106] Iteration 20700, lr = 0.0676562
I0704 07:27:14.776576 22258 solver.cpp:290] Iteration 20800 (48.5802 iter/s, 2.05845s/100 iter), loss = 0.0476153
I0704 07:27:14.776598 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:27:14.776605 22258 sgd_solver.cpp:106] Iteration 20800, lr = 0.0675
I0704 07:27:16.837772 22258 solver.cpp:290] Iteration 20900 (48.5176 iter/s, 2.06111s/100 iter), loss = 0.142853
I0704 07:27:16.837798 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:27:16.837807 22258 sgd_solver.cpp:106] Iteration 20900, lr = 0.0673437
I0704 07:27:18.877101 22258 solver.cpp:466] Iteration 21000, Testing net (#0)
I0704 07:27:20.532564 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.783
I0704 07:27:20.532583 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9862
I0704 07:27:20.532588 22258 solver.cpp:539]     Test net output #2: loss = 0.6548 (* 1 = 0.6548 loss)
I0704 07:27:20.552361 22258 solver.cpp:290] Iteration 21000 (26.9218 iter/s, 3.71446s/100 iter), loss = -3.78489e-06
I0704 07:27:20.552378 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:27:20.552392 22258 sgd_solver.cpp:106] Iteration 21000, lr = 0.0671875
I0704 07:27:22.611837 22258 solver.cpp:290] Iteration 21100 (48.5579 iter/s, 2.0594s/100 iter), loss = 0.190472
I0704 07:27:22.611860 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:27:22.611867 22258 sgd_solver.cpp:106] Iteration 21100, lr = 0.0670313
I0704 07:27:24.669558 22258 solver.cpp:290] Iteration 21200 (48.5995 iter/s, 2.05763s/100 iter), loss = 0.142853
I0704 07:27:24.669580 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:27:24.669589 22258 sgd_solver.cpp:106] Iteration 21200, lr = 0.066875
I0704 07:27:26.727159 22258 solver.cpp:290] Iteration 21300 (48.6023 iter/s, 2.05751s/100 iter), loss = 0.238091
I0704 07:27:26.727183 22258 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0704 07:27:26.727191 22258 sgd_solver.cpp:106] Iteration 21300, lr = 0.0667187
I0704 07:27:28.785603 22258 solver.cpp:290] Iteration 21400 (48.5824 iter/s, 2.05836s/100 iter), loss = -3.78489e-06
I0704 07:27:28.785625 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:27:28.785632 22258 sgd_solver.cpp:106] Iteration 21400, lr = 0.0665625
I0704 07:27:30.843837 22258 solver.cpp:290] Iteration 21500 (48.5873 iter/s, 2.05815s/100 iter), loss = 0.190472
I0704 07:27:30.843861 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:27:30.843870 22258 sgd_solver.cpp:106] Iteration 21500, lr = 0.0664062
I0704 07:27:32.903451 22258 solver.cpp:290] Iteration 21600 (48.5548 iter/s, 2.05953s/100 iter), loss = 0.0476153
I0704 07:27:32.903475 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:27:32.903481 22258 sgd_solver.cpp:106] Iteration 21600, lr = 0.06625
I0704 07:27:34.998839 22258 solver.cpp:290] Iteration 21700 (47.7259 iter/s, 2.0953s/100 iter), loss = -3.80725e-06
I0704 07:27:34.998874 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:27:34.998881 22258 sgd_solver.cpp:106] Iteration 21700, lr = 0.0660938
I0704 07:27:37.053716 22258 solver.cpp:290] Iteration 21800 (48.667 iter/s, 2.05478s/100 iter), loss = 0.190472
I0704 07:27:37.053783 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:27:37.053791 22258 sgd_solver.cpp:106] Iteration 21800, lr = 0.0659375
I0704 07:27:39.110642 22258 solver.cpp:290] Iteration 21900 (48.6193 iter/s, 2.0568s/100 iter), loss = 0.190472
I0704 07:27:39.110666 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:27:39.110673 22258 sgd_solver.cpp:106] Iteration 21900, lr = 0.0657813
I0704 07:27:41.153872 22258 solver.cpp:466] Iteration 22000, Testing net (#0)
I0704 07:27:42.806921 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.7118
I0704 07:27:42.806941 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.942201
I0704 07:27:42.806946 22258 solver.cpp:539]     Test net output #2: loss = 1.4105 (* 1 = 1.4105 loss)
I0704 07:27:42.830188 22258 solver.cpp:290] Iteration 22000 (26.8859 iter/s, 3.71942s/100 iter), loss = 0.190472
I0704 07:27:42.830209 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:27:42.830219 22258 sgd_solver.cpp:106] Iteration 22000, lr = 0.065625
I0704 07:27:44.887698 22258 solver.cpp:290] Iteration 22100 (48.6044 iter/s, 2.05743s/100 iter), loss = -3.78489e-06
I0704 07:27:44.887723 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:27:44.887729 22258 sgd_solver.cpp:106] Iteration 22100, lr = 0.0654688
I0704 07:27:46.943603 22258 solver.cpp:290] Iteration 22200 (48.6426 iter/s, 2.05581s/100 iter), loss = 0.428568
I0704 07:27:46.943626 22258 solver.cpp:309]     Train net output #0: loss = 0.428571 (* 1 = 0.428571 loss)
I0704 07:27:46.943634 22258 sgd_solver.cpp:106] Iteration 22200, lr = 0.0653125
I0704 07:27:48.999356 22258 solver.cpp:290] Iteration 22300 (48.646 iter/s, 2.05567s/100 iter), loss = 0.0952343
I0704 07:27:48.999378 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:27:48.999385 22258 sgd_solver.cpp:106] Iteration 22300, lr = 0.0651563
I0704 07:27:51.056602 22258 solver.cpp:290] Iteration 22400 (48.6107 iter/s, 2.05716s/100 iter), loss = 0.0476152
I0704 07:27:51.056625 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:27:51.056634 22258 sgd_solver.cpp:106] Iteration 22400, lr = 0.065
I0704 07:27:53.111749 22258 solver.cpp:290] Iteration 22500 (48.6603 iter/s, 2.05506s/100 iter), loss = -3.7998e-06
I0704 07:27:53.111774 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:27:53.111783 22258 sgd_solver.cpp:106] Iteration 22500, lr = 0.0648438
I0704 07:27:55.169920 22258 solver.cpp:290] Iteration 22600 (48.589 iter/s, 2.05808s/100 iter), loss = -3.78862e-06
I0704 07:27:55.169950 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:27:55.169960 22258 sgd_solver.cpp:106] Iteration 22600, lr = 0.0646875
I0704 07:27:57.226300 22258 solver.cpp:290] Iteration 22700 (48.6313 iter/s, 2.05629s/100 iter), loss = 0.142853
I0704 07:27:57.226335 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:27:57.226346 22258 sgd_solver.cpp:106] Iteration 22700, lr = 0.0645313
I0704 07:27:59.285231 22258 solver.cpp:290] Iteration 22800 (48.5711 iter/s, 2.05884s/100 iter), loss = 0.0476153
I0704 07:27:59.285255 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:27:59.285262 22258 sgd_solver.cpp:106] Iteration 22800, lr = 0.064375
I0704 07:28:01.342772 22258 solver.cpp:290] Iteration 22900 (48.6038 iter/s, 2.05745s/100 iter), loss = -3.80725e-06
I0704 07:28:01.342797 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:01.342804 22258 sgd_solver.cpp:106] Iteration 22900, lr = 0.0642188
I0704 07:28:03.379123 22258 solver.cpp:466] Iteration 23000, Testing net (#0)
I0704 07:28:05.024663 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8253
I0704 07:28:05.024682 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9917
I0704 07:28:05.024688 22258 solver.cpp:539]     Test net output #2: loss = 0.489 (* 1 = 0.489 loss)
I0704 07:28:05.044512 22258 solver.cpp:290] Iteration 23000 (27.0153 iter/s, 3.70161s/100 iter), loss = -3.8445e-06
I0704 07:28:05.044531 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:05.044543 22258 sgd_solver.cpp:106] Iteration 23000, lr = 0.0640625
I0704 07:28:07.104579 22258 solver.cpp:290] Iteration 23100 (48.5441 iter/s, 2.05998s/100 iter), loss = 0.0476152
I0704 07:28:07.104656 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:28:07.104665 22258 sgd_solver.cpp:106] Iteration 23100, lr = 0.0639063
I0704 07:28:09.164913 22258 solver.cpp:290] Iteration 23200 (48.5391 iter/s, 2.0602s/100 iter), loss = -3.8594e-06
I0704 07:28:09.164937 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:09.164943 22258 sgd_solver.cpp:106] Iteration 23200, lr = 0.06375
I0704 07:28:11.222507 22258 solver.cpp:290] Iteration 23300 (48.6025 iter/s, 2.05751s/100 iter), loss = -3.8594e-06
I0704 07:28:11.222529 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:11.222537 22258 sgd_solver.cpp:106] Iteration 23300, lr = 0.0635938
I0704 07:28:13.286068 22258 solver.cpp:290] Iteration 23400 (48.4619 iter/s, 2.06348s/100 iter), loss = -3.8445e-06
I0704 07:28:13.286092 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:13.286098 22258 sgd_solver.cpp:106] Iteration 23400, lr = 0.0634375
I0704 07:28:15.342887 22258 solver.cpp:290] Iteration 23500 (48.6208 iter/s, 2.05673s/100 iter), loss = -3.8445e-06
I0704 07:28:15.342911 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:15.342916 22258 sgd_solver.cpp:106] Iteration 23500, lr = 0.0632813
I0704 07:28:17.398075 22258 solver.cpp:290] Iteration 23600 (48.6594 iter/s, 2.0551s/100 iter), loss = 0.0476152
I0704 07:28:17.398102 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:28:17.398110 22258 sgd_solver.cpp:106] Iteration 23600, lr = 0.063125
I0704 07:28:19.457896 22258 solver.cpp:290] Iteration 23700 (48.55 iter/s, 2.05973s/100 iter), loss = -3.8445e-06
I0704 07:28:19.457919 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:19.457926 22258 sgd_solver.cpp:106] Iteration 23700, lr = 0.0629688
I0704 07:28:21.527788 22258 solver.cpp:290] Iteration 23800 (48.3137 iter/s, 2.06981s/100 iter), loss = -3.86685e-06
I0704 07:28:21.527813 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:21.527822 22258 sgd_solver.cpp:106] Iteration 23800, lr = 0.0628125
I0704 07:28:23.584384 22258 solver.cpp:290] Iteration 23900 (48.6261 iter/s, 2.05651s/100 iter), loss = -3.8743e-06
I0704 07:28:23.584410 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:23.584419 22258 sgd_solver.cpp:106] Iteration 23900, lr = 0.0626562
I0704 07:28:25.621624 22258 solver.cpp:466] Iteration 24000, Testing net (#0)
I0704 07:28:27.266964 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.7536
I0704 07:28:27.266983 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9741
I0704 07:28:27.266988 22258 solver.cpp:539]     Test net output #2: loss = 0.9309 (* 1 = 0.9309 loss)
I0704 07:28:27.287721 22258 solver.cpp:290] Iteration 24000 (27.0036 iter/s, 3.70321s/100 iter), loss = 0.142853
I0704 07:28:27.287739 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:28:27.287751 22258 sgd_solver.cpp:106] Iteration 24000, lr = 0.0625
I0704 07:28:29.346467 22258 solver.cpp:290] Iteration 24100 (48.5753 iter/s, 2.05866s/100 iter), loss = -3.92273e-06
I0704 07:28:29.346493 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:29.346498 22258 sgd_solver.cpp:106] Iteration 24100, lr = 0.0623438
I0704 07:28:31.403316 22258 solver.cpp:290] Iteration 24200 (48.6202 iter/s, 2.05676s/100 iter), loss = -3.9041e-06
I0704 07:28:31.403343 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:31.403352 22258 sgd_solver.cpp:106] Iteration 24200, lr = 0.0621875
I0704 07:28:33.469159 22258 solver.cpp:290] Iteration 24300 (48.4084 iter/s, 2.06576s/100 iter), loss = -3.9041e-06
I0704 07:28:33.469180 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:33.469188 22258 sgd_solver.cpp:106] Iteration 24300, lr = 0.0620313
I0704 07:28:35.528273 22258 solver.cpp:290] Iteration 24400 (48.5666 iter/s, 2.05903s/100 iter), loss = -3.9041e-06
I0704 07:28:35.528309 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:35.528316 22258 sgd_solver.cpp:106] Iteration 24400, lr = 0.061875
I0704 07:28:37.585788 22258 solver.cpp:290] Iteration 24500 (48.6046 iter/s, 2.05742s/100 iter), loss = -3.91901e-06
I0704 07:28:37.585847 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:37.585855 22258 sgd_solver.cpp:106] Iteration 24500, lr = 0.0617188
I0704 07:28:39.642655 22258 solver.cpp:290] Iteration 24600 (48.6205 iter/s, 2.05675s/100 iter), loss = 0.0952342
I0704 07:28:39.642678 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:28:39.642684 22258 sgd_solver.cpp:106] Iteration 24600, lr = 0.0615625
I0704 07:28:41.701647 22258 solver.cpp:290] Iteration 24700 (48.5695 iter/s, 2.05891s/100 iter), loss = -3.96371e-06
I0704 07:28:41.701668 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:41.701676 22258 sgd_solver.cpp:106] Iteration 24700, lr = 0.0614063
I0704 07:28:43.760766 22258 solver.cpp:290] Iteration 24800 (48.5664 iter/s, 2.05903s/100 iter), loss = -3.96371e-06
I0704 07:28:43.760789 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:43.760795 22258 sgd_solver.cpp:106] Iteration 24800, lr = 0.06125
I0704 07:28:45.816702 22258 solver.cpp:290] Iteration 24900 (48.6417 iter/s, 2.05585s/100 iter), loss = 0.0476151
I0704 07:28:45.816725 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:28:45.816731 22258 sgd_solver.cpp:106] Iteration 24900, lr = 0.0610937
I0704 07:28:47.854575 22258 solver.cpp:466] Iteration 25000, Testing net (#0)
I0704 07:28:49.499590 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8403
I0704 07:28:49.499610 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9939
I0704 07:28:49.499615 22258 solver.cpp:539]     Test net output #2: loss = 0.385 (* 1 = 0.385 loss)
I0704 07:28:49.519563 22258 solver.cpp:290] Iteration 25000 (27.0071 iter/s, 3.70273s/100 iter), loss = -3.96743e-06
I0704 07:28:49.519580 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:49.519594 22258 sgd_solver.cpp:106] Iteration 25000, lr = 0.0609375
I0704 07:28:51.576751 22258 solver.cpp:290] Iteration 25100 (48.612 iter/s, 2.05711s/100 iter), loss = -3.96371e-06
I0704 07:28:51.576776 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:51.576783 22258 sgd_solver.cpp:106] Iteration 25100, lr = 0.0607813
I0704 07:28:53.635953 22258 solver.cpp:290] Iteration 25200 (48.5646 iter/s, 2.05911s/100 iter), loss = -3.96371e-06
I0704 07:28:53.635975 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:53.635982 22258 sgd_solver.cpp:106] Iteration 25200, lr = 0.060625
I0704 07:28:55.693014 22258 solver.cpp:290] Iteration 25300 (48.615 iter/s, 2.05698s/100 iter), loss = 0.0476151
I0704 07:28:55.693037 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:28:55.693044 22258 sgd_solver.cpp:106] Iteration 25300, lr = 0.0604688
I0704 07:28:57.753031 22258 solver.cpp:290] Iteration 25400 (48.5454 iter/s, 2.05993s/100 iter), loss = 0.0952341
I0704 07:28:57.753056 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:28:57.753064 22258 sgd_solver.cpp:106] Iteration 25400, lr = 0.0603125
I0704 07:28:59.815625 22258 solver.cpp:290] Iteration 25500 (48.4848 iter/s, 2.0625s/100 iter), loss = -3.97116e-06
I0704 07:28:59.815650 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:28:59.815656 22258 sgd_solver.cpp:106] Iteration 25500, lr = 0.0601563
I0704 07:29:01.875416 22258 solver.cpp:290] Iteration 25600 (48.5507 iter/s, 2.0597s/100 iter), loss = 0.0476151
I0704 07:29:01.875440 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:29:01.875449 22258 sgd_solver.cpp:106] Iteration 25600, lr = 0.06
I0704 07:29:03.933204 22258 solver.cpp:290] Iteration 25700 (48.5979 iter/s, 2.0577s/100 iter), loss = 0.0476151
I0704 07:29:03.933225 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:29:03.933233 22258 sgd_solver.cpp:106] Iteration 25700, lr = 0.0598437
I0704 07:29:05.994624 22258 solver.cpp:290] Iteration 25800 (48.5122 iter/s, 2.06134s/100 iter), loss = -3.96371e-06
I0704 07:29:05.994662 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:05.994668 22258 sgd_solver.cpp:106] Iteration 25800, lr = 0.0596875
I0704 07:29:08.056154 22258 solver.cpp:290] Iteration 25900 (48.51 iter/s, 2.06143s/100 iter), loss = -3.98234e-06
I0704 07:29:08.056262 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:08.056269 22258 sgd_solver.cpp:106] Iteration 25900, lr = 0.0595312
I0704 07:29:10.091596 22258 solver.cpp:466] Iteration 26000, Testing net (#0)
I0704 07:29:11.738962 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8558
I0704 07:29:11.738982 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9932
I0704 07:29:11.738987 22258 solver.cpp:539]     Test net output #2: loss = 0.4122 (* 1 = 0.4122 loss)
I0704 07:29:11.759104 22258 solver.cpp:290] Iteration 26000 (27.007 iter/s, 3.70274s/100 iter), loss = -3.97861e-06
I0704 07:29:11.759121 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:11.759135 22258 sgd_solver.cpp:106] Iteration 26000, lr = 0.059375
I0704 07:29:13.824398 22258 solver.cpp:290] Iteration 26100 (48.4211 iter/s, 2.06521s/100 iter), loss = 0.142853
I0704 07:29:13.824421 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:29:13.824429 22258 sgd_solver.cpp:106] Iteration 26100, lr = 0.0592188
I0704 07:29:15.881003 22258 solver.cpp:290] Iteration 26200 (48.6259 iter/s, 2.05652s/100 iter), loss = 0.0476151
I0704 07:29:15.881026 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:29:15.881036 22258 sgd_solver.cpp:106] Iteration 26200, lr = 0.0590625
I0704 07:29:17.936436 22258 solver.cpp:290] Iteration 26300 (48.6536 iter/s, 2.05535s/100 iter), loss = -3.97861e-06
I0704 07:29:17.936461 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:17.936470 22258 sgd_solver.cpp:106] Iteration 26300, lr = 0.0589063
I0704 07:29:19.993858 22258 solver.cpp:290] Iteration 26400 (48.6066 iter/s, 2.05734s/100 iter), loss = -3.97861e-06
I0704 07:29:19.993881 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:19.993888 22258 sgd_solver.cpp:106] Iteration 26400, lr = 0.05875
I0704 07:29:22.053959 22258 solver.cpp:290] Iteration 26500 (48.5434 iter/s, 2.06001s/100 iter), loss = 0.0952341
I0704 07:29:22.053984 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:29:22.053992 22258 sgd_solver.cpp:106] Iteration 26500, lr = 0.0585938
I0704 07:29:24.115592 22258 solver.cpp:290] Iteration 26600 (48.5073 iter/s, 2.06154s/100 iter), loss = -3.97861e-06
I0704 07:29:24.115613 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:24.115619 22258 sgd_solver.cpp:106] Iteration 26600, lr = 0.0584375
I0704 07:29:26.177435 22258 solver.cpp:290] Iteration 26700 (48.5023 iter/s, 2.06176s/100 iter), loss = 0.190472
I0704 07:29:26.177458 22258 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:29:26.177464 22258 sgd_solver.cpp:106] Iteration 26700, lr = 0.0582813
I0704 07:29:28.233144 22258 solver.cpp:290] Iteration 26800 (48.647 iter/s, 2.05562s/100 iter), loss = -4.00841e-06
I0704 07:29:28.233167 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:28.233175 22258 sgd_solver.cpp:106] Iteration 26800, lr = 0.058125
I0704 07:29:30.291137 22258 solver.cpp:290] Iteration 26900 (48.5931 iter/s, 2.05791s/100 iter), loss = -4.02331e-06
I0704 07:29:30.291160 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:30.291167 22258 sgd_solver.cpp:106] Iteration 26900, lr = 0.0579687
I0704 07:29:32.329260 22258 solver.cpp:466] Iteration 27000, Testing net (#0)
I0704 07:29:33.974506 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8284
I0704 07:29:33.974527 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9905
I0704 07:29:33.974532 22258 solver.cpp:539]     Test net output #2: loss = 0.4912 (* 1 = 0.4912 loss)
I0704 07:29:33.995085 22258 solver.cpp:290] Iteration 27000 (26.9992 iter/s, 3.70382s/100 iter), loss = -4.04567e-06
I0704 07:29:33.995101 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:33.995115 22258 sgd_solver.cpp:106] Iteration 27000, lr = 0.0578125
I0704 07:29:36.055366 22258 solver.cpp:290] Iteration 27100 (48.539 iter/s, 2.0602s/100 iter), loss = -4.05312e-06
I0704 07:29:36.055388 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:36.055395 22258 sgd_solver.cpp:106] Iteration 27100, lr = 0.0576563
I0704 07:29:38.111802 22258 solver.cpp:290] Iteration 27200 (48.6298 iter/s, 2.05635s/100 iter), loss = 0.142853
I0704 07:29:38.111863 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:29:38.111873 22258 sgd_solver.cpp:106] Iteration 27200, lr = 0.0575
I0704 07:29:40.171406 22258 solver.cpp:290] Iteration 27300 (48.5559 iter/s, 2.05948s/100 iter), loss = -4.06802e-06
I0704 07:29:40.171430 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:40.171438 22258 sgd_solver.cpp:106] Iteration 27300, lr = 0.0573438
I0704 07:29:42.228133 22258 solver.cpp:290] Iteration 27400 (48.623 iter/s, 2.05664s/100 iter), loss = -4.09782e-06
I0704 07:29:42.228155 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:42.228162 22258 sgd_solver.cpp:106] Iteration 27400, lr = 0.0571875
I0704 07:29:44.285676 22258 solver.cpp:290] Iteration 27500 (48.6037 iter/s, 2.05746s/100 iter), loss = -4.11272e-06
I0704 07:29:44.285698 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:44.285706 22258 sgd_solver.cpp:106] Iteration 27500, lr = 0.0570313
I0704 07:29:46.342885 22258 solver.cpp:290] Iteration 27600 (48.6115 iter/s, 2.05712s/100 iter), loss = 0.142853
I0704 07:29:46.342908 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:29:46.342916 22258 sgd_solver.cpp:106] Iteration 27600, lr = 0.056875
I0704 07:29:48.399257 22258 solver.cpp:290] Iteration 27700 (48.6314 iter/s, 2.05628s/100 iter), loss = -4.09037e-06
I0704 07:29:48.399283 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:48.399289 22258 sgd_solver.cpp:106] Iteration 27700, lr = 0.0567187
I0704 07:29:50.453773 22258 solver.cpp:290] Iteration 27800 (48.6754 iter/s, 2.05443s/100 iter), loss = -4.09782e-06
I0704 07:29:50.453806 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:50.453819 22258 sgd_solver.cpp:106] Iteration 27800, lr = 0.0565625
I0704 07:29:52.513103 22258 solver.cpp:290] Iteration 27900 (48.5617 iter/s, 2.05923s/100 iter), loss = -4.10154e-06
I0704 07:29:52.513124 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:52.513131 22258 sgd_solver.cpp:106] Iteration 27900, lr = 0.0564062
I0704 07:29:54.549207 22258 solver.cpp:466] Iteration 28000, Testing net (#0)
I0704 07:29:56.194304 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8411
I0704 07:29:56.194324 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9919
I0704 07:29:56.194329 22258 solver.cpp:539]     Test net output #2: loss = 0.4894 (* 1 = 0.4894 loss)
I0704 07:29:56.213968 22258 solver.cpp:290] Iteration 28000 (27.0216 iter/s, 3.70074s/100 iter), loss = -4.08664e-06
I0704 07:29:56.213985 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:56.213996 22258 sgd_solver.cpp:106] Iteration 28000, lr = 0.05625
I0704 07:29:58.271042 22258 solver.cpp:290] Iteration 28100 (48.6146 iter/s, 2.05699s/100 iter), loss = -4.08292e-06
I0704 07:29:58.271065 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:29:58.271071 22258 sgd_solver.cpp:106] Iteration 28100, lr = 0.0560938
I0704 07:30:00.335150 22258 solver.cpp:290] Iteration 28200 (48.4491 iter/s, 2.06402s/100 iter), loss = -4.12762e-06
I0704 07:30:00.335175 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:00.335180 22258 sgd_solver.cpp:106] Iteration 28200, lr = 0.0559375
I0704 07:30:02.395174 22258 solver.cpp:290] Iteration 28300 (48.5452 iter/s, 2.05994s/100 iter), loss = 0.0476149
I0704 07:30:02.395197 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:30:02.395205 22258 sgd_solver.cpp:106] Iteration 28300, lr = 0.0557813
I0704 07:30:04.451278 22258 solver.cpp:290] Iteration 28400 (48.6377 iter/s, 2.05602s/100 iter), loss = -4.14252e-06
I0704 07:30:04.451301 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:04.451308 22258 sgd_solver.cpp:106] Iteration 28400, lr = 0.055625
I0704 07:30:06.511994 22258 solver.cpp:290] Iteration 28500 (48.5288 iter/s, 2.06063s/100 iter), loss = -4.15742e-06
I0704 07:30:06.512034 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:06.512042 22258 sgd_solver.cpp:106] Iteration 28500, lr = 0.0554687
I0704 07:30:08.571597 22258 solver.cpp:290] Iteration 28600 (48.5555 iter/s, 2.0595s/100 iter), loss = -4.15742e-06
I0704 07:30:08.571655 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:08.571661 22258 sgd_solver.cpp:106] Iteration 28600, lr = 0.0553125
I0704 07:30:10.628180 22258 solver.cpp:290] Iteration 28700 (48.6271 iter/s, 2.05646s/100 iter), loss = 0.0952339
I0704 07:30:10.628202 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:30:10.628209 22258 sgd_solver.cpp:106] Iteration 28700, lr = 0.0551562
I0704 07:30:12.689532 22258 solver.cpp:290] Iteration 28800 (48.5139 iter/s, 2.06127s/100 iter), loss = -4.15742e-06
I0704 07:30:12.689555 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:12.689563 22258 sgd_solver.cpp:106] Iteration 28800, lr = 0.055
I0704 07:30:14.749747 22258 solver.cpp:290] Iteration 28900 (48.5406 iter/s, 2.06013s/100 iter), loss = -4.16115e-06
I0704 07:30:14.749770 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:14.749778 22258 sgd_solver.cpp:106] Iteration 28900, lr = 0.0548437
I0704 07:30:16.787537 22258 solver.cpp:466] Iteration 29000, Testing net (#0)
I0704 07:30:18.433709 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.78
I0704 07:30:18.433729 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9801
I0704 07:30:18.433735 22258 solver.cpp:539]     Test net output #2: loss = 0.7215 (* 1 = 0.7215 loss)
I0704 07:30:18.453373 22258 solver.cpp:290] Iteration 29000 (27.0015 iter/s, 3.7035s/100 iter), loss = -4.14252e-06
I0704 07:30:18.453392 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:18.453403 22258 sgd_solver.cpp:106] Iteration 29000, lr = 0.0546875
I0704 07:30:20.510854 22258 solver.cpp:290] Iteration 29100 (48.6051 iter/s, 2.0574s/100 iter), loss = 0.0952339
I0704 07:30:20.510875 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:30:20.510882 22258 sgd_solver.cpp:106] Iteration 29100, lr = 0.0545313
I0704 07:30:22.568847 22258 solver.cpp:290] Iteration 29200 (48.593 iter/s, 2.05791s/100 iter), loss = -4.14252e-06
I0704 07:30:22.568869 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:22.568876 22258 sgd_solver.cpp:106] Iteration 29200, lr = 0.054375
I0704 07:30:24.627961 22258 solver.cpp:290] Iteration 29300 (48.5666 iter/s, 2.05903s/100 iter), loss = 0.28571
I0704 07:30:24.627995 22258 solver.cpp:309]     Train net output #0: loss = 0.285714 (* 1 = 0.285714 loss)
I0704 07:30:24.628007 22258 sgd_solver.cpp:106] Iteration 29300, lr = 0.0542188
I0704 07:30:26.685132 22258 solver.cpp:290] Iteration 29400 (48.6127 iter/s, 2.05708s/100 iter), loss = -4.14252e-06
I0704 07:30:26.685158 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:26.685164 22258 sgd_solver.cpp:106] Iteration 29400, lr = 0.0540625
I0704 07:30:28.751240 22258 solver.cpp:290] Iteration 29500 (48.4022 iter/s, 2.06602s/100 iter), loss = -4.14252e-06
I0704 07:30:28.751263 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:28.751269 22258 sgd_solver.cpp:106] Iteration 29500, lr = 0.0539063
I0704 07:30:30.809509 22258 solver.cpp:290] Iteration 29600 (48.5866 iter/s, 2.05818s/100 iter), loss = 0.0476149
I0704 07:30:30.809535 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:30:30.809543 22258 sgd_solver.cpp:106] Iteration 29600, lr = 0.05375
I0704 07:30:32.869463 22258 solver.cpp:290] Iteration 29700 (48.5468 iter/s, 2.05987s/100 iter), loss = -4.14252e-06
I0704 07:30:32.869488 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:32.869496 22258 sgd_solver.cpp:106] Iteration 29700, lr = 0.0535938
I0704 07:30:34.955137 22258 solver.cpp:290] Iteration 29800 (47.9481 iter/s, 2.08559s/100 iter), loss = 0.0476149
I0704 07:30:34.955162 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:30:34.955168 22258 sgd_solver.cpp:106] Iteration 29800, lr = 0.0534375
I0704 07:30:37.012848 22258 solver.cpp:290] Iteration 29900 (48.5998 iter/s, 2.05762s/100 iter), loss = 0.142853
I0704 07:30:37.012882 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:30:37.012889 22258 sgd_solver.cpp:106] Iteration 29900, lr = 0.0532812
I0704 07:30:39.052799 22258 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/cifar10_jacintonet11v2_iter_30000.caffemodel
I0704 07:30:39.069841 22258 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/cifar10_jacintonet11v2_iter_30000.solverstate
I0704 07:30:39.077718 22258 solver.cpp:466] Iteration 30000, Testing net (#0)
I0704 07:30:40.723335 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8059
I0704 07:30:40.723352 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9914
I0704 07:30:40.723357 22258 solver.cpp:539]     Test net output #2: loss = 0.6526 (* 1 = 0.6526 loss)
I0704 07:30:40.743007 22258 solver.cpp:290] Iteration 30000 (26.8095 iter/s, 3.73002s/100 iter), loss = -4.14997e-06
I0704 07:30:40.743024 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:40.743038 22258 sgd_solver.cpp:106] Iteration 30000, lr = 0.053125
I0704 07:30:42.802209 22258 solver.cpp:290] Iteration 30100 (48.5644 iter/s, 2.05912s/100 iter), loss = -4.15742e-06
I0704 07:30:42.802232 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:42.802238 22258 sgd_solver.cpp:106] Iteration 30100, lr = 0.0529688
I0704 07:30:44.857180 22258 solver.cpp:290] Iteration 30200 (48.6645 iter/s, 2.05489s/100 iter), loss = -4.14252e-06
I0704 07:30:44.857203 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:44.857210 22258 sgd_solver.cpp:106] Iteration 30200, lr = 0.0528125
I0704 07:30:46.921542 22258 solver.cpp:290] Iteration 30300 (48.4431 iter/s, 2.06428s/100 iter), loss = -4.13135e-06
I0704 07:30:46.921566 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:46.921571 22258 sgd_solver.cpp:106] Iteration 30300, lr = 0.0526563
I0704 07:30:48.978507 22258 solver.cpp:290] Iteration 30400 (48.6174 iter/s, 2.05688s/100 iter), loss = -4.13135e-06
I0704 07:30:48.978528 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:48.978535 22258 sgd_solver.cpp:106] Iteration 30400, lr = 0.0525
I0704 07:30:51.034785 22258 solver.cpp:290] Iteration 30500 (48.6336 iter/s, 2.05619s/100 iter), loss = -4.12762e-06
I0704 07:30:51.034808 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:51.034814 22258 sgd_solver.cpp:106] Iteration 30500, lr = 0.0523438
I0704 07:30:53.090847 22258 solver.cpp:290] Iteration 30600 (48.6387 iter/s, 2.05598s/100 iter), loss = -4.15742e-06
I0704 07:30:53.090869 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:53.090876 22258 sgd_solver.cpp:106] Iteration 30600, lr = 0.0521875
I0704 07:30:55.151810 22258 solver.cpp:290] Iteration 30700 (48.5232 iter/s, 2.06087s/100 iter), loss = -4.15742e-06
I0704 07:30:55.151870 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:55.151882 22258 sgd_solver.cpp:106] Iteration 30700, lr = 0.0520312
I0704 07:30:57.212024 22258 solver.cpp:290] Iteration 30800 (48.5414 iter/s, 2.0601s/100 iter), loss = -4.15742e-06
I0704 07:30:57.212047 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:57.212054 22258 sgd_solver.cpp:106] Iteration 30800, lr = 0.051875
I0704 07:30:59.269448 22258 solver.cpp:290] Iteration 30900 (48.6065 iter/s, 2.05734s/100 iter), loss = -4.14997e-06
I0704 07:30:59.269471 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:30:59.269477 22258 sgd_solver.cpp:106] Iteration 30900, lr = 0.0517187
I0704 07:31:01.309208 22258 solver.cpp:466] Iteration 31000, Testing net (#0)
I0704 07:31:02.953630 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8404
I0704 07:31:02.953649 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9917
I0704 07:31:02.953655 22258 solver.cpp:539]     Test net output #2: loss = 0.4787 (* 1 = 0.4787 loss)
I0704 07:31:02.974756 22258 solver.cpp:290] Iteration 31000 (26.9892 iter/s, 3.70518s/100 iter), loss = -4.14252e-06
I0704 07:31:02.974773 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:02.974786 22258 sgd_solver.cpp:106] Iteration 31000, lr = 0.0515625
I0704 07:31:05.032901 22258 solver.cpp:290] Iteration 31100 (48.5894 iter/s, 2.05806s/100 iter), loss = 0.0476149
I0704 07:31:05.032923 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:31:05.032930 22258 sgd_solver.cpp:106] Iteration 31100, lr = 0.0514063
I0704 07:31:07.090770 22258 solver.cpp:290] Iteration 31200 (48.596 iter/s, 2.05778s/100 iter), loss = 0.0476149
I0704 07:31:07.090792 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:31:07.090800 22258 sgd_solver.cpp:106] Iteration 31200, lr = 0.05125
I0704 07:31:09.147387 22258 solver.cpp:290] Iteration 31300 (48.6256 iter/s, 2.05653s/100 iter), loss = -4.12762e-06
I0704 07:31:09.147456 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:09.147464 22258 sgd_solver.cpp:106] Iteration 31300, lr = 0.0510938
I0704 07:31:11.206727 22258 solver.cpp:290] Iteration 31400 (48.5623 iter/s, 2.05921s/100 iter), loss = 0.095234
I0704 07:31:11.206751 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:31:11.206760 22258 sgd_solver.cpp:106] Iteration 31400, lr = 0.0509375
I0704 07:31:13.262729 22258 solver.cpp:290] Iteration 31500 (48.6401 iter/s, 2.05592s/100 iter), loss = -4.14997e-06
I0704 07:31:13.262753 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:13.262763 22258 sgd_solver.cpp:106] Iteration 31500, lr = 0.0507812
I0704 07:31:15.327494 22258 solver.cpp:290] Iteration 31600 (48.4337 iter/s, 2.06468s/100 iter), loss = 0.0476149
I0704 07:31:15.327517 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:31:15.327524 22258 sgd_solver.cpp:106] Iteration 31600, lr = 0.050625
I0704 07:31:17.384901 22258 solver.cpp:290] Iteration 31700 (48.6069 iter/s, 2.05732s/100 iter), loss = -4.18723e-06
I0704 07:31:17.384923 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:17.384929 22258 sgd_solver.cpp:106] Iteration 31700, lr = 0.0504688
I0704 07:31:19.441540 22258 solver.cpp:290] Iteration 31800 (48.625 iter/s, 2.05655s/100 iter), loss = -4.20213e-06
I0704 07:31:19.441565 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:19.441573 22258 sgd_solver.cpp:106] Iteration 31800, lr = 0.0503125
I0704 07:31:21.508139 22258 solver.cpp:290] Iteration 31900 (48.3907 iter/s, 2.06651s/100 iter), loss = -4.20213e-06
I0704 07:31:21.508162 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:21.508168 22258 sgd_solver.cpp:106] Iteration 31900, lr = 0.0501562
I0704 07:31:23.548688 22258 solver.cpp:466] Iteration 32000, Testing net (#0)
I0704 07:31:25.193497 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8006
I0704 07:31:25.193519 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9895
I0704 07:31:25.193526 22258 solver.cpp:539]     Test net output #2: loss = 0.7039 (* 1 = 0.7039 loss)
I0704 07:31:25.213227 22258 solver.cpp:290] Iteration 32000 (26.9909 iter/s, 3.70495s/100 iter), loss = 0.0952339
I0704 07:31:25.213255 22258 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:31:25.213261 22258 sgd_solver.cpp:106] Iteration 32000, lr = 0.05
I0704 07:31:27.272495 22258 solver.cpp:290] Iteration 32100 (48.563 iter/s, 2.05918s/100 iter), loss = -4.20585e-06
I0704 07:31:27.272518 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:27.272526 22258 sgd_solver.cpp:106] Iteration 32100, lr = 0.0498438
I0704 07:31:29.327901 22258 solver.cpp:290] Iteration 32200 (48.6542 iter/s, 2.05532s/100 iter), loss = -4.21703e-06
I0704 07:31:29.327925 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:29.327932 22258 sgd_solver.cpp:106] Iteration 32200, lr = 0.0496875
I0704 07:31:31.387190 22258 solver.cpp:290] Iteration 32300 (48.5625 iter/s, 2.0592s/100 iter), loss = -4.21703e-06
I0704 07:31:31.387214 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:31.387223 22258 sgd_solver.cpp:106] Iteration 32300, lr = 0.0495313
I0704 07:31:33.477000 22258 solver.cpp:290] Iteration 32400 (47.8532 iter/s, 2.08972s/100 iter), loss = -4.22075e-06
I0704 07:31:33.477025 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:33.477030 22258 sgd_solver.cpp:106] Iteration 32400, lr = 0.049375
I0704 07:31:35.537803 22258 solver.cpp:290] Iteration 32500 (48.5268 iter/s, 2.06072s/100 iter), loss = -4.20958e-06
I0704 07:31:35.537827 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:35.537834 22258 sgd_solver.cpp:106] Iteration 32500, lr = 0.0492188
I0704 07:31:37.595345 22258 solver.cpp:290] Iteration 32600 (48.6038 iter/s, 2.05745s/100 iter), loss = -4.24683e-06
I0704 07:31:37.595382 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:37.595389 22258 sgd_solver.cpp:106] Iteration 32600, lr = 0.0490625
I0704 07:31:39.654559 22258 solver.cpp:290] Iteration 32700 (48.5646 iter/s, 2.05911s/100 iter), loss = -4.25056e-06
I0704 07:31:39.654649 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:39.654659 22258 sgd_solver.cpp:106] Iteration 32700, lr = 0.0489062
I0704 07:31:41.709440 22258 solver.cpp:290] Iteration 32800 (48.6682 iter/s, 2.05473s/100 iter), loss = -4.24683e-06
I0704 07:31:41.709463 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:41.709470 22258 sgd_solver.cpp:106] Iteration 32800, lr = 0.04875
I0704 07:31:43.765894 22258 solver.cpp:290] Iteration 32900 (48.6294 iter/s, 2.05637s/100 iter), loss = -4.24683e-06
I0704 07:31:43.765916 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:43.765924 22258 sgd_solver.cpp:106] Iteration 32900, lr = 0.0485937
I0704 07:31:45.802311 22258 solver.cpp:466] Iteration 33000, Testing net (#0)
I0704 07:31:47.445047 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8555
I0704 07:31:47.445066 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9925
I0704 07:31:47.445072 22258 solver.cpp:539]     Test net output #2: loss = 0.4413 (* 1 = 0.4413 loss)
I0704 07:31:47.464771 22258 solver.cpp:290] Iteration 33000 (27.0362 iter/s, 3.69875s/100 iter), loss = -4.26918e-06
I0704 07:31:47.464787 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:47.464802 22258 sgd_solver.cpp:106] Iteration 33000, lr = 0.0484375
I0704 07:31:49.523048 22258 solver.cpp:290] Iteration 33100 (48.5862 iter/s, 2.0582s/100 iter), loss = 0.0476148
I0704 07:31:49.523069 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:31:49.523075 22258 sgd_solver.cpp:106] Iteration 33100, lr = 0.0482813
I0704 07:31:51.581141 22258 solver.cpp:290] Iteration 33200 (48.5906 iter/s, 2.05801s/100 iter), loss = -4.28408e-06
I0704 07:31:51.581166 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:51.581174 22258 sgd_solver.cpp:106] Iteration 33200, lr = 0.048125
I0704 07:31:53.641331 22258 solver.cpp:290] Iteration 33300 (48.5413 iter/s, 2.0601s/100 iter), loss = -4.26173e-06
I0704 07:31:53.641352 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:53.641360 22258 sgd_solver.cpp:106] Iteration 33300, lr = 0.0479688
I0704 07:31:55.698835 22258 solver.cpp:290] Iteration 33400 (48.6045 iter/s, 2.05742s/100 iter), loss = -4.26173e-06
I0704 07:31:55.698858 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:55.698865 22258 sgd_solver.cpp:106] Iteration 33400, lr = 0.0478125
I0704 07:31:57.757197 22258 solver.cpp:290] Iteration 33500 (48.5843 iter/s, 2.05828s/100 iter), loss = 0.0476148
I0704 07:31:57.757222 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:31:57.757231 22258 sgd_solver.cpp:106] Iteration 33500, lr = 0.0476562
I0704 07:31:59.815436 22258 solver.cpp:290] Iteration 33600 (48.5873 iter/s, 2.05815s/100 iter), loss = -4.26173e-06
I0704 07:31:59.815459 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:31:59.815467 22258 sgd_solver.cpp:106] Iteration 33600, lr = 0.0475
I0704 07:32:01.876822 22258 solver.cpp:290] Iteration 33700 (48.5131 iter/s, 2.0613s/100 iter), loss = -4.26173e-06
I0704 07:32:01.876845 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:01.876852 22258 sgd_solver.cpp:106] Iteration 33700, lr = 0.0473437
I0704 07:32:03.932516 22258 solver.cpp:290] Iteration 33800 (48.6474 iter/s, 2.05561s/100 iter), loss = -4.26173e-06
I0704 07:32:03.932539 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:03.932546 22258 sgd_solver.cpp:106] Iteration 33800, lr = 0.0471875
I0704 07:32:05.992719 22258 solver.cpp:290] Iteration 33900 (48.541 iter/s, 2.06012s/100 iter), loss = -4.26173e-06
I0704 07:32:05.992744 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:05.992753 22258 sgd_solver.cpp:106] Iteration 33900, lr = 0.0470312
I0704 07:32:08.030169 22258 solver.cpp:466] Iteration 34000, Testing net (#0)
I0704 07:32:09.680107 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8182
I0704 07:32:09.680160 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9797
I0704 07:32:09.680166 22258 solver.cpp:539]     Test net output #2: loss = 0.7002 (* 1 = 0.7002 loss)
I0704 07:32:09.699818 22258 solver.cpp:290] Iteration 34000 (26.9762 iter/s, 3.70697s/100 iter), loss = -4.26173e-06
I0704 07:32:09.699836 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:09.699846 22258 sgd_solver.cpp:106] Iteration 34000, lr = 0.046875
I0704 07:32:11.759791 22258 solver.cpp:290] Iteration 34100 (48.5462 iter/s, 2.05989s/100 iter), loss = -4.26173e-06
I0704 07:32:11.759814 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:11.759820 22258 sgd_solver.cpp:106] Iteration 34100, lr = 0.0467188
I0704 07:32:13.818289 22258 solver.cpp:290] Iteration 34200 (48.5811 iter/s, 2.05841s/100 iter), loss = 0.0476148
I0704 07:32:13.818313 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:32:13.818321 22258 sgd_solver.cpp:106] Iteration 34200, lr = 0.0465625
I0704 07:32:15.879756 22258 solver.cpp:290] Iteration 34300 (48.5112 iter/s, 2.06138s/100 iter), loss = -4.27663e-06
I0704 07:32:15.879781 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:15.879786 22258 sgd_solver.cpp:106] Iteration 34300, lr = 0.0464063
I0704 07:32:17.937507 22258 solver.cpp:290] Iteration 34400 (48.5988 iter/s, 2.05766s/100 iter), loss = 0.0476148
I0704 07:32:17.937531 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:32:17.937538 22258 sgd_solver.cpp:106] Iteration 34400, lr = 0.04625
I0704 07:32:19.996743 22258 solver.cpp:290] Iteration 34500 (48.5638 iter/s, 2.05915s/100 iter), loss = 0.142853
I0704 07:32:19.996767 22258 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:32:19.996776 22258 sgd_solver.cpp:106] Iteration 34500, lr = 0.0460938
I0704 07:32:22.053777 22258 solver.cpp:290] Iteration 34600 (48.6157 iter/s, 2.05695s/100 iter), loss = -4.28408e-06
I0704 07:32:22.053805 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:22.053814 22258 sgd_solver.cpp:106] Iteration 34600, lr = 0.0459375
I0704 07:32:24.109560 22258 solver.cpp:290] Iteration 34700 (48.6454 iter/s, 2.05569s/100 iter), loss = -4.27663e-06
I0704 07:32:24.109587 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:24.109596 22258 sgd_solver.cpp:106] Iteration 34700, lr = 0.0457813
I0704 07:32:26.165510 22258 solver.cpp:290] Iteration 34800 (48.6413 iter/s, 2.05586s/100 iter), loss = -4.26173e-06
I0704 07:32:26.165534 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:26.165541 22258 sgd_solver.cpp:106] Iteration 34800, lr = 0.045625
I0704 07:32:28.228832 22258 solver.cpp:290] Iteration 34900 (48.4676 iter/s, 2.06324s/100 iter), loss = 0.0476148
I0704 07:32:28.228855 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:32:28.228863 22258 sgd_solver.cpp:106] Iteration 34900, lr = 0.0454687
I0704 07:32:30.266830 22258 solver.cpp:466] Iteration 35000, Testing net (#0)
I0704 07:32:31.923547 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8464
I0704 07:32:31.923566 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9919
I0704 07:32:31.923573 22258 solver.cpp:539]     Test net output #2: loss = 0.5015 (* 1 = 0.5015 loss)
I0704 07:32:31.943616 22258 solver.cpp:290] Iteration 35000 (26.9204 iter/s, 3.71466s/100 iter), loss = -4.28408e-06
I0704 07:32:31.943634 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:31.943648 22258 sgd_solver.cpp:106] Iteration 35000, lr = 0.0453125
I0704 07:32:34.036572 22258 solver.cpp:290] Iteration 35100 (47.7812 iter/s, 2.09287s/100 iter), loss = -4.27663e-06
I0704 07:32:34.036597 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:34.036607 22258 sgd_solver.cpp:106] Iteration 35100, lr = 0.0451563
I0704 07:32:36.093822 22258 solver.cpp:290] Iteration 35200 (48.6107 iter/s, 2.05716s/100 iter), loss = -4.27663e-06
I0704 07:32:36.093859 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:36.093868 22258 sgd_solver.cpp:106] Iteration 35200, lr = 0.045
I0704 07:32:38.152698 22258 solver.cpp:290] Iteration 35300 (48.5725 iter/s, 2.05878s/100 iter), loss = -4.26173e-06
I0704 07:32:38.152719 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:38.152726 22258 sgd_solver.cpp:106] Iteration 35300, lr = 0.0448438
I0704 07:32:40.213850 22258 solver.cpp:290] Iteration 35400 (48.5186 iter/s, 2.06107s/100 iter), loss = -4.26173e-06
I0704 07:32:40.213920 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:40.213928 22258 sgd_solver.cpp:106] Iteration 35400, lr = 0.0446875
I0704 07:32:42.272251 22258 solver.cpp:290] Iteration 35500 (48.5846 iter/s, 2.05827s/100 iter), loss = -4.27663e-06
I0704 07:32:42.272284 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:42.272292 22258 sgd_solver.cpp:106] Iteration 35500, lr = 0.0445313
I0704 07:32:44.331811 22258 solver.cpp:290] Iteration 35600 (48.5563 iter/s, 2.05947s/100 iter), loss = 0.238091
I0704 07:32:44.331835 22258 solver.cpp:309]     Train net output #0: loss = 0.238095 (* 1 = 0.238095 loss)
I0704 07:32:44.331845 22258 sgd_solver.cpp:106] Iteration 35600, lr = 0.044375
I0704 07:32:46.396739 22258 solver.cpp:290] Iteration 35700 (48.4299 iter/s, 2.06484s/100 iter), loss = -4.27663e-06
I0704 07:32:46.396761 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:46.396770 22258 sgd_solver.cpp:106] Iteration 35700, lr = 0.0442187
I0704 07:32:48.453832 22258 solver.cpp:290] Iteration 35800 (48.6143 iter/s, 2.05701s/100 iter), loss = -4.28036e-06
I0704 07:32:48.453855 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:48.453861 22258 sgd_solver.cpp:106] Iteration 35800, lr = 0.0440625
I0704 07:32:50.514259 22258 solver.cpp:290] Iteration 35900 (48.5356 iter/s, 2.06034s/100 iter), loss = -4.27663e-06
I0704 07:32:50.514283 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:50.514292 22258 sgd_solver.cpp:106] Iteration 35900, lr = 0.0439062
I0704 07:32:52.550629 22258 solver.cpp:466] Iteration 36000, Testing net (#0)
I0704 07:32:54.194819 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8559
I0704 07:32:54.194839 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9946
I0704 07:32:54.194844 22258 solver.cpp:539]     Test net output #2: loss = 0.419 (* 1 = 0.419 loss)
I0704 07:32:54.215956 22258 solver.cpp:290] Iteration 36000 (27.0156 iter/s, 3.70157s/100 iter), loss = -4.27663e-06
I0704 07:32:54.215972 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:54.215986 22258 sgd_solver.cpp:106] Iteration 36000, lr = 0.04375
I0704 07:32:56.271142 22258 solver.cpp:290] Iteration 36100 (48.6594 iter/s, 2.0551s/100 iter), loss = -4.27663e-06
I0704 07:32:56.271173 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:56.271183 22258 sgd_solver.cpp:106] Iteration 36100, lr = 0.0435938
I0704 07:32:58.327908 22258 solver.cpp:290] Iteration 36200 (48.6223 iter/s, 2.05667s/100 iter), loss = -4.26173e-06
I0704 07:32:58.327934 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:32:58.327944 22258 sgd_solver.cpp:106] Iteration 36200, lr = 0.0434375
I0704 07:33:00.382444 22258 solver.cpp:290] Iteration 36300 (48.6749 iter/s, 2.05445s/100 iter), loss = 0.0476148
I0704 07:33:00.382469 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:33:00.382478 22258 sgd_solver.cpp:106] Iteration 36300, lr = 0.0432813
I0704 07:33:02.444897 22258 solver.cpp:290] Iteration 36400 (48.488 iter/s, 2.06236s/100 iter), loss = -4.26546e-06
I0704 07:33:02.444923 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:02.444932 22258 sgd_solver.cpp:106] Iteration 36400, lr = 0.043125
I0704 07:33:04.503798 22258 solver.cpp:290] Iteration 36500 (48.5717 iter/s, 2.05881s/100 iter), loss = -4.26918e-06
I0704 07:33:04.503821 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:04.503828 22258 sgd_solver.cpp:106] Iteration 36500, lr = 0.0429688
I0704 07:33:06.563796 22258 solver.cpp:290] Iteration 36600 (48.5458 iter/s, 2.05991s/100 iter), loss = -4.26173e-06
I0704 07:33:06.563820 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:06.563827 22258 sgd_solver.cpp:106] Iteration 36600, lr = 0.0428125
I0704 07:33:08.624455 22258 solver.cpp:290] Iteration 36700 (48.5302 iter/s, 2.06057s/100 iter), loss = -4.26918e-06
I0704 07:33:08.624492 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:08.624498 22258 sgd_solver.cpp:106] Iteration 36700, lr = 0.0426563
I0704 07:33:10.686239 22258 solver.cpp:290] Iteration 36800 (48.504 iter/s, 2.06169s/100 iter), loss = -4.26173e-06
I0704 07:33:10.686298 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:10.686308 22258 sgd_solver.cpp:106] Iteration 36800, lr = 0.0425
I0704 07:33:12.746116 22258 solver.cpp:290] Iteration 36900 (48.5494 iter/s, 2.05976s/100 iter), loss = -4.26173e-06
I0704 07:33:12.746139 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:12.746145 22258 sgd_solver.cpp:106] Iteration 36900, lr = 0.0423437
I0704 07:33:14.782622 22258 solver.cpp:466] Iteration 37000, Testing net (#0)
I0704 07:33:16.434216 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8594
I0704 07:33:16.434234 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9929
I0704 07:33:16.434240 22258 solver.cpp:539]     Test net output #2: loss = 0.42 (* 1 = 0.42 loss)
I0704 07:33:16.453925 22258 solver.cpp:290] Iteration 37000 (26.971 iter/s, 3.70768s/100 iter), loss = -4.26546e-06
I0704 07:33:16.453943 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:16.453954 22258 sgd_solver.cpp:106] Iteration 37000, lr = 0.0421875
I0704 07:33:18.511737 22258 solver.cpp:290] Iteration 37100 (48.5973 iter/s, 2.05773s/100 iter), loss = -4.26918e-06
I0704 07:33:18.511761 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:18.511768 22258 sgd_solver.cpp:106] Iteration 37100, lr = 0.0420313
I0704 07:33:20.579502 22258 solver.cpp:290] Iteration 37200 (48.3634 iter/s, 2.06768s/100 iter), loss = -4.27663e-06
I0704 07:33:20.579525 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:20.579532 22258 sgd_solver.cpp:106] Iteration 37200, lr = 0.041875
I0704 07:33:22.637738 22258 solver.cpp:290] Iteration 37300 (48.5873 iter/s, 2.05815s/100 iter), loss = -4.26173e-06
I0704 07:33:22.637761 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:22.637768 22258 sgd_solver.cpp:106] Iteration 37300, lr = 0.0417188
I0704 07:33:24.705727 22258 solver.cpp:290] Iteration 37400 (48.3582 iter/s, 2.0679s/100 iter), loss = 0.0476148
I0704 07:33:24.705749 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:33:24.705756 22258 sgd_solver.cpp:106] Iteration 37400, lr = 0.0415625
I0704 07:33:26.763715 22258 solver.cpp:290] Iteration 37500 (48.5932 iter/s, 2.0579s/100 iter), loss = -4.27663e-06
I0704 07:33:26.763737 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:26.763746 22258 sgd_solver.cpp:106] Iteration 37500, lr = 0.0414063
I0704 07:33:28.824168 22258 solver.cpp:290] Iteration 37600 (48.5351 iter/s, 2.06036s/100 iter), loss = -4.27663e-06
I0704 07:33:28.824194 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:28.824203 22258 sgd_solver.cpp:106] Iteration 37600, lr = 0.04125
I0704 07:33:30.884277 22258 solver.cpp:290] Iteration 37700 (48.5432 iter/s, 2.06002s/100 iter), loss = -4.27663e-06
I0704 07:33:30.884299 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:30.884307 22258 sgd_solver.cpp:106] Iteration 37700, lr = 0.0410937
I0704 07:33:32.943205 22258 solver.cpp:290] Iteration 37800 (48.571 iter/s, 2.05884s/100 iter), loss = -4.26173e-06
I0704 07:33:32.943230 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:32.943239 22258 sgd_solver.cpp:106] Iteration 37800, lr = 0.0409375
I0704 07:33:35.005724 22258 solver.cpp:290] Iteration 37900 (48.4865 iter/s, 2.06243s/100 iter), loss = -4.26173e-06
I0704 07:33:35.005749 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:35.005758 22258 sgd_solver.cpp:106] Iteration 37900, lr = 0.0407812
I0704 07:33:37.047266 22258 solver.cpp:466] Iteration 38000, Testing net (#0)
I0704 07:33:38.692030 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8534
I0704 07:33:38.692050 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9906
I0704 07:33:38.692057 22258 solver.cpp:539]     Test net output #2: loss = 0.4886 (* 1 = 0.4886 loss)
I0704 07:33:38.711900 22258 solver.cpp:290] Iteration 38000 (26.9829 iter/s, 3.70605s/100 iter), loss = -4.26173e-06
I0704 07:33:38.711930 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:38.711941 22258 sgd_solver.cpp:106] Iteration 38000, lr = 0.040625
I0704 07:33:40.770329 22258 solver.cpp:290] Iteration 38100 (48.5829 iter/s, 2.05834s/100 iter), loss = 0.0476148
I0704 07:33:40.770375 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:33:40.770382 22258 sgd_solver.cpp:106] Iteration 38100, lr = 0.0404688
I0704 07:33:42.830030 22258 solver.cpp:290] Iteration 38200 (48.5533 iter/s, 2.05959s/100 iter), loss = -4.26173e-06
I0704 07:33:42.830055 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:42.830065 22258 sgd_solver.cpp:106] Iteration 38200, lr = 0.0403125
I0704 07:33:44.890030 22258 solver.cpp:290] Iteration 38300 (48.5458 iter/s, 2.05991s/100 iter), loss = 0.0476148
I0704 07:33:44.890053 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:33:44.890061 22258 sgd_solver.cpp:106] Iteration 38300, lr = 0.0401563
I0704 07:33:46.947796 22258 solver.cpp:290] Iteration 38400 (48.5984 iter/s, 2.05768s/100 iter), loss = -4.26173e-06
I0704 07:33:46.947819 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:46.947826 22258 sgd_solver.cpp:106] Iteration 38400, lr = 0.04
I0704 07:33:49.006731 22258 solver.cpp:290] Iteration 38500 (48.5708 iter/s, 2.05885s/100 iter), loss = -4.26173e-06
I0704 07:33:49.006753 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:49.006759 22258 sgd_solver.cpp:106] Iteration 38500, lr = 0.0398437
I0704 07:33:51.064679 22258 solver.cpp:290] Iteration 38600 (48.5941 iter/s, 2.05786s/100 iter), loss = -4.26173e-06
I0704 07:33:51.064702 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:51.064709 22258 sgd_solver.cpp:106] Iteration 38600, lr = 0.0396875
I0704 07:33:53.120816 22258 solver.cpp:290] Iteration 38700 (48.6369 iter/s, 2.05605s/100 iter), loss = -4.26173e-06
I0704 07:33:53.120838 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:53.120846 22258 sgd_solver.cpp:106] Iteration 38700, lr = 0.0395312
I0704 07:33:55.178911 22258 solver.cpp:290] Iteration 38800 (48.5906 iter/s, 2.05801s/100 iter), loss = -4.26173e-06
I0704 07:33:55.178935 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:55.178941 22258 sgd_solver.cpp:106] Iteration 38800, lr = 0.039375
I0704 07:33:57.241607 22258 solver.cpp:290] Iteration 38900 (48.4823 iter/s, 2.06261s/100 iter), loss = -4.26173e-06
I0704 07:33:57.241631 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:33:57.241638 22258 sgd_solver.cpp:106] Iteration 38900, lr = 0.0392187
I0704 07:33:59.278486 22258 solver.cpp:466] Iteration 39000, Testing net (#0)
I0704 07:34:00.931758 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.855
I0704 07:34:00.931778 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9892
I0704 07:34:00.931784 22258 solver.cpp:539]     Test net output #2: loss = 0.441 (* 1 = 0.441 loss)
I0704 07:34:00.951982 22258 solver.cpp:290] Iteration 39000 (26.9524 iter/s, 3.71024s/100 iter), loss = -4.26546e-06
I0704 07:34:00.952002 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:00.952010 22258 sgd_solver.cpp:106] Iteration 39000, lr = 0.0390625
I0704 07:34:03.015444 22258 solver.cpp:290] Iteration 39100 (48.4642 iter/s, 2.06338s/100 iter), loss = -4.26918e-06
I0704 07:34:03.015468 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:03.015475 22258 sgd_solver.cpp:106] Iteration 39100, lr = 0.0389063
I0704 07:34:05.072736 22258 solver.cpp:290] Iteration 39200 (48.6096 iter/s, 2.0572s/100 iter), loss = -4.27663e-06
I0704 07:34:05.072760 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:05.072769 22258 sgd_solver.cpp:106] Iteration 39200, lr = 0.03875
I0704 07:34:07.130800 22258 solver.cpp:290] Iteration 39300 (48.5914 iter/s, 2.05798s/100 iter), loss = -4.27663e-06
I0704 07:34:07.130822 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:07.130828 22258 sgd_solver.cpp:106] Iteration 39300, lr = 0.0385938
I0704 07:34:09.190724 22258 solver.cpp:290] Iteration 39400 (48.5475 iter/s, 2.05984s/100 iter), loss = -4.27663e-06
I0704 07:34:09.190764 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:09.190774 22258 sgd_solver.cpp:106] Iteration 39400, lr = 0.0384375
I0704 07:34:11.249033 22258 solver.cpp:290] Iteration 39500 (48.586 iter/s, 2.05821s/100 iter), loss = -4.28036e-06
I0704 07:34:11.249091 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:11.249099 22258 sgd_solver.cpp:106] Iteration 39500, lr = 0.0382813
I0704 07:34:13.304940 22258 solver.cpp:290] Iteration 39600 (48.6432 iter/s, 2.05579s/100 iter), loss = 0.0476148
I0704 07:34:13.304963 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:34:13.304970 22258 sgd_solver.cpp:106] Iteration 39600, lr = 0.038125
I0704 07:34:15.363942 22258 solver.cpp:290] Iteration 39700 (48.5693 iter/s, 2.05891s/100 iter), loss = -4.27663e-06
I0704 07:34:15.363968 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:15.363976 22258 sgd_solver.cpp:106] Iteration 39700, lr = 0.0379688
I0704 07:34:17.422291 22258 solver.cpp:290] Iteration 39800 (48.5848 iter/s, 2.05826s/100 iter), loss = -4.27663e-06
I0704 07:34:17.422317 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:17.422323 22258 sgd_solver.cpp:106] Iteration 39800, lr = 0.0378125
I0704 07:34:19.479971 22258 solver.cpp:290] Iteration 39900 (48.6006 iter/s, 2.05759s/100 iter), loss = -4.27663e-06
I0704 07:34:19.479996 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:19.480002 22258 sgd_solver.cpp:106] Iteration 39900, lr = 0.0376562
I0704 07:34:21.517273 22258 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/cifar10_jacintonet11v2_iter_40000.caffemodel
I0704 07:34:21.533663 22258 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/cifar10_jacintonet11v2_iter_40000.solverstate
I0704 07:34:21.541682 22258 solver.cpp:466] Iteration 40000, Testing net (#0)
I0704 07:34:23.188443 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8271
I0704 07:34:23.188463 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9881
I0704 07:34:23.188469 22258 solver.cpp:539]     Test net output #2: loss = 0.5876 (* 1 = 0.5876 loss)
I0704 07:34:23.208238 22258 solver.cpp:290] Iteration 40000 (26.8231 iter/s, 3.72814s/100 iter), loss = -4.27663e-06
I0704 07:34:23.208259 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:23.208267 22258 sgd_solver.cpp:106] Iteration 40000, lr = 0.0375
I0704 07:34:25.269345 22258 solver.cpp:290] Iteration 40100 (48.5196 iter/s, 2.06102s/100 iter), loss = -4.27663e-06
I0704 07:34:25.269368 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:25.269374 22258 sgd_solver.cpp:106] Iteration 40100, lr = 0.0373438
I0704 07:34:27.325814 22258 solver.cpp:290] Iteration 40200 (48.6291 iter/s, 2.05638s/100 iter), loss = -4.27663e-06
I0704 07:34:27.325836 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:27.325842 22258 sgd_solver.cpp:106] Iteration 40200, lr = 0.0371875
I0704 07:34:29.383517 22258 solver.cpp:290] Iteration 40300 (48.6 iter/s, 2.05761s/100 iter), loss = -4.27663e-06
I0704 07:34:29.383543 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:29.383551 22258 sgd_solver.cpp:106] Iteration 40300, lr = 0.0370313
I0704 07:34:31.444036 22258 solver.cpp:290] Iteration 40400 (48.5335 iter/s, 2.06043s/100 iter), loss = 0.0476148
I0704 07:34:31.444056 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:34:31.444063 22258 sgd_solver.cpp:106] Iteration 40400, lr = 0.036875
I0704 07:34:33.537891 22258 solver.cpp:290] Iteration 40500 (47.7607 iter/s, 2.09377s/100 iter), loss = -4.27663e-06
I0704 07:34:33.537914 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:33.537921 22258 sgd_solver.cpp:106] Iteration 40500, lr = 0.0367188
I0704 07:34:35.594281 22258 solver.cpp:290] Iteration 40600 (48.6309 iter/s, 2.0563s/100 iter), loss = -4.27663e-06
I0704 07:34:35.594305 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:35.594310 22258 sgd_solver.cpp:106] Iteration 40600, lr = 0.0365625
I0704 07:34:37.651644 22258 solver.cpp:290] Iteration 40700 (48.6079 iter/s, 2.05728s/100 iter), loss = -4.26173e-06
I0704 07:34:37.651666 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:37.651674 22258 sgd_solver.cpp:106] Iteration 40700, lr = 0.0364062
I0704 07:34:39.713636 22258 solver.cpp:290] Iteration 40800 (48.4988 iter/s, 2.06191s/100 iter), loss = -4.26173e-06
I0704 07:34:39.713663 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:39.713672 22258 sgd_solver.cpp:106] Iteration 40800, lr = 0.03625
I0704 07:34:41.774133 22258 solver.cpp:290] Iteration 40900 (48.5341 iter/s, 2.06041s/100 iter), loss = -4.26546e-06
I0704 07:34:41.774184 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:41.774193 22258 sgd_solver.cpp:106] Iteration 40900, lr = 0.0360937
I0704 07:34:43.814867 22258 solver.cpp:466] Iteration 41000, Testing net (#0)
I0704 07:34:45.472419 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.867299
I0704 07:34:45.472437 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9932
I0704 07:34:45.472442 22258 solver.cpp:539]     Test net output #2: loss = 0.4182 (* 1 = 0.4182 loss)
I0704 07:34:45.492173 22258 solver.cpp:290] Iteration 41000 (26.897 iter/s, 3.71789s/100 iter), loss = 0.0476148
I0704 07:34:45.492190 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:34:45.492204 22258 sgd_solver.cpp:106] Iteration 41000, lr = 0.0359375
I0704 07:34:47.556841 22258 solver.cpp:290] Iteration 41100 (48.4359 iter/s, 2.06458s/100 iter), loss = -4.26173e-06
I0704 07:34:47.556867 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:47.556875 22258 sgd_solver.cpp:106] Iteration 41100, lr = 0.0357813
I0704 07:34:49.615797 22258 solver.cpp:290] Iteration 41200 (48.5704 iter/s, 2.05887s/100 iter), loss = -4.26173e-06
I0704 07:34:49.615820 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:49.615828 22258 sgd_solver.cpp:106] Iteration 41200, lr = 0.035625
I0704 07:34:51.672560 22258 solver.cpp:290] Iteration 41300 (48.6221 iter/s, 2.05668s/100 iter), loss = -4.26173e-06
I0704 07:34:51.672582 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:51.672590 22258 sgd_solver.cpp:106] Iteration 41300, lr = 0.0354688
I0704 07:34:53.729164 22258 solver.cpp:290] Iteration 41400 (48.6259 iter/s, 2.05652s/100 iter), loss = -4.26173e-06
I0704 07:34:53.729187 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:53.729194 22258 sgd_solver.cpp:106] Iteration 41400, lr = 0.0353125
I0704 07:34:55.788463 22258 solver.cpp:290] Iteration 41500 (48.5623 iter/s, 2.05921s/100 iter), loss = -4.26173e-06
I0704 07:34:55.788497 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:55.788508 22258 sgd_solver.cpp:106] Iteration 41500, lr = 0.0351562
I0704 07:34:57.844913 22258 solver.cpp:290] Iteration 41600 (48.6298 iter/s, 2.05635s/100 iter), loss = -4.26173e-06
I0704 07:34:57.844938 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:57.844944 22258 sgd_solver.cpp:106] Iteration 41600, lr = 0.035
I0704 07:34:59.900017 22258 solver.cpp:290] Iteration 41700 (48.6614 iter/s, 2.05502s/100 iter), loss = -4.26173e-06
I0704 07:34:59.900039 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:34:59.900045 22258 sgd_solver.cpp:106] Iteration 41700, lr = 0.0348438
I0704 07:35:01.964921 22258 solver.cpp:290] Iteration 41800 (48.4305 iter/s, 2.06482s/100 iter), loss = -4.26173e-06
I0704 07:35:01.964953 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:01.964959 22258 sgd_solver.cpp:106] Iteration 41800, lr = 0.0346875
I0704 07:35:04.022902 22258 solver.cpp:290] Iteration 41900 (48.5935 iter/s, 2.05789s/100 iter), loss = -4.26173e-06
I0704 07:35:04.022927 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:04.022933 22258 sgd_solver.cpp:106] Iteration 41900, lr = 0.0345312
I0704 07:35:06.061306 22258 solver.cpp:466] Iteration 42000, Testing net (#0)
I0704 07:35:07.717591 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8309
I0704 07:35:07.717608 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9883
I0704 07:35:07.717614 22258 solver.cpp:539]     Test net output #2: loss = 0.5865 (* 1 = 0.5865 loss)
I0704 07:35:07.737366 22258 solver.cpp:290] Iteration 42000 (26.9227 iter/s, 3.71434s/100 iter), loss = -4.26173e-06
I0704 07:35:07.737385 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:07.737397 22258 sgd_solver.cpp:106] Iteration 42000, lr = 0.034375
I0704 07:35:09.792572 22258 solver.cpp:290] Iteration 42100 (48.6588 iter/s, 2.05512s/100 iter), loss = -4.26173e-06
I0704 07:35:09.792610 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:09.792618 22258 sgd_solver.cpp:106] Iteration 42100, lr = 0.0342188
I0704 07:35:11.851795 22258 solver.cpp:290] Iteration 42200 (48.5644 iter/s, 2.05912s/100 iter), loss = -4.26173e-06
I0704 07:35:11.851884 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:11.851891 22258 sgd_solver.cpp:106] Iteration 42200, lr = 0.0340625
I0704 07:35:13.908632 22258 solver.cpp:290] Iteration 42300 (48.6218 iter/s, 2.05669s/100 iter), loss = -4.26173e-06
I0704 07:35:13.908656 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:13.908663 22258 sgd_solver.cpp:106] Iteration 42300, lr = 0.0339063
I0704 07:35:15.969501 22258 solver.cpp:290] Iteration 42400 (48.5253 iter/s, 2.06078s/100 iter), loss = -4.26173e-06
I0704 07:35:15.969523 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:15.969530 22258 sgd_solver.cpp:106] Iteration 42400, lr = 0.03375
I0704 07:35:18.026178 22258 solver.cpp:290] Iteration 42500 (48.6242 iter/s, 2.05659s/100 iter), loss = -4.26173e-06
I0704 07:35:18.026204 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:18.026213 22258 sgd_solver.cpp:106] Iteration 42500, lr = 0.0335938
I0704 07:35:20.083413 22258 solver.cpp:290] Iteration 42600 (48.611 iter/s, 2.05715s/100 iter), loss = 0.0476148
I0704 07:35:20.083437 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:35:20.083444 22258 sgd_solver.cpp:106] Iteration 42600, lr = 0.0334375
I0704 07:35:22.140087 22258 solver.cpp:290] Iteration 42700 (48.6242 iter/s, 2.05659s/100 iter), loss = 0.0476148
I0704 07:35:22.140111 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:35:22.140120 22258 sgd_solver.cpp:106] Iteration 42700, lr = 0.0332812
I0704 07:35:24.199487 22258 solver.cpp:290] Iteration 42800 (48.5599 iter/s, 2.05931s/100 iter), loss = -4.26173e-06
I0704 07:35:24.199509 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:24.199517 22258 sgd_solver.cpp:106] Iteration 42800, lr = 0.033125
I0704 07:35:26.262598 22258 solver.cpp:290] Iteration 42900 (48.4725 iter/s, 2.06302s/100 iter), loss = -4.26173e-06
I0704 07:35:26.262620 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:26.262626 22258 sgd_solver.cpp:106] Iteration 42900, lr = 0.0329687
I0704 07:35:28.299687 22258 solver.cpp:466] Iteration 43000, Testing net (#0)
I0704 07:35:29.946631 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8961
I0704 07:35:29.946650 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9943
I0704 07:35:29.946655 22258 solver.cpp:539]     Test net output #2: loss = 0.3258 (* 1 = 0.3258 loss)
I0704 07:35:29.966506 22258 solver.cpp:290] Iteration 43000 (26.9995 iter/s, 3.70378s/100 iter), loss = -4.26173e-06
I0704 07:35:29.966523 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:29.966533 22258 sgd_solver.cpp:106] Iteration 43000, lr = 0.0328125
I0704 07:35:32.025400 22258 solver.cpp:290] Iteration 43100 (48.5717 iter/s, 2.05881s/100 iter), loss = -4.26173e-06
I0704 07:35:32.025425 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:32.025432 22258 sgd_solver.cpp:106] Iteration 43100, lr = 0.0326563
I0704 07:35:34.120672 22258 solver.cpp:290] Iteration 43200 (47.7286 iter/s, 2.09518s/100 iter), loss = -4.26173e-06
I0704 07:35:34.120699 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:34.120708 22258 sgd_solver.cpp:106] Iteration 43200, lr = 0.0325
I0704 07:35:36.178181 22258 solver.cpp:290] Iteration 43300 (48.6045 iter/s, 2.05742s/100 iter), loss = 0.0476148
I0704 07:35:36.178203 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:35:36.178211 22258 sgd_solver.cpp:106] Iteration 43300, lr = 0.0323438
I0704 07:35:38.236140 22258 solver.cpp:290] Iteration 43400 (48.5939 iter/s, 2.05787s/100 iter), loss = 0.0476148
I0704 07:35:38.236162 22258 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:35:38.236169 22258 sgd_solver.cpp:106] Iteration 43400, lr = 0.0321875
I0704 07:35:40.296151 22258 solver.cpp:290] Iteration 43500 (48.5454 iter/s, 2.05993s/100 iter), loss = -4.26173e-06
I0704 07:35:40.296190 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:40.296197 22258 sgd_solver.cpp:106] Iteration 43500, lr = 0.0320312
I0704 07:35:42.351068 22258 solver.cpp:290] Iteration 43600 (48.6662 iter/s, 2.05482s/100 iter), loss = -4.26173e-06
I0704 07:35:42.351119 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:42.351127 22258 sgd_solver.cpp:106] Iteration 43600, lr = 0.031875
I0704 07:35:44.408128 22258 solver.cpp:290] Iteration 43700 (48.6157 iter/s, 2.05695s/100 iter), loss = -4.26173e-06
I0704 07:35:44.408152 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:44.408160 22258 sgd_solver.cpp:106] Iteration 43700, lr = 0.0317187
I0704 07:35:46.470974 22258 solver.cpp:290] Iteration 43800 (48.4788 iter/s, 2.06276s/100 iter), loss = -4.26173e-06
I0704 07:35:46.470999 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:46.471005 22258 sgd_solver.cpp:106] Iteration 43800, lr = 0.0315625
I0704 07:35:48.532533 22258 solver.cpp:290] Iteration 43900 (48.509 iter/s, 2.06147s/100 iter), loss = -4.26173e-06
I0704 07:35:48.532555 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:48.532562 22258 sgd_solver.cpp:106] Iteration 43900, lr = 0.0314062
I0704 07:35:50.567706 22258 solver.cpp:466] Iteration 44000, Testing net (#0)
I0704 07:35:52.214431 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8771
I0704 07:35:52.214450 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9944
I0704 07:35:52.214455 22258 solver.cpp:539]     Test net output #2: loss = 0.4064 (* 1 = 0.4064 loss)
I0704 07:35:52.234333 22258 solver.cpp:290] Iteration 44000 (27.0148 iter/s, 3.70167s/100 iter), loss = -4.26173e-06
I0704 07:35:52.234349 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:52.234361 22258 sgd_solver.cpp:106] Iteration 44000, lr = 0.03125
I0704 07:35:54.290459 22258 solver.cpp:290] Iteration 44100 (48.637 iter/s, 2.05605s/100 iter), loss = -4.26173e-06
I0704 07:35:54.290483 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:54.290491 22258 sgd_solver.cpp:106] Iteration 44100, lr = 0.0310938
I0704 07:35:56.349622 22258 solver.cpp:290] Iteration 44200 (48.5655 iter/s, 2.05908s/100 iter), loss = -4.26173e-06
I0704 07:35:56.349644 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:56.349650 22258 sgd_solver.cpp:106] Iteration 44200, lr = 0.0309375
I0704 07:35:58.413172 22258 solver.cpp:290] Iteration 44300 (48.4623 iter/s, 2.06346s/100 iter), loss = -4.26173e-06
I0704 07:35:58.413195 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:35:58.413203 22258 sgd_solver.cpp:106] Iteration 44300, lr = 0.0307813
I0704 07:36:00.476979 22258 solver.cpp:290] Iteration 44400 (48.4562 iter/s, 2.06372s/100 iter), loss = -4.26173e-06
I0704 07:36:00.477010 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:00.477016 22258 sgd_solver.cpp:106] Iteration 44400, lr = 0.030625
I0704 07:36:02.539607 22258 solver.cpp:290] Iteration 44500 (48.484 iter/s, 2.06254s/100 iter), loss = -4.26173e-06
I0704 07:36:02.539628 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:02.539634 22258 sgd_solver.cpp:106] Iteration 44500, lr = 0.0304688
I0704 07:36:04.602802 22258 solver.cpp:290] Iteration 44600 (48.4705 iter/s, 2.06311s/100 iter), loss = -4.26173e-06
I0704 07:36:04.602825 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:04.602833 22258 sgd_solver.cpp:106] Iteration 44600, lr = 0.0303125
I0704 07:36:06.659860 22258 solver.cpp:290] Iteration 44700 (48.6151 iter/s, 2.05697s/100 iter), loss = -4.26173e-06
I0704 07:36:06.659883 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:06.659890 22258 sgd_solver.cpp:106] Iteration 44700, lr = 0.0301562
I0704 07:36:08.716002 22258 solver.cpp:290] Iteration 44800 (48.6368 iter/s, 2.05606s/100 iter), loss = -4.26173e-06
I0704 07:36:08.716024 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:08.716032 22258 sgd_solver.cpp:106] Iteration 44800, lr = 0.03
I0704 07:36:10.772183 22258 solver.cpp:290] Iteration 44900 (48.6359 iter/s, 2.0561s/100 iter), loss = -4.26173e-06
I0704 07:36:10.772222 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:10.772228 22258 sgd_solver.cpp:106] Iteration 44900, lr = 0.0298437
I0704 07:36:12.811127 22258 solver.cpp:466] Iteration 45000, Testing net (#0)
I0704 07:36:14.455199 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8964
I0704 07:36:14.455219 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9968
I0704 07:36:14.455224 22258 solver.cpp:539]     Test net output #2: loss = 0.3096 (* 1 = 0.3096 loss)
I0704 07:36:14.474795 22258 solver.cpp:290] Iteration 45000 (27.009 iter/s, 3.70247s/100 iter), loss = -4.26173e-06
I0704 07:36:14.474812 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:14.474825 22258 sgd_solver.cpp:106] Iteration 45000, lr = 0.0296875
I0704 07:36:16.532634 22258 solver.cpp:290] Iteration 45100 (48.5966 iter/s, 2.05776s/100 iter), loss = -4.26173e-06
I0704 07:36:16.532656 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:16.532665 22258 sgd_solver.cpp:106] Iteration 45100, lr = 0.0295313
I0704 07:36:18.591634 22258 solver.cpp:290] Iteration 45200 (48.5693 iter/s, 2.05891s/100 iter), loss = -4.26173e-06
I0704 07:36:18.591658 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:18.591667 22258 sgd_solver.cpp:106] Iteration 45200, lr = 0.029375
I0704 07:36:20.647927 22258 solver.cpp:290] Iteration 45300 (48.6333 iter/s, 2.05621s/100 iter), loss = -4.26173e-06
I0704 07:36:20.647950 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:20.647959 22258 sgd_solver.cpp:106] Iteration 45300, lr = 0.0292188
I0704 07:36:22.707609 22258 solver.cpp:290] Iteration 45400 (48.5532 iter/s, 2.0596s/100 iter), loss = -4.26173e-06
I0704 07:36:22.707633 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:22.707638 22258 sgd_solver.cpp:106] Iteration 45400, lr = 0.0290625
I0704 07:36:24.766050 22258 solver.cpp:290] Iteration 45500 (48.5825 iter/s, 2.05835s/100 iter), loss = -4.26173e-06
I0704 07:36:24.766072 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:24.766080 22258 sgd_solver.cpp:106] Iteration 45500, lr = 0.0289063
I0704 07:36:26.823823 22258 solver.cpp:290] Iteration 45600 (48.5983 iter/s, 2.05769s/100 iter), loss = -4.26173e-06
I0704 07:36:26.823846 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:26.823853 22258 sgd_solver.cpp:106] Iteration 45600, lr = 0.02875
I0704 07:36:28.879663 22258 solver.cpp:290] Iteration 45700 (48.644 iter/s, 2.05575s/100 iter), loss = -4.26173e-06
I0704 07:36:28.879688 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:28.879698 22258 sgd_solver.cpp:106] Iteration 45700, lr = 0.0285937
I0704 07:36:30.938555 22258 solver.cpp:290] Iteration 45800 (48.572 iter/s, 2.0588s/100 iter), loss = -4.26173e-06
I0704 07:36:30.938582 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:30.938591 22258 sgd_solver.cpp:106] Iteration 45800, lr = 0.0284375
I0704 07:36:32.999742 22258 solver.cpp:290] Iteration 45900 (48.5179 iter/s, 2.0611s/100 iter), loss = -4.26173e-06
I0704 07:36:32.999763 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:32.999769 22258 sgd_solver.cpp:106] Iteration 45900, lr = 0.0282812
I0704 07:36:35.061331 22258 solver.cpp:466] Iteration 46000, Testing net (#0)
I0704 07:36:36.710063 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9055
I0704 07:36:36.710081 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9976
I0704 07:36:36.710086 22258 solver.cpp:539]     Test net output #2: loss = 0.2594 (* 1 = 0.2594 loss)
I0704 07:36:36.729749 22258 solver.cpp:290] Iteration 46000 (26.8105 iter/s, 3.72988s/100 iter), loss = -4.26173e-06
I0704 07:36:36.729768 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:36.729780 22258 sgd_solver.cpp:106] Iteration 46000, lr = 0.028125
I0704 07:36:38.792208 22258 solver.cpp:290] Iteration 46100 (48.4878 iter/s, 2.06238s/100 iter), loss = -4.26173e-06
I0704 07:36:38.792232 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:38.792237 22258 sgd_solver.cpp:106] Iteration 46100, lr = 0.0279688
I0704 07:36:40.848083 22258 solver.cpp:290] Iteration 46200 (48.6431 iter/s, 2.05579s/100 iter), loss = -4.26173e-06
I0704 07:36:40.848104 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:40.848112 22258 sgd_solver.cpp:106] Iteration 46200, lr = 0.0278125
I0704 07:36:42.906352 22258 solver.cpp:290] Iteration 46300 (48.5865 iter/s, 2.05818s/100 iter), loss = -4.26173e-06
I0704 07:36:42.906422 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:42.906432 22258 sgd_solver.cpp:106] Iteration 46300, lr = 0.0276563
I0704 07:36:44.962581 22258 solver.cpp:290] Iteration 46400 (48.6358 iter/s, 2.0561s/100 iter), loss = -4.26173e-06
I0704 07:36:44.962604 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:44.962610 22258 sgd_solver.cpp:106] Iteration 46400, lr = 0.0275
I0704 07:36:47.020045 22258 solver.cpp:290] Iteration 46500 (48.6055 iter/s, 2.05738s/100 iter), loss = -4.26173e-06
I0704 07:36:47.020066 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:47.020074 22258 sgd_solver.cpp:106] Iteration 46500, lr = 0.0273438
I0704 07:36:49.078635 22258 solver.cpp:290] Iteration 46600 (48.5789 iter/s, 2.05851s/100 iter), loss = -4.26173e-06
I0704 07:36:49.078662 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:49.078671 22258 sgd_solver.cpp:106] Iteration 46600, lr = 0.0271875
I0704 07:36:51.135725 22258 solver.cpp:290] Iteration 46700 (48.6144 iter/s, 2.057s/100 iter), loss = -4.26173e-06
I0704 07:36:51.135747 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:51.135754 22258 sgd_solver.cpp:106] Iteration 46700, lr = 0.0270312
I0704 07:36:53.193702 22258 solver.cpp:290] Iteration 46800 (48.5935 iter/s, 2.05789s/100 iter), loss = -4.26173e-06
I0704 07:36:53.193732 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:53.193740 22258 sgd_solver.cpp:106] Iteration 46800, lr = 0.026875
I0704 07:36:55.252248 22258 solver.cpp:290] Iteration 46900 (48.5802 iter/s, 2.05845s/100 iter), loss = -4.26173e-06
I0704 07:36:55.252274 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:55.252282 22258 sgd_solver.cpp:106] Iteration 46900, lr = 0.0267187
I0704 07:36:57.287852 22258 solver.cpp:466] Iteration 47000, Testing net (#0)
I0704 07:36:58.934201 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9061
I0704 07:36:58.934221 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9962
I0704 07:36:58.934226 22258 solver.cpp:539]     Test net output #2: loss = 0.2588 (* 1 = 0.2588 loss)
I0704 07:36:58.954228 22258 solver.cpp:290] Iteration 47000 (27.0135 iter/s, 3.70185s/100 iter), loss = -4.26173e-06
I0704 07:36:58.954246 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:36:58.954258 22258 sgd_solver.cpp:106] Iteration 47000, lr = 0.0265625
I0704 07:37:01.014870 22258 solver.cpp:290] Iteration 47100 (48.5305 iter/s, 2.06056s/100 iter), loss = -4.26173e-06
I0704 07:37:01.014894 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:01.014900 22258 sgd_solver.cpp:106] Iteration 47100, lr = 0.0264063
I0704 07:37:03.078799 22258 solver.cpp:290] Iteration 47200 (48.4533 iter/s, 2.06384s/100 iter), loss = -4.26173e-06
I0704 07:37:03.078821 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:03.078829 22258 sgd_solver.cpp:106] Iteration 47200, lr = 0.02625
I0704 07:37:05.136745 22258 solver.cpp:290] Iteration 47300 (48.5941 iter/s, 2.05786s/100 iter), loss = -4.26173e-06
I0704 07:37:05.136768 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:05.136775 22258 sgd_solver.cpp:106] Iteration 47300, lr = 0.0260938
I0704 07:37:07.195950 22258 solver.cpp:290] Iteration 47400 (48.5645 iter/s, 2.05912s/100 iter), loss = -4.26173e-06
I0704 07:37:07.195972 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:07.195979 22258 sgd_solver.cpp:106] Iteration 47400, lr = 0.0259375
I0704 07:37:09.252172 22258 solver.cpp:290] Iteration 47500 (48.6349 iter/s, 2.05614s/100 iter), loss = -4.26173e-06
I0704 07:37:09.252194 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:09.252200 22258 sgd_solver.cpp:106] Iteration 47500, lr = 0.0257812
I0704 07:37:11.308356 22258 solver.cpp:290] Iteration 47600 (48.6358 iter/s, 2.0561s/100 iter), loss = -4.26173e-06
I0704 07:37:11.308393 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:11.308399 22258 sgd_solver.cpp:106] Iteration 47600, lr = 0.025625
I0704 07:37:13.370016 22258 solver.cpp:290] Iteration 47700 (48.5069 iter/s, 2.06156s/100 iter), loss = -4.26173e-06
I0704 07:37:13.370105 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:13.370113 22258 sgd_solver.cpp:106] Iteration 47700, lr = 0.0254687
I0704 07:37:15.430119 22258 solver.cpp:290] Iteration 47800 (48.5448 iter/s, 2.05995s/100 iter), loss = -4.26173e-06
I0704 07:37:15.430142 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:15.430148 22258 sgd_solver.cpp:106] Iteration 47800, lr = 0.0253125
I0704 07:37:17.487715 22258 solver.cpp:290] Iteration 47900 (48.6024 iter/s, 2.05751s/100 iter), loss = -4.26173e-06
I0704 07:37:17.487737 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:17.487745 22258 sgd_solver.cpp:106] Iteration 47900, lr = 0.0251562
I0704 07:37:19.530753 22258 solver.cpp:466] Iteration 48000, Testing net (#0)
I0704 07:37:21.175889 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9084
I0704 07:37:21.175907 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9967
I0704 07:37:21.175912 22258 solver.cpp:539]     Test net output #2: loss = 0.2529 (* 1 = 0.2529 loss)
I0704 07:37:21.195749 22258 solver.cpp:290] Iteration 48000 (26.9694 iter/s, 3.70791s/100 iter), loss = -4.26173e-06
I0704 07:37:21.195767 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:21.195780 22258 sgd_solver.cpp:106] Iteration 48000, lr = 0.025
I0704 07:37:23.250564 22258 solver.cpp:290] Iteration 48100 (48.6681 iter/s, 2.05473s/100 iter), loss = -4.26173e-06
I0704 07:37:23.250588 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:23.250594 22258 sgd_solver.cpp:106] Iteration 48100, lr = 0.0248438
I0704 07:37:25.312463 22258 solver.cpp:290] Iteration 48200 (48.5011 iter/s, 2.06181s/100 iter), loss = -4.26173e-06
I0704 07:37:25.312489 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:25.312496 22258 sgd_solver.cpp:106] Iteration 48200, lr = 0.0246875
I0704 07:37:27.368885 22258 solver.cpp:290] Iteration 48300 (48.6302 iter/s, 2.05633s/100 iter), loss = -4.26173e-06
I0704 07:37:27.368908 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:27.368916 22258 sgd_solver.cpp:106] Iteration 48300, lr = 0.0245313
I0704 07:37:29.427399 22258 solver.cpp:290] Iteration 48400 (48.5808 iter/s, 2.05843s/100 iter), loss = -4.26173e-06
I0704 07:37:29.427433 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:29.427443 22258 sgd_solver.cpp:106] Iteration 48400, lr = 0.024375
I0704 07:37:31.482827 22258 solver.cpp:290] Iteration 48500 (48.6539 iter/s, 2.05533s/100 iter), loss = -4.26173e-06
I0704 07:37:31.482861 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:31.482872 22258 sgd_solver.cpp:106] Iteration 48500, lr = 0.0242188
I0704 07:37:33.565243 22258 solver.cpp:290] Iteration 48600 (48.0233 iter/s, 2.08232s/100 iter), loss = -4.26173e-06
I0704 07:37:33.565266 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:33.565273 22258 sgd_solver.cpp:106] Iteration 48600, lr = 0.0240625
I0704 07:37:35.623999 22258 solver.cpp:290] Iteration 48700 (48.575 iter/s, 2.05867s/100 iter), loss = -4.26173e-06
I0704 07:37:35.624024 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:35.624033 22258 sgd_solver.cpp:106] Iteration 48700, lr = 0.0239062
I0704 07:37:37.682130 22258 solver.cpp:290] Iteration 48800 (48.5898 iter/s, 2.05804s/100 iter), loss = -4.26173e-06
I0704 07:37:37.682153 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:37.682159 22258 sgd_solver.cpp:106] Iteration 48800, lr = 0.02375
I0704 07:37:39.745856 22258 solver.cpp:290] Iteration 48900 (48.4581 iter/s, 2.06364s/100 iter), loss = -4.26173e-06
I0704 07:37:39.745879 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:39.745885 22258 sgd_solver.cpp:106] Iteration 48900, lr = 0.0235937
I0704 07:37:41.782372 22258 solver.cpp:466] Iteration 49000, Testing net (#0)
I0704 07:37:43.427295 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9147
I0704 07:37:43.427386 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9975
I0704 07:37:43.427392 22258 solver.cpp:539]     Test net output #2: loss = 0.2368 (* 1 = 0.2368 loss)
I0704 07:37:43.446909 22258 solver.cpp:290] Iteration 49000 (27.0203 iter/s, 3.70093s/100 iter), loss = -4.26173e-06
I0704 07:37:43.446925 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:43.446938 22258 sgd_solver.cpp:106] Iteration 49000, lr = 0.0234375
I0704 07:37:45.503232 22258 solver.cpp:290] Iteration 49100 (48.6324 iter/s, 2.05624s/100 iter), loss = -4.26173e-06
I0704 07:37:45.503255 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:45.503262 22258 sgd_solver.cpp:106] Iteration 49100, lr = 0.0232813
I0704 07:37:47.562263 22258 solver.cpp:290] Iteration 49200 (48.5686 iter/s, 2.05894s/100 iter), loss = -4.26173e-06
I0704 07:37:47.562288 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:47.562295 22258 sgd_solver.cpp:106] Iteration 49200, lr = 0.023125
I0704 07:37:49.619388 22258 solver.cpp:290] Iteration 49300 (48.6136 iter/s, 2.05704s/100 iter), loss = -4.26173e-06
I0704 07:37:49.619411 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:49.619417 22258 sgd_solver.cpp:106] Iteration 49300, lr = 0.0229688
I0704 07:37:51.681527 22258 solver.cpp:290] Iteration 49400 (48.4954 iter/s, 2.06205s/100 iter), loss = -4.26173e-06
I0704 07:37:51.681550 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:51.681558 22258 sgd_solver.cpp:106] Iteration 49400, lr = 0.0228125
I0704 07:37:53.740841 22258 solver.cpp:290] Iteration 49500 (48.5619 iter/s, 2.05923s/100 iter), loss = -4.26173e-06
I0704 07:37:53.740864 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:53.740871 22258 sgd_solver.cpp:106] Iteration 49500, lr = 0.0226563
I0704 07:37:55.799716 22258 solver.cpp:290] Iteration 49600 (48.5722 iter/s, 2.05879s/100 iter), loss = -4.26173e-06
I0704 07:37:55.799739 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:55.799746 22258 sgd_solver.cpp:106] Iteration 49600, lr = 0.0225
I0704 07:37:57.857655 22258 solver.cpp:290] Iteration 49700 (48.5943 iter/s, 2.05785s/100 iter), loss = -4.26173e-06
I0704 07:37:57.857678 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:57.857686 22258 sgd_solver.cpp:106] Iteration 49700, lr = 0.0223437
I0704 07:37:59.915313 22258 solver.cpp:290] Iteration 49800 (48.601 iter/s, 2.05757s/100 iter), loss = -4.26173e-06
I0704 07:37:59.915335 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:37:59.915341 22258 sgd_solver.cpp:106] Iteration 49800, lr = 0.0221875
I0704 07:38:01.972905 22258 solver.cpp:290] Iteration 49900 (48.6025 iter/s, 2.05751s/100 iter), loss = -4.26173e-06
I0704 07:38:01.972929 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:01.972936 22258 sgd_solver.cpp:106] Iteration 49900, lr = 0.0220312
I0704 07:38:04.010587 22258 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/cifar10_jacintonet11v2_iter_50000.caffemodel
I0704 07:38:04.027418 22258 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/cifar10_jacintonet11v2_iter_50000.solverstate
I0704 07:38:04.034898 22258 solver.cpp:466] Iteration 50000, Testing net (#0)
I0704 07:38:05.686525 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9147
I0704 07:38:05.686545 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9973
I0704 07:38:05.686550 22258 solver.cpp:539]     Test net output #2: loss = 0.2317 (* 1 = 0.2317 loss)
I0704 07:38:05.706647 22258 solver.cpp:290] Iteration 50000 (26.7837 iter/s, 3.73361s/100 iter), loss = -4.26173e-06
I0704 07:38:05.706673 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:05.706681 22258 sgd_solver.cpp:106] Iteration 50000, lr = 0.021875
I0704 07:38:07.766191 22258 solver.cpp:290] Iteration 50100 (48.5565 iter/s, 2.05946s/100 iter), loss = -4.26173e-06
I0704 07:38:07.766213 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:07.766221 22258 sgd_solver.cpp:106] Iteration 50100, lr = 0.0217188
I0704 07:38:09.822051 22258 solver.cpp:290] Iteration 50200 (48.6435 iter/s, 2.05577s/100 iter), loss = -4.26173e-06
I0704 07:38:09.822073 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:09.822079 22258 sgd_solver.cpp:106] Iteration 50200, lr = 0.0215625
I0704 07:38:11.880060 22258 solver.cpp:290] Iteration 50300 (48.5927 iter/s, 2.05792s/100 iter), loss = -4.26173e-06
I0704 07:38:11.880084 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:11.880089 22258 sgd_solver.cpp:106] Iteration 50300, lr = 0.0214063
I0704 07:38:13.939748 22258 solver.cpp:290] Iteration 50400 (48.5531 iter/s, 2.0596s/100 iter), loss = -4.26173e-06
I0704 07:38:13.939831 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:13.939841 22258 sgd_solver.cpp:106] Iteration 50400, lr = 0.02125
I0704 07:38:15.994006 22258 solver.cpp:290] Iteration 50500 (48.6828 iter/s, 2.05411s/100 iter), loss = -4.26173e-06
I0704 07:38:15.994029 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:15.994035 22258 sgd_solver.cpp:106] Iteration 50500, lr = 0.0210938
I0704 07:38:18.050192 22258 solver.cpp:290] Iteration 50600 (48.6357 iter/s, 2.0561s/100 iter), loss = -4.26173e-06
I0704 07:38:18.050215 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:18.050223 22258 sgd_solver.cpp:106] Iteration 50600, lr = 0.0209375
I0704 07:38:20.107125 22258 solver.cpp:290] Iteration 50700 (48.6181 iter/s, 2.05685s/100 iter), loss = -4.26173e-06
I0704 07:38:20.107146 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:20.107153 22258 sgd_solver.cpp:106] Iteration 50700, lr = 0.0207812
I0704 07:38:22.165683 22258 solver.cpp:290] Iteration 50800 (48.5797 iter/s, 2.05847s/100 iter), loss = -4.26173e-06
I0704 07:38:22.165707 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:22.165715 22258 sgd_solver.cpp:106] Iteration 50800, lr = 0.020625
I0704 07:38:24.221673 22258 solver.cpp:290] Iteration 50900 (48.6404 iter/s, 2.0559s/100 iter), loss = -4.26173e-06
I0704 07:38:24.221698 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:24.221706 22258 sgd_solver.cpp:106] Iteration 50900, lr = 0.0204687
I0704 07:38:26.262132 22258 solver.cpp:466] Iteration 51000, Testing net (#0)
I0704 07:38:27.907512 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9157
I0704 07:38:27.907532 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9975
I0704 07:38:27.907537 22258 solver.cpp:539]     Test net output #2: loss = 0.2279 (* 1 = 0.2279 loss)
I0704 07:38:27.927110 22258 solver.cpp:290] Iteration 51000 (26.9883 iter/s, 3.70531s/100 iter), loss = -4.26173e-06
I0704 07:38:27.927127 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:27.927141 22258 sgd_solver.cpp:106] Iteration 51000, lr = 0.0203125
I0704 07:38:29.989799 22258 solver.cpp:290] Iteration 51100 (48.4823 iter/s, 2.06261s/100 iter), loss = -4.26173e-06
I0704 07:38:29.989821 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:29.989828 22258 sgd_solver.cpp:106] Iteration 51100, lr = 0.0201563
I0704 07:38:32.048707 22258 solver.cpp:290] Iteration 51200 (48.5714 iter/s, 2.05882s/100 iter), loss = -4.26173e-06
I0704 07:38:32.048730 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:32.048738 22258 sgd_solver.cpp:106] Iteration 51200, lr = 0.02
I0704 07:38:34.129899 22258 solver.cpp:290] Iteration 51300 (48.0514 iter/s, 2.0811s/100 iter), loss = -4.26173e-06
I0704 07:38:34.129920 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:34.129927 22258 sgd_solver.cpp:106] Iteration 51300, lr = 0.0198438
I0704 07:38:36.188789 22258 solver.cpp:290] Iteration 51400 (48.5719 iter/s, 2.0588s/100 iter), loss = -4.26173e-06
I0704 07:38:36.188812 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:36.188818 22258 sgd_solver.cpp:106] Iteration 51400, lr = 0.0196875
I0704 07:38:38.250090 22258 solver.cpp:290] Iteration 51500 (48.5151 iter/s, 2.06121s/100 iter), loss = -4.26173e-06
I0704 07:38:38.250116 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:38.250124 22258 sgd_solver.cpp:106] Iteration 51500, lr = 0.0195312
I0704 07:38:40.307937 22258 solver.cpp:290] Iteration 51600 (48.5966 iter/s, 2.05776s/100 iter), loss = -4.26173e-06
I0704 07:38:40.307960 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:40.307965 22258 sgd_solver.cpp:106] Iteration 51600, lr = 0.019375
I0704 07:38:42.363989 22258 solver.cpp:290] Iteration 51700 (48.6389 iter/s, 2.05597s/100 iter), loss = -4.26173e-06
I0704 07:38:42.364028 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:42.364034 22258 sgd_solver.cpp:106] Iteration 51700, lr = 0.0192187
I0704 07:38:44.421344 22258 solver.cpp:290] Iteration 51800 (48.6085 iter/s, 2.05725s/100 iter), loss = -4.26173e-06
I0704 07:38:44.421411 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:44.421418 22258 sgd_solver.cpp:106] Iteration 51800, lr = 0.0190625
I0704 07:38:46.478930 22258 solver.cpp:290] Iteration 51900 (48.6037 iter/s, 2.05746s/100 iter), loss = -4.26173e-06
I0704 07:38:46.478952 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:46.478960 22258 sgd_solver.cpp:106] Iteration 51900, lr = 0.0189062
I0704 07:38:48.516233 22258 solver.cpp:466] Iteration 52000, Testing net (#0)
I0704 07:38:50.160106 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9154
I0704 07:38:50.160125 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9976
I0704 07:38:50.160131 22258 solver.cpp:539]     Test net output #2: loss = 0.2163 (* 1 = 0.2163 loss)
I0704 07:38:50.179814 22258 solver.cpp:290] Iteration 52000 (27.0215 iter/s, 3.70076s/100 iter), loss = -4.26173e-06
I0704 07:38:50.179831 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:50.179846 22258 sgd_solver.cpp:106] Iteration 52000, lr = 0.01875
I0704 07:38:52.239774 22258 solver.cpp:290] Iteration 52100 (48.5465 iter/s, 2.05988s/100 iter), loss = -4.26173e-06
I0704 07:38:52.239799 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:52.239804 22258 sgd_solver.cpp:106] Iteration 52100, lr = 0.0185938
I0704 07:38:54.300403 22258 solver.cpp:290] Iteration 52200 (48.5309 iter/s, 2.06054s/100 iter), loss = -4.26173e-06
I0704 07:38:54.300427 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:54.300434 22258 sgd_solver.cpp:106] Iteration 52200, lr = 0.0184375
I0704 07:38:56.355777 22258 solver.cpp:290] Iteration 52300 (48.6551 iter/s, 2.05528s/100 iter), loss = -4.26173e-06
I0704 07:38:56.355803 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:56.355811 22258 sgd_solver.cpp:106] Iteration 52300, lr = 0.0182813
I0704 07:38:58.415343 22258 solver.cpp:290] Iteration 52400 (48.5559 iter/s, 2.05948s/100 iter), loss = -4.26173e-06
I0704 07:38:58.415367 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:38:58.415374 22258 sgd_solver.cpp:106] Iteration 52400, lr = 0.018125
I0704 07:39:00.475128 22258 solver.cpp:290] Iteration 52500 (48.5508 iter/s, 2.0597s/100 iter), loss = -4.26173e-06
I0704 07:39:00.475152 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:00.475158 22258 sgd_solver.cpp:106] Iteration 52500, lr = 0.0179687
I0704 07:39:02.534900 22258 solver.cpp:290] Iteration 52600 (48.5511 iter/s, 2.05968s/100 iter), loss = -4.26173e-06
I0704 07:39:02.534924 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:02.534929 22258 sgd_solver.cpp:106] Iteration 52600, lr = 0.0178125
I0704 07:39:04.591455 22258 solver.cpp:290] Iteration 52700 (48.6271 iter/s, 2.05647s/100 iter), loss = -4.26173e-06
I0704 07:39:04.591480 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:04.591485 22258 sgd_solver.cpp:106] Iteration 52700, lr = 0.0176562
I0704 07:39:06.649933 22258 solver.cpp:290] Iteration 52800 (48.5816 iter/s, 2.05839s/100 iter), loss = -4.26173e-06
I0704 07:39:06.649956 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:06.649963 22258 sgd_solver.cpp:106] Iteration 52800, lr = 0.0175
I0704 07:39:08.708201 22258 solver.cpp:290] Iteration 52900 (48.5866 iter/s, 2.05818s/100 iter), loss = -4.26173e-06
I0704 07:39:08.708228 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:08.708237 22258 sgd_solver.cpp:106] Iteration 52900, lr = 0.0173437
I0704 07:39:10.752503 22258 solver.cpp:466] Iteration 53000, Testing net (#0)
I0704 07:39:12.396705 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.918
I0704 07:39:12.396725 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9972
I0704 07:39:12.396731 22258 solver.cpp:539]     Test net output #2: loss = 0.2204 (* 1 = 0.2204 loss)
I0704 07:39:12.416335 22258 solver.cpp:290] Iteration 53000 (26.9687 iter/s, 3.708s/100 iter), loss = -4.26173e-06
I0704 07:39:12.416364 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:12.416371 22258 sgd_solver.cpp:106] Iteration 53000, lr = 0.0171875
I0704 07:39:14.475561 22258 solver.cpp:290] Iteration 53100 (48.5641 iter/s, 2.05913s/100 iter), loss = -4.26173e-06
I0704 07:39:14.475638 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:14.475646 22258 sgd_solver.cpp:106] Iteration 53100, lr = 0.0170313
I0704 07:39:16.533246 22258 solver.cpp:290] Iteration 53200 (48.6016 iter/s, 2.05755s/100 iter), loss = -4.26173e-06
I0704 07:39:16.533269 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:16.533275 22258 sgd_solver.cpp:106] Iteration 53200, lr = 0.016875
I0704 07:39:18.590282 22258 solver.cpp:290] Iteration 53300 (48.6157 iter/s, 2.05695s/100 iter), loss = -4.26173e-06
I0704 07:39:18.590306 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:18.590312 22258 sgd_solver.cpp:106] Iteration 53300, lr = 0.0167188
I0704 07:39:20.647150 22258 solver.cpp:290] Iteration 53400 (48.6196 iter/s, 2.05678s/100 iter), loss = -4.26173e-06
I0704 07:39:20.647172 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:20.647178 22258 sgd_solver.cpp:106] Iteration 53400, lr = 0.0165625
I0704 07:39:22.704097 22258 solver.cpp:290] Iteration 53500 (48.6179 iter/s, 2.05686s/100 iter), loss = -4.26173e-06
I0704 07:39:22.704123 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:22.704133 22258 sgd_solver.cpp:106] Iteration 53500, lr = 0.0164063
I0704 07:39:24.761752 22258 solver.cpp:290] Iteration 53600 (48.6011 iter/s, 2.05756s/100 iter), loss = -4.26173e-06
I0704 07:39:24.761780 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:24.761790 22258 sgd_solver.cpp:106] Iteration 53600, lr = 0.01625
I0704 07:39:26.820319 22258 solver.cpp:290] Iteration 53700 (48.5796 iter/s, 2.05848s/100 iter), loss = -4.26173e-06
I0704 07:39:26.820348 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:26.820355 22258 sgd_solver.cpp:106] Iteration 53700, lr = 0.0160937
I0704 07:39:28.875638 22258 solver.cpp:290] Iteration 53800 (48.6564 iter/s, 2.05523s/100 iter), loss = -4.26173e-06
I0704 07:39:28.875660 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:28.875668 22258 sgd_solver.cpp:106] Iteration 53800, lr = 0.0159375
I0704 07:39:30.933946 22258 solver.cpp:290] Iteration 53900 (48.5856 iter/s, 2.05822s/100 iter), loss = -4.26173e-06
I0704 07:39:30.933969 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:30.933975 22258 sgd_solver.cpp:106] Iteration 53900, lr = 0.0157812
I0704 07:39:32.972805 22258 solver.cpp:466] Iteration 54000, Testing net (#0)
I0704 07:39:34.617648 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9178
I0704 07:39:34.617668 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9973
I0704 07:39:34.617673 22258 solver.cpp:539]     Test net output #2: loss = 0.211 (* 1 = 0.211 loss)
I0704 07:39:34.637291 22258 solver.cpp:290] Iteration 54000 (27.0036 iter/s, 3.70322s/100 iter), loss = -4.26173e-06
I0704 07:39:34.637311 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:34.637320 22258 sgd_solver.cpp:106] Iteration 54000, lr = 0.015625
I0704 07:39:36.701212 22258 solver.cpp:290] Iteration 54100 (48.4535 iter/s, 2.06384s/100 iter), loss = -4.26173e-06
I0704 07:39:36.701236 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:36.701241 22258 sgd_solver.cpp:106] Iteration 54100, lr = 0.0154688
I0704 07:39:38.758759 22258 solver.cpp:290] Iteration 54200 (48.6036 iter/s, 2.05746s/100 iter), loss = -4.26173e-06
I0704 07:39:38.758780 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:38.758787 22258 sgd_solver.cpp:106] Iteration 54200, lr = 0.0153125
I0704 07:39:40.817674 22258 solver.cpp:290] Iteration 54300 (48.5713 iter/s, 2.05883s/100 iter), loss = -4.26173e-06
I0704 07:39:40.817698 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:40.817703 22258 sgd_solver.cpp:106] Iteration 54300, lr = 0.0151563
I0704 07:39:42.875064 22258 solver.cpp:290] Iteration 54400 (48.6073 iter/s, 2.0573s/100 iter), loss = -4.26173e-06
I0704 07:39:42.875105 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:42.875113 22258 sgd_solver.cpp:106] Iteration 54400, lr = 0.015
I0704 07:39:44.934171 22258 solver.cpp:290] Iteration 54500 (48.5672 iter/s, 2.059s/100 iter), loss = -4.26173e-06
I0704 07:39:44.934243 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:44.934252 22258 sgd_solver.cpp:106] Iteration 54500, lr = 0.0148437
I0704 07:39:46.991870 22258 solver.cpp:290] Iteration 54600 (48.6011 iter/s, 2.05757s/100 iter), loss = -4.26173e-06
I0704 07:39:46.991897 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:46.991906 22258 sgd_solver.cpp:106] Iteration 54600, lr = 0.0146875
I0704 07:39:49.051126 22258 solver.cpp:290] Iteration 54700 (48.5633 iter/s, 2.05917s/100 iter), loss = -4.26173e-06
I0704 07:39:49.051149 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:49.051158 22258 sgd_solver.cpp:106] Iteration 54700, lr = 0.0145312
I0704 07:39:51.106485 22258 solver.cpp:290] Iteration 54800 (48.6553 iter/s, 2.05527s/100 iter), loss = -4.26173e-06
I0704 07:39:51.106508 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:51.106514 22258 sgd_solver.cpp:106] Iteration 54800, lr = 0.014375
I0704 07:39:53.168742 22258 solver.cpp:290] Iteration 54900 (48.4926 iter/s, 2.06217s/100 iter), loss = -4.26173e-06
I0704 07:39:53.168766 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:53.168773 22258 sgd_solver.cpp:106] Iteration 54900, lr = 0.0142187
I0704 07:39:55.206797 22258 solver.cpp:466] Iteration 55000, Testing net (#0)
I0704 07:39:56.861954 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9158
I0704 07:39:56.861974 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9973
I0704 07:39:56.861979 22258 solver.cpp:539]     Test net output #2: loss = 0.2116 (* 1 = 0.2116 loss)
I0704 07:39:56.882087 22258 solver.cpp:290] Iteration 55000 (26.9308 iter/s, 3.71321s/100 iter), loss = -4.26173e-06
I0704 07:39:56.882108 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:56.882117 22258 sgd_solver.cpp:106] Iteration 55000, lr = 0.0140625
I0704 07:39:58.942793 22258 solver.cpp:290] Iteration 55100 (48.5291 iter/s, 2.06062s/100 iter), loss = -4.26173e-06
I0704 07:39:58.942821 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:39:58.942831 22258 sgd_solver.cpp:106] Iteration 55100, lr = 0.0139063
I0704 07:40:01.002266 22258 solver.cpp:290] Iteration 55200 (48.5582 iter/s, 2.05938s/100 iter), loss = -4.26173e-06
I0704 07:40:01.002293 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:01.002302 22258 sgd_solver.cpp:106] Iteration 55200, lr = 0.01375
I0704 07:40:03.059466 22258 solver.cpp:290] Iteration 55300 (48.6118 iter/s, 2.05711s/100 iter), loss = -4.26173e-06
I0704 07:40:03.059489 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:03.059495 22258 sgd_solver.cpp:106] Iteration 55300, lr = 0.0135938
I0704 07:40:05.118923 22258 solver.cpp:290] Iteration 55400 (48.5585 iter/s, 2.05937s/100 iter), loss = -4.26173e-06
I0704 07:40:05.118945 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:05.118952 22258 sgd_solver.cpp:106] Iteration 55400, lr = 0.0134375
I0704 07:40:07.175948 22258 solver.cpp:290] Iteration 55500 (48.6159 iter/s, 2.05694s/100 iter), loss = -4.26173e-06
I0704 07:40:07.175971 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:07.175978 22258 sgd_solver.cpp:106] Iteration 55500, lr = 0.0132813
I0704 07:40:09.235553 22258 solver.cpp:290] Iteration 55600 (48.5551 iter/s, 2.05951s/100 iter), loss = -4.26173e-06
I0704 07:40:09.235579 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:09.235587 22258 sgd_solver.cpp:106] Iteration 55600, lr = 0.013125
I0704 07:40:11.297719 22258 solver.cpp:290] Iteration 55700 (48.4947 iter/s, 2.06208s/100 iter), loss = -4.26173e-06
I0704 07:40:11.297741 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:11.297749 22258 sgd_solver.cpp:106] Iteration 55700, lr = 0.0129687
I0704 07:40:13.366673 22258 solver.cpp:290] Iteration 55800 (48.3356 iter/s, 2.06887s/100 iter), loss = -4.26173e-06
I0704 07:40:13.366706 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:13.366714 22258 sgd_solver.cpp:106] Iteration 55800, lr = 0.0128125
I0704 07:40:15.424494 22258 solver.cpp:290] Iteration 55900 (48.5974 iter/s, 2.05772s/100 iter), loss = -4.26173e-06
I0704 07:40:15.424572 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:15.424581 22258 sgd_solver.cpp:106] Iteration 55900, lr = 0.0126562
I0704 07:40:17.459738 22258 solver.cpp:466] Iteration 56000, Testing net (#0)
I0704 07:40:19.105350 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9177
I0704 07:40:19.105370 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9973
I0704 07:40:19.105376 22258 solver.cpp:539]     Test net output #2: loss = 0.2081 (* 1 = 0.2081 loss)
I0704 07:40:19.125164 22258 solver.cpp:290] Iteration 56000 (27.0235 iter/s, 3.70049s/100 iter), loss = -4.26173e-06
I0704 07:40:19.125185 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:19.125195 22258 sgd_solver.cpp:106] Iteration 56000, lr = 0.0125
I0704 07:40:21.186853 22258 solver.cpp:290] Iteration 56100 (48.5059 iter/s, 2.06161s/100 iter), loss = -4.26173e-06
I0704 07:40:21.186877 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:21.186883 22258 sgd_solver.cpp:106] Iteration 56100, lr = 0.0123438
I0704 07:40:23.246155 22258 solver.cpp:290] Iteration 56200 (48.5622 iter/s, 2.05921s/100 iter), loss = -4.26173e-06
I0704 07:40:23.246176 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:23.246183 22258 sgd_solver.cpp:106] Iteration 56200, lr = 0.0121875
I0704 07:40:25.303707 22258 solver.cpp:290] Iteration 56300 (48.6034 iter/s, 2.05747s/100 iter), loss = -4.26173e-06
I0704 07:40:25.303730 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:25.303737 22258 sgd_solver.cpp:106] Iteration 56300, lr = 0.0120313
I0704 07:40:27.361001 22258 solver.cpp:290] Iteration 56400 (48.6096 iter/s, 2.05721s/100 iter), loss = -4.26173e-06
I0704 07:40:27.361026 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:27.361032 22258 sgd_solver.cpp:106] Iteration 56400, lr = 0.011875
I0704 07:40:29.417850 22258 solver.cpp:290] Iteration 56500 (48.6201 iter/s, 2.05676s/100 iter), loss = -4.26173e-06
I0704 07:40:29.417873 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:29.417881 22258 sgd_solver.cpp:106] Iteration 56500, lr = 0.0117188
I0704 07:40:31.480113 22258 solver.cpp:290] Iteration 56600 (48.4925 iter/s, 2.06218s/100 iter), loss = -4.26173e-06
I0704 07:40:31.480135 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:31.480141 22258 sgd_solver.cpp:106] Iteration 56600, lr = 0.0115625
I0704 07:40:33.563107 22258 solver.cpp:290] Iteration 56700 (48.0098 iter/s, 2.08291s/100 iter), loss = -4.26173e-06
I0704 07:40:33.563129 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:33.563135 22258 sgd_solver.cpp:106] Iteration 56700, lr = 0.0114062
I0704 07:40:35.621034 22258 solver.cpp:290] Iteration 56800 (48.5946 iter/s, 2.05784s/100 iter), loss = -4.26173e-06
I0704 07:40:35.621057 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:35.621065 22258 sgd_solver.cpp:106] Iteration 56800, lr = 0.01125
I0704 07:40:37.676704 22258 solver.cpp:290] Iteration 56900 (48.648 iter/s, 2.05558s/100 iter), loss = -4.26173e-06
I0704 07:40:37.676726 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:37.676733 22258 sgd_solver.cpp:106] Iteration 56900, lr = 0.0110937
I0704 07:40:39.712076 22258 solver.cpp:466] Iteration 57000, Testing net (#0)
I0704 07:40:41.358007 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9174
I0704 07:40:41.358027 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9972
I0704 07:40:41.358032 22258 solver.cpp:539]     Test net output #2: loss = 0.2106 (* 1 = 0.2106 loss)
I0704 07:40:41.377816 22258 solver.cpp:290] Iteration 57000 (27.0198 iter/s, 3.70098s/100 iter), loss = -4.26173e-06
I0704 07:40:41.377835 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:41.377846 22258 sgd_solver.cpp:106] Iteration 57000, lr = 0.0109375
I0704 07:40:43.434414 22258 solver.cpp:290] Iteration 57100 (48.6259 iter/s, 2.05652s/100 iter), loss = -4.26173e-06
I0704 07:40:43.434453 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:43.434460 22258 sgd_solver.cpp:106] Iteration 57100, lr = 0.0107813
I0704 07:40:45.497501 22258 solver.cpp:290] Iteration 57200 (48.4734 iter/s, 2.06299s/100 iter), loss = -4.26173e-06
I0704 07:40:45.497567 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:45.497575 22258 sgd_solver.cpp:106] Iteration 57200, lr = 0.010625
I0704 07:40:47.554260 22258 solver.cpp:290] Iteration 57300 (48.6232 iter/s, 2.05663s/100 iter), loss = -4.26173e-06
I0704 07:40:47.554283 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:47.554292 22258 sgd_solver.cpp:106] Iteration 57300, lr = 0.0104688
I0704 07:40:49.611804 22258 solver.cpp:290] Iteration 57400 (48.6036 iter/s, 2.05746s/100 iter), loss = -4.26173e-06
I0704 07:40:49.611827 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:49.611835 22258 sgd_solver.cpp:106] Iteration 57400, lr = 0.0103125
I0704 07:40:51.670959 22258 solver.cpp:290] Iteration 57500 (48.5657 iter/s, 2.05907s/100 iter), loss = -4.26173e-06
I0704 07:40:51.670981 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:51.670989 22258 sgd_solver.cpp:106] Iteration 57500, lr = 0.0101563
I0704 07:40:53.731776 22258 solver.cpp:290] Iteration 57600 (48.5265 iter/s, 2.06073s/100 iter), loss = -4.26173e-06
I0704 07:40:53.731802 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:53.731812 22258 sgd_solver.cpp:106] Iteration 57600, lr = 0.01
I0704 07:40:55.790047 22258 solver.cpp:290] Iteration 57700 (48.5865 iter/s, 2.05818s/100 iter), loss = -4.26173e-06
I0704 07:40:55.790071 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:55.790081 22258 sgd_solver.cpp:106] Iteration 57700, lr = 0.00984375
I0704 07:40:57.850039 22258 solver.cpp:290] Iteration 57800 (48.5459 iter/s, 2.05991s/100 iter), loss = -4.26173e-06
I0704 07:40:57.850061 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:57.850067 22258 sgd_solver.cpp:106] Iteration 57800, lr = 0.0096875
I0704 07:40:59.908918 22258 solver.cpp:290] Iteration 57900 (48.5721 iter/s, 2.05879s/100 iter), loss = -4.26173e-06
I0704 07:40:59.908941 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:40:59.908947 22258 sgd_solver.cpp:106] Iteration 57900, lr = 0.00953125
I0704 07:41:01.953531 22258 solver.cpp:466] Iteration 58000, Testing net (#0)
I0704 07:41:03.607975 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9183
I0704 07:41:03.607995 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9974
I0704 07:41:03.608000 22258 solver.cpp:539]     Test net output #2: loss = 0.2112 (* 1 = 0.2112 loss)
I0704 07:41:03.629915 22258 solver.cpp:290] Iteration 58000 (26.8755 iter/s, 3.72087s/100 iter), loss = -4.26173e-06
I0704 07:41:03.629932 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:03.629945 22258 sgd_solver.cpp:106] Iteration 58000, lr = 0.009375
I0704 07:41:05.688256 22258 solver.cpp:290] Iteration 58100 (48.5847 iter/s, 2.05826s/100 iter), loss = -4.26173e-06
I0704 07:41:05.688280 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:05.688287 22258 sgd_solver.cpp:106] Iteration 58100, lr = 0.00921875
I0704 07:41:07.745968 22258 solver.cpp:290] Iteration 58200 (48.5997 iter/s, 2.05762s/100 iter), loss = -4.26173e-06
I0704 07:41:07.745990 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:07.745997 22258 sgd_solver.cpp:106] Iteration 58200, lr = 0.0090625
I0704 07:41:09.811396 22258 solver.cpp:290] Iteration 58300 (48.4182 iter/s, 2.06534s/100 iter), loss = -4.26173e-06
I0704 07:41:09.811429 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:09.811439 22258 sgd_solver.cpp:106] Iteration 58300, lr = 0.00890625
I0704 07:41:11.870442 22258 solver.cpp:290] Iteration 58400 (48.5684 iter/s, 2.05895s/100 iter), loss = -4.26173e-06
I0704 07:41:11.870465 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:11.870471 22258 sgd_solver.cpp:106] Iteration 58400, lr = 0.00875
I0704 07:41:13.927192 22258 solver.cpp:290] Iteration 58500 (48.6224 iter/s, 2.05667s/100 iter), loss = -4.26173e-06
I0704 07:41:13.927232 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:13.927240 22258 sgd_solver.cpp:106] Iteration 58500, lr = 0.00859375
I0704 07:41:15.983713 22258 solver.cpp:290] Iteration 58600 (48.6282 iter/s, 2.05642s/100 iter), loss = -4.26173e-06
I0704 07:41:15.983786 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:15.983794 22258 sgd_solver.cpp:106] Iteration 58600, lr = 0.0084375
I0704 07:41:18.043277 22258 solver.cpp:290] Iteration 58700 (48.5572 iter/s, 2.05943s/100 iter), loss = -4.26173e-06
I0704 07:41:18.043303 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:18.043311 22258 sgd_solver.cpp:106] Iteration 58700, lr = 0.00828125
I0704 07:41:20.104604 22258 solver.cpp:290] Iteration 58800 (48.5145 iter/s, 2.06124s/100 iter), loss = -4.26173e-06
I0704 07:41:20.104638 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:20.104650 22258 sgd_solver.cpp:106] Iteration 58800, lr = 0.008125
I0704 07:41:22.160873 22258 solver.cpp:290] Iteration 58900 (48.634 iter/s, 2.05617s/100 iter), loss = -4.26173e-06
I0704 07:41:22.160907 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:22.160917 22258 sgd_solver.cpp:106] Iteration 58900, lr = 0.00796875
I0704 07:41:24.196151 22258 solver.cpp:466] Iteration 59000, Testing net (#0)
I0704 07:41:25.843616 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.919
I0704 07:41:25.843637 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9973
I0704 07:41:25.843643 22258 solver.cpp:539]     Test net output #2: loss = 0.2075 (* 1 = 0.2075 loss)
I0704 07:41:25.864090 22258 solver.cpp:290] Iteration 59000 (27.0046 iter/s, 3.70308s/100 iter), loss = -4.26173e-06
I0704 07:41:25.864117 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:25.864125 22258 sgd_solver.cpp:106] Iteration 59000, lr = 0.0078125
I0704 07:41:27.923041 22258 solver.cpp:290] Iteration 59100 (48.5705 iter/s, 2.05886s/100 iter), loss = -4.26173e-06
I0704 07:41:27.923065 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:27.923071 22258 sgd_solver.cpp:106] Iteration 59100, lr = 0.00765625
I0704 07:41:29.981050 22258 solver.cpp:290] Iteration 59200 (48.5927 iter/s, 2.05792s/100 iter), loss = -4.26173e-06
I0704 07:41:29.981072 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:29.981079 22258 sgd_solver.cpp:106] Iteration 59200, lr = 0.0075
I0704 07:41:32.041404 22258 solver.cpp:290] Iteration 59300 (48.5374 iter/s, 2.06027s/100 iter), loss = -4.26173e-06
I0704 07:41:32.041427 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:32.041436 22258 sgd_solver.cpp:106] Iteration 59300, lr = 0.00734375
I0704 07:41:34.101410 22258 solver.cpp:290] Iteration 59400 (48.5456 iter/s, 2.05992s/100 iter), loss = -4.26173e-06
I0704 07:41:34.101433 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:34.101439 22258 sgd_solver.cpp:106] Iteration 59400, lr = 0.0071875
I0704 07:41:36.157590 22258 solver.cpp:290] Iteration 59500 (48.6359 iter/s, 2.05609s/100 iter), loss = -4.26173e-06
I0704 07:41:36.157613 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:36.157619 22258 sgd_solver.cpp:106] Iteration 59500, lr = 0.00703125
I0704 07:41:38.215030 22258 solver.cpp:290] Iteration 59600 (48.6061 iter/s, 2.05736s/100 iter), loss = -4.26173e-06
I0704 07:41:38.215054 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:38.215062 22258 sgd_solver.cpp:106] Iteration 59600, lr = 0.006875
I0704 07:41:40.272415 22258 solver.cpp:290] Iteration 59700 (48.6074 iter/s, 2.0573s/100 iter), loss = -4.26173e-06
I0704 07:41:40.272439 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:40.272446 22258 sgd_solver.cpp:106] Iteration 59700, lr = 0.00671875
I0704 07:41:42.332901 22258 solver.cpp:290] Iteration 59800 (48.5343 iter/s, 2.0604s/100 iter), loss = -4.26173e-06
I0704 07:41:42.332923 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:42.332931 22258 sgd_solver.cpp:106] Iteration 59800, lr = 0.0065625
I0704 07:41:44.392101 22258 solver.cpp:290] Iteration 59900 (48.5646 iter/s, 2.05911s/100 iter), loss = -4.26173e-06
I0704 07:41:44.392139 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:44.392146 22258 sgd_solver.cpp:106] Iteration 59900, lr = 0.00640625
I0704 07:41:46.429967 22258 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/cifar10_jacintonet11v2_iter_60000.caffemodel
I0704 07:41:46.446333 22258 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/cifar10_jacintonet11v2_iter_60000.solverstate
I0704 07:41:46.454126 22258 solver.cpp:466] Iteration 60000, Testing net (#0)
I0704 07:41:48.100771 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9179
I0704 07:41:48.100790 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9974
I0704 07:41:48.100796 22258 solver.cpp:539]     Test net output #2: loss = 0.2045 (* 1 = 0.2045 loss)
I0704 07:41:48.122009 22258 solver.cpp:290] Iteration 60000 (26.8113 iter/s, 3.72976s/100 iter), loss = -4.26173e-06
I0704 07:41:48.122027 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:48.122037 22258 sgd_solver.cpp:106] Iteration 60000, lr = 0.00625
I0704 07:41:50.179755 22258 solver.cpp:290] Iteration 60100 (48.5988 iter/s, 2.05766s/100 iter), loss = -4.26173e-06
I0704 07:41:50.179778 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:50.179786 22258 sgd_solver.cpp:106] Iteration 60100, lr = 0.00609375
I0704 07:41:52.240717 22258 solver.cpp:290] Iteration 60200 (48.523 iter/s, 2.06088s/100 iter), loss = -4.26173e-06
I0704 07:41:52.240739 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:52.240747 22258 sgd_solver.cpp:106] Iteration 60200, lr = 0.0059375
I0704 07:41:54.298523 22258 solver.cpp:290] Iteration 60300 (48.5975 iter/s, 2.05772s/100 iter), loss = -4.26173e-06
I0704 07:41:54.298543 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:54.298552 22258 sgd_solver.cpp:106] Iteration 60300, lr = 0.00578125
I0704 07:41:56.358355 22258 solver.cpp:290] Iteration 60400 (48.5497 iter/s, 2.05975s/100 iter), loss = -4.26173e-06
I0704 07:41:56.358381 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:56.358393 22258 sgd_solver.cpp:106] Iteration 60400, lr = 0.005625
I0704 07:41:58.416954 22258 solver.cpp:290] Iteration 60500 (48.5788 iter/s, 2.05851s/100 iter), loss = -4.26173e-06
I0704 07:41:58.416983 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:41:58.416992 22258 sgd_solver.cpp:106] Iteration 60500, lr = 0.00546875
I0704 07:42:00.475899 22258 solver.cpp:290] Iteration 60600 (48.5707 iter/s, 2.05886s/100 iter), loss = -4.26173e-06
I0704 07:42:00.475924 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:00.475929 22258 sgd_solver.cpp:106] Iteration 60600, lr = 0.0053125
I0704 07:42:02.534356 22258 solver.cpp:290] Iteration 60700 (48.5821 iter/s, 2.05837s/100 iter), loss = -4.26173e-06
I0704 07:42:02.534379 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:02.534389 22258 sgd_solver.cpp:106] Iteration 60700, lr = 0.00515625
I0704 07:42:04.593042 22258 solver.cpp:290] Iteration 60800 (48.5767 iter/s, 2.0586s/100 iter), loss = -4.26173e-06
I0704 07:42:04.593065 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:04.593071 22258 sgd_solver.cpp:106] Iteration 60800, lr = 0.005
I0704 07:42:06.648612 22258 solver.cpp:290] Iteration 60900 (48.6504 iter/s, 2.05548s/100 iter), loss = -4.26173e-06
I0704 07:42:06.648634 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:06.648641 22258 sgd_solver.cpp:106] Iteration 60900, lr = 0.00484375
I0704 07:42:08.686985 22258 solver.cpp:466] Iteration 61000, Testing net (#0)
I0704 07:42:10.333034 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9179
I0704 07:42:10.333052 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9974
I0704 07:42:10.333057 22258 solver.cpp:539]     Test net output #2: loss = 0.2079 (* 1 = 0.2079 loss)
I0704 07:42:10.354537 22258 solver.cpp:290] Iteration 61000 (26.9847 iter/s, 3.7058s/100 iter), loss = -4.26173e-06
I0704 07:42:10.354555 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:10.354567 22258 sgd_solver.cpp:106] Iteration 61000, lr = 0.0046875
I0704 07:42:12.416187 22258 solver.cpp:290] Iteration 61100 (48.5068 iter/s, 2.06157s/100 iter), loss = -4.26173e-06
I0704 07:42:12.416209 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:12.416216 22258 sgd_solver.cpp:106] Iteration 61100, lr = 0.00453125
I0704 07:42:14.474272 22258 solver.cpp:290] Iteration 61200 (48.5909 iter/s, 2.058s/100 iter), loss = -4.26173e-06
I0704 07:42:14.474293 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:14.474301 22258 sgd_solver.cpp:106] Iteration 61200, lr = 0.004375
I0704 07:42:16.532968 22258 solver.cpp:290] Iteration 61300 (48.5765 iter/s, 2.05861s/100 iter), loss = -4.26173e-06
I0704 07:42:16.533051 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:16.533058 22258 sgd_solver.cpp:106] Iteration 61300, lr = 0.00421875
I0704 07:42:18.595618 22258 solver.cpp:290] Iteration 61400 (48.4847 iter/s, 2.0625s/100 iter), loss = -4.26173e-06
I0704 07:42:18.595640 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:18.595648 22258 sgd_solver.cpp:106] Iteration 61400, lr = 0.0040625
I0704 07:42:20.655062 22258 solver.cpp:290] Iteration 61500 (48.5588 iter/s, 2.05936s/100 iter), loss = -4.26173e-06
I0704 07:42:20.655086 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:20.655094 22258 sgd_solver.cpp:106] Iteration 61500, lr = 0.00390625
I0704 07:42:22.711663 22258 solver.cpp:290] Iteration 61600 (48.626 iter/s, 2.05651s/100 iter), loss = -4.26173e-06
I0704 07:42:22.711684 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:22.711691 22258 sgd_solver.cpp:106] Iteration 61600, lr = 0.00375
I0704 07:42:24.770740 22258 solver.cpp:290] Iteration 61700 (48.5674 iter/s, 2.05899s/100 iter), loss = -4.26173e-06
I0704 07:42:24.770762 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:24.770768 22258 sgd_solver.cpp:106] Iteration 61700, lr = 0.00359375
I0704 07:42:26.829275 22258 solver.cpp:290] Iteration 61800 (48.5803 iter/s, 2.05845s/100 iter), loss = -4.26173e-06
I0704 07:42:26.829298 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:26.829304 22258 sgd_solver.cpp:106] Iteration 61800, lr = 0.0034375
I0704 07:42:28.892685 22258 solver.cpp:290] Iteration 61900 (48.4655 iter/s, 2.06333s/100 iter), loss = -4.26173e-06
I0704 07:42:28.892711 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:28.892720 22258 sgd_solver.cpp:106] Iteration 61900, lr = 0.00328125
I0704 07:42:30.932548 22258 solver.cpp:466] Iteration 62000, Testing net (#0)
I0704 07:42:32.573901 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9181
I0704 07:42:32.573920 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9972
I0704 07:42:32.573926 22258 solver.cpp:539]     Test net output #2: loss = 0.2061 (* 1 = 0.2061 loss)
I0704 07:42:32.598366 22258 solver.cpp:290] Iteration 62000 (26.9866 iter/s, 3.70555s/100 iter), loss = -4.26173e-06
I0704 07:42:32.598402 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:32.598408 22258 sgd_solver.cpp:106] Iteration 62000, lr = 0.003125
I0704 07:42:34.677390 22258 solver.cpp:290] Iteration 62100 (48.1018 iter/s, 2.07892s/100 iter), loss = -4.26173e-06
I0704 07:42:34.677420 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:34.677429 22258 sgd_solver.cpp:106] Iteration 62100, lr = 0.00296875
I0704 07:42:36.735694 22258 solver.cpp:290] Iteration 62200 (48.5858 iter/s, 2.05821s/100 iter), loss = -4.26173e-06
I0704 07:42:36.735716 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:36.735723 22258 sgd_solver.cpp:106] Iteration 62200, lr = 0.0028125
I0704 07:42:38.789741 22258 solver.cpp:290] Iteration 62300 (48.6864 iter/s, 2.05396s/100 iter), loss = -4.26173e-06
I0704 07:42:38.789762 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:38.789769 22258 sgd_solver.cpp:106] Iteration 62300, lr = 0.00265625
I0704 07:42:40.844818 22258 solver.cpp:290] Iteration 62400 (48.662 iter/s, 2.05499s/100 iter), loss = -4.26173e-06
I0704 07:42:40.844841 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:40.844848 22258 sgd_solver.cpp:106] Iteration 62400, lr = 0.0025
I0704 07:42:42.910193 22258 solver.cpp:290] Iteration 62500 (48.4195 iter/s, 2.06528s/100 iter), loss = -4.26173e-06
I0704 07:42:42.910223 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:42.910231 22258 sgd_solver.cpp:106] Iteration 62500, lr = 0.00234375
I0704 07:42:44.967561 22258 solver.cpp:290] Iteration 62600 (48.6079 iter/s, 2.05728s/100 iter), loss = -4.26173e-06
I0704 07:42:44.967602 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:44.967608 22258 sgd_solver.cpp:106] Iteration 62600, lr = 0.0021875
I0704 07:42:47.030961 22258 solver.cpp:290] Iteration 62700 (48.4661 iter/s, 2.0633s/100 iter), loss = -4.26173e-06
I0704 07:42:47.031026 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:47.031034 22258 sgd_solver.cpp:106] Iteration 62700, lr = 0.00203125
I0704 07:42:49.087462 22258 solver.cpp:290] Iteration 62800 (48.6293 iter/s, 2.05637s/100 iter), loss = -4.26173e-06
I0704 07:42:49.087486 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:49.087493 22258 sgd_solver.cpp:106] Iteration 62800, lr = 0.001875
I0704 07:42:51.148769 22258 solver.cpp:290] Iteration 62900 (48.515 iter/s, 2.06122s/100 iter), loss = -4.26173e-06
I0704 07:42:51.148795 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:51.148804 22258 sgd_solver.cpp:106] Iteration 62900, lr = 0.00171875
I0704 07:42:53.185147 22258 solver.cpp:466] Iteration 63000, Testing net (#0)
I0704 07:42:54.827890 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9179
I0704 07:42:54.827909 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9973
I0704 07:42:54.827914 22258 solver.cpp:539]     Test net output #2: loss = 0.2064 (* 1 = 0.2064 loss)
I0704 07:42:54.848331 22258 solver.cpp:290] Iteration 63000 (27.0312 iter/s, 3.69943s/100 iter), loss = -4.26173e-06
I0704 07:42:54.848352 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:54.848361 22258 sgd_solver.cpp:106] Iteration 63000, lr = 0.0015625
I0704 07:42:56.903462 22258 solver.cpp:290] Iteration 63100 (48.6607 iter/s, 2.05505s/100 iter), loss = -4.26173e-06
I0704 07:42:56.903483 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:56.903491 22258 sgd_solver.cpp:106] Iteration 63100, lr = 0.00140625
I0704 07:42:58.965718 22258 solver.cpp:290] Iteration 63200 (48.4927 iter/s, 2.06217s/100 iter), loss = -4.26173e-06
I0704 07:42:58.965739 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:42:58.965746 22258 sgd_solver.cpp:106] Iteration 63200, lr = 0.00125
I0704 07:43:01.025502 22258 solver.cpp:290] Iteration 63300 (48.5508 iter/s, 2.0597s/100 iter), loss = -4.26173e-06
I0704 07:43:01.025526 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:01.025532 22258 sgd_solver.cpp:106] Iteration 63300, lr = 0.00109375
I0704 07:43:03.081986 22258 solver.cpp:290] Iteration 63400 (48.6287 iter/s, 2.0564s/100 iter), loss = -4.26173e-06
I0704 07:43:03.082010 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:03.082016 22258 sgd_solver.cpp:106] Iteration 63400, lr = 0.000937498
I0704 07:43:05.138159 22258 solver.cpp:290] Iteration 63500 (48.6361 iter/s, 2.05609s/100 iter), loss = -4.26173e-06
I0704 07:43:05.138182 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:05.138190 22258 sgd_solver.cpp:106] Iteration 63500, lr = 0.00078125
I0704 07:43:07.195611 22258 solver.cpp:290] Iteration 63600 (48.6059 iter/s, 2.05737s/100 iter), loss = -4.26173e-06
I0704 07:43:07.195633 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:07.195641 22258 sgd_solver.cpp:106] Iteration 63600, lr = 0.000625002
I0704 07:43:09.253885 22258 solver.cpp:290] Iteration 63700 (48.5865 iter/s, 2.05819s/100 iter), loss = -4.26173e-06
I0704 07:43:09.253911 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:09.253919 22258 sgd_solver.cpp:106] Iteration 63700, lr = 0.000468749
I0704 07:43:11.316545 22258 solver.cpp:290] Iteration 63800 (48.4832 iter/s, 2.06257s/100 iter), loss = -4.26173e-06
I0704 07:43:11.316566 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:11.316572 22258 sgd_solver.cpp:106] Iteration 63800, lr = 0.000312501
I0704 07:43:13.376256 22258 solver.cpp:290] Iteration 63900 (48.5525 iter/s, 2.05962s/100 iter), loss = -4.26173e-06
I0704 07:43:13.376279 22258 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:13.376286 22258 sgd_solver.cpp:106] Iteration 63900, lr = 0.000156248
I0704 07:43:15.418951 22258 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/cifar10_jacintonet11v2_iter_64000.caffemodel
I0704 07:43:15.435813 22258 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/cifar10_jacintonet11v2_iter_64000.solverstate
I0704 07:43:15.447899 22258 solver.cpp:446] Iteration 64000, loss = -4.26173e-06
I0704 07:43:15.447921 22258 solver.cpp:466] Iteration 64000, Testing net (#0)
I0704 07:43:17.090873 22258 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.917
I0704 07:43:17.090960 22258 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9974
I0704 07:43:17.090970 22258 solver.cpp:539]     Test net output #2: loss = 0.2069 (* 1 = 0.2069 loss)
I0704 07:43:17.090973 22258 solver.cpp:451] Optimization Done.
I0704 07:43:17.135788 22258 caffe.cpp:246] Optimization Done.
training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse
I0704 07:43:18.417459 25348 caffe.cpp:209] Using GPUs 0, 1, 2
I0704 07:43:18.417932 25348 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0704 07:43:18.418257 25348 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0704 07:43:18.418592 25348 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0704 07:43:18.813833 25348 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/train.prototxt"
test_net: "training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/test.prototxt"
test_iter: 200
test_interval: 1000
base_lr: 0.01
display: 100
max_iter: 64000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.001
snapshot: 10000
snapshot_prefix: "training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: true
iter_size: 1
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.02
sparsity_step_iter: 1000
sparsity_start_iter: 4000
sparsity_start_factor: 0
I0704 07:43:18.813925 25348 solver.cpp:82] Creating training net from train_net file: training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/train.prototxt
I0704 07:43:18.814389 25348 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0704 07:43:18.814395 25348 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0704 07:43:18.814551 25348 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_train_lmdb"
    batch_size: 21
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
I0704 07:43:18.814636 25348 layer_factory.hpp:77] Creating layer data
I0704 07:43:18.814719 25348 net.cpp:98] Creating Layer data
I0704 07:43:18.814725 25348 net.cpp:413] data -> data
I0704 07:43:18.814741 25348 net.cpp:413] data -> label
I0704 07:43:18.815533 25377 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_train_lmdb
I0704 07:43:18.831476 25348 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0704 07:43:18.831531 25348 data_layer.cpp:83] output data size: 21,3,32,32
I0704 07:43:18.833170 25348 net.cpp:148] Setting up data
I0704 07:43:18.833183 25348 net.cpp:155] Top shape: 21 3 32 32 (64512)
I0704 07:43:18.833185 25348 net.cpp:155] Top shape: 21 (21)
I0704 07:43:18.833187 25348 net.cpp:163] Memory required for data: 258132
I0704 07:43:18.833192 25348 layer_factory.hpp:77] Creating layer data/bias
I0704 07:43:18.833199 25348 net.cpp:98] Creating Layer data/bias
I0704 07:43:18.833202 25348 net.cpp:439] data/bias <- data
I0704 07:43:18.833209 25348 net.cpp:413] data/bias -> data/bias
I0704 07:43:18.834234 25348 net.cpp:148] Setting up data/bias
I0704 07:43:18.834244 25348 net.cpp:155] Top shape: 21 3 32 32 (64512)
I0704 07:43:18.834246 25348 net.cpp:163] Memory required for data: 516180
I0704 07:43:18.834254 25348 layer_factory.hpp:77] Creating layer conv1a
I0704 07:43:18.834262 25348 net.cpp:98] Creating Layer conv1a
I0704 07:43:18.834264 25348 net.cpp:439] conv1a <- data/bias
I0704 07:43:18.834267 25348 net.cpp:413] conv1a -> conv1a
I0704 07:43:18.834519 25379 blocking_queue.cpp:50] Waiting for data
I0704 07:43:18.835582 25348 net.cpp:148] Setting up conv1a
I0704 07:43:18.835590 25348 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:43:18.835592 25348 net.cpp:163] Memory required for data: 3268692
I0704 07:43:18.835597 25348 layer_factory.hpp:77] Creating layer conv1a/bn
I0704 07:43:18.835603 25348 net.cpp:98] Creating Layer conv1a/bn
I0704 07:43:18.835605 25348 net.cpp:439] conv1a/bn <- conv1a
I0704 07:43:18.835608 25348 net.cpp:413] conv1a/bn -> conv1a/bn
I0704 07:43:18.836293 25348 net.cpp:148] Setting up conv1a/bn
I0704 07:43:18.836299 25348 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:43:18.836302 25348 net.cpp:163] Memory required for data: 6021204
I0704 07:43:18.836308 25348 layer_factory.hpp:77] Creating layer conv1a/relu
I0704 07:43:18.836311 25348 net.cpp:98] Creating Layer conv1a/relu
I0704 07:43:18.836313 25348 net.cpp:439] conv1a/relu <- conv1a/bn
I0704 07:43:18.836316 25348 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0704 07:43:18.836328 25348 net.cpp:148] Setting up conv1a/relu
I0704 07:43:18.836330 25348 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:43:18.836341 25348 net.cpp:163] Memory required for data: 8773716
I0704 07:43:18.836344 25348 layer_factory.hpp:77] Creating layer conv1b
I0704 07:43:18.836347 25348 net.cpp:98] Creating Layer conv1b
I0704 07:43:18.836350 25348 net.cpp:439] conv1b <- conv1a/bn
I0704 07:43:18.836352 25348 net.cpp:413] conv1b -> conv1b
I0704 07:43:18.836679 25348 net.cpp:148] Setting up conv1b
I0704 07:43:18.836685 25348 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:43:18.836688 25348 net.cpp:163] Memory required for data: 11526228
I0704 07:43:18.836693 25348 layer_factory.hpp:77] Creating layer conv1b/bn
I0704 07:43:18.836695 25348 net.cpp:98] Creating Layer conv1b/bn
I0704 07:43:18.836699 25348 net.cpp:439] conv1b/bn <- conv1b
I0704 07:43:18.836700 25348 net.cpp:413] conv1b/bn -> conv1b/bn
I0704 07:43:18.837368 25348 net.cpp:148] Setting up conv1b/bn
I0704 07:43:18.837373 25348 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:43:18.837375 25348 net.cpp:163] Memory required for data: 14278740
I0704 07:43:18.837380 25348 layer_factory.hpp:77] Creating layer conv1b/relu
I0704 07:43:18.837384 25348 net.cpp:98] Creating Layer conv1b/relu
I0704 07:43:18.837386 25348 net.cpp:439] conv1b/relu <- conv1b/bn
I0704 07:43:18.837388 25348 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0704 07:43:18.837393 25348 net.cpp:148] Setting up conv1b/relu
I0704 07:43:18.837395 25348 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:43:18.837397 25348 net.cpp:163] Memory required for data: 17031252
I0704 07:43:18.837399 25348 layer_factory.hpp:77] Creating layer pool1
I0704 07:43:18.837405 25348 net.cpp:98] Creating Layer pool1
I0704 07:43:18.837407 25348 net.cpp:439] pool1 <- conv1b/bn
I0704 07:43:18.837410 25348 net.cpp:413] pool1 -> pool1
I0704 07:43:18.837458 25348 net.cpp:148] Setting up pool1
I0704 07:43:18.837463 25348 net.cpp:155] Top shape: 21 32 32 32 (688128)
I0704 07:43:18.837466 25348 net.cpp:163] Memory required for data: 19783764
I0704 07:43:18.837467 25348 layer_factory.hpp:77] Creating layer res2a_branch2a
I0704 07:43:18.837471 25348 net.cpp:98] Creating Layer res2a_branch2a
I0704 07:43:18.837474 25348 net.cpp:439] res2a_branch2a <- pool1
I0704 07:43:18.837477 25348 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0704 07:43:18.838111 25348 net.cpp:148] Setting up res2a_branch2a
I0704 07:43:18.838117 25348 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:43:18.838119 25348 net.cpp:163] Memory required for data: 25288788
I0704 07:43:18.838124 25348 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0704 07:43:18.838127 25348 net.cpp:98] Creating Layer res2a_branch2a/bn
I0704 07:43:18.838130 25348 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0704 07:43:18.838132 25348 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0704 07:43:18.838798 25348 net.cpp:148] Setting up res2a_branch2a/bn
I0704 07:43:18.838805 25348 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:43:18.838807 25348 net.cpp:163] Memory required for data: 30793812
I0704 07:43:18.838812 25348 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0704 07:43:18.838815 25348 net.cpp:98] Creating Layer res2a_branch2a/relu
I0704 07:43:18.838819 25348 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0704 07:43:18.838821 25348 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0704 07:43:18.838825 25348 net.cpp:148] Setting up res2a_branch2a/relu
I0704 07:43:18.838829 25348 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:43:18.838830 25348 net.cpp:163] Memory required for data: 36298836
I0704 07:43:18.838832 25348 layer_factory.hpp:77] Creating layer res2a_branch2b
I0704 07:43:18.838836 25348 net.cpp:98] Creating Layer res2a_branch2b
I0704 07:43:18.838840 25348 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0704 07:43:18.838845 25348 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0704 07:43:18.840179 25348 net.cpp:148] Setting up res2a_branch2b
I0704 07:43:18.840188 25348 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:43:18.840190 25348 net.cpp:163] Memory required for data: 41803860
I0704 07:43:18.840203 25348 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0704 07:43:18.840206 25348 net.cpp:98] Creating Layer res2a_branch2b/bn
I0704 07:43:18.840209 25348 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0704 07:43:18.840214 25348 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0704 07:43:18.840867 25348 net.cpp:148] Setting up res2a_branch2b/bn
I0704 07:43:18.840873 25348 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:43:18.840874 25348 net.cpp:163] Memory required for data: 47308884
I0704 07:43:18.840881 25348 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0704 07:43:18.840886 25348 net.cpp:98] Creating Layer res2a_branch2b/relu
I0704 07:43:18.840891 25348 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0704 07:43:18.840895 25348 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0704 07:43:18.840903 25348 net.cpp:148] Setting up res2a_branch2b/relu
I0704 07:43:18.840905 25348 net.cpp:155] Top shape: 21 64 32 32 (1376256)
I0704 07:43:18.840908 25348 net.cpp:163] Memory required for data: 52813908
I0704 07:43:18.840909 25348 layer_factory.hpp:77] Creating layer pool2
I0704 07:43:18.840912 25348 net.cpp:98] Creating Layer pool2
I0704 07:43:18.840914 25348 net.cpp:439] pool2 <- res2a_branch2b/bn
I0704 07:43:18.840916 25348 net.cpp:413] pool2 -> pool2
I0704 07:43:18.840952 25348 net.cpp:148] Setting up pool2
I0704 07:43:18.840956 25348 net.cpp:155] Top shape: 21 64 16 16 (344064)
I0704 07:43:18.840958 25348 net.cpp:163] Memory required for data: 54190164
I0704 07:43:18.840960 25348 layer_factory.hpp:77] Creating layer res3a_branch2a
I0704 07:43:18.840965 25348 net.cpp:98] Creating Layer res3a_branch2a
I0704 07:43:18.840967 25348 net.cpp:439] res3a_branch2a <- pool2
I0704 07:43:18.840970 25348 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0704 07:43:18.843564 25348 net.cpp:148] Setting up res3a_branch2a
I0704 07:43:18.843574 25348 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:43:18.843576 25348 net.cpp:163] Memory required for data: 56942676
I0704 07:43:18.843580 25348 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0704 07:43:18.843585 25348 net.cpp:98] Creating Layer res3a_branch2a/bn
I0704 07:43:18.843588 25348 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0704 07:43:18.843591 25348 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0704 07:43:18.844173 25348 net.cpp:148] Setting up res3a_branch2a/bn
I0704 07:43:18.844179 25348 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:43:18.844182 25348 net.cpp:163] Memory required for data: 59695188
I0704 07:43:18.844192 25348 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0704 07:43:18.844195 25348 net.cpp:98] Creating Layer res3a_branch2a/relu
I0704 07:43:18.844197 25348 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0704 07:43:18.844199 25348 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0704 07:43:18.844203 25348 net.cpp:148] Setting up res3a_branch2a/relu
I0704 07:43:18.844208 25348 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:43:18.844208 25348 net.cpp:163] Memory required for data: 62447700
I0704 07:43:18.844210 25348 layer_factory.hpp:77] Creating layer res3a_branch2b
I0704 07:43:18.844214 25348 net.cpp:98] Creating Layer res3a_branch2b
I0704 07:43:18.844216 25348 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0704 07:43:18.844219 25348 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0704 07:43:18.845216 25348 net.cpp:148] Setting up res3a_branch2b
I0704 07:43:18.845221 25348 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:43:18.845223 25348 net.cpp:163] Memory required for data: 65200212
I0704 07:43:18.845227 25348 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0704 07:43:18.845232 25348 net.cpp:98] Creating Layer res3a_branch2b/bn
I0704 07:43:18.845234 25348 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0704 07:43:18.845237 25348 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0704 07:43:18.845834 25348 net.cpp:148] Setting up res3a_branch2b/bn
I0704 07:43:18.845839 25348 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:43:18.845850 25348 net.cpp:163] Memory required for data: 67952724
I0704 07:43:18.845856 25348 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0704 07:43:18.845860 25348 net.cpp:98] Creating Layer res3a_branch2b/relu
I0704 07:43:18.845863 25348 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0704 07:43:18.845865 25348 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0704 07:43:18.845870 25348 net.cpp:148] Setting up res3a_branch2b/relu
I0704 07:43:18.845873 25348 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:43:18.845875 25348 net.cpp:163] Memory required for data: 70705236
I0704 07:43:18.845877 25348 layer_factory.hpp:77] Creating layer pool3
I0704 07:43:18.845880 25348 net.cpp:98] Creating Layer pool3
I0704 07:43:18.845883 25348 net.cpp:439] pool3 <- res3a_branch2b/bn
I0704 07:43:18.845888 25348 net.cpp:413] pool3 -> pool3
I0704 07:43:18.845933 25348 net.cpp:148] Setting up pool3
I0704 07:43:18.845939 25348 net.cpp:155] Top shape: 21 128 16 16 (688128)
I0704 07:43:18.845942 25348 net.cpp:163] Memory required for data: 73457748
I0704 07:43:18.845943 25348 layer_factory.hpp:77] Creating layer res4a_branch2a
I0704 07:43:18.845948 25348 net.cpp:98] Creating Layer res4a_branch2a
I0704 07:43:18.845952 25348 net.cpp:439] res4a_branch2a <- pool3
I0704 07:43:18.845954 25348 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0704 07:43:18.852285 25348 net.cpp:148] Setting up res4a_branch2a
I0704 07:43:18.852298 25348 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:43:18.852300 25348 net.cpp:163] Memory required for data: 78962772
I0704 07:43:18.852305 25348 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0704 07:43:18.852311 25348 net.cpp:98] Creating Layer res4a_branch2a/bn
I0704 07:43:18.852313 25348 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0704 07:43:18.852318 25348 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0704 07:43:18.852974 25348 net.cpp:148] Setting up res4a_branch2a/bn
I0704 07:43:18.852982 25348 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:43:18.852984 25348 net.cpp:163] Memory required for data: 84467796
I0704 07:43:18.852990 25348 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0704 07:43:18.852994 25348 net.cpp:98] Creating Layer res4a_branch2a/relu
I0704 07:43:18.852998 25348 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0704 07:43:18.853000 25348 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0704 07:43:18.853004 25348 net.cpp:148] Setting up res4a_branch2a/relu
I0704 07:43:18.853008 25348 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:43:18.853009 25348 net.cpp:163] Memory required for data: 89972820
I0704 07:43:18.853011 25348 layer_factory.hpp:77] Creating layer res4a_branch2b
I0704 07:43:18.853016 25348 net.cpp:98] Creating Layer res4a_branch2b
I0704 07:43:18.853018 25348 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0704 07:43:18.853021 25348 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0704 07:43:18.856284 25348 net.cpp:148] Setting up res4a_branch2b
I0704 07:43:18.856292 25348 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:43:18.856294 25348 net.cpp:163] Memory required for data: 95477844
I0704 07:43:18.856297 25348 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0704 07:43:18.856302 25348 net.cpp:98] Creating Layer res4a_branch2b/bn
I0704 07:43:18.856304 25348 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0704 07:43:18.856308 25348 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0704 07:43:18.856950 25348 net.cpp:148] Setting up res4a_branch2b/bn
I0704 07:43:18.856957 25348 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:43:18.856959 25348 net.cpp:163] Memory required for data: 100982868
I0704 07:43:18.856966 25348 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0704 07:43:18.856968 25348 net.cpp:98] Creating Layer res4a_branch2b/relu
I0704 07:43:18.856971 25348 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0704 07:43:18.856974 25348 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0704 07:43:18.856986 25348 net.cpp:148] Setting up res4a_branch2b/relu
I0704 07:43:18.856990 25348 net.cpp:155] Top shape: 21 256 16 16 (1376256)
I0704 07:43:18.856992 25348 net.cpp:163] Memory required for data: 106487892
I0704 07:43:18.856994 25348 layer_factory.hpp:77] Creating layer pool4
I0704 07:43:18.856998 25348 net.cpp:98] Creating Layer pool4
I0704 07:43:18.857000 25348 net.cpp:439] pool4 <- res4a_branch2b/bn
I0704 07:43:18.857003 25348 net.cpp:413] pool4 -> pool4
I0704 07:43:18.857046 25348 net.cpp:148] Setting up pool4
I0704 07:43:18.857053 25348 net.cpp:155] Top shape: 21 256 8 8 (344064)
I0704 07:43:18.857056 25348 net.cpp:163] Memory required for data: 107864148
I0704 07:43:18.857060 25348 layer_factory.hpp:77] Creating layer res5a_branch2a
I0704 07:43:18.857067 25348 net.cpp:98] Creating Layer res5a_branch2a
I0704 07:43:18.857071 25348 net.cpp:439] res5a_branch2a <- pool4
I0704 07:43:18.857076 25348 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0704 07:43:18.882076 25348 net.cpp:148] Setting up res5a_branch2a
I0704 07:43:18.882092 25348 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:43:18.882094 25348 net.cpp:163] Memory required for data: 110616660
I0704 07:43:18.882099 25348 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0704 07:43:18.882105 25348 net.cpp:98] Creating Layer res5a_branch2a/bn
I0704 07:43:18.882108 25348 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0704 07:43:18.882112 25348 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0704 07:43:18.882800 25348 net.cpp:148] Setting up res5a_branch2a/bn
I0704 07:43:18.882808 25348 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:43:18.882810 25348 net.cpp:163] Memory required for data: 113369172
I0704 07:43:18.882815 25348 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0704 07:43:18.882819 25348 net.cpp:98] Creating Layer res5a_branch2a/relu
I0704 07:43:18.882822 25348 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0704 07:43:18.882825 25348 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0704 07:43:18.882829 25348 net.cpp:148] Setting up res5a_branch2a/relu
I0704 07:43:18.882832 25348 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:43:18.882833 25348 net.cpp:163] Memory required for data: 116121684
I0704 07:43:18.882835 25348 layer_factory.hpp:77] Creating layer res5a_branch2b
I0704 07:43:18.882843 25348 net.cpp:98] Creating Layer res5a_branch2b
I0704 07:43:18.882846 25348 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0704 07:43:18.882849 25348 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0704 07:43:18.895859 25348 net.cpp:148] Setting up res5a_branch2b
I0704 07:43:18.895879 25348 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:43:18.895881 25348 net.cpp:163] Memory required for data: 118874196
I0704 07:43:18.895889 25348 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0704 07:43:18.895896 25348 net.cpp:98] Creating Layer res5a_branch2b/bn
I0704 07:43:18.895900 25348 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0704 07:43:18.895903 25348 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0704 07:43:18.896600 25348 net.cpp:148] Setting up res5a_branch2b/bn
I0704 07:43:18.896608 25348 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:43:18.896610 25348 net.cpp:163] Memory required for data: 121626708
I0704 07:43:18.896616 25348 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0704 07:43:18.896620 25348 net.cpp:98] Creating Layer res5a_branch2b/relu
I0704 07:43:18.896622 25348 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0704 07:43:18.896625 25348 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0704 07:43:18.896628 25348 net.cpp:148] Setting up res5a_branch2b/relu
I0704 07:43:18.896631 25348 net.cpp:155] Top shape: 21 512 8 8 (688128)
I0704 07:43:18.896632 25348 net.cpp:163] Memory required for data: 124379220
I0704 07:43:18.896636 25348 layer_factory.hpp:77] Creating layer pool5
I0704 07:43:18.896641 25348 net.cpp:98] Creating Layer pool5
I0704 07:43:18.896643 25348 net.cpp:439] pool5 <- res5a_branch2b/bn
I0704 07:43:18.896656 25348 net.cpp:413] pool5 -> pool5
I0704 07:43:18.896687 25348 net.cpp:148] Setting up pool5
I0704 07:43:18.896693 25348 net.cpp:155] Top shape: 21 512 1 1 (10752)
I0704 07:43:18.896697 25348 net.cpp:163] Memory required for data: 124422228
I0704 07:43:18.896699 25348 layer_factory.hpp:77] Creating layer fc10
I0704 07:43:18.896703 25348 net.cpp:98] Creating Layer fc10
I0704 07:43:18.896704 25348 net.cpp:439] fc10 <- pool5
I0704 07:43:18.896708 25348 net.cpp:413] fc10 -> fc10
I0704 07:43:18.896934 25348 net.cpp:148] Setting up fc10
I0704 07:43:18.896939 25348 net.cpp:155] Top shape: 21 10 (210)
I0704 07:43:18.896940 25348 net.cpp:163] Memory required for data: 124423068
I0704 07:43:18.896944 25348 layer_factory.hpp:77] Creating layer loss
I0704 07:43:18.896952 25348 net.cpp:98] Creating Layer loss
I0704 07:43:18.896955 25348 net.cpp:439] loss <- fc10
I0704 07:43:18.896956 25348 net.cpp:439] loss <- label
I0704 07:43:18.896960 25348 net.cpp:413] loss -> loss
I0704 07:43:18.896966 25348 layer_factory.hpp:77] Creating layer loss
I0704 07:43:18.897073 25348 net.cpp:148] Setting up loss
I0704 07:43:18.897076 25348 net.cpp:155] Top shape: (1)
I0704 07:43:18.897078 25348 net.cpp:158]     with loss weight 1
I0704 07:43:18.897089 25348 net.cpp:163] Memory required for data: 124423072
I0704 07:43:18.897091 25348 net.cpp:224] loss needs backward computation.
I0704 07:43:18.897094 25348 net.cpp:224] fc10 needs backward computation.
I0704 07:43:18.897095 25348 net.cpp:224] pool5 needs backward computation.
I0704 07:43:18.897096 25348 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0704 07:43:18.897099 25348 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0704 07:43:18.897100 25348 net.cpp:224] res5a_branch2b needs backward computation.
I0704 07:43:18.897102 25348 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0704 07:43:18.897104 25348 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0704 07:43:18.897106 25348 net.cpp:224] res5a_branch2a needs backward computation.
I0704 07:43:18.897109 25348 net.cpp:224] pool4 needs backward computation.
I0704 07:43:18.897110 25348 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0704 07:43:18.897112 25348 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0704 07:43:18.897114 25348 net.cpp:224] res4a_branch2b needs backward computation.
I0704 07:43:18.897117 25348 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0704 07:43:18.897119 25348 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0704 07:43:18.897121 25348 net.cpp:224] res4a_branch2a needs backward computation.
I0704 07:43:18.897123 25348 net.cpp:224] pool3 needs backward computation.
I0704 07:43:18.897126 25348 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0704 07:43:18.897128 25348 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0704 07:43:18.897130 25348 net.cpp:224] res3a_branch2b needs backward computation.
I0704 07:43:18.897132 25348 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0704 07:43:18.897135 25348 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0704 07:43:18.897137 25348 net.cpp:224] res3a_branch2a needs backward computation.
I0704 07:43:18.897140 25348 net.cpp:224] pool2 needs backward computation.
I0704 07:43:18.897142 25348 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0704 07:43:18.897145 25348 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0704 07:43:18.897150 25348 net.cpp:224] res2a_branch2b needs backward computation.
I0704 07:43:18.897152 25348 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0704 07:43:18.897156 25348 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0704 07:43:18.897161 25348 net.cpp:224] res2a_branch2a needs backward computation.
I0704 07:43:18.897164 25348 net.cpp:224] pool1 needs backward computation.
I0704 07:43:18.897168 25348 net.cpp:224] conv1b/relu needs backward computation.
I0704 07:43:18.897171 25348 net.cpp:224] conv1b/bn needs backward computation.
I0704 07:43:18.897178 25348 net.cpp:224] conv1b needs backward computation.
I0704 07:43:18.897181 25348 net.cpp:224] conv1a/relu needs backward computation.
I0704 07:43:18.897182 25348 net.cpp:224] conv1a/bn needs backward computation.
I0704 07:43:18.897184 25348 net.cpp:224] conv1a needs backward computation.
I0704 07:43:18.897187 25348 net.cpp:226] data/bias does not need backward computation.
I0704 07:43:18.897191 25348 net.cpp:226] data does not need backward computation.
I0704 07:43:18.897192 25348 net.cpp:268] This network produces output loss
I0704 07:43:18.897209 25348 net.cpp:288] Network initialization done.
I0704 07:43:18.897649 25348 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/test.prototxt
I0704 07:43:18.897832 25348 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0704 07:43:18.897933 25348 layer_factory.hpp:77] Creating layer data
I0704 07:43:18.898000 25348 net.cpp:98] Creating Layer data
I0704 07:43:18.898006 25348 net.cpp:413] data -> data
I0704 07:43:18.898012 25348 net.cpp:413] data -> label
I0704 07:43:18.899343 25380 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0704 07:43:18.899446 25348 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0704 07:43:18.899503 25348 data_layer.cpp:83] output data size: 50,3,32,32
I0704 07:43:18.901779 25348 net.cpp:148] Setting up data
I0704 07:43:18.901787 25348 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0704 07:43:18.901790 25348 net.cpp:155] Top shape: 50 (50)
I0704 07:43:18.901793 25348 net.cpp:163] Memory required for data: 614600
I0704 07:43:18.901795 25348 layer_factory.hpp:77] Creating layer label_data_1_split
I0704 07:43:18.901799 25348 net.cpp:98] Creating Layer label_data_1_split
I0704 07:43:18.901801 25348 net.cpp:439] label_data_1_split <- label
I0704 07:43:18.901804 25348 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0704 07:43:18.901808 25348 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0704 07:43:18.901811 25348 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0704 07:43:18.901940 25348 net.cpp:148] Setting up label_data_1_split
I0704 07:43:18.901952 25348 net.cpp:155] Top shape: 50 (50)
I0704 07:43:18.901957 25348 net.cpp:155] Top shape: 50 (50)
I0704 07:43:18.901960 25348 net.cpp:155] Top shape: 50 (50)
I0704 07:43:18.901963 25348 net.cpp:163] Memory required for data: 615200
I0704 07:43:18.901968 25348 layer_factory.hpp:77] Creating layer data/bias
I0704 07:43:18.901973 25348 net.cpp:98] Creating Layer data/bias
I0704 07:43:18.901978 25348 net.cpp:439] data/bias <- data
I0704 07:43:18.901981 25348 net.cpp:413] data/bias -> data/bias
I0704 07:43:18.902103 25348 net.cpp:148] Setting up data/bias
I0704 07:43:18.902110 25348 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0704 07:43:18.902113 25348 net.cpp:163] Memory required for data: 1229600
I0704 07:43:18.902119 25348 layer_factory.hpp:77] Creating layer conv1a
I0704 07:43:18.902125 25348 net.cpp:98] Creating Layer conv1a
I0704 07:43:18.902129 25348 net.cpp:439] conv1a <- data/bias
I0704 07:43:18.902133 25348 net.cpp:413] conv1a -> conv1a
I0704 07:43:18.902578 25348 net.cpp:148] Setting up conv1a
I0704 07:43:18.902585 25348 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:43:18.902588 25348 net.cpp:163] Memory required for data: 7783200
I0704 07:43:18.902592 25348 layer_factory.hpp:77] Creating layer conv1a/bn
I0704 07:43:18.902596 25348 net.cpp:98] Creating Layer conv1a/bn
I0704 07:43:18.902598 25348 net.cpp:439] conv1a/bn <- conv1a
I0704 07:43:18.902601 25348 net.cpp:413] conv1a/bn -> conv1a/bn
I0704 07:43:18.903496 25348 net.cpp:148] Setting up conv1a/bn
I0704 07:43:18.903506 25348 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:43:18.903508 25348 net.cpp:163] Memory required for data: 14336800
I0704 07:43:18.903514 25348 layer_factory.hpp:77] Creating layer conv1a/relu
I0704 07:43:18.903529 25348 net.cpp:98] Creating Layer conv1a/relu
I0704 07:43:18.903535 25348 net.cpp:439] conv1a/relu <- conv1a/bn
I0704 07:43:18.903540 25348 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0704 07:43:18.903548 25348 net.cpp:148] Setting up conv1a/relu
I0704 07:43:18.903553 25348 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:43:18.903558 25348 net.cpp:163] Memory required for data: 20890400
I0704 07:43:18.903560 25348 layer_factory.hpp:77] Creating layer conv1b
I0704 07:43:18.903568 25348 net.cpp:98] Creating Layer conv1b
I0704 07:43:18.903571 25348 net.cpp:439] conv1b <- conv1a/bn
I0704 07:43:18.903578 25348 net.cpp:413] conv1b -> conv1b
I0704 07:43:18.904013 25348 net.cpp:148] Setting up conv1b
I0704 07:43:18.904021 25348 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:43:18.904023 25348 net.cpp:163] Memory required for data: 27444000
I0704 07:43:18.904028 25348 layer_factory.hpp:77] Creating layer conv1b/bn
I0704 07:43:18.904036 25348 net.cpp:98] Creating Layer conv1b/bn
I0704 07:43:18.904040 25348 net.cpp:439] conv1b/bn <- conv1b
I0704 07:43:18.904045 25348 net.cpp:413] conv1b/bn -> conv1b/bn
I0704 07:43:18.904844 25348 net.cpp:148] Setting up conv1b/bn
I0704 07:43:18.904851 25348 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:43:18.904855 25348 net.cpp:163] Memory required for data: 33997600
I0704 07:43:18.904861 25348 layer_factory.hpp:77] Creating layer conv1b/relu
I0704 07:43:18.904867 25348 net.cpp:98] Creating Layer conv1b/relu
I0704 07:43:18.904871 25348 net.cpp:439] conv1b/relu <- conv1b/bn
I0704 07:43:18.904876 25348 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0704 07:43:18.904882 25348 net.cpp:148] Setting up conv1b/relu
I0704 07:43:18.904887 25348 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:43:18.904891 25348 net.cpp:163] Memory required for data: 40551200
I0704 07:43:18.904896 25348 layer_factory.hpp:77] Creating layer pool1
I0704 07:43:18.904901 25348 net.cpp:98] Creating Layer pool1
I0704 07:43:18.904904 25348 net.cpp:439] pool1 <- conv1b/bn
I0704 07:43:18.904908 25348 net.cpp:413] pool1 -> pool1
I0704 07:43:18.904959 25348 net.cpp:148] Setting up pool1
I0704 07:43:18.904966 25348 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 07:43:18.904969 25348 net.cpp:163] Memory required for data: 47104800
I0704 07:43:18.904973 25348 layer_factory.hpp:77] Creating layer res2a_branch2a
I0704 07:43:18.904980 25348 net.cpp:98] Creating Layer res2a_branch2a
I0704 07:43:18.904985 25348 net.cpp:439] res2a_branch2a <- pool1
I0704 07:43:18.904990 25348 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0704 07:43:18.905736 25348 net.cpp:148] Setting up res2a_branch2a
I0704 07:43:18.905742 25348 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:43:18.905745 25348 net.cpp:163] Memory required for data: 60212000
I0704 07:43:18.905751 25348 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0704 07:43:18.905757 25348 net.cpp:98] Creating Layer res2a_branch2a/bn
I0704 07:43:18.905761 25348 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0704 07:43:18.905766 25348 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0704 07:43:18.906514 25348 net.cpp:148] Setting up res2a_branch2a/bn
I0704 07:43:18.906522 25348 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:43:18.906524 25348 net.cpp:163] Memory required for data: 73319200
I0704 07:43:18.906533 25348 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0704 07:43:18.906538 25348 net.cpp:98] Creating Layer res2a_branch2a/relu
I0704 07:43:18.906543 25348 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0704 07:43:18.906548 25348 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0704 07:43:18.906553 25348 net.cpp:148] Setting up res2a_branch2a/relu
I0704 07:43:18.906558 25348 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:43:18.906561 25348 net.cpp:163] Memory required for data: 86426400
I0704 07:43:18.906565 25348 layer_factory.hpp:77] Creating layer res2a_branch2b
I0704 07:43:18.906572 25348 net.cpp:98] Creating Layer res2a_branch2b
I0704 07:43:18.906584 25348 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0704 07:43:18.906589 25348 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0704 07:43:18.907138 25348 net.cpp:148] Setting up res2a_branch2b
I0704 07:43:18.907145 25348 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:43:18.907148 25348 net.cpp:163] Memory required for data: 99533600
I0704 07:43:18.907153 25348 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0704 07:43:18.907158 25348 net.cpp:98] Creating Layer res2a_branch2b/bn
I0704 07:43:18.907163 25348 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0704 07:43:18.907168 25348 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0704 07:43:18.907902 25348 net.cpp:148] Setting up res2a_branch2b/bn
I0704 07:43:18.907908 25348 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:43:18.907912 25348 net.cpp:163] Memory required for data: 112640800
I0704 07:43:18.907918 25348 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0704 07:43:18.907923 25348 net.cpp:98] Creating Layer res2a_branch2b/relu
I0704 07:43:18.907927 25348 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0704 07:43:18.907932 25348 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0704 07:43:18.907938 25348 net.cpp:148] Setting up res2a_branch2b/relu
I0704 07:43:18.907943 25348 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 07:43:18.907948 25348 net.cpp:163] Memory required for data: 125748000
I0704 07:43:18.907950 25348 layer_factory.hpp:77] Creating layer pool2
I0704 07:43:18.907956 25348 net.cpp:98] Creating Layer pool2
I0704 07:43:18.907960 25348 net.cpp:439] pool2 <- res2a_branch2b/bn
I0704 07:43:18.907964 25348 net.cpp:413] pool2 -> pool2
I0704 07:43:18.908015 25348 net.cpp:148] Setting up pool2
I0704 07:43:18.908021 25348 net.cpp:155] Top shape: 50 64 16 16 (819200)
I0704 07:43:18.908025 25348 net.cpp:163] Memory required for data: 129024800
I0704 07:43:18.908030 25348 layer_factory.hpp:77] Creating layer res3a_branch2a
I0704 07:43:18.908035 25348 net.cpp:98] Creating Layer res3a_branch2a
I0704 07:43:18.908041 25348 net.cpp:439] res3a_branch2a <- pool2
I0704 07:43:18.908046 25348 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0704 07:43:18.910835 25348 net.cpp:148] Setting up res3a_branch2a
I0704 07:43:18.910845 25348 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:43:18.910847 25348 net.cpp:163] Memory required for data: 135578400
I0704 07:43:18.910853 25348 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0704 07:43:18.910859 25348 net.cpp:98] Creating Layer res3a_branch2a/bn
I0704 07:43:18.910864 25348 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0704 07:43:18.910871 25348 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0704 07:43:18.911497 25348 net.cpp:148] Setting up res3a_branch2a/bn
I0704 07:43:18.911505 25348 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:43:18.911507 25348 net.cpp:163] Memory required for data: 142132000
I0704 07:43:18.911516 25348 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0704 07:43:18.911523 25348 net.cpp:98] Creating Layer res3a_branch2a/relu
I0704 07:43:18.911527 25348 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0704 07:43:18.911532 25348 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0704 07:43:18.911538 25348 net.cpp:148] Setting up res3a_branch2a/relu
I0704 07:43:18.911543 25348 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:43:18.911547 25348 net.cpp:163] Memory required for data: 148685600
I0704 07:43:18.911551 25348 layer_factory.hpp:77] Creating layer res3a_branch2b
I0704 07:43:18.911558 25348 net.cpp:98] Creating Layer res3a_branch2b
I0704 07:43:18.911562 25348 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0704 07:43:18.911567 25348 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0704 07:43:18.912601 25348 net.cpp:148] Setting up res3a_branch2b
I0704 07:43:18.912608 25348 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:43:18.912611 25348 net.cpp:163] Memory required for data: 155239200
I0704 07:43:18.912616 25348 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0704 07:43:18.912631 25348 net.cpp:98] Creating Layer res3a_branch2b/bn
I0704 07:43:18.912634 25348 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0704 07:43:18.912641 25348 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0704 07:43:18.913298 25348 net.cpp:148] Setting up res3a_branch2b/bn
I0704 07:43:18.913305 25348 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:43:18.913308 25348 net.cpp:163] Memory required for data: 161792800
I0704 07:43:18.913314 25348 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0704 07:43:18.913321 25348 net.cpp:98] Creating Layer res3a_branch2b/relu
I0704 07:43:18.913324 25348 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0704 07:43:18.913328 25348 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0704 07:43:18.913336 25348 net.cpp:148] Setting up res3a_branch2b/relu
I0704 07:43:18.913341 25348 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:43:18.913343 25348 net.cpp:163] Memory required for data: 168346400
I0704 07:43:18.913347 25348 layer_factory.hpp:77] Creating layer pool3
I0704 07:43:18.913352 25348 net.cpp:98] Creating Layer pool3
I0704 07:43:18.913357 25348 net.cpp:439] pool3 <- res3a_branch2b/bn
I0704 07:43:18.913360 25348 net.cpp:413] pool3 -> pool3
I0704 07:43:18.913413 25348 net.cpp:148] Setting up pool3
I0704 07:43:18.913419 25348 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 07:43:18.913422 25348 net.cpp:163] Memory required for data: 174900000
I0704 07:43:18.913426 25348 layer_factory.hpp:77] Creating layer res4a_branch2a
I0704 07:43:18.913434 25348 net.cpp:98] Creating Layer res4a_branch2a
I0704 07:43:18.913437 25348 net.cpp:439] res4a_branch2a <- pool3
I0704 07:43:18.913442 25348 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0704 07:43:18.919603 25348 net.cpp:148] Setting up res4a_branch2a
I0704 07:43:18.919611 25348 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:43:18.919613 25348 net.cpp:163] Memory required for data: 188007200
I0704 07:43:18.919617 25348 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0704 07:43:18.919622 25348 net.cpp:98] Creating Layer res4a_branch2a/bn
I0704 07:43:18.919625 25348 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0704 07:43:18.919627 25348 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0704 07:43:18.920300 25348 net.cpp:148] Setting up res4a_branch2a/bn
I0704 07:43:18.920306 25348 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:43:18.920308 25348 net.cpp:163] Memory required for data: 201114400
I0704 07:43:18.920313 25348 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0704 07:43:18.920317 25348 net.cpp:98] Creating Layer res4a_branch2a/relu
I0704 07:43:18.920320 25348 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0704 07:43:18.920322 25348 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0704 07:43:18.920326 25348 net.cpp:148] Setting up res4a_branch2a/relu
I0704 07:43:18.920328 25348 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:43:18.920331 25348 net.cpp:163] Memory required for data: 214221600
I0704 07:43:18.920333 25348 layer_factory.hpp:77] Creating layer res4a_branch2b
I0704 07:43:18.920336 25348 net.cpp:98] Creating Layer res4a_branch2b
I0704 07:43:18.920341 25348 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0704 07:43:18.920346 25348 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0704 07:43:18.923609 25348 net.cpp:148] Setting up res4a_branch2b
I0704 07:43:18.923616 25348 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:43:18.923619 25348 net.cpp:163] Memory required for data: 227328800
I0704 07:43:18.923624 25348 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0704 07:43:18.923629 25348 net.cpp:98] Creating Layer res4a_branch2b/bn
I0704 07:43:18.923633 25348 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0704 07:43:18.923641 25348 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0704 07:43:18.924309 25348 net.cpp:148] Setting up res4a_branch2b/bn
I0704 07:43:18.924314 25348 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:43:18.924324 25348 net.cpp:163] Memory required for data: 240436000
I0704 07:43:18.924329 25348 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0704 07:43:18.924332 25348 net.cpp:98] Creating Layer res4a_branch2b/relu
I0704 07:43:18.924335 25348 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0704 07:43:18.924338 25348 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0704 07:43:18.924342 25348 net.cpp:148] Setting up res4a_branch2b/relu
I0704 07:43:18.924345 25348 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 07:43:18.924346 25348 net.cpp:163] Memory required for data: 253543200
I0704 07:43:18.924350 25348 layer_factory.hpp:77] Creating layer pool4
I0704 07:43:18.924355 25348 net.cpp:98] Creating Layer pool4
I0704 07:43:18.924358 25348 net.cpp:439] pool4 <- res4a_branch2b/bn
I0704 07:43:18.924363 25348 net.cpp:413] pool4 -> pool4
I0704 07:43:18.924415 25348 net.cpp:148] Setting up pool4
I0704 07:43:18.924422 25348 net.cpp:155] Top shape: 50 256 8 8 (819200)
I0704 07:43:18.924425 25348 net.cpp:163] Memory required for data: 256820000
I0704 07:43:18.924429 25348 layer_factory.hpp:77] Creating layer res5a_branch2a
I0704 07:43:18.924435 25348 net.cpp:98] Creating Layer res5a_branch2a
I0704 07:43:18.924439 25348 net.cpp:439] res5a_branch2a <- pool4
I0704 07:43:18.924444 25348 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0704 07:43:18.949861 25348 net.cpp:148] Setting up res5a_branch2a
I0704 07:43:18.949882 25348 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:43:18.949884 25348 net.cpp:163] Memory required for data: 263373600
I0704 07:43:18.949892 25348 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0704 07:43:18.949911 25348 net.cpp:98] Creating Layer res5a_branch2a/bn
I0704 07:43:18.949916 25348 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0704 07:43:18.949923 25348 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0704 07:43:18.950729 25348 net.cpp:148] Setting up res5a_branch2a/bn
I0704 07:43:18.950743 25348 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:43:18.950747 25348 net.cpp:163] Memory required for data: 269927200
I0704 07:43:18.950754 25348 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0704 07:43:18.950762 25348 net.cpp:98] Creating Layer res5a_branch2a/relu
I0704 07:43:18.950767 25348 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0704 07:43:18.950772 25348 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0704 07:43:18.950778 25348 net.cpp:148] Setting up res5a_branch2a/relu
I0704 07:43:18.950783 25348 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:43:18.950788 25348 net.cpp:163] Memory required for data: 276480800
I0704 07:43:18.950791 25348 layer_factory.hpp:77] Creating layer res5a_branch2b
I0704 07:43:18.950799 25348 net.cpp:98] Creating Layer res5a_branch2b
I0704 07:43:18.950803 25348 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0704 07:43:18.950809 25348 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0704 07:43:18.964371 25348 net.cpp:148] Setting up res5a_branch2b
I0704 07:43:18.964390 25348 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:43:18.964392 25348 net.cpp:163] Memory required for data: 283034400
I0704 07:43:18.964401 25348 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0704 07:43:18.964409 25348 net.cpp:98] Creating Layer res5a_branch2b/bn
I0704 07:43:18.964412 25348 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0704 07:43:18.964416 25348 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0704 07:43:18.965201 25348 net.cpp:148] Setting up res5a_branch2b/bn
I0704 07:43:18.965210 25348 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:43:18.965212 25348 net.cpp:163] Memory required for data: 289588000
I0704 07:43:18.965217 25348 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0704 07:43:18.965221 25348 net.cpp:98] Creating Layer res5a_branch2b/relu
I0704 07:43:18.965224 25348 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0704 07:43:18.965226 25348 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0704 07:43:18.965240 25348 net.cpp:148] Setting up res5a_branch2b/relu
I0704 07:43:18.965242 25348 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 07:43:18.965245 25348 net.cpp:163] Memory required for data: 296141600
I0704 07:43:18.965246 25348 layer_factory.hpp:77] Creating layer pool5
I0704 07:43:18.965250 25348 net.cpp:98] Creating Layer pool5
I0704 07:43:18.965252 25348 net.cpp:439] pool5 <- res5a_branch2b/bn
I0704 07:43:18.965255 25348 net.cpp:413] pool5 -> pool5
I0704 07:43:18.965282 25348 net.cpp:148] Setting up pool5
I0704 07:43:18.965288 25348 net.cpp:155] Top shape: 50 512 1 1 (25600)
I0704 07:43:18.965293 25348 net.cpp:163] Memory required for data: 296244000
I0704 07:43:18.965297 25348 layer_factory.hpp:77] Creating layer fc10
I0704 07:43:18.965307 25348 net.cpp:98] Creating Layer fc10
I0704 07:43:18.965312 25348 net.cpp:439] fc10 <- pool5
I0704 07:43:18.965317 25348 net.cpp:413] fc10 -> fc10
I0704 07:43:18.965646 25348 net.cpp:148] Setting up fc10
I0704 07:43:18.965653 25348 net.cpp:155] Top shape: 50 10 (500)
I0704 07:43:18.965657 25348 net.cpp:163] Memory required for data: 296246000
I0704 07:43:18.965663 25348 layer_factory.hpp:77] Creating layer fc10_fc10_0_split
I0704 07:43:18.965669 25348 net.cpp:98] Creating Layer fc10_fc10_0_split
I0704 07:43:18.965673 25348 net.cpp:439] fc10_fc10_0_split <- fc10
I0704 07:43:18.965678 25348 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0704 07:43:18.965684 25348 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0704 07:43:18.965690 25348 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0704 07:43:18.965768 25348 net.cpp:148] Setting up fc10_fc10_0_split
I0704 07:43:18.965775 25348 net.cpp:155] Top shape: 50 10 (500)
I0704 07:43:18.965780 25348 net.cpp:155] Top shape: 50 10 (500)
I0704 07:43:18.965783 25348 net.cpp:155] Top shape: 50 10 (500)
I0704 07:43:18.965787 25348 net.cpp:163] Memory required for data: 296252000
I0704 07:43:18.965791 25348 layer_factory.hpp:77] Creating layer loss
I0704 07:43:18.965796 25348 net.cpp:98] Creating Layer loss
I0704 07:43:18.965801 25348 net.cpp:439] loss <- fc10_fc10_0_split_0
I0704 07:43:18.965806 25348 net.cpp:439] loss <- label_data_1_split_0
I0704 07:43:18.965811 25348 net.cpp:413] loss -> loss
I0704 07:43:18.965818 25348 layer_factory.hpp:77] Creating layer loss
I0704 07:43:18.965968 25348 net.cpp:148] Setting up loss
I0704 07:43:18.965976 25348 net.cpp:155] Top shape: (1)
I0704 07:43:18.965981 25348 net.cpp:158]     with loss weight 1
I0704 07:43:18.965991 25348 net.cpp:163] Memory required for data: 296252004
I0704 07:43:18.965994 25348 layer_factory.hpp:77] Creating layer accuracy/top1
I0704 07:43:18.966006 25348 net.cpp:98] Creating Layer accuracy/top1
I0704 07:43:18.966011 25348 net.cpp:439] accuracy/top1 <- fc10_fc10_0_split_1
I0704 07:43:18.966015 25348 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0704 07:43:18.966020 25348 net.cpp:413] accuracy/top1 -> accuracy/top1
I0704 07:43:18.966028 25348 net.cpp:148] Setting up accuracy/top1
I0704 07:43:18.966033 25348 net.cpp:155] Top shape: (1)
I0704 07:43:18.966037 25348 net.cpp:163] Memory required for data: 296252008
I0704 07:43:18.966042 25348 layer_factory.hpp:77] Creating layer accuracy/top5
I0704 07:43:18.966048 25348 net.cpp:98] Creating Layer accuracy/top5
I0704 07:43:18.966051 25348 net.cpp:439] accuracy/top5 <- fc10_fc10_0_split_2
I0704 07:43:18.966056 25348 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0704 07:43:18.966061 25348 net.cpp:413] accuracy/top5 -> accuracy/top5
I0704 07:43:18.966068 25348 net.cpp:148] Setting up accuracy/top5
I0704 07:43:18.966073 25348 net.cpp:155] Top shape: (1)
I0704 07:43:18.966078 25348 net.cpp:163] Memory required for data: 296252012
I0704 07:43:18.966080 25348 net.cpp:226] accuracy/top5 does not need backward computation.
I0704 07:43:18.966085 25348 net.cpp:226] accuracy/top1 does not need backward computation.
I0704 07:43:18.966089 25348 net.cpp:224] loss needs backward computation.
I0704 07:43:18.966094 25348 net.cpp:224] fc10_fc10_0_split needs backward computation.
I0704 07:43:18.966097 25348 net.cpp:224] fc10 needs backward computation.
I0704 07:43:18.966109 25348 net.cpp:224] pool5 needs backward computation.
I0704 07:43:18.966114 25348 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0704 07:43:18.966116 25348 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0704 07:43:18.966120 25348 net.cpp:224] res5a_branch2b needs backward computation.
I0704 07:43:18.966125 25348 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0704 07:43:18.966128 25348 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0704 07:43:18.966132 25348 net.cpp:224] res5a_branch2a needs backward computation.
I0704 07:43:18.966136 25348 net.cpp:224] pool4 needs backward computation.
I0704 07:43:18.966140 25348 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0704 07:43:18.966145 25348 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0704 07:43:18.966150 25348 net.cpp:224] res4a_branch2b needs backward computation.
I0704 07:43:18.966153 25348 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0704 07:43:18.966157 25348 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0704 07:43:18.966161 25348 net.cpp:224] res4a_branch2a needs backward computation.
I0704 07:43:18.966166 25348 net.cpp:224] pool3 needs backward computation.
I0704 07:43:18.966169 25348 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0704 07:43:18.966173 25348 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0704 07:43:18.966177 25348 net.cpp:224] res3a_branch2b needs backward computation.
I0704 07:43:18.966181 25348 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0704 07:43:18.966186 25348 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0704 07:43:18.966189 25348 net.cpp:224] res3a_branch2a needs backward computation.
I0704 07:43:18.966193 25348 net.cpp:224] pool2 needs backward computation.
I0704 07:43:18.966197 25348 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0704 07:43:18.966200 25348 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0704 07:43:18.966204 25348 net.cpp:224] res2a_branch2b needs backward computation.
I0704 07:43:18.966209 25348 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0704 07:43:18.966212 25348 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0704 07:43:18.966217 25348 net.cpp:224] res2a_branch2a needs backward computation.
I0704 07:43:18.966220 25348 net.cpp:224] pool1 needs backward computation.
I0704 07:43:18.966224 25348 net.cpp:224] conv1b/relu needs backward computation.
I0704 07:43:18.966228 25348 net.cpp:224] conv1b/bn needs backward computation.
I0704 07:43:18.966233 25348 net.cpp:224] conv1b needs backward computation.
I0704 07:43:18.966238 25348 net.cpp:224] conv1a/relu needs backward computation.
I0704 07:43:18.966240 25348 net.cpp:224] conv1a/bn needs backward computation.
I0704 07:43:18.966244 25348 net.cpp:224] conv1a needs backward computation.
I0704 07:43:18.966248 25348 net.cpp:226] data/bias does not need backward computation.
I0704 07:43:18.966253 25348 net.cpp:226] label_data_1_split does not need backward computation.
I0704 07:43:18.966259 25348 net.cpp:226] data does not need backward computation.
I0704 07:43:18.966261 25348 net.cpp:268] This network produces output accuracy/top1
I0704 07:43:18.966265 25348 net.cpp:268] This network produces output accuracy/top5
I0704 07:43:18.966269 25348 net.cpp:268] This network produces output loss
I0704 07:43:18.966297 25348 net.cpp:288] Network initialization done.
I0704 07:43:18.966397 25348 solver.cpp:60] Solver scaffolding done.
I0704 07:43:18.970155 25348 caffe.cpp:145] Finetuning from training/cifar10_jacintonet11v2_2017-07-04_07-19-29/initial/cifar10_jacintonet11v2_iter_64000.caffemodel
I0704 07:43:18.998423 25348 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0704 07:43:18.998492 25348 data_layer.cpp:83] output data size: 21,3,32,32
I0704 07:43:19.464810 25348 data_layer.cpp:78] ReshapePrefetch 21, 3, 32, 32
I0704 07:43:19.464876 25348 data_layer.cpp:83] output data size: 21,3,32,32
I0704 07:43:19.959833 25348 parallel.cpp:334] Starting Optimization
I0704 07:43:19.959899 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:43:19.970299 25348 solver.cpp:408] Solving jacintonet11v2_train
I0704 07:43:19.970319 25348 solver.cpp:409] Learning Rate Policy: poly
I0704 07:43:19.972193 25348 solver.cpp:466] Iteration 0, Testing net (#0)
I0704 07:43:21.647377 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9171
I0704 07:43:21.647399 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9974
I0704 07:43:21.647405 25348 solver.cpp:539]     Test net output #2: loss = 0.2066 (* 1 = 0.2066 loss)
I0704 07:43:21.756060 25348 solver.cpp:290] Iteration 0 (0 iter/s, 1.78567s/100 iter), loss = 0
I0704 07:43:21.756083 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:21.756089 25348 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0704 07:43:23.826246 25348 solver.cpp:290] Iteration 100 (48.3069 iter/s, 2.0701s/100 iter), loss = 0
I0704 07:43:23.826277 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:23.826287 25348 sgd_solver.cpp:106] Iteration 100, lr = 0.00998437
I0704 07:43:25.884728 25348 solver.cpp:290] Iteration 200 (48.5816 iter/s, 2.05839s/100 iter), loss = 0
I0704 07:43:25.884750 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:25.884757 25348 sgd_solver.cpp:106] Iteration 200, lr = 0.00996875
I0704 07:43:27.942723 25348 solver.cpp:290] Iteration 300 (48.5931 iter/s, 2.05791s/100 iter), loss = 0
I0704 07:43:27.942744 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:27.942751 25348 sgd_solver.cpp:106] Iteration 300, lr = 0.00995312
I0704 07:43:30.006409 25348 solver.cpp:290] Iteration 400 (48.459 iter/s, 2.0636s/100 iter), loss = 0
I0704 07:43:30.006431 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:30.006438 25348 sgd_solver.cpp:106] Iteration 400, lr = 0.0099375
I0704 07:43:32.065022 25348 solver.cpp:290] Iteration 500 (48.5785 iter/s, 2.05853s/100 iter), loss = 0
I0704 07:43:32.065047 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:32.065054 25348 sgd_solver.cpp:106] Iteration 500, lr = 0.00992187
I0704 07:43:34.139252 25348 solver.cpp:290] Iteration 600 (48.2127 iter/s, 2.07414s/100 iter), loss = 0
I0704 07:43:34.139274 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:34.139283 25348 sgd_solver.cpp:106] Iteration 600, lr = 0.00990625
I0704 07:43:36.198233 25348 solver.cpp:290] Iteration 700 (48.5697 iter/s, 2.0589s/100 iter), loss = 0
I0704 07:43:36.198256 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:36.198262 25348 sgd_solver.cpp:106] Iteration 700, lr = 0.00989062
I0704 07:43:38.260052 25348 solver.cpp:290] Iteration 800 (48.5029 iter/s, 2.06173s/100 iter), loss = 0
I0704 07:43:38.260074 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:38.260083 25348 sgd_solver.cpp:106] Iteration 800, lr = 0.009875
I0704 07:43:40.317914 25348 solver.cpp:290] Iteration 900 (48.5961 iter/s, 2.05778s/100 iter), loss = 0
I0704 07:43:40.317936 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:40.317944 25348 sgd_solver.cpp:106] Iteration 900, lr = 0.00985937
I0704 07:43:42.354902 25348 solver.cpp:354] Sparsity after update:
I0704 07:43:42.356168 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:43:42.356176 25348 net.cpp:1851] conv1a_param_0(0) 
I0704 07:43:42.356189 25348 net.cpp:1851] conv1b_param_0(0) 
I0704 07:43:42.356194 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:43:42.356196 25348 net.cpp:1851] res2a_branch2a_param_0(0) 
I0704 07:43:42.356200 25348 net.cpp:1851] res2a_branch2b_param_0(0) 
I0704 07:43:42.356204 25348 net.cpp:1851] res3a_branch2a_param_0(0) 
I0704 07:43:42.356209 25348 net.cpp:1851] res3a_branch2b_param_0(0) 
I0704 07:43:42.356211 25348 net.cpp:1851] res4a_branch2a_param_0(0) 
I0704 07:43:42.356227 25348 net.cpp:1851] res4a_branch2b_param_0(0) 
I0704 07:43:42.356231 25348 net.cpp:1851] res5a_branch2a_param_0(0) 
I0704 07:43:42.356235 25348 net.cpp:1851] res5a_branch2b_param_0(0) 
I0704 07:43:42.356240 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0704 07:43:42.356329 25348 solver.cpp:466] Iteration 1000, Testing net (#0)
I0704 07:43:43.999199 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9143
I0704 07:43:43.999220 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9977
I0704 07:43:43.999228 25348 solver.cpp:539]     Test net output #2: loss = 0.1789 (* 1 = 0.1789 loss)
I0704 07:43:44.019098 25348 solver.cpp:290] Iteration 1000 (27.0193 iter/s, 3.70105s/100 iter), loss = 0
I0704 07:43:44.019125 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:44.019134 25348 sgd_solver.cpp:106] Iteration 1000, lr = 0.00984375
I0704 07:43:46.080176 25348 solver.cpp:290] Iteration 1100 (48.5204 iter/s, 2.06099s/100 iter), loss = 0
I0704 07:43:46.080196 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:46.080204 25348 sgd_solver.cpp:106] Iteration 1100, lr = 0.00982813
I0704 07:43:48.137805 25348 solver.cpp:290] Iteration 1200 (48.6016 iter/s, 2.05754s/100 iter), loss = 0
I0704 07:43:48.137827 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:48.137835 25348 sgd_solver.cpp:106] Iteration 1200, lr = 0.0098125
I0704 07:43:50.194833 25348 solver.cpp:290] Iteration 1300 (48.6158 iter/s, 2.05694s/100 iter), loss = 0
I0704 07:43:50.196455 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:50.196465 25348 sgd_solver.cpp:106] Iteration 1300, lr = 0.00979687
I0704 07:43:52.256178 25348 solver.cpp:290] Iteration 1400 (48.5516 iter/s, 2.05966s/100 iter), loss = 0
I0704 07:43:52.256201 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:52.256207 25348 sgd_solver.cpp:106] Iteration 1400, lr = 0.00978125
I0704 07:43:54.312544 25348 solver.cpp:290] Iteration 1500 (48.6315 iter/s, 2.05628s/100 iter), loss = 0
I0704 07:43:54.312567 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:54.312573 25348 sgd_solver.cpp:106] Iteration 1500, lr = 0.00976562
I0704 07:43:56.370326 25348 solver.cpp:290] Iteration 1600 (48.598 iter/s, 2.0577s/100 iter), loss = 0
I0704 07:43:56.370348 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:56.370355 25348 sgd_solver.cpp:106] Iteration 1600, lr = 0.00975
I0704 07:43:58.426820 25348 solver.cpp:290] Iteration 1700 (48.6285 iter/s, 2.05641s/100 iter), loss = 0
I0704 07:43:58.426842 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:43:58.426849 25348 sgd_solver.cpp:106] Iteration 1700, lr = 0.00973437
I0704 07:44:00.484913 25348 solver.cpp:290] Iteration 1800 (48.5907 iter/s, 2.05801s/100 iter), loss = 0
I0704 07:44:00.484935 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:00.484941 25348 sgd_solver.cpp:106] Iteration 1800, lr = 0.00971875
I0704 07:44:02.545107 25348 solver.cpp:290] Iteration 1900 (48.5411 iter/s, 2.06011s/100 iter), loss = 0
I0704 07:44:02.545130 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:02.545137 25348 sgd_solver.cpp:106] Iteration 1900, lr = 0.00970312
I0704 07:44:04.582180 25348 solver.cpp:354] Sparsity after update:
I0704 07:44:04.583443 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:44:04.583451 25348 net.cpp:1851] conv1a_param_0(0) 
I0704 07:44:04.583458 25348 net.cpp:1851] conv1b_param_0(0) 
I0704 07:44:04.583461 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:44:04.583464 25348 net.cpp:1851] res2a_branch2a_param_0(0) 
I0704 07:44:04.583467 25348 net.cpp:1851] res2a_branch2b_param_0(0) 
I0704 07:44:04.583468 25348 net.cpp:1851] res3a_branch2a_param_0(0) 
I0704 07:44:04.583472 25348 net.cpp:1851] res3a_branch2b_param_0(0) 
I0704 07:44:04.583473 25348 net.cpp:1851] res4a_branch2a_param_0(0) 
I0704 07:44:04.583475 25348 net.cpp:1851] res4a_branch2b_param_0(0) 
I0704 07:44:04.583477 25348 net.cpp:1851] res5a_branch2a_param_0(0) 
I0704 07:44:04.583479 25348 net.cpp:1851] res5a_branch2b_param_0(0) 
I0704 07:44:04.583482 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0704 07:44:04.583614 25348 solver.cpp:466] Iteration 2000, Testing net (#0)
I0704 07:44:06.224511 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.914
I0704 07:44:06.224531 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9976
I0704 07:44:06.224536 25348 solver.cpp:539]     Test net output #2: loss = 0.1599 (* 1 = 0.1599 loss)
I0704 07:44:06.244096 25348 solver.cpp:290] Iteration 2000 (27.0353 iter/s, 3.69886s/100 iter), loss = 0
I0704 07:44:06.244112 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:06.244125 25348 sgd_solver.cpp:106] Iteration 2000, lr = 0.0096875
I0704 07:44:08.305510 25348 solver.cpp:290] Iteration 2100 (48.5123 iter/s, 2.06133s/100 iter), loss = 0
I0704 07:44:08.305532 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:08.305539 25348 sgd_solver.cpp:106] Iteration 2100, lr = 0.00967188
I0704 07:44:10.372706 25348 solver.cpp:290] Iteration 2200 (48.3767 iter/s, 2.06711s/100 iter), loss = 0
I0704 07:44:10.372730 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:10.372736 25348 sgd_solver.cpp:106] Iteration 2200, lr = 0.00965625
I0704 07:44:12.432227 25348 solver.cpp:290] Iteration 2300 (48.557 iter/s, 2.05943s/100 iter), loss = 0
I0704 07:44:12.432265 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:12.432271 25348 sgd_solver.cpp:106] Iteration 2300, lr = 0.00964062
I0704 07:44:14.489509 25348 solver.cpp:290] Iteration 2400 (48.6102 iter/s, 2.05718s/100 iter), loss = 0
I0704 07:44:14.489531 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:14.489537 25348 sgd_solver.cpp:106] Iteration 2400, lr = 0.009625
I0704 07:44:16.549288 25348 solver.cpp:290] Iteration 2500 (48.5509 iter/s, 2.05969s/100 iter), loss = 0
I0704 07:44:16.549311 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:16.549317 25348 sgd_solver.cpp:106] Iteration 2500, lr = 0.00960938
I0704 07:44:18.613245 25348 solver.cpp:290] Iteration 2600 (48.4526 iter/s, 2.06387s/100 iter), loss = 0
I0704 07:44:18.613267 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:18.613275 25348 sgd_solver.cpp:106] Iteration 2600, lr = 0.00959375
I0704 07:44:20.672076 25348 solver.cpp:290] Iteration 2700 (48.5733 iter/s, 2.05875s/100 iter), loss = 0
I0704 07:44:20.672139 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:20.672147 25348 sgd_solver.cpp:106] Iteration 2700, lr = 0.00957812
I0704 07:44:22.728775 25348 solver.cpp:290] Iteration 2800 (48.6246 iter/s, 2.05657s/100 iter), loss = 0
I0704 07:44:22.728797 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:22.728803 25348 sgd_solver.cpp:106] Iteration 2800, lr = 0.0095625
I0704 07:44:24.789860 25348 solver.cpp:290] Iteration 2900 (48.5202 iter/s, 2.061s/100 iter), loss = 0
I0704 07:44:24.789881 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:24.789890 25348 sgd_solver.cpp:106] Iteration 2900, lr = 0.00954687
I0704 07:44:26.827599 25348 solver.cpp:354] Sparsity after update:
I0704 07:44:26.828850 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:44:26.828857 25348 net.cpp:1851] conv1a_param_0(0) 
I0704 07:44:26.828867 25348 net.cpp:1851] conv1b_param_0(0) 
I0704 07:44:26.828871 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:44:26.828876 25348 net.cpp:1851] res2a_branch2a_param_0(0) 
I0704 07:44:26.828879 25348 net.cpp:1851] res2a_branch2b_param_0(0) 
I0704 07:44:26.828883 25348 net.cpp:1851] res3a_branch2a_param_0(0) 
I0704 07:44:26.828887 25348 net.cpp:1851] res3a_branch2b_param_0(0) 
I0704 07:44:26.828891 25348 net.cpp:1851] res4a_branch2a_param_0(0) 
I0704 07:44:26.828894 25348 net.cpp:1851] res4a_branch2b_param_0(0) 
I0704 07:44:26.828898 25348 net.cpp:1851] res5a_branch2a_param_0(0) 
I0704 07:44:26.828902 25348 net.cpp:1851] res5a_branch2b_param_0(0) 
I0704 07:44:26.828907 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0704 07:44:26.828999 25348 solver.cpp:466] Iteration 3000, Testing net (#0)
I0704 07:44:28.473506 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9142
I0704 07:44:28.473525 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9975
I0704 07:44:28.473529 25348 solver.cpp:539]     Test net output #2: loss = 0.1568 (* 1 = 0.1568 loss)
I0704 07:44:28.494060 25348 solver.cpp:290] Iteration 3000 (26.9973 iter/s, 3.70407s/100 iter), loss = 0
I0704 07:44:28.494077 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:28.494091 25348 sgd_solver.cpp:106] Iteration 3000, lr = 0.00953125
I0704 07:44:30.554286 25348 solver.cpp:290] Iteration 3100 (48.5403 iter/s, 2.06014s/100 iter), loss = 0
I0704 07:44:30.554308 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:30.554316 25348 sgd_solver.cpp:106] Iteration 3100, lr = 0.00951563
I0704 07:44:32.613104 25348 solver.cpp:290] Iteration 3200 (48.5735 iter/s, 2.05873s/100 iter), loss = 0
I0704 07:44:32.613127 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:32.613134 25348 sgd_solver.cpp:106] Iteration 3200, lr = 0.0095
I0704 07:44:34.688091 25348 solver.cpp:290] Iteration 3300 (48.1951 iter/s, 2.0749s/100 iter), loss = 0
I0704 07:44:34.688113 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:34.688122 25348 sgd_solver.cpp:106] Iteration 3300, lr = 0.00948437
I0704 07:44:36.756675 25348 solver.cpp:290] Iteration 3400 (48.3443 iter/s, 2.0685s/100 iter), loss = 0
I0704 07:44:36.756696 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:36.756703 25348 sgd_solver.cpp:106] Iteration 3400, lr = 0.00946875
I0704 07:44:38.816352 25348 solver.cpp:290] Iteration 3500 (48.5533 iter/s, 2.05959s/100 iter), loss = 0
I0704 07:44:38.816375 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:38.816383 25348 sgd_solver.cpp:106] Iteration 3500, lr = 0.00945312
I0704 07:44:40.872460 25348 solver.cpp:290] Iteration 3600 (48.6376 iter/s, 2.05602s/100 iter), loss = 0
I0704 07:44:40.872481 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:40.872488 25348 sgd_solver.cpp:106] Iteration 3600, lr = 0.0094375
I0704 07:44:42.936831 25348 solver.cpp:290] Iteration 3700 (48.4429 iter/s, 2.06429s/100 iter), loss = 0
I0704 07:44:42.936882 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:42.936893 25348 sgd_solver.cpp:106] Iteration 3700, lr = 0.00942187
I0704 07:44:44.995880 25348 solver.cpp:290] Iteration 3800 (48.5687 iter/s, 2.05894s/100 iter), loss = 0
I0704 07:44:44.995909 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:44.995918 25348 sgd_solver.cpp:106] Iteration 3800, lr = 0.00940625
I0704 07:44:47.054013 25348 solver.cpp:290] Iteration 3900 (48.59 iter/s, 2.05804s/100 iter), loss = 0
I0704 07:44:47.054049 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:47.054059 25348 sgd_solver.cpp:106] Iteration 3900, lr = 0.00939062
I0704 07:44:49.091100 25348 solver.cpp:354] Sparsity after update:
I0704 07:44:49.092226 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:44:49.092233 25348 net.cpp:1851] conv1a_param_0(0) 
I0704 07:44:49.092239 25348 net.cpp:1851] conv1b_param_0(0) 
I0704 07:44:49.092242 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:44:49.092243 25348 net.cpp:1851] res2a_branch2a_param_0(0) 
I0704 07:44:49.092245 25348 net.cpp:1851] res2a_branch2b_param_0(0) 
I0704 07:44:49.092247 25348 net.cpp:1851] res3a_branch2a_param_0(0) 
I0704 07:44:49.092249 25348 net.cpp:1851] res3a_branch2b_param_0(0) 
I0704 07:44:49.092252 25348 net.cpp:1851] res4a_branch2a_param_0(0) 
I0704 07:44:49.092253 25348 net.cpp:1851] res4a_branch2b_param_0(0) 
I0704 07:44:49.092255 25348 net.cpp:1851] res5a_branch2a_param_0(0) 
I0704 07:44:49.092257 25348 net.cpp:1851] res5a_branch2b_param_0(0) 
I0704 07:44:49.092259 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0704 07:44:49.092345 25348 solver.cpp:466] Iteration 4000, Testing net (#0)
I0704 07:44:50.733412 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.917
I0704 07:44:50.733518 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9969
I0704 07:44:50.733525 25348 solver.cpp:539]     Test net output #2: loss = 0.1545 (* 1 = 0.1545 loss)
I0704 07:44:50.754206 25348 solver.cpp:290] Iteration 4000 (27.0266 iter/s, 3.70006s/100 iter), loss = 0
I0704 07:44:50.754222 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:50.754236 25348 sgd_solver.cpp:106] Iteration 4000, lr = 0.009375
I0704 07:44:50.754789 25348 solver.cpp:375] Finding and applying sparsity: 0
I0704 07:44:50.779611 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:44:52.870257 25348 solver.cpp:290] Iteration 4100 (47.2597 iter/s, 2.11597s/100 iter), loss = 0
I0704 07:44:52.870280 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:52.870285 25348 sgd_solver.cpp:106] Iteration 4100, lr = 0.00935937
I0704 07:44:54.943976 25348 solver.cpp:290] Iteration 4200 (48.2245 iter/s, 2.07363s/100 iter), loss = 0
I0704 07:44:54.944000 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:54.944008 25348 sgd_solver.cpp:106] Iteration 4200, lr = 0.00934375
I0704 07:44:57.020900 25348 solver.cpp:290] Iteration 4300 (48.1501 iter/s, 2.07684s/100 iter), loss = 0
I0704 07:44:57.020920 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:57.020927 25348 sgd_solver.cpp:106] Iteration 4300, lr = 0.00932813
I0704 07:44:59.093367 25348 solver.cpp:290] Iteration 4400 (48.2537 iter/s, 2.07238s/100 iter), loss = 0
I0704 07:44:59.093394 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:44:59.093402 25348 sgd_solver.cpp:106] Iteration 4400, lr = 0.0093125
I0704 07:45:01.165673 25348 solver.cpp:290] Iteration 4500 (48.2575 iter/s, 2.07222s/100 iter), loss = 0
I0704 07:45:01.165694 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:01.165702 25348 sgd_solver.cpp:106] Iteration 4500, lr = 0.00929687
I0704 07:45:03.239133 25348 solver.cpp:290] Iteration 4600 (48.2305 iter/s, 2.07337s/100 iter), loss = 0
I0704 07:45:03.239156 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:03.239162 25348 sgd_solver.cpp:106] Iteration 4600, lr = 0.00928125
I0704 07:45:05.311424 25348 solver.cpp:290] Iteration 4700 (48.2578 iter/s, 2.0722s/100 iter), loss = 0
I0704 07:45:05.311447 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:05.311456 25348 sgd_solver.cpp:106] Iteration 4700, lr = 0.00926562
I0704 07:45:07.393344 25348 solver.cpp:290] Iteration 4800 (48.0346 iter/s, 2.08183s/100 iter), loss = 0
I0704 07:45:07.393368 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:07.393376 25348 sgd_solver.cpp:106] Iteration 4800, lr = 0.00925
I0704 07:45:09.463078 25348 solver.cpp:290] Iteration 4900 (48.3174 iter/s, 2.06965s/100 iter), loss = 0
I0704 07:45:09.463101 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:09.463109 25348 sgd_solver.cpp:106] Iteration 4900, lr = 0.00923437
I0704 07:45:11.526651 25348 solver.cpp:354] Sparsity after update:
I0704 07:45:11.528034 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:45:11.528041 25348 net.cpp:1851] conv1a_param_0(0) 
I0704 07:45:11.528048 25348 net.cpp:1851] conv1b_param_0(0) 
I0704 07:45:11.528050 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:45:11.528053 25348 net.cpp:1851] res2a_branch2a_param_0(0) 
I0704 07:45:11.528054 25348 net.cpp:1851] res2a_branch2b_param_0(0) 
I0704 07:45:11.528056 25348 net.cpp:1851] res3a_branch2a_param_0(0) 
I0704 07:45:11.528059 25348 net.cpp:1851] res3a_branch2b_param_0(0) 
I0704 07:45:11.528060 25348 net.cpp:1851] res4a_branch2a_param_0(0) 
I0704 07:45:11.528062 25348 net.cpp:1851] res4a_branch2b_param_0(0) 
I0704 07:45:11.528064 25348 net.cpp:1851] res5a_branch2a_param_0(0) 
I0704 07:45:11.528065 25348 net.cpp:1851] res5a_branch2b_param_0(0) 
I0704 07:45:11.528067 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.3599e+06) 0
I0704 07:45:11.528164 25348 solver.cpp:466] Iteration 5000, Testing net (#0)
I0704 07:45:13.172219 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9122
I0704 07:45:13.172238 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9978
I0704 07:45:13.172245 25348 solver.cpp:539]     Test net output #2: loss = 0.1563 (* 1 = 0.1563 loss)
I0704 07:45:13.192526 25348 solver.cpp:290] Iteration 5000 (26.8146 iter/s, 3.72932s/100 iter), loss = 0
I0704 07:45:13.192543 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:13.192556 25348 sgd_solver.cpp:106] Iteration 5000, lr = 0.00921875
I0704 07:45:13.193089 25348 solver.cpp:375] Finding and applying sparsity: 0.02
I0704 07:45:13.449759 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:45:15.537963 25348 solver.cpp:290] Iteration 5100 (42.6376 iter/s, 2.34535s/100 iter), loss = 0
I0704 07:45:15.537986 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:15.537992 25348 sgd_solver.cpp:106] Iteration 5100, lr = 0.00920312
I0704 07:45:17.613046 25348 solver.cpp:290] Iteration 5200 (48.1929 iter/s, 2.075s/100 iter), loss = 0
I0704 07:45:17.613068 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:17.613075 25348 sgd_solver.cpp:106] Iteration 5200, lr = 0.0091875
I0704 07:45:19.686949 25348 solver.cpp:290] Iteration 5300 (48.2202 iter/s, 2.07382s/100 iter), loss = 0
I0704 07:45:19.686970 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:19.686976 25348 sgd_solver.cpp:106] Iteration 5300, lr = 0.00917188
I0704 07:45:21.759461 25348 solver.cpp:290] Iteration 5400 (48.2526 iter/s, 2.07243s/100 iter), loss = 0
I0704 07:45:21.759538 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:21.759547 25348 sgd_solver.cpp:106] Iteration 5400, lr = 0.00915625
I0704 07:45:23.840195 25348 solver.cpp:290] Iteration 5500 (48.0632 iter/s, 2.0806s/100 iter), loss = 0
I0704 07:45:23.840219 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:23.840224 25348 sgd_solver.cpp:106] Iteration 5500, lr = 0.00914062
I0704 07:45:25.913897 25348 solver.cpp:290] Iteration 5600 (48.2249 iter/s, 2.07362s/100 iter), loss = 0
I0704 07:45:25.913920 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:25.913926 25348 sgd_solver.cpp:106] Iteration 5600, lr = 0.009125
I0704 07:45:27.992074 25348 solver.cpp:290] Iteration 5700 (48.1211 iter/s, 2.07809s/100 iter), loss = 0
I0704 07:45:27.992095 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:27.992101 25348 sgd_solver.cpp:106] Iteration 5700, lr = 0.00910938
I0704 07:45:30.065008 25348 solver.cpp:290] Iteration 5800 (48.2428 iter/s, 2.07285s/100 iter), loss = 0
I0704 07:45:30.065029 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:30.065037 25348 sgd_solver.cpp:106] Iteration 5800, lr = 0.00909375
I0704 07:45:32.141949 25348 solver.cpp:290] Iteration 5900 (48.1497 iter/s, 2.07685s/100 iter), loss = 0
I0704 07:45:32.141969 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:32.141976 25348 sgd_solver.cpp:106] Iteration 5900, lr = 0.00907812
I0704 07:45:34.201381 25348 solver.cpp:354] Sparsity after update:
I0704 07:45:34.202790 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:45:34.202797 25348 net.cpp:1851] conv1a_param_0(0.01) 
I0704 07:45:34.202805 25348 net.cpp:1851] conv1b_param_0(0.02) 
I0704 07:45:34.202807 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:45:34.202810 25348 net.cpp:1851] res2a_branch2a_param_0(0.02) 
I0704 07:45:34.202811 25348 net.cpp:1851] res2a_branch2b_param_0(0.02) 
I0704 07:45:34.202813 25348 net.cpp:1851] res3a_branch2a_param_0(0.02) 
I0704 07:45:34.202816 25348 net.cpp:1851] res3a_branch2b_param_0(0.02) 
I0704 07:45:34.202817 25348 net.cpp:1851] res4a_branch2a_param_0(0.02) 
I0704 07:45:34.202819 25348 net.cpp:1851] res4a_branch2b_param_0(0.02) 
I0704 07:45:34.202821 25348 net.cpp:1851] res5a_branch2a_param_0(0.02) 
I0704 07:45:34.202823 25348 net.cpp:1851] res5a_branch2b_param_0(0.02) 
I0704 07:45:34.202826 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (47061/2.3599e+06) 0.0199
I0704 07:45:34.202919 25348 solver.cpp:466] Iteration 6000, Testing net (#0)
I0704 07:45:35.852412 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9125
I0704 07:45:35.852430 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9968
I0704 07:45:35.852437 25348 solver.cpp:539]     Test net output #2: loss = 0.1561 (* 1 = 0.1561 loss)
I0704 07:45:35.872300 25348 solver.cpp:290] Iteration 6000 (26.808 iter/s, 3.73023s/100 iter), loss = 0
I0704 07:45:35.872318 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:35.872334 25348 sgd_solver.cpp:106] Iteration 6000, lr = 0.0090625
I0704 07:45:35.872882 25348 solver.cpp:375] Finding and applying sparsity: 0.04
I0704 07:45:36.135357 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:45:38.230183 25348 solver.cpp:290] Iteration 6100 (42.4126 iter/s, 2.35779s/100 iter), loss = 0
I0704 07:45:38.230208 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:38.230217 25348 sgd_solver.cpp:106] Iteration 6100, lr = 0.00904687
I0704 07:45:40.302433 25348 solver.cpp:290] Iteration 6200 (48.2588 iter/s, 2.07216s/100 iter), loss = 0
I0704 07:45:40.302458 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:40.302466 25348 sgd_solver.cpp:106] Iteration 6200, lr = 0.00903125
I0704 07:45:42.377840 25348 solver.cpp:290] Iteration 6300 (48.1854 iter/s, 2.07532s/100 iter), loss = 0
I0704 07:45:42.377864 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:42.377890 25348 sgd_solver.cpp:106] Iteration 6300, lr = 0.00901563
I0704 07:45:44.454025 25348 solver.cpp:290] Iteration 6400 (48.1672 iter/s, 2.0761s/100 iter), loss = 0
I0704 07:45:44.454049 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:44.454057 25348 sgd_solver.cpp:106] Iteration 6400, lr = 0.009
I0704 07:45:46.527431 25348 solver.cpp:290] Iteration 6500 (48.2318 iter/s, 2.07332s/100 iter), loss = 0
I0704 07:45:46.527453 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:46.527462 25348 sgd_solver.cpp:106] Iteration 6500, lr = 0.00898437
I0704 07:45:48.599530 25348 solver.cpp:290] Iteration 6600 (48.2623 iter/s, 2.07201s/100 iter), loss = 0
I0704 07:45:48.599553 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:48.599560 25348 sgd_solver.cpp:106] Iteration 6600, lr = 0.00896875
I0704 07:45:50.674157 25348 solver.cpp:290] Iteration 6700 (48.2034 iter/s, 2.07454s/100 iter), loss = 0
I0704 07:45:50.674180 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:50.674187 25348 sgd_solver.cpp:106] Iteration 6700, lr = 0.00895312
I0704 07:45:52.744110 25348 solver.cpp:290] Iteration 6800 (48.3123 iter/s, 2.06987s/100 iter), loss = 0
I0704 07:45:52.744173 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:52.744179 25348 sgd_solver.cpp:106] Iteration 6800, lr = 0.0089375
I0704 07:45:54.816160 25348 solver.cpp:290] Iteration 6900 (48.2643 iter/s, 2.07193s/100 iter), loss = 0
I0704 07:45:54.816182 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:54.816189 25348 sgd_solver.cpp:106] Iteration 6900, lr = 0.00892187
I0704 07:45:56.868746 25348 solver.cpp:354] Sparsity after update:
I0704 07:45:56.870124 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:45:56.870132 25348 net.cpp:1851] conv1a_param_0(0.02) 
I0704 07:45:56.870143 25348 net.cpp:1851] conv1b_param_0(0.0399) 
I0704 07:45:56.870149 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:45:56.870153 25348 net.cpp:1851] res2a_branch2a_param_0(0.04) 
I0704 07:45:56.870158 25348 net.cpp:1851] res2a_branch2b_param_0(0.0399) 
I0704 07:45:56.870162 25348 net.cpp:1851] res3a_branch2a_param_0(0.04) 
I0704 07:45:56.870167 25348 net.cpp:1851] res3a_branch2b_param_0(0.04) 
I0704 07:45:56.870170 25348 net.cpp:1851] res4a_branch2a_param_0(0.04) 
I0704 07:45:56.870174 25348 net.cpp:1851] res4a_branch2b_param_0(0.04) 
I0704 07:45:56.870178 25348 net.cpp:1851] res5a_branch2a_param_0(0.04) 
I0704 07:45:56.870182 25348 net.cpp:1851] res5a_branch2b_param_0(0.04) 
I0704 07:45:56.870187 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (94133/2.3599e+06) 0.0399
I0704 07:45:56.870277 25348 solver.cpp:466] Iteration 7000, Testing net (#0)
I0704 07:45:58.528292 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.906
I0704 07:45:58.528312 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9955
I0704 07:45:58.528318 25348 solver.cpp:539]     Test net output #2: loss = 0.186 (* 1 = 0.186 loss)
I0704 07:45:58.547979 25348 solver.cpp:290] Iteration 7000 (26.7975 iter/s, 3.73169s/100 iter), loss = 0
I0704 07:45:58.547996 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:45:58.548008 25348 sgd_solver.cpp:106] Iteration 7000, lr = 0.00890625
I0704 07:45:58.548506 25348 solver.cpp:375] Finding and applying sparsity: 0.06
I0704 07:45:58.824502 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:46:00.909068 25348 solver.cpp:290] Iteration 7100 (42.3549 iter/s, 2.361s/100 iter), loss = 0
I0704 07:46:00.909090 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:00.909096 25348 sgd_solver.cpp:106] Iteration 7100, lr = 0.00889063
I0704 07:46:02.984323 25348 solver.cpp:290] Iteration 7200 (48.1888 iter/s, 2.07517s/100 iter), loss = 0
I0704 07:46:02.984344 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:02.984351 25348 sgd_solver.cpp:106] Iteration 7200, lr = 0.008875
I0704 07:46:05.056545 25348 solver.cpp:290] Iteration 7300 (48.2594 iter/s, 2.07214s/100 iter), loss = 0
I0704 07:46:05.056565 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:05.056572 25348 sgd_solver.cpp:106] Iteration 7300, lr = 0.00885937
I0704 07:46:07.132844 25348 solver.cpp:290] Iteration 7400 (48.1646 iter/s, 2.07621s/100 iter), loss = 0
I0704 07:46:07.132874 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:07.132884 25348 sgd_solver.cpp:106] Iteration 7400, lr = 0.00884375
I0704 07:46:09.208468 25348 solver.cpp:290] Iteration 7500 (48.1804 iter/s, 2.07553s/100 iter), loss = 0
I0704 07:46:09.208489 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:09.208498 25348 sgd_solver.cpp:106] Iteration 7500, lr = 0.00882812
I0704 07:46:11.280071 25348 solver.cpp:290] Iteration 7600 (48.2738 iter/s, 2.07152s/100 iter), loss = 0
I0704 07:46:11.280092 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:11.280100 25348 sgd_solver.cpp:106] Iteration 7600, lr = 0.0088125
I0704 07:46:13.356956 25348 solver.cpp:290] Iteration 7700 (48.1511 iter/s, 2.0768s/100 iter), loss = 0
I0704 07:46:13.356982 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:13.357013 25348 sgd_solver.cpp:106] Iteration 7700, lr = 0.00879687
I0704 07:46:15.432950 25348 solver.cpp:290] Iteration 7800 (48.1718 iter/s, 2.07591s/100 iter), loss = 0
I0704 07:46:15.432972 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:15.432979 25348 sgd_solver.cpp:106] Iteration 7800, lr = 0.00878125
I0704 07:46:17.508100 25348 solver.cpp:290] Iteration 7900 (48.1913 iter/s, 2.07506s/100 iter), loss = 0
I0704 07:46:17.508121 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:17.508128 25348 sgd_solver.cpp:106] Iteration 7900, lr = 0.00876562
I0704 07:46:19.560109 25348 solver.cpp:354] Sparsity after update:
I0704 07:46:19.561491 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:46:19.561498 25348 net.cpp:1851] conv1a_param_0(0.03) 
I0704 07:46:19.561506 25348 net.cpp:1851] conv1b_param_0(0.0599) 
I0704 07:46:19.561508 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:46:19.561511 25348 net.cpp:1851] res2a_branch2a_param_0(0.06) 
I0704 07:46:19.561512 25348 net.cpp:1851] res2a_branch2b_param_0(0.0599) 
I0704 07:46:19.561514 25348 net.cpp:1851] res3a_branch2a_param_0(0.06) 
I0704 07:46:19.561517 25348 net.cpp:1851] res3a_branch2b_param_0(0.06) 
I0704 07:46:19.561518 25348 net.cpp:1851] res4a_branch2a_param_0(0.06) 
I0704 07:46:19.561520 25348 net.cpp:1851] res4a_branch2b_param_0(0.06) 
I0704 07:46:19.561522 25348 net.cpp:1851] res5a_branch2a_param_0(0.06) 
I0704 07:46:19.561524 25348 net.cpp:1851] res5a_branch2b_param_0(0.06) 
I0704 07:46:19.561527 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (141203/2.3599e+06) 0.0598
I0704 07:46:19.561612 25348 solver.cpp:466] Iteration 8000, Testing net (#0)
I0704 07:46:21.205999 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9029
I0704 07:46:21.206018 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9963
I0704 07:46:21.206024 25348 solver.cpp:539]     Test net output #2: loss = 0.1849 (* 1 = 0.1849 loss)
I0704 07:46:21.226096 25348 solver.cpp:290] Iteration 8000 (26.8971 iter/s, 3.71787s/100 iter), loss = 0
I0704 07:46:21.226114 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:21.226125 25348 sgd_solver.cpp:106] Iteration 8000, lr = 0.00875
I0704 07:46:21.226650 25348 solver.cpp:375] Finding and applying sparsity: 0.08
I0704 07:46:21.523664 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:46:23.611918 25348 solver.cpp:290] Iteration 8100 (41.9158 iter/s, 2.38573s/100 iter), loss = 0
I0704 07:46:23.611994 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:23.612002 25348 sgd_solver.cpp:106] Iteration 8100, lr = 0.00873438
I0704 07:46:25.685705 25348 solver.cpp:290] Iteration 8200 (48.2242 iter/s, 2.07365s/100 iter), loss = 0
I0704 07:46:25.685729 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:25.685736 25348 sgd_solver.cpp:106] Iteration 8200, lr = 0.00871875
I0704 07:46:27.761958 25348 solver.cpp:290] Iteration 8300 (48.1657 iter/s, 2.07617s/100 iter), loss = 0
I0704 07:46:27.761982 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:27.761988 25348 sgd_solver.cpp:106] Iteration 8300, lr = 0.00870312
I0704 07:46:29.835227 25348 solver.cpp:290] Iteration 8400 (48.2351 iter/s, 2.07318s/100 iter), loss = 0
I0704 07:46:29.835247 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:29.835254 25348 sgd_solver.cpp:106] Iteration 8400, lr = 0.0086875
I0704 07:46:31.905133 25348 solver.cpp:290] Iteration 8500 (48.3134 iter/s, 2.06982s/100 iter), loss = 0
I0704 07:46:31.905154 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:31.905161 25348 sgd_solver.cpp:106] Iteration 8500, lr = 0.00867188
I0704 07:46:34.003264 25348 solver.cpp:290] Iteration 8600 (47.6634 iter/s, 2.09805s/100 iter), loss = 0
I0704 07:46:34.003288 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:34.003298 25348 sgd_solver.cpp:106] Iteration 8600, lr = 0.00865625
I0704 07:46:36.074009 25348 solver.cpp:290] Iteration 8700 (48.2939 iter/s, 2.07066s/100 iter), loss = 0
I0704 07:46:36.074038 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:36.074045 25348 sgd_solver.cpp:106] Iteration 8700, lr = 0.00864062
I0704 07:46:38.146744 25348 solver.cpp:290] Iteration 8800 (48.2475 iter/s, 2.07264s/100 iter), loss = 0
I0704 07:46:38.146771 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:38.146780 25348 sgd_solver.cpp:106] Iteration 8800, lr = 0.008625
I0704 07:46:40.225381 25348 solver.cpp:290] Iteration 8900 (48.1106 iter/s, 2.07855s/100 iter), loss = 0
I0704 07:46:40.225407 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:40.225416 25348 sgd_solver.cpp:106] Iteration 8900, lr = 0.00860937
I0704 07:46:42.278522 25348 solver.cpp:354] Sparsity after update:
I0704 07:46:42.279929 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:46:42.279937 25348 net.cpp:1851] conv1a_param_0(0.0396) 
I0704 07:46:42.279947 25348 net.cpp:1851] conv1b_param_0(0.0799) 
I0704 07:46:42.279952 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:46:42.279955 25348 net.cpp:1851] res2a_branch2a_param_0(0.08) 
I0704 07:46:42.279960 25348 net.cpp:1851] res2a_branch2b_param_0(0.08) 
I0704 07:46:42.279965 25348 net.cpp:1851] res3a_branch2a_param_0(0.08) 
I0704 07:46:42.279969 25348 net.cpp:1851] res3a_branch2b_param_0(0.08) 
I0704 07:46:42.279973 25348 net.cpp:1851] res4a_branch2a_param_0(0.08) 
I0704 07:46:42.279978 25348 net.cpp:1851] res4a_branch2b_param_0(0.08) 
I0704 07:46:42.279983 25348 net.cpp:1851] res5a_branch2a_param_0(0.08) 
I0704 07:46:42.279986 25348 net.cpp:1851] res5a_branch2b_param_0(0.08) 
I0704 07:46:42.279991 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (188278/2.3599e+06) 0.0798
I0704 07:46:42.280091 25348 solver.cpp:466] Iteration 9000, Testing net (#0)
I0704 07:46:43.925376 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8996
I0704 07:46:43.925397 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9926
I0704 07:46:43.925402 25348 solver.cpp:539]     Test net output #2: loss = 0.2014 (* 1 = 0.2014 loss)
I0704 07:46:43.945425 25348 solver.cpp:290] Iteration 9000 (26.8824 iter/s, 3.71991s/100 iter), loss = 0
I0704 07:46:43.945446 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:43.945454 25348 sgd_solver.cpp:106] Iteration 9000, lr = 0.00859375
I0704 07:46:43.946218 25348 solver.cpp:375] Finding and applying sparsity: 0.1
I0704 07:46:44.238622 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:46:46.345155 25348 solver.cpp:290] Iteration 9100 (41.673 iter/s, 2.39963s/100 iter), loss = 0
I0704 07:46:46.345180 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:46.345188 25348 sgd_solver.cpp:106] Iteration 9100, lr = 0.00857813
I0704 07:46:48.420380 25348 solver.cpp:290] Iteration 9200 (48.1895 iter/s, 2.07514s/100 iter), loss = 0
I0704 07:46:48.420402 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:48.420408 25348 sgd_solver.cpp:106] Iteration 9200, lr = 0.0085625
I0704 07:46:50.499253 25348 solver.cpp:290] Iteration 9300 (48.1049 iter/s, 2.07879s/100 iter), loss = 0
I0704 07:46:50.499277 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:50.499286 25348 sgd_solver.cpp:106] Iteration 9300, lr = 0.00854687
I0704 07:46:52.572290 25348 solver.cpp:290] Iteration 9400 (48.2405 iter/s, 2.07295s/100 iter), loss = 0
I0704 07:46:52.572314 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:52.572321 25348 sgd_solver.cpp:106] Iteration 9400, lr = 0.00853125
I0704 07:46:54.648319 25348 solver.cpp:290] Iteration 9500 (48.1709 iter/s, 2.07594s/100 iter), loss = 0
I0704 07:46:54.648393 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:54.648401 25348 sgd_solver.cpp:106] Iteration 9500, lr = 0.00851563
I0704 07:46:56.722234 25348 solver.cpp:290] Iteration 9600 (48.2211 iter/s, 2.07378s/100 iter), loss = 0
I0704 07:46:56.722256 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:46:56.722263 25348 sgd_solver.cpp:106] Iteration 9600, lr = 0.0085
I0704 07:46:58.792049 25348 solver.cpp:290] Iteration 9700 (48.3155 iter/s, 2.06973s/100 iter), loss = 0.190476
I0704 07:46:58.792071 25348 solver.cpp:309]     Train net output #0: loss = 0.190476 (* 1 = 0.190476 loss)
I0704 07:46:58.792080 25348 sgd_solver.cpp:106] Iteration 9700, lr = 0.00848437
I0704 07:47:00.869410 25348 solver.cpp:290] Iteration 9800 (48.14 iter/s, 2.07728s/100 iter), loss = 0.047619
I0704 07:47:00.869431 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:00.869439 25348 sgd_solver.cpp:106] Iteration 9800, lr = 0.00846875
I0704 07:47:02.944416 25348 solver.cpp:290] Iteration 9900 (48.1946 iter/s, 2.07492s/100 iter), loss = -4.47035e-08
I0704 07:47:02.944438 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:02.944444 25348 sgd_solver.cpp:106] Iteration 9900, lr = 0.00845312
I0704 07:47:04.994676 25348 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_10000.caffemodel
I0704 07:47:05.018944 25348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_10000.solverstate
I0704 07:47:05.026366 25348 solver.cpp:354] Sparsity after update:
I0704 07:47:05.027302 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:47:05.027309 25348 net.cpp:1851] conv1a_param_0(0.0496) 
I0704 07:47:05.027318 25348 net.cpp:1851] conv1b_param_0(0.0998) 
I0704 07:47:05.027319 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:47:05.027323 25348 net.cpp:1851] res2a_branch2a_param_0(0.1) 
I0704 07:47:05.027324 25348 net.cpp:1851] res2a_branch2b_param_0(0.0999) 
I0704 07:47:05.027326 25348 net.cpp:1851] res3a_branch2a_param_0(0.1) 
I0704 07:47:05.027328 25348 net.cpp:1851] res3a_branch2b_param_0(0.1) 
I0704 07:47:05.027330 25348 net.cpp:1851] res4a_branch2a_param_0(0.1) 
I0704 07:47:05.027333 25348 net.cpp:1851] res4a_branch2b_param_0(0.1) 
I0704 07:47:05.027334 25348 net.cpp:1851] res5a_branch2a_param_0(0.1) 
I0704 07:47:05.027336 25348 net.cpp:1851] res5a_branch2b_param_0(0.1) 
I0704 07:47:05.027338 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (235342/2.3599e+06) 0.0997
I0704 07:47:05.027441 25348 solver.cpp:466] Iteration 10000, Testing net (#0)
I0704 07:47:06.671926 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8089
I0704 07:47:06.671944 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9867
I0704 07:47:06.671949 25348 solver.cpp:539]     Test net output #2: loss = 0.4254 (* 1 = 0.4254 loss)
I0704 07:47:06.692207 25348 solver.cpp:290] Iteration 10000 (26.6833 iter/s, 3.74766s/100 iter), loss = 0.047619
I0704 07:47:06.692224 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:06.692234 25348 sgd_solver.cpp:106] Iteration 10000, lr = 0.0084375
I0704 07:47:06.692782 25348 solver.cpp:375] Finding and applying sparsity: 0.12
I0704 07:47:06.946877 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:47:09.033807 25348 solver.cpp:290] Iteration 10100 (42.7074 iter/s, 2.34151s/100 iter), loss = 0.047619
I0704 07:47:09.033828 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:09.033834 25348 sgd_solver.cpp:106] Iteration 10100, lr = 0.00842187
I0704 07:47:11.116773 25348 solver.cpp:290] Iteration 10200 (48.0104 iter/s, 2.08288s/100 iter), loss = 0.047619
I0704 07:47:11.116794 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:11.116802 25348 sgd_solver.cpp:106] Iteration 10200, lr = 0.00840625
I0704 07:47:13.188920 25348 solver.cpp:290] Iteration 10300 (48.2611 iter/s, 2.07206s/100 iter), loss = -1.04308e-07
I0704 07:47:13.188942 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:13.188949 25348 sgd_solver.cpp:106] Iteration 10300, lr = 0.00839063
I0704 07:47:15.261929 25348 solver.cpp:290] Iteration 10400 (48.2411 iter/s, 2.07292s/100 iter), loss = 0.047619
I0704 07:47:15.261953 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:15.261962 25348 sgd_solver.cpp:106] Iteration 10400, lr = 0.008375
I0704 07:47:17.336333 25348 solver.cpp:290] Iteration 10500 (48.2086 iter/s, 2.07432s/100 iter), loss = -1.04308e-07
I0704 07:47:17.336356 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:17.336362 25348 sgd_solver.cpp:106] Iteration 10500, lr = 0.00835937
I0704 07:47:19.407399 25348 solver.cpp:290] Iteration 10600 (48.2863 iter/s, 2.07098s/100 iter), loss = -1.04308e-07
I0704 07:47:19.407421 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:19.407428 25348 sgd_solver.cpp:106] Iteration 10600, lr = 0.00834375
I0704 07:47:21.482646 25348 solver.cpp:290] Iteration 10700 (48.189 iter/s, 2.07516s/100 iter), loss = 0.047619
I0704 07:47:21.482669 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:21.482676 25348 sgd_solver.cpp:106] Iteration 10700, lr = 0.00832812
I0704 07:47:23.554105 25348 solver.cpp:290] Iteration 10800 (48.2772 iter/s, 2.07137s/100 iter), loss = -1.04308e-07
I0704 07:47:23.554127 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:23.554134 25348 sgd_solver.cpp:106] Iteration 10800, lr = 0.0083125
I0704 07:47:25.627172 25348 solver.cpp:290] Iteration 10900 (48.2397 iter/s, 2.07298s/100 iter), loss = 0.0476189
I0704 07:47:25.627244 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:25.627259 25348 sgd_solver.cpp:106] Iteration 10900, lr = 0.00829687
I0704 07:47:27.680713 25348 solver.cpp:354] Sparsity after update:
I0704 07:47:27.682126 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:47:27.682134 25348 net.cpp:1851] conv1a_param_0(0.0596) 
I0704 07:47:27.682144 25348 net.cpp:1851] conv1b_param_0(0.12) 
I0704 07:47:27.682149 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:47:27.682154 25348 net.cpp:1851] res2a_branch2a_param_0(0.12) 
I0704 07:47:27.682158 25348 net.cpp:1851] res2a_branch2b_param_0(0.12) 
I0704 07:47:27.682163 25348 net.cpp:1851] res3a_branch2a_param_0(0.12) 
I0704 07:47:27.682166 25348 net.cpp:1851] res3a_branch2b_param_0(0.12) 
I0704 07:47:27.682170 25348 net.cpp:1851] res4a_branch2a_param_0(0.12) 
I0704 07:47:27.682174 25348 net.cpp:1851] res4a_branch2b_param_0(0.12) 
I0704 07:47:27.682178 25348 net.cpp:1851] res5a_branch2a_param_0(0.12) 
I0704 07:47:27.682183 25348 net.cpp:1851] res5a_branch2b_param_0(0.12) 
I0704 07:47:27.682186 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (282421/2.3599e+06) 0.12
I0704 07:47:27.682276 25348 solver.cpp:466] Iteration 11000, Testing net (#0)
I0704 07:47:29.332545 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.751
I0704 07:47:29.332563 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9884
I0704 07:47:29.332571 25348 solver.cpp:539]     Test net output #2: loss = 0.725 (* 1 = 0.725 loss)
I0704 07:47:29.352393 25348 solver.cpp:290] Iteration 11000 (26.8453 iter/s, 3.72505s/100 iter), loss = 0.0476189
I0704 07:47:29.352412 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:29.352427 25348 sgd_solver.cpp:106] Iteration 11000, lr = 0.00828125
I0704 07:47:29.352993 25348 solver.cpp:375] Finding and applying sparsity: 0.14
I0704 07:47:29.587641 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:47:31.679364 25348 solver.cpp:290] Iteration 11100 (42.9759 iter/s, 2.32688s/100 iter), loss = 0.0476189
I0704 07:47:31.679385 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:31.679392 25348 sgd_solver.cpp:106] Iteration 11100, lr = 0.00826562
I0704 07:47:33.789124 25348 solver.cpp:290] Iteration 11200 (47.4007 iter/s, 2.10967s/100 iter), loss = 0.142857
I0704 07:47:33.789151 25348 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:47:33.789160 25348 sgd_solver.cpp:106] Iteration 11200, lr = 0.00825
I0704 07:47:35.859431 25348 solver.cpp:290] Iteration 11300 (48.3041 iter/s, 2.07022s/100 iter), loss = -1.37836e-07
I0704 07:47:35.859457 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:35.859467 25348 sgd_solver.cpp:106] Iteration 11300, lr = 0.00823438
I0704 07:47:37.936087 25348 solver.cpp:290] Iteration 11400 (48.1564 iter/s, 2.07657s/100 iter), loss = 0.095238
I0704 07:47:37.936113 25348 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:47:37.936122 25348 sgd_solver.cpp:106] Iteration 11400, lr = 0.00821875
I0704 07:47:40.009002 25348 solver.cpp:290] Iteration 11500 (48.2433 iter/s, 2.07283s/100 iter), loss = 0.0476189
I0704 07:47:40.009026 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:40.009032 25348 sgd_solver.cpp:106] Iteration 11500, lr = 0.00820312
I0704 07:47:42.084405 25348 solver.cpp:290] Iteration 11600 (48.1854 iter/s, 2.07532s/100 iter), loss = -1.3411e-07
I0704 07:47:42.084427 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:42.084434 25348 sgd_solver.cpp:106] Iteration 11600, lr = 0.0081875
I0704 07:47:44.156862 25348 solver.cpp:290] Iteration 11700 (48.2539 iter/s, 2.07237s/100 iter), loss = -1.3411e-07
I0704 07:47:44.156885 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:44.156891 25348 sgd_solver.cpp:106] Iteration 11700, lr = 0.00817188
I0704 07:47:46.236239 25348 solver.cpp:290] Iteration 11800 (48.0933 iter/s, 2.07929s/100 iter), loss = 0.0476189
I0704 07:47:46.236261 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:46.236268 25348 sgd_solver.cpp:106] Iteration 11800, lr = 0.00815625
I0704 07:47:48.310560 25348 solver.cpp:290] Iteration 11900 (48.2106 iter/s, 2.07423s/100 iter), loss = -1.49012e-07
I0704 07:47:48.310583 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:48.310590 25348 sgd_solver.cpp:106] Iteration 11900, lr = 0.00814062
I0704 07:47:50.365936 25348 solver.cpp:354] Sparsity after update:
I0704 07:47:50.367327 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:47:50.367336 25348 net.cpp:1851] conv1a_param_0(0.0696) 
I0704 07:47:50.367344 25348 net.cpp:1851] conv1b_param_0(0.14) 
I0704 07:47:50.367350 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:47:50.367354 25348 net.cpp:1851] res2a_branch2a_param_0(0.14) 
I0704 07:47:50.367358 25348 net.cpp:1851] res2a_branch2b_param_0(0.14) 
I0704 07:47:50.367363 25348 net.cpp:1851] res3a_branch2a_param_0(0.14) 
I0704 07:47:50.367368 25348 net.cpp:1851] res3a_branch2b_param_0(0.14) 
I0704 07:47:50.367372 25348 net.cpp:1851] res4a_branch2a_param_0(0.14) 
I0704 07:47:50.367377 25348 net.cpp:1851] res4a_branch2b_param_0(0.14) 
I0704 07:47:50.367380 25348 net.cpp:1851] res5a_branch2a_param_0(0.14) 
I0704 07:47:50.367385 25348 net.cpp:1851] res5a_branch2b_param_0(0.14) 
I0704 07:47:50.367388 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (329490/2.3599e+06) 0.14
I0704 07:47:50.367480 25348 solver.cpp:466] Iteration 12000, Testing net (#0)
I0704 07:47:52.025094 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8343
I0704 07:47:52.025112 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9907
I0704 07:47:52.025118 25348 solver.cpp:539]     Test net output #2: loss = 0.3233 (* 1 = 0.3233 loss)
I0704 07:47:52.045275 25348 solver.cpp:290] Iteration 12000 (26.7767 iter/s, 3.73459s/100 iter), loss = 0.0952379
I0704 07:47:52.045294 25348 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:47:52.045306 25348 sgd_solver.cpp:106] Iteration 12000, lr = 0.008125
I0704 07:47:52.045859 25348 solver.cpp:375] Finding and applying sparsity: 0.16
I0704 07:47:52.322541 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:47:54.399521 25348 solver.cpp:290] Iteration 12100 (42.4781 iter/s, 2.35415s/100 iter), loss = -1.49012e-07
I0704 07:47:54.399544 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:54.399550 25348 sgd_solver.cpp:106] Iteration 12100, lr = 0.00810937
I0704 07:47:56.476440 25348 solver.cpp:290] Iteration 12200 (48.1502 iter/s, 2.07683s/100 iter), loss = 0.0476189
I0704 07:47:56.476542 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:47:56.476549 25348 sgd_solver.cpp:106] Iteration 12200, lr = 0.00809375
I0704 07:47:58.548276 25348 solver.cpp:290] Iteration 12300 (48.2702 iter/s, 2.07167s/100 iter), loss = -1.63913e-07
I0704 07:47:58.548300 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:47:58.548305 25348 sgd_solver.cpp:106] Iteration 12300, lr = 0.00807813
I0704 07:48:00.622593 25348 solver.cpp:290] Iteration 12400 (48.2106 iter/s, 2.07423s/100 iter), loss = -1.63913e-07
I0704 07:48:00.622615 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:00.622622 25348 sgd_solver.cpp:106] Iteration 12400, lr = 0.0080625
I0704 07:48:02.703852 25348 solver.cpp:290] Iteration 12500 (48.0498 iter/s, 2.08117s/100 iter), loss = -1.49012e-07
I0704 07:48:02.703873 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:02.703881 25348 sgd_solver.cpp:106] Iteration 12500, lr = 0.00804687
I0704 07:48:04.781750 25348 solver.cpp:290] Iteration 12600 (48.1275 iter/s, 2.07781s/100 iter), loss = 0.0952379
I0704 07:48:04.781774 25348 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:48:04.781780 25348 sgd_solver.cpp:106] Iteration 12600, lr = 0.00803125
I0704 07:48:06.855885 25348 solver.cpp:290] Iteration 12700 (48.2149 iter/s, 2.07405s/100 iter), loss = -1.52737e-07
I0704 07:48:06.855906 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:06.855913 25348 sgd_solver.cpp:106] Iteration 12700, lr = 0.00801562
I0704 07:48:08.929366 25348 solver.cpp:290] Iteration 12800 (48.23 iter/s, 2.0734s/100 iter), loss = -1.63913e-07
I0704 07:48:08.929389 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:08.929396 25348 sgd_solver.cpp:106] Iteration 12800, lr = 0.008
I0704 07:48:11.001621 25348 solver.cpp:290] Iteration 12900 (48.2587 iter/s, 2.07217s/100 iter), loss = -1.63913e-07
I0704 07:48:11.001646 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:11.001654 25348 sgd_solver.cpp:106] Iteration 12900, lr = 0.00798437
I0704 07:48:13.062945 25348 solver.cpp:354] Sparsity after update:
I0704 07:48:13.064328 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:48:13.064337 25348 net.cpp:1851] conv1a_param_0(0.0796) 
I0704 07:48:13.064347 25348 net.cpp:1851] conv1b_param_0(0.16) 
I0704 07:48:13.064350 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:48:13.064354 25348 net.cpp:1851] res2a_branch2a_param_0(0.16) 
I0704 07:48:13.064359 25348 net.cpp:1851] res2a_branch2b_param_0(0.16) 
I0704 07:48:13.064364 25348 net.cpp:1851] res3a_branch2a_param_0(0.16) 
I0704 07:48:13.064368 25348 net.cpp:1851] res3a_branch2b_param_0(0.16) 
I0704 07:48:13.064373 25348 net.cpp:1851] res4a_branch2a_param_0(0.16) 
I0704 07:48:13.064378 25348 net.cpp:1851] res4a_branch2b_param_0(0.16) 
I0704 07:48:13.064381 25348 net.cpp:1851] res5a_branch2a_param_0(0.16) 
I0704 07:48:13.064386 25348 net.cpp:1851] res5a_branch2b_param_0(0.16) 
I0704 07:48:13.064390 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (376562/2.3599e+06) 0.16
I0704 07:48:13.064483 25348 solver.cpp:466] Iteration 13000, Testing net (#0)
I0704 07:48:14.708025 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8348
I0704 07:48:14.708045 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9875
I0704 07:48:14.708050 25348 solver.cpp:539]     Test net output #2: loss = 0.4223 (* 1 = 0.4223 loss)
I0704 07:48:14.727730 25348 solver.cpp:290] Iteration 13000 (26.8386 iter/s, 3.72598s/100 iter), loss = 0.0476189
I0704 07:48:14.727747 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:48:14.727759 25348 sgd_solver.cpp:106] Iteration 13000, lr = 0.00796875
I0704 07:48:14.728308 25348 solver.cpp:375] Finding and applying sparsity: 0.18
I0704 07:48:14.993615 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:48:17.085896 25348 solver.cpp:290] Iteration 13100 (42.4074 iter/s, 2.35808s/100 iter), loss = -1.63913e-07
I0704 07:48:17.085934 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:17.085942 25348 sgd_solver.cpp:106] Iteration 13100, lr = 0.00795313
I0704 07:48:19.156523 25348 solver.cpp:290] Iteration 13200 (48.2969 iter/s, 2.07053s/100 iter), loss = -1.67638e-07
I0704 07:48:19.156545 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:19.156553 25348 sgd_solver.cpp:106] Iteration 13200, lr = 0.0079375
I0704 07:48:21.231608 25348 solver.cpp:290] Iteration 13300 (48.1928 iter/s, 2.075s/100 iter), loss = 0.0476189
I0704 07:48:21.231631 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:48:21.231637 25348 sgd_solver.cpp:106] Iteration 13300, lr = 0.00792187
I0704 07:48:23.303719 25348 solver.cpp:290] Iteration 13400 (48.2621 iter/s, 2.07202s/100 iter), loss = -1.63913e-07
I0704 07:48:23.303743 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:23.303752 25348 sgd_solver.cpp:106] Iteration 13400, lr = 0.00790625
I0704 07:48:25.380127 25348 solver.cpp:290] Iteration 13500 (48.1621 iter/s, 2.07632s/100 iter), loss = -1.63913e-07
I0704 07:48:25.380149 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:25.380157 25348 sgd_solver.cpp:106] Iteration 13500, lr = 0.00789062
I0704 07:48:27.455042 25348 solver.cpp:290] Iteration 13600 (48.1968 iter/s, 2.07483s/100 iter), loss = -1.63913e-07
I0704 07:48:27.455106 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:27.455117 25348 sgd_solver.cpp:106] Iteration 13600, lr = 0.007875
I0704 07:48:29.530053 25348 solver.cpp:290] Iteration 13700 (48.1954 iter/s, 2.07488s/100 iter), loss = -1.71363e-07
I0704 07:48:29.530076 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:29.530082 25348 sgd_solver.cpp:106] Iteration 13700, lr = 0.00785937
I0704 07:48:31.606386 25348 solver.cpp:290] Iteration 13800 (48.1639 iter/s, 2.07624s/100 iter), loss = -1.78814e-07
I0704 07:48:31.606410 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:31.606416 25348 sgd_solver.cpp:106] Iteration 13800, lr = 0.00784375
I0704 07:48:33.695430 25348 solver.cpp:290] Iteration 13900 (47.8708 iter/s, 2.08895s/100 iter), loss = 0.0476189
I0704 07:48:33.695451 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:48:33.695458 25348 sgd_solver.cpp:106] Iteration 13900, lr = 0.00782812
I0704 07:48:35.751704 25348 solver.cpp:354] Sparsity after update:
I0704 07:48:35.753101 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:48:35.753108 25348 net.cpp:1851] conv1a_param_0(0.0896) 
I0704 07:48:35.753115 25348 net.cpp:1851] conv1b_param_0(0.18) 
I0704 07:48:35.753118 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:48:35.753121 25348 net.cpp:1851] res2a_branch2a_param_0(0.18) 
I0704 07:48:35.753124 25348 net.cpp:1851] res2a_branch2b_param_0(0.18) 
I0704 07:48:35.753126 25348 net.cpp:1851] res3a_branch2a_param_0(0.18) 
I0704 07:48:35.753129 25348 net.cpp:1851] res3a_branch2b_param_0(0.18) 
I0704 07:48:35.753131 25348 net.cpp:1851] res4a_branch2a_param_0(0.18) 
I0704 07:48:35.753134 25348 net.cpp:1851] res4a_branch2b_param_0(0.18) 
I0704 07:48:35.753135 25348 net.cpp:1851] res5a_branch2a_param_0(0.18) 
I0704 07:48:35.753139 25348 net.cpp:1851] res5a_branch2b_param_0(0.18) 
I0704 07:48:35.753141 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (423636/2.3599e+06) 0.18
I0704 07:48:35.753229 25348 solver.cpp:466] Iteration 14000, Testing net (#0)
I0704 07:48:37.396482 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8013
I0704 07:48:37.396502 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9882
I0704 07:48:37.396507 25348 solver.cpp:539]     Test net output #2: loss = 0.4968 (* 1 = 0.4968 loss)
I0704 07:48:37.416868 25348 solver.cpp:290] Iteration 14000 (26.8723 iter/s, 3.72131s/100 iter), loss = -1.86265e-07
I0704 07:48:37.416887 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:37.416900 25348 sgd_solver.cpp:106] Iteration 14000, lr = 0.0078125
I0704 07:48:37.417474 25348 solver.cpp:375] Finding and applying sparsity: 0.2
I0704 07:48:37.707029 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:48:39.803529 25348 solver.cpp:290] Iteration 14100 (41.9011 iter/s, 2.38657s/100 iter), loss = -2.12342e-07
I0704 07:48:39.803552 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:39.803560 25348 sgd_solver.cpp:106] Iteration 14100, lr = 0.00779688
I0704 07:48:41.880117 25348 solver.cpp:290] Iteration 14200 (48.1579 iter/s, 2.0765s/100 iter), loss = -2.30968e-07
I0704 07:48:41.880139 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:41.880146 25348 sgd_solver.cpp:106] Iteration 14200, lr = 0.00778125
I0704 07:48:43.955965 25348 solver.cpp:290] Iteration 14300 (48.1751 iter/s, 2.07576s/100 iter), loss = -2.23517e-07
I0704 07:48:43.955988 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:43.955996 25348 sgd_solver.cpp:106] Iteration 14300, lr = 0.00776563
I0704 07:48:46.037762 25348 solver.cpp:290] Iteration 14400 (48.0375 iter/s, 2.08171s/100 iter), loss = -2.23517e-07
I0704 07:48:46.037786 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:46.037792 25348 sgd_solver.cpp:106] Iteration 14400, lr = 0.00775
I0704 07:48:48.116003 25348 solver.cpp:290] Iteration 14500 (48.1196 iter/s, 2.07815s/100 iter), loss = 0.0476188
I0704 07:48:48.116039 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:48:48.116046 25348 sgd_solver.cpp:106] Iteration 14500, lr = 0.00773437
I0704 07:48:50.188387 25348 solver.cpp:290] Iteration 14600 (48.2559 iter/s, 2.07229s/100 iter), loss = -2.23517e-07
I0704 07:48:50.188410 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:50.188417 25348 sgd_solver.cpp:106] Iteration 14600, lr = 0.00771875
I0704 07:48:52.261140 25348 solver.cpp:290] Iteration 14700 (48.247 iter/s, 2.07267s/100 iter), loss = -2.42144e-07
I0704 07:48:52.261163 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:48:52.261170 25348 sgd_solver.cpp:106] Iteration 14700, lr = 0.00770312
I0704 07:48:54.335252 25348 solver.cpp:290] Iteration 14800 (48.2155 iter/s, 2.07402s/100 iter), loss = 0.0952379
I0704 07:48:54.335274 25348 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:48:54.335281 25348 sgd_solver.cpp:106] Iteration 14800, lr = 0.0076875
I0704 07:48:56.407382 25348 solver.cpp:290] Iteration 14900 (48.2615 iter/s, 2.07205s/100 iter), loss = 0.0952379
I0704 07:48:56.407404 25348 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:48:56.407411 25348 sgd_solver.cpp:106] Iteration 14900, lr = 0.00767187
I0704 07:48:58.462117 25348 solver.cpp:354] Sparsity after update:
I0704 07:48:58.463522 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:48:58.463531 25348 net.cpp:1851] conv1a_param_0(0.0996) 
I0704 07:48:58.463537 25348 net.cpp:1851] conv1b_param_0(0.2) 
I0704 07:48:58.463541 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:48:58.463543 25348 net.cpp:1851] res2a_branch2a_param_0(0.2) 
I0704 07:48:58.463546 25348 net.cpp:1851] res2a_branch2b_param_0(0.2) 
I0704 07:48:58.463547 25348 net.cpp:1851] res3a_branch2a_param_0(0.2) 
I0704 07:48:58.463551 25348 net.cpp:1851] res3a_branch2b_param_0(0.2) 
I0704 07:48:58.463552 25348 net.cpp:1851] res4a_branch2a_param_0(0.2) 
I0704 07:48:58.463554 25348 net.cpp:1851] res4a_branch2b_param_0(0.2) 
I0704 07:48:58.463557 25348 net.cpp:1851] res5a_branch2a_param_0(0.2) 
I0704 07:48:58.463558 25348 net.cpp:1851] res5a_branch2b_param_0(0.2) 
I0704 07:48:58.463560 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (470709/2.3599e+06) 0.199
I0704 07:48:58.463646 25348 solver.cpp:466] Iteration 15000, Testing net (#0)
I0704 07:49:00.106513 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8296
I0704 07:49:00.106533 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9896
I0704 07:49:00.106537 25348 solver.cpp:539]     Test net output #2: loss = 0.4104 (* 1 = 0.4104 loss)
I0704 07:49:00.127310 25348 solver.cpp:290] Iteration 15000 (26.8832 iter/s, 3.7198s/100 iter), loss = -2.38419e-07
I0704 07:49:00.127328 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:00.127338 25348 sgd_solver.cpp:106] Iteration 15000, lr = 0.00765625
I0704 07:49:00.127878 25348 solver.cpp:375] Finding and applying sparsity: 0.22
I0704 07:49:00.403573 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:49:02.500529 25348 solver.cpp:290] Iteration 15100 (42.1385 iter/s, 2.37313s/100 iter), loss = -2.38419e-07
I0704 07:49:02.500551 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:02.500558 25348 sgd_solver.cpp:106] Iteration 15100, lr = 0.00764062
I0704 07:49:04.571518 25348 solver.cpp:290] Iteration 15200 (48.2881 iter/s, 2.0709s/100 iter), loss = -2.38419e-07
I0704 07:49:04.571542 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:04.571548 25348 sgd_solver.cpp:106] Iteration 15200, lr = 0.007625
I0704 07:49:06.643508 25348 solver.cpp:290] Iteration 15300 (48.2648 iter/s, 2.0719s/100 iter), loss = -2.38419e-07
I0704 07:49:06.643529 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:06.643537 25348 sgd_solver.cpp:106] Iteration 15300, lr = 0.00760937
I0704 07:49:08.715504 25348 solver.cpp:290] Iteration 15400 (48.2646 iter/s, 2.07191s/100 iter), loss = -2.38419e-07
I0704 07:49:08.715525 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:08.715533 25348 sgd_solver.cpp:106] Iteration 15400, lr = 0.00759375
I0704 07:49:10.789386 25348 solver.cpp:290] Iteration 15500 (48.2207 iter/s, 2.0738s/100 iter), loss = -2.38419e-07
I0704 07:49:10.789408 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:10.789415 25348 sgd_solver.cpp:106] Iteration 15500, lr = 0.00757812
I0704 07:49:12.863679 25348 solver.cpp:290] Iteration 15600 (48.2112 iter/s, 2.07421s/100 iter), loss = -2.38419e-07
I0704 07:49:12.863701 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:12.863708 25348 sgd_solver.cpp:106] Iteration 15600, lr = 0.0075625
I0704 07:49:14.935485 25348 solver.cpp:290] Iteration 15700 (48.2691 iter/s, 2.07172s/100 iter), loss = -2.38419e-07
I0704 07:49:14.935506 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:14.935514 25348 sgd_solver.cpp:106] Iteration 15700, lr = 0.00754687
I0704 07:49:17.010493 25348 solver.cpp:290] Iteration 15800 (48.1945 iter/s, 2.07492s/100 iter), loss = 0.0952379
I0704 07:49:17.010514 25348 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:49:17.010521 25348 sgd_solver.cpp:106] Iteration 15800, lr = 0.00753125
I0704 07:49:19.085664 25348 solver.cpp:290] Iteration 15900 (48.1908 iter/s, 2.07509s/100 iter), loss = -2.38419e-07
I0704 07:49:19.085688 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:19.085697 25348 sgd_solver.cpp:106] Iteration 15900, lr = 0.00751562
I0704 07:49:21.138309 25348 solver.cpp:354] Sparsity after update:
I0704 07:49:21.139711 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:49:21.139719 25348 net.cpp:1851] conv1a_param_0(0.11) 
I0704 07:49:21.139727 25348 net.cpp:1851] conv1b_param_0(0.22) 
I0704 07:49:21.139730 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:49:21.139732 25348 net.cpp:1851] res2a_branch2a_param_0(0.22) 
I0704 07:49:21.139735 25348 net.cpp:1851] res2a_branch2b_param_0(0.22) 
I0704 07:49:21.139736 25348 net.cpp:1851] res3a_branch2a_param_0(0.22) 
I0704 07:49:21.139739 25348 net.cpp:1851] res3a_branch2b_param_0(0.22) 
I0704 07:49:21.139740 25348 net.cpp:1851] res4a_branch2a_param_0(0.22) 
I0704 07:49:21.139742 25348 net.cpp:1851] res4a_branch2b_param_0(0.22) 
I0704 07:49:21.139744 25348 net.cpp:1851] res5a_branch2a_param_0(0.22) 
I0704 07:49:21.139746 25348 net.cpp:1851] res5a_branch2b_param_0(0.22) 
I0704 07:49:21.139750 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (517780/2.3599e+06) 0.219
I0704 07:49:21.139835 25348 solver.cpp:466] Iteration 16000, Testing net (#0)
I0704 07:49:22.784217 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8087
I0704 07:49:22.784236 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9865
I0704 07:49:22.784242 25348 solver.cpp:539]     Test net output #2: loss = 0.4633 (* 1 = 0.4633 loss)
I0704 07:49:22.804522 25348 solver.cpp:290] Iteration 16000 (26.8909 iter/s, 3.71873s/100 iter), loss = -2.5332e-07
I0704 07:49:22.804540 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:22.804551 25348 sgd_solver.cpp:106] Iteration 16000, lr = 0.0075
I0704 07:49:22.805122 25348 solver.cpp:375] Finding and applying sparsity: 0.24
I0704 07:49:23.082512 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:49:25.177712 25348 solver.cpp:290] Iteration 16100 (42.139 iter/s, 2.3731s/100 iter), loss = 0.0476188
I0704 07:49:25.177734 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:49:25.177742 25348 sgd_solver.cpp:106] Iteration 16100, lr = 0.00748438
I0704 07:49:27.255579 25348 solver.cpp:290] Iteration 16200 (48.1283 iter/s, 2.07778s/100 iter), loss = 0.0476188
I0704 07:49:27.255604 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:49:27.255614 25348 sgd_solver.cpp:106] Iteration 16200, lr = 0.00746875
I0704 07:49:29.331255 25348 solver.cpp:290] Iteration 16300 (48.1791 iter/s, 2.07559s/100 iter), loss = -2.5332e-07
I0704 07:49:29.331326 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:29.331337 25348 sgd_solver.cpp:106] Iteration 16300, lr = 0.00745312
I0704 07:49:31.404942 25348 solver.cpp:290] Iteration 16400 (48.2264 iter/s, 2.07355s/100 iter), loss = 0.142857
I0704 07:49:31.404963 25348 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:49:31.404969 25348 sgd_solver.cpp:106] Iteration 16400, lr = 0.0074375
I0704 07:49:33.500304 25348 solver.cpp:290] Iteration 16500 (47.7264 iter/s, 2.09528s/100 iter), loss = -2.38419e-07
I0704 07:49:33.500325 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:33.500334 25348 sgd_solver.cpp:106] Iteration 16500, lr = 0.00742187
I0704 07:49:35.581120 25348 solver.cpp:290] Iteration 16600 (48.06 iter/s, 2.08073s/100 iter), loss = -2.5332e-07
I0704 07:49:35.581143 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:35.581151 25348 sgd_solver.cpp:106] Iteration 16600, lr = 0.00740625
I0704 07:49:37.652907 25348 solver.cpp:290] Iteration 16700 (48.2696 iter/s, 2.0717s/100 iter), loss = -2.6077e-07
I0704 07:49:37.652930 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:37.652936 25348 sgd_solver.cpp:106] Iteration 16700, lr = 0.00739062
I0704 07:49:39.729225 25348 solver.cpp:290] Iteration 16800 (48.1642 iter/s, 2.07623s/100 iter), loss = -2.6077e-07
I0704 07:49:39.729246 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:39.729254 25348 sgd_solver.cpp:106] Iteration 16800, lr = 0.007375
I0704 07:49:41.804369 25348 solver.cpp:290] Iteration 16900 (48.1914 iter/s, 2.07506s/100 iter), loss = -2.57045e-07
I0704 07:49:41.804395 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:41.804402 25348 sgd_solver.cpp:106] Iteration 16900, lr = 0.00735937
I0704 07:49:43.855830 25348 solver.cpp:354] Sparsity after update:
I0704 07:49:43.857110 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:49:43.857122 25348 net.cpp:1851] conv1a_param_0(0.12) 
I0704 07:49:43.857131 25348 net.cpp:1851] conv1b_param_0(0.24) 
I0704 07:49:43.857132 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:49:43.857136 25348 net.cpp:1851] res2a_branch2a_param_0(0.24) 
I0704 07:49:43.857138 25348 net.cpp:1851] res2a_branch2b_param_0(0.24) 
I0704 07:49:43.857141 25348 net.cpp:1851] res3a_branch2a_param_0(0.24) 
I0704 07:49:43.857143 25348 net.cpp:1851] res3a_branch2b_param_0(0.24) 
I0704 07:49:43.857146 25348 net.cpp:1851] res4a_branch2a_param_0(0.24) 
I0704 07:49:43.857148 25348 net.cpp:1851] res4a_branch2b_param_0(0.24) 
I0704 07:49:43.857151 25348 net.cpp:1851] res5a_branch2a_param_0(0.24) 
I0704 07:49:43.857152 25348 net.cpp:1851] res5a_branch2b_param_0(0.24) 
I0704 07:49:43.857154 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (564848/2.3599e+06) 0.239
I0704 07:49:43.857246 25348 solver.cpp:466] Iteration 17000, Testing net (#0)
I0704 07:49:45.512115 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8748
I0704 07:49:45.512135 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.995
I0704 07:49:45.512141 25348 solver.cpp:539]     Test net output #2: loss = 0.276 (* 1 = 0.276 loss)
I0704 07:49:45.532071 25348 solver.cpp:290] Iteration 17000 (26.8271 iter/s, 3.72757s/100 iter), loss = -2.5332e-07
I0704 07:49:45.532091 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:45.532100 25348 sgd_solver.cpp:106] Iteration 17000, lr = 0.00734375
I0704 07:49:45.532842 25348 solver.cpp:375] Finding and applying sparsity: 0.26
I0704 07:49:45.841781 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:49:47.930665 25348 solver.cpp:290] Iteration 17100 (41.6927 iter/s, 2.3985s/100 iter), loss = -2.6077e-07
I0704 07:49:47.930688 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:47.930696 25348 sgd_solver.cpp:106] Iteration 17100, lr = 0.00732813
I0704 07:49:50.008671 25348 solver.cpp:290] Iteration 17200 (48.1251 iter/s, 2.07792s/100 iter), loss = 0.142857
I0704 07:49:50.008708 25348 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 07:49:50.008714 25348 sgd_solver.cpp:106] Iteration 17200, lr = 0.0073125
I0704 07:49:52.078630 25348 solver.cpp:290] Iteration 17300 (48.3124 iter/s, 2.06986s/100 iter), loss = -2.5332e-07
I0704 07:49:52.078655 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:52.078663 25348 sgd_solver.cpp:106] Iteration 17300, lr = 0.00729688
I0704 07:49:54.151522 25348 solver.cpp:290] Iteration 17400 (48.2438 iter/s, 2.07281s/100 iter), loss = -2.5332e-07
I0704 07:49:54.151545 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:54.151551 25348 sgd_solver.cpp:106] Iteration 17400, lr = 0.00728125
I0704 07:49:56.225327 25348 solver.cpp:290] Iteration 17500 (48.2225 iter/s, 2.07372s/100 iter), loss = -2.5332e-07
I0704 07:49:56.225350 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:56.225359 25348 sgd_solver.cpp:106] Iteration 17500, lr = 0.00726563
I0704 07:49:58.296921 25348 solver.cpp:290] Iteration 17600 (48.274 iter/s, 2.07151s/100 iter), loss = -2.5332e-07
I0704 07:49:58.296944 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:49:58.296952 25348 sgd_solver.cpp:106] Iteration 17600, lr = 0.00725
I0704 07:50:00.367300 25348 solver.cpp:290] Iteration 17700 (48.3024 iter/s, 2.07029s/100 iter), loss = 0.0476188
I0704 07:50:00.367372 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:50:00.367379 25348 sgd_solver.cpp:106] Iteration 17700, lr = 0.00723437
I0704 07:50:02.441882 25348 solver.cpp:290] Iteration 17800 (48.2056 iter/s, 2.07445s/100 iter), loss = -2.5332e-07
I0704 07:50:02.441903 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:02.441910 25348 sgd_solver.cpp:106] Iteration 17800, lr = 0.00721875
I0704 07:50:04.519093 25348 solver.cpp:290] Iteration 17900 (48.1435 iter/s, 2.07712s/100 iter), loss = -2.5332e-07
I0704 07:50:04.519114 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:04.519121 25348 sgd_solver.cpp:106] Iteration 17900, lr = 0.00720312
I0704 07:50:06.569792 25348 solver.cpp:354] Sparsity after update:
I0704 07:50:06.571192 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:50:06.571199 25348 net.cpp:1851] conv1a_param_0(0.13) 
I0704 07:50:06.571208 25348 net.cpp:1851] conv1b_param_0(0.26) 
I0704 07:50:06.571209 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:50:06.571213 25348 net.cpp:1851] res2a_branch2a_param_0(0.26) 
I0704 07:50:06.571214 25348 net.cpp:1851] res2a_branch2b_param_0(0.26) 
I0704 07:50:06.571218 25348 net.cpp:1851] res3a_branch2a_param_0(0.26) 
I0704 07:50:06.571219 25348 net.cpp:1851] res3a_branch2b_param_0(0.26) 
I0704 07:50:06.571223 25348 net.cpp:1851] res4a_branch2a_param_0(0.26) 
I0704 07:50:06.571224 25348 net.cpp:1851] res4a_branch2b_param_0(0.26) 
I0704 07:50:06.571228 25348 net.cpp:1851] res5a_branch2a_param_0(0.26) 
I0704 07:50:06.571229 25348 net.cpp:1851] res5a_branch2b_param_0(0.26) 
I0704 07:50:06.571233 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (611920/2.3599e+06) 0.259
I0704 07:50:06.571326 25348 solver.cpp:466] Iteration 18000, Testing net (#0)
I0704 07:50:08.213593 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8536
I0704 07:50:08.213613 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9882
I0704 07:50:08.213620 25348 solver.cpp:539]     Test net output #2: loss = 0.3859 (* 1 = 0.3859 loss)
I0704 07:50:08.233319 25348 solver.cpp:290] Iteration 18000 (26.9244 iter/s, 3.7141s/100 iter), loss = 0.0476188
I0704 07:50:08.233337 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:50:08.233348 25348 sgd_solver.cpp:106] Iteration 18000, lr = 0.0071875
I0704 07:50:08.233906 25348 solver.cpp:375] Finding and applying sparsity: 0.28
I0704 07:50:08.577478 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:50:10.678467 25348 solver.cpp:290] Iteration 18100 (40.8989 iter/s, 2.44506s/100 iter), loss = -2.6077e-07
I0704 07:50:10.678493 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:10.678500 25348 sgd_solver.cpp:106] Iteration 18100, lr = 0.00717187
I0704 07:50:12.757352 25348 solver.cpp:290] Iteration 18200 (48.1047 iter/s, 2.0788s/100 iter), loss = -2.5332e-07
I0704 07:50:12.757375 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:12.757381 25348 sgd_solver.cpp:106] Iteration 18200, lr = 0.00715625
I0704 07:50:14.830209 25348 solver.cpp:290] Iteration 18300 (48.2446 iter/s, 2.07277s/100 iter), loss = -2.5332e-07
I0704 07:50:14.830234 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:14.830242 25348 sgd_solver.cpp:106] Iteration 18300, lr = 0.00714062
I0704 07:50:16.902987 25348 solver.cpp:290] Iteration 18400 (48.2465 iter/s, 2.07269s/100 iter), loss = -2.5332e-07
I0704 07:50:16.903010 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:16.903018 25348 sgd_solver.cpp:106] Iteration 18400, lr = 0.007125
I0704 07:50:18.978760 25348 solver.cpp:290] Iteration 18500 (48.1768 iter/s, 2.07569s/100 iter), loss = -2.5332e-07
I0704 07:50:18.978785 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:18.978793 25348 sgd_solver.cpp:106] Iteration 18500, lr = 0.00710937
I0704 07:50:21.050155 25348 solver.cpp:290] Iteration 18600 (48.2787 iter/s, 2.07131s/100 iter), loss = -2.5332e-07
I0704 07:50:21.050194 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:21.050200 25348 sgd_solver.cpp:106] Iteration 18600, lr = 0.00709375
I0704 07:50:23.125754 25348 solver.cpp:290] Iteration 18700 (48.1812 iter/s, 2.0755s/100 iter), loss = -2.5332e-07
I0704 07:50:23.125778 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:23.125787 25348 sgd_solver.cpp:106] Iteration 18700, lr = 0.00707812
I0704 07:50:25.200147 25348 solver.cpp:290] Iteration 18800 (48.2089 iter/s, 2.07431s/100 iter), loss = -2.5332e-07
I0704 07:50:25.200170 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:25.200177 25348 sgd_solver.cpp:106] Iteration 18800, lr = 0.0070625
I0704 07:50:27.274163 25348 solver.cpp:290] Iteration 18900 (48.2177 iter/s, 2.07393s/100 iter), loss = -2.5332e-07
I0704 07:50:27.274186 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:27.274194 25348 sgd_solver.cpp:106] Iteration 18900, lr = 0.00704687
I0704 07:50:29.331185 25348 solver.cpp:354] Sparsity after update:
I0704 07:50:29.332568 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:50:29.332576 25348 net.cpp:1851] conv1a_param_0(0.14) 
I0704 07:50:29.332583 25348 net.cpp:1851] conv1b_param_0(0.28) 
I0704 07:50:29.332588 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:50:29.332592 25348 net.cpp:1851] res2a_branch2a_param_0(0.28) 
I0704 07:50:29.332597 25348 net.cpp:1851] res2a_branch2b_param_0(0.28) 
I0704 07:50:29.332602 25348 net.cpp:1851] res3a_branch2a_param_0(0.28) 
I0704 07:50:29.332605 25348 net.cpp:1851] res3a_branch2b_param_0(0.28) 
I0704 07:50:29.332609 25348 net.cpp:1851] res4a_branch2a_param_0(0.28) 
I0704 07:50:29.332612 25348 net.cpp:1851] res4a_branch2b_param_0(0.28) 
I0704 07:50:29.332617 25348 net.cpp:1851] res5a_branch2a_param_0(0.28) 
I0704 07:50:29.332620 25348 net.cpp:1851] res5a_branch2b_param_0(0.28) 
I0704 07:50:29.332624 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (658995/2.3599e+06) 0.279
I0704 07:50:29.332761 25348 solver.cpp:466] Iteration 19000, Testing net (#0)
I0704 07:50:30.980499 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.7929
I0704 07:50:30.980593 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9901
I0704 07:50:30.980600 25348 solver.cpp:539]     Test net output #2: loss = 0.5299 (* 1 = 0.5299 loss)
I0704 07:50:31.000399 25348 solver.cpp:290] Iteration 19000 (26.8377 iter/s, 3.72611s/100 iter), loss = -2.5332e-07
I0704 07:50:31.000416 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:31.000429 25348 sgd_solver.cpp:106] Iteration 19000, lr = 0.00703125
I0704 07:50:31.000924 25348 solver.cpp:375] Finding and applying sparsity: 0.3
I0704 07:50:31.357607 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:50:33.476666 25348 solver.cpp:290] Iteration 19100 (40.3849 iter/s, 2.47617s/100 iter), loss = 0.0476188
I0704 07:50:33.476687 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:50:33.476694 25348 sgd_solver.cpp:106] Iteration 19100, lr = 0.00701563
I0704 07:50:35.556211 25348 solver.cpp:290] Iteration 19200 (48.0894 iter/s, 2.07946s/100 iter), loss = -2.5332e-07
I0704 07:50:35.556233 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:35.556241 25348 sgd_solver.cpp:106] Iteration 19200, lr = 0.007
I0704 07:50:37.626026 25348 solver.cpp:290] Iteration 19300 (48.3155 iter/s, 2.06973s/100 iter), loss = -2.5332e-07
I0704 07:50:37.626049 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:37.626055 25348 sgd_solver.cpp:106] Iteration 19300, lr = 0.00698437
I0704 07:50:39.697494 25348 solver.cpp:290] Iteration 19400 (48.277 iter/s, 2.07138s/100 iter), loss = 0.0476188
I0704 07:50:39.697515 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:50:39.697522 25348 sgd_solver.cpp:106] Iteration 19400, lr = 0.00696875
I0704 07:50:41.768862 25348 solver.cpp:290] Iteration 19500 (48.2793 iter/s, 2.07128s/100 iter), loss = -2.5332e-07
I0704 07:50:41.768885 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:41.768893 25348 sgd_solver.cpp:106] Iteration 19500, lr = 0.00695312
I0704 07:50:43.838079 25348 solver.cpp:290] Iteration 19600 (48.3295 iter/s, 2.06913s/100 iter), loss = -2.5332e-07
I0704 07:50:43.838102 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:43.838109 25348 sgd_solver.cpp:106] Iteration 19600, lr = 0.0069375
I0704 07:50:45.907618 25348 solver.cpp:290] Iteration 19700 (48.322 iter/s, 2.06945s/100 iter), loss = -2.5332e-07
I0704 07:50:45.907640 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:45.907646 25348 sgd_solver.cpp:106] Iteration 19700, lr = 0.00692187
I0704 07:50:47.977444 25348 solver.cpp:290] Iteration 19800 (48.3153 iter/s, 2.06974s/100 iter), loss = -2.5332e-07
I0704 07:50:47.977474 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:47.977484 25348 sgd_solver.cpp:106] Iteration 19800, lr = 0.00690625
I0704 07:50:50.058801 25348 solver.cpp:290] Iteration 19900 (48.0477 iter/s, 2.08126s/100 iter), loss = -2.5332e-07
I0704 07:50:50.058828 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:50.058837 25348 sgd_solver.cpp:106] Iteration 19900, lr = 0.00689062
I0704 07:50:52.108675 25348 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_20000.caffemodel
I0704 07:50:52.125344 25348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_20000.solverstate
I0704 07:50:52.133010 25348 solver.cpp:354] Sparsity after update:
I0704 07:50:52.133965 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:50:52.133975 25348 net.cpp:1851] conv1a_param_0(0.15) 
I0704 07:50:52.133985 25348 net.cpp:1851] conv1b_param_0(0.3) 
I0704 07:50:52.133991 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:50:52.133996 25348 net.cpp:1851] res2a_branch2a_param_0(0.3) 
I0704 07:50:52.134001 25348 net.cpp:1851] res2a_branch2b_param_0(0.3) 
I0704 07:50:52.134014 25348 net.cpp:1851] res3a_branch2a_param_0(0.3) 
I0704 07:50:52.134018 25348 net.cpp:1851] res3a_branch2b_param_0(0.3) 
I0704 07:50:52.134022 25348 net.cpp:1851] res4a_branch2a_param_0(0.3) 
I0704 07:50:52.134027 25348 net.cpp:1851] res4a_branch2b_param_0(0.3) 
I0704 07:50:52.134030 25348 net.cpp:1851] res5a_branch2a_param_0(0.3) 
I0704 07:50:52.134034 25348 net.cpp:1851] res5a_branch2b_param_0(0.3) 
I0704 07:50:52.134038 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (706069/2.3599e+06) 0.299
I0704 07:50:52.134140 25348 solver.cpp:466] Iteration 20000, Testing net (#0)
I0704 07:50:53.790336 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8362
I0704 07:50:53.790355 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9914
I0704 07:50:53.790362 25348 solver.cpp:539]     Test net output #2: loss = 0.4035 (* 1 = 0.4035 loss)
I0704 07:50:53.810799 25348 solver.cpp:290] Iteration 20000 (26.6534 iter/s, 3.75186s/100 iter), loss = -2.5332e-07
I0704 07:50:53.810818 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:53.810830 25348 sgd_solver.cpp:106] Iteration 20000, lr = 0.006875
I0704 07:50:53.811416 25348 solver.cpp:375] Finding and applying sparsity: 0.32
I0704 07:50:54.174671 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:50:56.270416 25348 solver.cpp:290] Iteration 20100 (40.6583 iter/s, 2.45953s/100 iter), loss = -2.5332e-07
I0704 07:50:56.270439 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:56.270445 25348 sgd_solver.cpp:106] Iteration 20100, lr = 0.00685938
I0704 07:50:58.343616 25348 solver.cpp:290] Iteration 20200 (48.2366 iter/s, 2.07311s/100 iter), loss = -2.5332e-07
I0704 07:50:58.343641 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:50:58.343647 25348 sgd_solver.cpp:106] Iteration 20200, lr = 0.00684375
I0704 07:51:00.421456 25348 solver.cpp:290] Iteration 20300 (48.1289 iter/s, 2.07775s/100 iter), loss = -2.5332e-07
I0704 07:51:00.421479 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:00.421486 25348 sgd_solver.cpp:106] Iteration 20300, lr = 0.00682813
I0704 07:51:02.500118 25348 solver.cpp:290] Iteration 20400 (48.1099 iter/s, 2.07857s/100 iter), loss = -2.5332e-07
I0704 07:51:02.500191 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:02.500200 25348 sgd_solver.cpp:106] Iteration 20400, lr = 0.0068125
I0704 07:51:04.571827 25348 solver.cpp:290] Iteration 20500 (48.2725 iter/s, 2.07157s/100 iter), loss = 0.0952378
I0704 07:51:04.571849 25348 solver.cpp:309]     Train net output #0: loss = 0.0952381 (* 1 = 0.0952381 loss)
I0704 07:51:04.571856 25348 sgd_solver.cpp:106] Iteration 20500, lr = 0.00679688
I0704 07:51:06.647766 25348 solver.cpp:290] Iteration 20600 (48.173 iter/s, 2.07585s/100 iter), loss = -2.5332e-07
I0704 07:51:06.647789 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:06.647795 25348 sgd_solver.cpp:106] Iteration 20600, lr = 0.00678125
I0704 07:51:08.720703 25348 solver.cpp:290] Iteration 20700 (48.2427 iter/s, 2.07285s/100 iter), loss = -2.5332e-07
I0704 07:51:08.720724 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:08.720731 25348 sgd_solver.cpp:106] Iteration 20700, lr = 0.00676562
I0704 07:51:10.795603 25348 solver.cpp:290] Iteration 20800 (48.1971 iter/s, 2.07481s/100 iter), loss = -2.5332e-07
I0704 07:51:10.795625 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:10.795631 25348 sgd_solver.cpp:106] Iteration 20800, lr = 0.00675
I0704 07:51:12.870414 25348 solver.cpp:290] Iteration 20900 (48.1992 iter/s, 2.07472s/100 iter), loss = -2.5332e-07
I0704 07:51:12.870435 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:12.870442 25348 sgd_solver.cpp:106] Iteration 20900, lr = 0.00673437
I0704 07:51:14.923179 25348 solver.cpp:354] Sparsity after update:
I0704 07:51:14.924594 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:51:14.924602 25348 net.cpp:1851] conv1a_param_0(0.16) 
I0704 07:51:14.924612 25348 net.cpp:1851] conv1b_param_0(0.32) 
I0704 07:51:14.924616 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:51:14.924621 25348 net.cpp:1851] res2a_branch2a_param_0(0.32) 
I0704 07:51:14.924625 25348 net.cpp:1851] res2a_branch2b_param_0(0.32) 
I0704 07:51:14.924629 25348 net.cpp:1851] res3a_branch2a_param_0(0.32) 
I0704 07:51:14.924633 25348 net.cpp:1851] res3a_branch2b_param_0(0.32) 
I0704 07:51:14.924638 25348 net.cpp:1851] res4a_branch2a_param_0(0.32) 
I0704 07:51:14.924641 25348 net.cpp:1851] res4a_branch2b_param_0(0.32) 
I0704 07:51:14.924645 25348 net.cpp:1851] res5a_branch2a_param_0(0.32) 
I0704 07:51:14.924649 25348 net.cpp:1851] res5a_branch2b_param_0(0.32) 
I0704 07:51:14.924654 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (753141/2.3599e+06) 0.319
I0704 07:51:14.924746 25348 solver.cpp:466] Iteration 21000, Testing net (#0)
I0704 07:51:16.568506 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8713
I0704 07:51:16.568526 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9932
I0704 07:51:16.568533 25348 solver.cpp:539]     Test net output #2: loss = 0.2795 (* 1 = 0.2795 loss)
I0704 07:51:16.589910 25348 solver.cpp:290] Iteration 21000 (26.8863 iter/s, 3.71937s/100 iter), loss = -2.6077e-07
I0704 07:51:16.589929 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:16.589939 25348 sgd_solver.cpp:106] Iteration 21000, lr = 0.00671875
I0704 07:51:16.590468 25348 solver.cpp:375] Finding and applying sparsity: 0.34
I0704 07:51:17.014266 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:51:19.103078 25348 solver.cpp:290] Iteration 21100 (39.7919 iter/s, 2.51308s/100 iter), loss = -2.6077e-07
I0704 07:51:19.103099 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:19.103107 25348 sgd_solver.cpp:106] Iteration 21100, lr = 0.00670313
I0704 07:51:21.175451 25348 solver.cpp:290] Iteration 21200 (48.2559 iter/s, 2.07229s/100 iter), loss = -2.6077e-07
I0704 07:51:21.175473 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:21.175480 25348 sgd_solver.cpp:106] Iteration 21200, lr = 0.0066875
I0704 07:51:23.252328 25348 solver.cpp:290] Iteration 21300 (48.1512 iter/s, 2.07679s/100 iter), loss = -2.6077e-07
I0704 07:51:23.252367 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:23.252374 25348 sgd_solver.cpp:106] Iteration 21300, lr = 0.00667187
I0704 07:51:25.322289 25348 solver.cpp:290] Iteration 21400 (48.3125 iter/s, 2.06986s/100 iter), loss = -2.5332e-07
I0704 07:51:25.322311 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:25.322319 25348 sgd_solver.cpp:106] Iteration 21400, lr = 0.00665625
I0704 07:51:27.394964 25348 solver.cpp:290] Iteration 21500 (48.2488 iter/s, 2.07259s/100 iter), loss = -2.5332e-07
I0704 07:51:27.394987 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:27.394994 25348 sgd_solver.cpp:106] Iteration 21500, lr = 0.00664062
I0704 07:51:29.467912 25348 solver.cpp:290] Iteration 21600 (48.2425 iter/s, 2.07286s/100 iter), loss = -2.5332e-07
I0704 07:51:29.467936 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:29.467941 25348 sgd_solver.cpp:106] Iteration 21600, lr = 0.006625
I0704 07:51:31.544471 25348 solver.cpp:290] Iteration 21700 (48.1586 iter/s, 2.07647s/100 iter), loss = -2.5332e-07
I0704 07:51:31.544492 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:31.544499 25348 sgd_solver.cpp:106] Iteration 21700, lr = 0.00660937
I0704 07:51:33.644775 25348 solver.cpp:290] Iteration 21800 (47.6141 iter/s, 2.10022s/100 iter), loss = -2.5332e-07
I0704 07:51:33.644847 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:33.644858 25348 sgd_solver.cpp:106] Iteration 21800, lr = 0.00659375
I0704 07:51:35.719146 25348 solver.cpp:290] Iteration 21900 (48.2105 iter/s, 2.07424s/100 iter), loss = -2.5332e-07
I0704 07:51:35.719167 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:35.719174 25348 sgd_solver.cpp:106] Iteration 21900, lr = 0.00657812
I0704 07:51:37.773211 25348 solver.cpp:354] Sparsity after update:
I0704 07:51:37.774622 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:51:37.774631 25348 net.cpp:1851] conv1a_param_0(0.17) 
I0704 07:51:37.774641 25348 net.cpp:1851] conv1b_param_0(0.34) 
I0704 07:51:37.774646 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:51:37.774649 25348 net.cpp:1851] res2a_branch2a_param_0(0.34) 
I0704 07:51:37.774654 25348 net.cpp:1851] res2a_branch2b_param_0(0.34) 
I0704 07:51:37.774658 25348 net.cpp:1851] res3a_branch2a_param_0(0.34) 
I0704 07:51:37.774662 25348 net.cpp:1851] res3a_branch2b_param_0(0.34) 
I0704 07:51:37.774667 25348 net.cpp:1851] res4a_branch2a_param_0(0.34) 
I0704 07:51:37.774672 25348 net.cpp:1851] res4a_branch2b_param_0(0.34) 
I0704 07:51:37.774677 25348 net.cpp:1851] res5a_branch2a_param_0(0.34) 
I0704 07:51:37.774680 25348 net.cpp:1851] res5a_branch2b_param_0(0.34) 
I0704 07:51:37.774685 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (800212/2.3599e+06) 0.339
I0704 07:51:37.774780 25348 solver.cpp:466] Iteration 22000, Testing net (#0)
I0704 07:51:39.432461 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8707
I0704 07:51:39.432482 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9875
I0704 07:51:39.432487 25348 solver.cpp:539]     Test net output #2: loss = 0.2971 (* 1 = 0.2971 loss)
I0704 07:51:39.452585 25348 solver.cpp:290] Iteration 22000 (26.7859 iter/s, 3.73331s/100 iter), loss = -2.5332e-07
I0704 07:51:39.452603 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:39.452616 25348 sgd_solver.cpp:106] Iteration 22000, lr = 0.0065625
I0704 07:51:39.453168 25348 solver.cpp:375] Finding and applying sparsity: 0.36
I0704 07:51:39.901208 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:51:41.995666 25348 solver.cpp:290] Iteration 22100 (39.3239 iter/s, 2.54299s/100 iter), loss = -2.5332e-07
I0704 07:51:41.995687 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:41.995693 25348 sgd_solver.cpp:106] Iteration 22100, lr = 0.00654687
I0704 07:51:44.068675 25348 solver.cpp:290] Iteration 22200 (48.2411 iter/s, 2.07292s/100 iter), loss = -2.5332e-07
I0704 07:51:44.068697 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:44.068706 25348 sgd_solver.cpp:106] Iteration 22200, lr = 0.00653125
I0704 07:51:46.147434 25348 solver.cpp:290] Iteration 22300 (48.1076 iter/s, 2.07867s/100 iter), loss = -2.5332e-07
I0704 07:51:46.147457 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:46.147465 25348 sgd_solver.cpp:106] Iteration 22300, lr = 0.00651562
I0704 07:51:48.219835 25348 solver.cpp:290] Iteration 22400 (48.2553 iter/s, 2.07231s/100 iter), loss = -2.5332e-07
I0704 07:51:48.219857 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:48.219866 25348 sgd_solver.cpp:106] Iteration 22400, lr = 0.0065
I0704 07:51:50.291261 25348 solver.cpp:290] Iteration 22500 (48.2779 iter/s, 2.07134s/100 iter), loss = -2.5332e-07
I0704 07:51:50.291285 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:50.291290 25348 sgd_solver.cpp:106] Iteration 22500, lr = 0.00648437
I0704 07:51:52.362565 25348 solver.cpp:290] Iteration 22600 (48.2808 iter/s, 2.07122s/100 iter), loss = -2.5332e-07
I0704 07:51:52.362588 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:52.362594 25348 sgd_solver.cpp:106] Iteration 22600, lr = 0.00646875
I0704 07:51:54.435712 25348 solver.cpp:290] Iteration 22700 (48.2378 iter/s, 2.07306s/100 iter), loss = 0.0476188
I0704 07:51:54.435752 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:51:54.435760 25348 sgd_solver.cpp:106] Iteration 22700, lr = 0.00645312
I0704 07:51:56.505913 25348 solver.cpp:290] Iteration 22800 (48.3069 iter/s, 2.0701s/100 iter), loss = -2.5332e-07
I0704 07:51:56.505935 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:56.505941 25348 sgd_solver.cpp:106] Iteration 22800, lr = 0.0064375
I0704 07:51:58.582919 25348 solver.cpp:290] Iteration 22900 (48.1482 iter/s, 2.07692s/100 iter), loss = -2.5332e-07
I0704 07:51:58.582942 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:51:58.582948 25348 sgd_solver.cpp:106] Iteration 22900, lr = 0.00642187
I0704 07:52:00.641165 25348 solver.cpp:354] Sparsity after update:
I0704 07:52:00.642571 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:52:00.642580 25348 net.cpp:1851] conv1a_param_0(0.18) 
I0704 07:52:00.642588 25348 net.cpp:1851] conv1b_param_0(0.36) 
I0704 07:52:00.642590 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:52:00.642593 25348 net.cpp:1851] res2a_branch2a_param_0(0.36) 
I0704 07:52:00.642596 25348 net.cpp:1851] res2a_branch2b_param_0(0.36) 
I0704 07:52:00.642597 25348 net.cpp:1851] res3a_branch2a_param_0(0.36) 
I0704 07:52:00.642599 25348 net.cpp:1851] res3a_branch2b_param_0(0.36) 
I0704 07:52:00.642602 25348 net.cpp:1851] res4a_branch2a_param_0(0.36) 
I0704 07:52:00.642604 25348 net.cpp:1851] res4a_branch2b_param_0(0.36) 
I0704 07:52:00.642606 25348 net.cpp:1851] res5a_branch2a_param_0(0.36) 
I0704 07:52:00.642608 25348 net.cpp:1851] res5a_branch2b_param_0(0.36) 
I0704 07:52:00.642611 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (847286/2.3599e+06) 0.359
I0704 07:52:00.642741 25348 solver.cpp:466] Iteration 23000, Testing net (#0)
I0704 07:52:02.284694 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8621
I0704 07:52:02.284713 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9948
I0704 07:52:02.284718 25348 solver.cpp:539]     Test net output #2: loss = 0.2992 (* 1 = 0.2992 loss)
I0704 07:52:02.304344 25348 solver.cpp:290] Iteration 23000 (26.8724 iter/s, 3.7213s/100 iter), loss = -2.6077e-07
I0704 07:52:02.304361 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:02.304376 25348 sgd_solver.cpp:106] Iteration 23000, lr = 0.00640625
I0704 07:52:02.304910 25348 solver.cpp:375] Finding and applying sparsity: 0.38
I0704 07:52:02.802547 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:52:04.901708 25348 solver.cpp:290] Iteration 23100 (38.502 iter/s, 2.59727s/100 iter), loss = -2.5332e-07
I0704 07:52:04.901783 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:04.901792 25348 sgd_solver.cpp:106] Iteration 23100, lr = 0.00639063
I0704 07:52:06.974179 25348 solver.cpp:290] Iteration 23200 (48.2548 iter/s, 2.07233s/100 iter), loss = -2.5332e-07
I0704 07:52:06.974203 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:06.974210 25348 sgd_solver.cpp:106] Iteration 23200, lr = 0.006375
I0704 07:52:09.048966 25348 solver.cpp:290] Iteration 23300 (48.1998 iter/s, 2.0747s/100 iter), loss = -2.5332e-07
I0704 07:52:09.048996 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:09.049005 25348 sgd_solver.cpp:106] Iteration 23300, lr = 0.00635938
I0704 07:52:11.121562 25348 solver.cpp:290] Iteration 23400 (48.2508 iter/s, 2.0725s/100 iter), loss = -2.5332e-07
I0704 07:52:11.121584 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:11.121593 25348 sgd_solver.cpp:106] Iteration 23400, lr = 0.00634375
I0704 07:52:13.193598 25348 solver.cpp:290] Iteration 23500 (48.2637 iter/s, 2.07195s/100 iter), loss = -2.5332e-07
I0704 07:52:13.193620 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:13.193629 25348 sgd_solver.cpp:106] Iteration 23500, lr = 0.00632813
I0704 07:52:15.271147 25348 solver.cpp:290] Iteration 23600 (48.1356 iter/s, 2.07746s/100 iter), loss = -2.5332e-07
I0704 07:52:15.271173 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:15.271180 25348 sgd_solver.cpp:106] Iteration 23600, lr = 0.0063125
I0704 07:52:17.345437 25348 solver.cpp:290] Iteration 23700 (48.2113 iter/s, 2.0742s/100 iter), loss = -2.5332e-07
I0704 07:52:17.345458 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:17.345465 25348 sgd_solver.cpp:106] Iteration 23700, lr = 0.00629687
I0704 07:52:19.423104 25348 solver.cpp:290] Iteration 23800 (48.1329 iter/s, 2.07758s/100 iter), loss = -2.5332e-07
I0704 07:52:19.423125 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:19.423132 25348 sgd_solver.cpp:106] Iteration 23800, lr = 0.00628125
I0704 07:52:21.494153 25348 solver.cpp:290] Iteration 23900 (48.2867 iter/s, 2.07096s/100 iter), loss = -2.5332e-07
I0704 07:52:21.494174 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:21.494181 25348 sgd_solver.cpp:106] Iteration 23900, lr = 0.00626562
I0704 07:52:23.546813 25348 solver.cpp:354] Sparsity after update:
I0704 07:52:23.548399 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:52:23.548408 25348 net.cpp:1851] conv1a_param_0(0.19) 
I0704 07:52:23.548415 25348 net.cpp:1851] conv1b_param_0(0.38) 
I0704 07:52:23.548418 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:52:23.548420 25348 net.cpp:1851] res2a_branch2a_param_0(0.38) 
I0704 07:52:23.548422 25348 net.cpp:1851] res2a_branch2b_param_0(0.38) 
I0704 07:52:23.548424 25348 net.cpp:1851] res3a_branch2a_param_0(0.38) 
I0704 07:52:23.548426 25348 net.cpp:1851] res3a_branch2b_param_0(0.38) 
I0704 07:52:23.548429 25348 net.cpp:1851] res4a_branch2a_param_0(0.38) 
I0704 07:52:23.548430 25348 net.cpp:1851] res4a_branch2b_param_0(0.38) 
I0704 07:52:23.548432 25348 net.cpp:1851] res5a_branch2a_param_0(0.38) 
I0704 07:52:23.548434 25348 net.cpp:1851] res5a_branch2b_param_0(0.38) 
I0704 07:52:23.548436 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (894352/2.3599e+06) 0.379
I0704 07:52:23.548537 25348 solver.cpp:466] Iteration 24000, Testing net (#0)
I0704 07:52:25.192459 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8765
I0704 07:52:25.192479 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9947
I0704 07:52:25.192484 25348 solver.cpp:539]     Test net output #2: loss = 0.2554 (* 1 = 0.2554 loss)
I0704 07:52:25.213939 25348 solver.cpp:290] Iteration 24000 (26.8842 iter/s, 3.71966s/100 iter), loss = -2.5332e-07
I0704 07:52:25.213958 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:25.213971 25348 sgd_solver.cpp:106] Iteration 24000, lr = 0.00625
I0704 07:52:25.214490 25348 solver.cpp:375] Finding and applying sparsity: 0.4
I0704 07:52:25.721491 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:52:27.807413 25348 solver.cpp:290] Iteration 24100 (38.5597 iter/s, 2.59338s/100 iter), loss = -2.5332e-07
I0704 07:52:27.807435 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:27.807445 25348 sgd_solver.cpp:106] Iteration 24100, lr = 0.00623438
I0704 07:52:29.879256 25348 solver.cpp:290] Iteration 24200 (48.2682 iter/s, 2.07176s/100 iter), loss = -2.5332e-07
I0704 07:52:29.879279 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:29.879286 25348 sgd_solver.cpp:106] Iteration 24200, lr = 0.00621875
I0704 07:52:31.954195 25348 solver.cpp:290] Iteration 24300 (48.1962 iter/s, 2.07485s/100 iter), loss = -2.5332e-07
I0704 07:52:31.954217 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:31.954224 25348 sgd_solver.cpp:106] Iteration 24300, lr = 0.00620312
I0704 07:52:34.032842 25348 solver.cpp:290] Iteration 24400 (48.1102 iter/s, 2.07856s/100 iter), loss = -2.5332e-07
I0704 07:52:34.032866 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:34.032874 25348 sgd_solver.cpp:106] Iteration 24400, lr = 0.0061875
I0704 07:52:36.103787 25348 solver.cpp:290] Iteration 24500 (48.2891 iter/s, 2.07086s/100 iter), loss = -2.5332e-07
I0704 07:52:36.103838 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:36.103847 25348 sgd_solver.cpp:106] Iteration 24500, lr = 0.00617187
I0704 07:52:38.177705 25348 solver.cpp:290] Iteration 24600 (48.2206 iter/s, 2.0738s/100 iter), loss = -2.5332e-07
I0704 07:52:38.177726 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:38.177733 25348 sgd_solver.cpp:106] Iteration 24600, lr = 0.00615625
I0704 07:52:40.248935 25348 solver.cpp:290] Iteration 24700 (48.2825 iter/s, 2.07115s/100 iter), loss = -2.5332e-07
I0704 07:52:40.248956 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:40.248963 25348 sgd_solver.cpp:106] Iteration 24700, lr = 0.00614062
I0704 07:52:42.326685 25348 solver.cpp:290] Iteration 24800 (48.1309 iter/s, 2.07767s/100 iter), loss = -2.5332e-07
I0704 07:52:42.326707 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:42.326714 25348 sgd_solver.cpp:106] Iteration 24800, lr = 0.006125
I0704 07:52:44.399396 25348 solver.cpp:290] Iteration 24900 (48.2479 iter/s, 2.07263s/100 iter), loss = -2.5332e-07
I0704 07:52:44.399420 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:44.399425 25348 sgd_solver.cpp:106] Iteration 24900, lr = 0.00610937
I0704 07:52:46.454818 25348 solver.cpp:354] Sparsity after update:
I0704 07:52:46.456205 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:52:46.456213 25348 net.cpp:1851] conv1a_param_0(0.2) 
I0704 07:52:46.456220 25348 net.cpp:1851] conv1b_param_0(0.4) 
I0704 07:52:46.456223 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:52:46.456224 25348 net.cpp:1851] res2a_branch2a_param_0(0.4) 
I0704 07:52:46.456226 25348 net.cpp:1851] res2a_branch2b_param_0(0.4) 
I0704 07:52:46.456228 25348 net.cpp:1851] res3a_branch2a_param_0(0.4) 
I0704 07:52:46.456230 25348 net.cpp:1851] res3a_branch2b_param_0(0.4) 
I0704 07:52:46.456233 25348 net.cpp:1851] res4a_branch2a_param_0(0.4) 
I0704 07:52:46.456234 25348 net.cpp:1851] res4a_branch2b_param_0(0.4) 
I0704 07:52:46.456236 25348 net.cpp:1851] res5a_branch2a_param_0(0.4) 
I0704 07:52:46.456238 25348 net.cpp:1851] res5a_branch2b_param_0(0.4) 
I0704 07:52:46.456240 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (941425/2.3599e+06) 0.399
I0704 07:52:46.456368 25348 solver.cpp:466] Iteration 25000, Testing net (#0)
I0704 07:52:48.101647 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8679
I0704 07:52:48.101666 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9886
I0704 07:52:48.101671 25348 solver.cpp:539]     Test net output #2: loss = 0.2986 (* 1 = 0.2986 loss)
I0704 07:52:48.122709 25348 solver.cpp:290] Iteration 25000 (26.8587 iter/s, 3.72318s/100 iter), loss = -2.5332e-07
I0704 07:52:48.122725 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:48.122735 25348 sgd_solver.cpp:106] Iteration 25000, lr = 0.00609375
I0704 07:52:48.123291 25348 solver.cpp:375] Finding and applying sparsity: 0.42
I0704 07:52:48.712899 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:52:50.812081 25348 solver.cpp:290] Iteration 25100 (37.1847 iter/s, 2.68927s/100 iter), loss = -2.5332e-07
I0704 07:52:50.812103 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:50.812111 25348 sgd_solver.cpp:106] Iteration 25100, lr = 0.00607812
I0704 07:52:52.884665 25348 solver.cpp:290] Iteration 25200 (48.251 iter/s, 2.0725s/100 iter), loss = -2.5332e-07
I0704 07:52:52.884687 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:52.884696 25348 sgd_solver.cpp:106] Iteration 25200, lr = 0.0060625
I0704 07:52:54.956020 25348 solver.cpp:290] Iteration 25300 (48.2795 iter/s, 2.07127s/100 iter), loss = -2.5332e-07
I0704 07:52:54.956043 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:54.956049 25348 sgd_solver.cpp:106] Iteration 25300, lr = 0.00604687
I0704 07:52:57.027595 25348 solver.cpp:290] Iteration 25400 (48.2745 iter/s, 2.07149s/100 iter), loss = -2.5332e-07
I0704 07:52:57.027631 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:57.027637 25348 sgd_solver.cpp:106] Iteration 25400, lr = 0.00603125
I0704 07:52:59.104912 25348 solver.cpp:290] Iteration 25500 (48.1413 iter/s, 2.07722s/100 iter), loss = -2.5332e-07
I0704 07:52:59.104935 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:52:59.104943 25348 sgd_solver.cpp:106] Iteration 25500, lr = 0.00601562
I0704 07:53:01.186434 25348 solver.cpp:290] Iteration 25600 (48.0438 iter/s, 2.08143s/100 iter), loss = -2.5332e-07
I0704 07:53:01.186455 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:01.186462 25348 sgd_solver.cpp:106] Iteration 25600, lr = 0.006
I0704 07:53:03.258388 25348 solver.cpp:290] Iteration 25700 (48.2657 iter/s, 2.07186s/100 iter), loss = -2.5332e-07
I0704 07:53:03.258409 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:03.258415 25348 sgd_solver.cpp:106] Iteration 25700, lr = 0.00598437
I0704 07:53:05.330869 25348 solver.cpp:290] Iteration 25800 (48.2533 iter/s, 2.0724s/100 iter), loss = -2.5332e-07
I0704 07:53:05.330893 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:05.330899 25348 sgd_solver.cpp:106] Iteration 25800, lr = 0.00596875
I0704 07:53:07.406355 25348 solver.cpp:290] Iteration 25900 (48.1835 iter/s, 2.0754s/100 iter), loss = -2.5332e-07
I0704 07:53:07.406422 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:07.406430 25348 sgd_solver.cpp:106] Iteration 25900, lr = 0.00595312
I0704 07:53:09.457937 25348 solver.cpp:354] Sparsity after update:
I0704 07:53:09.459336 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:53:09.459342 25348 net.cpp:1851] conv1a_param_0(0.21) 
I0704 07:53:09.459350 25348 net.cpp:1851] conv1b_param_0(0.42) 
I0704 07:53:09.459353 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:53:09.459355 25348 net.cpp:1851] res2a_branch2a_param_0(0.42) 
I0704 07:53:09.459358 25348 net.cpp:1851] res2a_branch2b_param_0(0.42) 
I0704 07:53:09.459360 25348 net.cpp:1851] res3a_branch2a_param_0(0.42) 
I0704 07:53:09.459362 25348 net.cpp:1851] res3a_branch2b_param_0(0.42) 
I0704 07:53:09.459364 25348 net.cpp:1851] res4a_branch2a_param_0(0.42) 
I0704 07:53:09.459367 25348 net.cpp:1851] res4a_branch2b_param_0(0.42) 
I0704 07:53:09.459368 25348 net.cpp:1851] res5a_branch2a_param_0(0.42) 
I0704 07:53:09.459370 25348 net.cpp:1851] res5a_branch2b_param_0(0.42) 
I0704 07:53:09.459372 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (988498/2.3599e+06) 0.419
I0704 07:53:09.459460 25348 solver.cpp:466] Iteration 26000, Testing net (#0)
I0704 07:53:11.102599 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.846
I0704 07:53:11.102618 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9886
I0704 07:53:11.102623 25348 solver.cpp:539]     Test net output #2: loss = 0.3719 (* 1 = 0.3719 loss)
I0704 07:53:11.123780 25348 solver.cpp:290] Iteration 26000 (26.9016 iter/s, 3.71725s/100 iter), loss = -2.5332e-07
I0704 07:53:11.123796 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:11.123808 25348 sgd_solver.cpp:106] Iteration 26000, lr = 0.0059375
I0704 07:53:11.124349 25348 solver.cpp:375] Finding and applying sparsity: 0.44
I0704 07:53:11.757253 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:53:13.857389 25348 solver.cpp:290] Iteration 26100 (36.583 iter/s, 2.73351s/100 iter), loss = -2.5332e-07
I0704 07:53:13.857412 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:13.857419 25348 sgd_solver.cpp:106] Iteration 26100, lr = 0.00592188
I0704 07:53:15.929682 25348 solver.cpp:290] Iteration 26200 (48.2578 iter/s, 2.07221s/100 iter), loss = -2.5332e-07
I0704 07:53:15.929703 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:15.929711 25348 sgd_solver.cpp:106] Iteration 26200, lr = 0.00590625
I0704 07:53:18.000599 25348 solver.cpp:290] Iteration 26300 (48.2898 iter/s, 2.07083s/100 iter), loss = -2.5332e-07
I0704 07:53:18.000623 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:18.000633 25348 sgd_solver.cpp:106] Iteration 26300, lr = 0.00589063
I0704 07:53:20.078788 25348 solver.cpp:290] Iteration 26400 (48.1208 iter/s, 2.0781s/100 iter), loss = -2.5332e-07
I0704 07:53:20.078810 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:20.078817 25348 sgd_solver.cpp:106] Iteration 26400, lr = 0.005875
I0704 07:53:22.158401 25348 solver.cpp:290] Iteration 26500 (48.0879 iter/s, 2.07953s/100 iter), loss = -2.5332e-07
I0704 07:53:22.158422 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:22.158428 25348 sgd_solver.cpp:106] Iteration 26500, lr = 0.00585938
I0704 07:53:24.237972 25348 solver.cpp:290] Iteration 26600 (48.0889 iter/s, 2.07948s/100 iter), loss = -2.5332e-07
I0704 07:53:24.237998 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:24.238006 25348 sgd_solver.cpp:106] Iteration 26600, lr = 0.00584375
I0704 07:53:26.314012 25348 solver.cpp:290] Iteration 26700 (48.1707 iter/s, 2.07595s/100 iter), loss = -2.5332e-07
I0704 07:53:26.314034 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:26.314041 25348 sgd_solver.cpp:106] Iteration 26700, lr = 0.00582812
I0704 07:53:28.388267 25348 solver.cpp:290] Iteration 26800 (48.2121 iter/s, 2.07417s/100 iter), loss = -2.5332e-07
I0704 07:53:28.388304 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:28.388311 25348 sgd_solver.cpp:106] Iteration 26800, lr = 0.0058125
I0704 07:53:30.460083 25348 solver.cpp:290] Iteration 26900 (48.2692 iter/s, 2.07172s/100 iter), loss = -2.5332e-07
I0704 07:53:30.460108 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:30.460115 25348 sgd_solver.cpp:106] Iteration 26900, lr = 0.00579687
I0704 07:53:32.511368 25348 solver.cpp:354] Sparsity after update:
I0704 07:53:32.512759 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:53:32.512768 25348 net.cpp:1851] conv1a_param_0(0.22) 
I0704 07:53:32.512776 25348 net.cpp:1851] conv1b_param_0(0.44) 
I0704 07:53:32.512780 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:53:32.512784 25348 net.cpp:1851] res2a_branch2a_param_0(0.44) 
I0704 07:53:32.512789 25348 net.cpp:1851] res2a_branch2b_param_0(0.44) 
I0704 07:53:32.512792 25348 net.cpp:1851] res3a_branch2a_param_0(0.44) 
I0704 07:53:32.512796 25348 net.cpp:1851] res3a_branch2b_param_0(0.44) 
I0704 07:53:32.512800 25348 net.cpp:1851] res4a_branch2a_param_0(0.44) 
I0704 07:53:32.512804 25348 net.cpp:1851] res4a_branch2b_param_0(0.44) 
I0704 07:53:32.512809 25348 net.cpp:1851] res5a_branch2a_param_0(0.44) 
I0704 07:53:32.512812 25348 net.cpp:1851] res5a_branch2b_param_0(0.44) 
I0704 07:53:32.512816 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.03557e+06/2.3599e+06) 0.439
I0704 07:53:32.512908 25348 solver.cpp:466] Iteration 27000, Testing net (#0)
I0704 07:53:34.156785 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8887
I0704 07:53:34.156805 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9937
I0704 07:53:34.156810 25348 solver.cpp:539]     Test net output #2: loss = 0.2318 (* 1 = 0.2318 loss)
I0704 07:53:34.176414 25348 solver.cpp:290] Iteration 27000 (26.9092 iter/s, 3.7162s/100 iter), loss = -2.5332e-07
I0704 07:53:34.176430 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:34.176442 25348 sgd_solver.cpp:106] Iteration 27000, lr = 0.00578125
I0704 07:53:34.176956 25348 solver.cpp:375] Finding and applying sparsity: 0.46
I0704 07:53:34.890564 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:53:36.988903 25348 solver.cpp:290] Iteration 27100 (35.557 iter/s, 2.81239s/100 iter), loss = -2.5332e-07
I0704 07:53:36.988924 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:36.988931 25348 sgd_solver.cpp:106] Iteration 27100, lr = 0.00576563
I0704 07:53:39.064262 25348 solver.cpp:290] Iteration 27200 (48.1864 iter/s, 2.07527s/100 iter), loss = -2.5332e-07
I0704 07:53:39.064343 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:39.064353 25348 sgd_solver.cpp:106] Iteration 27200, lr = 0.00575
I0704 07:53:41.137244 25348 solver.cpp:290] Iteration 27300 (48.243 iter/s, 2.07284s/100 iter), loss = -2.5332e-07
I0704 07:53:41.137269 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:41.137277 25348 sgd_solver.cpp:106] Iteration 27300, lr = 0.00573438
I0704 07:53:43.211098 25348 solver.cpp:290] Iteration 27400 (48.2214 iter/s, 2.07377s/100 iter), loss = -2.5332e-07
I0704 07:53:43.211120 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:43.211127 25348 sgd_solver.cpp:106] Iteration 27400, lr = 0.00571875
I0704 07:53:45.282551 25348 solver.cpp:290] Iteration 27500 (48.2773 iter/s, 2.07137s/100 iter), loss = -2.5332e-07
I0704 07:53:45.282572 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:45.282578 25348 sgd_solver.cpp:106] Iteration 27500, lr = 0.00570312
I0704 07:53:47.355146 25348 solver.cpp:290] Iteration 27600 (48.2506 iter/s, 2.07251s/100 iter), loss = -2.5332e-07
I0704 07:53:47.355171 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:47.355180 25348 sgd_solver.cpp:106] Iteration 27600, lr = 0.0056875
I0704 07:53:49.433187 25348 solver.cpp:290] Iteration 27700 (48.1243 iter/s, 2.07795s/100 iter), loss = -2.5332e-07
I0704 07:53:49.433213 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:49.433219 25348 sgd_solver.cpp:106] Iteration 27700, lr = 0.00567187
I0704 07:53:51.507174 25348 solver.cpp:290] Iteration 27800 (48.2184 iter/s, 2.07389s/100 iter), loss = -2.5332e-07
I0704 07:53:51.507200 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:51.507207 25348 sgd_solver.cpp:106] Iteration 27800, lr = 0.00565625
I0704 07:53:53.585644 25348 solver.cpp:290] Iteration 27900 (48.1144 iter/s, 2.07838s/100 iter), loss = -2.5332e-07
I0704 07:53:53.585675 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:53.585687 25348 sgd_solver.cpp:106] Iteration 27900, lr = 0.00564062
I0704 07:53:55.644053 25348 solver.cpp:354] Sparsity after update:
I0704 07:53:55.645606 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:53:55.645614 25348 net.cpp:1851] conv1a_param_0(0.23) 
I0704 07:53:55.645622 25348 net.cpp:1851] conv1b_param_0(0.46) 
I0704 07:53:55.645624 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:53:55.645627 25348 net.cpp:1851] res2a_branch2a_param_0(0.46) 
I0704 07:53:55.645628 25348 net.cpp:1851] res2a_branch2b_param_0(0.46) 
I0704 07:53:55.645632 25348 net.cpp:1851] res3a_branch2a_param_0(0.46) 
I0704 07:53:55.645633 25348 net.cpp:1851] res3a_branch2b_param_0(0.46) 
I0704 07:53:55.645637 25348 net.cpp:1851] res4a_branch2a_param_0(0.46) 
I0704 07:53:55.645638 25348 net.cpp:1851] res4a_branch2b_param_0(0.46) 
I0704 07:53:55.645642 25348 net.cpp:1851] res5a_branch2a_param_0(0.46) 
I0704 07:53:55.645643 25348 net.cpp:1851] res5a_branch2b_param_0(0.46) 
I0704 07:53:55.645645 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.08264e+06/2.3599e+06) 0.459
I0704 07:53:55.645735 25348 solver.cpp:466] Iteration 28000, Testing net (#0)
I0704 07:53:57.288645 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8791
I0704 07:53:57.288663 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9951
I0704 07:53:57.288668 25348 solver.cpp:539]     Test net output #2: loss = 0.2633 (* 1 = 0.2633 loss)
I0704 07:53:57.308953 25348 solver.cpp:290] Iteration 28000 (26.8588 iter/s, 3.72317s/100 iter), loss = -2.5332e-07
I0704 07:53:57.308972 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:53:57.308980 25348 sgd_solver.cpp:106] Iteration 28000, lr = 0.005625
I0704 07:53:57.309518 25348 solver.cpp:375] Finding and applying sparsity: 0.48
I0704 07:53:58.133551 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:54:00.225497 25348 solver.cpp:290] Iteration 28100 (34.2884 iter/s, 2.91644s/100 iter), loss = -2.5332e-07
I0704 07:54:00.225533 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:00.225540 25348 sgd_solver.cpp:106] Iteration 28100, lr = 0.00560937
I0704 07:54:02.297598 25348 solver.cpp:290] Iteration 28200 (48.2625 iter/s, 2.072s/100 iter), loss = -2.5332e-07
I0704 07:54:02.297621 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:02.297626 25348 sgd_solver.cpp:106] Iteration 28200, lr = 0.00559375
I0704 07:54:04.374299 25348 solver.cpp:290] Iteration 28300 (48.1554 iter/s, 2.07661s/100 iter), loss = -2.5332e-07
I0704 07:54:04.374326 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:04.374332 25348 sgd_solver.cpp:106] Iteration 28300, lr = 0.00557812
I0704 07:54:06.447976 25348 solver.cpp:290] Iteration 28400 (48.2256 iter/s, 2.07359s/100 iter), loss = -2.5332e-07
I0704 07:54:06.448000 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:06.448007 25348 sgd_solver.cpp:106] Iteration 28400, lr = 0.0055625
I0704 07:54:08.521772 25348 solver.cpp:290] Iteration 28500 (48.2227 iter/s, 2.07371s/100 iter), loss = -2.5332e-07
I0704 07:54:08.521795 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:08.521803 25348 sgd_solver.cpp:106] Iteration 28500, lr = 0.00554687
I0704 07:54:10.599828 25348 solver.cpp:290] Iteration 28600 (48.1239 iter/s, 2.07797s/100 iter), loss = -2.5332e-07
I0704 07:54:10.599869 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:10.599875 25348 sgd_solver.cpp:106] Iteration 28600, lr = 0.00553125
I0704 07:54:12.672439 25348 solver.cpp:290] Iteration 28700 (48.2507 iter/s, 2.07251s/100 iter), loss = -2.5332e-07
I0704 07:54:12.672463 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:12.672472 25348 sgd_solver.cpp:106] Iteration 28700, lr = 0.00551562
I0704 07:54:14.746520 25348 solver.cpp:290] Iteration 28800 (48.2161 iter/s, 2.074s/100 iter), loss = -2.5332e-07
I0704 07:54:14.746543 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:14.746552 25348 sgd_solver.cpp:106] Iteration 28800, lr = 0.0055
I0704 07:54:16.816256 25348 solver.cpp:290] Iteration 28900 (48.3173 iter/s, 2.06965s/100 iter), loss = -2.5332e-07
I0704 07:54:16.816278 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:16.816285 25348 sgd_solver.cpp:106] Iteration 28900, lr = 0.00548437
I0704 07:54:18.869832 25348 solver.cpp:354] Sparsity after update:
I0704 07:54:18.871220 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:54:18.871228 25348 net.cpp:1851] conv1a_param_0(0.24) 
I0704 07:54:18.871235 25348 net.cpp:1851] conv1b_param_0(0.479) 
I0704 07:54:18.871238 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:54:18.871242 25348 net.cpp:1851] res2a_branch2a_param_0(0.48) 
I0704 07:54:18.871243 25348 net.cpp:1851] res2a_branch2b_param_0(0.48) 
I0704 07:54:18.871245 25348 net.cpp:1851] res3a_branch2a_param_0(0.48) 
I0704 07:54:18.871248 25348 net.cpp:1851] res3a_branch2b_param_0(0.48) 
I0704 07:54:18.871249 25348 net.cpp:1851] res4a_branch2a_param_0(0.48) 
I0704 07:54:18.871253 25348 net.cpp:1851] res4a_branch2b_param_0(0.48) 
I0704 07:54:18.871254 25348 net.cpp:1851] res5a_branch2a_param_0(0.48) 
I0704 07:54:18.871258 25348 net.cpp:1851] res5a_branch2b_param_0(0.48) 
I0704 07:54:18.871259 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.12971e+06/2.3599e+06) 0.479
I0704 07:54:18.871346 25348 solver.cpp:466] Iteration 29000, Testing net (#0)
I0704 07:54:20.514633 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8845
I0704 07:54:20.514652 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9952
I0704 07:54:20.514658 25348 solver.cpp:539]     Test net output #2: loss = 0.2534 (* 1 = 0.2534 loss)
I0704 07:54:20.534303 25348 solver.cpp:290] Iteration 29000 (26.8968 iter/s, 3.71792s/100 iter), loss = -2.5332e-07
I0704 07:54:20.534322 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:20.534332 25348 sgd_solver.cpp:106] Iteration 29000, lr = 0.00546875
I0704 07:54:20.534890 25348 solver.cpp:375] Finding and applying sparsity: 0.5
I0704 07:54:21.456465 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:54:23.544469 25348 solver.cpp:290] Iteration 29100 (33.2219 iter/s, 3.01006s/100 iter), loss = -2.5332e-07
I0704 07:54:23.544494 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:23.544502 25348 sgd_solver.cpp:106] Iteration 29100, lr = 0.00545313
I0704 07:54:25.613539 25348 solver.cpp:290] Iteration 29200 (48.333 iter/s, 2.06898s/100 iter), loss = -2.5332e-07
I0704 07:54:25.613565 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:25.613572 25348 sgd_solver.cpp:106] Iteration 29200, lr = 0.0054375
I0704 07:54:27.690219 25348 solver.cpp:290] Iteration 29300 (48.1558 iter/s, 2.07659s/100 iter), loss = -2.5332e-07
I0704 07:54:27.690243 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:27.690249 25348 sgd_solver.cpp:106] Iteration 29300, lr = 0.00542188
I0704 07:54:29.761780 25348 solver.cpp:290] Iteration 29400 (48.2748 iter/s, 2.07147s/100 iter), loss = -2.5332e-07
I0704 07:54:29.761803 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:29.761811 25348 sgd_solver.cpp:106] Iteration 29400, lr = 0.00540625
I0704 07:54:31.839049 25348 solver.cpp:290] Iteration 29500 (48.1422 iter/s, 2.07718s/100 iter), loss = -2.5332e-07
I0704 07:54:31.839087 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:31.839094 25348 sgd_solver.cpp:106] Iteration 29500, lr = 0.00539062
I0704 07:54:33.917202 25348 solver.cpp:290] Iteration 29600 (48.122 iter/s, 2.07805s/100 iter), loss = -2.5332e-07
I0704 07:54:33.917224 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:33.917232 25348 sgd_solver.cpp:106] Iteration 29600, lr = 0.005375
I0704 07:54:35.996539 25348 solver.cpp:290] Iteration 29700 (48.0942 iter/s, 2.07925s/100 iter), loss = -2.5332e-07
I0704 07:54:35.996562 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:35.996569 25348 sgd_solver.cpp:106] Iteration 29700, lr = 0.00535937
I0704 07:54:38.068172 25348 solver.cpp:290] Iteration 29800 (48.2732 iter/s, 2.07154s/100 iter), loss = -2.5332e-07
I0704 07:54:38.068198 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:38.068207 25348 sgd_solver.cpp:106] Iteration 29800, lr = 0.00534375
I0704 07:54:40.140393 25348 solver.cpp:290] Iteration 29900 (48.2594 iter/s, 2.07213s/100 iter), loss = -2.5332e-07
I0704 07:54:40.140415 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:40.140424 25348 sgd_solver.cpp:106] Iteration 29900, lr = 0.00532812
I0704 07:54:42.192168 25348 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_30000.caffemodel
I0704 07:54:42.208760 25348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_30000.solverstate
I0704 07:54:42.216162 25348 solver.cpp:354] Sparsity after update:
I0704 07:54:42.217093 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:54:42.217102 25348 net.cpp:1851] conv1a_param_0(0.25) 
I0704 07:54:42.217109 25348 net.cpp:1851] conv1b_param_0(0.5) 
I0704 07:54:42.217111 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:54:42.217113 25348 net.cpp:1851] res2a_branch2a_param_0(0.5) 
I0704 07:54:42.217115 25348 net.cpp:1851] res2a_branch2b_param_0(0.5) 
I0704 07:54:42.217118 25348 net.cpp:1851] res3a_branch2a_param_0(0.5) 
I0704 07:54:42.217119 25348 net.cpp:1851] res3a_branch2b_param_0(0.5) 
I0704 07:54:42.217121 25348 net.cpp:1851] res4a_branch2a_param_0(0.5) 
I0704 07:54:42.217123 25348 net.cpp:1851] res4a_branch2b_param_0(0.5) 
I0704 07:54:42.217125 25348 net.cpp:1851] res5a_branch2a_param_0(0.5) 
I0704 07:54:42.217128 25348 net.cpp:1851] res5a_branch2b_param_0(0.5) 
I0704 07:54:42.217129 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.17679e+06/2.3599e+06) 0.499
I0704 07:54:42.217233 25348 solver.cpp:466] Iteration 30000, Testing net (#0)
I0704 07:54:43.862226 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8809
I0704 07:54:43.862244 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9934
I0704 07:54:43.862251 25348 solver.cpp:539]     Test net output #2: loss = 0.255 (* 1 = 0.255 loss)
I0704 07:54:43.882848 25348 solver.cpp:290] Iteration 30000 (26.7213 iter/s, 3.74233s/100 iter), loss = -2.5332e-07
I0704 07:54:43.882864 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:43.882877 25348 sgd_solver.cpp:106] Iteration 30000, lr = 0.0053125
I0704 07:54:43.883414 25348 solver.cpp:375] Finding and applying sparsity: 0.52
I0704 07:54:44.846622 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:54:46.933162 25348 solver.cpp:290] Iteration 30100 (32.7847 iter/s, 3.05021s/100 iter), loss = -2.5332e-07
I0704 07:54:46.933185 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:46.933192 25348 sgd_solver.cpp:106] Iteration 30100, lr = 0.00529688
I0704 07:54:49.004132 25348 solver.cpp:290] Iteration 30200 (48.2885 iter/s, 2.07089s/100 iter), loss = -2.5332e-07
I0704 07:54:49.004154 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:49.004161 25348 sgd_solver.cpp:106] Iteration 30200, lr = 0.00528125
I0704 07:54:51.075896 25348 solver.cpp:290] Iteration 30300 (48.27 iter/s, 2.07168s/100 iter), loss = -2.5332e-07
I0704 07:54:51.075918 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:51.075925 25348 sgd_solver.cpp:106] Iteration 30300, lr = 0.00526563
I0704 07:54:53.146976 25348 solver.cpp:290] Iteration 30400 (48.286 iter/s, 2.07099s/100 iter), loss = -2.5332e-07
I0704 07:54:53.146999 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:53.147006 25348 sgd_solver.cpp:106] Iteration 30400, lr = 0.00525
I0704 07:54:55.222872 25348 solver.cpp:290] Iteration 30500 (48.174 iter/s, 2.07581s/100 iter), loss = -2.5332e-07
I0704 07:54:55.222894 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:55.222903 25348 sgd_solver.cpp:106] Iteration 30500, lr = 0.00523437
I0704 07:54:57.293830 25348 solver.cpp:290] Iteration 30600 (48.2889 iter/s, 2.07087s/100 iter), loss = -2.5332e-07
I0704 07:54:57.293853 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:57.293861 25348 sgd_solver.cpp:106] Iteration 30600, lr = 0.00521875
I0704 07:54:59.367960 25348 solver.cpp:290] Iteration 30700 (48.215 iter/s, 2.07404s/100 iter), loss = -2.5332e-07
I0704 07:54:59.367983 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:54:59.367990 25348 sgd_solver.cpp:106] Iteration 30700, lr = 0.00520312
I0704 07:55:01.441632 25348 solver.cpp:290] Iteration 30800 (48.2257 iter/s, 2.07358s/100 iter), loss = -2.5332e-07
I0704 07:55:01.441670 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:01.441678 25348 sgd_solver.cpp:106] Iteration 30800, lr = 0.0051875
I0704 07:55:03.519050 25348 solver.cpp:290] Iteration 30900 (48.139 iter/s, 2.07732s/100 iter), loss = -2.5332e-07
I0704 07:55:03.519076 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:03.519086 25348 sgd_solver.cpp:106] Iteration 30900, lr = 0.00517187
I0704 07:55:05.572597 25348 solver.cpp:354] Sparsity after update:
I0704 07:55:05.574000 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:55:05.574007 25348 net.cpp:1851] conv1a_param_0(0.26) 
I0704 07:55:05.574014 25348 net.cpp:1851] conv1b_param_0(0.52) 
I0704 07:55:05.574017 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:55:05.574018 25348 net.cpp:1851] res2a_branch2a_param_0(0.52) 
I0704 07:55:05.574020 25348 net.cpp:1851] res2a_branch2b_param_0(0.52) 
I0704 07:55:05.574023 25348 net.cpp:1851] res3a_branch2a_param_0(0.52) 
I0704 07:55:05.574024 25348 net.cpp:1851] res3a_branch2b_param_0(0.52) 
I0704 07:55:05.574026 25348 net.cpp:1851] res4a_branch2a_param_0(0.52) 
I0704 07:55:05.574028 25348 net.cpp:1851] res4a_branch2b_param_0(0.52) 
I0704 07:55:05.574030 25348 net.cpp:1851] res5a_branch2a_param_0(0.52) 
I0704 07:55:05.574033 25348 net.cpp:1851] res5a_branch2b_param_0(0.52) 
I0704 07:55:05.574034 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.22386e+06/2.3599e+06) 0.519
I0704 07:55:05.574123 25348 solver.cpp:466] Iteration 31000, Testing net (#0)
I0704 07:55:07.217237 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8992
I0704 07:55:07.217255 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9936
I0704 07:55:07.217262 25348 solver.cpp:539]     Test net output #2: loss = 0.2147 (* 1 = 0.2147 loss)
I0704 07:55:07.237318 25348 solver.cpp:290] Iteration 31000 (26.8952 iter/s, 3.71814s/100 iter), loss = -2.5332e-07
I0704 07:55:07.237336 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:07.237344 25348 sgd_solver.cpp:106] Iteration 31000, lr = 0.00515625
I0704 07:55:07.238111 25348 solver.cpp:375] Finding and applying sparsity: 0.54
I0704 07:55:08.208667 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:55:10.310719 25348 solver.cpp:290] Iteration 31100 (32.5384 iter/s, 3.07329s/100 iter), loss = -2.5332e-07
I0704 07:55:10.310740 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:10.310748 25348 sgd_solver.cpp:106] Iteration 31100, lr = 0.00514062
I0704 07:55:12.381945 25348 solver.cpp:290] Iteration 31200 (48.2826 iter/s, 2.07114s/100 iter), loss = -2.5332e-07
I0704 07:55:12.382025 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:12.382036 25348 sgd_solver.cpp:106] Iteration 31200, lr = 0.005125
I0704 07:55:14.452172 25348 solver.cpp:290] Iteration 31300 (48.3072 iter/s, 2.07009s/100 iter), loss = -2.5332e-07
I0704 07:55:14.452194 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:14.452201 25348 sgd_solver.cpp:106] Iteration 31300, lr = 0.00510937
I0704 07:55:16.529171 25348 solver.cpp:290] Iteration 31400 (48.1484 iter/s, 2.07691s/100 iter), loss = -2.5332e-07
I0704 07:55:16.529194 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:16.529201 25348 sgd_solver.cpp:106] Iteration 31400, lr = 0.00509375
I0704 07:55:18.606731 25348 solver.cpp:290] Iteration 31500 (48.1354 iter/s, 2.07747s/100 iter), loss = -2.5332e-07
I0704 07:55:18.606752 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:18.606758 25348 sgd_solver.cpp:106] Iteration 31500, lr = 0.00507812
I0704 07:55:20.677386 25348 solver.cpp:290] Iteration 31600 (48.2959 iter/s, 2.07057s/100 iter), loss = -2.5332e-07
I0704 07:55:20.677408 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:20.677417 25348 sgd_solver.cpp:106] Iteration 31600, lr = 0.0050625
I0704 07:55:22.750090 25348 solver.cpp:290] Iteration 31700 (48.2482 iter/s, 2.07262s/100 iter), loss = -2.5332e-07
I0704 07:55:22.750116 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:22.750125 25348 sgd_solver.cpp:106] Iteration 31700, lr = 0.00504687
I0704 07:55:24.821933 25348 solver.cpp:290] Iteration 31800 (48.2683 iter/s, 2.07175s/100 iter), loss = -2.5332e-07
I0704 07:55:24.821959 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:24.821966 25348 sgd_solver.cpp:106] Iteration 31800, lr = 0.00503125
I0704 07:55:26.893606 25348 solver.cpp:290] Iteration 31900 (48.2722 iter/s, 2.07158s/100 iter), loss = -2.5332e-07
I0704 07:55:26.893630 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:26.893636 25348 sgd_solver.cpp:106] Iteration 31900, lr = 0.00501562
I0704 07:55:28.950845 25348 solver.cpp:354] Sparsity after update:
I0704 07:55:28.952277 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:55:28.952283 25348 net.cpp:1851] conv1a_param_0(0.27) 
I0704 07:55:28.952291 25348 net.cpp:1851] conv1b_param_0(0.54) 
I0704 07:55:28.952293 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:55:28.952296 25348 net.cpp:1851] res2a_branch2a_param_0(0.54) 
I0704 07:55:28.952297 25348 net.cpp:1851] res2a_branch2b_param_0(0.54) 
I0704 07:55:28.952299 25348 net.cpp:1851] res3a_branch2a_param_0(0.54) 
I0704 07:55:28.952301 25348 net.cpp:1851] res3a_branch2b_param_0(0.54) 
I0704 07:55:28.952303 25348 net.cpp:1851] res4a_branch2a_param_0(0.54) 
I0704 07:55:28.952306 25348 net.cpp:1851] res4a_branch2b_param_0(0.54) 
I0704 07:55:28.952307 25348 net.cpp:1851] res5a_branch2a_param_0(0.54) 
I0704 07:55:28.952309 25348 net.cpp:1851] res5a_branch2b_param_0(0.54) 
I0704 07:55:28.952311 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.27093e+06/2.3599e+06) 0.539
I0704 07:55:28.952399 25348 solver.cpp:466] Iteration 32000, Testing net (#0)
I0704 07:55:30.600853 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8717
I0704 07:55:30.600874 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9928
I0704 07:55:30.600879 25348 solver.cpp:539]     Test net output #2: loss = 0.287 (* 1 = 0.287 loss)
I0704 07:55:30.621198 25348 solver.cpp:290] Iteration 32000 (26.8279 iter/s, 3.72746s/100 iter), loss = -2.5332e-07
I0704 07:55:30.621217 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:30.621228 25348 sgd_solver.cpp:106] Iteration 32000, lr = 0.005
I0704 07:55:30.621772 25348 solver.cpp:375] Finding and applying sparsity: 0.56
I0704 07:55:31.617177 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:55:33.726516 25348 solver.cpp:290] Iteration 32100 (32.2039 iter/s, 3.10521s/100 iter), loss = 0.0476188
I0704 07:55:33.726547 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:55:33.726553 25348 sgd_solver.cpp:106] Iteration 32100, lr = 0.00498438
I0704 07:55:35.801352 25348 solver.cpp:290] Iteration 32200 (48.1987 iter/s, 2.07474s/100 iter), loss = -2.5332e-07
I0704 07:55:35.801376 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:35.801385 25348 sgd_solver.cpp:106] Iteration 32200, lr = 0.00496875
I0704 07:55:37.881471 25348 solver.cpp:290] Iteration 32300 (48.0762 iter/s, 2.08003s/100 iter), loss = -2.5332e-07
I0704 07:55:37.881494 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:37.881501 25348 sgd_solver.cpp:106] Iteration 32300, lr = 0.00495313
I0704 07:55:39.957617 25348 solver.cpp:290] Iteration 32400 (48.1681 iter/s, 2.07606s/100 iter), loss = -2.5332e-07
I0704 07:55:39.957639 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:39.957646 25348 sgd_solver.cpp:106] Iteration 32400, lr = 0.0049375
I0704 07:55:42.034205 25348 solver.cpp:290] Iteration 32500 (48.1579 iter/s, 2.0765s/100 iter), loss = -2.5332e-07
I0704 07:55:42.034227 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:42.034235 25348 sgd_solver.cpp:106] Iteration 32500, lr = 0.00492187
I0704 07:55:44.106542 25348 solver.cpp:290] Iteration 32600 (48.2567 iter/s, 2.07225s/100 iter), loss = -2.5332e-07
I0704 07:55:44.106609 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:44.106619 25348 sgd_solver.cpp:106] Iteration 32600, lr = 0.00490625
I0704 07:55:46.184480 25348 solver.cpp:290] Iteration 32700 (48.1276 iter/s, 2.07781s/100 iter), loss = -2.5332e-07
I0704 07:55:46.184502 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:46.184509 25348 sgd_solver.cpp:106] Iteration 32700, lr = 0.00489062
I0704 07:55:48.257105 25348 solver.cpp:290] Iteration 32800 (48.25 iter/s, 2.07254s/100 iter), loss = -2.5332e-07
I0704 07:55:48.257128 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:48.257135 25348 sgd_solver.cpp:106] Iteration 32800, lr = 0.004875
I0704 07:55:50.331696 25348 solver.cpp:290] Iteration 32900 (48.2043 iter/s, 2.0745s/100 iter), loss = -2.5332e-07
I0704 07:55:50.331718 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:50.331724 25348 sgd_solver.cpp:106] Iteration 32900, lr = 0.00485937
I0704 07:55:52.385601 25348 solver.cpp:354] Sparsity after update:
I0704 07:55:52.386989 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:55:52.386997 25348 net.cpp:1851] conv1a_param_0(0.28) 
I0704 07:55:52.387006 25348 net.cpp:1851] conv1b_param_0(0.56) 
I0704 07:55:52.387007 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:55:52.387009 25348 net.cpp:1851] res2a_branch2a_param_0(0.56) 
I0704 07:55:52.387012 25348 net.cpp:1851] res2a_branch2b_param_0(0.56) 
I0704 07:55:52.387013 25348 net.cpp:1851] res3a_branch2a_param_0(0.56) 
I0704 07:55:52.387015 25348 net.cpp:1851] res3a_branch2b_param_0(0.56) 
I0704 07:55:52.387017 25348 net.cpp:1851] res4a_branch2a_param_0(0.56) 
I0704 07:55:52.387019 25348 net.cpp:1851] res4a_branch2b_param_0(0.56) 
I0704 07:55:52.387022 25348 net.cpp:1851] res5a_branch2a_param_0(0.56) 
I0704 07:55:52.387024 25348 net.cpp:1851] res5a_branch2b_param_0(0.56) 
I0704 07:55:52.387027 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.318e+06/2.3599e+06) 0.558
I0704 07:55:52.387114 25348 solver.cpp:466] Iteration 33000, Testing net (#0)
I0704 07:55:54.030304 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.7612
I0704 07:55:54.030323 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9878
I0704 07:55:54.030328 25348 solver.cpp:539]     Test net output #2: loss = 0.684 (* 1 = 0.684 loss)
I0704 07:55:54.050001 25348 solver.cpp:290] Iteration 33000 (26.8949 iter/s, 3.71818s/100 iter), loss = -2.5332e-07
I0704 07:55:54.050019 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:54.050031 25348 sgd_solver.cpp:106] Iteration 33000, lr = 0.00484375
I0704 07:55:54.050592 25348 solver.cpp:375] Finding and applying sparsity: 0.58
I0704 07:55:54.944296 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:55:57.053640 25348 solver.cpp:290] Iteration 33100 (33.2941 iter/s, 3.00353s/100 iter), loss = -2.5332e-07
I0704 07:55:57.053661 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:57.053668 25348 sgd_solver.cpp:106] Iteration 33100, lr = 0.00482813
I0704 07:55:59.125533 25348 solver.cpp:290] Iteration 33200 (48.267 iter/s, 2.07181s/100 iter), loss = -2.5332e-07
I0704 07:55:59.125556 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:55:59.125564 25348 sgd_solver.cpp:106] Iteration 33200, lr = 0.0048125
I0704 07:56:01.207129 25348 solver.cpp:290] Iteration 33300 (48.042 iter/s, 2.08151s/100 iter), loss = -2.5332e-07
I0704 07:56:01.207152 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:01.207159 25348 sgd_solver.cpp:106] Iteration 33300, lr = 0.00479688
I0704 07:56:03.279147 25348 solver.cpp:290] Iteration 33400 (48.2642 iter/s, 2.07193s/100 iter), loss = -2.5332e-07
I0704 07:56:03.279170 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:03.279176 25348 sgd_solver.cpp:106] Iteration 33400, lr = 0.00478125
I0704 07:56:05.355294 25348 solver.cpp:290] Iteration 33500 (48.1681 iter/s, 2.07606s/100 iter), loss = 0.0476188
I0704 07:56:05.355332 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:56:05.355340 25348 sgd_solver.cpp:106] Iteration 33500, lr = 0.00476563
I0704 07:56:07.425720 25348 solver.cpp:290] Iteration 33600 (48.3015 iter/s, 2.07033s/100 iter), loss = 0.0476188
I0704 07:56:07.425743 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:56:07.425750 25348 sgd_solver.cpp:106] Iteration 33600, lr = 0.00475
I0704 07:56:09.498747 25348 solver.cpp:290] Iteration 33700 (48.2407 iter/s, 2.07294s/100 iter), loss = -2.5332e-07
I0704 07:56:09.498770 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:09.498777 25348 sgd_solver.cpp:106] Iteration 33700, lr = 0.00473437
I0704 07:56:11.583765 25348 solver.cpp:290] Iteration 33800 (47.9633 iter/s, 2.08493s/100 iter), loss = -2.6077e-07
I0704 07:56:11.583791 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:11.583797 25348 sgd_solver.cpp:106] Iteration 33800, lr = 0.00471875
I0704 07:56:13.676417 25348 solver.cpp:290] Iteration 33900 (47.7883 iter/s, 2.09256s/100 iter), loss = -2.5332e-07
I0704 07:56:13.676445 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:13.676455 25348 sgd_solver.cpp:106] Iteration 33900, lr = 0.00470312
I0704 07:56:15.752421 25348 solver.cpp:354] Sparsity after update:
I0704 07:56:15.753824 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:56:15.753834 25348 net.cpp:1851] conv1a_param_0(0.29) 
I0704 07:56:15.753844 25348 net.cpp:1851] conv1b_param_0(0.58) 
I0704 07:56:15.753849 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:56:15.753854 25348 net.cpp:1851] res2a_branch2a_param_0(0.58) 
I0704 07:56:15.753857 25348 net.cpp:1851] res2a_branch2b_param_0(0.58) 
I0704 07:56:15.753861 25348 net.cpp:1851] res3a_branch2a_param_0(0.58) 
I0704 07:56:15.753865 25348 net.cpp:1851] res3a_branch2b_param_0(0.58) 
I0704 07:56:15.753868 25348 net.cpp:1851] res4a_branch2a_param_0(0.58) 
I0704 07:56:15.753872 25348 net.cpp:1851] res4a_branch2b_param_0(0.58) 
I0704 07:56:15.753876 25348 net.cpp:1851] res5a_branch2a_param_0(0.58) 
I0704 07:56:15.753880 25348 net.cpp:1851] res5a_branch2b_param_0(0.58) 
I0704 07:56:15.753885 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.36507e+06/2.3599e+06) 0.578
I0704 07:56:15.753976 25348 solver.cpp:466] Iteration 34000, Testing net (#0)
I0704 07:56:17.406510 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8524
I0704 07:56:17.406530 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9881
I0704 07:56:17.406535 25348 solver.cpp:539]     Test net output #2: loss = 0.3518 (* 1 = 0.3518 loss)
I0704 07:56:17.426501 25348 solver.cpp:290] Iteration 34000 (26.667 iter/s, 3.74995s/100 iter), loss = -2.5332e-07
I0704 07:56:17.426518 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:17.426529 25348 sgd_solver.cpp:106] Iteration 34000, lr = 0.0046875
I0704 07:56:17.427050 25348 solver.cpp:375] Finding and applying sparsity: 0.6
I0704 07:56:18.359688 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:56:20.459000 25348 solver.cpp:290] Iteration 34100 (32.9773 iter/s, 3.03239s/100 iter), loss = -2.5332e-07
I0704 07:56:20.459022 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:20.459028 25348 sgd_solver.cpp:106] Iteration 34100, lr = 0.00467187
I0704 07:56:22.534564 25348 solver.cpp:290] Iteration 34200 (48.1816 iter/s, 2.07548s/100 iter), loss = -2.5332e-07
I0704 07:56:22.534587 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:22.534595 25348 sgd_solver.cpp:106] Iteration 34200, lr = 0.00465625
I0704 07:56:24.605175 25348 solver.cpp:290] Iteration 34300 (48.297 iter/s, 2.07052s/100 iter), loss = 0.0476188
I0704 07:56:24.605199 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:56:24.605208 25348 sgd_solver.cpp:106] Iteration 34300, lr = 0.00464062
I0704 07:56:26.685298 25348 solver.cpp:290] Iteration 34400 (48.0761 iter/s, 2.08004s/100 iter), loss = 0.0476188
I0704 07:56:26.685322 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:56:26.685330 25348 sgd_solver.cpp:106] Iteration 34400, lr = 0.004625
I0704 07:56:28.758674 25348 solver.cpp:290] Iteration 34500 (48.2325 iter/s, 2.07329s/100 iter), loss = -2.5332e-07
I0704 07:56:28.758695 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:28.758703 25348 sgd_solver.cpp:106] Iteration 34500, lr = 0.00460937
I0704 07:56:30.835386 25348 solver.cpp:290] Iteration 34600 (48.155 iter/s, 2.07663s/100 iter), loss = -2.5332e-07
I0704 07:56:30.835407 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:30.835413 25348 sgd_solver.cpp:106] Iteration 34600, lr = 0.00459375
I0704 07:56:32.911777 25348 solver.cpp:290] Iteration 34700 (48.1624 iter/s, 2.07631s/100 iter), loss = -2.5332e-07
I0704 07:56:32.911798 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:32.911805 25348 sgd_solver.cpp:106] Iteration 34700, lr = 0.00457812
I0704 07:56:35.024590 25348 solver.cpp:290] Iteration 34800 (47.3322 iter/s, 2.11273s/100 iter), loss = -2.5332e-07
I0704 07:56:35.024611 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:35.024618 25348 sgd_solver.cpp:106] Iteration 34800, lr = 0.0045625
I0704 07:56:37.099426 25348 solver.cpp:290] Iteration 34900 (48.1985 iter/s, 2.07475s/100 iter), loss = -2.5332e-07
I0704 07:56:37.099448 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:37.099454 25348 sgd_solver.cpp:106] Iteration 34900, lr = 0.00454687
I0704 07:56:39.151453 25348 solver.cpp:354] Sparsity after update:
I0704 07:56:39.152845 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:56:39.152853 25348 net.cpp:1851] conv1a_param_0(0.3) 
I0704 07:56:39.152860 25348 net.cpp:1851] conv1b_param_0(0.6) 
I0704 07:56:39.152864 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:56:39.152868 25348 net.cpp:1851] res2a_branch2a_param_0(0.6) 
I0704 07:56:39.152871 25348 net.cpp:1851] res2a_branch2b_param_0(0.6) 
I0704 07:56:39.152876 25348 net.cpp:1851] res3a_branch2a_param_0(0.6) 
I0704 07:56:39.152879 25348 net.cpp:1851] res3a_branch2b_param_0(0.6) 
I0704 07:56:39.152884 25348 net.cpp:1851] res4a_branch2a_param_0(0.6) 
I0704 07:56:39.152887 25348 net.cpp:1851] res4a_branch2b_param_0(0.6) 
I0704 07:56:39.152891 25348 net.cpp:1851] res5a_branch2a_param_0(0.6) 
I0704 07:56:39.152894 25348 net.cpp:1851] res5a_branch2b_param_0(0.6) 
I0704 07:56:39.152899 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.41214e+06/2.3599e+06) 0.598
I0704 07:56:39.153034 25348 solver.cpp:466] Iteration 35000, Testing net (#0)
I0704 07:56:40.793711 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8773
I0704 07:56:40.793730 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9926
I0704 07:56:40.793735 25348 solver.cpp:539]     Test net output #2: loss = 0.2769 (* 1 = 0.2769 loss)
I0704 07:56:40.814010 25348 solver.cpp:290] Iteration 35000 (26.9219 iter/s, 3.71445s/100 iter), loss = -2.5332e-07
I0704 07:56:40.814038 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:40.814043 25348 sgd_solver.cpp:106] Iteration 35000, lr = 0.00453125
I0704 07:56:40.814651 25348 solver.cpp:375] Finding and applying sparsity: 0.62
I0704 07:56:41.819806 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:56:43.932368 25348 solver.cpp:290] Iteration 35100 (32.0693 iter/s, 3.11824s/100 iter), loss = -2.5332e-07
I0704 07:56:43.932389 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:43.932397 25348 sgd_solver.cpp:106] Iteration 35100, lr = 0.00451563
I0704 07:56:46.010854 25348 solver.cpp:290] Iteration 35200 (48.1139 iter/s, 2.0784s/100 iter), loss = -2.5332e-07
I0704 07:56:46.010912 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:46.010921 25348 sgd_solver.cpp:106] Iteration 35200, lr = 0.0045
I0704 07:56:48.085492 25348 solver.cpp:290] Iteration 35300 (48.204 iter/s, 2.07452s/100 iter), loss = -2.5332e-07
I0704 07:56:48.085517 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:48.085526 25348 sgd_solver.cpp:106] Iteration 35300, lr = 0.00448438
I0704 07:56:50.170425 25348 solver.cpp:290] Iteration 35400 (47.9653 iter/s, 2.08484s/100 iter), loss = -2.5332e-07
I0704 07:56:50.170449 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:50.170455 25348 sgd_solver.cpp:106] Iteration 35400, lr = 0.00446875
I0704 07:56:52.250138 25348 solver.cpp:290] Iteration 35500 (48.0856 iter/s, 2.07963s/100 iter), loss = -2.5332e-07
I0704 07:56:52.250159 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:52.250165 25348 sgd_solver.cpp:106] Iteration 35500, lr = 0.00445312
I0704 07:56:54.322988 25348 solver.cpp:290] Iteration 35600 (48.2447 iter/s, 2.07277s/100 iter), loss = -2.5332e-07
I0704 07:56:54.323011 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:54.323017 25348 sgd_solver.cpp:106] Iteration 35600, lr = 0.0044375
I0704 07:56:56.408324 25348 solver.cpp:290] Iteration 35700 (47.9559 iter/s, 2.08525s/100 iter), loss = -2.5332e-07
I0704 07:56:56.408346 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:56.408352 25348 sgd_solver.cpp:106] Iteration 35700, lr = 0.00442187
I0704 07:56:58.485533 25348 solver.cpp:290] Iteration 35800 (48.1435 iter/s, 2.07712s/100 iter), loss = -2.5332e-07
I0704 07:56:58.485558 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:56:58.485564 25348 sgd_solver.cpp:106] Iteration 35800, lr = 0.00440625
I0704 07:57:00.557409 25348 solver.cpp:290] Iteration 35900 (48.2675 iter/s, 2.07179s/100 iter), loss = -2.5332e-07
I0704 07:57:00.557431 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:00.557437 25348 sgd_solver.cpp:106] Iteration 35900, lr = 0.00439062
I0704 07:57:02.615828 25348 solver.cpp:354] Sparsity after update:
I0704 07:57:02.617236 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:57:02.617244 25348 net.cpp:1851] conv1a_param_0(0.31) 
I0704 07:57:02.617251 25348 net.cpp:1851] conv1b_param_0(0.62) 
I0704 07:57:02.617254 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:57:02.617259 25348 net.cpp:1851] res2a_branch2a_param_0(0.62) 
I0704 07:57:02.617264 25348 net.cpp:1851] res2a_branch2b_param_0(0.62) 
I0704 07:57:02.617267 25348 net.cpp:1851] res3a_branch2a_param_0(0.62) 
I0704 07:57:02.617271 25348 net.cpp:1851] res3a_branch2b_param_0(0.62) 
I0704 07:57:02.617275 25348 net.cpp:1851] res4a_branch2a_param_0(0.62) 
I0704 07:57:02.617278 25348 net.cpp:1851] res4a_branch2b_param_0(0.62) 
I0704 07:57:02.617282 25348 net.cpp:1851] res5a_branch2a_param_0(0.62) 
I0704 07:57:02.617286 25348 net.cpp:1851] res5a_branch2b_param_0(0.62) 
I0704 07:57:02.617290 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.45921e+06/2.3599e+06) 0.618
I0704 07:57:02.617427 25348 solver.cpp:466] Iteration 36000, Testing net (#0)
I0704 07:57:04.260339 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8987
I0704 07:57:04.260359 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9954
I0704 07:57:04.260363 25348 solver.cpp:539]     Test net output #2: loss = 0.2169 (* 1 = 0.2169 loss)
I0704 07:57:04.280020 25348 solver.cpp:290] Iteration 36000 (26.8638 iter/s, 3.72248s/100 iter), loss = -2.5332e-07
I0704 07:57:04.280038 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:04.280051 25348 sgd_solver.cpp:106] Iteration 36000, lr = 0.004375
I0704 07:57:04.280585 25348 solver.cpp:375] Finding and applying sparsity: 0.64
I0704 07:57:05.510733 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:57:07.616991 25348 solver.cpp:290] Iteration 36100 (29.9683 iter/s, 3.33685s/100 iter), loss = -2.5332e-07
I0704 07:57:07.617025 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:07.617034 25348 sgd_solver.cpp:106] Iteration 36100, lr = 0.00435938
I0704 07:57:09.695698 25348 solver.cpp:290] Iteration 36200 (48.1091 iter/s, 2.07861s/100 iter), loss = -2.5332e-07
I0704 07:57:09.695727 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:09.695734 25348 sgd_solver.cpp:106] Iteration 36200, lr = 0.00434375
I0704 07:57:11.771351 25348 solver.cpp:290] Iteration 36300 (48.1798 iter/s, 2.07556s/100 iter), loss = -2.5332e-07
I0704 07:57:11.771374 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:11.771381 25348 sgd_solver.cpp:106] Iteration 36300, lr = 0.00432813
I0704 07:57:13.846949 25348 solver.cpp:290] Iteration 36400 (48.181 iter/s, 2.07551s/100 iter), loss = -2.5332e-07
I0704 07:57:13.846972 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:13.846979 25348 sgd_solver.cpp:106] Iteration 36400, lr = 0.0043125
I0704 07:57:15.917484 25348 solver.cpp:290] Iteration 36500 (48.2987 iter/s, 2.07045s/100 iter), loss = -2.5332e-07
I0704 07:57:15.917506 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:15.917515 25348 sgd_solver.cpp:106] Iteration 36500, lr = 0.00429688
I0704 07:57:17.988018 25348 solver.cpp:290] Iteration 36600 (48.2987 iter/s, 2.07045s/100 iter), loss = -2.5332e-07
I0704 07:57:17.988073 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:17.988081 25348 sgd_solver.cpp:106] Iteration 36600, lr = 0.00428125
I0704 07:57:20.060633 25348 solver.cpp:290] Iteration 36700 (48.251 iter/s, 2.0725s/100 iter), loss = -2.5332e-07
I0704 07:57:20.060655 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:20.060662 25348 sgd_solver.cpp:106] Iteration 36700, lr = 0.00426562
I0704 07:57:22.131702 25348 solver.cpp:290] Iteration 36800 (48.2862 iter/s, 2.07098s/100 iter), loss = -2.5332e-07
I0704 07:57:22.131724 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:22.131731 25348 sgd_solver.cpp:106] Iteration 36800, lr = 0.00425
I0704 07:57:24.203840 25348 solver.cpp:290] Iteration 36900 (48.2614 iter/s, 2.07205s/100 iter), loss = -2.5332e-07
I0704 07:57:24.203861 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:24.203867 25348 sgd_solver.cpp:106] Iteration 36900, lr = 0.00423437
I0704 07:57:26.255271 25348 solver.cpp:354] Sparsity after update:
I0704 07:57:26.256654 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:57:26.256661 25348 net.cpp:1851] conv1a_param_0(0.32) 
I0704 07:57:26.256669 25348 net.cpp:1851] conv1b_param_0(0.64) 
I0704 07:57:26.256671 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:57:26.256674 25348 net.cpp:1851] res2a_branch2a_param_0(0.64) 
I0704 07:57:26.256675 25348 net.cpp:1851] res2a_branch2b_param_0(0.64) 
I0704 07:57:26.256678 25348 net.cpp:1851] res3a_branch2a_param_0(0.64) 
I0704 07:57:26.256680 25348 net.cpp:1851] res3a_branch2b_param_0(0.64) 
I0704 07:57:26.256682 25348 net.cpp:1851] res4a_branch2a_param_0(0.64) 
I0704 07:57:26.256685 25348 net.cpp:1851] res4a_branch2b_param_0(0.64) 
I0704 07:57:26.256687 25348 net.cpp:1851] res5a_branch2a_param_0(0.64) 
I0704 07:57:26.256690 25348 net.cpp:1851] res5a_branch2b_param_0(0.64) 
I0704 07:57:26.256691 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.50629e+06/2.3599e+06) 0.638
I0704 07:57:26.256779 25348 solver.cpp:466] Iteration 37000, Testing net (#0)
I0704 07:57:27.900789 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8989
I0704 07:57:27.900806 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9954
I0704 07:57:27.900811 25348 solver.cpp:539]     Test net output #2: loss = 0.2078 (* 1 = 0.2078 loss)
I0704 07:57:27.920603 25348 solver.cpp:290] Iteration 37000 (26.9061 iter/s, 3.71663s/100 iter), loss = -2.5332e-07
I0704 07:57:27.920626 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:27.920632 25348 sgd_solver.cpp:106] Iteration 37000, lr = 0.00421875
I0704 07:57:27.921172 25348 solver.cpp:375] Finding and applying sparsity: 0.66
I0704 07:57:29.259896 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:57:31.373196 25348 solver.cpp:290] Iteration 37100 (28.9648 iter/s, 3.45247s/100 iter), loss = -2.5332e-07
I0704 07:57:31.373217 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:31.373225 25348 sgd_solver.cpp:106] Iteration 37100, lr = 0.00420313
I0704 07:57:33.463826 25348 solver.cpp:290] Iteration 37200 (47.8344 iter/s, 2.09055s/100 iter), loss = -2.5332e-07
I0704 07:57:33.463848 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:33.463855 25348 sgd_solver.cpp:106] Iteration 37200, lr = 0.0041875
I0704 07:57:35.534981 25348 solver.cpp:290] Iteration 37300 (48.2843 iter/s, 2.07107s/100 iter), loss = -2.5332e-07
I0704 07:57:35.535003 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:35.535010 25348 sgd_solver.cpp:106] Iteration 37300, lr = 0.00417187
I0704 07:57:37.612264 25348 solver.cpp:290] Iteration 37400 (48.1418 iter/s, 2.0772s/100 iter), loss = -2.5332e-07
I0704 07:57:37.612287 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:37.612293 25348 sgd_solver.cpp:106] Iteration 37400, lr = 0.00415625
I0704 07:57:39.694124 25348 solver.cpp:290] Iteration 37500 (48.0359 iter/s, 2.08178s/100 iter), loss = -2.5332e-07
I0704 07:57:39.694164 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:39.694170 25348 sgd_solver.cpp:106] Iteration 37500, lr = 0.00414062
I0704 07:57:41.767738 25348 solver.cpp:290] Iteration 37600 (48.2273 iter/s, 2.07351s/100 iter), loss = -2.5332e-07
I0704 07:57:41.767761 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:41.767767 25348 sgd_solver.cpp:106] Iteration 37600, lr = 0.004125
I0704 07:57:43.841697 25348 solver.cpp:290] Iteration 37700 (48.219 iter/s, 2.07387s/100 iter), loss = -2.5332e-07
I0704 07:57:43.841719 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:43.841727 25348 sgd_solver.cpp:106] Iteration 37700, lr = 0.00410937
I0704 07:57:45.911545 25348 solver.cpp:290] Iteration 37800 (48.3147 iter/s, 2.06976s/100 iter), loss = -2.5332e-07
I0704 07:57:45.911568 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:45.911577 25348 sgd_solver.cpp:106] Iteration 37800, lr = 0.00409375
I0704 07:57:47.981704 25348 solver.cpp:290] Iteration 37900 (48.3074 iter/s, 2.07007s/100 iter), loss = -2.5332e-07
I0704 07:57:47.981727 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:47.981734 25348 sgd_solver.cpp:106] Iteration 37900, lr = 0.00407812
I0704 07:57:50.035944 25348 solver.cpp:354] Sparsity after update:
I0704 07:57:50.037374 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:57:50.037382 25348 net.cpp:1851] conv1a_param_0(0.33) 
I0704 07:57:50.037389 25348 net.cpp:1851] conv1b_param_0(0.66) 
I0704 07:57:50.037391 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:57:50.037395 25348 net.cpp:1851] res2a_branch2a_param_0(0.66) 
I0704 07:57:50.037396 25348 net.cpp:1851] res2a_branch2b_param_0(0.66) 
I0704 07:57:50.037398 25348 net.cpp:1851] res3a_branch2a_param_0(0.66) 
I0704 07:57:50.037400 25348 net.cpp:1851] res3a_branch2b_param_0(0.66) 
I0704 07:57:50.037401 25348 net.cpp:1851] res4a_branch2a_param_0(0.66) 
I0704 07:57:50.037403 25348 net.cpp:1851] res4a_branch2b_param_0(0.66) 
I0704 07:57:50.037405 25348 net.cpp:1851] res5a_branch2a_param_0(0.66) 
I0704 07:57:50.037407 25348 net.cpp:1851] res5a_branch2b_param_0(0.66) 
I0704 07:57:50.037410 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.55336e+06/2.3599e+06) 0.658
I0704 07:57:50.037505 25348 solver.cpp:466] Iteration 38000, Testing net (#0)
I0704 07:57:51.680637 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9005
I0704 07:57:51.680655 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9962
I0704 07:57:51.680660 25348 solver.cpp:539]     Test net output #2: loss = 0.2165 (* 1 = 0.2165 loss)
I0704 07:57:51.700919 25348 solver.cpp:290] Iteration 38000 (26.8883 iter/s, 3.71908s/100 iter), loss = -2.5332e-07
I0704 07:57:51.700937 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:51.700951 25348 sgd_solver.cpp:106] Iteration 38000, lr = 0.0040625
I0704 07:57:51.701506 25348 solver.cpp:375] Finding and applying sparsity: 0.68
I0704 07:57:53.198724 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:57:55.321061 25348 solver.cpp:290] Iteration 38100 (27.6242 iter/s, 3.62002s/100 iter), loss = -2.5332e-07
I0704 07:57:55.321087 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:55.321095 25348 sgd_solver.cpp:106] Iteration 38100, lr = 0.00404688
I0704 07:57:57.397101 25348 solver.cpp:290] Iteration 38200 (48.1707 iter/s, 2.07595s/100 iter), loss = -2.5332e-07
I0704 07:57:57.397122 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:57.397130 25348 sgd_solver.cpp:106] Iteration 38200, lr = 0.00403125
I0704 07:57:59.477758 25348 solver.cpp:290] Iteration 38300 (48.0637 iter/s, 2.08057s/100 iter), loss = -2.5332e-07
I0704 07:57:59.477779 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:57:59.477787 25348 sgd_solver.cpp:106] Iteration 38300, lr = 0.00401562
I0704 07:58:01.549474 25348 solver.cpp:290] Iteration 38400 (48.2712 iter/s, 2.07163s/100 iter), loss = -2.5332e-07
I0704 07:58:01.549497 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:01.549505 25348 sgd_solver.cpp:106] Iteration 38400, lr = 0.004
I0704 07:58:03.632295 25348 solver.cpp:290] Iteration 38500 (48.0139 iter/s, 2.08273s/100 iter), loss = -2.5332e-07
I0704 07:58:03.632324 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:03.632334 25348 sgd_solver.cpp:106] Iteration 38500, lr = 0.00398437
I0704 07:58:05.707015 25348 solver.cpp:290] Iteration 38600 (48.2015 iter/s, 2.07463s/100 iter), loss = -2.5332e-07
I0704 07:58:05.707041 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:05.707051 25348 sgd_solver.cpp:106] Iteration 38600, lr = 0.00396875
I0704 07:58:07.780300 25348 solver.cpp:290] Iteration 38700 (48.2347 iter/s, 2.0732s/100 iter), loss = -2.5332e-07
I0704 07:58:07.780323 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:07.780329 25348 sgd_solver.cpp:106] Iteration 38700, lr = 0.00395312
I0704 07:58:09.852792 25348 solver.cpp:290] Iteration 38800 (48.2531 iter/s, 2.07241s/100 iter), loss = -2.5332e-07
I0704 07:58:09.852816 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:09.852823 25348 sgd_solver.cpp:106] Iteration 38800, lr = 0.0039375
I0704 07:58:11.921771 25348 solver.cpp:290] Iteration 38900 (48.3351 iter/s, 2.06889s/100 iter), loss = -2.5332e-07
I0704 07:58:11.921793 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:11.921800 25348 sgd_solver.cpp:106] Iteration 38900, lr = 0.00392187
I0704 07:58:13.972512 25348 solver.cpp:354] Sparsity after update:
I0704 07:58:13.973904 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:58:13.973912 25348 net.cpp:1851] conv1a_param_0(0.34) 
I0704 07:58:13.973920 25348 net.cpp:1851] conv1b_param_0(0.68) 
I0704 07:58:13.973922 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:58:13.973925 25348 net.cpp:1851] res2a_branch2a_param_0(0.68) 
I0704 07:58:13.973927 25348 net.cpp:1851] res2a_branch2b_param_0(0.68) 
I0704 07:58:13.973929 25348 net.cpp:1851] res3a_branch2a_param_0(0.68) 
I0704 07:58:13.973932 25348 net.cpp:1851] res3a_branch2b_param_0(0.68) 
I0704 07:58:13.973933 25348 net.cpp:1851] res4a_branch2a_param_0(0.68) 
I0704 07:58:13.973937 25348 net.cpp:1851] res4a_branch2b_param_0(0.68) 
I0704 07:58:13.973938 25348 net.cpp:1851] res5a_branch2a_param_0(0.68) 
I0704 07:58:13.973940 25348 net.cpp:1851] res5a_branch2b_param_0(0.68) 
I0704 07:58:13.973942 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.60043e+06/2.3599e+06) 0.678
I0704 07:58:13.974030 25348 solver.cpp:466] Iteration 39000, Testing net (#0)
I0704 07:58:15.616323 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9034
I0704 07:58:15.616340 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9963
I0704 07:58:15.616345 25348 solver.cpp:539]     Test net output #2: loss = 0.1934 (* 1 = 0.1934 loss)
I0704 07:58:15.636051 25348 solver.cpp:290] Iteration 39000 (26.9241 iter/s, 3.71415s/100 iter), loss = -2.5332e-07
I0704 07:58:15.636067 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:15.636080 25348 sgd_solver.cpp:106] Iteration 39000, lr = 0.00390625
I0704 07:58:15.636621 25348 solver.cpp:375] Finding and applying sparsity: 0.7
I0704 07:58:17.203591 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:58:19.311373 25348 solver.cpp:290] Iteration 39100 (27.2094 iter/s, 3.6752s/100 iter), loss = -2.5332e-07
I0704 07:58:19.311398 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:19.311406 25348 sgd_solver.cpp:106] Iteration 39100, lr = 0.00389063
I0704 07:58:21.381268 25348 solver.cpp:290] Iteration 39200 (48.3137 iter/s, 2.06981s/100 iter), loss = -2.5332e-07
I0704 07:58:21.381322 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:21.381330 25348 sgd_solver.cpp:106] Iteration 39200, lr = 0.003875
I0704 07:58:23.460855 25348 solver.cpp:290] Iteration 39300 (48.0892 iter/s, 2.07947s/100 iter), loss = -2.5332e-07
I0704 07:58:23.460888 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:23.460897 25348 sgd_solver.cpp:106] Iteration 39300, lr = 0.00385938
I0704 07:58:25.572104 25348 solver.cpp:290] Iteration 39400 (47.3675 iter/s, 2.11115s/100 iter), loss = -2.5332e-07
I0704 07:58:25.572134 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:25.572139 25348 sgd_solver.cpp:106] Iteration 39400, lr = 0.00384375
I0704 07:58:27.670536 25348 solver.cpp:290] Iteration 39500 (47.6567 iter/s, 2.09834s/100 iter), loss = -2.5332e-07
I0704 07:58:27.670559 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:27.670567 25348 sgd_solver.cpp:106] Iteration 39500, lr = 0.00382812
I0704 07:58:29.744562 25348 solver.cpp:290] Iteration 39600 (48.2174 iter/s, 2.07394s/100 iter), loss = -2.5332e-07
I0704 07:58:29.744586 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:29.744596 25348 sgd_solver.cpp:106] Iteration 39600, lr = 0.0038125
I0704 07:58:31.821180 25348 solver.cpp:290] Iteration 39700 (48.1572 iter/s, 2.07653s/100 iter), loss = -2.5332e-07
I0704 07:58:31.821202 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:31.821209 25348 sgd_solver.cpp:106] Iteration 39700, lr = 0.00379687
I0704 07:58:33.905793 25348 solver.cpp:290] Iteration 39800 (47.9725 iter/s, 2.08453s/100 iter), loss = -2.5332e-07
I0704 07:58:33.905815 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:33.905822 25348 sgd_solver.cpp:106] Iteration 39800, lr = 0.00378125
I0704 07:58:35.977787 25348 solver.cpp:290] Iteration 39900 (48.2647 iter/s, 2.07191s/100 iter), loss = -2.5332e-07
I0704 07:58:35.977808 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:35.977814 25348 sgd_solver.cpp:106] Iteration 39900, lr = 0.00376562
I0704 07:58:38.035850 25348 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_40000.caffemodel
I0704 07:58:38.053175 25348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_40000.solverstate
I0704 07:58:38.060578 25348 solver.cpp:354] Sparsity after update:
I0704 07:58:38.061509 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:58:38.061518 25348 net.cpp:1851] conv1a_param_0(0.35) 
I0704 07:58:38.061527 25348 net.cpp:1851] conv1b_param_0(0.7) 
I0704 07:58:38.061529 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:58:38.061532 25348 net.cpp:1851] res2a_branch2a_param_0(0.7) 
I0704 07:58:38.061534 25348 net.cpp:1851] res2a_branch2b_param_0(0.7) 
I0704 07:58:38.061537 25348 net.cpp:1851] res3a_branch2a_param_0(0.7) 
I0704 07:58:38.061540 25348 net.cpp:1851] res3a_branch2b_param_0(0.7) 
I0704 07:58:38.061542 25348 net.cpp:1851] res4a_branch2a_param_0(0.7) 
I0704 07:58:38.061544 25348 net.cpp:1851] res4a_branch2b_param_0(0.7) 
I0704 07:58:38.061547 25348 net.cpp:1851] res5a_branch2a_param_0(0.7) 
I0704 07:58:38.061549 25348 net.cpp:1851] res5a_branch2b_param_0(0.7) 
I0704 07:58:38.061553 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.6475e+06/2.3599e+06) 0.698
I0704 07:58:38.061655 25348 solver.cpp:466] Iteration 40000, Testing net (#0)
I0704 07:58:39.701607 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8875
I0704 07:58:39.701627 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.995
I0704 07:58:39.701632 25348 solver.cpp:539]     Test net output #2: loss = 0.2593 (* 1 = 0.2593 loss)
I0704 07:58:39.721287 25348 solver.cpp:290] Iteration 40000 (26.7139 iter/s, 3.74337s/100 iter), loss = -2.5332e-07
I0704 07:58:39.721307 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:39.721325 25348 sgd_solver.cpp:106] Iteration 40000, lr = 0.00375
I0704 07:58:39.721863 25348 solver.cpp:375] Finding and applying sparsity: 0.72
I0704 07:58:41.394453 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:58:43.517740 25348 solver.cpp:290] Iteration 40100 (26.3413 iter/s, 3.79632s/100 iter), loss = -2.5332e-07
I0704 07:58:43.517763 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:43.517771 25348 sgd_solver.cpp:106] Iteration 40100, lr = 0.00373438
I0704 07:58:45.595809 25348 solver.cpp:290] Iteration 40200 (48.1236 iter/s, 2.07798s/100 iter), loss = -2.5332e-07
I0704 07:58:45.595831 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:45.595839 25348 sgd_solver.cpp:106] Iteration 40200, lr = 0.00371875
I0704 07:58:47.667376 25348 solver.cpp:290] Iteration 40300 (48.2746 iter/s, 2.07148s/100 iter), loss = -2.5332e-07
I0704 07:58:47.667402 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:47.667409 25348 sgd_solver.cpp:106] Iteration 40300, lr = 0.00370313
I0704 07:58:49.747005 25348 solver.cpp:290] Iteration 40400 (48.0876 iter/s, 2.07954s/100 iter), loss = -2.5332e-07
I0704 07:58:49.747028 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:49.747035 25348 sgd_solver.cpp:106] Iteration 40400, lr = 0.0036875
I0704 07:58:51.834978 25348 solver.cpp:290] Iteration 40500 (47.8954 iter/s, 2.08788s/100 iter), loss = 0.0476188
I0704 07:58:51.835095 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:58:51.835104 25348 sgd_solver.cpp:106] Iteration 40500, lr = 0.00367187
I0704 07:58:53.912271 25348 solver.cpp:290] Iteration 40600 (48.1437 iter/s, 2.07712s/100 iter), loss = -2.5332e-07
I0704 07:58:53.912295 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:53.912304 25348 sgd_solver.cpp:106] Iteration 40600, lr = 0.00365625
I0704 07:58:55.983777 25348 solver.cpp:290] Iteration 40700 (48.2762 iter/s, 2.07142s/100 iter), loss = -2.5332e-07
I0704 07:58:55.983800 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:55.983808 25348 sgd_solver.cpp:106] Iteration 40700, lr = 0.00364062
I0704 07:58:58.055027 25348 solver.cpp:290] Iteration 40800 (48.2821 iter/s, 2.07116s/100 iter), loss = -2.5332e-07
I0704 07:58:58.055048 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:58:58.055055 25348 sgd_solver.cpp:106] Iteration 40800, lr = 0.003625
I0704 07:59:00.127351 25348 solver.cpp:290] Iteration 40900 (48.257 iter/s, 2.07224s/100 iter), loss = -2.5332e-07
I0704 07:59:00.127372 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:00.127380 25348 sgd_solver.cpp:106] Iteration 40900, lr = 0.00360937
I0704 07:59:02.177968 25348 solver.cpp:354] Sparsity after update:
I0704 07:59:02.179388 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:59:02.179396 25348 net.cpp:1851] conv1a_param_0(0.36) 
I0704 07:59:02.179406 25348 net.cpp:1851] conv1b_param_0(0.72) 
I0704 07:59:02.179410 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:59:02.179414 25348 net.cpp:1851] res2a_branch2a_param_0(0.72) 
I0704 07:59:02.179419 25348 net.cpp:1851] res2a_branch2b_param_0(0.72) 
I0704 07:59:02.179422 25348 net.cpp:1851] res3a_branch2a_param_0(0.72) 
I0704 07:59:02.179426 25348 net.cpp:1851] res3a_branch2b_param_0(0.72) 
I0704 07:59:02.179430 25348 net.cpp:1851] res4a_branch2a_param_0(0.72) 
I0704 07:59:02.179435 25348 net.cpp:1851] res4a_branch2b_param_0(0.72) 
I0704 07:59:02.179438 25348 net.cpp:1851] res5a_branch2a_param_0(0.72) 
I0704 07:59:02.179442 25348 net.cpp:1851] res5a_branch2b_param_0(0.72) 
I0704 07:59:02.179446 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.69458e+06/2.3599e+06) 0.718
I0704 07:59:02.179538 25348 solver.cpp:466] Iteration 41000, Testing net (#0)
I0704 07:59:03.838712 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8925
I0704 07:59:03.838733 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9958
I0704 07:59:03.838738 25348 solver.cpp:539]     Test net output #2: loss = 0.2211 (* 1 = 0.2211 loss)
I0704 07:59:03.858335 25348 solver.cpp:290] Iteration 41000 (26.8035 iter/s, 3.73086s/100 iter), loss = -2.5332e-07
I0704 07:59:03.858350 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:03.858366 25348 sgd_solver.cpp:106] Iteration 41000, lr = 0.00359375
I0704 07:59:03.858898 25348 solver.cpp:375] Finding and applying sparsity: 0.74
I0704 07:59:05.645359 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:59:07.756130 25348 solver.cpp:290] Iteration 41100 (25.6564 iter/s, 3.89767s/100 iter), loss = -2.5332e-07
I0704 07:59:07.756155 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:07.756162 25348 sgd_solver.cpp:106] Iteration 41100, lr = 0.00357813
I0704 07:59:09.847944 25348 solver.cpp:290] Iteration 41200 (47.8075 iter/s, 2.09172s/100 iter), loss = -2.5332e-07
I0704 07:59:09.847973 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:09.847983 25348 sgd_solver.cpp:106] Iteration 41200, lr = 0.0035625
I0704 07:59:11.927654 25348 solver.cpp:290] Iteration 41300 (48.0858 iter/s, 2.07962s/100 iter), loss = -2.5332e-07
I0704 07:59:11.927687 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:11.927698 25348 sgd_solver.cpp:106] Iteration 41300, lr = 0.00354687
I0704 07:59:14.001346 25348 solver.cpp:290] Iteration 41400 (48.2254 iter/s, 2.0736s/100 iter), loss = -2.5332e-07
I0704 07:59:14.001389 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:14.001399 25348 sgd_solver.cpp:106] Iteration 41400, lr = 0.00353125
I0704 07:59:16.087136 25348 solver.cpp:290] Iteration 41500 (47.9459 iter/s, 2.08569s/100 iter), loss = -2.5332e-07
I0704 07:59:16.087158 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:16.087165 25348 sgd_solver.cpp:106] Iteration 41500, lr = 0.00351562
I0704 07:59:18.178632 25348 solver.cpp:290] Iteration 41600 (47.8148 iter/s, 2.0914s/100 iter), loss = -2.5332e-07
I0704 07:59:18.178663 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:18.178673 25348 sgd_solver.cpp:106] Iteration 41600, lr = 0.0035
I0704 07:59:20.261375 25348 solver.cpp:290] Iteration 41700 (48.0158 iter/s, 2.08265s/100 iter), loss = -2.5332e-07
I0704 07:59:20.261401 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:20.261410 25348 sgd_solver.cpp:106] Iteration 41700, lr = 0.00348437
I0704 07:59:22.356503 25348 solver.cpp:290] Iteration 41800 (47.7318 iter/s, 2.09504s/100 iter), loss = -2.5332e-07
I0704 07:59:22.356593 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:22.356601 25348 sgd_solver.cpp:106] Iteration 41800, lr = 0.00346875
I0704 07:59:24.445863 25348 solver.cpp:290] Iteration 41900 (47.8651 iter/s, 2.0892s/100 iter), loss = -2.5332e-07
I0704 07:59:24.445888 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:24.445897 25348 sgd_solver.cpp:106] Iteration 41900, lr = 0.00345312
I0704 07:59:26.513860 25348 solver.cpp:354] Sparsity after update:
I0704 07:59:26.515285 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:59:26.515293 25348 net.cpp:1851] conv1a_param_0(0.37) 
I0704 07:59:26.515300 25348 net.cpp:1851] conv1b_param_0(0.74) 
I0704 07:59:26.515302 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:59:26.515305 25348 net.cpp:1851] res2a_branch2a_param_0(0.74) 
I0704 07:59:26.515306 25348 net.cpp:1851] res2a_branch2b_param_0(0.74) 
I0704 07:59:26.515308 25348 net.cpp:1851] res3a_branch2a_param_0(0.74) 
I0704 07:59:26.515311 25348 net.cpp:1851] res3a_branch2b_param_0(0.74) 
I0704 07:59:26.515312 25348 net.cpp:1851] res4a_branch2a_param_0(0.74) 
I0704 07:59:26.515314 25348 net.cpp:1851] res4a_branch2b_param_0(0.74) 
I0704 07:59:26.515316 25348 net.cpp:1851] res5a_branch2a_param_0(0.74) 
I0704 07:59:26.515318 25348 net.cpp:1851] res5a_branch2b_param_0(0.74) 
I0704 07:59:26.515321 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.74164e+06/2.3599e+06) 0.738
I0704 07:59:26.515408 25348 solver.cpp:466] Iteration 42000, Testing net (#0)
I0704 07:59:28.162900 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8879
I0704 07:59:28.162927 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9949
I0704 07:59:28.162935 25348 solver.cpp:539]     Test net output #2: loss = 0.2223 (* 1 = 0.2223 loss)
I0704 07:59:28.183058 25348 solver.cpp:290] Iteration 42000 (26.759 iter/s, 3.73706s/100 iter), loss = -2.5332e-07
I0704 07:59:28.183089 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:28.183099 25348 sgd_solver.cpp:106] Iteration 42000, lr = 0.0034375
I0704 07:59:28.183832 25348 solver.cpp:375] Finding and applying sparsity: 0.76
I0704 07:59:29.995735 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:59:32.105505 25348 solver.cpp:290] Iteration 42100 (25.4952 iter/s, 3.9223s/100 iter), loss = -2.5332e-07
I0704 07:59:32.105528 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:32.105535 25348 sgd_solver.cpp:106] Iteration 42100, lr = 0.00342188
I0704 07:59:34.224274 25348 solver.cpp:290] Iteration 42200 (47.1992 iter/s, 2.11868s/100 iter), loss = -2.5332e-07
I0704 07:59:34.224297 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:34.224303 25348 sgd_solver.cpp:106] Iteration 42200, lr = 0.00340625
I0704 07:59:36.300348 25348 solver.cpp:290] Iteration 42300 (48.1698 iter/s, 2.07599s/100 iter), loss = 0.0476188
I0704 07:59:36.300369 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:59:36.300376 25348 sgd_solver.cpp:106] Iteration 42300, lr = 0.00339063
I0704 07:59:38.371667 25348 solver.cpp:290] Iteration 42400 (48.2804 iter/s, 2.07123s/100 iter), loss = -2.5332e-07
I0704 07:59:38.371688 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:38.371695 25348 sgd_solver.cpp:106] Iteration 42400, lr = 0.003375
I0704 07:59:40.446701 25348 solver.cpp:290] Iteration 42500 (48.194 iter/s, 2.07495s/100 iter), loss = -2.5332e-07
I0704 07:59:40.446727 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:40.446735 25348 sgd_solver.cpp:106] Iteration 42500, lr = 0.00335937
I0704 07:59:42.530836 25348 solver.cpp:290] Iteration 42600 (47.9835 iter/s, 2.08405s/100 iter), loss = -2.5332e-07
I0704 07:59:42.530859 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:42.530865 25348 sgd_solver.cpp:106] Iteration 42600, lr = 0.00334375
I0704 07:59:44.615247 25348 solver.cpp:290] Iteration 42700 (47.9772 iter/s, 2.08432s/100 iter), loss = -2.5332e-07
I0704 07:59:44.615290 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:44.615299 25348 sgd_solver.cpp:106] Iteration 42700, lr = 0.00332812
I0704 07:59:46.694444 25348 solver.cpp:290] Iteration 42800 (48.0979 iter/s, 2.07909s/100 iter), loss = -2.5332e-07
I0704 07:59:46.694466 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:46.694473 25348 sgd_solver.cpp:106] Iteration 42800, lr = 0.0033125
I0704 07:59:48.783452 25348 solver.cpp:290] Iteration 42900 (47.8716 iter/s, 2.08892s/100 iter), loss = -2.5332e-07
I0704 07:59:48.783474 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:48.783481 25348 sgd_solver.cpp:106] Iteration 42900, lr = 0.00329687
I0704 07:59:50.858790 25348 solver.cpp:354] Sparsity after update:
I0704 07:59:50.860355 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 07:59:50.860364 25348 net.cpp:1851] conv1a_param_0(0.38) 
I0704 07:59:50.860375 25348 net.cpp:1851] conv1b_param_0(0.76) 
I0704 07:59:50.860380 25348 net.cpp:1851] fc10_param_0(0) 
I0704 07:59:50.860384 25348 net.cpp:1851] res2a_branch2a_param_0(0.76) 
I0704 07:59:50.860388 25348 net.cpp:1851] res2a_branch2b_param_0(0.76) 
I0704 07:59:50.860391 25348 net.cpp:1851] res3a_branch2a_param_0(0.76) 
I0704 07:59:50.860396 25348 net.cpp:1851] res3a_branch2b_param_0(0.76) 
I0704 07:59:50.860399 25348 net.cpp:1851] res4a_branch2a_param_0(0.76) 
I0704 07:59:50.860404 25348 net.cpp:1851] res4a_branch2b_param_0(0.76) 
I0704 07:59:50.860407 25348 net.cpp:1851] res5a_branch2a_param_0(0.76) 
I0704 07:59:50.860411 25348 net.cpp:1851] res5a_branch2b_param_0(0.76) 
I0704 07:59:50.860414 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.78872e+06/2.3599e+06) 0.758
I0704 07:59:50.860551 25348 solver.cpp:466] Iteration 43000, Testing net (#0)
I0704 07:59:52.508286 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8888
I0704 07:59:52.508338 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9941
I0704 07:59:52.508345 25348 solver.cpp:539]     Test net output #2: loss = 0.2339 (* 1 = 0.2339 loss)
I0704 07:59:52.528066 25348 solver.cpp:290] Iteration 43000 (26.7059 iter/s, 3.74448s/100 iter), loss = -2.5332e-07
I0704 07:59:52.528084 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 07:59:52.528096 25348 sgd_solver.cpp:106] Iteration 43000, lr = 0.00328125
I0704 07:59:52.528620 25348 solver.cpp:375] Finding and applying sparsity: 0.78
I0704 07:59:54.320488 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 07:59:56.438158 25348 solver.cpp:290] Iteration 43100 (25.5757 iter/s, 3.90996s/100 iter), loss = 0.0476188
I0704 07:59:56.438186 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:59:56.438194 25348 sgd_solver.cpp:106] Iteration 43100, lr = 0.00326563
I0704 07:59:58.522915 25348 solver.cpp:290] Iteration 43200 (47.9693 iter/s, 2.08467s/100 iter), loss = 0.0476188
I0704 07:59:58.522936 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 07:59:58.522943 25348 sgd_solver.cpp:106] Iteration 43200, lr = 0.00325
I0704 08:00:00.594065 25348 solver.cpp:290] Iteration 43300 (48.2844 iter/s, 2.07106s/100 iter), loss = -2.5332e-07
I0704 08:00:00.594087 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:00.594094 25348 sgd_solver.cpp:106] Iteration 43300, lr = 0.00323438
I0704 08:00:02.674525 25348 solver.cpp:290] Iteration 43400 (48.0685 iter/s, 2.08037s/100 iter), loss = -2.5332e-07
I0704 08:00:02.674561 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:02.674571 25348 sgd_solver.cpp:106] Iteration 43400, lr = 0.00321875
I0704 08:00:04.748981 25348 solver.cpp:290] Iteration 43500 (48.2077 iter/s, 2.07436s/100 iter), loss = -2.5332e-07
I0704 08:00:04.749004 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:04.749011 25348 sgd_solver.cpp:106] Iteration 43500, lr = 0.00320312
I0704 08:00:06.817955 25348 solver.cpp:290] Iteration 43600 (48.3352 iter/s, 2.06889s/100 iter), loss = -2.5332e-07
I0704 08:00:06.817980 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:06.817987 25348 sgd_solver.cpp:106] Iteration 43600, lr = 0.0031875
I0704 08:00:08.892122 25348 solver.cpp:290] Iteration 43700 (48.2142 iter/s, 2.07408s/100 iter), loss = -2.5332e-07
I0704 08:00:08.892145 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:08.892154 25348 sgd_solver.cpp:106] Iteration 43700, lr = 0.00317187
I0704 08:00:10.970543 25348 solver.cpp:290] Iteration 43800 (48.1155 iter/s, 2.07833s/100 iter), loss = -2.5332e-07
I0704 08:00:10.970568 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:10.970577 25348 sgd_solver.cpp:106] Iteration 43800, lr = 0.00315625
I0704 08:00:13.045102 25348 solver.cpp:290] Iteration 43900 (48.2051 iter/s, 2.07447s/100 iter), loss = -2.5332e-07
I0704 08:00:13.045125 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:13.045131 25348 sgd_solver.cpp:106] Iteration 43900, lr = 0.00314062
I0704 08:00:15.098747 25348 solver.cpp:354] Sparsity after update:
I0704 08:00:15.100158 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:00:15.100165 25348 net.cpp:1851] conv1a_param_0(0.39) 
I0704 08:00:15.100172 25348 net.cpp:1851] conv1b_param_0(0.78) 
I0704 08:00:15.100175 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:00:15.100178 25348 net.cpp:1851] res2a_branch2a_param_0(0.78) 
I0704 08:00:15.100179 25348 net.cpp:1851] res2a_branch2b_param_0(0.78) 
I0704 08:00:15.100181 25348 net.cpp:1851] res3a_branch2a_param_0(0.78) 
I0704 08:00:15.100183 25348 net.cpp:1851] res3a_branch2b_param_0(0.78) 
I0704 08:00:15.100185 25348 net.cpp:1851] res4a_branch2a_param_0(0.78) 
I0704 08:00:15.100186 25348 net.cpp:1851] res4a_branch2b_param_0(0.78) 
I0704 08:00:15.100188 25348 net.cpp:1851] res5a_branch2a_param_0(0.78) 
I0704 08:00:15.100201 25348 net.cpp:1851] res5a_branch2b_param_0(0.78) 
I0704 08:00:15.100205 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.83579e+06/2.3599e+06) 0.778
I0704 08:00:15.100296 25348 solver.cpp:466] Iteration 44000, Testing net (#0)
I0704 08:00:16.742854 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.882
I0704 08:00:16.742873 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9922
I0704 08:00:16.742879 25348 solver.cpp:539]     Test net output #2: loss = 0.2602 (* 1 = 0.2602 loss)
I0704 08:00:16.762612 25348 solver.cpp:290] Iteration 44000 (26.9007 iter/s, 3.71738s/100 iter), loss = -2.5332e-07
I0704 08:00:16.762630 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:16.762641 25348 sgd_solver.cpp:106] Iteration 44000, lr = 0.003125
I0704 08:00:16.763146 25348 solver.cpp:375] Finding and applying sparsity: 0.8
I0704 08:00:18.703826 25348 net.cpp:1824] All zero weights of convolution layers are frozen
I0704 08:00:20.803694 25348 solver.cpp:290] Iteration 44100 (24.7467 iter/s, 4.04095s/100 iter), loss = 0.142857
I0704 08:00:20.803716 25348 solver.cpp:309]     Train net output #0: loss = 0.142857 (* 1 = 0.142857 loss)
I0704 08:00:20.803725 25348 sgd_solver.cpp:106] Iteration 44100, lr = 0.00310938
I0704 08:00:22.874913 25348 solver.cpp:290] Iteration 44200 (48.2827 iter/s, 2.07113s/100 iter), loss = -2.5332e-07
I0704 08:00:22.874984 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:22.874992 25348 sgd_solver.cpp:106] Iteration 44200, lr = 0.00309375
I0704 08:00:24.949051 25348 solver.cpp:290] Iteration 44300 (48.2159 iter/s, 2.074s/100 iter), loss = -2.5332e-07
I0704 08:00:24.949076 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:24.949085 25348 sgd_solver.cpp:106] Iteration 44300, lr = 0.00307812
I0704 08:00:27.021338 25348 solver.cpp:290] Iteration 44400 (48.2579 iter/s, 2.0722s/100 iter), loss = -2.5332e-07
I0704 08:00:27.021365 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:27.021375 25348 sgd_solver.cpp:106] Iteration 44400, lr = 0.0030625
I0704 08:00:29.100705 25348 solver.cpp:290] Iteration 44500 (48.0936 iter/s, 2.07928s/100 iter), loss = -2.5332e-07
I0704 08:00:29.100726 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:29.100733 25348 sgd_solver.cpp:106] Iteration 44500, lr = 0.00304687
I0704 08:00:31.175432 25348 solver.cpp:290] Iteration 44600 (48.2011 iter/s, 2.07464s/100 iter), loss = -2.57045e-07
I0704 08:00:31.175454 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:31.175462 25348 sgd_solver.cpp:106] Iteration 44600, lr = 0.00303125
I0704 08:00:33.248586 25348 solver.cpp:290] Iteration 44700 (48.2377 iter/s, 2.07307s/100 iter), loss = -2.5332e-07
I0704 08:00:33.248620 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:33.248630 25348 sgd_solver.cpp:106] Iteration 44700, lr = 0.00301562
I0704 08:00:35.349755 25348 solver.cpp:290] Iteration 44800 (47.5948 iter/s, 2.10107s/100 iter), loss = -2.5332e-07
I0704 08:00:35.349787 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:35.349798 25348 sgd_solver.cpp:106] Iteration 44800, lr = 0.003
I0704 08:00:37.419860 25348 solver.cpp:290] Iteration 44900 (48.3089 iter/s, 2.07001s/100 iter), loss = -2.5332e-07
I0704 08:00:37.419886 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:37.419895 25348 sgd_solver.cpp:106] Iteration 44900, lr = 0.00298437
I0704 08:00:39.474380 25348 solver.cpp:354] Sparsity after update:
I0704 08:00:39.475797 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:00:39.475805 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:00:39.475813 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:00:39.475816 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:00:39.475817 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:00:39.475819 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:00:39.475821 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:00:39.475823 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:00:39.475826 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:00:39.475827 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:00:39.475829 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:00:39.475831 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:00:39.475833 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:00:39.475961 25348 solver.cpp:466] Iteration 45000, Testing net (#0)
I0704 08:00:41.118183 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8523
I0704 08:00:41.118202 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.991
I0704 08:00:41.118207 25348 solver.cpp:539]     Test net output #2: loss = 0.366 (* 1 = 0.366 loss)
I0704 08:00:41.138463 25348 solver.cpp:290] Iteration 45000 (26.8928 iter/s, 3.71847s/100 iter), loss = -2.5332e-07
I0704 08:00:41.138478 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:41.138494 25348 sgd_solver.cpp:106] Iteration 45000, lr = 0.00296875
I0704 08:00:43.210952 25348 solver.cpp:290] Iteration 45100 (48.253 iter/s, 2.07241s/100 iter), loss = -2.5332e-07
I0704 08:00:43.210975 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:43.210981 25348 sgd_solver.cpp:106] Iteration 45100, lr = 0.00295313
I0704 08:00:45.284284 25348 solver.cpp:290] Iteration 45200 (48.2336 iter/s, 2.07324s/100 iter), loss = -2.5332e-07
I0704 08:00:45.284307 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:45.284313 25348 sgd_solver.cpp:106] Iteration 45200, lr = 0.0029375
I0704 08:00:47.356322 25348 solver.cpp:290] Iteration 45300 (48.2637 iter/s, 2.07195s/100 iter), loss = -2.5332e-07
I0704 08:00:47.356343 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:47.356350 25348 sgd_solver.cpp:106] Iteration 45300, lr = 0.00292188
I0704 08:00:49.429651 25348 solver.cpp:290] Iteration 45400 (48.2336 iter/s, 2.07324s/100 iter), loss = -2.5332e-07
I0704 08:00:49.429673 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:49.429680 25348 sgd_solver.cpp:106] Iteration 45400, lr = 0.00290625
I0704 08:00:51.501461 25348 solver.cpp:290] Iteration 45500 (48.269 iter/s, 2.07172s/100 iter), loss = -2.5332e-07
I0704 08:00:51.501484 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:51.501492 25348 sgd_solver.cpp:106] Iteration 45500, lr = 0.00289063
I0704 08:00:53.574278 25348 solver.cpp:290] Iteration 45600 (48.2456 iter/s, 2.07273s/100 iter), loss = -2.5332e-07
I0704 08:00:53.574337 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:53.574347 25348 sgd_solver.cpp:106] Iteration 45600, lr = 0.002875
I0704 08:00:55.648495 25348 solver.cpp:290] Iteration 45700 (48.2137 iter/s, 2.0741s/100 iter), loss = -2.5332e-07
I0704 08:00:55.648517 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:55.648524 25348 sgd_solver.cpp:106] Iteration 45700, lr = 0.00285937
I0704 08:00:57.721370 25348 solver.cpp:290] Iteration 45800 (48.2442 iter/s, 2.07279s/100 iter), loss = -2.5332e-07
I0704 08:00:57.721393 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:00:57.721400 25348 sgd_solver.cpp:106] Iteration 45800, lr = 0.00284375
I0704 08:00:59.803424 25348 solver.cpp:290] Iteration 45900 (48.0315 iter/s, 2.08197s/100 iter), loss = 0.0476188
I0704 08:00:59.803450 25348 solver.cpp:309]     Train net output #0: loss = 0.047619 (* 1 = 0.047619 loss)
I0704 08:00:59.803459 25348 sgd_solver.cpp:106] Iteration 45900, lr = 0.00282812
I0704 08:01:01.855618 25348 solver.cpp:354] Sparsity after update:
I0704 08:01:01.857007 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:01:01.857014 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:01:01.857022 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:01:01.857023 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:01:01.857025 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:01:01.857028 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:01:01.857029 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:01:01.857031 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:01:01.857033 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:01:01.857035 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:01:01.857038 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:01:01.857039 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:01:01.857041 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:01:01.857127 25348 solver.cpp:466] Iteration 46000, Testing net (#0)
I0704 08:01:03.498353 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.8824
I0704 08:01:03.498373 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9959
I0704 08:01:03.498378 25348 solver.cpp:539]     Test net output #2: loss = 0.258 (* 1 = 0.258 loss)
I0704 08:01:03.518144 25348 solver.cpp:290] Iteration 46000 (26.9209 iter/s, 3.71459s/100 iter), loss = -2.5332e-07
I0704 08:01:03.518162 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:03.518174 25348 sgd_solver.cpp:106] Iteration 46000, lr = 0.0028125
I0704 08:01:05.590648 25348 solver.cpp:290] Iteration 46100 (48.2527 iter/s, 2.07242s/100 iter), loss = -2.5332e-07
I0704 08:01:05.590670 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:05.590677 25348 sgd_solver.cpp:106] Iteration 46100, lr = 0.00279688
I0704 08:01:07.671330 25348 solver.cpp:290] Iteration 46200 (48.0632 iter/s, 2.0806s/100 iter), loss = -2.5332e-07
I0704 08:01:07.671351 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:07.671358 25348 sgd_solver.cpp:106] Iteration 46200, lr = 0.00278125
I0704 08:01:09.747411 25348 solver.cpp:290] Iteration 46300 (48.1697 iter/s, 2.076s/100 iter), loss = -2.5332e-07
I0704 08:01:09.747434 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:09.747440 25348 sgd_solver.cpp:106] Iteration 46300, lr = 0.00276563
I0704 08:01:11.822172 25348 solver.cpp:290] Iteration 46400 (48.2003 iter/s, 2.07467s/100 iter), loss = -2.5332e-07
I0704 08:01:11.822193 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:11.822201 25348 sgd_solver.cpp:106] Iteration 46400, lr = 0.00275
I0704 08:01:13.894567 25348 solver.cpp:290] Iteration 46500 (48.2553 iter/s, 2.07231s/100 iter), loss = -2.5332e-07
I0704 08:01:13.894588 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:13.894596 25348 sgd_solver.cpp:106] Iteration 46500, lr = 0.00273437
I0704 08:01:15.968050 25348 solver.cpp:290] Iteration 46600 (48.2301 iter/s, 2.07339s/100 iter), loss = -2.5332e-07
I0704 08:01:15.968077 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:15.968086 25348 sgd_solver.cpp:106] Iteration 46600, lr = 0.00271875
I0704 08:01:18.040457 25348 solver.cpp:290] Iteration 46700 (48.2552 iter/s, 2.07232s/100 iter), loss = -2.5332e-07
I0704 08:01:18.040482 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:18.040489 25348 sgd_solver.cpp:106] Iteration 46700, lr = 0.00270312
I0704 08:01:20.117025 25348 solver.cpp:290] Iteration 46800 (48.1584 iter/s, 2.07648s/100 iter), loss = -2.5332e-07
I0704 08:01:20.117049 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:20.117055 25348 sgd_solver.cpp:106] Iteration 46800, lr = 0.0026875
I0704 08:01:22.193729 25348 solver.cpp:290] Iteration 46900 (48.1553 iter/s, 2.07662s/100 iter), loss = -2.5332e-07
I0704 08:01:22.193750 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:22.193758 25348 sgd_solver.cpp:106] Iteration 46900, lr = 0.00267187
I0704 08:01:24.247692 25348 solver.cpp:354] Sparsity after update:
I0704 08:01:24.249078 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:01:24.249086 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:01:24.249097 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:01:24.249101 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:01:24.249106 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:01:24.249110 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:01:24.249114 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:01:24.249119 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:01:24.249121 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:01:24.249125 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:01:24.249130 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:01:24.249132 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:01:24.249136 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:01:24.249228 25348 solver.cpp:466] Iteration 47000, Testing net (#0)
I0704 08:01:25.892215 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.904
I0704 08:01:25.892235 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9957
I0704 08:01:25.892241 25348 solver.cpp:539]     Test net output #2: loss = 0.1985 (* 1 = 0.1985 loss)
I0704 08:01:25.911939 25348 solver.cpp:290] Iteration 47000 (26.8956 iter/s, 3.71808s/100 iter), loss = -2.5332e-07
I0704 08:01:25.911957 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:25.911969 25348 sgd_solver.cpp:106] Iteration 47000, lr = 0.00265625
I0704 08:01:27.990439 25348 solver.cpp:290] Iteration 47100 (48.1136 iter/s, 2.07842s/100 iter), loss = -2.5332e-07
I0704 08:01:27.990463 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:27.990471 25348 sgd_solver.cpp:106] Iteration 47100, lr = 0.00264063
I0704 08:01:30.063395 25348 solver.cpp:290] Iteration 47200 (48.2423 iter/s, 2.07287s/100 iter), loss = -2.5332e-07
I0704 08:01:30.063416 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:30.063423 25348 sgd_solver.cpp:106] Iteration 47200, lr = 0.002625
I0704 08:01:32.134570 25348 solver.cpp:290] Iteration 47300 (48.2838 iter/s, 2.07109s/100 iter), loss = -2.5332e-07
I0704 08:01:32.134593 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:32.134600 25348 sgd_solver.cpp:106] Iteration 47300, lr = 0.00260938
I0704 08:01:34.215946 25348 solver.cpp:290] Iteration 47400 (48.0471 iter/s, 2.08129s/100 iter), loss = -2.5332e-07
I0704 08:01:34.215970 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:34.215978 25348 sgd_solver.cpp:106] Iteration 47400, lr = 0.00259375
I0704 08:01:36.295346 25348 solver.cpp:290] Iteration 47500 (48.0928 iter/s, 2.07931s/100 iter), loss = -2.5332e-07
I0704 08:01:36.295367 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:36.295375 25348 sgd_solver.cpp:106] Iteration 47500, lr = 0.00257812
I0704 08:01:38.368974 25348 solver.cpp:290] Iteration 47600 (48.2267 iter/s, 2.07354s/100 iter), loss = -2.5332e-07
I0704 08:01:38.369000 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:38.369009 25348 sgd_solver.cpp:106] Iteration 47600, lr = 0.0025625
I0704 08:01:40.440454 25348 solver.cpp:290] Iteration 47700 (48.2767 iter/s, 2.07139s/100 iter), loss = -2.5332e-07
I0704 08:01:40.440477 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:40.440485 25348 sgd_solver.cpp:106] Iteration 47700, lr = 0.00254687
I0704 08:01:42.516633 25348 solver.cpp:290] Iteration 47800 (48.1675 iter/s, 2.07609s/100 iter), loss = -2.5332e-07
I0704 08:01:42.516655 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:42.516661 25348 sgd_solver.cpp:106] Iteration 47800, lr = 0.00253125
I0704 08:01:44.586904 25348 solver.cpp:290] Iteration 47900 (48.3049 iter/s, 2.07018s/100 iter), loss = -2.5332e-07
I0704 08:01:44.586925 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:44.586956 25348 sgd_solver.cpp:106] Iteration 47900, lr = 0.00251562
I0704 08:01:46.638833 25348 solver.cpp:354] Sparsity after update:
I0704 08:01:46.640235 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:01:46.640242 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:01:46.640249 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:01:46.640251 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:01:46.640254 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:01:46.640255 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:01:46.640257 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:01:46.640259 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:01:46.640261 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:01:46.640264 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:01:46.640265 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:01:46.640267 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:01:46.640269 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:01:46.640359 25348 solver.cpp:466] Iteration 48000, Testing net (#0)
I0704 08:01:48.282627 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.904
I0704 08:01:48.282647 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9948
I0704 08:01:48.282654 25348 solver.cpp:539]     Test net output #2: loss = 0.2056 (* 1 = 0.2056 loss)
I0704 08:01:48.302356 25348 solver.cpp:290] Iteration 48000 (26.9155 iter/s, 3.71532s/100 iter), loss = -2.5332e-07
I0704 08:01:48.302373 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:48.302389 25348 sgd_solver.cpp:106] Iteration 48000, lr = 0.0025
I0704 08:01:50.379235 25348 solver.cpp:290] Iteration 48100 (48.1511 iter/s, 2.0768s/100 iter), loss = -2.5332e-07
I0704 08:01:50.379257 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:50.379264 25348 sgd_solver.cpp:106] Iteration 48100, lr = 0.00248438
I0704 08:01:52.458756 25348 solver.cpp:290] Iteration 48200 (48.09 iter/s, 2.07943s/100 iter), loss = -2.5332e-07
I0704 08:01:52.458783 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:52.458791 25348 sgd_solver.cpp:106] Iteration 48200, lr = 0.00246875
I0704 08:01:54.528787 25348 solver.cpp:290] Iteration 48300 (48.3105 iter/s, 2.06994s/100 iter), loss = -2.5332e-07
I0704 08:01:54.528863 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:54.528872 25348 sgd_solver.cpp:106] Iteration 48300, lr = 0.00245313
I0704 08:01:56.598598 25348 solver.cpp:290] Iteration 48400 (48.3168 iter/s, 2.06967s/100 iter), loss = -2.5332e-07
I0704 08:01:56.598620 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:56.598629 25348 sgd_solver.cpp:106] Iteration 48400, lr = 0.0024375
I0704 08:01:58.666834 25348 solver.cpp:290] Iteration 48500 (48.3524 iter/s, 2.06815s/100 iter), loss = -2.5332e-07
I0704 08:01:58.666860 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:01:58.666869 25348 sgd_solver.cpp:106] Iteration 48500, lr = 0.00242188
I0704 08:02:00.735390 25348 solver.cpp:290] Iteration 48600 (48.345 iter/s, 2.06847s/100 iter), loss = -2.5332e-07
I0704 08:02:00.735416 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:00.735425 25348 sgd_solver.cpp:106] Iteration 48600, lr = 0.00240625
I0704 08:02:02.811290 25348 solver.cpp:290] Iteration 48700 (48.1739 iter/s, 2.07581s/100 iter), loss = -2.5332e-07
I0704 08:02:02.811313 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:02.811321 25348 sgd_solver.cpp:106] Iteration 48700, lr = 0.00239062
I0704 08:02:04.884481 25348 solver.cpp:290] Iteration 48800 (48.2368 iter/s, 2.07311s/100 iter), loss = -2.5332e-07
I0704 08:02:04.884505 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:04.884510 25348 sgd_solver.cpp:106] Iteration 48800, lr = 0.002375
I0704 08:02:06.954874 25348 solver.cpp:290] Iteration 48900 (48.3021 iter/s, 2.0703s/100 iter), loss = -2.5332e-07
I0704 08:02:06.954895 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:06.954902 25348 sgd_solver.cpp:106] Iteration 48900, lr = 0.00235937
I0704 08:02:09.005672 25348 solver.cpp:354] Sparsity after update:
I0704 08:02:09.007086 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:02:09.007093 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:02:09.007100 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:02:09.007103 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:02:09.007105 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:02:09.007107 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:02:09.007109 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:02:09.007112 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:02:09.007113 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:02:09.007115 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:02:09.007117 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:02:09.007119 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:02:09.007120 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:02:09.007218 25348 solver.cpp:466] Iteration 49000, Testing net (#0)
I0704 08:02:10.649369 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9086
I0704 08:02:10.649387 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9967
I0704 08:02:10.649392 25348 solver.cpp:539]     Test net output #2: loss = 0.1895 (* 1 = 0.1895 loss)
I0704 08:02:10.669046 25348 solver.cpp:290] Iteration 49000 (26.9248 iter/s, 3.71404s/100 iter), loss = -2.5332e-07
I0704 08:02:10.669064 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:10.669076 25348 sgd_solver.cpp:106] Iteration 49000, lr = 0.00234375
I0704 08:02:12.740764 25348 solver.cpp:290] Iteration 49100 (48.271 iter/s, 2.07164s/100 iter), loss = -2.5332e-07
I0704 08:02:12.740787 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:12.740794 25348 sgd_solver.cpp:106] Iteration 49100, lr = 0.00232813
I0704 08:02:14.811699 25348 solver.cpp:290] Iteration 49200 (48.2894 iter/s, 2.07085s/100 iter), loss = -2.5332e-07
I0704 08:02:14.811722 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:14.811728 25348 sgd_solver.cpp:106] Iteration 49200, lr = 0.0023125
I0704 08:02:16.881594 25348 solver.cpp:290] Iteration 49300 (48.3136 iter/s, 2.06981s/100 iter), loss = -2.5332e-07
I0704 08:02:16.881616 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:16.881623 25348 sgd_solver.cpp:106] Iteration 49300, lr = 0.00229687
I0704 08:02:18.956161 25348 solver.cpp:290] Iteration 49400 (48.2049 iter/s, 2.07448s/100 iter), loss = -2.5332e-07
I0704 08:02:18.956182 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:18.956188 25348 sgd_solver.cpp:106] Iteration 49400, lr = 0.00228125
I0704 08:02:21.030202 25348 solver.cpp:290] Iteration 49500 (48.217 iter/s, 2.07396s/100 iter), loss = -2.5332e-07
I0704 08:02:21.030226 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:21.030236 25348 sgd_solver.cpp:106] Iteration 49500, lr = 0.00226562
I0704 08:02:23.102231 25348 solver.cpp:290] Iteration 49600 (48.2639 iter/s, 2.07194s/100 iter), loss = -2.5332e-07
I0704 08:02:23.102253 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:23.102259 25348 sgd_solver.cpp:106] Iteration 49600, lr = 0.00225
I0704 08:02:25.171476 25348 solver.cpp:290] Iteration 49700 (48.3289 iter/s, 2.06916s/100 iter), loss = -2.5332e-07
I0704 08:02:25.171527 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:25.171535 25348 sgd_solver.cpp:106] Iteration 49700, lr = 0.00223437
I0704 08:02:27.245054 25348 solver.cpp:290] Iteration 49800 (48.2285 iter/s, 2.07346s/100 iter), loss = -2.5332e-07
I0704 08:02:27.245079 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:27.245086 25348 sgd_solver.cpp:106] Iteration 49800, lr = 0.00221875
I0704 08:02:29.316233 25348 solver.cpp:290] Iteration 49900 (48.2837 iter/s, 2.07109s/100 iter), loss = -2.5332e-07
I0704 08:02:29.316256 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:29.316263 25348 sgd_solver.cpp:106] Iteration 49900, lr = 0.00220312
I0704 08:02:31.367466 25348 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_50000.caffemodel
I0704 08:02:31.384235 25348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_50000.solverstate
I0704 08:02:31.391520 25348 solver.cpp:354] Sparsity after update:
I0704 08:02:31.392475 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:02:31.392483 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:02:31.392491 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:02:31.392493 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:02:31.392496 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:02:31.392498 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:02:31.392499 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:02:31.392501 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:02:31.392503 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:02:31.392505 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:02:31.392508 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:02:31.392509 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:02:31.392511 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:02:31.392607 25348 solver.cpp:466] Iteration 50000, Testing net (#0)
I0704 08:02:33.034262 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.909
I0704 08:02:33.034282 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9967
I0704 08:02:33.034287 25348 solver.cpp:539]     Test net output #2: loss = 0.1869 (* 1 = 0.1869 loss)
I0704 08:02:33.054738 25348 solver.cpp:290] Iteration 50000 (26.7496 iter/s, 3.73837s/100 iter), loss = -2.5332e-07
I0704 08:02:33.054755 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:33.054766 25348 sgd_solver.cpp:106] Iteration 50000, lr = 0.0021875
I0704 08:02:35.150411 25348 solver.cpp:290] Iteration 50100 (47.7192 iter/s, 2.09559s/100 iter), loss = -2.5332e-07
I0704 08:02:35.150434 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:35.150440 25348 sgd_solver.cpp:106] Iteration 50100, lr = 0.00217188
I0704 08:02:37.219923 25348 solver.cpp:290] Iteration 50200 (48.3227 iter/s, 2.06942s/100 iter), loss = -2.5332e-07
I0704 08:02:37.219947 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:37.219955 25348 sgd_solver.cpp:106] Iteration 50200, lr = 0.00215625
I0704 08:02:39.293373 25348 solver.cpp:290] Iteration 50300 (48.2308 iter/s, 2.07336s/100 iter), loss = -2.5332e-07
I0704 08:02:39.293395 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:39.293402 25348 sgd_solver.cpp:106] Iteration 50300, lr = 0.00214063
I0704 08:02:41.362026 25348 solver.cpp:290] Iteration 50400 (48.3427 iter/s, 2.06857s/100 iter), loss = -2.5332e-07
I0704 08:02:41.362061 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:41.362071 25348 sgd_solver.cpp:106] Iteration 50400, lr = 0.002125
I0704 08:02:43.434051 25348 solver.cpp:290] Iteration 50500 (48.2643 iter/s, 2.07193s/100 iter), loss = -2.5332e-07
I0704 08:02:43.434078 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:43.434103 25348 sgd_solver.cpp:106] Iteration 50500, lr = 0.00210937
I0704 08:02:45.504045 25348 solver.cpp:290] Iteration 50600 (48.3114 iter/s, 2.0699s/100 iter), loss = -2.5332e-07
I0704 08:02:45.504068 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:45.504075 25348 sgd_solver.cpp:106] Iteration 50600, lr = 0.00209375
I0704 08:02:47.576961 25348 solver.cpp:290] Iteration 50700 (48.2432 iter/s, 2.07283s/100 iter), loss = -2.5332e-07
I0704 08:02:47.576983 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:47.576992 25348 sgd_solver.cpp:106] Iteration 50700, lr = 0.00207812
I0704 08:02:49.649227 25348 solver.cpp:290] Iteration 50800 (48.2584 iter/s, 2.07218s/100 iter), loss = -2.5332e-07
I0704 08:02:49.649250 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:49.649256 25348 sgd_solver.cpp:106] Iteration 50800, lr = 0.0020625
I0704 08:02:51.720217 25348 solver.cpp:290] Iteration 50900 (48.2881 iter/s, 2.0709s/100 iter), loss = -2.5332e-07
I0704 08:02:51.720240 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:51.720247 25348 sgd_solver.cpp:106] Iteration 50900, lr = 0.00204687
I0704 08:02:53.770952 25348 solver.cpp:354] Sparsity after update:
I0704 08:02:53.772341 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:02:53.772347 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:02:53.772354 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:02:53.772356 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:02:53.772359 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:02:53.772361 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:02:53.772364 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:02:53.772367 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:02:53.772368 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:02:53.772370 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:02:53.772373 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:02:53.772375 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:02:53.772378 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:02:53.772466 25348 solver.cpp:466] Iteration 51000, Testing net (#0)
I0704 08:02:55.414664 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.908
I0704 08:02:55.414722 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9963
I0704 08:02:55.414729 25348 solver.cpp:539]     Test net output #2: loss = 0.1874 (* 1 = 0.1874 loss)
I0704 08:02:55.434356 25348 solver.cpp:290] Iteration 51000 (26.9251 iter/s, 3.71401s/100 iter), loss = -2.5332e-07
I0704 08:02:55.434373 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:55.434389 25348 sgd_solver.cpp:106] Iteration 51000, lr = 0.00203125
I0704 08:02:57.504086 25348 solver.cpp:290] Iteration 51100 (48.3174 iter/s, 2.06965s/100 iter), loss = -2.5332e-07
I0704 08:02:57.504108 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:57.504115 25348 sgd_solver.cpp:106] Iteration 51100, lr = 0.00201563
I0704 08:02:59.577030 25348 solver.cpp:290] Iteration 51200 (48.2426 iter/s, 2.07286s/100 iter), loss = -2.5332e-07
I0704 08:02:59.577054 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:02:59.577064 25348 sgd_solver.cpp:106] Iteration 51200, lr = 0.002
I0704 08:03:01.647611 25348 solver.cpp:290] Iteration 51300 (48.2977 iter/s, 2.07049s/100 iter), loss = -2.5332e-07
I0704 08:03:01.647634 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:01.647639 25348 sgd_solver.cpp:106] Iteration 51300, lr = 0.00198438
I0704 08:03:03.717363 25348 solver.cpp:290] Iteration 51400 (48.317 iter/s, 2.06967s/100 iter), loss = -2.5332e-07
I0704 08:03:03.717386 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:03.717393 25348 sgd_solver.cpp:106] Iteration 51400, lr = 0.00196875
I0704 08:03:05.789881 25348 solver.cpp:290] Iteration 51500 (48.2525 iter/s, 2.07243s/100 iter), loss = -2.5332e-07
I0704 08:03:05.789904 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:05.789911 25348 sgd_solver.cpp:106] Iteration 51500, lr = 0.00195312
I0704 08:03:07.866638 25348 solver.cpp:290] Iteration 51600 (48.1541 iter/s, 2.07667s/100 iter), loss = -2.5332e-07
I0704 08:03:07.866662 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:07.866669 25348 sgd_solver.cpp:106] Iteration 51600, lr = 0.0019375
I0704 08:03:09.942589 25348 solver.cpp:290] Iteration 51700 (48.1727 iter/s, 2.07586s/100 iter), loss = -2.5332e-07
I0704 08:03:09.942613 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:09.942621 25348 sgd_solver.cpp:106] Iteration 51700, lr = 0.00192187
I0704 08:03:12.018702 25348 solver.cpp:290] Iteration 51800 (48.1689 iter/s, 2.07603s/100 iter), loss = -2.5332e-07
I0704 08:03:12.018725 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:12.018731 25348 sgd_solver.cpp:106] Iteration 51800, lr = 0.00190625
I0704 08:03:14.090252 25348 solver.cpp:290] Iteration 51900 (48.2751 iter/s, 2.07146s/100 iter), loss = -2.5332e-07
I0704 08:03:14.090276 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:14.090281 25348 sgd_solver.cpp:106] Iteration 51900, lr = 0.00189062
I0704 08:03:16.140334 25348 solver.cpp:354] Sparsity after update:
I0704 08:03:16.141726 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:03:16.141732 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:03:16.141739 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:03:16.141742 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:03:16.141744 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:03:16.141746 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:03:16.141748 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:03:16.141751 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:03:16.141752 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:03:16.141754 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:03:16.141757 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:03:16.141758 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:03:16.141760 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:03:16.141860 25348 solver.cpp:466] Iteration 52000, Testing net (#0)
I0704 08:03:17.783646 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.911
I0704 08:03:17.783665 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9964
I0704 08:03:17.783670 25348 solver.cpp:539]     Test net output #2: loss = 0.1871 (* 1 = 0.1871 loss)
I0704 08:03:17.803330 25348 solver.cpp:290] Iteration 52000 (26.9328 iter/s, 3.71295s/100 iter), loss = -2.5332e-07
I0704 08:03:17.803349 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:17.803359 25348 sgd_solver.cpp:106] Iteration 52000, lr = 0.001875
I0704 08:03:19.874171 25348 solver.cpp:290] Iteration 52100 (48.2915 iter/s, 2.07076s/100 iter), loss = -2.5332e-07
I0704 08:03:19.874193 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:19.874200 25348 sgd_solver.cpp:106] Iteration 52100, lr = 0.00185938
I0704 08:03:21.944897 25348 solver.cpp:290] Iteration 52200 (48.2943 iter/s, 2.07064s/100 iter), loss = -2.5332e-07
I0704 08:03:21.944922 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:21.944931 25348 sgd_solver.cpp:106] Iteration 52200, lr = 0.00184375
I0704 08:03:24.015745 25348 solver.cpp:290] Iteration 52300 (48.2915 iter/s, 2.07076s/100 iter), loss = -2.5332e-07
I0704 08:03:24.015779 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:24.015789 25348 sgd_solver.cpp:106] Iteration 52300, lr = 0.00182813
I0704 08:03:26.088711 25348 solver.cpp:290] Iteration 52400 (48.2423 iter/s, 2.07287s/100 iter), loss = -2.5332e-07
I0704 08:03:26.088783 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:26.088795 25348 sgd_solver.cpp:106] Iteration 52400, lr = 0.0018125
I0704 08:03:28.157580 25348 solver.cpp:290] Iteration 52500 (48.3386 iter/s, 2.06874s/100 iter), loss = -2.5332e-07
I0704 08:03:28.157605 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:28.157613 25348 sgd_solver.cpp:106] Iteration 52500, lr = 0.00179687
I0704 08:03:30.230128 25348 solver.cpp:290] Iteration 52600 (48.2519 iter/s, 2.07246s/100 iter), loss = -2.5332e-07
I0704 08:03:30.230159 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:30.230168 25348 sgd_solver.cpp:106] Iteration 52600, lr = 0.00178125
I0704 08:03:32.305192 25348 solver.cpp:290] Iteration 52700 (48.1934 iter/s, 2.07497s/100 iter), loss = -2.5332e-07
I0704 08:03:32.305214 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:32.305222 25348 sgd_solver.cpp:106] Iteration 52700, lr = 0.00176562
I0704 08:03:34.391844 25348 solver.cpp:290] Iteration 52800 (47.9257 iter/s, 2.08657s/100 iter), loss = -2.5332e-07
I0704 08:03:34.391865 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:34.391872 25348 sgd_solver.cpp:106] Iteration 52800, lr = 0.00175
I0704 08:03:36.463367 25348 solver.cpp:290] Iteration 52900 (48.2757 iter/s, 2.07144s/100 iter), loss = -2.5332e-07
I0704 08:03:36.463389 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:36.463397 25348 sgd_solver.cpp:106] Iteration 52900, lr = 0.00173437
I0704 08:03:38.513762 25348 solver.cpp:354] Sparsity after update:
I0704 08:03:38.515188 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:03:38.515195 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:03:38.515203 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:03:38.515206 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:03:38.515209 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:03:38.515213 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:03:38.515215 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:03:38.515218 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:03:38.515220 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:03:38.515223 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:03:38.515225 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:03:38.515228 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:03:38.515229 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:03:38.515316 25348 solver.cpp:466] Iteration 53000, Testing net (#0)
I0704 08:03:40.156697 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9098
I0704 08:03:40.156715 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9971
I0704 08:03:40.156720 25348 solver.cpp:539]     Test net output #2: loss = 0.1755 (* 1 = 0.1755 loss)
I0704 08:03:40.176373 25348 solver.cpp:290] Iteration 53000 (26.9333 iter/s, 3.71288s/100 iter), loss = -2.5332e-07
I0704 08:03:40.176388 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:40.176403 25348 sgd_solver.cpp:106] Iteration 53000, lr = 0.00171875
I0704 08:03:42.249794 25348 solver.cpp:290] Iteration 53100 (48.2314 iter/s, 2.07334s/100 iter), loss = -2.5332e-07
I0704 08:03:42.249821 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:42.249830 25348 sgd_solver.cpp:106] Iteration 53100, lr = 0.00170313
I0704 08:03:44.319524 25348 solver.cpp:290] Iteration 53200 (48.3176 iter/s, 2.06964s/100 iter), loss = -2.5332e-07
I0704 08:03:44.319545 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:44.319552 25348 sgd_solver.cpp:106] Iteration 53200, lr = 0.0016875
I0704 08:03:46.391937 25348 solver.cpp:290] Iteration 53300 (48.255 iter/s, 2.07233s/100 iter), loss = -2.5332e-07
I0704 08:03:46.391959 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:46.391966 25348 sgd_solver.cpp:106] Iteration 53300, lr = 0.00167188
I0704 08:03:48.468461 25348 solver.cpp:290] Iteration 53400 (48.1594 iter/s, 2.07644s/100 iter), loss = -2.5332e-07
I0704 08:03:48.468483 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:48.468490 25348 sgd_solver.cpp:106] Iteration 53400, lr = 0.00165625
I0704 08:03:50.539525 25348 solver.cpp:290] Iteration 53500 (48.2864 iter/s, 2.07098s/100 iter), loss = -2.5332e-07
I0704 08:03:50.539548 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:50.539556 25348 sgd_solver.cpp:106] Iteration 53500, lr = 0.00164062
I0704 08:03:52.615279 25348 solver.cpp:290] Iteration 53600 (48.1772 iter/s, 2.07567s/100 iter), loss = -2.5332e-07
I0704 08:03:52.615303 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:52.615309 25348 sgd_solver.cpp:106] Iteration 53600, lr = 0.001625
I0704 08:03:54.684483 25348 solver.cpp:290] Iteration 53700 (48.3298 iter/s, 2.06912s/100 iter), loss = -2.5332e-07
I0704 08:03:54.684505 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:54.684512 25348 sgd_solver.cpp:106] Iteration 53700, lr = 0.00160937
I0704 08:03:56.757211 25348 solver.cpp:290] Iteration 53800 (48.2476 iter/s, 2.07264s/100 iter), loss = -2.5332e-07
I0704 08:03:56.757290 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:56.757298 25348 sgd_solver.cpp:106] Iteration 53800, lr = 0.00159375
I0704 08:03:58.828912 25348 solver.cpp:290] Iteration 53900 (48.2728 iter/s, 2.07156s/100 iter), loss = -2.5332e-07
I0704 08:03:58.828935 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:03:58.828943 25348 sgd_solver.cpp:106] Iteration 53900, lr = 0.00157812
I0704 08:04:00.880380 25348 solver.cpp:354] Sparsity after update:
I0704 08:04:00.881773 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:04:00.881780 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:04:00.881790 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:04:00.881794 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:04:00.881799 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:04:00.881804 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:04:00.881808 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:04:00.881813 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:04:00.881816 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:04:00.881821 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:04:00.881825 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:04:00.881830 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:04:00.881834 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:04:00.881927 25348 solver.cpp:466] Iteration 54000, Testing net (#0)
I0704 08:04:02.522730 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.911
I0704 08:04:02.522749 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9959
I0704 08:04:02.522754 25348 solver.cpp:539]     Test net output #2: loss = 0.1867 (* 1 = 0.1867 loss)
I0704 08:04:02.542512 25348 solver.cpp:290] Iteration 54000 (26.929 iter/s, 3.71347s/100 iter), loss = -2.5332e-07
I0704 08:04:02.542529 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:02.542542 25348 sgd_solver.cpp:106] Iteration 54000, lr = 0.0015625
I0704 08:04:04.620020 25348 solver.cpp:290] Iteration 54100 (48.1365 iter/s, 2.07743s/100 iter), loss = -2.5332e-07
I0704 08:04:04.620043 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:04.620049 25348 sgd_solver.cpp:106] Iteration 54100, lr = 0.00154688
I0704 08:04:06.694241 25348 solver.cpp:290] Iteration 54200 (48.2129 iter/s, 2.07413s/100 iter), loss = -2.5332e-07
I0704 08:04:06.694272 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:06.694281 25348 sgd_solver.cpp:106] Iteration 54200, lr = 0.00153125
I0704 08:04:08.769978 25348 solver.cpp:290] Iteration 54300 (48.1779 iter/s, 2.07564s/100 iter), loss = -2.5332e-07
I0704 08:04:08.770006 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:08.770016 25348 sgd_solver.cpp:106] Iteration 54300, lr = 0.00151563
I0704 08:04:10.839845 25348 solver.cpp:290] Iteration 54400 (48.3144 iter/s, 2.06978s/100 iter), loss = -2.5332e-07
I0704 08:04:10.839874 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:10.839881 25348 sgd_solver.cpp:106] Iteration 54400, lr = 0.0015
I0704 08:04:12.912994 25348 solver.cpp:290] Iteration 54500 (48.2379 iter/s, 2.07306s/100 iter), loss = -2.5332e-07
I0704 08:04:12.913017 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:12.913024 25348 sgd_solver.cpp:106] Iteration 54500, lr = 0.00148437
I0704 08:04:14.986187 25348 solver.cpp:290] Iteration 54600 (48.2368 iter/s, 2.07311s/100 iter), loss = -2.5332e-07
I0704 08:04:14.986212 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:14.986222 25348 sgd_solver.cpp:106] Iteration 54600, lr = 0.00146875
I0704 08:04:17.056661 25348 solver.cpp:290] Iteration 54700 (48.3002 iter/s, 2.07039s/100 iter), loss = -2.5332e-07
I0704 08:04:17.056685 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:17.056694 25348 sgd_solver.cpp:106] Iteration 54700, lr = 0.00145312
I0704 08:04:19.131032 25348 solver.cpp:290] Iteration 54800 (48.2094 iter/s, 2.07428s/100 iter), loss = -2.5332e-07
I0704 08:04:19.131057 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:19.131065 25348 sgd_solver.cpp:106] Iteration 54800, lr = 0.0014375
I0704 08:04:21.201319 25348 solver.cpp:290] Iteration 54900 (48.3045 iter/s, 2.0702s/100 iter), loss = -2.5332e-07
I0704 08:04:21.201342 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:21.201350 25348 sgd_solver.cpp:106] Iteration 54900, lr = 0.00142187
I0704 08:04:23.255657 25348 solver.cpp:354] Sparsity after update:
I0704 08:04:23.257051 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:04:23.257058 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:04:23.257068 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:04:23.257073 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:04:23.257078 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:04:23.257082 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:04:23.257086 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:04:23.257089 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:04:23.257094 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:04:23.257098 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:04:23.257102 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:04:23.257105 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:04:23.257110 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:04:23.257201 25348 solver.cpp:466] Iteration 55000, Testing net (#0)
I0704 08:04:24.899339 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9125
I0704 08:04:24.899358 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9966
I0704 08:04:24.899364 25348 solver.cpp:539]     Test net output #2: loss = 0.1754 (* 1 = 0.1754 loss)
I0704 08:04:24.919103 25348 solver.cpp:290] Iteration 55000 (26.8987 iter/s, 3.71765s/100 iter), loss = -2.5332e-07
I0704 08:04:24.919123 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:24.919134 25348 sgd_solver.cpp:106] Iteration 55000, lr = 0.00140625
I0704 08:04:26.997964 25348 solver.cpp:290] Iteration 55100 (48.1052 iter/s, 2.07878s/100 iter), loss = -2.5332e-07
I0704 08:04:26.998051 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:26.998061 25348 sgd_solver.cpp:106] Iteration 55100, lr = 0.00139063
I0704 08:04:29.068749 25348 solver.cpp:290] Iteration 55200 (48.2943 iter/s, 2.07064s/100 iter), loss = -2.5332e-07
I0704 08:04:29.068773 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:29.068779 25348 sgd_solver.cpp:106] Iteration 55200, lr = 0.001375
I0704 08:04:31.139755 25348 solver.cpp:290] Iteration 55300 (48.2878 iter/s, 2.07092s/100 iter), loss = -2.5332e-07
I0704 08:04:31.139778 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:31.139785 25348 sgd_solver.cpp:106] Iteration 55300, lr = 0.00135938
I0704 08:04:33.217241 25348 solver.cpp:290] Iteration 55400 (48.1371 iter/s, 2.0774s/100 iter), loss = -2.5332e-07
I0704 08:04:33.217265 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:33.217274 25348 sgd_solver.cpp:106] Iteration 55400, lr = 0.00134375
I0704 08:04:35.330760 25348 solver.cpp:290] Iteration 55500 (47.3164 iter/s, 2.11343s/100 iter), loss = -2.5332e-07
I0704 08:04:35.330780 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:35.330790 25348 sgd_solver.cpp:106] Iteration 55500, lr = 0.00132813
I0704 08:04:37.400964 25348 solver.cpp:290] Iteration 55600 (48.3064 iter/s, 2.07012s/100 iter), loss = -2.5332e-07
I0704 08:04:37.400986 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:37.400992 25348 sgd_solver.cpp:106] Iteration 55600, lr = 0.0013125
I0704 08:04:39.473687 25348 solver.cpp:290] Iteration 55700 (48.2478 iter/s, 2.07264s/100 iter), loss = -2.5332e-07
I0704 08:04:39.473711 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:39.473721 25348 sgd_solver.cpp:106] Iteration 55700, lr = 0.00129687
I0704 08:04:41.549100 25348 solver.cpp:290] Iteration 55800 (48.1852 iter/s, 2.07533s/100 iter), loss = -2.5332e-07
I0704 08:04:41.549124 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:41.549129 25348 sgd_solver.cpp:106] Iteration 55800, lr = 0.00128125
I0704 08:04:43.622618 25348 solver.cpp:290] Iteration 55900 (48.2292 iter/s, 2.07343s/100 iter), loss = -2.5332e-07
I0704 08:04:43.622640 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:43.622648 25348 sgd_solver.cpp:106] Iteration 55900, lr = 0.00126562
I0704 08:04:45.677562 25348 solver.cpp:354] Sparsity after update:
I0704 08:04:45.678825 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:04:45.678833 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:04:45.678839 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:04:45.678843 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:04:45.678844 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:04:45.678846 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:04:45.678848 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:04:45.678850 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:04:45.678853 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:04:45.678854 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:04:45.678856 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:04:45.678858 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:04:45.678860 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:04:45.678946 25348 solver.cpp:466] Iteration 56000, Testing net (#0)
I0704 08:04:47.320372 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9119
I0704 08:04:47.320392 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9968
I0704 08:04:47.320397 25348 solver.cpp:539]     Test net output #2: loss = 0.1705 (* 1 = 0.1705 loss)
I0704 08:04:47.340554 25348 solver.cpp:290] Iteration 56000 (26.8976 iter/s, 3.71781s/100 iter), loss = -2.5332e-07
I0704 08:04:47.340572 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:47.340584 25348 sgd_solver.cpp:106] Iteration 56000, lr = 0.00125
I0704 08:04:49.414075 25348 solver.cpp:290] Iteration 56100 (48.229 iter/s, 2.07344s/100 iter), loss = -2.5332e-07
I0704 08:04:49.414098 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:49.414104 25348 sgd_solver.cpp:106] Iteration 56100, lr = 0.00123438
I0704 08:04:51.487831 25348 solver.cpp:290] Iteration 56200 (48.2237 iter/s, 2.07367s/100 iter), loss = -2.5332e-07
I0704 08:04:51.487857 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:51.487864 25348 sgd_solver.cpp:106] Iteration 56200, lr = 0.00121875
I0704 08:04:53.559255 25348 solver.cpp:290] Iteration 56300 (48.2781 iter/s, 2.07133s/100 iter), loss = -2.5332e-07
I0704 08:04:53.559283 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:53.559293 25348 sgd_solver.cpp:106] Iteration 56300, lr = 0.00120313
I0704 08:04:55.631796 25348 solver.cpp:290] Iteration 56400 (48.2521 iter/s, 2.07245s/100 iter), loss = -2.5332e-07
I0704 08:04:55.631820 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:55.631829 25348 sgd_solver.cpp:106] Iteration 56400, lr = 0.0011875
I0704 08:04:57.702633 25348 solver.cpp:290] Iteration 56500 (48.2917 iter/s, 2.07075s/100 iter), loss = -2.5332e-07
I0704 08:04:57.702713 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:57.702724 25348 sgd_solver.cpp:106] Iteration 56500, lr = 0.00117187
I0704 08:04:59.777993 25348 solver.cpp:290] Iteration 56600 (48.1877 iter/s, 2.07522s/100 iter), loss = -2.5332e-07
I0704 08:04:59.778015 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:04:59.778023 25348 sgd_solver.cpp:106] Iteration 56600, lr = 0.00115625
I0704 08:05:01.846787 25348 solver.cpp:290] Iteration 56700 (48.3393 iter/s, 2.06871s/100 iter), loss = -2.5332e-07
I0704 08:05:01.846812 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:01.846817 25348 sgd_solver.cpp:106] Iteration 56700, lr = 0.00114062
I0704 08:05:03.918018 25348 solver.cpp:290] Iteration 56800 (48.2825 iter/s, 2.07114s/100 iter), loss = -2.5332e-07
I0704 08:05:03.918040 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:03.918046 25348 sgd_solver.cpp:106] Iteration 56800, lr = 0.001125
I0704 08:05:05.988574 25348 solver.cpp:290] Iteration 56900 (48.2982 iter/s, 2.07047s/100 iter), loss = -2.5332e-07
I0704 08:05:05.988596 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:05.988603 25348 sgd_solver.cpp:106] Iteration 56900, lr = 0.00110937
I0704 08:05:08.044821 25348 solver.cpp:354] Sparsity after update:
I0704 08:05:08.046197 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:05:08.046205 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:05:08.046214 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:05:08.046219 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:05:08.046224 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:05:08.046228 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:05:08.046233 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:05:08.046238 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:05:08.046242 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:05:08.046247 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:05:08.046252 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:05:08.046255 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:05:08.046259 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:05:08.046351 25348 solver.cpp:466] Iteration 57000, Testing net (#0)
I0704 08:05:09.689340 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9119
I0704 08:05:09.689359 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9961
I0704 08:05:09.689365 25348 solver.cpp:539]     Test net output #2: loss = 0.1757 (* 1 = 0.1757 loss)
I0704 08:05:09.709202 25348 solver.cpp:290] Iteration 57000 (26.8781 iter/s, 3.7205s/100 iter), loss = -2.5332e-07
I0704 08:05:09.709218 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:09.709233 25348 sgd_solver.cpp:106] Iteration 57000, lr = 0.00109375
I0704 08:05:11.779510 25348 solver.cpp:290] Iteration 57100 (48.3039 iter/s, 2.07023s/100 iter), loss = -2.5332e-07
I0704 08:05:11.779533 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:11.779541 25348 sgd_solver.cpp:106] Iteration 57100, lr = 0.00107813
I0704 08:05:13.853374 25348 solver.cpp:290] Iteration 57200 (48.2211 iter/s, 2.07378s/100 iter), loss = -2.5332e-07
I0704 08:05:13.853397 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:13.853405 25348 sgd_solver.cpp:106] Iteration 57200, lr = 0.0010625
I0704 08:05:15.922807 25348 solver.cpp:290] Iteration 57300 (48.3245 iter/s, 2.06935s/100 iter), loss = -2.5332e-07
I0704 08:05:15.922829 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:15.922837 25348 sgd_solver.cpp:106] Iteration 57300, lr = 0.00104688
I0704 08:05:17.994293 25348 solver.cpp:290] Iteration 57400 (48.2765 iter/s, 2.0714s/100 iter), loss = -2.5332e-07
I0704 08:05:17.994315 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:17.994323 25348 sgd_solver.cpp:106] Iteration 57400, lr = 0.00103125
I0704 08:05:20.065922 25348 solver.cpp:290] Iteration 57500 (48.2732 iter/s, 2.07154s/100 iter), loss = -2.5332e-07
I0704 08:05:20.065943 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:20.065950 25348 sgd_solver.cpp:106] Iteration 57500, lr = 0.00101562
I0704 08:05:22.142550 25348 solver.cpp:290] Iteration 57600 (48.157 iter/s, 2.07654s/100 iter), loss = -2.5332e-07
I0704 08:05:22.142573 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:22.142580 25348 sgd_solver.cpp:106] Iteration 57600, lr = 0.001
I0704 08:05:24.218351 25348 solver.cpp:290] Iteration 57700 (48.1761 iter/s, 2.07572s/100 iter), loss = -2.5332e-07
I0704 08:05:24.218375 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:24.218381 25348 sgd_solver.cpp:106] Iteration 57700, lr = 0.000984375
I0704 08:05:26.287920 25348 solver.cpp:290] Iteration 57800 (48.3213 iter/s, 2.06948s/100 iter), loss = -2.5332e-07
I0704 08:05:26.287943 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:26.287952 25348 sgd_solver.cpp:106] Iteration 57800, lr = 0.00096875
I0704 08:05:28.367661 25348 solver.cpp:290] Iteration 57900 (48.085 iter/s, 2.07965s/100 iter), loss = -2.5332e-07
I0704 08:05:28.367770 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:28.367792 25348 sgd_solver.cpp:106] Iteration 57900, lr = 0.000953125
I0704 08:05:30.417819 25348 solver.cpp:354] Sparsity after update:
I0704 08:05:30.419212 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:05:30.419219 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:05:30.419226 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:05:30.419229 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:05:30.419230 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:05:30.419232 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:05:30.419234 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:05:30.419236 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:05:30.419239 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:05:30.419240 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:05:30.419242 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:05:30.419245 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:05:30.419246 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:05:30.419333 25348 solver.cpp:466] Iteration 58000, Testing net (#0)
I0704 08:05:32.059862 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9129
I0704 08:05:32.059883 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9961
I0704 08:05:32.059888 25348 solver.cpp:539]     Test net output #2: loss = 0.1754 (* 1 = 0.1754 loss)
I0704 08:05:32.081885 25348 solver.cpp:290] Iteration 58000 (26.9251 iter/s, 3.71401s/100 iter), loss = -2.5332e-07
I0704 08:05:32.081914 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:32.081924 25348 sgd_solver.cpp:106] Iteration 58000, lr = 0.0009375
I0704 08:05:34.196832 25348 solver.cpp:290] Iteration 58100 (47.2846 iter/s, 2.11485s/100 iter), loss = -2.5332e-07
I0704 08:05:34.196861 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:34.196869 25348 sgd_solver.cpp:106] Iteration 58100, lr = 0.000921875
I0704 08:05:36.265882 25348 solver.cpp:290] Iteration 58200 (48.3335 iter/s, 2.06896s/100 iter), loss = -2.5332e-07
I0704 08:05:36.265908 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:36.265918 25348 sgd_solver.cpp:106] Iteration 58200, lr = 0.00090625
I0704 08:05:38.338917 25348 solver.cpp:290] Iteration 58300 (48.2405 iter/s, 2.07295s/100 iter), loss = -2.5332e-07
I0704 08:05:38.338940 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:38.338946 25348 sgd_solver.cpp:106] Iteration 58300, lr = 0.000890625
I0704 08:05:40.411526 25348 solver.cpp:290] Iteration 58400 (48.2504 iter/s, 2.07252s/100 iter), loss = -2.5332e-07
I0704 08:05:40.411547 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:40.411556 25348 sgd_solver.cpp:106] Iteration 58400, lr = 0.000875
I0704 08:05:42.481514 25348 solver.cpp:290] Iteration 58500 (48.3114 iter/s, 2.0699s/100 iter), loss = -2.5332e-07
I0704 08:05:42.481537 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:42.481544 25348 sgd_solver.cpp:106] Iteration 58500, lr = 0.000859375
I0704 08:05:44.555968 25348 solver.cpp:290] Iteration 58600 (48.2075 iter/s, 2.07436s/100 iter), loss = -2.5332e-07
I0704 08:05:44.555994 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:44.556004 25348 sgd_solver.cpp:106] Iteration 58600, lr = 0.00084375
I0704 08:05:46.629209 25348 solver.cpp:290] Iteration 58700 (48.2357 iter/s, 2.07315s/100 iter), loss = -2.5332e-07
I0704 08:05:46.629232 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:46.629238 25348 sgd_solver.cpp:106] Iteration 58700, lr = 0.000828125
I0704 08:05:48.698076 25348 solver.cpp:290] Iteration 58800 (48.3377 iter/s, 2.06878s/100 iter), loss = -2.5332e-07
I0704 08:05:48.698097 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:48.698106 25348 sgd_solver.cpp:106] Iteration 58800, lr = 0.0008125
I0704 08:05:50.767051 25348 solver.cpp:290] Iteration 58900 (48.3351 iter/s, 2.06889s/100 iter), loss = -2.5332e-07
I0704 08:05:50.767074 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:50.767081 25348 sgd_solver.cpp:106] Iteration 58900, lr = 0.000796875
I0704 08:05:52.817042 25348 solver.cpp:354] Sparsity after update:
I0704 08:05:52.818452 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:05:52.818460 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:05:52.818470 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:05:52.818475 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:05:52.818480 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:05:52.818485 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:05:52.818488 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:05:52.818492 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:05:52.818497 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:05:52.818501 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:05:52.818506 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:05:52.818511 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:05:52.818514 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:05:52.818606 25348 solver.cpp:466] Iteration 59000, Testing net (#0)
I0704 08:05:54.460489 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9146
I0704 08:05:54.460507 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.997
I0704 08:05:54.460512 25348 solver.cpp:539]     Test net output #2: loss = 0.1667 (* 1 = 0.1667 loss)
I0704 08:05:54.480088 25348 solver.cpp:290] Iteration 59000 (26.9331 iter/s, 3.71291s/100 iter), loss = -2.5332e-07
I0704 08:05:54.480105 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:54.480119 25348 sgd_solver.cpp:106] Iteration 59000, lr = 0.00078125
I0704 08:05:56.551357 25348 solver.cpp:290] Iteration 59100 (48.2815 iter/s, 2.07119s/100 iter), loss = -2.5332e-07
I0704 08:05:56.551383 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:56.551393 25348 sgd_solver.cpp:106] Iteration 59100, lr = 0.000765625
I0704 08:05:58.622308 25348 solver.cpp:290] Iteration 59200 (48.2891 iter/s, 2.07086s/100 iter), loss = -2.5332e-07
I0704 08:05:58.622376 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:05:58.622391 25348 sgd_solver.cpp:106] Iteration 59200, lr = 0.00075
I0704 08:06:00.697468 25348 solver.cpp:290] Iteration 59300 (48.1921 iter/s, 2.07503s/100 iter), loss = -2.5332e-07
I0704 08:06:00.697490 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:00.697496 25348 sgd_solver.cpp:106] Iteration 59300, lr = 0.000734375
I0704 08:06:02.769433 25348 solver.cpp:290] Iteration 59400 (48.2654 iter/s, 2.07188s/100 iter), loss = -2.5332e-07
I0704 08:06:02.769456 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:02.769462 25348 sgd_solver.cpp:106] Iteration 59400, lr = 0.00071875
I0704 08:06:04.843685 25348 solver.cpp:290] Iteration 59500 (48.2122 iter/s, 2.07417s/100 iter), loss = -2.5332e-07
I0704 08:06:04.843708 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:04.843715 25348 sgd_solver.cpp:106] Iteration 59500, lr = 0.000703125
I0704 08:06:06.920531 25348 solver.cpp:290] Iteration 59600 (48.152 iter/s, 2.07676s/100 iter), loss = -2.5332e-07
I0704 08:06:06.920553 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:06.920559 25348 sgd_solver.cpp:106] Iteration 59600, lr = 0.0006875
I0704 08:06:08.994865 25348 solver.cpp:290] Iteration 59700 (48.2103 iter/s, 2.07425s/100 iter), loss = -2.5332e-07
I0704 08:06:08.994887 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:08.994894 25348 sgd_solver.cpp:106] Iteration 59700, lr = 0.000671875
I0704 08:06:11.065526 25348 solver.cpp:290] Iteration 59800 (48.2958 iter/s, 2.07057s/100 iter), loss = -2.5332e-07
I0704 08:06:11.065548 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:11.065556 25348 sgd_solver.cpp:106] Iteration 59800, lr = 0.00065625
I0704 08:06:13.139819 25348 solver.cpp:290] Iteration 59900 (48.2112 iter/s, 2.07421s/100 iter), loss = -2.5332e-07
I0704 08:06:13.139842 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:13.139848 25348 sgd_solver.cpp:106] Iteration 59900, lr = 0.000640625
I0704 08:06:15.195750 25348 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_60000.caffemodel
I0704 08:06:15.212489 25348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_60000.solverstate
I0704 08:06:15.219972 25348 solver.cpp:354] Sparsity after update:
I0704 08:06:15.220912 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:06:15.220919 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:06:15.220927 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:06:15.220929 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:06:15.220932 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:06:15.220933 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:06:15.220935 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:06:15.220937 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:06:15.220939 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:06:15.220942 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:06:15.220943 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:06:15.220945 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:06:15.220947 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:06:15.221040 25348 solver.cpp:466] Iteration 60000, Testing net (#0)
I0704 08:06:16.864121 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.915
I0704 08:06:16.864141 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9972
I0704 08:06:16.864147 25348 solver.cpp:539]     Test net output #2: loss = 0.1674 (* 1 = 0.1674 loss)
I0704 08:06:16.883782 25348 solver.cpp:290] Iteration 60000 (26.7106 iter/s, 3.74383s/100 iter), loss = -2.5332e-07
I0704 08:06:16.883807 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:16.883824 25348 sgd_solver.cpp:106] Iteration 60000, lr = 0.000625
I0704 08:06:18.954923 25348 solver.cpp:290] Iteration 60100 (48.2846 iter/s, 2.07105s/100 iter), loss = -2.5332e-07
I0704 08:06:18.954949 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:18.954957 25348 sgd_solver.cpp:106] Iteration 60100, lr = 0.000609375
I0704 08:06:21.025010 25348 solver.cpp:290] Iteration 60200 (48.3093 iter/s, 2.07s/100 iter), loss = -2.5332e-07
I0704 08:06:21.025035 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:21.025041 25348 sgd_solver.cpp:106] Iteration 60200, lr = 0.00059375
I0704 08:06:23.098119 25348 solver.cpp:290] Iteration 60300 (48.2388 iter/s, 2.07302s/100 iter), loss = -2.5332e-07
I0704 08:06:23.098143 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:23.098151 25348 sgd_solver.cpp:106] Iteration 60300, lr = 0.000578125
I0704 08:06:25.171129 25348 solver.cpp:290] Iteration 60400 (48.2411 iter/s, 2.07292s/100 iter), loss = -2.5332e-07
I0704 08:06:25.171154 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:25.171162 25348 sgd_solver.cpp:106] Iteration 60400, lr = 0.0005625
I0704 08:06:27.253141 25348 solver.cpp:290] Iteration 60500 (48.0325 iter/s, 2.08192s/100 iter), loss = -2.5332e-07
I0704 08:06:27.253163 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:27.253171 25348 sgd_solver.cpp:106] Iteration 60500, lr = 0.000546875
I0704 08:06:29.327792 25348 solver.cpp:290] Iteration 60600 (48.2029 iter/s, 2.07456s/100 iter), loss = -2.5332e-07
I0704 08:06:29.327867 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:29.327875 25348 sgd_solver.cpp:106] Iteration 60600, lr = 0.00053125
I0704 08:06:31.399304 25348 solver.cpp:290] Iteration 60700 (48.2771 iter/s, 2.07137s/100 iter), loss = -2.5332e-07
I0704 08:06:31.399327 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:31.399333 25348 sgd_solver.cpp:106] Iteration 60700, lr = 0.000515625
I0704 08:06:33.474877 25348 solver.cpp:290] Iteration 60800 (48.1815 iter/s, 2.07549s/100 iter), loss = -2.5332e-07
I0704 08:06:33.474907 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:33.474917 25348 sgd_solver.cpp:106] Iteration 60800, lr = 0.0005
I0704 08:06:35.552253 25348 solver.cpp:290] Iteration 60900 (48.1398 iter/s, 2.07728s/100 iter), loss = -2.5332e-07
I0704 08:06:35.552274 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:35.552283 25348 sgd_solver.cpp:106] Iteration 60900, lr = 0.000484375
I0704 08:06:37.605747 25348 solver.cpp:354] Sparsity after update:
I0704 08:06:37.607130 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:06:37.607136 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:06:37.607143 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:06:37.607146 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:06:37.607148 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:06:37.607151 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:06:37.607152 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:06:37.607154 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:06:37.607156 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:06:37.607158 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:06:37.607161 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:06:37.607162 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:06:37.607164 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:06:37.607252 25348 solver.cpp:466] Iteration 61000, Testing net (#0)
I0704 08:06:39.251441 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.914
I0704 08:06:39.251459 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9966
I0704 08:06:39.251466 25348 solver.cpp:539]     Test net output #2: loss = 0.1693 (* 1 = 0.1693 loss)
I0704 08:06:39.271239 25348 solver.cpp:290] Iteration 61000 (26.89 iter/s, 3.71886s/100 iter), loss = -2.5332e-07
I0704 08:06:39.271258 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:39.271267 25348 sgd_solver.cpp:106] Iteration 61000, lr = 0.00046875
I0704 08:06:41.343469 25348 solver.cpp:290] Iteration 61100 (48.2591 iter/s, 2.07215s/100 iter), loss = -2.5332e-07
I0704 08:06:41.343492 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:41.343500 25348 sgd_solver.cpp:106] Iteration 61100, lr = 0.000453125
I0704 08:06:43.418529 25348 solver.cpp:290] Iteration 61200 (48.1934 iter/s, 2.07497s/100 iter), loss = -2.5332e-07
I0704 08:06:43.418550 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:43.418556 25348 sgd_solver.cpp:106] Iteration 61200, lr = 0.0004375
I0704 08:06:45.491545 25348 solver.cpp:290] Iteration 61300 (48.241 iter/s, 2.07293s/100 iter), loss = -2.5332e-07
I0704 08:06:45.491569 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:45.491577 25348 sgd_solver.cpp:106] Iteration 61300, lr = 0.000421875
I0704 08:06:47.566174 25348 solver.cpp:290] Iteration 61400 (48.2034 iter/s, 2.07454s/100 iter), loss = -2.5332e-07
I0704 08:06:47.566196 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:47.566205 25348 sgd_solver.cpp:106] Iteration 61400, lr = 0.00040625
I0704 08:06:49.637663 25348 solver.cpp:290] Iteration 61500 (48.2765 iter/s, 2.0714s/100 iter), loss = -2.5332e-07
I0704 08:06:49.637686 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:49.637693 25348 sgd_solver.cpp:106] Iteration 61500, lr = 0.000390625
I0704 08:06:51.708540 25348 solver.cpp:290] Iteration 61600 (48.2907 iter/s, 2.07079s/100 iter), loss = -2.5332e-07
I0704 08:06:51.708562 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:51.708570 25348 sgd_solver.cpp:106] Iteration 61600, lr = 0.000375
I0704 08:06:53.784720 25348 solver.cpp:290] Iteration 61700 (48.1674 iter/s, 2.07609s/100 iter), loss = -2.5332e-07
I0704 08:06:53.784744 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:53.784750 25348 sgd_solver.cpp:106] Iteration 61700, lr = 0.000359375
I0704 08:06:55.858744 25348 solver.cpp:290] Iteration 61800 (48.2175 iter/s, 2.07394s/100 iter), loss = -2.5332e-07
I0704 08:06:55.858767 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:55.858773 25348 sgd_solver.cpp:106] Iteration 61800, lr = 0.00034375
I0704 08:06:57.931777 25348 solver.cpp:290] Iteration 61900 (48.2405 iter/s, 2.07295s/100 iter), loss = -2.5332e-07
I0704 08:06:57.931798 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:06:57.931807 25348 sgd_solver.cpp:106] Iteration 61900, lr = 0.000328125
I0704 08:06:59.986757 25348 solver.cpp:354] Sparsity after update:
I0704 08:06:59.988160 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:06:59.988168 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:06:59.988174 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:06:59.988178 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:06:59.988179 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:06:59.988181 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:06:59.988183 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:06:59.988185 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:06:59.988188 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:06:59.988189 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:06:59.988191 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:06:59.988193 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:06:59.988194 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:06:59.988278 25348 solver.cpp:466] Iteration 62000, Testing net (#0)
I0704 08:07:01.628139 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9157
I0704 08:07:01.628156 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9965
I0704 08:07:01.628161 25348 solver.cpp:539]     Test net output #2: loss = 0.1676 (* 1 = 0.1676 loss)
I0704 08:07:01.648098 25348 solver.cpp:290] Iteration 62000 (26.9093 iter/s, 3.71619s/100 iter), loss = -2.5332e-07
I0704 08:07:01.648123 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:01.648129 25348 sgd_solver.cpp:106] Iteration 62000, lr = 0.0003125
I0704 08:07:03.725587 25348 solver.cpp:290] Iteration 62100 (48.137 iter/s, 2.0774s/100 iter), loss = -2.5332e-07
I0704 08:07:03.725608 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:03.725615 25348 sgd_solver.cpp:106] Iteration 62100, lr = 0.000296875
I0704 08:07:05.800490 25348 solver.cpp:290] Iteration 62200 (48.197 iter/s, 2.07482s/100 iter), loss = -2.5332e-07
I0704 08:07:05.800513 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:05.800520 25348 sgd_solver.cpp:106] Iteration 62200, lr = 0.00028125
I0704 08:07:07.875686 25348 solver.cpp:290] Iteration 62300 (48.1902 iter/s, 2.07511s/100 iter), loss = -2.5332e-07
I0704 08:07:07.875710 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:07.875716 25348 sgd_solver.cpp:106] Iteration 62300, lr = 0.000265625
I0704 08:07:09.947206 25348 solver.cpp:290] Iteration 62400 (48.2758 iter/s, 2.07143s/100 iter), loss = -2.5332e-07
I0704 08:07:09.947229 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:09.947235 25348 sgd_solver.cpp:106] Iteration 62400, lr = 0.00025
I0704 08:07:12.016764 25348 solver.cpp:290] Iteration 62500 (48.3215 iter/s, 2.06947s/100 iter), loss = -2.5332e-07
I0704 08:07:12.016786 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:12.016794 25348 sgd_solver.cpp:106] Iteration 62500, lr = 0.000234375
I0704 08:07:14.091687 25348 solver.cpp:290] Iteration 62600 (48.1966 iter/s, 2.07484s/100 iter), loss = -2.5332e-07
I0704 08:07:14.091709 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:14.091717 25348 sgd_solver.cpp:106] Iteration 62600, lr = 0.00021875
I0704 08:07:16.161609 25348 solver.cpp:290] Iteration 62700 (48.313 iter/s, 2.06983s/100 iter), loss = -2.5332e-07
I0704 08:07:16.161634 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:16.161643 25348 sgd_solver.cpp:106] Iteration 62700, lr = 0.000203125
I0704 08:07:18.233100 25348 solver.cpp:290] Iteration 62800 (48.2765 iter/s, 2.0714s/100 iter), loss = -2.5332e-07
I0704 08:07:18.233124 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:18.233129 25348 sgd_solver.cpp:106] Iteration 62800, lr = 0.0001875
I0704 08:07:20.301028 25348 solver.cpp:290] Iteration 62900 (48.3596 iter/s, 2.06784s/100 iter), loss = -2.5332e-07
I0704 08:07:20.301049 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:20.301075 25348 sgd_solver.cpp:106] Iteration 62900, lr = 0.000171875
I0704 08:07:22.350026 25348 solver.cpp:354] Sparsity after update:
I0704 08:07:22.351424 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:07:22.351431 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:07:22.351442 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:07:22.351446 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:07:22.351451 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:07:22.351455 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:07:22.351459 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:07:22.351464 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:07:22.351469 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:07:22.351472 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:07:22.351476 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:07:22.351481 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:07:22.351485 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:07:22.351577 25348 solver.cpp:466] Iteration 63000, Testing net (#0)
I0704 08:07:23.991703 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.914
I0704 08:07:23.991722 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9971
I0704 08:07:23.991727 25348 solver.cpp:539]     Test net output #2: loss = 0.1664 (* 1 = 0.1664 loss)
I0704 08:07:24.011265 25348 solver.cpp:290] Iteration 63000 (26.9534 iter/s, 3.71011s/100 iter), loss = -2.5332e-07
I0704 08:07:24.011282 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:24.011294 25348 sgd_solver.cpp:106] Iteration 63000, lr = 0.00015625
I0704 08:07:26.083037 25348 solver.cpp:290] Iteration 63100 (48.2697 iter/s, 2.07169s/100 iter), loss = -2.5332e-07
I0704 08:07:26.083060 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:26.083067 25348 sgd_solver.cpp:106] Iteration 63100, lr = 0.000140625
I0704 08:07:28.152573 25348 solver.cpp:290] Iteration 63200 (48.3221 iter/s, 2.06945s/100 iter), loss = -2.5332e-07
I0704 08:07:28.152600 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:28.152607 25348 sgd_solver.cpp:106] Iteration 63200, lr = 0.000125
I0704 08:07:30.224687 25348 solver.cpp:290] Iteration 63300 (48.262 iter/s, 2.07203s/100 iter), loss = -2.5332e-07
I0704 08:07:30.224763 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:30.224771 25348 sgd_solver.cpp:106] Iteration 63300, lr = 0.000109375
I0704 08:07:32.302664 25348 solver.cpp:290] Iteration 63400 (48.1269 iter/s, 2.07784s/100 iter), loss = -2.5332e-07
I0704 08:07:32.302687 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:32.302693 25348 sgd_solver.cpp:106] Iteration 63400, lr = 9.37498e-05
I0704 08:07:34.374064 25348 solver.cpp:290] Iteration 63500 (48.2785 iter/s, 2.07131s/100 iter), loss = -2.5332e-07
I0704 08:07:34.374086 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:34.374094 25348 sgd_solver.cpp:106] Iteration 63500, lr = 7.8125e-05
I0704 08:07:36.445134 25348 solver.cpp:290] Iteration 63600 (48.2863 iter/s, 2.07098s/100 iter), loss = -2.5332e-07
I0704 08:07:36.445163 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:36.445168 25348 sgd_solver.cpp:106] Iteration 63600, lr = 6.25002e-05
I0704 08:07:38.519754 25348 solver.cpp:290] Iteration 63700 (48.2038 iter/s, 2.07452s/100 iter), loss = -2.5332e-07
I0704 08:07:38.519778 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:38.519784 25348 sgd_solver.cpp:106] Iteration 63700, lr = 4.68749e-05
I0704 08:07:40.596948 25348 solver.cpp:290] Iteration 63800 (48.1439 iter/s, 2.07711s/100 iter), loss = -2.5332e-07
I0704 08:07:40.596971 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:40.596977 25348 sgd_solver.cpp:106] Iteration 63800, lr = 3.12501e-05
I0704 08:07:42.669821 25348 solver.cpp:290] Iteration 63900 (48.2442 iter/s, 2.07279s/100 iter), loss = -2.5332e-07
I0704 08:07:42.669842 25348 solver.cpp:309]     Train net output #0: loss = 0 (* 1 = 0 loss)
I0704 08:07:42.669849 25348 sgd_solver.cpp:106] Iteration 63900, lr = 1.56248e-05
I0704 08:07:44.722931 25348 solver.cpp:354] Sparsity after update:
I0704 08:07:44.724344 25348 net.cpp:1842] Num Params(11), Sparsity (zero_weights/count): 
I0704 08:07:44.724351 25348 net.cpp:1851] conv1a_param_0(0.4) 
I0704 08:07:44.724360 25348 net.cpp:1851] conv1b_param_0(0.8) 
I0704 08:07:44.724365 25348 net.cpp:1851] fc10_param_0(0) 
I0704 08:07:44.724370 25348 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0704 08:07:44.724375 25348 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0704 08:07:44.724380 25348 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0704 08:07:44.724383 25348 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0704 08:07:44.724388 25348 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0704 08:07:44.724392 25348 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0704 08:07:44.724397 25348 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0704 08:07:44.724400 25348 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0704 08:07:44.724406 25348 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.88286e+06/2.3599e+06) 0.798
I0704 08:07:44.724417 25348 solver.cpp:593] Snapshotting to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_64000.caffemodel
I0704 08:07:44.740816 25348 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cifar10_jacintonet11v2_2017-07-04_07-19-29/sparse/cifar10_jacintonet11v2_iter_64000.solverstate
I0704 08:07:44.752998 25348 solver.cpp:446] Iteration 64000, loss = -2.5332e-07
I0704 08:07:44.753016 25348 solver.cpp:466] Iteration 64000, Testing net (#0)
I0704 08:07:46.392413 25348 solver.cpp:539]     Test net output #0: accuracy/top1 = 0.9154
I0704 08:07:46.392434 25348 solver.cpp:539]     Test net output #1: accuracy/top5 = 0.9967
I0704 08:07:46.392439 25348 solver.cpp:539]     Test net output #2: loss = 0.1661 (* 1 = 0.1661 loss)
I0704 08:07:46.392442 25348 solver.cpp:451] Optimization Done.
I0704 08:07:46.440244 25348 caffe.cpp:246] Optimization Done.
training/cifar10_jacintonet11v2_2017-07-04_07-19-29/test
I0704 08:07:47.231745 27993 caffe.cpp:264] Not using GPU #2 for single-GPU function
I0704 08:07:47.231874 27993 caffe.cpp:264] Not using GPU #1 for single-GPU function
I0704 08:07:47.415587 27993 caffe.cpp:273] Use GPU with device ID 0
I0704 08:07:47.415944 27993 caffe.cpp:277] GPU device name: GeForce GTX 1080
I0704 08:07:47.808641 27993 net.cpp:56] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/cifar10_test_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b/bn"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
I0704 08:07:47.808768 27993 layer_factory.hpp:77] Creating layer data
I0704 08:07:47.809132 27993 net.cpp:98] Creating Layer data
I0704 08:07:47.809140 27993 net.cpp:413] data -> data
I0704 08:07:47.809157 27993 net.cpp:413] data -> label
I0704 08:07:47.809940 28013 db_lmdb.cpp:35] Opened lmdb ./data/cifar10_test_lmdb
I0704 08:07:47.810580 27993 data_layer.cpp:78] ReshapePrefetch 50, 3, 32, 32
I0704 08:07:47.810611 27993 data_layer.cpp:83] output data size: 50,3,32,32
I0704 08:07:47.812201 27993 net.cpp:148] Setting up data
I0704 08:07:47.812211 27993 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0704 08:07:47.812216 27993 net.cpp:155] Top shape: 50 (50)
I0704 08:07:47.812217 27993 net.cpp:163] Memory required for data: 614600
I0704 08:07:47.812222 27993 layer_factory.hpp:77] Creating layer label_data_1_split
I0704 08:07:47.812232 27993 net.cpp:98] Creating Layer label_data_1_split
I0704 08:07:47.812234 27993 net.cpp:439] label_data_1_split <- label
I0704 08:07:47.812242 27993 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0704 08:07:47.812245 27993 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0704 08:07:47.812248 27993 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0704 08:07:47.812327 27993 net.cpp:148] Setting up label_data_1_split
I0704 08:07:47.812337 27993 net.cpp:155] Top shape: 50 (50)
I0704 08:07:47.812340 27993 net.cpp:155] Top shape: 50 (50)
I0704 08:07:47.812342 27993 net.cpp:155] Top shape: 50 (50)
I0704 08:07:47.812345 27993 net.cpp:163] Memory required for data: 615200
I0704 08:07:47.812348 27993 layer_factory.hpp:77] Creating layer data/bias
I0704 08:07:47.812356 27993 net.cpp:98] Creating Layer data/bias
I0704 08:07:47.812360 27993 net.cpp:439] data/bias <- data
I0704 08:07:47.812362 27993 net.cpp:413] data/bias -> data/bias
I0704 08:07:47.812966 27993 net.cpp:148] Setting up data/bias
I0704 08:07:47.812974 27993 net.cpp:155] Top shape: 50 3 32 32 (153600)
I0704 08:07:47.812976 27993 net.cpp:163] Memory required for data: 1229600
I0704 08:07:47.812985 27993 layer_factory.hpp:77] Creating layer conv1a
I0704 08:07:47.812994 27993 net.cpp:98] Creating Layer conv1a
I0704 08:07:47.812996 27993 net.cpp:439] conv1a <- data/bias
I0704 08:07:47.812999 27993 net.cpp:413] conv1a -> conv1a
I0704 08:07:47.813902 27993 net.cpp:148] Setting up conv1a
I0704 08:07:47.813911 27993 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 08:07:47.813915 27993 net.cpp:163] Memory required for data: 7783200
I0704 08:07:47.813918 27993 layer_factory.hpp:77] Creating layer conv1a/bn
I0704 08:07:47.813925 27993 net.cpp:98] Creating Layer conv1a/bn
I0704 08:07:47.813926 27993 net.cpp:439] conv1a/bn <- conv1a
I0704 08:07:47.813930 27993 net.cpp:413] conv1a/bn -> conv1a/bn
I0704 08:07:47.814249 27993 net.cpp:148] Setting up conv1a/bn
I0704 08:07:47.814256 27993 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 08:07:47.814258 27993 net.cpp:163] Memory required for data: 14336800
I0704 08:07:47.814265 27993 layer_factory.hpp:77] Creating layer conv1a/relu
I0704 08:07:47.814267 27993 net.cpp:98] Creating Layer conv1a/relu
I0704 08:07:47.814270 27993 net.cpp:439] conv1a/relu <- conv1a/bn
I0704 08:07:47.814272 27993 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0704 08:07:47.814280 27993 net.cpp:148] Setting up conv1a/relu
I0704 08:07:47.814283 27993 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 08:07:47.814285 27993 net.cpp:163] Memory required for data: 20890400
I0704 08:07:47.814287 27993 layer_factory.hpp:77] Creating layer conv1b
I0704 08:07:47.814291 27993 net.cpp:98] Creating Layer conv1b
I0704 08:07:47.814307 27993 net.cpp:439] conv1b <- conv1a/bn
I0704 08:07:47.814311 27993 net.cpp:413] conv1b -> conv1b
I0704 08:07:47.814503 27993 net.cpp:148] Setting up conv1b
I0704 08:07:47.814509 27993 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 08:07:47.814512 27993 net.cpp:163] Memory required for data: 27444000
I0704 08:07:47.814515 27993 layer_factory.hpp:77] Creating layer conv1b/bn
I0704 08:07:47.814519 27993 net.cpp:98] Creating Layer conv1b/bn
I0704 08:07:47.814522 27993 net.cpp:439] conv1b/bn <- conv1b
I0704 08:07:47.814523 27993 net.cpp:413] conv1b/bn -> conv1b/bn
I0704 08:07:47.814816 27993 net.cpp:148] Setting up conv1b/bn
I0704 08:07:47.814821 27993 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 08:07:47.814823 27993 net.cpp:163] Memory required for data: 33997600
I0704 08:07:47.814828 27993 layer_factory.hpp:77] Creating layer conv1b/relu
I0704 08:07:47.814831 27993 net.cpp:98] Creating Layer conv1b/relu
I0704 08:07:47.814833 27993 net.cpp:439] conv1b/relu <- conv1b/bn
I0704 08:07:47.814836 27993 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0704 08:07:47.814839 27993 net.cpp:148] Setting up conv1b/relu
I0704 08:07:47.814841 27993 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 08:07:47.814843 27993 net.cpp:163] Memory required for data: 40551200
I0704 08:07:47.814844 27993 layer_factory.hpp:77] Creating layer pool1
I0704 08:07:47.814849 27993 net.cpp:98] Creating Layer pool1
I0704 08:07:47.814851 27993 net.cpp:439] pool1 <- conv1b/bn
I0704 08:07:47.814854 27993 net.cpp:413] pool1 -> pool1
I0704 08:07:47.814882 27993 net.cpp:148] Setting up pool1
I0704 08:07:47.814885 27993 net.cpp:155] Top shape: 50 32 32 32 (1638400)
I0704 08:07:47.814888 27993 net.cpp:163] Memory required for data: 47104800
I0704 08:07:47.814889 27993 layer_factory.hpp:77] Creating layer res2a_branch2a
I0704 08:07:47.814900 27993 net.cpp:98] Creating Layer res2a_branch2a
I0704 08:07:47.814903 27993 net.cpp:439] res2a_branch2a <- pool1
I0704 08:07:47.814905 27993 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0704 08:07:47.816004 27993 net.cpp:148] Setting up res2a_branch2a
I0704 08:07:47.816012 27993 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 08:07:47.816015 27993 net.cpp:163] Memory required for data: 60212000
I0704 08:07:47.816020 27993 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0704 08:07:47.816023 27993 net.cpp:98] Creating Layer res2a_branch2a/bn
I0704 08:07:47.816025 27993 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0704 08:07:47.816028 27993 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0704 08:07:47.816316 27993 net.cpp:148] Setting up res2a_branch2a/bn
I0704 08:07:47.816323 27993 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 08:07:47.816324 27993 net.cpp:163] Memory required for data: 73319200
I0704 08:07:47.816329 27993 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0704 08:07:47.816331 27993 net.cpp:98] Creating Layer res2a_branch2a/relu
I0704 08:07:47.816334 27993 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0704 08:07:47.816336 27993 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0704 08:07:47.816339 27993 net.cpp:148] Setting up res2a_branch2a/relu
I0704 08:07:47.816342 27993 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 08:07:47.816344 27993 net.cpp:163] Memory required for data: 86426400
I0704 08:07:47.816345 27993 layer_factory.hpp:77] Creating layer res2a_branch2b
I0704 08:07:47.816349 27993 net.cpp:98] Creating Layer res2a_branch2b
I0704 08:07:47.816351 27993 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0704 08:07:47.816354 27993 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0704 08:07:47.817205 27993 net.cpp:148] Setting up res2a_branch2b
I0704 08:07:47.817214 27993 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 08:07:47.817215 27993 net.cpp:163] Memory required for data: 99533600
I0704 08:07:47.817219 27993 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0704 08:07:47.817224 27993 net.cpp:98] Creating Layer res2a_branch2b/bn
I0704 08:07:47.817226 27993 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0704 08:07:47.817235 27993 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0704 08:07:47.817610 27993 net.cpp:148] Setting up res2a_branch2b/bn
I0704 08:07:47.817617 27993 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 08:07:47.817620 27993 net.cpp:163] Memory required for data: 112640800
I0704 08:07:47.817625 27993 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0704 08:07:47.817627 27993 net.cpp:98] Creating Layer res2a_branch2b/relu
I0704 08:07:47.817629 27993 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0704 08:07:47.817632 27993 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0704 08:07:47.817636 27993 net.cpp:148] Setting up res2a_branch2b/relu
I0704 08:07:47.817638 27993 net.cpp:155] Top shape: 50 64 32 32 (3276800)
I0704 08:07:47.817639 27993 net.cpp:163] Memory required for data: 125748000
I0704 08:07:47.817641 27993 layer_factory.hpp:77] Creating layer pool2
I0704 08:07:47.817646 27993 net.cpp:98] Creating Layer pool2
I0704 08:07:47.817647 27993 net.cpp:439] pool2 <- res2a_branch2b/bn
I0704 08:07:47.817651 27993 net.cpp:413] pool2 -> pool2
I0704 08:07:47.817667 27993 net.cpp:148] Setting up pool2
I0704 08:07:47.817672 27993 net.cpp:155] Top shape: 50 64 16 16 (819200)
I0704 08:07:47.817673 27993 net.cpp:163] Memory required for data: 129024800
I0704 08:07:47.817675 27993 layer_factory.hpp:77] Creating layer res3a_branch2a
I0704 08:07:47.817682 27993 net.cpp:98] Creating Layer res3a_branch2a
I0704 08:07:47.817684 27993 net.cpp:439] res3a_branch2a <- pool2
I0704 08:07:47.817687 27993 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0704 08:07:47.819794 27993 net.cpp:148] Setting up res3a_branch2a
I0704 08:07:47.819803 27993 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 08:07:47.819805 27993 net.cpp:163] Memory required for data: 135578400
I0704 08:07:47.819809 27993 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0704 08:07:47.819818 27993 net.cpp:98] Creating Layer res3a_branch2a/bn
I0704 08:07:47.819819 27993 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0704 08:07:47.819823 27993 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0704 08:07:47.820077 27993 net.cpp:148] Setting up res3a_branch2a/bn
I0704 08:07:47.820082 27993 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 08:07:47.820085 27993 net.cpp:163] Memory required for data: 142132000
I0704 08:07:47.820091 27993 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0704 08:07:47.820094 27993 net.cpp:98] Creating Layer res3a_branch2a/relu
I0704 08:07:47.820097 27993 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0704 08:07:47.820099 27993 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0704 08:07:47.820102 27993 net.cpp:148] Setting up res3a_branch2a/relu
I0704 08:07:47.820106 27993 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 08:07:47.820106 27993 net.cpp:163] Memory required for data: 148685600
I0704 08:07:47.820108 27993 layer_factory.hpp:77] Creating layer res3a_branch2b
I0704 08:07:47.820112 27993 net.cpp:98] Creating Layer res3a_branch2b
I0704 08:07:47.820116 27993 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0704 08:07:47.820118 27993 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0704 08:07:47.820969 27993 net.cpp:148] Setting up res3a_branch2b
I0704 08:07:47.820976 27993 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 08:07:47.820977 27993 net.cpp:163] Memory required for data: 155239200
I0704 08:07:47.820981 27993 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0704 08:07:47.820984 27993 net.cpp:98] Creating Layer res3a_branch2b/bn
I0704 08:07:47.820987 27993 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0704 08:07:47.820989 27993 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0704 08:07:47.821244 27993 net.cpp:148] Setting up res3a_branch2b/bn
I0704 08:07:47.821249 27993 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 08:07:47.821250 27993 net.cpp:163] Memory required for data: 161792800
I0704 08:07:47.821254 27993 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0704 08:07:47.821264 27993 net.cpp:98] Creating Layer res3a_branch2b/relu
I0704 08:07:47.821267 27993 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0704 08:07:47.821269 27993 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0704 08:07:47.821274 27993 net.cpp:148] Setting up res3a_branch2b/relu
I0704 08:07:47.821276 27993 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 08:07:47.821279 27993 net.cpp:163] Memory required for data: 168346400
I0704 08:07:47.821280 27993 layer_factory.hpp:77] Creating layer pool3
I0704 08:07:47.821285 27993 net.cpp:98] Creating Layer pool3
I0704 08:07:47.821286 27993 net.cpp:439] pool3 <- res3a_branch2b/bn
I0704 08:07:47.821288 27993 net.cpp:413] pool3 -> pool3
I0704 08:07:47.821310 27993 net.cpp:148] Setting up pool3
I0704 08:07:47.821315 27993 net.cpp:155] Top shape: 50 128 16 16 (1638400)
I0704 08:07:47.821316 27993 net.cpp:163] Memory required for data: 174900000
I0704 08:07:47.821318 27993 layer_factory.hpp:77] Creating layer res4a_branch2a
I0704 08:07:47.821322 27993 net.cpp:98] Creating Layer res4a_branch2a
I0704 08:07:47.821324 27993 net.cpp:439] res4a_branch2a <- pool3
I0704 08:07:47.821327 27993 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0704 08:07:47.827198 27993 net.cpp:148] Setting up res4a_branch2a
I0704 08:07:47.827204 27993 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 08:07:47.827206 27993 net.cpp:163] Memory required for data: 188007200
I0704 08:07:47.827209 27993 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0704 08:07:47.827214 27993 net.cpp:98] Creating Layer res4a_branch2a/bn
I0704 08:07:47.827216 27993 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0704 08:07:47.827219 27993 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0704 08:07:47.827472 27993 net.cpp:148] Setting up res4a_branch2a/bn
I0704 08:07:47.827477 27993 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 08:07:47.827479 27993 net.cpp:163] Memory required for data: 201114400
I0704 08:07:47.827486 27993 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0704 08:07:47.827488 27993 net.cpp:98] Creating Layer res4a_branch2a/relu
I0704 08:07:47.827491 27993 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0704 08:07:47.827494 27993 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0704 08:07:47.827498 27993 net.cpp:148] Setting up res4a_branch2a/relu
I0704 08:07:47.827502 27993 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 08:07:47.827503 27993 net.cpp:163] Memory required for data: 214221600
I0704 08:07:47.827505 27993 layer_factory.hpp:77] Creating layer res4a_branch2b
I0704 08:07:47.827510 27993 net.cpp:98] Creating Layer res4a_branch2b
I0704 08:07:47.827512 27993 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0704 08:07:47.827515 27993 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0704 08:07:47.830515 27993 net.cpp:148] Setting up res4a_branch2b
I0704 08:07:47.830520 27993 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 08:07:47.830523 27993 net.cpp:163] Memory required for data: 227328800
I0704 08:07:47.830526 27993 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0704 08:07:47.830530 27993 net.cpp:98] Creating Layer res4a_branch2b/bn
I0704 08:07:47.830533 27993 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0704 08:07:47.830538 27993 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0704 08:07:47.830788 27993 net.cpp:148] Setting up res4a_branch2b/bn
I0704 08:07:47.830792 27993 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 08:07:47.830796 27993 net.cpp:163] Memory required for data: 240436000
I0704 08:07:47.830801 27993 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0704 08:07:47.830803 27993 net.cpp:98] Creating Layer res4a_branch2b/relu
I0704 08:07:47.830806 27993 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0704 08:07:47.830809 27993 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0704 08:07:47.830813 27993 net.cpp:148] Setting up res4a_branch2b/relu
I0704 08:07:47.830816 27993 net.cpp:155] Top shape: 50 256 16 16 (3276800)
I0704 08:07:47.830823 27993 net.cpp:163] Memory required for data: 253543200
I0704 08:07:47.830826 27993 layer_factory.hpp:77] Creating layer pool4
I0704 08:07:47.830831 27993 net.cpp:98] Creating Layer pool4
I0704 08:07:47.830833 27993 net.cpp:439] pool4 <- res4a_branch2b/bn
I0704 08:07:47.830835 27993 net.cpp:413] pool4 -> pool4
I0704 08:07:47.830853 27993 net.cpp:148] Setting up pool4
I0704 08:07:47.830857 27993 net.cpp:155] Top shape: 50 256 8 8 (819200)
I0704 08:07:47.830859 27993 net.cpp:163] Memory required for data: 256820000
I0704 08:07:47.830862 27993 layer_factory.hpp:77] Creating layer res5a_branch2a
I0704 08:07:47.830865 27993 net.cpp:98] Creating Layer res5a_branch2a
I0704 08:07:47.830868 27993 net.cpp:439] res5a_branch2a <- pool4
I0704 08:07:47.830871 27993 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0704 08:07:47.854976 27993 net.cpp:148] Setting up res5a_branch2a
I0704 08:07:47.854991 27993 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 08:07:47.854995 27993 net.cpp:163] Memory required for data: 263373600
I0704 08:07:47.855000 27993 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0704 08:07:47.855006 27993 net.cpp:98] Creating Layer res5a_branch2a/bn
I0704 08:07:47.855010 27993 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0704 08:07:47.855013 27993 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0704 08:07:47.855293 27993 net.cpp:148] Setting up res5a_branch2a/bn
I0704 08:07:47.855298 27993 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 08:07:47.855300 27993 net.cpp:163] Memory required for data: 269927200
I0704 08:07:47.855305 27993 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0704 08:07:47.855309 27993 net.cpp:98] Creating Layer res5a_branch2a/relu
I0704 08:07:47.855310 27993 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0704 08:07:47.855314 27993 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0704 08:07:47.855316 27993 net.cpp:148] Setting up res5a_branch2a/relu
I0704 08:07:47.855319 27993 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 08:07:47.855320 27993 net.cpp:163] Memory required for data: 276480800
I0704 08:07:47.855322 27993 layer_factory.hpp:77] Creating layer res5a_branch2b
I0704 08:07:47.855326 27993 net.cpp:98] Creating Layer res5a_branch2b
I0704 08:07:47.855329 27993 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0704 08:07:47.855331 27993 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0704 08:07:47.867575 27993 net.cpp:148] Setting up res5a_branch2b
I0704 08:07:47.867584 27993 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 08:07:47.867588 27993 net.cpp:163] Memory required for data: 283034400
I0704 08:07:47.867594 27993 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0704 08:07:47.867599 27993 net.cpp:98] Creating Layer res5a_branch2b/bn
I0704 08:07:47.867602 27993 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0704 08:07:47.867605 27993 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0704 08:07:47.867892 27993 net.cpp:148] Setting up res5a_branch2b/bn
I0704 08:07:47.867897 27993 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 08:07:47.867899 27993 net.cpp:163] Memory required for data: 289588000
I0704 08:07:47.867904 27993 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0704 08:07:47.867908 27993 net.cpp:98] Creating Layer res5a_branch2b/relu
I0704 08:07:47.867910 27993 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0704 08:07:47.867913 27993 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0704 08:07:47.867916 27993 net.cpp:148] Setting up res5a_branch2b/relu
I0704 08:07:47.867918 27993 net.cpp:155] Top shape: 50 512 8 8 (1638400)
I0704 08:07:47.867920 27993 net.cpp:163] Memory required for data: 296141600
I0704 08:07:47.867921 27993 layer_factory.hpp:77] Creating layer pool5
I0704 08:07:47.867928 27993 net.cpp:98] Creating Layer pool5
I0704 08:07:47.867929 27993 net.cpp:439] pool5 <- res5a_branch2b/bn
I0704 08:07:47.867931 27993 net.cpp:413] pool5 -> pool5
I0704 08:07:47.867949 27993 net.cpp:148] Setting up pool5
I0704 08:07:47.867954 27993 net.cpp:155] Top shape: 50 512 1 1 (25600)
I0704 08:07:47.867962 27993 net.cpp:163] Memory required for data: 296244000
I0704 08:07:47.867964 27993 layer_factory.hpp:77] Creating layer fc10
I0704 08:07:47.867972 27993 net.cpp:98] Creating Layer fc10
I0704 08:07:47.867974 27993 net.cpp:439] fc10 <- pool5
I0704 08:07:47.867977 27993 net.cpp:413] fc10 -> fc10
I0704 08:07:47.868134 27993 net.cpp:148] Setting up fc10
I0704 08:07:47.868139 27993 net.cpp:155] Top shape: 50 10 (500)
I0704 08:07:47.868140 27993 net.cpp:163] Memory required for data: 296246000
I0704 08:07:47.868144 27993 layer_factory.hpp:77] Creating layer fc10_fc10_0_split
I0704 08:07:47.868147 27993 net.cpp:98] Creating Layer fc10_fc10_0_split
I0704 08:07:47.868149 27993 net.cpp:439] fc10_fc10_0_split <- fc10
I0704 08:07:47.868152 27993 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_0
I0704 08:07:47.868155 27993 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_1
I0704 08:07:47.868158 27993 net.cpp:413] fc10_fc10_0_split -> fc10_fc10_0_split_2
I0704 08:07:47.868188 27993 net.cpp:148] Setting up fc10_fc10_0_split
I0704 08:07:47.868192 27993 net.cpp:155] Top shape: 50 10 (500)
I0704 08:07:47.868194 27993 net.cpp:155] Top shape: 50 10 (500)
I0704 08:07:47.868196 27993 net.cpp:155] Top shape: 50 10 (500)
I0704 08:07:47.868198 27993 net.cpp:163] Memory required for data: 296252000
I0704 08:07:47.868201 27993 layer_factory.hpp:77] Creating layer loss
I0704 08:07:47.868206 27993 net.cpp:98] Creating Layer loss
I0704 08:07:47.868207 27993 net.cpp:439] loss <- fc10_fc10_0_split_0
I0704 08:07:47.868211 27993 net.cpp:439] loss <- label_data_1_split_0
I0704 08:07:47.868212 27993 net.cpp:413] loss -> loss
I0704 08:07:47.868217 27993 layer_factory.hpp:77] Creating layer loss
I0704 08:07:47.868268 27993 net.cpp:148] Setting up loss
I0704 08:07:47.868271 27993 net.cpp:155] Top shape: (1)
I0704 08:07:47.868273 27993 net.cpp:158]     with loss weight 1
I0704 08:07:47.868285 27993 net.cpp:163] Memory required for data: 296252004
I0704 08:07:47.868288 27993 layer_factory.hpp:77] Creating layer accuracy/top1
I0704 08:07:47.868291 27993 net.cpp:98] Creating Layer accuracy/top1
I0704 08:07:47.868294 27993 net.cpp:439] accuracy/top1 <- fc10_fc10_0_split_1
I0704 08:07:47.868296 27993 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0704 08:07:47.868299 27993 net.cpp:413] accuracy/top1 -> accuracy/top1
I0704 08:07:47.868305 27993 net.cpp:148] Setting up accuracy/top1
I0704 08:07:47.868309 27993 net.cpp:155] Top shape: (1)
I0704 08:07:47.868310 27993 net.cpp:163] Memory required for data: 296252008
I0704 08:07:47.868311 27993 layer_factory.hpp:77] Creating layer accuracy/top5
I0704 08:07:47.868315 27993 net.cpp:98] Creating Layer accuracy/top5
I0704 08:07:47.868317 27993 net.cpp:439] accuracy/top5 <- fc10_fc10_0_split_2
I0704 08:07:47.868320 27993 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0704 08:07:47.868322 27993 net.cpp:413] accuracy/top5 -> accuracy/top5
I0704 08:07:47.868327 27993 net.cpp:148] Setting up accuracy/top5
I0704 08:07:47.868330 27993 net.cpp:155] Top shape: (1)
I0704 08:07:47.868332 27993 net.cpp:163] Memory required for data: 296252012
I0704 08:07:47.868335 27993 net.cpp:226] accuracy/top5 does not need backward computation.
I0704 08:07:47.868337 27993 net.cpp:226] accuracy/top1 does not need backward computation.
I0704 08:07:47.868340 27993 net.cpp:224] loss needs backward computation.
I0704 08:07:47.868342 27993 net.cpp:224] fc10_fc10_0_split needs backward computation.
I0704 08:07:47.868345 27993 net.cpp:224] fc10 needs backward computation.
I0704 08:07:47.868346 27993 net.cpp:224] pool5 needs backward computation.
I0704 08:07:47.868350 27993 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0704 08:07:47.868352 27993 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0704 08:07:47.868355 27993 net.cpp:224] res5a_branch2b needs backward computation.
I0704 08:07:47.868356 27993 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0704 08:07:47.868360 27993 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0704 08:07:47.868366 27993 net.cpp:224] res5a_branch2a needs backward computation.
I0704 08:07:47.868368 27993 net.cpp:224] pool4 needs backward computation.
I0704 08:07:47.868371 27993 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0704 08:07:47.868374 27993 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0704 08:07:47.868377 27993 net.cpp:224] res4a_branch2b needs backward computation.
I0704 08:07:47.868379 27993 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0704 08:07:47.868382 27993 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0704 08:07:47.868384 27993 net.cpp:224] res4a_branch2a needs backward computation.
I0704 08:07:47.868387 27993 net.cpp:224] pool3 needs backward computation.
I0704 08:07:47.868391 27993 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0704 08:07:47.868393 27993 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0704 08:07:47.868396 27993 net.cpp:224] res3a_branch2b needs backward computation.
I0704 08:07:47.868398 27993 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0704 08:07:47.868401 27993 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0704 08:07:47.868403 27993 net.cpp:224] res3a_branch2a needs backward computation.
I0704 08:07:47.868405 27993 net.cpp:224] pool2 needs backward computation.
I0704 08:07:47.868407 27993 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0704 08:07:47.868410 27993 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0704 08:07:47.868413 27993 net.cpp:224] res2a_branch2b needs backward computation.
I0704 08:07:47.868415 27993 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0704 08:07:47.868418 27993 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0704 08:07:47.868420 27993 net.cpp:224] res2a_branch2a needs backward computation.
I0704 08:07:47.868423 27993 net.cpp:224] pool1 needs backward computation.
I0704 08:07:47.868425 27993 net.cpp:224] conv1b/relu needs backward computation.
I0704 08:07:47.868427 27993 net.cpp:224] conv1b/bn needs backward computation.
I0704 08:07:47.868430 27993 net.cpp:224] conv1b needs backward computation.
I0704 08:07:47.868432 27993 net.cpp:224] conv1a/relu needs backward computation.
I0704 08:07:47.868435 27993 net.cpp:224] conv1a/bn needs backward computation.
I0704 08:07:47.868438 27993 net.cpp:224] conv1a needs backward computation.
I0704 08:07:47.868440 27993 net.cpp:226] data/bias does not need backward computation.
I0704 08:07:47.868443 27993 net.cpp:226] label_data_1_split does not need backward computation.
I0704 08:07:47.868446 27993 net.cpp:226] data does not need backward computation.
I0704 08:07:47.868448 27993 net.cpp:268] This network produces output accuracy/top1
I0704 08:07:47.868450 27993 net.cpp:268] This network produces output accuracy/top5
I0704 08:07:47.868453 27993 net.cpp:268] This network produces output loss
I0704 08:07:47.868472 27993 net.cpp:288] Network initialization done.
I0704 08:07:47.879020 27993 caffe.cpp:289] Running for 200 iterations.
I0704 08:07:47.901266 27993 caffe.cpp:312] Batch 0, accuracy/top1 = 0.96
I0704 08:07:47.901284 27993 caffe.cpp:312] Batch 0, accuracy/top5 = 1
I0704 08:07:47.901288 27993 caffe.cpp:312] Batch 0, loss = 0.08
I0704 08:07:47.909477 27993 caffe.cpp:312] Batch 1, accuracy/top1 = 0.92
I0704 08:07:47.909487 27993 caffe.cpp:312] Batch 1, accuracy/top5 = 1
I0704 08:07:47.909488 27993 caffe.cpp:312] Batch 1, loss = 0.14
I0704 08:07:47.917752 27993 caffe.cpp:312] Batch 2, accuracy/top1 = 0.9
I0704 08:07:47.917760 27993 caffe.cpp:312] Batch 2, accuracy/top5 = 1
I0704 08:07:47.917763 27993 caffe.cpp:312] Batch 2, loss = 0.18
I0704 08:07:47.925891 27993 caffe.cpp:312] Batch 3, accuracy/top1 = 0.9
I0704 08:07:47.925899 27993 caffe.cpp:312] Batch 3, accuracy/top5 = 1
I0704 08:07:47.925902 27993 caffe.cpp:312] Batch 3, loss = 0.1
I0704 08:07:47.934108 27993 caffe.cpp:312] Batch 4, accuracy/top1 = 0.88
I0704 08:07:47.934116 27993 caffe.cpp:312] Batch 4, accuracy/top5 = 1
I0704 08:07:47.934119 27993 caffe.cpp:312] Batch 4, loss = 0.16
I0704 08:07:47.942284 27993 caffe.cpp:312] Batch 5, accuracy/top1 = 0.92
I0704 08:07:47.942292 27993 caffe.cpp:312] Batch 5, accuracy/top5 = 1
I0704 08:07:47.942296 27993 caffe.cpp:312] Batch 5, loss = 0.16
I0704 08:07:47.950485 27993 caffe.cpp:312] Batch 6, accuracy/top1 = 0.9
I0704 08:07:47.950492 27993 caffe.cpp:312] Batch 6, accuracy/top5 = 1
I0704 08:07:47.950495 27993 caffe.cpp:312] Batch 6, loss = 0.18
I0704 08:07:47.958602 27993 caffe.cpp:312] Batch 7, accuracy/top1 = 0.9
I0704 08:07:47.958611 27993 caffe.cpp:312] Batch 7, accuracy/top5 = 1
I0704 08:07:47.958613 27993 caffe.cpp:312] Batch 7, loss = 0.26
I0704 08:07:47.966838 27993 caffe.cpp:312] Batch 8, accuracy/top1 = 0.88
I0704 08:07:47.966846 27993 caffe.cpp:312] Batch 8, accuracy/top5 = 1
I0704 08:07:47.966848 27993 caffe.cpp:312] Batch 8, loss = 0.14
I0704 08:07:47.975044 27993 caffe.cpp:312] Batch 9, accuracy/top1 = 0.86
I0704 08:07:47.975050 27993 caffe.cpp:312] Batch 9, accuracy/top5 = 1
I0704 08:07:47.975054 27993 caffe.cpp:312] Batch 9, loss = 0.16
I0704 08:07:47.983283 27993 caffe.cpp:312] Batch 10, accuracy/top1 = 0.94
I0704 08:07:47.983289 27993 caffe.cpp:312] Batch 10, accuracy/top5 = 1
I0704 08:07:47.983292 27993 caffe.cpp:312] Batch 10, loss = 0.12
I0704 08:07:47.991459 27993 caffe.cpp:312] Batch 11, accuracy/top1 = 0.98
I0704 08:07:47.991466 27993 caffe.cpp:312] Batch 11, accuracy/top5 = 1
I0704 08:07:47.991469 27993 caffe.cpp:312] Batch 11, loss = 0.08
I0704 08:07:47.999577 27993 caffe.cpp:312] Batch 12, accuracy/top1 = 0.96
I0704 08:07:47.999584 27993 caffe.cpp:312] Batch 12, accuracy/top5 = 1
I0704 08:07:47.999588 27993 caffe.cpp:312] Batch 12, loss = 0.04
I0704 08:07:48.007774 27993 caffe.cpp:312] Batch 13, accuracy/top1 = 0.88
I0704 08:07:48.007782 27993 caffe.cpp:312] Batch 13, accuracy/top5 = 1
I0704 08:07:48.007786 27993 caffe.cpp:312] Batch 13, loss = 0.26
I0704 08:07:48.015961 27993 caffe.cpp:312] Batch 14, accuracy/top1 = 0.94
I0704 08:07:48.015969 27993 caffe.cpp:312] Batch 14, accuracy/top5 = 1
I0704 08:07:48.015971 27993 caffe.cpp:312] Batch 14, loss = 0.18
I0704 08:07:48.024180 27993 caffe.cpp:312] Batch 15, accuracy/top1 = 0.86
I0704 08:07:48.024188 27993 caffe.cpp:312] Batch 15, accuracy/top5 = 1
I0704 08:07:48.024191 27993 caffe.cpp:312] Batch 15, loss = 0.3
I0704 08:07:48.032402 27993 caffe.cpp:312] Batch 16, accuracy/top1 = 0.9
I0704 08:07:48.032409 27993 caffe.cpp:312] Batch 16, accuracy/top5 = 0.98
I0704 08:07:48.032413 27993 caffe.cpp:312] Batch 16, loss = 0.34
I0704 08:07:48.040560 27993 caffe.cpp:312] Batch 17, accuracy/top1 = 0.9
I0704 08:07:48.040568 27993 caffe.cpp:312] Batch 17, accuracy/top5 = 1
I0704 08:07:48.040571 27993 caffe.cpp:312] Batch 17, loss = 0.26
I0704 08:07:48.048737 27993 caffe.cpp:312] Batch 18, accuracy/top1 = 0.94
I0704 08:07:48.048743 27993 caffe.cpp:312] Batch 18, accuracy/top5 = 1
I0704 08:07:48.048746 27993 caffe.cpp:312] Batch 18, loss = 0.14
I0704 08:07:48.056922 27993 caffe.cpp:312] Batch 19, accuracy/top1 = 0.92
I0704 08:07:48.056929 27993 caffe.cpp:312] Batch 19, accuracy/top5 = 1
I0704 08:07:48.056932 27993 caffe.cpp:312] Batch 19, loss = 0.1
I0704 08:07:48.065099 27993 caffe.cpp:312] Batch 20, accuracy/top1 = 0.94
I0704 08:07:48.065106 27993 caffe.cpp:312] Batch 20, accuracy/top5 = 1
I0704 08:07:48.065109 27993 caffe.cpp:312] Batch 20, loss = 0.14
I0704 08:07:48.073314 27993 caffe.cpp:312] Batch 21, accuracy/top1 = 0.88
I0704 08:07:48.073321 27993 caffe.cpp:312] Batch 21, accuracy/top5 = 1
I0704 08:07:48.073324 27993 caffe.cpp:312] Batch 21, loss = 0.16
I0704 08:07:48.081497 27993 caffe.cpp:312] Batch 22, accuracy/top1 = 0.9
I0704 08:07:48.081504 27993 caffe.cpp:312] Batch 22, accuracy/top5 = 1
I0704 08:07:48.081507 27993 caffe.cpp:312] Batch 22, loss = 0.26
I0704 08:07:48.089696 27993 caffe.cpp:312] Batch 23, accuracy/top1 = 0.9
I0704 08:07:48.089704 27993 caffe.cpp:312] Batch 23, accuracy/top5 = 1
I0704 08:07:48.089706 27993 caffe.cpp:312] Batch 23, loss = 0.2
I0704 08:07:48.097893 27993 caffe.cpp:312] Batch 24, accuracy/top1 = 0.88
I0704 08:07:48.097901 27993 caffe.cpp:312] Batch 24, accuracy/top5 = 1
I0704 08:07:48.097913 27993 caffe.cpp:312] Batch 24, loss = 0.24
I0704 08:07:48.106073 27993 caffe.cpp:312] Batch 25, accuracy/top1 = 0.98
I0704 08:07:48.106081 27993 caffe.cpp:312] Batch 25, accuracy/top5 = 1
I0704 08:07:48.106083 27993 caffe.cpp:312] Batch 25, loss = 0.02
I0704 08:07:48.114256 27993 caffe.cpp:312] Batch 26, accuracy/top1 = 0.9
I0704 08:07:48.114264 27993 caffe.cpp:312] Batch 26, accuracy/top5 = 1
I0704 08:07:48.114266 27993 caffe.cpp:312] Batch 26, loss = 0.32
I0704 08:07:48.122360 27993 caffe.cpp:312] Batch 27, accuracy/top1 = 0.92
I0704 08:07:48.122369 27993 caffe.cpp:312] Batch 27, accuracy/top5 = 1
I0704 08:07:48.122370 27993 caffe.cpp:312] Batch 27, loss = 0.16
I0704 08:07:48.130573 27993 caffe.cpp:312] Batch 28, accuracy/top1 = 0.94
I0704 08:07:48.130581 27993 caffe.cpp:312] Batch 28, accuracy/top5 = 1
I0704 08:07:48.130584 27993 caffe.cpp:312] Batch 28, loss = 0.06
I0704 08:07:48.138772 27993 caffe.cpp:312] Batch 29, accuracy/top1 = 0.88
I0704 08:07:48.138778 27993 caffe.cpp:312] Batch 29, accuracy/top5 = 1
I0704 08:07:48.138782 27993 caffe.cpp:312] Batch 29, loss = 0.26
I0704 08:07:48.146925 27993 caffe.cpp:312] Batch 30, accuracy/top1 = 0.9
I0704 08:07:48.146934 27993 caffe.cpp:312] Batch 30, accuracy/top5 = 1
I0704 08:07:48.146935 27993 caffe.cpp:312] Batch 30, loss = 0.28
I0704 08:07:48.155120 27993 caffe.cpp:312] Batch 31, accuracy/top1 = 0.92
I0704 08:07:48.155128 27993 caffe.cpp:312] Batch 31, accuracy/top5 = 1
I0704 08:07:48.155130 27993 caffe.cpp:312] Batch 31, loss = 0.18
I0704 08:07:48.163326 27993 caffe.cpp:312] Batch 32, accuracy/top1 = 0.94
I0704 08:07:48.163333 27993 caffe.cpp:312] Batch 32, accuracy/top5 = 1
I0704 08:07:48.163336 27993 caffe.cpp:312] Batch 32, loss = 0.1
I0704 08:07:48.171499 27993 caffe.cpp:312] Batch 33, accuracy/top1 = 0.9
I0704 08:07:48.171505 27993 caffe.cpp:312] Batch 33, accuracy/top5 = 1
I0704 08:07:48.171509 27993 caffe.cpp:312] Batch 33, loss = 0.24
I0704 08:07:48.179610 27993 caffe.cpp:312] Batch 34, accuracy/top1 = 0.92
I0704 08:07:48.179617 27993 caffe.cpp:312] Batch 34, accuracy/top5 = 1
I0704 08:07:48.179620 27993 caffe.cpp:312] Batch 34, loss = 0.1
I0704 08:07:48.187860 27993 caffe.cpp:312] Batch 35, accuracy/top1 = 0.94
I0704 08:07:48.187866 27993 caffe.cpp:312] Batch 35, accuracy/top5 = 1
I0704 08:07:48.187870 27993 caffe.cpp:312] Batch 35, loss = 0.08
I0704 08:07:48.196000 27993 caffe.cpp:312] Batch 36, accuracy/top1 = 0.88
I0704 08:07:48.196007 27993 caffe.cpp:312] Batch 36, accuracy/top5 = 1
I0704 08:07:48.196010 27993 caffe.cpp:312] Batch 36, loss = 0.18
I0704 08:07:48.204212 27993 caffe.cpp:312] Batch 37, accuracy/top1 = 0.88
I0704 08:07:48.204219 27993 caffe.cpp:312] Batch 37, accuracy/top5 = 1
I0704 08:07:48.204222 27993 caffe.cpp:312] Batch 37, loss = 0.22
I0704 08:07:48.212427 27993 caffe.cpp:312] Batch 38, accuracy/top1 = 0.92
I0704 08:07:48.212435 27993 caffe.cpp:312] Batch 38, accuracy/top5 = 0.96
I0704 08:07:48.212437 27993 caffe.cpp:312] Batch 38, loss = 0.38
I0704 08:07:48.220492 27993 caffe.cpp:312] Batch 39, accuracy/top1 = 0.94
I0704 08:07:48.220499 27993 caffe.cpp:312] Batch 39, accuracy/top5 = 0.98
I0704 08:07:48.220502 27993 caffe.cpp:312] Batch 39, loss = 0.2
I0704 08:07:48.228646 27993 caffe.cpp:312] Batch 40, accuracy/top1 = 0.92
I0704 08:07:48.228652 27993 caffe.cpp:312] Batch 40, accuracy/top5 = 1
I0704 08:07:48.228655 27993 caffe.cpp:312] Batch 40, loss = 0.2
I0704 08:07:48.236816 27993 caffe.cpp:312] Batch 41, accuracy/top1 = 0.94
I0704 08:07:48.236824 27993 caffe.cpp:312] Batch 41, accuracy/top5 = 1
I0704 08:07:48.236825 27993 caffe.cpp:312] Batch 41, loss = 0.14
I0704 08:07:48.244946 27993 caffe.cpp:312] Batch 42, accuracy/top1 = 0.92
I0704 08:07:48.244952 27993 caffe.cpp:312] Batch 42, accuracy/top5 = 1
I0704 08:07:48.244954 27993 caffe.cpp:312] Batch 42, loss = 0.16
I0704 08:07:48.253160 27993 caffe.cpp:312] Batch 43, accuracy/top1 = 0.86
I0704 08:07:48.253167 27993 caffe.cpp:312] Batch 43, accuracy/top5 = 1
I0704 08:07:48.253170 27993 caffe.cpp:312] Batch 43, loss = 0.3
I0704 08:07:48.261328 27993 caffe.cpp:312] Batch 44, accuracy/top1 = 0.86
I0704 08:07:48.261342 27993 caffe.cpp:312] Batch 44, accuracy/top5 = 0.98
I0704 08:07:48.261344 27993 caffe.cpp:312] Batch 44, loss = 0.26
I0704 08:07:48.269577 27993 caffe.cpp:312] Batch 45, accuracy/top1 = 0.86
I0704 08:07:48.269584 27993 caffe.cpp:312] Batch 45, accuracy/top5 = 1
I0704 08:07:48.269587 27993 caffe.cpp:312] Batch 45, loss = 0.3
I0704 08:07:48.277709 27993 caffe.cpp:312] Batch 46, accuracy/top1 = 0.94
I0704 08:07:48.277716 27993 caffe.cpp:312] Batch 46, accuracy/top5 = 1
I0704 08:07:48.277719 27993 caffe.cpp:312] Batch 46, loss = 0.04
I0704 08:07:48.285895 27993 caffe.cpp:312] Batch 47, accuracy/top1 = 0.88
I0704 08:07:48.285902 27993 caffe.cpp:312] Batch 47, accuracy/top5 = 1
I0704 08:07:48.285905 27993 caffe.cpp:312] Batch 47, loss = 0.22
I0704 08:07:48.293944 27993 caffe.cpp:312] Batch 48, accuracy/top1 = 0.96
I0704 08:07:48.293951 27993 caffe.cpp:312] Batch 48, accuracy/top5 = 0.98
I0704 08:07:48.293954 27993 caffe.cpp:312] Batch 48, loss = 0.26
I0704 08:07:48.302145 27993 caffe.cpp:312] Batch 49, accuracy/top1 = 0.88
I0704 08:07:48.302153 27993 caffe.cpp:312] Batch 49, accuracy/top5 = 1
I0704 08:07:48.302156 27993 caffe.cpp:312] Batch 49, loss = 0.2
I0704 08:07:48.310333 27993 caffe.cpp:312] Batch 50, accuracy/top1 = 0.88
I0704 08:07:48.310339 27993 caffe.cpp:312] Batch 50, accuracy/top5 = 0.96
I0704 08:07:48.310343 27993 caffe.cpp:312] Batch 50, loss = 0.32
I0704 08:07:48.318521 27993 caffe.cpp:312] Batch 51, accuracy/top1 = 0.92
I0704 08:07:48.318527 27993 caffe.cpp:312] Batch 51, accuracy/top5 = 0.98
I0704 08:07:48.318531 27993 caffe.cpp:312] Batch 51, loss = 0.24
I0704 08:07:48.326678 27993 caffe.cpp:312] Batch 52, accuracy/top1 = 0.94
I0704 08:07:48.326685 27993 caffe.cpp:312] Batch 52, accuracy/top5 = 1
I0704 08:07:48.326689 27993 caffe.cpp:312] Batch 52, loss = 0.06
I0704 08:07:48.334848 27993 caffe.cpp:312] Batch 53, accuracy/top1 = 0.9
I0704 08:07:48.334856 27993 caffe.cpp:312] Batch 53, accuracy/top5 = 0.98
I0704 08:07:48.334858 27993 caffe.cpp:312] Batch 53, loss = 0.1
I0704 08:07:48.343055 27993 caffe.cpp:312] Batch 54, accuracy/top1 = 0.94
I0704 08:07:48.343062 27993 caffe.cpp:312] Batch 54, accuracy/top5 = 1
I0704 08:07:48.343065 27993 caffe.cpp:312] Batch 54, loss = 0.18
I0704 08:07:48.351253 27993 caffe.cpp:312] Batch 55, accuracy/top1 = 0.9
I0704 08:07:48.351259 27993 caffe.cpp:312] Batch 55, accuracy/top5 = 1
I0704 08:07:48.351263 27993 caffe.cpp:312] Batch 55, loss = 0.26
I0704 08:07:48.359467 27993 caffe.cpp:312] Batch 56, accuracy/top1 = 0.88
I0704 08:07:48.359474 27993 caffe.cpp:312] Batch 56, accuracy/top5 = 0.98
I0704 08:07:48.359477 27993 caffe.cpp:312] Batch 56, loss = 0.34
I0704 08:07:48.367684 27993 caffe.cpp:312] Batch 57, accuracy/top1 = 0.98
I0704 08:07:48.367692 27993 caffe.cpp:312] Batch 57, accuracy/top5 = 1
I0704 08:07:48.367694 27993 caffe.cpp:312] Batch 57, loss = 0.06
I0704 08:07:48.375849 27993 caffe.cpp:312] Batch 58, accuracy/top1 = 0.94
I0704 08:07:48.375856 27993 caffe.cpp:312] Batch 58, accuracy/top5 = 1
I0704 08:07:48.375859 27993 caffe.cpp:312] Batch 58, loss = 0.12
I0704 08:07:48.384014 27993 caffe.cpp:312] Batch 59, accuracy/top1 = 0.92
I0704 08:07:48.384021 27993 caffe.cpp:312] Batch 59, accuracy/top5 = 1
I0704 08:07:48.384024 27993 caffe.cpp:312] Batch 59, loss = 0.12
I0704 08:07:48.392225 27993 caffe.cpp:312] Batch 60, accuracy/top1 = 0.92
I0704 08:07:48.392232 27993 caffe.cpp:312] Batch 60, accuracy/top5 = 0.98
I0704 08:07:48.392235 27993 caffe.cpp:312] Batch 60, loss = 0.22
I0704 08:07:48.400454 27993 caffe.cpp:312] Batch 61, accuracy/top1 = 0.86
I0704 08:07:48.400460 27993 caffe.cpp:312] Batch 61, accuracy/top5 = 0.98
I0704 08:07:48.400463 27993 caffe.cpp:312] Batch 61, loss = 0.34
I0704 08:07:48.408658 27993 caffe.cpp:312] Batch 62, accuracy/top1 = 0.98
I0704 08:07:48.408665 27993 caffe.cpp:312] Batch 62, accuracy/top5 = 1
I0704 08:07:48.408668 27993 caffe.cpp:312] Batch 62, loss = 0.02
I0704 08:07:48.416858 27993 caffe.cpp:312] Batch 63, accuracy/top1 = 0.9
I0704 08:07:48.416865 27993 caffe.cpp:312] Batch 63, accuracy/top5 = 1
I0704 08:07:48.416875 27993 caffe.cpp:312] Batch 63, loss = 0.18
I0704 08:07:48.425014 27993 caffe.cpp:312] Batch 64, accuracy/top1 = 0.84
I0704 08:07:48.425021 27993 caffe.cpp:312] Batch 64, accuracy/top5 = 1
I0704 08:07:48.425024 27993 caffe.cpp:312] Batch 64, loss = 0.16
I0704 08:07:48.433219 27993 caffe.cpp:312] Batch 65, accuracy/top1 = 0.94
I0704 08:07:48.433226 27993 caffe.cpp:312] Batch 65, accuracy/top5 = 0.98
I0704 08:07:48.433228 27993 caffe.cpp:312] Batch 65, loss = 0.12
I0704 08:07:48.441427 27993 caffe.cpp:312] Batch 66, accuracy/top1 = 0.88
I0704 08:07:48.441435 27993 caffe.cpp:312] Batch 66, accuracy/top5 = 1
I0704 08:07:48.441437 27993 caffe.cpp:312] Batch 66, loss = 0.14
I0704 08:07:48.449640 27993 caffe.cpp:312] Batch 67, accuracy/top1 = 0.88
I0704 08:07:48.449647 27993 caffe.cpp:312] Batch 67, accuracy/top5 = 0.98
I0704 08:07:48.449651 27993 caffe.cpp:312] Batch 67, loss = 0.2
I0704 08:07:48.457861 27993 caffe.cpp:312] Batch 68, accuracy/top1 = 0.9
I0704 08:07:48.457868 27993 caffe.cpp:312] Batch 68, accuracy/top5 = 1
I0704 08:07:48.457871 27993 caffe.cpp:312] Batch 68, loss = 0.26
I0704 08:07:48.465982 27993 caffe.cpp:312] Batch 69, accuracy/top1 = 0.92
I0704 08:07:48.465989 27993 caffe.cpp:312] Batch 69, accuracy/top5 = 1
I0704 08:07:48.465992 27993 caffe.cpp:312] Batch 69, loss = 0.08
I0704 08:07:48.474179 27993 caffe.cpp:312] Batch 70, accuracy/top1 = 0.94
I0704 08:07:48.474186 27993 caffe.cpp:312] Batch 70, accuracy/top5 = 0.98
I0704 08:07:48.474189 27993 caffe.cpp:312] Batch 70, loss = 0.22
I0704 08:07:48.482354 27993 caffe.cpp:312] Batch 71, accuracy/top1 = 0.86
I0704 08:07:48.482362 27993 caffe.cpp:312] Batch 71, accuracy/top5 = 1
I0704 08:07:48.482365 27993 caffe.cpp:312] Batch 71, loss = 0.26
I0704 08:07:48.490499 27993 caffe.cpp:312] Batch 72, accuracy/top1 = 0.86
I0704 08:07:48.490506 27993 caffe.cpp:312] Batch 72, accuracy/top5 = 0.98
I0704 08:07:48.490509 27993 caffe.cpp:312] Batch 72, loss = 0.34
I0704 08:07:48.498591 27993 caffe.cpp:312] Batch 73, accuracy/top1 = 0.94
I0704 08:07:48.498600 27993 caffe.cpp:312] Batch 73, accuracy/top5 = 1
I0704 08:07:48.498601 27993 caffe.cpp:312] Batch 73, loss = 0.2
I0704 08:07:48.506778 27993 caffe.cpp:312] Batch 74, accuracy/top1 = 0.94
I0704 08:07:48.506784 27993 caffe.cpp:312] Batch 74, accuracy/top5 = 1
I0704 08:07:48.506788 27993 caffe.cpp:312] Batch 74, loss = 0.08
I0704 08:07:48.514987 27993 caffe.cpp:312] Batch 75, accuracy/top1 = 0.92
I0704 08:07:48.514994 27993 caffe.cpp:312] Batch 75, accuracy/top5 = 1
I0704 08:07:48.514997 27993 caffe.cpp:312] Batch 75, loss = 0.16
I0704 08:07:48.523181 27993 caffe.cpp:312] Batch 76, accuracy/top1 = 0.92
I0704 08:07:48.523192 27993 caffe.cpp:312] Batch 76, accuracy/top5 = 1
I0704 08:07:48.523195 27993 caffe.cpp:312] Batch 76, loss = 0.14
I0704 08:07:48.531455 27993 caffe.cpp:312] Batch 77, accuracy/top1 = 0.92
I0704 08:07:48.531473 27993 caffe.cpp:312] Batch 77, accuracy/top5 = 1
I0704 08:07:48.531476 27993 caffe.cpp:312] Batch 77, loss = 0.08
I0704 08:07:48.539728 27993 caffe.cpp:312] Batch 78, accuracy/top1 = 0.94
I0704 08:07:48.539744 27993 caffe.cpp:312] Batch 78, accuracy/top5 = 1
I0704 08:07:48.539747 27993 caffe.cpp:312] Batch 78, loss = 0.02
I0704 08:07:48.547925 27993 caffe.cpp:312] Batch 79, accuracy/top1 = 0.94
I0704 08:07:48.547933 27993 caffe.cpp:312] Batch 79, accuracy/top5 = 1
I0704 08:07:48.547935 27993 caffe.cpp:312] Batch 79, loss = 0.16
I0704 08:07:48.556143 27993 caffe.cpp:312] Batch 80, accuracy/top1 = 0.94
I0704 08:07:48.556150 27993 caffe.cpp:312] Batch 80, accuracy/top5 = 0.98
I0704 08:07:48.556154 27993 caffe.cpp:312] Batch 80, loss = 0.14
I0704 08:07:48.564318 27993 caffe.cpp:312] Batch 81, accuracy/top1 = 0.88
I0704 08:07:48.564327 27993 caffe.cpp:312] Batch 81, accuracy/top5 = 1
I0704 08:07:48.564328 27993 caffe.cpp:312] Batch 81, loss = 0.16
I0704 08:07:48.572521 27993 caffe.cpp:312] Batch 82, accuracy/top1 = 0.92
I0704 08:07:48.572530 27993 caffe.cpp:312] Batch 82, accuracy/top5 = 0.98
I0704 08:07:48.572533 27993 caffe.cpp:312] Batch 82, loss = 0.32
I0704 08:07:48.580770 27993 caffe.cpp:312] Batch 83, accuracy/top1 = 0.94
I0704 08:07:48.580780 27993 caffe.cpp:312] Batch 83, accuracy/top5 = 1
I0704 08:07:48.580783 27993 caffe.cpp:312] Batch 83, loss = 0.1
I0704 08:07:48.588977 27993 caffe.cpp:312] Batch 84, accuracy/top1 = 0.98
I0704 08:07:48.588991 27993 caffe.cpp:312] Batch 84, accuracy/top5 = 1
I0704 08:07:48.588994 27993 caffe.cpp:312] Batch 84, loss = 0.08
I0704 08:07:48.597172 27993 caffe.cpp:312] Batch 85, accuracy/top1 = 0.92
I0704 08:07:48.597185 27993 caffe.cpp:312] Batch 85, accuracy/top5 = 1
I0704 08:07:48.597188 27993 caffe.cpp:312] Batch 85, loss = 0.14
I0704 08:07:48.605397 27993 caffe.cpp:312] Batch 86, accuracy/top1 = 0.9
I0704 08:07:48.605408 27993 caffe.cpp:312] Batch 86, accuracy/top5 = 1
I0704 08:07:48.605412 27993 caffe.cpp:312] Batch 86, loss = 0.18
I0704 08:07:48.613657 27993 caffe.cpp:312] Batch 87, accuracy/top1 = 0.96
I0704 08:07:48.613672 27993 caffe.cpp:312] Batch 87, accuracy/top5 = 1
I0704 08:07:48.613674 27993 caffe.cpp:312] Batch 87, loss = 0.1
I0704 08:07:48.621924 27993 caffe.cpp:312] Batch 88, accuracy/top1 = 0.94
I0704 08:07:48.621937 27993 caffe.cpp:312] Batch 88, accuracy/top5 = 1
I0704 08:07:48.621940 27993 caffe.cpp:312] Batch 88, loss = 0.14
I0704 08:07:48.630183 27993 caffe.cpp:312] Batch 89, accuracy/top1 = 0.88
I0704 08:07:48.630201 27993 caffe.cpp:312] Batch 89, accuracy/top5 = 1
I0704 08:07:48.630204 27993 caffe.cpp:312] Batch 89, loss = 0.2
I0704 08:07:48.638300 27993 caffe.cpp:312] Batch 90, accuracy/top1 = 0.84
I0704 08:07:48.638317 27993 caffe.cpp:312] Batch 90, accuracy/top5 = 1
I0704 08:07:48.638319 27993 caffe.cpp:312] Batch 90, loss = 0.32
I0704 08:07:48.646574 27993 caffe.cpp:312] Batch 91, accuracy/top1 = 0.92
I0704 08:07:48.646587 27993 caffe.cpp:312] Batch 91, accuracy/top5 = 1
I0704 08:07:48.646590 27993 caffe.cpp:312] Batch 91, loss = 0.16
I0704 08:07:48.654858 27993 caffe.cpp:312] Batch 92, accuracy/top1 = 0.82
I0704 08:07:48.654870 27993 caffe.cpp:312] Batch 92, accuracy/top5 = 1
I0704 08:07:48.654873 27993 caffe.cpp:312] Batch 92, loss = 0.22
I0704 08:07:48.663075 27993 caffe.cpp:312] Batch 93, accuracy/top1 = 0.96
I0704 08:07:48.663089 27993 caffe.cpp:312] Batch 93, accuracy/top5 = 1
I0704 08:07:48.663091 27993 caffe.cpp:312] Batch 93, loss = 0.04
I0704 08:07:48.671326 27993 caffe.cpp:312] Batch 94, accuracy/top1 = 0.94
I0704 08:07:48.671344 27993 caffe.cpp:312] Batch 94, accuracy/top5 = 1
I0704 08:07:48.671346 27993 caffe.cpp:312] Batch 94, loss = 0.18
I0704 08:07:48.679502 27993 caffe.cpp:312] Batch 95, accuracy/top1 = 0.86
I0704 08:07:48.679514 27993 caffe.cpp:312] Batch 95, accuracy/top5 = 0.98
I0704 08:07:48.679517 27993 caffe.cpp:312] Batch 95, loss = 0.32
I0704 08:07:48.687713 27993 caffe.cpp:312] Batch 96, accuracy/top1 = 0.98
I0704 08:07:48.687726 27993 caffe.cpp:312] Batch 96, accuracy/top5 = 1
I0704 08:07:48.687728 27993 caffe.cpp:312] Batch 96, loss = 0
I0704 08:07:48.695912 27993 caffe.cpp:312] Batch 97, accuracy/top1 = 0.96
I0704 08:07:48.695926 27993 caffe.cpp:312] Batch 97, accuracy/top5 = 1
I0704 08:07:48.695929 27993 caffe.cpp:312] Batch 97, loss = 0.04
I0704 08:07:48.704177 27993 caffe.cpp:312] Batch 98, accuracy/top1 = 0.92
I0704 08:07:48.704190 27993 caffe.cpp:312] Batch 98, accuracy/top5 = 0.98
I0704 08:07:48.704192 27993 caffe.cpp:312] Batch 98, loss = 0.16
I0704 08:07:48.712383 27993 caffe.cpp:312] Batch 99, accuracy/top1 = 0.9
I0704 08:07:48.712390 27993 caffe.cpp:312] Batch 99, accuracy/top5 = 1
I0704 08:07:48.712393 27993 caffe.cpp:312] Batch 99, loss = 0.38
I0704 08:07:48.720633 27993 caffe.cpp:312] Batch 100, accuracy/top1 = 0.96
I0704 08:07:48.720640 27993 caffe.cpp:312] Batch 100, accuracy/top5 = 1
I0704 08:07:48.720643 27993 caffe.cpp:312] Batch 100, loss = 0.04
I0704 08:07:48.728821 27993 caffe.cpp:312] Batch 101, accuracy/top1 = 0.96
I0704 08:07:48.728828 27993 caffe.cpp:312] Batch 101, accuracy/top5 = 1
I0704 08:07:48.728830 27993 caffe.cpp:312] Batch 101, loss = 0.02
I0704 08:07:48.737038 27993 caffe.cpp:312] Batch 102, accuracy/top1 = 0.9
I0704 08:07:48.737046 27993 caffe.cpp:312] Batch 102, accuracy/top5 = 1
I0704 08:07:48.737056 27993 caffe.cpp:312] Batch 102, loss = 0.16
I0704 08:07:48.745218 27993 caffe.cpp:312] Batch 103, accuracy/top1 = 0.92
I0704 08:07:48.745225 27993 caffe.cpp:312] Batch 103, accuracy/top5 = 1
I0704 08:07:48.745229 27993 caffe.cpp:312] Batch 103, loss = 0.14
I0704 08:07:48.753432 27993 caffe.cpp:312] Batch 104, accuracy/top1 = 0.88
I0704 08:07:48.753438 27993 caffe.cpp:312] Batch 104, accuracy/top5 = 1
I0704 08:07:48.753442 27993 caffe.cpp:312] Batch 104, loss = 0.14
I0704 08:07:48.761590 27993 caffe.cpp:312] Batch 105, accuracy/top1 = 0.94
I0704 08:07:48.761596 27993 caffe.cpp:312] Batch 105, accuracy/top5 = 1
I0704 08:07:48.761598 27993 caffe.cpp:312] Batch 105, loss = 0.12
I0704 08:07:48.769852 27993 caffe.cpp:312] Batch 106, accuracy/top1 = 0.96
I0704 08:07:48.769860 27993 caffe.cpp:312] Batch 106, accuracy/top5 = 1
I0704 08:07:48.769862 27993 caffe.cpp:312] Batch 106, loss = 0.1
I0704 08:07:48.778000 27993 caffe.cpp:312] Batch 107, accuracy/top1 = 0.9
I0704 08:07:48.778007 27993 caffe.cpp:312] Batch 107, accuracy/top5 = 1
I0704 08:07:48.778010 27993 caffe.cpp:312] Batch 107, loss = 0.18
I0704 08:07:48.786187 27993 caffe.cpp:312] Batch 108, accuracy/top1 = 0.92
I0704 08:07:48.786195 27993 caffe.cpp:312] Batch 108, accuracy/top5 = 1
I0704 08:07:48.786197 27993 caffe.cpp:312] Batch 108, loss = 0.1
I0704 08:07:48.794353 27993 caffe.cpp:312] Batch 109, accuracy/top1 = 0.94
I0704 08:07:48.794360 27993 caffe.cpp:312] Batch 109, accuracy/top5 = 1
I0704 08:07:48.794363 27993 caffe.cpp:312] Batch 109, loss = 0.08
I0704 08:07:48.802553 27993 caffe.cpp:312] Batch 110, accuracy/top1 = 0.9
I0704 08:07:48.802561 27993 caffe.cpp:312] Batch 110, accuracy/top5 = 1
I0704 08:07:48.802563 27993 caffe.cpp:312] Batch 110, loss = 0.3
I0704 08:07:48.810746 27993 caffe.cpp:312] Batch 111, accuracy/top1 = 0.94
I0704 08:07:48.810755 27993 caffe.cpp:312] Batch 111, accuracy/top5 = 1
I0704 08:07:48.810756 27993 caffe.cpp:312] Batch 111, loss = 0.06
I0704 08:07:48.818912 27993 caffe.cpp:312] Batch 112, accuracy/top1 = 0.92
I0704 08:07:48.818918 27993 caffe.cpp:312] Batch 112, accuracy/top5 = 1
I0704 08:07:48.818920 27993 caffe.cpp:312] Batch 112, loss = 0.22
I0704 08:07:48.827082 27993 caffe.cpp:312] Batch 113, accuracy/top1 = 0.94
I0704 08:07:48.827090 27993 caffe.cpp:312] Batch 113, accuracy/top5 = 1
I0704 08:07:48.827092 27993 caffe.cpp:312] Batch 113, loss = 0.12
I0704 08:07:48.835223 27993 caffe.cpp:312] Batch 114, accuracy/top1 = 0.9
I0704 08:07:48.835230 27993 caffe.cpp:312] Batch 114, accuracy/top5 = 0.98
I0704 08:07:48.835233 27993 caffe.cpp:312] Batch 114, loss = 0.22
I0704 08:07:48.843399 27993 caffe.cpp:312] Batch 115, accuracy/top1 = 0.98
I0704 08:07:48.843405 27993 caffe.cpp:312] Batch 115, accuracy/top5 = 1
I0704 08:07:48.843408 27993 caffe.cpp:312] Batch 115, loss = 0.02
I0704 08:07:48.851527 27993 caffe.cpp:312] Batch 116, accuracy/top1 = 0.84
I0704 08:07:48.851536 27993 caffe.cpp:312] Batch 116, accuracy/top5 = 1
I0704 08:07:48.851537 27993 caffe.cpp:312] Batch 116, loss = 0.16
I0704 08:07:48.859688 27993 caffe.cpp:312] Batch 117, accuracy/top1 = 0.86
I0704 08:07:48.859694 27993 caffe.cpp:312] Batch 117, accuracy/top5 = 1
I0704 08:07:48.859697 27993 caffe.cpp:312] Batch 117, loss = 0.22
I0704 08:07:48.867877 27993 caffe.cpp:312] Batch 118, accuracy/top1 = 0.9
I0704 08:07:48.867883 27993 caffe.cpp:312] Batch 118, accuracy/top5 = 1
I0704 08:07:48.867887 27993 caffe.cpp:312] Batch 118, loss = 0.08
I0704 08:07:48.875998 27993 caffe.cpp:312] Batch 119, accuracy/top1 = 0.88
I0704 08:07:48.876004 27993 caffe.cpp:312] Batch 119, accuracy/top5 = 1
I0704 08:07:48.876008 27993 caffe.cpp:312] Batch 119, loss = 0.28
I0704 08:07:48.884263 27993 caffe.cpp:312] Batch 120, accuracy/top1 = 0.92
I0704 08:07:48.884269 27993 caffe.cpp:312] Batch 120, accuracy/top5 = 1
I0704 08:07:48.884272 27993 caffe.cpp:312] Batch 120, loss = 0.14
I0704 08:07:48.892422 27993 caffe.cpp:312] Batch 121, accuracy/top1 = 0.92
I0704 08:07:48.892429 27993 caffe.cpp:312] Batch 121, accuracy/top5 = 1
I0704 08:07:48.892438 27993 caffe.cpp:312] Batch 121, loss = 0.2
I0704 08:07:48.900635 27993 caffe.cpp:312] Batch 122, accuracy/top1 = 0.9
I0704 08:07:48.900641 27993 caffe.cpp:312] Batch 122, accuracy/top5 = 1
I0704 08:07:48.900645 27993 caffe.cpp:312] Batch 122, loss = 0.1
I0704 08:07:48.908807 27993 caffe.cpp:312] Batch 123, accuracy/top1 = 0.9
I0704 08:07:48.908814 27993 caffe.cpp:312] Batch 123, accuracy/top5 = 1
I0704 08:07:48.908818 27993 caffe.cpp:312] Batch 123, loss = 0.2
I0704 08:07:48.916985 27993 caffe.cpp:312] Batch 124, accuracy/top1 = 0.92
I0704 08:07:48.916991 27993 caffe.cpp:312] Batch 124, accuracy/top5 = 1
I0704 08:07:48.916995 27993 caffe.cpp:312] Batch 124, loss = 0.14
I0704 08:07:48.925127 27993 caffe.cpp:312] Batch 125, accuracy/top1 = 0.94
I0704 08:07:48.925134 27993 caffe.cpp:312] Batch 125, accuracy/top5 = 1
I0704 08:07:48.925137 27993 caffe.cpp:312] Batch 125, loss = 0.14
I0704 08:07:48.933313 27993 caffe.cpp:312] Batch 126, accuracy/top1 = 0.96
I0704 08:07:48.933320 27993 caffe.cpp:312] Batch 126, accuracy/top5 = 1
I0704 08:07:48.933322 27993 caffe.cpp:312] Batch 126, loss = 0.04
I0704 08:07:48.941470 27993 caffe.cpp:312] Batch 127, accuracy/top1 = 0.96
I0704 08:07:48.941478 27993 caffe.cpp:312] Batch 127, accuracy/top5 = 1
I0704 08:07:48.941480 27993 caffe.cpp:312] Batch 127, loss = 0.02
I0704 08:07:48.949692 27993 caffe.cpp:312] Batch 128, accuracy/top1 = 0.86
I0704 08:07:48.949698 27993 caffe.cpp:312] Batch 128, accuracy/top5 = 1
I0704 08:07:48.949700 27993 caffe.cpp:312] Batch 128, loss = 0.18
I0704 08:07:48.957895 27993 caffe.cpp:312] Batch 129, accuracy/top1 = 0.96
I0704 08:07:48.957902 27993 caffe.cpp:312] Batch 129, accuracy/top5 = 1
I0704 08:07:48.957906 27993 caffe.cpp:312] Batch 129, loss = 0.08
I0704 08:07:48.966018 27993 caffe.cpp:312] Batch 130, accuracy/top1 = 0.9
I0704 08:07:48.966025 27993 caffe.cpp:312] Batch 130, accuracy/top5 = 1
I0704 08:07:48.966027 27993 caffe.cpp:312] Batch 130, loss = 0.18
I0704 08:07:48.974154 27993 caffe.cpp:312] Batch 131, accuracy/top1 = 0.9
I0704 08:07:48.974161 27993 caffe.cpp:312] Batch 131, accuracy/top5 = 1
I0704 08:07:48.974164 27993 caffe.cpp:312] Batch 131, loss = 0.16
I0704 08:07:48.982321 27993 caffe.cpp:312] Batch 132, accuracy/top1 = 0.96
I0704 08:07:48.982327 27993 caffe.cpp:312] Batch 132, accuracy/top5 = 1
I0704 08:07:48.982331 27993 caffe.cpp:312] Batch 132, loss = 0.06
I0704 08:07:48.990487 27993 caffe.cpp:312] Batch 133, accuracy/top1 = 0.94
I0704 08:07:48.990494 27993 caffe.cpp:312] Batch 133, accuracy/top5 = 1
I0704 08:07:48.990496 27993 caffe.cpp:312] Batch 133, loss = 0.06
I0704 08:07:48.998677 27993 caffe.cpp:312] Batch 134, accuracy/top1 = 0.92
I0704 08:07:48.998685 27993 caffe.cpp:312] Batch 134, accuracy/top5 = 1
I0704 08:07:48.998687 27993 caffe.cpp:312] Batch 134, loss = 0.16
I0704 08:07:49.006898 27993 caffe.cpp:312] Batch 135, accuracy/top1 = 0.86
I0704 08:07:49.006906 27993 caffe.cpp:312] Batch 135, accuracy/top5 = 0.98
I0704 08:07:49.006907 27993 caffe.cpp:312] Batch 135, loss = 0.44
I0704 08:07:49.015136 27993 caffe.cpp:312] Batch 136, accuracy/top1 = 0.96
I0704 08:07:49.015143 27993 caffe.cpp:312] Batch 136, accuracy/top5 = 1
I0704 08:07:49.015146 27993 caffe.cpp:312] Batch 136, loss = 0.08
I0704 08:07:49.023334 27993 caffe.cpp:312] Batch 137, accuracy/top1 = 0.9
I0704 08:07:49.023340 27993 caffe.cpp:312] Batch 137, accuracy/top5 = 1
I0704 08:07:49.023344 27993 caffe.cpp:312] Batch 137, loss = 0.2
I0704 08:07:49.031550 27993 caffe.cpp:312] Batch 138, accuracy/top1 = 0.92
I0704 08:07:49.031558 27993 caffe.cpp:312] Batch 138, accuracy/top5 = 0.98
I0704 08:07:49.031560 27993 caffe.cpp:312] Batch 138, loss = 0.14
I0704 08:07:49.039695 27993 caffe.cpp:312] Batch 139, accuracy/top1 = 0.88
I0704 08:07:49.039701 27993 caffe.cpp:312] Batch 139, accuracy/top5 = 1
I0704 08:07:49.039705 27993 caffe.cpp:312] Batch 139, loss = 0.24
I0704 08:07:49.047844 27993 caffe.cpp:312] Batch 140, accuracy/top1 = 0.9
I0704 08:07:49.047852 27993 caffe.cpp:312] Batch 140, accuracy/top5 = 1
I0704 08:07:49.047854 27993 caffe.cpp:312] Batch 140, loss = 0.3
I0704 08:07:49.056033 27993 caffe.cpp:312] Batch 141, accuracy/top1 = 0.92
I0704 08:07:49.056041 27993 caffe.cpp:312] Batch 141, accuracy/top5 = 1
I0704 08:07:49.056043 27993 caffe.cpp:312] Batch 141, loss = 0.18
I0704 08:07:49.064234 27993 caffe.cpp:312] Batch 142, accuracy/top1 = 0.94
I0704 08:07:49.064241 27993 caffe.cpp:312] Batch 142, accuracy/top5 = 1
I0704 08:07:49.064244 27993 caffe.cpp:312] Batch 142, loss = 0.12
I0704 08:07:49.072350 27993 caffe.cpp:312] Batch 143, accuracy/top1 = 0.94
I0704 08:07:49.072358 27993 caffe.cpp:312] Batch 143, accuracy/top5 = 1
I0704 08:07:49.072360 27993 caffe.cpp:312] Batch 143, loss = 0.12
I0704 08:07:49.080519 27993 caffe.cpp:312] Batch 144, accuracy/top1 = 0.92
I0704 08:07:49.080526 27993 caffe.cpp:312] Batch 144, accuracy/top5 = 1
I0704 08:07:49.080529 27993 caffe.cpp:312] Batch 144, loss = 0.2
I0704 08:07:49.088708 27993 caffe.cpp:312] Batch 145, accuracy/top1 = 0.94
I0704 08:07:49.088716 27993 caffe.cpp:312] Batch 145, accuracy/top5 = 1
I0704 08:07:49.088718 27993 caffe.cpp:312] Batch 145, loss = 0.1
I0704 08:07:49.096864 27993 caffe.cpp:312] Batch 146, accuracy/top1 = 0.94
I0704 08:07:49.096873 27993 caffe.cpp:312] Batch 146, accuracy/top5 = 1
I0704 08:07:49.096874 27993 caffe.cpp:312] Batch 146, loss = 0.14
I0704 08:07:49.105036 27993 caffe.cpp:312] Batch 147, accuracy/top1 = 0.92
I0704 08:07:49.105042 27993 caffe.cpp:312] Batch 147, accuracy/top5 = 1
I0704 08:07:49.105044 27993 caffe.cpp:312] Batch 147, loss = 0.18
I0704 08:07:49.113194 27993 caffe.cpp:312] Batch 148, accuracy/top1 = 0.92
I0704 08:07:49.113201 27993 caffe.cpp:312] Batch 148, accuracy/top5 = 1
I0704 08:07:49.113204 27993 caffe.cpp:312] Batch 148, loss = 0.08
I0704 08:07:49.121388 27993 caffe.cpp:312] Batch 149, accuracy/top1 = 0.92
I0704 08:07:49.121395 27993 caffe.cpp:312] Batch 149, accuracy/top5 = 1
I0704 08:07:49.121398 27993 caffe.cpp:312] Batch 149, loss = 0.08
I0704 08:07:49.129513 27993 caffe.cpp:312] Batch 150, accuracy/top1 = 0.92
I0704 08:07:49.129519 27993 caffe.cpp:312] Batch 150, accuracy/top5 = 0.98
I0704 08:07:49.129521 27993 caffe.cpp:312] Batch 150, loss = 0.16
I0704 08:07:49.137701 27993 caffe.cpp:312] Batch 151, accuracy/top1 = 0.9
I0704 08:07:49.137707 27993 caffe.cpp:312] Batch 151, accuracy/top5 = 1
I0704 08:07:49.137709 27993 caffe.cpp:312] Batch 151, loss = 0.16
I0704 08:07:49.145917 27993 caffe.cpp:312] Batch 152, accuracy/top1 = 0.9
I0704 08:07:49.145925 27993 caffe.cpp:312] Batch 152, accuracy/top5 = 1
I0704 08:07:49.145927 27993 caffe.cpp:312] Batch 152, loss = 0.14
I0704 08:07:49.154130 27993 caffe.cpp:312] Batch 153, accuracy/top1 = 0.92
I0704 08:07:49.154137 27993 caffe.cpp:312] Batch 153, accuracy/top5 = 0.98
I0704 08:07:49.154140 27993 caffe.cpp:312] Batch 153, loss = 0.34
I0704 08:07:49.162297 27993 caffe.cpp:312] Batch 154, accuracy/top1 = 0.94
I0704 08:07:49.162304 27993 caffe.cpp:312] Batch 154, accuracy/top5 = 1
I0704 08:07:49.162307 27993 caffe.cpp:312] Batch 154, loss = 0.08
I0704 08:07:49.170442 27993 caffe.cpp:312] Batch 155, accuracy/top1 = 0.86
I0704 08:07:49.170449 27993 caffe.cpp:312] Batch 155, accuracy/top5 = 1
I0704 08:07:49.170451 27993 caffe.cpp:312] Batch 155, loss = 0.36
I0704 08:07:49.178568 27993 caffe.cpp:312] Batch 156, accuracy/top1 = 0.88
I0704 08:07:49.178575 27993 caffe.cpp:312] Batch 156, accuracy/top5 = 1
I0704 08:07:49.178577 27993 caffe.cpp:312] Batch 156, loss = 0.22
I0704 08:07:49.186784 27993 caffe.cpp:312] Batch 157, accuracy/top1 = 0.92
I0704 08:07:49.186790 27993 caffe.cpp:312] Batch 157, accuracy/top5 = 0.98
I0704 08:07:49.186794 27993 caffe.cpp:312] Batch 157, loss = 0.22
I0704 08:07:49.194994 27993 caffe.cpp:312] Batch 158, accuracy/top1 = 0.96
I0704 08:07:49.195001 27993 caffe.cpp:312] Batch 158, accuracy/top5 = 1
I0704 08:07:49.195004 27993 caffe.cpp:312] Batch 158, loss = 0.1
I0704 08:07:49.203194 27993 caffe.cpp:312] Batch 159, accuracy/top1 = 0.94
I0704 08:07:49.203202 27993 caffe.cpp:312] Batch 159, accuracy/top5 = 1
I0704 08:07:49.203204 27993 caffe.cpp:312] Batch 159, loss = 0.08
I0704 08:07:49.211371 27993 caffe.cpp:312] Batch 160, accuracy/top1 = 0.92
I0704 08:07:49.211385 27993 caffe.cpp:312] Batch 160, accuracy/top5 = 1
I0704 08:07:49.211388 27993 caffe.cpp:312] Batch 160, loss = 0.1
I0704 08:07:49.219573 27993 caffe.cpp:312] Batch 161, accuracy/top1 = 0.94
I0704 08:07:49.219579 27993 caffe.cpp:312] Batch 161, accuracy/top5 = 1
I0704 08:07:49.219583 27993 caffe.cpp:312] Batch 161, loss = 0.1
I0704 08:07:49.227720 27993 caffe.cpp:312] Batch 162, accuracy/top1 = 0.94
I0704 08:07:49.227726 27993 caffe.cpp:312] Batch 162, accuracy/top5 = 1
I0704 08:07:49.227730 27993 caffe.cpp:312] Batch 162, loss = 0.14
I0704 08:07:49.235911 27993 caffe.cpp:312] Batch 163, accuracy/top1 = 0.94
I0704 08:07:49.235918 27993 caffe.cpp:312] Batch 163, accuracy/top5 = 1
I0704 08:07:49.235921 27993 caffe.cpp:312] Batch 163, loss = 0.12
I0704 08:07:49.243952 27993 caffe.cpp:312] Batch 164, accuracy/top1 = 0.92
I0704 08:07:49.243958 27993 caffe.cpp:312] Batch 164, accuracy/top5 = 1
I0704 08:07:49.243960 27993 caffe.cpp:312] Batch 164, loss = 0.1
I0704 08:07:49.252162 27993 caffe.cpp:312] Batch 165, accuracy/top1 = 0.86
I0704 08:07:49.252169 27993 caffe.cpp:312] Batch 165, accuracy/top5 = 1
I0704 08:07:49.252172 27993 caffe.cpp:312] Batch 165, loss = 0.2
I0704 08:07:49.260349 27993 caffe.cpp:312] Batch 166, accuracy/top1 = 0.94
I0704 08:07:49.260355 27993 caffe.cpp:312] Batch 166, accuracy/top5 = 1
I0704 08:07:49.260357 27993 caffe.cpp:312] Batch 166, loss = 0.1
I0704 08:07:49.268559 27993 caffe.cpp:312] Batch 167, accuracy/top1 = 0.96
I0704 08:07:49.268566 27993 caffe.cpp:312] Batch 167, accuracy/top5 = 1
I0704 08:07:49.268569 27993 caffe.cpp:312] Batch 167, loss = 0.08
I0704 08:07:49.276758 27993 caffe.cpp:312] Batch 168, accuracy/top1 = 0.88
I0704 08:07:49.276765 27993 caffe.cpp:312] Batch 168, accuracy/top5 = 1
I0704 08:07:49.276768 27993 caffe.cpp:312] Batch 168, loss = 0.28
I0704 08:07:49.284924 27993 caffe.cpp:312] Batch 169, accuracy/top1 = 0.84
I0704 08:07:49.284931 27993 caffe.cpp:312] Batch 169, accuracy/top5 = 0.98
I0704 08:07:49.284934 27993 caffe.cpp:312] Batch 169, loss = 0.2
I0704 08:07:49.293118 27993 caffe.cpp:312] Batch 170, accuracy/top1 = 0.9
I0704 08:07:49.293125 27993 caffe.cpp:312] Batch 170, accuracy/top5 = 0.98
I0704 08:07:49.293128 27993 caffe.cpp:312] Batch 170, loss = 0.3
I0704 08:07:49.301249 27993 caffe.cpp:312] Batch 171, accuracy/top1 = 0.88
I0704 08:07:49.301255 27993 caffe.cpp:312] Batch 171, accuracy/top5 = 1
I0704 08:07:49.301259 27993 caffe.cpp:312] Batch 171, loss = 0.32
I0704 08:07:49.309437 27993 caffe.cpp:312] Batch 172, accuracy/top1 = 0.88
I0704 08:07:49.309444 27993 caffe.cpp:312] Batch 172, accuracy/top5 = 1
I0704 08:07:49.309448 27993 caffe.cpp:312] Batch 172, loss = 0.12
I0704 08:07:49.317651 27993 caffe.cpp:312] Batch 173, accuracy/top1 = 0.96
I0704 08:07:49.317658 27993 caffe.cpp:312] Batch 173, accuracy/top5 = 1
I0704 08:07:49.317662 27993 caffe.cpp:312] Batch 173, loss = 0.04
I0704 08:07:49.325829 27993 caffe.cpp:312] Batch 174, accuracy/top1 = 0.86
I0704 08:07:49.325835 27993 caffe.cpp:312] Batch 174, accuracy/top5 = 1
I0704 08:07:49.325839 27993 caffe.cpp:312] Batch 174, loss = 0.46
I0704 08:07:49.334022 27993 caffe.cpp:312] Batch 175, accuracy/top1 = 0.98
I0704 08:07:49.334028 27993 caffe.cpp:312] Batch 175, accuracy/top5 = 1
I0704 08:07:49.334031 27993 caffe.cpp:312] Batch 175, loss = 0.02
I0704 08:07:49.342233 27993 caffe.cpp:312] Batch 176, accuracy/top1 = 0.9
I0704 08:07:49.342241 27993 caffe.cpp:312] Batch 176, accuracy/top5 = 0.98
I0704 08:07:49.342242 27993 caffe.cpp:312] Batch 176, loss = 0.32
I0704 08:07:49.350430 27993 caffe.cpp:312] Batch 177, accuracy/top1 = 0.94
I0704 08:07:49.350436 27993 caffe.cpp:312] Batch 177, accuracy/top5 = 1
I0704 08:07:49.350438 27993 caffe.cpp:312] Batch 177, loss = 0.08
I0704 08:07:49.358583 27993 caffe.cpp:312] Batch 178, accuracy/top1 = 0.9
I0704 08:07:49.358590 27993 caffe.cpp:312] Batch 178, accuracy/top5 = 1
I0704 08:07:49.358593 27993 caffe.cpp:312] Batch 178, loss = 0.28
I0704 08:07:49.366760 27993 caffe.cpp:312] Batch 179, accuracy/top1 = 0.96
I0704 08:07:49.366773 27993 caffe.cpp:312] Batch 179, accuracy/top5 = 1
I0704 08:07:49.366776 27993 caffe.cpp:312] Batch 179, loss = 0.14
I0704 08:07:49.374948 27993 caffe.cpp:312] Batch 180, accuracy/top1 = 0.92
I0704 08:07:49.374955 27993 caffe.cpp:312] Batch 180, accuracy/top5 = 1
I0704 08:07:49.374958 27993 caffe.cpp:312] Batch 180, loss = 0.06
I0704 08:07:49.383204 27993 caffe.cpp:312] Batch 181, accuracy/top1 = 0.96
I0704 08:07:49.383211 27993 caffe.cpp:312] Batch 181, accuracy/top5 = 1
I0704 08:07:49.383214 27993 caffe.cpp:312] Batch 181, loss = 0.06
I0704 08:07:49.391399 27993 caffe.cpp:312] Batch 182, accuracy/top1 = 0.92
I0704 08:07:49.391407 27993 caffe.cpp:312] Batch 182, accuracy/top5 = 0.98
I0704 08:07:49.391409 27993 caffe.cpp:312] Batch 182, loss = 0.14
I0704 08:07:49.399559 27993 caffe.cpp:312] Batch 183, accuracy/top1 = 0.96
I0704 08:07:49.399566 27993 caffe.cpp:312] Batch 183, accuracy/top5 = 1
I0704 08:07:49.399569 27993 caffe.cpp:312] Batch 183, loss = 0.1
I0704 08:07:49.407708 27993 caffe.cpp:312] Batch 184, accuracy/top1 = 0.88
I0704 08:07:49.407716 27993 caffe.cpp:312] Batch 184, accuracy/top5 = 1
I0704 08:07:49.407718 27993 caffe.cpp:312] Batch 184, loss = 0.26
I0704 08:07:49.415904 27993 caffe.cpp:312] Batch 185, accuracy/top1 = 0.92
I0704 08:07:49.415910 27993 caffe.cpp:312] Batch 185, accuracy/top5 = 1
I0704 08:07:49.415912 27993 caffe.cpp:312] Batch 185, loss = 0.16
I0704 08:07:49.424067 27993 caffe.cpp:312] Batch 186, accuracy/top1 = 0.92
I0704 08:07:49.424073 27993 caffe.cpp:312] Batch 186, accuracy/top5 = 1
I0704 08:07:49.424077 27993 caffe.cpp:312] Batch 186, loss = 0.08
I0704 08:07:49.432200 27993 caffe.cpp:312] Batch 187, accuracy/top1 = 0.86
I0704 08:07:49.432207 27993 caffe.cpp:312] Batch 187, accuracy/top5 = 0.98
I0704 08:07:49.432210 27993 caffe.cpp:312] Batch 187, loss = 0.34
I0704 08:07:49.440409 27993 caffe.cpp:312] Batch 188, accuracy/top1 = 0.88
I0704 08:07:49.440417 27993 caffe.cpp:312] Batch 188, accuracy/top5 = 1
I0704 08:07:49.440419 27993 caffe.cpp:312] Batch 188, loss = 0.08
I0704 08:07:49.448606 27993 caffe.cpp:312] Batch 189, accuracy/top1 = 0.98
I0704 08:07:49.448612 27993 caffe.cpp:312] Batch 189, accuracy/top5 = 1
I0704 08:07:49.448616 27993 caffe.cpp:312] Batch 189, loss = 0.1
I0704 08:07:49.456779 27993 caffe.cpp:312] Batch 190, accuracy/top1 = 0.94
I0704 08:07:49.456786 27993 caffe.cpp:312] Batch 190, accuracy/top5 = 1
I0704 08:07:49.456789 27993 caffe.cpp:312] Batch 190, loss = 0.08
I0704 08:07:49.465013 27993 caffe.cpp:312] Batch 191, accuracy/top1 = 0.94
I0704 08:07:49.465019 27993 caffe.cpp:312] Batch 191, accuracy/top5 = 1
I0704 08:07:49.465023 27993 caffe.cpp:312] Batch 191, loss = 0.08
I0704 08:07:49.473233 27993 caffe.cpp:312] Batch 192, accuracy/top1 = 0.9
I0704 08:07:49.473240 27993 caffe.cpp:312] Batch 192, accuracy/top5 = 1
I0704 08:07:49.473243 27993 caffe.cpp:312] Batch 192, loss = 0.14
I0704 08:07:49.481413 27993 caffe.cpp:312] Batch 193, accuracy/top1 = 0.98
I0704 08:07:49.481420 27993 caffe.cpp:312] Batch 193, accuracy/top5 = 1
I0704 08:07:49.481423 27993 caffe.cpp:312] Batch 193, loss = 0.02
I0704 08:07:49.489542 27993 caffe.cpp:312] Batch 194, accuracy/top1 = 0.94
I0704 08:07:49.489549 27993 caffe.cpp:312] Batch 194, accuracy/top5 = 0.98
I0704 08:07:49.489552 27993 caffe.cpp:312] Batch 194, loss = 0.2
I0704 08:07:49.497681 27993 caffe.cpp:312] Batch 195, accuracy/top1 = 0.9
I0704 08:07:49.497689 27993 caffe.cpp:312] Batch 195, accuracy/top5 = 1
I0704 08:07:49.497691 27993 caffe.cpp:312] Batch 195, loss = 0.24
I0704 08:07:49.505903 27993 caffe.cpp:312] Batch 196, accuracy/top1 = 0.84
I0704 08:07:49.505910 27993 caffe.cpp:312] Batch 196, accuracy/top5 = 1
I0704 08:07:49.505913 27993 caffe.cpp:312] Batch 196, loss = 0.3
I0704 08:07:49.514127 27993 caffe.cpp:312] Batch 197, accuracy/top1 = 0.9
I0704 08:07:49.514133 27993 caffe.cpp:312] Batch 197, accuracy/top5 = 1
I0704 08:07:49.514137 27993 caffe.cpp:312] Batch 197, loss = 0.16
I0704 08:07:49.522322 27993 caffe.cpp:312] Batch 198, accuracy/top1 = 0.92
I0704 08:07:49.522330 27993 caffe.cpp:312] Batch 198, accuracy/top5 = 1
I0704 08:07:49.522341 27993 caffe.cpp:312] Batch 198, loss = 0.1
I0704 08:07:49.530760 27993 caffe.cpp:312] Batch 199, accuracy/top1 = 0.94
I0704 08:07:49.530782 27993 caffe.cpp:312] Batch 199, accuracy/top5 = 1
I0704 08:07:49.530786 27993 caffe.cpp:312] Batch 199, loss = 0.12
I0704 08:07:49.530788 27993 caffe.cpp:317] Loss: 0.166
I0704 08:07:49.530797 27993 caffe.cpp:329] accuracy/top1 = 0.9155
I0704 08:07:49.530802 27993 caffe.cpp:329] accuracy/top5 = 0.9967
I0704 08:07:49.530807 27993 caffe.cpp:329] loss = 0.166 (* 1 = 0.166 loss)
